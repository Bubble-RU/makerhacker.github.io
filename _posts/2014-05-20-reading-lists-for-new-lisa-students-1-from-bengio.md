---
layout: post
title: reading lists for new lisa students 1 from bengio
categories: [paper]
tags: [bengio]
---


### Research in General

1. [How to write a great research paper](https://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/writing-a-paper-slides.pdf)

### Basics of deep learning

1. [Learning deep architectures for AI](http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf)

2. [Practical recommendations for gradient-based training of deep architectures](http://arxiv.org/pdf/1206.5533v2.pdf)

* [Quick’n’dirty introduction to deep learning: Advances in Deep Learning ](http://www.kyunghyuncho.me/)

* [A fast learning algorithm for deep belief nets](http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)

* [Greedy Layer-Wise Training of Deep Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf)

* [Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&rep=rep1&type=pdf)

* [Contractive auto-encoders: Explicit invariance during feature extraction](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf)

* [Why does unsupervised pre-training help deep learning?](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf)

* [An Analysis of Single Layer Networks in Unsupervised Feature Learning](http://web.eecs.umich.edu/~honglak/nipsdlufl10-AnalysisSingleLayerUnsupervisedFeatureLearning.pdf)

* [The importance of Encoding Versus Training With Sparse Coding and Vector Quantization](http://www.stanford.edu/~acoates/papers/coatesng_icml_2011.pdf)

* [Representation Learning: A Review and New Perspectives](http://arxiv.org/pdf/1206.5538v3.pdf)  

* [Deep Learning of Representations: Looking Forward](http://arxiv.org/pdf/1305.0445v2.pdf)  

* [Measuring Invariances in Deep Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2009_0463.pdf)

* [Neural networks course at USherbrooke](http://info.usherbrooke.ca/hlarochelle/cours/ift725_A2013/contenu.html) [[youtube](http://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)]

### Feedforward nets

* “[Improving Neural Nets with Dropout](http://www.cs.toronto.edu/~nitish/msc_thesis.pdf)” by Nitish Srivastava

* [“Deep Sparse Rectifier Neural Networks”](http://deeplearningworkshopnips2010.files.wordpress.com/2010/11/nipswrkshp2010-cameraready.pdf)

* [“What is the best multi-stage architecture for object recognition?”](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf)

* “[Maxout Networks](http://arxiv.org/pdf/1302.4389v4.pdf)”

### MCMC

* [Iain Murray’s MLSS slides](http://mlg.eng.cam.ac.uk/mlss09/mlss_slides/Murray_1.pdf)

* [Radford Neal’s Review Paper](http://www.cs.toronto.edu/pub/radford/review.pdf) (old but still very comprehensive)

* [Better Mixing via Deep Representations](http://arxiv.org/pdf/1207.4404v1.pdf)

