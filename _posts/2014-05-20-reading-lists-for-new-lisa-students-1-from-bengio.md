---
layout: post
title: reading lists for new lisa students 1 from bengio
categories: [paper]
tags: [bengio]
---


info coming from [bengio](https://docs.google.com/document/d/1IXF3h0RU5zz4ukmTrVKVotPQypChscNGf5k6E25HGvA/edit#heading=h.q1psyg5jqftd)

### Research in General

[1] [How to write a great research paper](https://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/writing-a-paper-slides.pdf)

### Basics of deep learning

[1] [Learning deep architectures for AI](http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf)

[2] [Practical recommendations for gradient-based training of deep architectures](http://arxiv.org/pdf/1206.5533v2.pdf)

[3] [Quick’n’dirty introduction to deep learning: Advances in Deep Learning ](http://www.kyunghyuncho.me/)

[4] [A fast learning algorithm for deep belief nets](http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)

[5] [Greedy Layer-Wise Training of Deep Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf)

[6] [Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&rep=rep1&type=pdf)

[7] [Contractive auto-encoders: Explicit invariance during feature extraction](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf)

[8] [Why does unsupervised pre-training help deep learning?](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf)

[9] [An Analysis of Single Layer Networks in Unsupervised Feature Learning](http://web.eecs.umich.edu/~honglak/nipsdlufl10-AnalysisSingleLayerUnsupervisedFeatureLearning.pdf)

[10] [The importance of Encoding Versus Training With Sparse Coding and Vector Quantization](http://www.stanford.edu/~acoates/papers/coatesng_icml_2011.pdf)

[11] [Representation Learning: A Review and New Perspectives](http://arxiv.org/pdf/1206.5538v3.pdf)  

[12] [Deep Learning of Representations: Looking Forward](http://arxiv.org/pdf/1305.0445v2.pdf)  

[13] [Measuring Invariances in Deep Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2009_0463.pdf)

[14] [Neural networks course at USherbrooke](http://info.usherbrooke.ca/hlarochelle/cours/ift725_A2013/contenu.html) [[youtube](http://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)]

### Feedforward nets

[1] “[Improving Neural Nets with Dropout](http://www.cs.toronto.edu/~nitish/msc_thesis.pdf)” by Nitish Srivastava

[2] [“Deep Sparse Rectifier Neural Networks”](http://deeplearningworkshopnips2010.files.wordpress.com/2010/11/nipswrkshp2010-cameraready.pdf)

[3] [“What is the best multi-stage architecture for object recognition?”](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf)

[4] “[Maxout Networks](http://arxiv.org/pdf/1302.4389v4.pdf)”

### MCMC

[1] [Iain Murray’s MLSS slides](http://mlg.eng.cam.ac.uk/mlss09/mlss_slides/Murray_1.pdf)

[2] [Radford Neal’s Review Paper](http://www.cs.toronto.edu/pub/radford/review.pdf) (old but still very comprehensive)

[3] [Better Mixing via Deep Representations](http://arxiv.org/pdf/1207.4404v1.pdf)

