---
layout: post
title: reading lists for new lisa students from bengio
categories: [paper]
tags: [bengio]
---


Research in General

* [How to write a great researchpaper](https://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/writing-a-paper-slides.pdf)



Basics of deep learning

* [Learning deep architectures for AI](http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf)

* [Practical recommendations for gradient-based training of deep architectures](http://arxiv.org/abs/1206.5533)

* [Quick’n’dirty introduction to deep learning: Advances in Deep Learning ](http://www.kyunghyuncho.me/)

* [A fast learning algorithm for deep belief nets](http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)

* [Greedy Layer-Wise Training of Deep Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf)

* [Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&rep=rep1&type=pdf)

* [Contractive auto-encoders: Explicit invariance during feature extraction](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf)

* [Why does unsupervised pre-training help deep learning?](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf)

* [An Analysis of Single Layer Networks in Unsupervised Feature Learning](http:
//www.google.com/url?q=http%3A%2F%2Fweb.eecs.umich.edu%2F%7Ehonglak%2Fnipsdluf
l10-AnalysisSingleLayerUnsupervisedFeatureLearning.pdf&sa=D&sntz=1&usg
=AFQjCNHjvI408ncLS5YgTa_J64e-Jh1qnQ)

* [The importance of Encoding Versus Training With Sparse Coding and Vector ](h
ttp://www.google.com/url?q=http%3A%2F%2Fwww.stanford.edu%2F%7Eacoates%2Fpapers
%2Fcoatesng_icml_2011.pdf&sa=D&sntz=1&usg=AFQjCNGHddYwJxzicEE-iyX2LKDw9Rlydg)

[Quantization](http://www.google.com/url?q=http%3A%2F%2Fwww.stanford.edu%2F%7E
acoates%2Fpapers%2Fcoatesng_icml_2011.pdf&sa=D&sntz=1&usg=AFQjCNGHddYwJxzicEE-
iyX2LKDw9Rlydg)

* [Representation Learning: A Review and New Perspectives](https://www.google.c
om/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1206.5538&sa=D&sntz=1&usg=AFQjCNFklJm
vtB8_88U8CRNPtSLkh0HWZA)  

* [Deep Learning of Representations: Looking Forward](https://www.google.com/ur
l?q=https%3A%2F%2Farxiv.org%2Fabs%2F1305.0445&sa=D&sntz=1&usg=AFQjCNF1JKHUH70c
nLd5ynOaSOmMiIAnrQ)  

* [Measuring Invariances in Deep Networks](http://www.google.com/url?q=http%3A%
2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fpaper_files%2FNIPS2009_0463.pdf&sa
=D&sntz=1&usg=AFQjCNEaMPGo7WC-_c0m1rUjSAQk5p_xvw)

* [Neural networks course at USherbrooke](http://www.google.com/url?q=http%3A%2
F%2Finfo.usherbrooke.ca%2Fhlarochelle%2Fcours%2Fift725_A2013%2Fcontenu.html&sa
=D&sntz=1&usg=AFQjCNGEhetktocQxx1Q0qgwjDeUSULwaQ) [[youtube](http://www.youtub
e.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)]

Feedforward nets

* “[Improving Neural Nets with Dropout](http://www.google.com/url?q=http%3A%2F%
2Fwww.cs.toronto.edu%2F%7Enitish%2Fmsc_thesis.pdf&sa=D&sntz=1&usg=AFQjCNGR6u4x
DCoM9c493EE2Z1x9JpnMuw)” by Nitish Srivastava

* [“Deep Sparse Rectifier Neural Networks”](http://www.google.com/url?q=http%3A
%2F%2Fdeeplearningworkshopnips2010.files.wordpress.com%2F2010%2F11%2Fnipswrksh
p2010-cameraready.pdf&sa=D&sntz=1&usg=AFQjCNEphdUoGlanDdL9ZExRYcyHYw17Gw)

* [“What is the best multi-stage architecture for object recognition?”](http://
www.google.com/url?q=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf
%2Fjarrett-iccv-09.pdf&sa=D&sntz=1&usg=AFQjCNFuJu2nCM9kQUSy-G7EnJu3if9mRQ)

* “[Maxout Networks](http://www.google.com/url?q=http%3A%2F%2Farxiv.org%2Fpdf%2
F1302.4389v4.pdf&sa=D&sntz=1&usg=AFQjCNFr8ZfT0cATbhxVER6pDcoeQ0Yqeg)”

MCMC

* [Iain Murray’s MLSS slides](http://www.google.com/url?q=http%3A%2F%2Fmlg.eng.
cam.ac.uk%2Fmlss09%2Fmlss_slides%2FMurray_1.pdf&sa=D&sntz=1&usg
=AFQjCNF4jYktiiAX854cY-jTi3kjsr4aLg)

* [Radford Neal’s Review Paper](http://www.google.com/url?q=http%3A%2F%2Fwww.cs
.toronto.edu%2Fpub%2Fradford%2Freview.pdf&sa=D&sntz=1&usg
=AFQjCNGqsD0j2NEb_ViJ-WcEycABWm4aWw) (old but still very comprehensive)

* [Better Mixing via Deep Representations](https://www.google.com/url?q=https%3
A%2F%2Farxiv.org%2Fabs%2F1207.4404&sa=D&sntz=1&usg=AFQjCNGWdO8wFpZddMLUFKQ-
x1qlFdCHtQ)

Restricted Boltzmann Machines

* [Unsupervised learning of distributions of binary vectors using 2-layer netwo
rks](http://www.google.com/url?q=http%3A%2F%2Fcseweb.ucsd.edu%2F%7Eyfreund%2Fp
apers%2Ffreund94unsupervised.pdf&sa=D&sntz=1&usg=AFQjCNGXcHwiL0ixnMgbDfHSkhor9
07bWw)

* [A practical guide to training restricted Boltzmann machines](http://www.goog
le.com/url?q=http%3A%2F%2Fwww.cs.toronto.edu%2F%7Ehinton%2Fabsps%2FguideTR.pdf
&sa=D&sntz=1&usg=AFQjCNH97nRFGfiGqSribtUgM-xeQPoePg)

* [Training restricted Boltzmann machines using approximations to the
likelihood gradient](http://www.google.com/url?q=http%3A%2F%2Ficml2008.cs.hels
inki.fi%2Fpapers%2F638.pdf&sa=D&sntz=1&usg=AFQjCNHE154_wUpPjWrWP_He_y5mE7vBtw)

* [Tempered Markov Chain Monte Carlo for training of Restricted Boltzmann Machi
ne](http://www.google.com/url?q=http%3A%2F%2Fwww.iro.umontreal.ca%2F%7Elisa%2F
pointeurs%2Ftempered_tech_report2009.pdf&sa=D&sntz=1&usg=AFQjCNHvcbF7XtjC6C3hg
fdWJdMySGzsVw)

* [How to Center Binary Restricted Boltzmann Machines](http://www.google.com/ur
l?q=http%3A%2F%2Farxiv.org%2Fpdf%2F1311.1354v1.pdf&sa=D&sntz=1&usg=AFQjCNElT0e
PH9MmA9w3W_HfbWFqSoTLCQ)

* [Enhanced Gradient for Training Restricted Boltzmann Machines](http://www.goo
gle.com/url?q=http%3A%2F%2Fusers.ics.aalto.fi%2Fkcho%2Fpapers%2Fnc13rbm.pdf&sa
=D&sntz=1&usg=AFQjCNES7Qgoe4jimqad6c6gnu2RuVLUSA)

* [Using fast weights to improve persistent contrastive divergence](http://www.
google.com/url?q=http%3A%2F%2Fwww.cs.toronto.edu%2F%7Etijmen%2Ffpcd%2Ffpcd.pdf
&sa=D&sntz=1&usg=AFQjCNF4Rl46Lr0lmozHwXVELCMqZGWvHA)

* [Training Products of Experts by Minimizing Contrastive Divergence](http://ww
w.google.com/url?q=http%3A%2F%2Fwww.cs.toronto.edu%2F%7Ehinton%2Fabsps%2Ftr00-
004.pdf&sa=D&sntz=1&usg=AFQjCNGUnnDBYyEl0ysmCvuwXRdncRi09w)



Boltzmann Machines

* [Deep Boltzmann Machines](http://www.google.com/url?q=http%3A%2F%2Fwww.cs.tor
onto.edu%2F%7Ehinton%2Fabsps%2Fdbm.pdf&sa=D&sntz=1&usg=AFQjCNGS_dLRB4wsi2n-
reQ_wqyxD9at3A) (Salakhutdinov & Hinton)

* [Multimodal Learning with Deep Boltzmann Machines](http://www.google.com/url?
q=http%3A%2F%2Fwww.cs.toronto.edu%2F%7Ersalakhu%2Fpapers%2FMultimodal_DBM.pdf&
sa=D&sntz=1&usg=AFQjCNFhMoekRF_KoCOfRDaQCsildisguA)

* [Multi-Prediction Deep Boltzmann Machines](http://www.google.com/url?q=http%3
A%2F%2Fpapers.nips.cc%2Fpaper%2F5024-multi-prediction-deep-boltzmann-
machines&sa=D&sntz=1&usg=AFQjCNEFwY1zIQWSU_7GJTQOwW4p7UOtKQ)  

* [A Two-stage Pretraining Algorithm for Deep Boltzmann Machines](http://www.go
ogle.com/url?q=http%3A%2F%2Fusers.ics.aalto.fi%2Fkcho%2Fpapers%2Fnips12worksho
p.pdf&sa=D&sntz=1&usg=AFQjCNFwgEm_wW9986ESAC3E4X9j1HAFPA)



Regularized Auto-Encoders

* [The Manifold Tangent Classifier](http://www.google.com/url?q=http%3A%2F%2Fbo
oks.nips.cc%2Fpapers%2Ffiles%2Fnips24%2FNIPS2011_1240.pdf&sa=D&sntz=1&usg=AFQj
CNGYFVmZjuoyuq6S0CDyP5NbFURHHw)



Regularization



Stochastic Nets & GSNs

* [Estimating or Propagating Gradients Through Stochastic Neurons for
Conditional ](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F130
8.3432&sa=D&sntz=1&usg=AFQjCNGEbriBUtj39ssaedGEKHyJHqvbhw)

[Computation](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F130
8.3432&sa=D&sntz=1&usg=AFQjCNGEbriBUtj39ssaedGEKHyJHqvbhw)

* [Learning Stochastic Feedforward Neural Networks](http://www.google.com/url?q
=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5026-learning-stochastic-feedforward-
neural-networks&sa=D&sntz=1&usg=AFQjCNGdnQpsTzVR4nUZ6-BOyoCtZwgYPA)

* [Generalized Denoising Auto-Encoders as Generative Models](https://www.google
.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1305.6663&sa=D&sntz=1&usg=AFQjCNHGN
KlTFwZMxtG2pu91gjXjZG9mUg)

* [Deep Generative Stochastic Networks Trainable by Backprop](https://www.googl
e.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1306.1091&sa=D&sntz=1&usg=AFQjCNGl
HpKQu0vs11MR58HGIXOkyompjg)  



Others

* [Slow, Decorrelated Features for Pretraining Complex Cell-like Networks](http
://www.google.com/url?q=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fpa
per_files%2FNIPS2009_0933.pdf&sa=D&sntz=1&usg=AFQjCNESeA-yBAY0TqWLV--
ZbS7YIwHqww)

* [What Regularized Auto-Encoders Learn from the Data Generating Distribution](
https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1211.4246&sa=D&sn
tz=1&usg=AFQjCNGlAT9SG6UGC5nTUBRm8dFIMd9g7w)  

* [Generalized Denoising Auto-Encoders as Generative Models](https://www.google
.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1305.6663&sa=D&sntz=1&usg=AFQjCNHGN
KlTFwZMxtG2pu91gjXjZG9mUg)  

* [Why the logistic function?](http://www.google.com/url?q=http%3A%2F%2Fwww.cs.
berkeley.edu%2F%7Ejordan%2Fpapers%2Fuai.ps&sa=D&sntz=1&usg=AFQjCNFZoqsst1I7YBX
6LMvVZrGsdY_dCA)



Recurrent Nets

* [Learning long-term dependencies with gradient descent is
difficult](http://www.google.com/url?q=http%3A%2F%2Fwww-dsi.ing.unifi.it%2F%7E
paolo%2Fps%2Ftnn-94-gradient.pdf&sa=D&sntz=1&usg=AFQjCNFXAyuaGA8sFwB4haWjjFVZE
lHv1w)

* [Advances in Optimizing Recurrent Networks](https://www.google.com/url?q=http
s%3A%2F%2Farxiv.org%2Fabs%2F1212.0901&sa=D&sntz=1&usg=AFQjCNE_Llp1a-
OWOw9kq4H5WryHky5g2Q)  

* [Learning recurrent neural networks with Hessian-free optimization](http://ww
w.google.com/url?q=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fpaper_f
iles%2FICML2011Martens_532.pdf&sa=D&sntz=1&usg=AFQjCNEIXcrTzTQemsRVXe4ZR6f9Noz
5Yg)

* [On the importance of momentum and initialization in deep learning,](http://w
ww.google.com/url?q=http%3A%2F%2Fwww.cs.utoronto.ca%2F%7Eilya%2Fpubs%2F2013%2F
1051_2.pdf&sa=D&sntz=1&usg=AFQjCNFJIThg-2LYEF93vRWBYsKWURMAyg)

* [Long short-term memory](http://www.google.com/url?q=http%3A%2F%2Fwww.bioinf.
jku.at%2Fpublications%2Folder%2F2604.pdf&sa=D&sntz=1&usg=AFQjCNEWGuct2Sd9DrB95
Ry2kSACzKB8gg) (Hochreiter & Schmidhuber)

* [Generating Sequences With Recurrent Neural Networks](http://www.google.com/u
rl?q=http%3A%2F%2Farxiv.org%2Fabs%2F1308.0850&sa=D&sntz=1&usg=AFQjCNGI7tQ_7I6F
cmS4EuyYxoisbp7QDQ)

* [Long Short-Term Memory in Echo State Networks: Details of a Simulation
Study](http://www.google.com/url?q=http%3A%2F%2Fminds.jacobs-university.de%2Fs
ites%2Fdefault%2Ffiles%2Fuploads%2Fpapers%2F2478_Jaeger12.pdf&sa=D&sntz=1&usg
=AFQjCNFRAmu7I16yOy0g-gvTZ6wo4x5Zuw)

* [The "echo state" approach to analysing and training recurrent neural
networks](http://www.google.com/url?q=http%3A%2F%2Fminds.jacobs-university.de%
2Fsites%2Fdefault%2Ffiles%2Fuploads%2Fpapers%2FEchoStatesTechRep.pdf&sa=D&sntz
=1&usg=AFQjCNGOVn9_W1inc0qgiaiPV3iJxYgleg)

* [Backpropagation-Decorrelation: online recurrent learning with O(N)
complexity](http://www.google.com/url?q=http%3A%2F%2Fni.www.techfak.uni-bielef
eld.de%2Ffiles%2FSteil2004-BDO.pdf&sa=D&sntz=1&usg=AFQjCNFezDyPboa1TqpbGWB2z3E
ibBiKIQ)

* [New results on recurrent network training:Unifying the algorithms and
accelerating ](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2
&ved=0CDsQFjAB&url=http%3A%2F%2Fwww.researchgate.net%2Fpublication%2F2375393_N
ew_Results_on_Recurrent_Network_Training_Unifying_the_Algorithms_and_Accelerat
ing_Convergence%2Ffile%2F9fcfd50fed618a36b7.pdf&ei=hBHBUs_VNur4yQHiqYHACw&usg=
AFQjCNFEP3Y5-E5iygiKMfSO4EnltItX0A&sig2=7r8_uIznXhPKXVfv9yVTPg&bvm=bv.58187178
,d.aWc&cad=rja)

[convergence](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&
ved=0CDsQFjAB&url=http%3A%2F%2Fwww.researchgate.net%2Fpublication%2F2375393_Ne
w_Results_on_Recurrent_Network_Training_Unifying_the_Algorithms_and_Accelerati
ng_Convergence%2Ffile%2F9fcfd50fed618a36b7.pdf&ei=hBHBUs_VNur4yQHiqYHACw&usg=A
FQjCNFEP3Y5-E5iygiKMfSO4EnltItX0A&sig2=7r8_uIznXhPKXVfv9yVTPg&bvm=bv.58187178,
d.aWc&cad=rja)

* [Audio Chord Recognition with Recurrent Neural
Networks](http://www.google.com/url?q=http%3A%2F%2Fwww-etud.iro.umontreal.ca%2
F%7Eboulanni%2FISMIR2013.pdf&sa=D&sntz=1&usg=AFQjCNHw6019RMZ8y5FWm2_eXFL_DwWD8
Q)

* [Modeling Temporal Dependencies in High-Dimensional Sequences: Application to
](http://www.google.com/url?q=http%3A%2F%2Farxiv.org%2Fpdf%2F1206.6392&sa=D&sn
tz=1&usg=AFQjCNGdvxx2YOruNcCqD3p9mdx3qPzLow)

[Polyphonic Music Generation and Transcription](http://www.google.com/url?q=ht
tp%3A%2F%2Farxiv.org%2Fpdf%2F1206.6392&sa=D&sntz=1&usg=AFQjCNGdvxx2YOruNcCqD3p
9mdx3qPzLow)



Convolutional Nets

* [Generalization and Network Design Strategies](http://www.google.com/url?q=ht
tp%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Flecun-89.pdf&sa=D&sntz=1&usg
=AFQjCNEUuXXzz_UQhHmSgXL6K5lLGS9D5w) (LeCun)

* [ImageNet Classification with Deep Convolutional Neural Networks](http://www.
google.com/url?q=http%3A%2F%2Fbooks.nips.cc%2Fpapers%2Ffiles%2Fnips25%2FNIPS20
12_0534.pdf&sa=D&sntz=1&usg=AFQjCNH9nvVxyzkyN6vmhPQcMCG6x5wNHw), Alex
Krizhevsky, Ilya

Sutskever, Geoffrey E Hinton, NIPS 2012.

* [On Random Weights and Unsupervised Feature Learning](http://www.google.com/u
rl?q=http%3A%2F%2Fwww.stanford.edu%2F%7Easaxe%2Fpapers%2FSaxe%2520et%2520al.%2
520-%25202010%2520-%2520On%2520Random%2520Weights%2520and%2520Unsupervised%252
0Feature%2520Learning.pdf&sa=D&sntz=1&usg=AFQjCNEtOAl46tckcQ_k-IB2ukUvCuQhmg)

Optimization issues with DL

* [Curriculum Learning](http://www.google.com/url?q=http%3A%2F%2Fwww.machinelea
rning.org%2Farchive%2Ficml2009%2Fpapers%2F119.pdf&sa=D&sntz=1&usg=AFQjCNGTbath
LDZBgNLVWNiRp0pEBz_faQ)

* [Evolving Culture vs Local Minima](http://www.google.com/url?q=http%3A%2F%2Fa
rxiv.org%2Fpdf%2F1203.2990v2.pdf&sa=D&sntz=1&usg=AFQjCNGMqJbygckiEzYtZgwBdptYr
0bcRA)

* [Knowledge Matters: Importance of Prior Information for Optimization](http://
www.google.com/url?q=http%3A%2F%2Farxiv.org%2Fpdf%2F1301.4083v6.pdf&sa=D&sntz=
1&usg=AFQjCNEPJe90tR1xdzuos9C_M7_9if8mbQ)

* [Efficient Backprop](http://www.google.com/url?q=http%3A%2F%2Fyann.lecun.com%
2Fexdb%2Fpublis%2Fpdf%2Flecun-
98b.pdf&sa=D&sntz=1&usg=AFQjCNGYjsmSP1mhqQD3yj2B0QVRCsU72g)

* [Practical recommendations for gradient-based training of deep architectures]
(https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F1206.5533&sa=D&s
ntz=1&usg=AFQjCNEGxQlx_u7DzBKCfdM_a1ltaS5jcw)  

* [Natural Gradient Works Efficiently (Amari 1998)](http://www.google.com/url?q
=http%3A%2F%2Fwww.maths.tcd.ie%2F%7Emnl%2Fstore%2FAmari1998a.pdf&sa=D&sntz=1&u
sg=AFQjCNEHUKlptdFaAQBSXp8KTtRNsTKmsw)

* Hessian Free

* Natural Gradient (TONGA)

* [Revisiting Natural Gradient](http://www.google.com/url?q=http%3A%2F%2Farxiv.
org%2Fabs%2F1301.3584&sa=D&sntz=1&usg=AFQjCNHWJ5pFJxSH4hV4QKYQBYcB6Vr4HQ)



NLP + DL

* [Natural Language Processing (Almost) from Scratch](http://static.googleuserc
ontent.com/media/research.google.com/en/us/pubs/archive/35671.pdf)

* [DeViSE: A Deep Visual-Semantic Embedding Model](http://www.google.com/url?q=
http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5204-devise-a-deep-visual-semantic-
embedding-model&sa=D&sntz=1&usg=AFQjCNF8J3ilRiRusGjjDFJJrrulaiIckA)

* [Distributed Representations of Words and Phrases and their Compositionality]
(http://www.google.com/url?q=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5021
-distributed-representations-of-words-and-phrases-and-their-
compositionality&sa=D&sntz=1&usg=AFQjCNEFH7FLS1KGqCyb5HQwCyDLHp6umQ)

* [Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detectio
n](http://www.google.com/url?q=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpape
rs%2Fpaper_files%2FNIPS2011_0538.pdf&sa=D&sntz=1&usg=AFQjCNHoLkEmd1P7UqMuJtsVX
9TsFN4X8A)



CV+RBM

* [Fields of
Experts](http://www.google.com/url?q=http%3A%2F%2Fwww.gris.informatik.tu-
darmstadt.de%2F%7Esroth%2Fpubs%2Ffoe-
ijcv.pdf&sa=D&sntz=1&usg=AFQjCNEHHkUqZLra5VvdAq8gzFJ3StP77w)

* [What makes a good model of natural images?](http://www.google.com/url?q=http
%3A%2F%2Fpeople.csail.mit.edu%2Fbillf%2Fpapers%2Ffoe-final.pdf&sa=D&sntz=1&usg
=AFQjCNEwFfjhPH9TC0IFpjOTeS-onxDWDw)

* [Phone Recognition with the mean-covariance restricted Boltzmann machine](htt
p://www.google.com/url?q=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fp
aper_files%2FNIPS2010_0160.pdf&sa=D&sntz=1&usg=AFQjCNGAfsZ9ejuUa-
nG9M8YHo6RCkbu7g)

* [Unsupervised Models of Images by Spike-and-Slab RBMs](http://www.google.com/
url?q=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fpaper_files%2FICML20
11Courville_591.pdf&sa=D&sntz=1&usg=AFQjCNFZZgUmafYSJQU42GgXPz9Wpu6U_Q)



CV + DL

* [Imagenet classiﬁcation with deep convolutional neural networks](http://www.g
oogle.com/url?q=http%3A%2F%2Fwww.cs.toronto.edu%2F%7Ehinton%2Fabsps%2Fimagenet
.pdf&sa=D&sntz=1&usg=AFQjCNHo7MobHo05TfCFjRDpX41KiBLcTw)

* [Learning to relate images](http://www.google.com/url?q=http%3A%2F%2Fwww.iro.
umontreal.ca%2F%7Ememisevr%2Fpubs%2Fpami_relational.pdf&sa=D&sntz=1&usg=AFQjCN
GrOvQdzgY6-PRVlVXkoOrMCByemw)



Scaling Up

* [Large Scale Distributed Deep Networks](http://www.google.com/url?q=http%3A%2
F%2Fwww.cs.toronto.edu%2F%7Eranzato%2Fpublications%2FDistBeliefNIPS2012_withAp
pendix.pdf&sa=D&sntz=1&usg=AFQjCNGyT5x-DCxOELUukj96EEVciTRb2g)

* [Random search for hyper-parameter optimization](http://www.google.com/url?q=
http%3A%2F%2Fjmlr.org%2Fpapers%2Fvolume13%2Fbergstra12a%2Fbergstra12a.pdf&sa=D
&sntz=1&usg=AFQjCNETdtQFK6uNOgo8yHPWr5z8OpCd3w)

* [Practical Bayesian Optimization of Machine Learning Algorithms](http://www.g
oogle.com/url?q=http%3A%2F%2Fwww.cs.toronto.edu%2F%7Ejasper%2Fbayesopt.pdf&sa=
D&sntz=1&usg=AFQjCNEvryDt-vjfCA3PuOjxxgZ2o4IX1g)



DL + Reinforcement learning

* [Playing Atari with Deep Reinforcement Learning (paper not officially
released yet!)](http://www.google.com/url?q=http%3A%2F%2Farxiv.org%2Fabs%2F131
2.5602&sa=D&sntz=1&usg=AFQjCNELlC7KOOHJbnqPksw7-bmuswtvmw)





Graphical Models Background

* [An Introduction to Graphical Models](http://www.google.com/url?q=http%3A%2F%
2Fwww.cis.upenn.edu%2F%7Emkearns%2Fpapers%2Fbarbados%2Fjordan-
tut.pdf&sa=D&sntz=1&usg=AFQjCNFBnN9aMbaPREIkuhGcIM8SlkJvMg) (Mike Jordan,
brief course notes)

* [A View of the EM Algorithm that Justifies Incremental, Sparse and Other Vari
ants](http://www.google.com/url?q=http%3A%2F%2Fwww.cs.toronto.edu%2F%7Eradford
%2Fftp%2Femk.pdf&sa=D&sntz=1&usg=AFQjCNEKmSxgmLLtpIkflxPW-T6lEyouQw) (Neal

& Hinton, important paper to the modern understanding of Expectation-
Maximization)

* [A Unifying Review of Linear Gaussian Models](http://www.google.com/url?q=htt
p%3A%2F%2Fauthors.library.caltech.edu%2F13697%2F1%2FROWnc99.pdf&sa=D&sntz=1&us
g=AFQjCNHZpRrYLjZi6J9FdUK6cMspg17XsQ) (Roweis & Ghahramani, ties together

PCA, factor analysis, hidden Markov models, Gaussian mixtures, k-means, linear

dynamical systems)

* [An Introduction to Variational Methods for Graphical Models](http://www.goog
le.com/url?q=http%3A%2F%2Fwww.cs.berkeley.edu%2F%7Ejordan%2Fpapers
%2Fvariational-intro.pdf&sa=D&sntz=1&usg=AFQjCNGjILJA5cMoWEIC7Z6wyxLybPzgEw)
(Jordan et al, mean-field,

etc.)



Writing

* [Writing a great research
paper](https://www.google.com/url?q=https%3A%2F%2Fresearch.microsoft.com%2Fen-
us%2Fum%2Fpeople%2Fsimonpj%2Fpapers%2Fgiving-a-talk%2Fwriting-a-paper-
slides.pdf&sa=D&sntz=1&usg=AFQjCNGZSq02IsBbhgNMpiC8vpEon2oDYg) [(video of the
presentation)](https://www.youtube.com/watch?v=g3dkRsTqdDA)



Software documentation

* [Python](http://www.google.com/url?q=http%3A%2F%2Fwww.deeplearning.net%2Fsoft
ware%2Ftheano%2Ftutorial%2Fpython.html&sa=D&sntz=1&usg=AFQjCNH4HuHFCv_4SPrUCzw
MERnsFX_ccw), [Theano](http://www.google.com/url?q=http%3A%2F%2Fwww.deeplearni
ng.net%2Fsoftware%2Ftheano%2Ftutorial%2F&sa=D&sntz=1&usg=AFQjCNFd1UCJhoJsSUa4k
JsDPAJsqohDaQ), [Pylearn2](http://www.google.com/url?q=http%3A%2F%2Fwww.deeple
arning.net%2Fsoftware%2Fpylearn2%2F%23documentation&sa=D&sntz=1&usg=AFQjCNEahr
6zvfoQPF9wqwfkg7hi2kzMxA), [Linux
(bash)](http://www.google.com/url?q=http%3A%2F%2Ftldp.org%2FHOWTO%2FBash-Prog-
Intro-HOWTO.html&sa=D&sntz=1&usg=AFQjCNHc9iF0ne1veVNyrB37cshw7u1W9w) (at least
the 5 first sections), [git](http://www.google.com/url?q=http%3A%2F%2Fgit-
scm.com%2Fbook&sa=D&sntz=1&usg=AFQjCNEd1qJ7IuaxNpaEWwlBNuXq4PQrcQ) (5 first

sections), [github/contributing to it](http://www.google.com/url?q=http%3A%2F%
2Fdeeplearning.net%2Fsoftware%2Ftheano%2Fdev_start_guide.html%23dev-start-
guide&sa=D&sntz=1&usg=AFQjCNG40bEZQbqWPOJoso-NkEos9eNhPg) (Theano doc), [vim t
utorial](http://www.google.com/url?q=http%3A%2F%2Fblog.interlinked.org%2Ftutor
ials%2Fvim_tutorial.html&sa=D&sntz=1&usg=AFQjCNF_cfa51xEOfQeOXptYAy2e8xJCfg)
or [emacs tutorial](http://www.google.com/url?q=http%3A%2F%2Fwww2.lib.uchicago
.edu%2Fkeith%2Ftcl-course%2Femacs-
tutorial.html&sa=D&sntz=1&usg=AFQjCNEzqrupmxngbxsO6FKnhT4amB0-Fg)



Software lists of built-in commands/functions

* [Bash commands](http://www.google.com/url?q=http%3A%2F%2Fss64.com%2Fbash%2F&s
a=D&sntz=1&usg=AFQjCNGJTR-Yy6WLiznBQVmJr1X-v_FMKA)

* [List of Built-in Python Functions](http://www.google.com/url?q=http%3A%2F%2F
docs.python.org%2F2%2Flibrary%2Ffunctions.html&sa=D&sntz=1&usg=AFQjCNHuRAl6y3Q
qOwuKSEX7e5i4mdLF0w)

* [vim commands](http://www.google.com/url?q=http%3A%2F%2Ftnerual.eriogerg.free
.fr%2Fvimqrc.html&sa=D&sntz=1&usg=AFQjCNEMAJp7EKSgHypzOjxCCSQLeYkABA)



Other Software stuff to know about:

* screen

* ssh

* ipython

* matplotlib

* 







