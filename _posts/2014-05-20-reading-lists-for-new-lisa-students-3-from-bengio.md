---
layout: post
title: reading lists for new lisa students 3 from bengio
categories: [paper]
tags: [bengio]
---


info coming from [bengio](https://docs.google.com/document/d/1IXF3h0RU5zz4ukmTrVKVotPQypChscNGf5k6E25HGvA/edit#heading=h.iyu3xc5qgwza)

### Recurrent Nets

[1] [Learning long-term dependencies with gradient descent is difficult](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)

[2] [Advances in Optimizing Recurrent Networks](http://arxiv.org/pdf/1212.0901v2.pdf)  

[3] [Learning recurrent neural networks with Hessian-free optimization](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf)

[4] [On the importance of momentum and initialization in deep learning,](http://www.cs.utoronto.ca/~ilya/pubs/2013/1051_2.pdf)

[5] [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (Hochreiter & Schmidhuber)

[6] [Generating Sequences With Recurrent Neural Networks](http://arxiv.org/pdf/1308.0850v4.pdf)

[7] [Long Short-Term Memory in Echo State Networks: Details of a Simulation Study](http://minds.jacobs-university.de/sites/default/files/uploads/papers/2478_Jaeger12.pdf)

[8] [The "echo state" approach to analysing and training recurrent neural networks](http://minds.jacobs-university.de/sites/default/files/uploads/papers/EchoStatesTechRep.pdf)

[9] [Backpropagation-Decorrelation: online recurrent learning with O(N) complexity](http://ni.www.techfak.uni-bielefeld.de/files/Steil2004-BDO.pdf)

[10] [New results on recurrent network training:Unifying the algorithms and accelerating convergence](http://www.researchgate.net/profile/Amir_Atiya/publication/5602147_New_results_on_recurrent_network_training_unifying_the_algorithms_and_accelerating_convergence/file/9fcfd50fed618a36b7.pdf)

[11] [Audio Chord Recognition with Recurrent Neural Networks](http://www-etud.iro.umontreal.ca/~boulanni/ISMIR2013.pdf)

[12] [Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription](http://arxiv.org/pdf/1206.6392.pdf)

### Convolutional Nets

[1] [Generalization and Network Design Strategies](http://yann.lecun.com/exdb/publis/pdf/lecun-89.pdf) (LeCun)

[2] [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS 2012.

[3] [On Random Weights and Unsupervised Feature Learning](http://www.stanford.edu/~asaxe/papers/Saxe%20et%20al.%20-%202010%20-%20On%20Random%20Weights%20and%20Unsupervised%20Feature%20Learning.pdf)

### Optimization issues with DL

[1] [Curriculum Learning](http://www.machinelearning.org/archive/icml2009/papers/119.pdf)

[2] [Evolving Culture vs Local Minima](http://arxiv.org/pdf/1203.2990v2.pdf)

[3] [Knowledge Matters: Importance of Prior Information for Optimization](http://arxiv.org/pdf/1301.4083v6.pdf)

[4] [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

[5] [Practical recommendations for gradient-based training of deep architectures](http://arxiv.org/pdf/1206.5533v2.pdf)  

[6] [Natural Gradient Works Efficiently (Amari 1998)](http://www.maths.tcd.ie/~mnl/store/Amari1998a.pdf)

[7] Hessian Free

[8] Natural Gradient (TONGA)

[9] [Revisiting Natural Gradient](http://arxiv.org/pdf/1301.3584v7.pdf)



