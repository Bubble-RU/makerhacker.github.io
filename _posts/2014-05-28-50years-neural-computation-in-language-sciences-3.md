---
layout: post
title: 50 years neural computation in language sciences 3
categories: [paper]
tags: [neural-computation, language-science]
---


50 years after the perceptron, 25 years after PDP: Neural computation in language sciences

**[1] Self-organizing map models of language acquisition**

Source: fpsyg-2013 [pdf](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2013.00828/pdf)

Author: Ping Li and Xiaowei Zhao

Abstract:

Connectionist models have had a profound impact on theories of language. While most early models were inspired by the classic parallel distributed processing architecture, recent models of language have explored various other types of models, including self-organizing models for language acquisition. In this paper, we aim at providing a review of the latter type of models, and highlight a number of simulation experiments that we have conducted based on these models. We show that self-organizing connectionist models can provide significant insights into long-standing debates in both monolingual and bilingual language development. We suggest future directions in which these models can be extended, to better connect with behavioral and neural data, and to make clear predictions in testing relevant psycholinguistic theories.

**[2] Beyond modeling abstractions: learning nouns over developmental time in atypical populations and individuals**

Source: fpsyg-2013 [pdf](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2013.00871/pdf)

Author: Clare E. Sims, Savannah M. Schilling and Eliana Colunga

Abstract:

Connectionist models that capture developmental change over time have much to offer in the field of language development research. Several models in the literature have made good contact with developmental data, effectively captured behavioral tasks, and accurately represented linguistic input available to young children. However, fewer models of language development have truly captured the process of developmental change over time. In this review paper, we discuss several prominent connectionist models of early word learning, focusing on semantic development, as well as our recent work modeling the emergence of word learning biases in different populations. We also discuss the potential of these kinds of models to capture children’s language development at the individual level. We argue that a modeling approach that truly captures change over time has the potential to inform theory, guide research, and lead to innovations in early language intervention.

**[3] Connectionism coming of age: legacy and future challenges**

Source: fpsyg-2014 [pdf](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00187/pdf)

Author: Julien Mayor, Pablo Gomez, Franklin Chang and Gary Lupyan

Abstract:

About 50 Years after the Introduction of the Perceptron and Some 25 Years after the Introduction of PDP Models, Where are we Now?

In 1986, Rumelhart and McClelland took the cognitive science community by storm with the Parallel Distributed Processing (PDP) framework. Rather than abstracting from the biological substrate as was sought by the “information processing” paradigms of the 1970s, connectionism, as it has come to be called, embraced it. An immediate appeal of the connectionist agenda was its aim: to construct at the algorithmic level models of cognition that were compatible with their implementation in the biological substrate.

The PDP group argued that this could be achieved by turning to networks of artificial neurons, originally introduced by McCulloch and Pitts (1943) which the group showed were able to provide insights into a wide range of psychological domains, from categorization, to perception, to memory, to language. This work built on an earlier formulation by Rosenblatt (1958) who introduced a simple type of feed-forward neural network called the perceptron. Perceptrons were limited to solving simple linearly-separable problems and although networks composed of perceptrons were known to be able to compute any Boolean function (including XOR, Minsky and Papert, 1969), there was no effective way of training such networks. In 1986, Rumelhart, Hinton and Williams introduced the back-propagation algorithm, providing an effective way of training multi-layered neural networks, which could easily learn non linearly-separable functions. In addition to providing the field with an effective learning algorithm, the PDP group published a series of demonstrations of how long standing questions in cognitive psychology could be elegantly solved using simple learning rules, distributed representations, and interactive processing.


