<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-149" href="../cvpr2013/cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">cvpr2013-149</a> <a title="cvpr-2013-149-reference" href="#">cvpr2013-149-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-149-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Everts_Evaluation_of_Color_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><br/>
<h2>reference text</h2><p>[1] M. Brown, G. Hua, and S. Winder. Discriminative learning of local image descriptors. PAMI, 2010. 5</p>
<p>[2] G. J. Burghouts and J. M. Geusebroek. Performance evaluation of local colour invariants. CVIU, 2009. 1, 2, 4, 7</p>
<p>[3] M. Chen and A. Hauptmann. Mosift: Recognizing human actions in surveillance videos, 2009. 2</p>
<p>[4] P. Doll a`r, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In ICCVVSPETS, 2005. 2, 4</p>
<p>[5] I. Everts, J. C. van Gemert, and T. Gevers. Per-patch descriptor selection using surface and scene attributes. In ECCV, 2012. 2, 4, 7</p>
<p>[6] A. Kl¨ aser, M. Marszalek, and C. Schmid. A spatio-temporal descriptor based on 3d-gradients. In BMVC, 2008. 1, 2, 4, 5</p>
<p>[7] I. Laptev. On space-time interest points. IJCV, 2005. 2, 3, 5</p>
<p>[8] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In CVPR, 2008. 1, 2, 5</p>
<p>[9] J. Liu, J. Luo, and M. Shah. Recognizing realistic actions from videos ”in the wild”. In CVPR, 2009. 1, 5</p>
<p>[10] K. K. Reddy and M. Shah. Recognizing 50 human action categories of web videos. MVAP, 2012. 2, 5, 7, 8</p>
<p>[11] M. D. Rodriguez, J. Ahmed, and M. Shah. Action mach: A spatio-temporal maximum average correlation height filter for action recognition. In CVPR, 2008. 5</p>
<p>[12] S. Sadanand and J. J. Corso. Action bank: A high-level representation of activity in video. In CVPR, 2012. 1, 2, 7, 8</p>
<p>[13] S. Shafer. Using color to separate reflection components. Color research and applications 10, 1985. 2</p>
<p>[14] B. Solmaz, S. M. Assari, and M. Shah. Classifying web videos using a global video descriptor. MVAP, 2012. 2, 7, 8</p>
<p>[15] F. Souza, E. Valle, G. C´ amara-Ch a´vez, and A. d. A. Ara u´jo. An evaluation on color invariant based local spatiotemporal features for action recognition. In IEEE SIBGRAPI, 2012. 2</p>
<p>[16] J. St ¨ottinger, A. Hanbury, N. Sebe, and T. Gevers. Sparse color interest points for image retrieval and object categorization. TIP, 2012. 2</p>
<p>[17] J. St ¨ottinger, S. Zambanini, R. Khan, and A. Hanbury. Feeval - a dataset for evaluation of spatio-temporal local features. In ICPR, 2010. 5</p>
<p>[18] A. Tamrakar, S. Ali, Q. Yu, J. Liu, O. Javed, A. Divakaran, H. Cheng, and H. Sawhney. Evaluation of low-level features and their combinations for complex event detection in open source videos. In CVPR, 2012. 1, 2</p>
<p>[19] K. E. A. van de Sande, T. Gevers, and C. G. M. Snoek. Evaluating color descriptors for object and scene recognition. PAMI, 2010. 1, 2, 4, 7</p>
<p>[20] J. van de Weijer, T. Gevers, and J. M. Geusebroek. Edge and corner detection by photometric quasi-invariants. PAMI, 2005. 2</p>
<p>[21] J. van de Weijer, T. Gevers, and A. W. M. Smeulders. Robust photometric invariant features from the colour tensor. TIP, 2006. 2, 3, 4</p>
<p>[22] H. Wang, A. Kl¨ aser, C. Schmid, and C. Liu. Action recognition by dense trajectories. In CVPR, 2011. 2, 7</p>
<p>[23] H. Wang, M. M. Ulla, A. Klaser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features for action recognition. In BMVC, 2009. 1, 2, 5, 6, 7  222888555755</p>
<br/>
<br/><br/><br/></body>
</html>
