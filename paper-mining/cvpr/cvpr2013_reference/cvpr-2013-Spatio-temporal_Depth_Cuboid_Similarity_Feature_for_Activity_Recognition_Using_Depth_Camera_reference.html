<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-407" href="../cvpr2013/cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">cvpr2013-407</a> <a title="cvpr-2013-407-reference" href="#">cvpr2013-407-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</h1>
<br/><p>Source: <a title="cvpr-2013-407-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xia_Spatio-temporal_Depth_Cuboid_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><br/>
<h2>reference text</h2><p>[1] J. K. Aggarwal and M. S. Ryoo. Human activity analysis: A review. ACM Computing Surveys (CSUR), 2011.</p>
<p>[2] S. Belongie, K. Branson, P. Dollar, and V. Rabaud. Monitoring ani-</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]  mal behavior in the smart vivarium. Measuring Behavior, 2005. C. Chang and C. Lin. Libsvm: a library for support vector machines. TIST, 2(3):27, 2011. Z. Cheng, L. Qin, Y. Ye, Q. Huang, and Q. Tian. Human daily action analysis with multi-view and color-depth data. In Computer Vision– ECCV. Workshops and Demonstrations, pages 52–61, 2012. P. Doll a´r, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In PETS, pages 65–72, 2005. J. Goles. Inside the race to hack the kinect. New Scientist, 208(2789):22, 2010. A. Hern a´ndez-Vela, M. Bautista, X. Perez-Sala, V. Ponce, X. Bar o´, O. Pujol, C. Angulo, and S. Escalera. Bovdw: Bag-of-visual-anddepth-words for gesture recognition. In ICPR, 2012. S. Ikemura and H. Fujiyoshi. Real-time human detection using relational depth similarity features. In ACCV 2010, pages 25–38. A. Kanezaki, H. Nakayama, T. Harada, and Y. Kuniyoshi. Highspeed 3d object recognition using additive features in a linear subspace. In ICRA, pages 3128–3134, 2010. A. Klaser and M. Marszalek. A spatio-temporal descriptor based on 3d-gradients. In BMVC, 2008. I. Laptev. On space-time interest points. International Journal of Computer Vision, 64(2): 107–123, 2005. I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In CVPR, pages 1–8, 2008. W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. CVPR Workshop, 1(5):9–14, 2010. B. Ni, G. Wang, and P. Moulin. Rgbd-hudaact: A color-depth video database for human daily activity recognition. In ICCV Workshop, pages 1147–1 153, 2011. J. Niebles, H. Wang, and L. Fei-Fei. Unsupervised learning of human action categories using spatial-temporal words. IJCV, 79(3):299– 318, 2008. M. Pauly, R. Keiser, and M. Gross. Multi-scale feature extraction on point-sampled surfaces. In Computer graphics forum, volume 22, pages 281–289, 2003.</p>
<p>[17] F. Porikli. Integral histogram: A fast way to extract histograms in cartesian spaces. In CVPR, volume 1, pages 829–836, 2005.</p>
<p>[18] R. Rusu, N. Blodow, and M. Beetz. Fast point feature histograms (fpfh) for 3d registration. In ICRA, pages 3212–3217, 2009.</p>
<p>[19] R. Rusu, G. Bradski, R. Thibaux, and J. Hsu. Fast 3d recognition and pose using the viewpoint feature histogram. In IROS, pages 2155– 2162, 2010.</p>
<p>[20] E. Shechtman and M. Irani. Matching local self-similarities across images and videos. In Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on, pages 1–8, 2007.</p>
<p>[21] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from a single depth image. CVPR, 2011.</p>
<p>[22] B. Steder, R. Rusu, K. Konolige, and W. Burgard. Narf: 3d range image features for object recognition. In IROS Workshop, volume 44, 2010.</p>
<p>[23] J. Stuckler and S. Behnke. Interest point detection in depth images through scale-space surface analysis. In ICRA, pages 3568–3574, 2011.</p>
<p>[24] B. C. Vemuri, A. Mitiche, and J. K. Aggarwal. Curvature-based representation of objects from range data. Image and vision computing, 4(2): 107–1 14, 1986.</p>
<p>[25] A. Vieira, E. Nascimento, G. Oliveira, Z. Liu, and M. Campos. Stop: Space-time occupancy patterns for 3d action recognition from depth map sequences. Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, pages 252–259, 2012.</p>
<p>[26] J. Wang, Z. Liu, J. Chorowski, Z. Chen, and Y. Wu. Robust 3d action recognition with random occupancy patterns. In ECCV, pages 872– 885. 2012.</p>
<p>[27] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras. In CVPR, pages 1290–1297, 2012.</p>
<p>[28] G. Willems, T. Tuytelaars, and L. Van Gool. An efficient dense and scale-invariant spatio-temporal interest point detector. In ECCV, pages 650–663, 2008.</p>
<p>[29] L. Xia, C. Chen, and J. K. Aggarwal. View invariant human action  recognition using histograms of 3d joints. In CVPR Workshop, pages 20–27, 2012.</p>
<p>[30] X. Yang and Y. Tian. Eigenjoints-based action recognition using na¨ ıve-bayes-nearest-neighbor. In CVPRW, pages 14–19, 2012.</p>
<p>[31] H. Zhang and L. Parker. 4-dimensional local spatio-temporal features for human activity recognition. In IROS, pages 2044–2049, 2011.</p>
<p>[32] Y. Zhao, Z. Liu, L. Yang, and H. Cheng. Combing rgb and depth map features for human activity recognition. In APSIPA ASC, pages 1–4, 2012. 222888334199</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
