<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-281" href="../cvpr2013/cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">cvpr2013-281</a> <a title="cvpr-2013-281-reference" href="#">cvpr2013-281-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</h1>
<br/><p>Source: <a title="cvpr-2013-281-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Pont-Tuset_Measures_and_Meta-Measures_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jordi Pont-Tuset, Ferran Marques</p><p>Abstract: This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p><br/>
<h2>reference text</h2><p>[13, 4]</p>
<p>[7]  Notation  DH dvD  Pixel-set clustering  SBeigpamreti neta gtiroanph co mvaetrcihnigng  [14[2,] 4]  BGCM</p>
<p>[19]  BCE</p>
<p>[22]  VoI  PPrroecbiasbioilnis-tRicec Ralaln fdor in rdegexions  [26[1,9 3]0]  PrP,R RIr  Precision-Recall for boundaries  [17, 19]  Pb, Rb  Bidirectional consistency  error  Variation of information  Pairs-of-pixels classification Boundary map  Table 1. Measure structure overview for the three interpretations of an image partition  scene interpretation. As the author points out, these measures are not suitable for general-purpose image segmentation evaluation. The same work proposes a measure that is not transparent to oversegmentation: the bidirectional consistency error (BCE), which can be rewritten as:  BCE(S,S?)=1−n1RR??∈∈SS?|R∩R?|min?|R|R ∩| R?|,|R|R ∩? R|?|? (4) The work in [22] introduced a new point of view to the measures of clustering assessment based on informationtheoretic results. The author defines a discrete random variable taking N values that consists in randomly picking any pixel in the partition S = {R1, . . . , RN} and observing the region nit t belongs tioo.n Assuming all the pixels equally probable to pick, the entropy H(S) associated with a partition is defined as the entropy of such random variable. The mutual information I(S,S?) between two partitions is defined equivalently. The variation of information is then: VoI(S, S?) = H(S) + H(S?) − 2I(S, S?)  (5)  It can be normalized by log N, its maximum possible value.  ×  2.2. Pairs-of-Pixels Classification An image partition can be viewed as a classification of all the pairs of pixels into two classes: pairs of pixels belonging to the same region, and pairs of pixels from different regions. Formally, let I {p1, . . . ,pn} be the set of pix= erelsg ioofn tsh.e F image ya,n dle tco Ins =ide {rp the set of} a blle pairs eotf o pixels P = { (pi, pj) ∈ I I|i< j}. Given two partitions S and SP?, = we (dpivide )P ∈ ∈in Ito × ×fo Iu|ri d  < γo and classify the regions in both partitions as described in Algorithm 1, where “←” means that a region is sclcarisbsiefdied in only oifr i tth previously edi “d← ←no”t mhaevaen a more f raevgoiroanbl ise classification. Let oc and oc? be the number of object candidates in S and G, respectively (note that they can differ, given that G Algorithm  1  Region  candidates  classification  Algorithm 1Region candidates classification  1:for all Ri∈ S, Rj?∈ G do 2: if OSij and OiGj then >γo  3:  >γo  Ri , Rj? ← Object candidates else  if OSij >γp  OiGj  if OSij  4: 5: 6:  OiGj  and >γo then Ri ← Fragmentation candidate Rj? ←←FParargtm caenndtiadtaioten  7: else >γo and >γp then 8: Ri ← Part candidate 9: Rj? ←← Fragmentation candidate 10: else 11: Ri, Rj? ← Noise 12: end if 13: end for  can be formed by more than one partition and thus a region in S can be matched as object with more than one region in G), and pc and pc? the number of part candidates. Regarding the fragmentation candidates, we compute the percentage of the object that could be formed from the matched parts. Formally, we define the amount of fragmentation fr(Ri) of a region Ri ∈ S as the addition of the relative overlaps of the part cand∈ida Ste ass sm thatech aeddd ttioo Rni o:  fr(Ri) =?j?OGijs.t. OSij> γo?  (8)  fr?(R?j)  and is defined equivalently for G. The global fragmentation fr and fr? is computed adding the amount offragmentation among all the fragmentation candidates of S and G, respectively. Figure 2 shows a toy example to illustrate the proposed classification and measures. We then defined the precision-recall for objects and parts as follows:  Pop=oc + f|rS +| β pc  Rop=oc?+ fr|G?+| β pc?  PartitionGround Truth  (9)  Figure 2. Classification of the regions into object and part candidates. The rectangles are classified as object candidates, despite not fully overlapping. The partition circle is a fragmentation candidate with a fragmentation of 1 (parts cover it totally), and the ground-truth half-circles are parts candidates. The opposite holds for the triangles, but in this case the fragmentation is 0.9. Both pentagons are classified as noise. 222111333422  Intuitively, in a completely oversegmented result, the recall would be high but the precision very low. Conversely, a completely undersegmented result (one single region) would entail a high precision but very low recall. As a summary measure, we propose to use the F measure (Fop) between Pop and Rop. 4. Meta-Measures This section is about how to compare the goodness of the segmentation evaluation measures. The objective of this section is therefore not to tell which segmentation algorithm to use, but which evaluation measures better summarize the quality of these algorithms. To distinguish these two analyses, we will refer to the quantitative metrics to compare segmentation measures as meta-measures. A meta-measure analysis must rely on accepted hypotheses about the segmentation results and assess how coherent the measures are with such hypotheses. As examples, an accepted hypothesis can be the human judgment of quality of some particular examples. The meta-measure is then defined as a quantization of how coherent the evaluation measures are with this judgment [30, 4]. To provide statistically significant results, however, one must go beyond a handful of examples and provide a quantitative analysis on an annotated database. The remainder of this section explains one meta-measure already published in the literature (Sec. 4.1) and presents two new meta-  measures (Sec. 4.2 and 4.3). 4.1. Swapped-Image Human Discrimination Given an image, there is no unique valid segmentation, since it depends on the perception of the scene, the level of details, etc. In order to cope with this variability, the Berkeley segmentation dataset (BSDS300 [21] and BSDS500 [2]) consists of a set of images each of them manually segmented by more than one individual. The hypothesis behind the first meta-measure is that an evaluation measure should be able to tell apart the groundtruth partitions coming from two different images. In other words, given a pair of ground-truth partitions from BSDS500, a measure should be able to tell whether they come from the same image (thus differences are an acceptable refinement) or different images (unacceptable discrepancies). As first proposed by [19] to evaluate the coherence of BSDS300, given an evaluation measure m, we compute the Probability Density Function (PDF) of the values of m for all the pairs of partitions in BSDS500, grouped in two classes: those coming from different images and those from the same one. Figure 3 shows the PDFs for these two types of pairs of partitions using the Fb measure. A simple classifier was then defined setting a threshold on the measure to discriminate the two types of pairs. The  () and different-image pairs (). In gray rectangles, four representative pairs of partitions: a pair of correctly classified as different image (up-left) and as same image (up-right); and a pair incorrectly classified as different image (down-left) and as same image (down-right). Swapped-Image Human Discrimination (SIHD) metameasure is defined as the percentage of correct classifi-  cations of that classifier, that is, the sum of the area under the curve above and below the threshold for the sameimage and different-image pairs, respectively. (In the original work, the authors reported the Bayes Risk.) As qualitative examples, Figure 3 depicts four pairs of partitions as representatives of the type of mistakes and correct classifications using Fb. 4.2. SoA-Baseline Discrimination One of the reasons why SIHD can be criticized is the fact that it is based only on human-made partitions, that is, it does not show how measures handle the real-world discrepancies found between SoA segmentation methods. This subsection and the following are devoted to present two meta-mesures based on SoA segmentation results. The hypothesis on which we base the meta-measure presented in this section is that evaluation measures should be able to distinguish between (i) partitions obtained by any SoA segmentation method on a given image and (ii) partitions obtained regardless of the image, that is, partitions that are created without taking into account the content of the image. These partitions are interpreted as a baseline, that is, the results that could be obtained by chance. As in [2], we use a quadtree as baseline. In particular, we build the hierarchical partitions starting from the whole 222111333533  image and iteratively dividing the regions into four equal  rectangles. Figure 1.b shows an example of partition obtained by a SoA method and by a quadtree. For each of the techniques considered as SoA segmentation methods, we compute the number of images in the dataset in which an evaluation measure correctly judges that the baseline result is worse than the SoA generated partition. We refer to the resulting meta-measure as SoABaseline Discrimination (SABD), and it is defined as the global percentage of correct judgments for a given measure. 4.3. Swapped-Image SoA Discrimination Segmentation evaluation measures are often used to adjust the parameters of a segmentation technique. They are therefore used to compare different partitions created by the same algorithm. To incorporate this type of comparisons to the meta-measures, we compare (i) the results created by a SoA segmentation technique with (ii) the results created by that same algorithm but on a different image. In other words, we compare the ground-truth of a certain image with two results obtained using the same algorithm and parameterization: (i) one segmentation of that same image and (ii) one of a different image. The hypothesis in this case is that the evaluation measures should judge that the same-image result is better than the different-image one. In the example of Figure 1.c, the measure should judge that the first partition is better than the second one compared both with the ground-truth of the former. In this meta-measure, evaluation measures have to tackle the potential bias of the  SoA methods towards their specific type of results. For each SoA segmentation technique, we compute the number of images in the dataset in which an evaluation measure correctly judges that the same-image SoA result is better than the different-image one. We define the metameasure Swapped-Image SoA Discrimination as the percentage of results in the database, for all the SoA methods, that the measures correctly discriminates. 5. Experimental Validation The state of the art of segmentation is represented in this paper by the following six methods: the Ultrametric Contour Maps on the gPb contour detector (gPb-OWTUCM) [2], the Efficient Graph-Based (EGB) image segmentation algorithm [10], the Mean Shift (MShift) algorithm [6], the Normalized Cuts (NCuts) algorithm [29], and two types of Binary Partition Trees [27]: the Normalized Weighted Euclidean distance between Models with Contour complexity (NWMC) tree [3 1], and the Independent Identically Distributed - Kullback Leibler (IID-KL) tree [3]. The exact parameterizations for each algorithm is detailed at [28], where we also publish the code of all measures and meta-measures used in this work. All methods are assessed  Measure  MeGtalo-Mbaelas. SIHDMetSa-AMBeDasureSISD  Fb Fop VoI C(S→ {Gi}) →dvD{G DH(S⇒ {Gi}) SB⇒CE{ BGM  98.4 96.7 94.0 91.5 90.7 89.5 89.2 88.1  99.5 98.4 96.9 93.1 95.1 78.5 93.3 90.7  95.6 94.2 87.5 86.0 86.9 91.3 78.9 81.6  100.0 97.5 97.7 95.3 90.1 98.8 95.4 92.0  PRI C({Gi} → S) F}r DH({Gi} ⇒ S)  86.7 86.3 86.1 80.5  77.7 91.3 77.0 73.0  88.8 77.4 84.2 92.1  93.7 90.1 97.1 76.5  ({G}⇒S)  Table 2. Measure comparison in terms of quantitative measures. Values refer to percentages of correct results  meta-  at the Optimal Dataset Scale (ODS) [2] with respect to each evaluation measure. The parameter values of the newly proposed measure are: γo = 0.95, γp = 0.25, and β = 0.1. They have been trained on the training set of BSDS500 [2], optimizing the global meta-measure described in the following section (See Table 2). Note that this optimization would not have been feasible without such quantitative meta-measures. Meta-Measures Results: Table 2 shows the three metameasure results for the test set of BSDS500, as well as a global summary meta-measure. Given that each meta-  measure represents a percentage of correct results, we define the global meta-measure as the global percentage of correct results. In global terms, Fb and Fop are the two top-ranked summary measures. On top of that, they both provide much richer information in form of precision-recall curves, thus we propose the pair Fb-Fop as the measures of choice. Regarding the computational cost of the measures, the mean time for image to compute the distances to the multiple-partition ground truth of BSDS500 is 3.79 2.06 s for Fb and at least one order of magnitude l3o.7w9er ± fo 2.r0 6thse froerst F of measures. In particular, Fop takes 0.078 0.020 s. 7In8 s ±cen 0a.0r2io0ss w.here the time limitations are tight, the authors believe that Fop would be the tool of choice. To provide an in-depth analysis of the final results, the tandem of precision-recall curves for boundaries and for objects-andparts would be the most adequate option. The following section provides a thorough analysis of both frameworks on the six SoA methods used in this paper.  ± ±  Precision-Recall Frameworks: Figure 4 shows the boundary and objects-and-parts precision-recall curves for 222111333644  Boundaries  Objects and Parts  Figure 4. Precision-Recall curves for boundaries (left) and for objects and parts (right). The solid curves represent the six SoA segmentation methods and the quadtree (see legends). In dashed lines with the same color, the SoA techniques assessed on a swapped image. The marker on each curve is placed on the Optimal Dataset Scale (ODS). The isolated red asterisks refer to the human performance assessed on the same image and on a swapped image. In the legend, the F measure of the marked point on each curve is presented in brackets.  the six SoA segmentation methods studied and the human performance. Prior to the assessment of segmentation techniques, let us focus on the comparison of the two evaluation frameworks. It is noticeable that the human baseline performance (human assessed on a different image) for Fb is 0.21, which  could be interpreted as Fb being too lax. In this same direction, the baseline boundary precision for Fb is between 0.2 and 0.3, that is, any result, no matter how wrong it is, will be judged as providing at least a 0.2 precision. While in the case of Fop the human baseline is correctly downgraded to 0.05 (as well as the swapped-image results), then the surprising fact is that human performance is as low as 0.56 (0.81 in Fb), which could entail that Fop is too strict. Although the dynamic range is a little higher in Fb (0.60 versus 0.51), the gap between the best method and humans is much higher in Fop (0.08 versus 0.21). In other words, Fop gives more resolution at the places where improvements over the SoA would be placed. Regarding the comparison among segmentation techniques, both frameworks confirm that the gPb-OWT-UCM technique has outstanding results with respect to the rest. The advantages of going beyond the summary measures are also clear on these plots. For instance, the summary Fb measure of quadtree (0.41) judges this technique close to NWMC (0.55), but in the precision-recall curves it is clear that quadtree is much worse. Similarly, judging by Fb, NWMC would be clearly discarded but if we are interested in low recall rates it could be of interest (apart from gPb-OWT-UCM). As common points between the two measures, NCuts is judged as being much better at high recall rates than at low ones and conversely, NWMC is much better at high precision rates. The measures are coherent also in the fact that  human results have a better precision than recall. As one of the main discrepant points, however, EGB is judged as the third best technique by Fb while being the worse for Fop. To further analyze this behavior, Figure 5 shows an image (a), an EGB result (b), and the associated ground truth (c). The EGB result consists of thin long regions that surround the object but do not close. The assessment value of this result is Fb = 0.62 and Fop = 0.05. From a region-based point of view, this type of results is correctly penalized by Fop and not by Fb, since as a contour detector the result is correct. To sum up, both measures are complementary thus we propose them in tandem as the tool of choice for image segmentation evaluation.  (a)(b)(c)  Figure 5. EGB result correctly penalized by Fop but not by Fb 222111333755  6. Conclusions This paper reviews an extensive set of segmentation evaluation measures and presents the new precision-recall measure for objects and parts. Three meta-measures are used  (two newly proposed) to quantitatively compare the goodness of the evaluation measures. The results show that the tandem boundary and objects-and-parts precision-recall curves is a good candidate for benchmarking segmentation algorithms; since apart from obtaining the best metameasure results, their precision-recall curves provide rich knowledge about the results. By making our code and datasets publicly available we allow researchers to easily assess their results and gain deeper understanding of their algorithms.</p>
<p>[1] P. Arbel ´aez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and J. Malik. Semantic segmentation using regions and parts. In CVPR, 2012. 1, 4</p>
<p>[2] P. Arbel ´aez, M. Maire, C. C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE TPAMI, 33(5):898–916, 2011. 1, 2, 3, 4, 5, 6</p>
<p>[3] F. Calderero and F. Marques. Region merging techniques using information theory statistical measures. IEEE TIP, 19(6): 1567–1586, 2010. 6</p>
<p>[4] J. S. Cardoso and L. Corte-Real. Toward a generic evaluation of image segmentation. IEEE TIP, 14(1 1): 1773–1782, 2005. 1, 2, 3, 5</p>
<p>[5] J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation. In CVPR, 2010. 1</p>
<p>[6] D. Comaniciu and P. Meer. Mean shift: a robust approach toward feature space analysis. IEEE TPAMI, 24(5):603 –619,</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]  2002. 6 S. Dongen. Performance criteria for graph clustering and markov cluster experiments. Technical Report INS-R0012, Centrum voor Wiskunde en Informatica (CWI), Amsterdam, The Nederlands, 2000. 1, 2, 3 M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. www.pascal-network.org/ challenges/VOC/voc2012/workshop/index.html. 1 P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained partbased models. IEEE TPAMI, 32(9): 1627–1645, 2010. 1 P. F. Felzenszwalb and D. P. Huttenlocher. Efficient graphbased image segmentation. IJCV, 59:2004, 2004. 6 D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error in object detectors. In ECCV, 2012. 1 A. Hoover, G. Jean-Baptiste, X. Jiang, P. Flynn, H. Bunke, D. Goldgof, K. Bowyer, D. Eggert, A. Fitzgibbon, and R. Fisher. An experimental comparison of range image segmentation algorithms. IEEE TPAMI, 18:673–689, 1996. 1, 4</p>
<p>[13] Q. Huang and B. Dom. Quantitative methods of evaluating image segmentation. In ICIP, 1995. 1, 2, 3</p>
<p>[14] X. Jiang, C. Marti, C. Irniger, and H. Bunke. Distance measures for image segmentation evaluation. EURASIP J. Appl. Signal Process. , 2006: 1–10, 2006. 2, 3</p>
<p>[15] T. Kanungo, B. Dom, W. Niblack, and D. Steele. A fast algorithm for MDL-based multi-band image segmentation. Technical report, IBM Reasearch Division, RJ 9754 (84640), 1994. 1, 2</p>
<p>[16] C. Lampert, M. Blaschko, and T. Hofmann. Beyond sliding windows: Object localization by efficient subwindow search. In CVPR, 2008. 1</p>
<p>[17] G. Liu and R. Haralick. Assignment problem in edge detection performance evaluation. In CVPR, 2000. 3, 4</p>
<p>[18] T. Malisiewicz and A. A. Efros. Improving spatial support for objects via multiple segmentations. In BMVC, 2007. 1, 4</p>
<p>[19] D. Martin. An Empirical Approach to Grouping and Segmentation. PhD thesis, EECS Department, University of California, Berkeley, Aug 2003. 2, 3, 4, 5</p>
<p>[20] D. Martin, C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using local brightness, color, and texture cues. IEEE TPAMI, 26(5):530–549, 2004. 1, 4</p>
<p>[21] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001. 5</p>
<p>[22] M. Meil a˘. Comparing clusterings: an axiomatic view. In ICML, 2005. 1, 2, 3</p>
<p>[23] S. Nowozin, P. Gehler, and C. Lampert. On parameter learning in crf-based approaches to object class image segmentation. In ECCV, 2010. 2</p>
<p>[24] B. Peng and L. Zhang. Evaluation of image segmentation quality by adaptive ground truth composition. In ECCV, 2012. 1</p>
<p>[25] J. Pont-Tuset and F. Marques. Supervised assessment of segmentation hierarchies. In ECCV, 2012. 1</p>
<p>[26] W. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850, 1971. 3</p>
<p>[27] P. Salembier and L. Garrido. Binary partition tree as an effi-</p>
<p>[28]</p>
<p>[29]</p>
<p>[30]  [3 1]</p>
<p>[32]  cient representation for image processing, segmentation, and information retrieval. IEEE TIP, 9(4):561–576, 2000. 6 Segmentation Evaluation Code. https://imatge.upc.edu/web /resources/supervised-evaluation-image-segmentation. 2, 6 J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE TPAMI, 22(8):888–905, 2000. 6 R. Unnikrishnan, C. Pantofaru, and M. Hebert. Toward objective evaluation of image segmentation algorithms. IEEE TPAMI, 29(6):929–944, 2007. 1, 3, 5 V. Vilaplana, F. Marques, and P. Salembier. Binary partition trees for object detection. IEEE TIP, 17(1 1):2201–2216, 2008. 6 P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001 . 1 222111333866</p>
<br/>
<br/><br/><br/></body>
</html>
