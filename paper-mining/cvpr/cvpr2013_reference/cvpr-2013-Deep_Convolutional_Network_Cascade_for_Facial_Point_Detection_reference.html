<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-104" href="../cvpr2013/cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">cvpr2013-104</a> <a title="cvpr-2013-104-reference" href="#">cvpr2013-104-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</h1>
<br/><p>Source: <a title="cvpr-2013-104-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sun_Deep_Convolutional_Network_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
