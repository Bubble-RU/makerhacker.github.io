<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-467" href="../cvpr2013/cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">cvpr2013-467</a> <a title="cvpr-2013-467-reference" href="#">cvpr2013-467-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</h1>
<br/><p>Source: <a title="cvpr-2013-467-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Luo_Wide-Baseline_Hair_Capture_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras. 1 Introduction Multi-view stereo methods have been widely used to reconstruct real world objects with ever improving quality [7, 1]. However, hair reconstruction remains one of the most challenging tasks due to many unique hair characteristics. For instance, omni-present occlusions and complex strand geometry preclude general surface-based smoothness priors [26] for hair reconstruction. The highly specular nature of hair [17] also violates the Lambertian surface assumption employed in most multi-view stereo methods. Consequently, many practical systems have either completely avoided hair reconstruction during facial capture (e.g. [1]), or relied on manual input to achieve plausible results [19]. Researchers have explored specialized hardware to facilitate hair capture, such as a fixed camera with moving light sources [21], a stage-mounted camera with macro lens [8], thermal imaging [12], etc. These methods are often costly, and require lengthy capture sessions that limit their applicability to only static hairstyles. An alternative approach is to deploy dense camera arrays that have small baselines. To capture complete full-head hairstyles, it is typical to have 20 to 30 camera views [25, 15, 12]. However, due to the complex hardware setup, it is challenging to adopt many cameras in real-world systems. In this work, we study hair capture with a wide-baseline camera setup. Merely 8 cameras are used to capture the complete hair geometry, with each adjacent pair of cameras having a large 45-degree wide angular baseline. Under such a setup, stereo matching based on aggregation schemes such as local window or surface patch in existing methods [15, 25] is unreliable and error-prone. Instead, we propose that 3D strand is a better “aggregation unit” for stereo matching in hair reconstruction because it models hair’s characteristic “strand-like” structural continuity and thus yields improved robustness against matching ambiguities in wide-baseline setups. The 3D strands are first generated separately from a 2D strand extraction step in each view and then jointly optimized in a strand-based refinement step. We also introduce a novel formulation of smoothness energy that regularizes the optimization at the strand, wisp and global levels to better account for real hair dynamics, hair wisp structures and 222666555 Figure 2: The overview of our reconstruction method. We take 8 input images from different views and compute the orientation map for each. Using the visual hull constructed from the segmented images, we extract strands on the orientation maps and project them from each view onto the visual hull for strand initialization. Finally we perform strand-based refinement to obtain the final strand positions. The hair surface can then be reconstructed from the refined strands using [10]. cross-view reconstruction consistency. We quantitatively evaluate our reconstruction method on synthetic hairstyles, and achieve an accuracy of ∼3mm. sIny atdhdetiitcio nh,a trhstey approach can hea andnle a a rwaicdye variety mof. hairstyles in static images and dynamic sequences, as demonstrated with real examples in the results section and the supplemental materials. 2 Related Work In this section, we review existing technologies for hair capture, including those using dedicated setups and dense camera arrays. In addition, we survey a few traditional multiview stereo methods that are closely related to the proposed algorithm, including refinement-based reconstruction and wide-baseline stereo. 2.1 Hair Capture A few dedicated systems in the literature have been designed for hair capture. Paris et al. [20] proposed to estimate the hair orientation in images and analyze the highlights on the hair. This analysis requires a fixed camera with a light source moving along a predefined trajectory. Later, Paris et al. [21] presented Hair Photobooth, a complex system made of several light sources, projectors, and video cameras that capture a rich set of data to extract the hair geometry and appearance. Jakob et al. [8] showed how to capture individual hair strands using focal sweeps with a macro-lens equipped camera controlled by a robotic gantry. Recently, thermal imaging has been applied for hair reconstruction to avoid shadowing and anisotropic reflectance [12]. While accurate, these techniques are expensive, and the capture process is usually slow and only applicable for static hairs. Work has also been done to capture hair with more flexible setups. Wei et al. [25] proposed a technique based on many hand-held photographs. Their approach uses a coarse visual hull as the approximate bounding geometry for hair growing constrained with orientation consistency. Yamaguchi et al. [27] used an array of 12 cameras to capture partial geometry of straight hair in moderate motion. Guided by hair simulation, Zhang et al. [29] reconstructed smooth hair dynamics with 7 cameras. Using only a single view, Chai et al. [4] proposed a method to generate a depth map for convincing view interpolation of different hairstyles. Beeler at al. [2] used a high resolution dense camera array to reconstruct facial hair strand geometry by matching distinctive strands. In contrast with these approaches, our method is capable of reconstructing accurate hair geometry from a wide-baseline sparse camera array. 2.2 Related Multi-view Stereo Methods There have been many multi-view stereo methods presented in the literature [22]. The proposed method belongs to the general category of refining a rough initial geometry (e.g., a visual hull) by optimizing for cross-view consistency. And the consistency is measured in novel ways in order to handle the wide-baseline and challenging hair characteristics. Space carving [11] reconstructs objects by eliminating voxels in a volume with low photo-consistency across visible views. Inspired by the active contour method [9], many reconstruction methods iteratively refine a rough initial shape (usually the visual hull) to obtain the final reconstruction by optimizing cross-view photo-consistency and surface smoothness. For instance, Hernandez and Schmitt [5] proposed a visual hull refinement method by iteratively minimizing the texture, silhouette and surface smoothness energies. Furukawa and Ponce [6] segmented the initial visual hull into surface areas between the rims and refined each via graph cuts. Another important line of research for wide-baseline camera setups is to find robust feature correspondences between images. The well-known SIFT descriptor [14] is capable of finding feature points on images with significantly different viewpoints and illuminations. Tola et al. [23] extended the idea of SIFT to find dense correspondences 222666666 Figure 3: Our hair capture setup and a few sample images. We use 8 cameras (outlined in red) in wide-baseline to capture the complete hair styles. Four area lamps (outlined in blue) are used to compensate for the short exposure time. across views for high quality reconstruction. Local regions with view-invariant properties have also been studied such as affinely invariant regions [24] and maximally stable extremal regions [18]. However, due to the lack of reliable texture and corner-like features on hair, it is difficult to apply these methods on hair reconstruction. 3 Overview Given a set of wide-baseline images (see Fig. 3 for some sample images), our goal is to compute a shape that best approximates the captured hair volume. We achieve this by refining the positions of a dense set of representative 3D hair strands derived from each camera view. Fig. 2 gives an overview of the various steps involved in our hair capture algorithm. We defer the description of our acquisition setup to Sec. 6. To create the initial 3D strands for refinement, we first compute the hair orientation map for each input image, and extract the 2D strands by tracking the confident ridges on the orientation map. The 2D strands are then back-projected onto the visual hull constructed from the segmented foreground of all input images to form the initial 3D strands. An iterative strand refinement algorithm is then applied to optimize the orientation consistency of the projected strands on all the orientation maps. We regularize the optimization with the silhouette constraint as well as a set of specialized smoothness priors for hair. The final hair shape is obtained using Poisson surface reconstruction [10] from the refined 3D strands. In the rest of the paper, we will describe strand initialization in Sec. 4, and present the novel strand-based refinement algorithm in Section 5. Experimental results and conclusions are given in Sections 6 and 7, respectively. (a)(b)(c) Figure 4: The steps of strand initialization in Sec. 4. For each input image (a), we compute the orientation map (b). Then we extract strands on the orientation map and project them onto the visual hull for initialization (c). 4 Strand initialization We first compute an orientation map for each image using the method proposed in [15], which uses a bank of rotated filters to detect the dominant orientation at each pixel. The orientation map is enhanced with 3 passes of iterative refinement to improve the signal-to-noise ratio as in [4]. To further reduce noises in regions with low confidence, we apply the bilateral filtering method in [20] to diffuse the orientations of the high confidence regions. We then track the confidence ridges of each orientation map (Fig. 4(b)) using hysteresis thresholding similar to [4]. The result is a set of poly-line 2D strands consisting of densely sampled vertices in about 1-pixel steps. We backproject each vertex of the resulting 2D strands onto the visual hull to determine the initial position of the 3D strands, as shown in Fig. 4(c). Note that the 3D strands are generally over-sampled after back-projection from 2D strands. Thus we down-sample each 3D strand by uniformly decimating the vertices to 20% of the original vertex count in order to reduce the computation cost in the following steps. 5 Strand-based refinement After initializing the 3D strands from the 2D strands in each reference view (the view from which the strands were extracted), we iteratively refine all the strands by optimizing the projected orientation consistency across all visible views with silhouette and smoothness constraints (Fig. 5). The optimization is formulated as an energy minimization problem. The total energy is defined as the weighted sum of a few specific energies, such as orientation energy, silhouette energy and smoothness energy: E = ∑α?E? (1) ? where ? denotes each specific energy term as we will describe in detail in the following sections. All the energy terms are formulated in squared forms so that we can minimize the total energy with efficient non-linear solvers such as Levenberg-Marquardt [16]. 222666777 ??????211...4280510 ?????????? Figure 5: In strand-based refinement (Sec. 5), the strands (first row) are refined over the iterations with their reconstructed surfaces (second row) revealing more hair details. 5.1 Notations and Definitions Let p denote a strand vertex on a 3D strand S. We use subscript to reference its successor p+1 and and predecessor p−1 on S. Similarly, we can define p+0.5 as the middle point between p and p+1 . The strand direction d(p) at p is defined as p+1 − p−1 . The reference view of p is denoted as R(p) and the visibility V(p) of p defines the set of views where p is visible. Since strand visibility is difficult to define exactly during strand refinement, we approximate V(p) by the visibility of its closest point h(p) on the visual hull H during the refinement. It is obvious to see that p’s reference view R(p) ∈ V(p). We define two different neighborhoods for vertex p: the same-view neighborhood N+ (p) and the different-view neighborhood N− (p), as shown in Fig. 6. N+ (p) is defined as the vertices from the same reference view as p and located within a certain 3D Euclidean distance from p. Vertices on the same strand as p are excluded from N+ (p). N− (p) is defined similarly but the neighboring vertices are from different reference views. Likewise, we define the same-view weight (p, q) between two vertices p and q if q ∈ N+ (p) and different-view weight ww−o (vep,r qtic) isf q a∈n dN q− i f(p q). ∈ T Nhe different-view weight w− (p, q) is( simply qde ∈fin Ned as the Gaussian weight: w+ w−(p,q) = exp?−?p2 −σe2 q?2? (2) where σe controls the influence radius around the strand vertices and is set to 0.05 of the diagonal length D of the visual hull’s bounding box. The same-view weight (p, q) is a bilateral weight that takes into account both the Euclidean w+ q1, 2∈Nqp2+q(1p) q∈Nq−p( ) Figure 6: The illustrations of same-view neighborhood N+ (p) and different-view neighborhood N− (p) for p. The neighbors are searched within the radius (2.5σe) indicated by the dashed circles. Same-view neighbors q1 and q2 can be weighted differently by how their orientations differ from p ’s. The different-view neighbor q is located on the strands from a different reference view (in blue). distance and the orientation difference between p and q: w+(p,q) = exp?−1 − ?d(2pσ)o2,d(q)?2−?p2 −σe2 q?2? (3) where σo controls the influence between strand vertices with similar orientations and is set to 0.5. The notation ?A, B? is sdiemfinileadr as tnhtea cioonsisn aen odf i sth see angle b.e Ttwheee nno tawtioo nve ?cAt,oBrs? i As and B, i.e., ?A, B? ? A · B/( ?A? ?B? ). This applies to both 3anDd a Bn,d i .2eD., cases. I ?f e Aith ·e Br/ / A( or ? B? Bis? zero, ?iAs ,a pB?p i=e s1 t.o Note that we often use the normalized weights for all the neighbors. We define the normalized same-view weight w¯+ (p, q) and different-view weight w¯ − (p, q) as: w¯+(p,q) =∑q∈Nw++(p()pw,+q()p,q), w¯ −(p,q) =∑q∈Nw−(−p()pw,−q()p,q). We also define a “surface” normal n(p) at each strand vertex p, which can be computed by finding the eigenvector with the smallest eigenvalue of the covariance matrix w+(p, q)(q − p)(q − p)?. ∑q∈N+(p) We use superscript pV to define the projected 2D point of p on one of the visible views V ∈ V(p). During the roeffi npe omnen ont, eth oef position oblfe p viine w3Ds space is( pre).stri Dcuterdin along the ray shooting from the optical center of the reference view R(p) to its projected point pR(p) on the reference view. This ensures that the vertex has the same projection on the reference view, and saves computation cost thanks to the reduced degrees-of-freedom. 5.2 Orientation Energy The orientation energy Eorient is designed to make sure that when a 3D strand is projected onto its visible views, the projected orientations are consistent with those indicated by the orientation maps of those views. Once we apply the diffusion scheme described in Sec. 4 to the orientation map OV of view V, an orientation vector OV (pV) is defined at any point pV in the hair region , otherwise we set OV (pV) = 0 (Fig. 7). We then define an orientation energy term eoVrient (pV) for each segment (p, p+1 ) on 222666888 pVpV+0.5pV+1OV(p+V0.5) Figure 7: The illustration of orientation energy. A strand is projected on the orientation map in similar color coding as in Fig. 4. The orientation energy term eoVrient(pV) is determined by the angle between OV(p+V0.5) and p+V1 − pV. S as follows: eoVrient(pV) = min ?1 − ?p+V1 − pV, OV(p+V0.5)?2, Torient? ?(4) where Torient = 0.5 i?s a threshold to make the energy rob?ust to outliers with large projected orientation inconsistency. Note that the square in the definition makes it invariant to ±180◦ directional ambiguity. Finally, we define the orientation energy Eorient as: Eorient=∑pV∈∑V(p)wV(p)eoVrient(pV), (5) where wV (p) = max(?n(p) , v(p)? , 0) is the visibility weight of p with respect atox (v?inew(p V),v, a(np)d? ,v(0)p) i si tsh tehe v dsiirbeiclittiyon w efrigohmt p to the optical center of view V. 5.3 Silhouette Energy We also enforce the 3D strands to be within and near the visual hull H using silhouette energy. As illustrated in Fig. 8, given p’s closest point h(p) on H and h(p)’s normal nh, we can define silhouette energy Esilh as: Esilh=D12∑pβ??p − h(p)?· nh?2 (6) represents inner product, and β is used to discrim- where · winaheter eth ·e r einprsiedsee atnsd in onuetrsi pdreo cases fnodr p with respect to H: β=?w1out ( p − − h ( p ) ) · n h >≤ 0 , (7) where wout is a large penalty (104) against the case where the vertex is outside the visual hull H. Note that the diagonal length D of H’s bounding box is used to make the energy unit-less. Similar approach is applied for unit-less energy formulation in the following sections. 5.4 Smoothness Energy Smoothness energy is formulated at three different levels to better control the smoothness granularity: the strand level, the wisp level and the global level. The formulation for strand level smoothness Estrand stems from the discrete elastic rod model [3] often used in hair simulation that minimizes the squared curvature along hair strands. Further inspired by [4], we take into account the orientation similarity in the bilateral same-view weight so that the w+ poutnhh(p)Hpinh(p)nh Figure 8: The illustration of silhouette energy. The sign of (p − h(p)) · nh determines if a point is inside H and thus t(hpe − −va hlu(ep ofβ in silhouette energy for pin and pout. wisp smoothness energy Ewisp can better adapt to the local wisp structures and hair’s depth discontinuities. Finally, the global smoothness energy Eglobal ensures the global consistency of strand geometry across different views. Strand smoothness energy Inspired by [3], we define the strand smoothness energy as the summation of squared curvature for each vertex along all the strands: Estrand = D2 ∑curv2(p) p where curvature is computed as: (8) × curv(p) =l+1+2 l−1????p+l1+−1 p−p −l−1 p−1???? (9) where l+1 = ?p+1 − p? and ???l−1 = ?p − p−1 ? . Wisp smoothness energy We use wisp smoothness energy to enforce a strand vertex and its small same-view neighborhood N+ (p) within the same wisp to lie on a local plane. We use the orientation similarity to estimate the likelihood of being in the same wisp and encode it in the same-view weight The wisp smoothness energy is thus defined as: w+. Ewisp=D12∑p??p −q∈N∑+(p) w¯+(p,q)q? · n(p)?2 (10) Global smoothness energy Finally, the global smoothness energy is defined similarly to the wisp smoothness energy to enforce global refinement consistency through local planar resemblance across different views: Eglobal=D12∑p??p −q∈N∑−(p) w¯−(p,q)q? · n(p)?2 (11) 6 Results We use a camera rig that contains 8 cameras around the subject powered by a single workstation, as shown in Fig. 3. The cameras are Point Grey Flea2 FireWire cameras, operating at 600 800 pixel resolution and 30 frames per secaotnidng. aAt f6e0w0 example images are oanls oa sdho 3w0n f ainm Fig. e3.r sWece- find the current 8-camera setup a good trade-off between reconstruction quality and acquisition complexity for full hairstyle capture. Fewer camera views will push the reconstruction quality towards visual hull due to smaller overlap between views. We also use the hair datasets from [13], but 222666999 × Figure 9: We evaluate the reconstruction accuracy on three synthetic hair styles (straight, wavy and wavythin, first row). We compute the depth maps of our reconstruction (second row) and compare them with the hair’s. The depth map differences are visualized in coded color (third row). The average reconstruction error is around 3 millimeters. only select 8 images with similar views as our setup from each original dataset for the reconstruction tests. The same set of energy weights are used for all the results in this work. Note that a relatively small αsilh is used to de-emphasize the importance of the visual hull on the reconstruction once the shape is inside the visual hull: αorient = 2 10−2 αstrand = 10−4 αglobal = 0.5 αsilh == 32 1100−5 αwisp = 0.5 The reconstruction results for all the examples are shown in Fig. 11. Note that we use [7] to reconstruct each subject’s facial area and then merge our hair reconstruction using Poisson surface reconstruction [10]. Our method can accurately reconstruct a variety of hair styles from short to long, from smooth to messy and from unconstrained to constrained. Also, our method is able to faithfully reveal interesting hair structures like wisps and curls. In contrast, general visual hull refinement on color texture [5] loses details (Fig. 1). Multi-view stereo methods with weak regularization, such as [7], fail to converge to the correct shape (Fig. 1) due to the challenging wide-baseline setup. Quantitative evaluation To quantitatively evaluate our method, we use the hair models by [28] and render them using the hair appearance model in [17], as shown in Fig. 9. Figure10:Sampleframes(firt ow)andtherconstruced surfaces (second row) from the dynamic hair capture setup. The three hair models (straight, wavy, wavythin) in the evaluation are representative for a variety of common hair types. Using the rendered images from viewpoints similar to our real capture setup, we are able to reconstruct the surface for the synthetic hair models. Since hair is volumetric, average closest point distance is not a good error measure. We therefore evaluate the reconstruction accuracy by comparing the depth maps of the hair model and the reconstructed surface on a specific view and visualize the differences in coded color (Fig. 9). The average reconstruction error is around 3mm. Larger errors can be observed in deep concave regions and regions at grazing angles to the cameras. Dynamic hair capture Compared to previous methods [21, 15], our method is able to capture complete moving hair with only 8 cameras. Three sample reconstructed frames for a hair-shaking performance are shown in Fig. 10 (please see the accompanying video for more results). Computation time The algorithm is implemented in C++. All the reconstruction tests are performed on a Core i7 2.3 GHz machine with 4GB memory. It takes 10 seconds to compute the orientation map for each 600× 800 input image acondm 1p steec tohned o trioe compute tahpe f ovirs euaacl hh 6ul0l0 ×fro8m00 08 i segmented images. The strand-based refinement takes 2 minutes. 7 Conclusion and Future Work We have proposed a novel algorithm to reconstruct complete full-head hair styles with strand-based refinement us- ing only 8 views. Compared to previous methods, our method is able to capture hair accurately with faithful hair structures even with a wide baseline setup. The reconstruction results are evaluated on a set of synthetic hair models and achieve ∼3mm reconstruction error on average. The falnexdib alceh requirement f roerc input acltlioowns e us otor capture complete hair in motion with an inexpensive camera rig. 222777000 However, our method does have a few limitations that need to be addressed in the future. The strand-based refinement relies on reasonably long strands to provide good regularization in the optimization. For certain extreme hair styles, like very short hair and fluffy hair, long continuous strands are scarce, which can adversely affect our reconstruction result. Also, because segmentation of hairy objects is still a very challenging problem in compute vision, the visual hull we used to reconstruct the hair is often too smooth, which causes our method to easily miss interesting stray hairs in the reconstruction. For dynamic capture, the motion blur can introduce “artificial strands” along the moving direction that undermines the reconstruction accuracy. The temporal coherence issue also needs to be addressed in the future by imposing temporal constraints. References [1] T. Beeler, B. Bickel, P. Beardsley, B. Sumner, and [2] [3] [4] [5] [6] [7] [8] [9] [10] M. Gross. High-quality single-shot capture of facial geometry. ACM Trans. Graph., 29, 2010. 1 T. Beeler, B. Bickel, G. Noris, S. Marschner, P. Beardsley, R. W. Sumner, and M. Gross. Coupled 3d reconstruction of sparse facial hair and skin. ACM Trans. Graph., 3 1:117: 1–1 17: 10, August 2012. 2 M. Bergou, M. Wardetzky, S. Robinson, B. Audoly, and E. Grinspun. Discrete elastic rods. In SIGGRAPH, 2008. 5 M. Chai, L. Wang, Y. Weng, Y. Yu, B. Guo, and K. Zhou. Single-view hair modeling for portrait manipulation. ACM Trans. Graph., 3 1(4): 116: 1–1 16:8, July 2012. 2, 3, 5 C. H. Esteban and F. Schmitt. Silhouette and stereo fusion for 3d object modeling. Comput. Vis. Image Underst., 96(3):367–392, Dec. 2004. 1, 2, 6 Y. Furukawa and J. Ponce. Carved visual hulls for image-based modeling. Int. J. Comput. Vision, 81(1):53–67, Jan 2009. 2 Y. Furukawa and J. Ponce. Accurate, dense, and robust multiview stereopsis. IEEE Trans. on PAMI, 32: 1362– 1376, 2010. 1, 6 W. Jakob, J. T. Moon, and S. Marschner. Capturing hair assemblies fiber by fiber. ACM Trans. Graph., 28(5): 164: 1–164:9, December 2009. 1, 2 M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contour models. International Journal of Computer Vision, 1(4):321–331, 1988. 2 M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson sur- [11] [12] [13] [14] [15] [16] [17] [18] face reconstruction. In Proc. SGP, 2006. 2, 3, 6 K. N. Kutulakos and S. M. Seitz. A theory of shape by space carving. International Journal of Computer Vision, 38(3): 199–218, 2000. 2 T. Lay Herrrera, A. Zinke, and A. Weber. Lighting hair from the inside: A thermal approach to hair reconstruction. In ACM Transactions on Graphics, volume 31, 2012. 1, 2 M. Lhuillier and L. Quan. A quasi-dense approach to surface reconstruction from uncalibrated images. IEEE Trans. PAMI, 27(3):418–433, Mar 2005. 5 D. G. Lowe. Object recognition from local scaleinvariant features. In Proc. ICCV, 1999. 2 L. Luo, H. Li, S. Paris, T. Weise, M. Pauly, and S. Rusinkiewicz. Multi-view hair capture using orientation fields. In Proc. CVPR, 2012. 1, 3, 6 D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial and Applied Mathematics, 11(2). 3 S. Marschner, H. W. Jensen, M. C. andS. Worley, and P. Hanrahan. Light scattering from human hair fibers. ACM Trans. Graph., 22(3):780–791, July 2003. 1, 6 J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-baseline stereo from maximally stable extremal regions. Image and Vision Computing, 22(10):761 767, 2004. 3 T. Mihashi, C. Tempelaar-Lietz, and G. Borshukov. Generating realistic human hair for The Matrix Reloaded. In ACM SIGGRAPH Sketches and Appli- – [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] cations Program, 2003. 1 S. Paris, H. Briceño, and F. Sillion. Capture of hair geometry from multiple images. ACM Trans. Graph., 23(3):712–719, August 2004. 2, 3 S. Paris, W. Chang, O. I. Kozhushnyan, W. Jarosz, W. Matusik, M. Zwicker, and F. Durand. Hair Photobooth: Geometric and photometric acquisition of real hairstyles. ACM Trans. Graph., 27(3):30: 1–30:9, August 2008. 1, 2, 6 S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski. A comparison and evaluation of multiview stereo reconstruction algorithms. In Proceedings of CVPR, 2006. 2 E. Tola, V.Lepetit, and P. Fua. A Fast Local Descriptor for Dense Matching. In Proc. CVPR, 2008. 2 T. Tuytelaars and L. V. Gool. Wide baseline stereo matching based on local, affinely invariant regions. In BMVC, Sep 2000. 3 Y. Wei, E. Ofek, L. Quan, and H.-Y. Shum. Modeling hair from multiple views. ACM Trans. Graph., 24(3):816–820, July 2005. 1, 2 O. Woodford, P. Torr, I. Reid, and A. Fitzgibbon. Global stereo reconstruction under second order smoothness priors. In Proc. CVPR, 2008. 1 T. Yamaguchi, B. Wilburn, and E. Ofek. Video-based modeling of dynamic hair. In Proc. PSIVT, 2008. 2 C. Yuksel, S. Schaefer, and J. Keyser. Hair meshes. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2009), 28(5): 166: 1–166:7, 2009. 6 [29] Q. Zhang, J. Tong, H. Wang, Z. Pan, and R. Yang. Simulation guided hair dynamics modeling from video. Computer Graphics Forum, 3 1(7pt1). 2 222777111 222777222</p><br/>
<h2>reference text</h2><p>[1] T. Beeler, B. Bickel, P. Beardsley, B. Sumner, and</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]  M. Gross. High-quality single-shot capture of facial geometry. ACM Trans. Graph., 29, 2010. 1 T. Beeler, B. Bickel, G. Noris, S. Marschner, P. Beardsley, R. W. Sumner, and M. Gross. Coupled 3d reconstruction of sparse facial hair and skin. ACM Trans. Graph., 3 1:117: 1–1 17: 10, August 2012. 2 M. Bergou, M. Wardetzky, S. Robinson, B. Audoly, and E. Grinspun. Discrete elastic rods. In SIGGRAPH, 2008. 5 M. Chai, L. Wang, Y. Weng, Y. Yu, B. Guo, and K. Zhou. Single-view hair modeling for portrait manipulation. ACM Trans. Graph., 3 1(4): 116: 1–1 16:8, July 2012. 2, 3, 5 C. H. Esteban and F. Schmitt. Silhouette and stereo fusion for 3d object modeling. Comput. Vis. Image Underst., 96(3):367–392, Dec. 2004. 1, 2, 6 Y. Furukawa and J. Ponce. Carved visual hulls for image-based modeling. Int. J. Comput. Vision, 81(1):53–67, Jan 2009. 2 Y. Furukawa and J. Ponce. Accurate, dense, and robust multiview stereopsis. IEEE Trans. on PAMI, 32: 1362– 1376, 2010. 1, 6 W. Jakob, J. T. Moon, and S. Marschner. Capturing hair assemblies fiber by fiber. ACM Trans. Graph., 28(5): 164: 1–164:9, December 2009. 1, 2 M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contour models. International Journal of Computer Vision, 1(4):321–331, 1988. 2 M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson sur-</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]  face reconstruction. In Proc. SGP, 2006. 2, 3, 6 K. N. Kutulakos and S. M. Seitz. A theory of shape by space carving. International Journal of Computer Vision, 38(3): 199–218, 2000. 2 T. Lay Herrrera, A. Zinke, and A. Weber. Lighting hair from the inside: A thermal approach to hair reconstruction. In ACM Transactions on Graphics, volume 31, 2012. 1, 2 M. Lhuillier and L. Quan. A quasi-dense approach to surface reconstruction from uncalibrated images. IEEE Trans. PAMI, 27(3):418–433, Mar 2005. 5 D. G. Lowe. Object recognition from local scaleinvariant features. In Proc. ICCV, 1999. 2 L. Luo, H. Li, S. Paris, T. Weise, M. Pauly, and S. Rusinkiewicz. Multi-view hair capture using orientation fields. In Proc. CVPR, 2012. 1, 3, 6 D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial and Applied Mathematics, 11(2). 3 S. Marschner, H. W. Jensen, M. C. andS. Worley, and P. Hanrahan. Light scattering from human hair fibers. ACM Trans. Graph., 22(3):780–791, July 2003. 1, 6 J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-baseline stereo from maximally stable extremal regions. Image and Vision Computing, 22(10):761 767, 2004. 3 T. Mihashi, C. Tempelaar-Lietz, and G. Borshukov. Generating realistic human hair for The Matrix Reloaded. In ACM SIGGRAPH Sketches and Appli-  –</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]  cations Program, 2003. 1 S. Paris, H. Briceño, and F. Sillion. Capture of hair geometry from multiple images. ACM Trans. Graph., 23(3):712–719, August 2004. 2, 3 S. Paris, W. Chang, O. I. Kozhushnyan, W. Jarosz, W. Matusik, M. Zwicker, and F. Durand. Hair Photobooth: Geometric and photometric acquisition of real hairstyles. ACM Trans. Graph., 27(3):30: 1–30:9, August 2008. 1, 2, 6 S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski. A comparison and evaluation of multiview stereo reconstruction algorithms. In Proceedings of CVPR, 2006. 2 E. Tola, V.Lepetit, and P. Fua. A Fast Local Descriptor for Dense Matching. In Proc. CVPR, 2008. 2 T. Tuytelaars and L. V. Gool. Wide baseline stereo matching based on local, affinely invariant regions. In BMVC, Sep 2000. 3 Y. Wei, E. Ofek, L. Quan, and H.-Y. Shum. Modeling hair from multiple views. ACM Trans. Graph., 24(3):816–820, July 2005. 1, 2 O. Woodford, P. Torr, I. Reid, and A. Fitzgibbon. Global stereo reconstruction under second order smoothness priors. In Proc. CVPR, 2008. 1 T. Yamaguchi, B. Wilburn, and E. Ofek. Video-based modeling of dynamic hair. In Proc. PSIVT, 2008. 2 C. Yuksel, S. Schaefer, and J. Keyser. Hair meshes. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2009), 28(5): 166: 1–166:7, 2009. 6</p>
<p>[29] Q. Zhang, J. Tong, H. Wang, Z. Pan, and R. Yang. Simulation guided hair dynamics modeling from video. Computer Graphics Forum, 3 1(7pt1). 2 222777111  222777222</p>
<br/>
<br/><br/><br/></body>
</html>
