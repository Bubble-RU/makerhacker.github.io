<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-98" href="../cvpr2013/cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">cvpr2013-98</a> <a title="cvpr-2013-98-reference" href="#">cvpr2013-98-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</h1>
<br/><p>Source: <a title="cvpr-2013-98-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_Cross-View_Action_Recognition_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu, Cunzhao Shi</p><p>Abstract: In this paper, we propose a novel method for cross-view action recognition via a continuous virtual path which connects the source view and the target view. Each point on this virtual path is a virtual view which is obtained by a linear transformation of the action descriptor. All the virtual views are concatenated into an infinite-dimensional feature to characterize continuous changes from the source to the target view. However, these infinite-dimensional features cannot be used directly. Thus, we propose a virtual view kernel to compute the value of similarity between two infinite-dimensional features, which can be readily used to construct any kernelized classifiers. In addition, there are a lot of unlabeled samples from the target view, which can be utilized to improve the performance of classifiers. Thus, we present a constraint strategy to explore the information contained in the unlabeled samples. The rationality behind the constraint is that any action video belongs to only one class. Our method is verified on the IXMAS dataset, and the experimental results demonstrate that our method achieves better performance than the state-of-the-art methods.</p><br/>
<h2>reference text</h2><p>[1] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. Analysis of representations for domain adaptation. In Proc. of NIPS, page 137, 2007.</p>
<p>[2] A. Bergamo and L. Torresani. Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach. In Proc. of NIPS, 2010.</p>
<p>[3] C. Chang and C. Lin. Libsvm: a library for support vector machines. ACM Trans. on TIST, 2(3):27, 2001.</p>
<p>[4] P. Doll a´r, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. In Proc. of ICCV workshop: VS-PETS, 2005.</p>
<p>[5] A. Efros, A. Berg, G. Mori, and J. Malik. Recognizing action at a distance. In Proc. of ICCV, pages 726–733, 2003.</p>
<p>[6] A. Farhadi and M. Tabrizi. Learning to recognize activities from the wrong view point. In Proc. of ECCV, pages 154– 166, 2008.</p>
<p>[7] A. Farhadi, M. Tabrizi, I. Endres, and D. Forsyth. A latent model ofdiscriminative aspect. In Proc. ofICCV, pages 948– 955, 2009.</p>
<p>[8] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Proc. of CVPR, pages 2066–2073, 2012.</p>
<p>[9] R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Proc. of ICCV, pages 999–1006, 2011.</p>
<p>[10] B. Hall. Lie algebras, and reprensentations: An elementary introduction. Springer, 2003.</p>
<p>[11] I. Junejo, E. Dexter, I. Laptev, and P. P ´erez. Cross-view action recognition from temporal self-similarities. In Proc. of ECCV, pages 293–306, 2008.</p>
<p>[12] I. Junejo, E. Dexter, I. Laptev, and P. P ´erez. Viewindependent action recognition from temporal selfsimilarities. PAMI, 33(1): 172–185, 2011.</p>
<p>[13] A. Kovashka and K. Grauman. Learning a hierarchy of discriminative space-time neighborhood features for human action recognition. In Proc. of CVPR, pages 2046–2053, 2010.</p>
<p>[14] R. Li, T. Tian, and S. Sclaroff. Simultaneous learning of nonlinear manifold and dynamical models for high-dimensional time series. In Proc. of ICCV, pages 1–8, 2007.</p>
<p>[15] R. Li and T. Zickler. Discriminative virtual views for crossview action recognition. In Proc. of CVPR, pages 2855– 2862, 2012.</p>
<p>[16] Z. Lin, Z. Jiang, and L. Davis. Recognizing actions by shapemotion prototype trees. In Proc. of ICCV, pages 444–45 1, 2009.</p>
<p>[17] J. Liu, M. Shah, B. Kuipers, and S. Savarese. Cross-view action recognition via view knowledge transfer. In Proc. of CVPR, pages 3209–3216, 2011.</p>
<p>[18] J. Liu, Y. Yang, and M. Shah. Learning semantic visual vocabularies using diffusion distance. In Proc. of CVPR, pages 461–468, 2009.</p>
<p>[19] F. Lv and R. Nevatia. Single view human action recognition using key pose matching and viterbi path searching. In Proc. of CVPR, pages 1–8, 2007.</p>
<p>[20] V. Parameswaran and R. Chellappa. View invariance for human action recognition. IJCV, 66(1):83–101, 2006.</p>
<p>[21] C. Rao, A. Yilmaz, and M. Shah. View-invariant representation and recognition of actions. IJCV, 50(2):203–226, 2002.</p>
<p>[22] M. Raptis and S. Soatto. Tracklet descriptors for action modeling and video analysis. In Proc. of ECCV, pages 577–590, 2010.</p>
<p>[23] A. Srivastava, I. Jermyn, and S. Joshi. Riemannian analysis of probability density functions with applications in vision. In Proc. of CVPR, pages 1–8, 2007.</p>
<p>[24] D. Tran and A. Sorokin. Human activity recognition with metric learning. In Proc. of ECCV, pages 548–561, 2008.</p>
<p>[25] H. Wang, M. Ullah, A. Klaser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features for action recognition. In Proc. of BMVC, 2009.</p>
<p>[26] D. Weinland, E. Boyer, and R. Ronfard. Action recognition</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30]  from arbitrary views using 3d exemplars. In Proc. of ICCV, pages 1–7, 2007. S. Xiang, F. Nie, Y. Song, and C. Zhang. Contour graph based human tracking and action sequence recognition. Pattern Recognition, 41(12):3653–3664, 2008. A. Yilmaz and M. Shah. Actions sketch: A novel action representation. In Proc. of CVPR, pages 984–989, 2005. Z. Zhang, C. Wang, B. Xiao, W. Zhou, and S. Liu. Action recognition using context-constrained linear coding. Signal Processing Letters, IEEE, 19(7):439–442, 2012. Z. Zhang, C. Wang, B. Xiao, W. Zhou, and S. Liu. Contextual fisher kernels for human action recognition. In Proc. of ICPR, pages 437–440, 2012. 222666999755</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
