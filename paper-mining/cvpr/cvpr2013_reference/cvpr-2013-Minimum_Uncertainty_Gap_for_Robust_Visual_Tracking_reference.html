<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-285" href="../cvpr2013/cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">cvpr2013-285</a> <a title="cvpr-2013-285-reference" href="#">cvpr2013-285-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</h1>
<br/><p>Source: <a title="cvpr-2013-285-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kwon_Minimum_Uncertainty_Gap_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><br/>
<h2>reference text</h2><p>[1] A. Adam, E. Rivlin, and I. Shimshoni. Robust fragments-based tracking using the integral histogram. CVPR, 2006.</p>
<p>[2] B. Babenko, M. Yang, and S. Belongie. Visual tracking with online multiple instance learning. CVPR, 2009.</p>
<p>[3] S. Birchfield. Elliptical head tracking using intensity gradients and color histograms. CVPR, 1998.</p>
<p>[4] CgSey.ns Iet.erma Blsyiz,ren Sedprs im annogemdre, An. 2t0 L0 pirn3od.bqlueimsts. A Co cnotnrovlex an odpt Mimoidzealti nogn o apfp Croomachpl texo</p>
<p>[5] R. T. Collins, Y. Liu, and M. Leordeanu. Online selection of discriminative tracking features. PAMI, 27(10): 1631–1643, 2005.</p>
<p>[6] D. Comaniciu, V. Ramesh, and P. Meer. Real-time tracking of nonrigid objects using mean shift. CVPR, 2000.</p>
<p>[7] A. C. Courville, N. D. Daw, G. J. Gordon, and D. S. Touretzky. Model uncertainty in classical conditioning. NIPS, 2003.</p>
<p>[8] M. Godec, P. M. Roth, , and H. Bischof. Hough-based tracking of non-rigid objects. ICCV, 2011.</p>
<p>[9] M. G. H. Grabner and H. Bischof. Real-time tracking via on-line boosting. BMVC, 2006.</p>
<p>[10] B. Han and L. Davis. On-line density-based appearance modeling for object tracking. ICCV, 2005.</p>
<p>[11] S. Hare, A. Saffari, and P. H. S. Torr. Struck: Structured output tracking with kernels. ICCV, 2011. 222333556199  ? ? ? ? ? ? ? ? ? ? ? ? ? (?a )?h?i gh-jumpseq.? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?(b? )?d?ivngseq.? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?(c? )?s?katerseq.? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (?d?) ?w?omanseq.? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?  Figure 6. Tracking results with lower and upper bounds of the likelihood obtained by MUG. Yellow, white, and purple rectangles represent tracking results of MUG, VTS, and MIL, respectively. Yellow and red curves represent lower and upper bounds of the likelihood over time in MUG, respectively. Green curve represents gap between the bounds over time in MUG.</p>
<p>[12] M. Isard and A. Blake. Icondensation: Unifying low-level and highlevel tracking in a stochastic framework. ECCV, 1998.</p>
<p>[13] A. D. Jepson, D. J. Fleet, and T. F. E. Maraghi. Robust online appearance models for visual tracking. PAMI, 25(10): 1296–131 1, 2003.</p>
<p>[14] C. Jia, H. Shenb, and M. Westc. Bounded approximations for marginal likelihoods. Technical report, 2010.</p>
<p>[15] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-detection. PAMI, 34(7): 1409–1422, 2012.</p>
<p>[16] Z. Khan, T. Balch, and F. Dellaert. MCMC-based particle filtering for tracking a variable number of interacting targets. PAMI, 27(1 1):1805–1918, 2005.</p>
<p>[17] J. Kwon and K. M. Lee. Tracking of a non-rigid object via patch-  bmaosendte d caynrloam siacm palpinpgea.r CanVcPeR m, 2o0de0l9i.ng and adaptive basin hopping</p>
<p>[18] J. Kwon and K. M. Lee. Visual tracking decomposition. CVPR, 2010.</p>
<p>[19] J. Kwon and K. M. Lee. Tracking by sampling trackers. ICCV, 2011.</p>
<p>[20] J. Kwon and K. M. Lee. Wang-landau monte carlo-based tracking methods for abrupt motions. PAMI, 35(4): 1011–1024, 2013.</p>
<p>[21] B. Leibe, K. Schindler, N. Cornelis, and L. Van Gool. Coupled object detection and tracking from static cameras and moving vehicles. PAMI, 30(10): 1683–1698, 2008.</p>
<p>[22] F. Liang, C. Liu, and R. J. Carroll. Stochastic approximation in monte carlo computation. J. Amer. Statist., 102(477):305–320, 2007.</p>
<p>[23] L. Matthews, T. Ishikawa, and S. Baker. The template update problem. PAMI, 26(6):810–815, 2004.</p>
<p>[24] X. Mei, H. Ling, Y. Wu, E. Blasch, and L. Bai. Minimum error bounded efficient l1tracker with occlusion detection. CVPR, 2011.</p>
<p>[25] S. M. S. Nejhum, J. Ho, and M.-H. Yang. Visual tracking with histograms and articulating blocks. CVPR, 2008.</p>
<p>[26] S. Oron, A. Bar-Hillel, D. Levi, and S. Avidan. Locally orderless tracking. CVPR, 2012.</p>
<p>[27] D. W. Park, J. Kwon, and K. M. Lee. Robust visual tracking using autoregressive hidden markov model. CVPR, 2012.</p>
<p>[28] P. Perez, C. Hue, J. Vermaak, and M. Gangnet. Color-based probabilistic tracking. ECCV, 2002.</p>
<p>[29] D. A. Ross, J. Lim, R. Lin, and M. Yang. Incremental learning for robust visual tracking. IJCV, 77(1): 125–141, 2008.</p>
<p>[30] J. Santner, C. Leistner, A. Saffari, T. Pock, and H. Bischof. Prost: Parallel robust online simple tracking. CVPR, 2010.</p>
<p>[31] L. Sevilla-Lara and E. Learned-Miller. Distribution fields for tracking. CVPR, 2012.</p>
<p>[32] K. Smith, D. Gatica-Perez, and J.-M. Odobez. Using particles to track varying numbers of interacting people. CVPR, 2005.</p>
<p>[33] S. Stalder, H. Grabner, and L. V. Gool. Cascaded confidence filtering for improved tracking-by-detection. ECCV, 2010.</p>
<p>[34] SIC.C WVa,n 2g0,11 H.. Lu, F. Yang, and M.-H. Yang. Superpixel tracking.</p>
<p>[35] M. Yang and Y. Wu. Tracking non-stationary appearances and dynamic feature selection. CVPR, 2005.</p>
<p>[36] A. Yilmaz, O. Javed, and M. Shah. Object tracking: A survey. ACM  Comput. Surv., 38(4), 2006.</p>
<p>[37] T. Zhang, B. Ghanem, S. Liu, and N. Ahuja. Robust visual tracking via multi-task sparse learning. CVPR, 2012. 222333666200</p>
<br/>
<br/><br/><br/></body>
</html>
