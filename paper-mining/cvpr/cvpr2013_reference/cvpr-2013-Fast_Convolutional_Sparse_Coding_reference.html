<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 cvpr-2013-Fast Convolutional Sparse Coding</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-164" href="../cvpr2013/cvpr-2013-Fast_Convolutional_Sparse_Coding.html">cvpr2013-164</a> <a title="cvpr-2013-164-reference" href="#">cvpr2013-164-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>164 cvpr-2013-Fast Convolutional Sparse Coding</h1>
<br/><p>Source: <a title="cvpr-2013-164-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bristow_Fast_Convolutional_Sparse_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Hilton Bristow, Anders Eriksson, Simon Lucey</p><p>Abstract: Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence.</p><br/>
<h2>reference text</h2><p>[1] A. Beck and M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIIMS, 2009. 1, 2</p>
<p>[2] D. Bertsekas, A. Nedic, and A. Ozdaglar. Con-</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]  vex Analysis and Optimization. Athena Scientific, 2003. 2 S. Boyd. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning, 2010. 3 E. Cand e`s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. Information Theory, 2006. 1 S. S. Chen, D. L. Donoho, and M. a. Saunders. Atomic Decomposition by Basis Pursuit. SIAM Review, 2001. 1 A. Del Bue, J. Xavier, L. Agapito, and M. Paladini. Bilinear Modelling via Augmented Lagrange Multipliers (BALM) . PAMI, Dec. 2011. 3 J. Eggert, H. Wersing, and K. Edgar. Transformation-invariant representation and NMF. Neural Networks, 2004. 2 W. Hashimoto and K. Kurata. Properties of basis functions generated by shift invariant sparse representations of natural images. Biological Cybernetics, 2000. 2 K. Kavukcuoglu, P. Sermanet, Y.-l. Boureau, K. Gregor, M. Mathieu, and Y. LeCun. Learning convolutional feature hierarchies for visual recognition. NIPS, 2010. 2 Y. LeCun and L. Bottou. Gradient-based learning</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]  applied to document recognition. 1998. 2 H. Lee, A. Battle, R. Raina, and A. Ng. Efficient sparse coding algorithms. NIPS, 2007. 1 M. Lewicki and T. Sejnowski. Coding time-varying signals using sparse, shift-invariant representations. NIPS, 1999. 2 J. Mairal, F. Bach, and J. Ponce. Online learning for matrix factorization and sparse coding. JMLR, 2010. 1 M. Morup, M. N. Schmidt, and L. K. Hansen. Shift invariant sparse coding of image and music data. JMLR, 2008. 2 B. Olshausen and D. Field. Emergence of simplecell receptive field properties by learning a sparse code for natural images. Nature, 1996. 2, 7 B. A. Olshausen and D. J. Field. Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1? Science, 1997. 2 A. V. Oppenheim, A. S. Willsky, and with S. Hamid. Signals and Systems (2nd Edition). Prentice Hall, 1996. 2, 6 R. Rockafellar. Monotone operators and the proximal point algorithm. SICON, 1976. 4 R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, 1996. 1 S. Ullman. High-Level Vision: Object Recognition and Visual Cognition. MIT Press, 1996. 2</p>
<p>[21] E. Wachsmuth, M. W. Oram, and D. I. Perrett. Recognition of objects and their component parts: responses of single units in the temporal cortex of the macaque. Cerebral Cortex, 1994. 2</p>
<p>[22] S. Wang and L. Liao. Decomposition Method with a Variable Parameter for a Class of Monotone Variational Inequality Problems. JOTA, 2001. 5</p>
<p>[23] H. Wersing, J. Eggert, and K. Edgar. Sparse coding with invariance constraints. ICANN, 2003. 2</p>
<p>[24] J. Yang, K. Yu, and Y. Gong. Linear spatial pyramid matching using sparse coding for image classification. CVPR, 2009. 1</p>
<p>[25] J. Yang and Y. I. N. Zhang. Alternating direction algorithms for in compressive sensing. Technical report, 2010. 2</p>
<p>[26] M. Zeiler and R. Fergus. Learning Image Decompositions with Hierarchical Sparse Coding. Technical report, 2010. 2</p>
<p>[27] M. Zeiler, D. Krishnan, G. Taylor, and R. Fergus. Deconvolutional networks. CVPR, 2010. 2, 6 333999668</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
