<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-196" href="../cvpr2013/cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">cvpr2013-196</a> <a title="cvpr-2013-196-reference" href="#">cvpr2013-196-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</h1>
<br/><p>Source: <a title="cvpr-2013-196-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oreifej_HON4D_Histogram_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><br/>
<h2>reference text</h2><p>[1] M. Camplani, L. Salgado, and G. Imagenes. Efficient spatiotemporal hole filling strategy for kinect depth maps. SPIE, 2012. 1</p>
<p>[2] H. S. Chen, H. T. Chen, Y. W. Chen, , and S. Y. Lee. Human action recognition using star skeleton. 4th ACM international workshop on Video surveillance and sensor networks, 2006. 2</p>
<p>[3] H. S. M. Coxeter. Regular polytopes. In 3rd. ed., Dover Publications. ISBN 0-486-61480-8, 1973. 2, 4</p>
<p>[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. CVPR, 2005. 2, 4</p>
<p>[5] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie. Behavior recognition via sparse spatio-temporal features. ICCV, 2005. 1, 2, 6</p>
<p>[6] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, D., and Ramanan. Object detection with discriminatively trained part based models. PAMI, 2010. 3</p>
<p>[7] B. Grnbaum, V. Kaibel, V. Klee, and G. M. Ziegler. Convex polytopes (2nd ed.). In New York and London: Springer-Verlag, ISBN 0-387-00424-6, 2003. 4</p>
<p>[8] L. Han, X. Wu, W. Liang, G. Hou, and Y. Jia. Discriminative human action recognition in the learned hierarchical manifold space. Image and Vision Computing, 2010. 2</p>
<p>[9] A. Klaser, M. Marszalek, and C. Schmid. A spatio-temporal descriptor based on 3d-gradients. In BMVC, 2008. 2, 3, 4, 6, 7</p>
<p>[10] I. Laptev. On space-time interst points. In IJCV, 2005. 1, 2, 6</p>
<p>[11] W. Li, Z. Zhang, and Z. Liu. Expandable data-driven graphical modeling of human actions based on salient postures. IEEE Transactions on Circuits and Systems for Video Technology, 2008. 2</p>
<p>[12] W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. In CVPR, 2010. 1, 2, 5, 6</p>
<p>[13] D. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004. 4</p>
<p>[14] F. Lv and R. Nevatia. Recognition and segmentation of 3-d human action using hmm and multi-class adaboost. ECCV, 2006. 2</p>
<p>[15] H. Ning, W. Xu, Y. Gong, and T. Huang. Latent pose estimator for continuous action. ECCV, 2008. 2</p>
<p>[16] G. Ratsch, J. Weston, B. Scholkopf, and K. Mullers. Fisher discriminant analysis with kernels. IEEE Signal Processing, 1999. 2</p>
<p>[17] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift descriptor and its application to action recognition. ACM MM, 2007. 4</p>
<p>[18] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. CVPR, 2011. 1, 2, 8</p>
<p>[19] J. Sung, C. Ponce, B. Selman, and A. Saxena. Human activity detection from rgbd images. In AAAI workshop on PAIR, 2011. 2</p>
<p>[20] S. Tang, X. Wang, T. Han, J. Keller, M. Skubic, S. Lao, and Z. He. Histogram of oriented normal vectors for object recognition with a depth sensor. ACCV, 2012. 2</p>
<p>[21] A. W. Vieira, E. R. Nascimento, G. L. Oliveira, Z. Liu, , and M. F. M.</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]  Campos. Stop: Space-time occupancy patterns for 3d action recognition from depth map sequences. In 17th Iberoamerican Congress on Pattern Recognition (CIARP), 2012. 2, 3, 6, 8 H. Wang, A. KlÂ¨ aser, C. Schmid, and C. Liu. Action recognition by dense trajectories. CVPR, 2011. 2 J. Wang, Z. Liu, J. Chorowski, Z. Chen, , and Y. Wu. Robust 3d action recognition with random occupancy patterns. In ECCV, 2012. 1, 2, 5, 6, 7, 8 J. Wang, Z. Liu, Y. Wu, , and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras. In CVPR, 2012. 1, 2, 3, 5, 6, 7, 8 S. Wu, O. Oreifej, and M. Shah. Action recognition in videos acquired by a moving camera using motion decomposition of lagrangian particle trajectories. ICCV, 2011. 2 X. Yang, C. Zhang, , and Y. Tian. Recognizing actions using depth motion maps-based histograms of oriented gradients. In ACM Multimedia, 2012. 2, 3, 6, 7, 8 7 7 7 2 2 2 31 131</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
