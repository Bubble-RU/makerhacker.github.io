<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-346" href="../cvpr2013/cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">cvpr2013-346</a> <a title="cvpr-2013-346-reference" href="#">cvpr2013-346-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</h1>
<br/><p>Source: <a title="cvpr-2013-346-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ye_Real-Time_No-Reference_Image_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><br/>
<h2>reference text</h2><p>[1] ISRI-OCR evaluation tool UNLV/ISRI. Available Online: http://code.google.com/p/isri-ocr-evaluation-tools/, 2010.</p>
<p>[2] A. Antonacopoulos, C. Clausner, C. Papadopoulos, and S. Pletschacher. Historical document layout analysis competition. In ICDAR, pages 1516–1520, Beijing, China, Sept. 2011.</p>
<p>[3] T. Brand ˜ao and M. P. Queluz. No-reference image quality assessment based on DCT domain statistics. Signal Processing, 88:822–833, Apr. 2008.</p>
<p>[4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011.</p>
<p>[5] I. Guyon, R. M. Haralick, J. J. Hull, and I. T. Phillips. Data sets for OCR and document image understanding research. In In Proceedings of the SPIE - Document Recognition IV, pages 779–799. World Scientific, 1997.</p>
<p>[6] A. K. Jain and K. Karu. Learning texture discrimination masks. IEEE Trans. Pattern Anal. Mach. Intell., 18(2): 195– 205, Feb. 1996.</p>
<p>[7] Z. Jiang, Z. Lin, and L. S. Davis. Learning a discriminative dictionary for sparse coding via label consistent k-svd. In IEEE Conf. on Computer Vision and Pattern Recognition, pages 1697–1704. IEEE, 2011.</p>
<p>[8] J. Kumar, P. Ye, and D. Doermann. DIQA: Document image quality assesment datasets. Available Online: http://lampsrv02.umiacs.umd.edu/projdb/project.php?id=73.</p>
<p>[9] D. Lewis. Building a test collection for complex document information processing. In 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 665–666, 2006.</p>
<p>[10] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. In NIPS, pages 1033–1040, 2008.</p>
<p>[11] P. Marziliano, F. Dufaux, S. Winkler, and T. Ebrahimi. Perceptual blur and ringing metrics: application to JPEG2000. Signal Processing: Image Communication, 19(2): 163–172, 2004.</p>
<p>[12] A. Mittal, A. Moorthy, and A. Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on Image Processing, PP(99): 1, 2012.</p>
<p>[13] A. K. Moorthy and A. C. Bovik. Blind image quality assessment: From natural scene statistics to perceptual quality. IEEE Transactions on Image Processing, 20(12):3350–3364, Dec. 2011.</p>
<p>[14] N. Ponomarenko, V. Lukin, A. Zelensky, K. Egiazarian, M. Carli, and F. Battisti. Tid2008 - a database for evaluation offull-reference visual quality assessment metrics. Advances of Modern Radio Electronics, 10:30–45, 2009.</p>
<p>[15] L. Prechelt. Early stopping - but when? In Neural Networks: Tricks of the Trade, volume 1524 of LNCS, Chapter 2, pages 55–69. Springer-Verlag, 1997.</p>
<p>[16] M. Saad, A. Bovik, and C. Charrier. Blind image quality assessment: A natural scene statistics approach in the DCT domain. IEEE Transactions on Image Processing, 21(8):3339 –3352, Aug. 2012.</p>
<p>[17] H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A statistical evaluation of recent full reference image quality assessment algorithms. IEEE Transactions on Image Processing, 15(1 1):3440–3451, 2006.</p>
<p>[18] H. R. Sheikh, Z. Wang, L. Cormack, and A. C. Bovik. LIVE image quality assessment database release 2. Online, http://live.ece.utexas.edu/research/quality.</p>
<p>[19] H. Tang, N. Joshi, and A. Kapoor. Learning a blind measure of perceptual image quality. In IEEE Conference on Computer Vision and Pattern Recognition, pages 305 – 3 12, 2011.</p>
<p>[20] Z. Wang, A. C. Bovik, and L. Lu. Why is image quality assessment so difficult? In IEEE International Conference on  Acoustics, Speech, and Signal Processing, volume 4, pages IV–3313 –IV–3316, May 2002.</p>
<p>[21] J. Yang, K. Yu, and T. Huang. Supervised translationinvariant sparse coding. In IEEE Conf. on Computer Vision and Pattern Recognition, pages 35 17 –3524, 2010.</p>
<p>[22] P. Ye, J. Kumar, L. Kang, and D. Doermann. Unsupervised Feature Learning Framework for No-reference Image Quality Assessment. In IEEE Conf. on Computer Vision and Pattern Recognition, pages 1098–1 105, 2012. 999999999944222</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
