<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-28" href="../cvpr2013/cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">cvpr2013-28</a> <a title="cvpr-2013-28-reference" href="#">cvpr2013-28-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</h1>
<br/><p>Source: <a title="cvpr-2013-28-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><br/>
<h2>reference text</h2><p>[1] A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. J. Dickinson, S. Fidler, A. Michaux, S. Mussman, S. Narayanaswamy, D. Salvi, L. Schmidt, J. Shangguan, J. M. Siskind, J. W. Waggoner, S. Wang, J. Wei, Y. Yin, and Z. Zhang. Video in sentences out. In UAI, 2012.</p>
<p>[2] A. Belz and E. Reiter. Comparing automatic and human evaluation of nlg systems. In EACL, 2006.</p>
<p>[3] P. Bilinski and F. Bremond. Evaluation of local descriptors for action recognition in videos. In ICCV, 2011.</p>
<p>[4] D. M. Blei and M. I. Jordan. Modeling annotated data. In SIGIR, 2003.</p>
<p>[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 2003.</p>
<p>[6] L. Cao and L. Fei-Fei. Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes. In ICCV, 2007.</p>
<p>[7] P. Das, R. K. Srihari, and J. J. Corso. Translating related words to videos and back through latent topics. In ACM WSDM, 2013.</p>
<p>[8] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV, 2010.</p>
<p>[9] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: generating sentences from images. In ECCV, 2010.</p>
<p>[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. 222666334088  Cleani gan p lianceSp5Kwae .renMsy thwoanei /n oScg rUedblBse afJ:ans-rioHnsremUfto MrpigOAseoiruNfnkramstSo.tr2io/cyO.vrseMo/tBewOJamaBncvJ1elc .mla weAiacnirstnpo/hiVnewrsEgapsRov aBen /r Ogie sfmBria.uJgns ie/pnrSogaUntBogdJreisH-./Nh3.OUtoM UwNAaenNfolc alcdenl/ aOdanB sh/JaVh nEisodRbmhelB e/lnOdb Ble bJrn. hdu4oes. rhWe/O oO Br JmvJa ocn vleuceaunl me/Oian Bgtos/JV ocElRedaBfno wpdaomnoeualtn wo/SfiUthrBefJkr-niHgoeUbrMsa tAonrNd. Human Synopsis: Two standing persons clean a stove top with a vacuum clean with a hose.  TownhalmetingwSKHtoUiewntyMhwebAhonNrcaldemrdsa:finmre/oSmet UinhBtOgeinJ.-bu4Va/Hr.UEcSWkRMy.osAB2m.teNmoaAwnch sp1a/e.NirAOsa/opnUkNesO arisUhtoNsanoplwci/eOlsanBpksmipJnegm aekti/cVorEn gapR.hsBto5m.nasMep/lsOmanBgkrsaJ/oVltiuvagEelpkRrsionBfup gqs/eutVoaeEnsfcRdthioBn tgsipn/eVgpohEe poRlaepB/otOlhpegBlciwvJa rinpetohg/drVebiuafoEmaRsr/mBOdaBilnJt gasprhtoe uwcbphna/ocOhfkBas.tlJ3wnm.odMeminatg n/pgSoe.UpBenJl-s Human Synopsis: A man talks to a mob of sitting persons who clap at the end of his short speech.  RenovatinghomeKOSdHAerBiumenJlmy,tlwaeonta pncplrae dySsanyso:rnafsvrp/VoapetmoEelsypiRwOasle:Bh/ouST,o UbridumBSislefy Jdo.-slHiont3aUerg.psm/MVAien:E vAp1RtiemNir.dB,Ase,ho flno apeompifsr esp/Ou deosB noivJpni,lged,steigmrluoes.owu/aikOnepigBl/nOgJptoB, faJlwr,oec rno ebncdvor aveitrlaedtl iastn/poOgho/TrVoeuHcEnhsEoeR wv.aBi4,t .mhe wamsnoa/rnShkndUoienulBagrs/yJVe-,s.HbE2oURr.uoMBAto,umA cnNrsd,o enuvlractlyahcigyneiu g/ufOgmoVBsrEpJienR,arnsB tdaonlpoi/tnVhsgeEurfRstloBino, glrmsipn.o gt.iwo5ne./r MetalcraftsprojectVSwKHEeu onrmRytkBwesaonih coreoSdpsn. y/4:Onfor.mBMopJemwsta iolnsr:O/ AwkBuosmrhJr kopSmasnya/vOsnias/tBSremsJUioh uBaoJps1l-dp.iHni UAge c/aMeVps eEArtoRNasrfBo mnwbwe nitoshtdar ikwaln.s g5oh/VraV.EkmEAiRnmRmBg e aswr.hntea itmwlh/oOmrpBeklrsJie/ VobrsEnic.Ryca2Blme. /peOMietaB cJlnec /rOhaBfatJmatomaoelwsr/iOnoBgrkJsmhroe dtpas./lO.B3J. Mha mn ber in dgi/nVgERmBetcarl fitn/  Cokingvde:HihROUGEscoreKSHtAbouengtmwye atowreh/mnioSrcan.iUSdelHngaBysJuwnf:dm-hrHopdiabUstemonipMwsce1l/oOSAgyc:NrluenoABdr oJwlipSafoeswdinybpmts2aud:wten /mrOoeSIaUrnsB.:tIirJhn?bt1gliJesmv.h-nHearAvdUyie pobMrac.e/nAOor,lsm.NBoa4Jrn.iewAdasodwf/mVcldrosEae/nmuVRkrsfEaB,inRrsgtBdbholwaesvduniltag/dhsOainruBybt.gJ/hoOpAe(wBsftiahlJengts/rqOknpesuBhdibecJusktin o/evsfmrOl)oiBtuxrnJipm.agi?n tbdhg2iprelt.habds?eitIozn ehtaodsp/aoObdwlsBnardJgle.vrsoBdpeilr/naOsicltBdehlnJasgtbinrhuelagtbdsroie wn/o.Oilts?B3Sl2tJ.hoans?emaldSitqhbueliodnw?b/aOdol.iwBnd5slJg. Cokingvde:LowROUGEscoreKfvSHitasnhryoedgbuvinemroywt/gdpOaen,lt.ioBcearhSn5edJwsy.tobnfi:alhcrmt/oebOpanoswmBi,cJosfri/Ob1lmp:aecusTmanBrtdbh/amJVSleidrbnEypwd.laRstine2ohBgtm.e/OaItvndehB:aiJd1nteshdvo.gi/rsVAdbgeha,EvicpnoRsdem/nBgaOrestodlBma,iJnfrcexgoiaswdtoim/kchVgoesnla/mEoVsnbpRkEaosduinlBaRbtgeBohrldaowisnt l s.ptg/hdaOrloecpgBdafJeirodnwgeiktasnph/boidOngaBrcbeoJadniltw eoianslm.hdtb2seyo.l/HOwAintuBlepJoa.m4nSrgwshTloaesSnmhydsainbowp /lScoawsUmcieltBoasdhJ2kne-i:sHodnUgsm MrwtehicAxdtsNhievnpatigdsehnfo./mnVr2t.EcheowARpie.BnwklTgio.prmhei3da.cnweI sdnt/oaOmwdskBheaiJns various other ingredients from various different bowls. She then mixes all the ingredients with a wooden spoon.  bacon, eggs, cheese onion in different containers. On a pan she cooks the bacon on low flame. Side by side she beats the eggs in a bowl. she removes the cooked bacon on a plate. In the pan she fries onions and then adds the beaten eggs. She sprinkles grated cheese on the pan and cooks well. She then adds the fried bacon on the eggs in the pan and cook well. She transfers the cooked egg with bacon to as serving plate. e,  Figure 6: Qualitative results from MER12 and our “YouCook” dataset. Only the top 5 sentences from our system are shown.</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]  TPAMI, 2010. Y. Feng and M. Lapata. Topic models for image annotation and text illustration. In NAACL HLT, 2010. M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation. In ICCV, 2009. T. M. Hospedales, S. Gong, and T. Xiang. A markov clustering topic model for mining behaviour in video. In ICCV, 2009. M. U. G. Khan, L. Zhang, and Y. Gotoh. Towards coherent natural language description of video streams. In ICCVW, 2011. A. Klaser, M. Marszalek, and C. Schmid. A spatio-temporal descriptor based on 3d-gradients. In BMVC, 2008. G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions. In CVPR, 2011. C.-Y. Lin and E. Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NAACL HLT, 2003. A. Makadia, V. Pavlovic, and S. Kumar. A new baseline for image annotation. In ECCV, 2008. G. Malkarnenkar, N. Krishnamoorthy, S. Guadarrama, K. Saenko, and R. Mooney. Generating natural-language video descriptions using text-mined knowledge. In AAAI, 2013. P. Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, B. Shaw, W. Kraaij, A. F. Smeaton, and G. Qu´ eenot. Trecvid 2012 – an overview of the goals, tasks, data, evaluation mechanisms and metrics. In TRECVID 2012, 2012. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30]</p>
<p>[31]</p>
<p>[32]</p>
<p>[33]</p>
<p>[34]  automatic evaluation of machine translation. In ACL, 2002. D. Putthividhya, H. T. Attias, and S. S. Nagarajan. Topic regression multi-modal latent dirichlet allocation for image annotation. In CVPR, 2010. C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier. Collecting image annotations using amazon’s mechanical turk. In NAACL HLT 2010, 2010. M. Rohrbach, M. Regneri, M. Andriluka, S. Amin, M. Pinkal, and B. Schiele. Script data for attribute-based recognition of composite activities. In ECCV, 2012. S. Sadanand and J. J. Corso. Action bank: A high-level representation of activity in video. In CVPR, 2012. M. A. Sadeghi and A. Farhadi. Recognition using visual phrases. In CVPR, 2011. A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR, 2011. K. E. A. van de Sande, T. Gevers, and C. G. M. Snoek. Evaluating color descriptors for object and scene recognition. TPAMI, 2010. C. Vondrick, D. Patterson, and D. Ramanan. Efficiently scaling up crowdsourced video annotation. IJCV. M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families, and Variational Inference. Now Publishers Inc., 2008. H. M. Wallach, D. Mimno, and A. McCallum. Rethinking LDA: Why priors matter. In NIPS, 2009. C. Wang, D. M. Blei, and F.-F. Li. Simultaneous image classification and annotation. In CVPR, 2009. J. Wanke, A. Ulges, C. H. Lampert, and T. M. Breuel. Topic models for semantics-preserving video compression. In MIR, 2010. Y. Yang, C. L. Teo, H. Daum e´ III, and Y. Aloimonos. Corpus-guided sentence generation of natural images. In EMNLP, 2011. 222666334199</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
