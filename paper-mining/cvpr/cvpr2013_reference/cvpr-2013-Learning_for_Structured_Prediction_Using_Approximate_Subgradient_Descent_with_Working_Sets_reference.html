<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-262" href="../cvpr2013/cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">cvpr2013-262</a> <a title="cvpr-2013-262-reference" href="#">cvpr2013-262-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</h1>
<br/><p>Source: <a title="cvpr-2013-262-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lucchi_Learning_for_Structured_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Aurélien Lucchi, Yunpeng Li, Pascal Fua</p><p>Abstract: We propose a working set based approximate subgradient descent algorithm to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM. We focus on the setting of general graphical models, such as loopy MRFs and CRFs commonly used in image segmentation, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM’s cutting plane algorithm as well as reducing the robustness of existing subgradient based methods. We show that the proposed method obtains better approximate subgradients through the use of working sets, leading to improved convergence properties and increased reliability. Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, which can be used to reduce learning time at only a small cost of performance. We demonstrate the strength of our method empirically on the segmentation of a new publicly available electron microscopy dataset as well as the popular MSRC data set and show state-of-the-art results.</p><br/>
<h2>reference text</h2><p>[1] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Suesstrunk. SLIC Superpixels Compared to State-Of-TheArt Superpixel Methods. PAMI, 2012. 5, 6</p>
<p>[2] J. Besag. Spatial Interaction and the Statistical Analysis of Lattice Systems. J. Royal Stat. Soc., B, 36(2), 1974. 1, 2</p>
<p>[3] Y. Boykov and M. Jolly. Interactive Graph Cuts for Optimal Boundary & Region Segmentation of Objects in N-D Images. In ICCV, pages 105–12, 2001 . 1</p>
<p>[4] M. Everingham, C. W. L. Van Gool and, J. Winn, and A. Zisserman. The Pascal Visual Object Classes Challenge 2010 (VOC2010) Results. 6</p>
<p>[5] T. Finley and T. Joachims. Training Structural SVMs When Exact Inference is Intractable. In ICML, 2008. 2</p>
<p>[6] W. Hoeffding. Probability Inequalities for Sums of Bounded Random Variables. Journal of the American Statistical Association, 58(301): 13–30, March 1963. 4</p>
<p>[7] V. Kolmogorov and R. Zabih. What Energy Functions Can Be Minimized via Graph Cuts? PAMI, 26(2): 147–159, 2004. 5</p>
<p>[8] P. Kr ¨ahenb u¨hl and V. Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In NIPS, 2011. 5</p>
<p>[9] L. Ladicky, C. Russell, P. Kohli, and P. H. S. Torr. Associative Hierarchical CRFs for Object Class Image Segmentation. In ICCV, 2009. 5</p>
<p>[10] L. Ladick` y, P. Sturgess, K. Alahari, C. Russell, and P. Torr. What, Where and How Many? Combining Object Detectors and Crfs. In ECCV, pages 424–437, 2010. 6</p>
<p>[11] J. Lafferty, A. Mccallum, and F. Pereira. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML, 2001. 1, 2</p>
<p>[12] A. Lucchi, Y. Li, K. Smith, and P. Fua. Structured Image Segmentation Using Kernelized Features. In ECCV, October 2012. 6</p>
<p>[13] A. Lucchi, K. Smith, R. Achanta, G. Knott, and P. Fua. Supervoxel-Based Segmentation of Mitochondria in EM Image Stacks with Learned Shape Features. TMI, 3 1(2):474– 486, 2011. 6, 7</p>
<p>[14] T. Malisiewicz and A. Efros. Improving Spatial Support for Object via Multiple Segmentations. In BMVC, 2007. 5</p>
<p>[15] D. Mcallester, T. Hazan, and J. Keshet. Direct Loss Minimization for Structured Prediction. In NIPS, 2010. 1, 2, 3</p>
<p>[16] K. Murphy, Y. Weiss, and M. Jordan. Loopy Belief Propagation for Approximate Inference: An Empirical Study. In UAI, 1999. 1</p>
<p>[17] Y. Nesterov. Primal-Dual Subgradient Methods for Convex Problems. Math. Program., 120(1):221–259, April 2009. 1, 3, 5</p>
<p>[18] J. Platt. Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines. MIT Press, 1998. 2</p>
<p>[19] N. Ratliff, J. A. Bagnell, and M. Zinkevich. (online) Subgradient Methods for Structured Prediction. In AISTATS, 2007. 1, 2, 3, 4, 5, 6, 7</p>
<p>[20] S. M. Robinson. Linear convergence of epsilon-subgradient descent methods for a class of convex functions. Mathematical Programming, 86:41–50, 1999. 4</p>
<p>[21] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal Estimated Sub-Gradient Solver for SVM. Math. Program., 127(1):3–30, March 2011. 2</p>
<p>[22] N. Shor, K. Kiwiel, and A. Ruszcay n`ski. Minimization Methods for Non-Differentiable Functions. Springer-Verlag New York, Inc., 1985. 4</p>
<p>[23] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost for Image Understanding: Multi-Class Object Recognition and Segmentation by Jointly Modeling Texture, Layout, and Context. IJCV, 81(1), January 2009. 5</p>
<p>[24] C. Sutton and A. Mccallum. Introduction to conditional random fields for relational learning. In L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning. MIT Press, 2006. 2</p>
<p>[25] B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning Structured Prediction Models: A Large Margin Approach. In ICML, 2005. 2</p>
<p>[26] B. Taskar, C. Guestrin, and D. Koller. Max-Margin Markov Networks. In NIPS, 2003. 1, 2, 3</p>
<p>[27] B. Taskar, S. Lacoste-julien, and M. Jordan. Structured Prediction, Dual Extragradient and Bregman Projections. JMLR, 7: 1627–1653, 2006. 2</p>
<p>[28] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support Vector Machine Learning for Interdependent and Structured Output Spaces. In ICML, 2004. 1, 2, 3, 5, 6, 7</p>
<p>[29] M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta, and A. Mccallum. Samplerank: Training Factor Graphs with Atomic Gradients. In ICML, 2011. 2, 5, 7</p>
<p>[30] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. JLMR, 11:2543–2596,  Dec. 2010. 1, 3, 5 [3 1] J. Yao, S. Fidler, and R. Urtasun. Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation. In CVPR, pages 702–709, 2012. 6 111999999422</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
