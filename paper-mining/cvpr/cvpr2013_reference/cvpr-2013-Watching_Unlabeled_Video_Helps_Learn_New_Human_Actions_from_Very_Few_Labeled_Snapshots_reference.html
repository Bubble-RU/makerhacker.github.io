<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-459" href="../cvpr2013/cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">cvpr2013-459</a> <a title="cvpr-2013-459-reference" href="#">cvpr2013-459-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</h1>
<br/><p>Source: <a title="cvpr-2013-459-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Watching_Unlabeled_Video_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chao-Yeh Chen, Kristen Grauman</p><p>Abstract: We propose an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process. Using unlabeled video containing various human activities, the system first learns how body pose tends to change locally in time. Then, given a small number of labeled static images, it uses that model to extrapolate beyond the given exemplars and generate “synthetic ” training examples—poses that could link the observed images and/or immediately precede or follow them in time. In this way, we expand the training set without requiring additional manually labeled examples. We explore both example-based and manifold-based methods to implement our idea. Applying our approach to recognize actions in both images and video, we show it enhances a state-of-the-art technique when very few labeled training examples are available.</p><br/>
<h2>reference text</h2><p>[1] J. K. Aggarwal and Q. Cai. Human motion analysis: a review. CVIU, 73(3):428–440, 1999.</p>
<p>[2] Y. Bengio, J. Paiement, P. Vincent, O. Delalleau, N. L. Roux, and M. Ouiment. Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and spectral clustering. In NIPS, 2003.</p>
<p>[3] J. Blackburn and E. Ribeiro. Human motion recognition using isomap and dynamic time warping. In Human Motion, 2007.</p>
<p>[4] L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3d human pose annotations. In ICCV, 2009.</p>
<p>[5] L. Cao, Z. Liu, and T. Huang. Cross-dataset action detection. In CVPR, 2010.</p>
<p>[6] H. Daume III. Frustratingly easy domain adaptation. In ACL, 2007.</p>
<p>[7] V. Delaitre, J. Sivic, and I. Laptev. Learning person-object interactions for action recognition in still images. In NIPS, 2011.</p>
<p>[8] C. Desai, D. Ramanan, and C. Fowlkes. Discriminative models for static human-object interactions. In Wrkshp on Structured models in Computer Vision, 2010.</p>
<p>[9] L. Duan, D. Xu, and S.-F. Chang. Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. In CVPR, 2012.</p>
<p>[10] L. Duan, D. Xu, I. Tsang, and J. Luo. Visual event recognition oin videos by learning from web data. In CVPR, 2010.</p>
<p>[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. In IJCV, volume 88, pages 303–338, June 2010.</p>
<p>[12] K. Grauman, G. Shakhnarovich, and T. Darrell. Inferring 3d structure with a statistical image-based shape model. In ICCV, 2003.</p>
<p>[13] N. Ikizler-Cinbis, R. Cinbis, and S. Sclaroff. Learning actions from the web. In ICCV, 2009.</p>
<p>[14] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for human motion recognition. In ICCV, 2011.</p>
<p>[15] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In CVPR, 2008.</p>
<p>[16] C. Lee and A. Elgammal. Human motion synthesis by motion manifold learning and motion primitive segmentation. In AMDO, 2006.</p>
<p>[17] S. Maji, L. Bourdev, and J. Malik. Action recognition from a distributed representation of pose and appearance. In ICCV, 2011.</p>
<p>[18] P. Matikainen, R. Sukthankar, and M. Hebert. Feature seeding for action recognition. In ICCV, 2011.</p>
<p>[19] D. Moore, I. Essa, and M. Hayes. Exploiting human actions and object context for recognition tasks. In CVPR, 1999.</p>
<p>[20] C. Papageorgiou and T. Poggio. A trainable system for object detec-</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30] [3 1]</p>
<p>[32]</p>
<p>[33]</p>
<p>[34]</p>
<p>[35]</p>
<p>[36]  tion. IJCV, 38(1):15–33, 2000. A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Ferrari. Learning object class detectors from weakly annotated video. In CVPR, 2012. D. Ramanan and D. Forsyth. Automatic annotation of everyday movements. In NIPS, 2003. C. Rao and M. Shah. View-Invariance in Action Recognition. In CVPR, 2001. M. D. Rodriguez, J. Ahmed, and M. Shah. Action MACH a spatiotemporal maximum average correlation height filter for action recognition. In CVPR, 2008. S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. In Science, volume 290, pages 2323–2326, December 2000. G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose estimation with parameter sensitive hashing. In ICCV, 2003. J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from a single depth image. In CVPR, 2011. N. Tang, C. Hsu, T. Lin, and H. Liao. Example-based human motion extrapolation based on manifold learning. In ICME, 2011. G. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invariance through imitation. In CVPR, 2011. C. Vondrick, D. Ramanan, and D. Patterson. Efficiently scaling up video annotation with crowdsourced marketplaces. In ECCV, 2010. H. Wang, M. Ullah, A. Klaser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features for action recognition. In BMVC, 2009. J. Wang, D. Fleet, and A. Hertzmann. Gaussian process dynamical models for human motion. TPAMI, pages 283–298, Feb 2008. W. Yang, Y. Wang, and G. Mori. Recognizing human actions from still images with latent poses. In CVPR, 2010. B. Yao and L. Fei-Fei. Modeling mutual context of object and human pose in human-object interaction activities. In CVPR, 2010. B. Yao and L. Fei-Fei. Action recognition with exemplar based 2.5d graph matching. In ECCV, 2012. B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. J. Guibas, and L. Fei-Fei.  Action recognition by learning bases of action attributes and parts. In ICCV, 2011. Please see http://vision.cs.utexas.edu/projects/action-snapshot/ for dataset annotation and project page. 555777779</p>
<br/>
<br/><br/><br/></body>
</html>
