<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-229" href="../cvpr2013/cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">cvpr2013-229</a> <a title="cvpr-2013-229-reference" href="#">cvpr2013-229-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</h1>
<br/><p>Source: <a title="cvpr-2013-229-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sadovnik_Its_Not_Polite_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><br/>
<h2>reference text</h2><p>[1] A. Berg, T. Berg, H. Daume, J. Dodge, A. Goyal, X. Han, A. Mensch, M. Mitchell, A. Sood, K. Stratos, et al. Understanding and predicting importance in images. In CVPR, 2012.</p>
<p>[2] O. Burdakov, O. Sysoev, A. Grimvall, and M. Hussian. An o (n 2) algorithm for isotonic regression. Large-Scale Nonlinear Optimization, pages 25–33, 2006.</p>
<p>[3] R. Dale. Cooking up referring expressions. In ACL. Association for Computational Linguistics, 1989.</p>
<p>[4] R. Dale and E. Reiter. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263, 1995.</p>
<p>[5] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In CVPR, 2009.</p>
<p>[6] A. Farhadi, S. M. M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. A. Forsyth. Every picture tells a story: Generating sentences from images. In ECCV, 2010.</p>
<p>[7] M. Fernandez and S. Williams. Closed-form expression for the poisson-binomial probability density function. Aerospace and Electronic Systems, IEEE Transactions on, 46(2):803 –817, 2010.</p>
<p>[8] A. Gallagher and T. Chen. Finding rows of people in group images. In ICME, 2009.</p>
<p>[9] A. Gallagher and T. Chen. Understanding images of groups of people. In Proc. CVPR, 2009.</p>
<p>[10] P. Grice. Logic and conversation. Syntax and Semantics, 3:43–58, 1975.</p>
<p>[11] A. Gupta, Y. Verma, and C. Jawahar. Choosing linguistics over vision to describe images. In AAAI, 2012.</p>
<p>[12] H. Horacek. Generating referential descriptions under conditions of uncertainty. In ENLG, 2005.</p>
<p>[13] A. Kovashka, D. Parikh, and K. Grauman. Whittlesearch: Image search with relative attribute feedback. In CVPR, 2012.</p>
<p>[14] E. Krahmer, S. Erk, and A. Verleg. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72, 2003.</p>
<p>[15] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions. In CVPR, 2011.</p>
<p>[16] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Describable visual attributes for face verification and image search. In PAMI, Oct 2011.</p>
<p>[17] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In CVPR, 2009.</p>
<p>[18] C. Mellish, D. Scott, L. Cahill, D. Paiva, R. Evans, and M. Reape. A reference architecture for natural language generation systems. Natural Language Engineering, 12(01): 1–34, 2006.</p>
<p>[19] V. Ordonez, G. Kulkarni, and T. Berg. Im2text: Describing images using 1 million captioned photographs. In NIPS, 2011.</p>
<p>[20] A. Parkash and D. Parikh. Attributes for classifier feedback. In ECCV, 2012.</p>
<p>[21] A. Sadovnik, Y. Chiu, N. Snavely, S. Edelman, and T. Chen. Image description with a goal: Building efficient discriminating expressions for images. In CVPR, 2012.</p>
<p>[22] W. Scheirer, N. Kumar, P. N. Belhumeur, and T. E. Boult. Multiattribute spaces: Calibration for attribute fusion and similarity search. In CVPR, 2012.</p>
<p>[23] B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability estimates. In ACM SIGKDD, 2002. 333000999644</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
