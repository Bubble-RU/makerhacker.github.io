<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-140" href="../cvpr2013/cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">cvpr2013-140</a> <a title="cvpr-2013-140-reference" href="#">cvpr2013-140-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</h1>
<br/><p>Source: <a title="cvpr-2013-140-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Efficient_Color_Boundary_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]  S. K. Shevell and F. A. A. Kingdom, </p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]  images,</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]  Inspired Color Image Descriptor,</p>
<p>[29] E. N. Johnson, M. J. Hawken, and R. Shapley, </p>
<p>[30] R. Shapley and M. Hawken, </p>
<p>[32] D. Martin, C. Fowlkes, D. Tal, and J. Malik, </p>
<p>[33] I. E. Abdou and W. K. Pratt, </p>
<p>[34] Z. M. Shen, W. F. Xu, and C. Y. Li, </p>
<p>[35] J. Rivest and P. Cabanagh, </p>
<p>[36] G. Loffler, </p>
<p>[37] P. Seriès, J. Lorenceau, and Y. Frégnac, </p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
