<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 cvpr-2013-Decoding Children's Social Behavior</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-103" href="../cvpr2013/cvpr-2013-Decoding_Children%27s_Social_Behavior.html">cvpr2013-103</a> <a title="cvpr-2013-103-reference" href="#">cvpr2013-103-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>103 cvpr-2013-Decoding Children's Social Behavior</h1>
<br/><p>Source: <a title="cvpr-2013-103-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Rehg_Decoding_Childrens_Social_2013_CVPR_paper.pdf">pdf</a></p><p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><br/>
<h2>reference text</h2><p>[1] W. Choi, K. Shahid, and S. Savarese. Learning context for collective activity recognition. In CVPR, 2011. 2</p>
<p>[2] F. Eyben, M. Wollmer, and B. Schuller. openSMILE-The munich versatile and fast open-source audio feature extractor. Proc. ACM Multimedia, pages 1459–1462, 2010. 7</p>
<p>[3] A. Fathi, A. Farhadi, and J. M. Rehg. Understanding egocentric activities. In ICCV, 2011. 2</p>
<p>[4] A. Fathi, J. K. Hodgins, and J. M. Rehg. Social interactions: a first-person perspective. In CVPR, 2012. 2</p>
<p>[5] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]  R. Basri. Actions as Space-Time Shapes. IEEE Trans. PAMI, 29(12):2247–53, Dec. 2007. 2 Y. Ke, R. Sukthankar, and M. Hebert. Volumetric features for video event detection. IJCV, 2010. 2 J. Kim, H. Rao, and M. Clements. Investigating the use of formant based features for detection of affective dimensions in speech. In Proc. 4th Intl. Conf. on Affective Computing and Intelligent Interaction, pages 369–377, 2011. 7 A. Kylli ¨ainen and J. K. Hietanen. Skin conductance responses to another person’s gaze in children with autism. Journal of Autism and Developmental Disorders, 36(4):517– 525, May 2006. 2 T. Lan, Y. Wang, W. Yang, and G. Mori. Beyond actions: discriminative models for contextual group activities. In NIPS, 2010. 2 I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning Realistic Human Actions from Movies. In CVPR, 2008. 2 M. J. Marin-Jimenez, A. Zisserman, and V. Ferrari. ”Here’s looking at you, kid.” Detecting people looking at each other in videos. In BMVC, 2011. 5 R. Messing, C. Pal, and H. Kautz. Activity Recognition Using the Velocity Histories of Tracked Keypoints. In ICCV, 2009. 2 V. I. Morariu and L. S. Davis. Multi-agent event recognition in structured scenarios. In CVPR, 2011. 2 O. Y. Ousley, R. Arriaga, G. D. Abowd, and M. Morrier. Rapid assessment of social-communicative abilities in infants at risk for autism. Technical Report CBI-100, Center for Behavior Imaging, Georgia Tech, Jan 2012. Available at www . cbi . gat e ch . edu / t e chreport s . 1, 3</p>
<p>[15] K. Prabhakar and J. M. Rehg. Categorizing turn-taking interactions. In ECCV, Florence, Italy, 2012. 2</p>
<p>[16] M. S. Ryoo and J. K. Aggarwal. Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities. In ICCV, Kyoto, Japan, 2009. 2</p>
<p>[17] B. Schuller, M. Valstar, F. Eyben, G. McKeown, R. Cowie, and M. Pantic. The first international audio/visual emotion challenge. In Proc. 4th Intl. Conf. on Affective Computing and Intelligent Interaction, 2011. 7</p>
<p>[18] D. Tran and A. Sorokin. Human Activity Recognition with Metric Learning. ECCV, pages 548–561, 2008. 2</p>
<p>[19] O. Tuzel, F. Porikli, and P. Meer. Region covariance: A fast descriptor for detection and classification. ECCV, 2006. 6</p>
<p>[20] A. Wetherby, J. Woods, L. Allen, J. Cleary, H. Dickinson, and C. Lord. Early indicators of autism spectrum disorders in the second year oflife. Journal ofAutism andDevelopmental Disorders, 34:473–493, 2004. 1</p>
<p>[21] Z. Ye, Y. Li, A. Fathi, Y. Han, A. Rozga, G. D. Abowd, and J. M. Rehg. Detecting eye contact using wearable eyetracking glasses. In 2nd Workshop on Pervasive Eye Tracking and Mobile Eye-based Interaction (PETMEI), 2012. 5</p>
<p>[22] J. Zhang, L. Lo Presti, and S. Sclaroff. Online multi-person tracking by tracker hierarchy. In Proc. IEEE Conf. on Advanced Video and Signal Based Surveillance, 2012. 6 333444112199</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
