<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-124" href="../cvpr2013/cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">cvpr2013-124</a> <a title="cvpr-2013-124-reference" href="#">cvpr2013-124-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</h1>
<br/><p>Source: <a title="cvpr-2013-124-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hui_Determining_Motion_Directly_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Tak-Wai Hui, Ronald Chung</p><p>Abstract: We address the problem of recovering camera motion from video data, which does not require the establishment of feature correspondences or computation of optical flows but from normal flows directly. We have designed an imaging system that has a wide field of view by fixating a number of cameras together to form an approximate spherical eye. With a substantially widened visual field, we discover that estimating the directions of translation and rotation components of the motion separately are possible and particularly efficient. In addition, the inherent ambiguities between translation and rotation also disappear. Magnitude of rotation is recovered subsequently. Experimental results on synthetic and real image data are provided. The results show that not only the accuracy of motion estimation is comparable to those of the state-of-the-art methods that require explicit feature correspondences or optical flows, but also a faster computation time.</p><br/>
<h2>reference text</h2><p>[1] Y. Aloimonos and Z. Duric. Estimating the heading direction using normal flow. IJCV, 13(1):33–56, 1994.</p>
<p>[2] P. Baker, R. Pless, C. Ferm u¨ller, and Y. Aloimonos. A spherical eye from multiple cameras (makes better models of the world). CVPR, pages 576–583, 2001.</p>
<p>[3] J.-V. Bouguet. http://vision.caltech.edu/bouguetj/calib doc, 2011.</p>
<p>[4] T. Brodsk´ y, C. Ferm u¨ller, and Y. Aloimonos. Directions of motion fields are hardly ever ambiguous. IJCV, 26:5–24, 1998.</p>
<p>[5] T. Brodsk´ y, C. Ferm u¨ller, and Y. Aloimonos. Structure from motion: beyond the epipolar constaint. IJCV, 37(3):23 1 258, 2000.</p>
<p>[6] A. Bruhn and J. Weickert. Lucas/Kanade meets Horn/Schunck: combining local and global optic flow methods. IJCV, 61(3):21 1–231, 2005.</p>
<p>[7] C. Ferm u¨ller and Y. Aloimonos. Direct perception of threedimensional motion through patterns of visual motion. Science, 15: 1973–1976, 1995.</p>
<p>[8] C. Ferm u¨ller and Y. Aloimonos. Qualitative egomotion.</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]  IJCV, 15:7–29, 1995. C. Ferm u¨ller and Y. Aloimonos. On the geometry of visual correspondence. IJCV, 23(3):223–247, 1997. C. Ferm u¨ller and Y. Aloimonos. Ambiguity in structure from motion: Sphere versus plane. IJCV, 28(2): 137–154, 1998. C. Ferm u¨ller and Y. Aloimonos. Observability of 3D motion. IJCV, 37(1):43–63, 2000. R. H. Hardin, N. J. A. Sloane, and W. D. Smith. Tables of spherical codes with icosahedral symmetry. http://research.att.com/∼njas/icosahedral.codes, 2011. R. Hartley and A. Zisserman. Multiple view geometry in computer vision. Cambridge University Press, 2003. D. J. Heeger and A. D. Jepson. Subspace methods for recovering rigid motion I: algorithm and implementation. IJCV, 7(2):95–1 17, 1992. B. Horn and B. Shunck. Determining optical flow. AI, 17: 175–203, 1981. B. Horn and E. Weldon. Direct methods for recovering motion. IJCV, 2:51–76, 1988. C. Hu and L. Cheong. Linear quasi-parallax SfM using laterally-placed eyes. IJCV, 84:21–39, 2009. T.-W. Hui and R. Chung. Determining spatial motion directly from normal flow: A comprehensive treatment. ACCV 2010 Workshops, pages 23–32, 2011. J. Kim, M. Hwangbo, and T. Kanade. Spherical approximation for multiple cameras in motion estimation:Its applicability and advantages. CVIU, 114(10):2068–2083, 2010. J. Kim, H. Li, and R. I. Hartley. Motion estimation for nonoverlapping multicamera rigs: Linear algebraic and L∞ geometric spoilnugtio mnus.l iTcaPAmMeraI, 32(6): 1i0ne44ar– a10lg5e9b, a2i0c10 an. L. Kneip, R. Siegwart, and M. Pollefeys. Finding the exact</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30]  [3 1]</p>
<p>[32]</p>
<p>[33]</p>
<p>[34]  rotation between two images independently of the translation. ECCV, pages 696–709, 2012. P. D. Kovesi. http://csse.uwa.edu.au/∼pk/research/matlabfns/, P20.D12..K H. Li and R. Hartley. Five-point motion estimation made easy. ICPR, pages 630–633, 2006. J. Lim and N. Barnes. Directions of egomotion from antipodal points. CVPR, pages 1–8, 2008. J. Lim and N. Barnes. Estimation of the epipole using optical flow at antipodal points. CVIU, 114:245–253, 2010. M. I. A. Lourakis. Egomotion estimation using quadruples of collinear image points. ECCV, pages 834–848, 2000. D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–1 10, 2004. A. Makadia, C. Geyer, and K. Daniilidis. Correspondencefree structure from motion. IJCV, 73(3):3 11–327, 2007. S. Mehta and R. Etienne-Cummings. A simplified normal optical flow measurement CMOS camera. Trans. on Circuits and Systems I, 53(6): 1223–1234, 2006. R. C. Nelson and J. Aloimonos. Finding motion parameters from spherical motion fields (or the advantages of having eyes in the back of your head). Biological Cybernetics, 58:261–273, 1988. D. Nister. An efficient solution to the five-point relative pose problem. TPAMI, 26(6):756–770, 2004. R. Pless. Using many cameras as one. CVPR, pages 578– 593, 2003. F. Raudies and H. Neumann. An efficient linear method for the estimation of ego-motion from optical flow. DAGM Sym. on PR, pages 11–20, 2009. C. Silva and J. Santos-Victor. Robust egomotion estiamtion from the normal flow using search subspaces. TPAMI,  19(9): 1026–1034, 1997.</p>
<p>[35] M. V. Srinivasan and S. W. Zhang. Visual motor computations in insects. Annual review of neuroscience, 27:679–696, 2004.</p>
<p>[36] D. Sun, S. Roth, and M. J. Black. Secrets of optical flows estimation and their principles. CVPR, pages 2432–2439, 2010. 222222777422</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
