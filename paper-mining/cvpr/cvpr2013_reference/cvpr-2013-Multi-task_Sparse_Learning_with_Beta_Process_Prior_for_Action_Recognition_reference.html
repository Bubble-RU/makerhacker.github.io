<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-302" href="../cvpr2013/cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">cvpr2013-302</a> <a title="cvpr-2013-302-reference" href="#">cvpr2013-302-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-302-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yuan_Multi-task_Sparse_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chunfeng Yuan, Weiming Hu, Guodong Tian, Shuang Yang, Haoran Wang</p><p>Abstract: In this paper, we formulate human action recognition as a novel Multi-Task Sparse Learning(MTSL) framework which aims to construct a test sample with multiple features from as few bases as possible. Learning the sparse representation under each feature modality is considered as a single task in MTSL. Since the tasks are generated from multiple features associated with the same visual input, they are not independent but inter-related. We introduce a Beta process(BP) prior to the hierarchical MTSL model, which efficiently learns a compact dictionary and infers the sparse structure shared across all the tasks. The MTSL model enforces the robustness in coefficient estimation compared with performing each task independently. Besides, the sparseness is achieved via the Beta process formulation rather than the computationally expensive l1 norm penalty. In terms of non-informative gamma hyper-priors, the sparsity level is totally decided by the data. Finally, the learning problem is solved by Gibbs sampling inference which estimates the full posterior on the model parameters. Experimental results on the KTH and UCF sports datasets demonstrate the effectiveness of the proposed MTSL approach for action recognition.</p><br/>
<h2>reference text</h2><p>[1] R. Poppe. A survey on vision-based human action recognition. Image and Vision Computing, Vol.28, No.6, pp.976-990, 2010.</p>
<p>[2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, Vol.73, No.3, pp.243-272, 2008.</p>
<p>[3] A. Quattoni, X. Carreras, M. Collins, and T. Darrell. An efficient projection for l 1,infinity regularization. In ICML, pp.857C864, 2009.</p>
<p>[4] T. Zhang, B. Ghanem, S. Liu, N. Ahuja. Robust Visual Tracking via Multi-Task Sparse Learning. CVPR, 2012.</p>
<p>[5] L. Zhang, P. Zhu, Q. Hu, and D. Zhang. A Linear Subspace Learning Approach via Sparse Coding. ICCV, 2011.</p>
<p>[6] T. Guha, and R.K. Ward. Learning Sparse Representations for Human Action Recognition. PAMI, Vol.34, No.8, pp.1576-1588, 2012.</p>
<p>[7] K. Guo, P. Ishwar, and J. Konrad. Action Recognition in Video by Sparse Representation on Covariance Manifolds of Silhouette Tunnels. ICPR Contests, pp.294-305, 2010.</p>
<p>[8] M. Zhou, H. Chen, J. Paisley, L. Ren, G. Sapiro, and L. Carin. Nonparametric Bayesian Dictionary Learning for Sparse Image Representations. In NIPS, 2009.</p>
<p>[9] S. Ji, D. Dunson, and L. Carin. Multi-Task Compressive Sensing. IEEE Trans. Signal Processing, Vol. 57, No. 1, pp. 92-106, 2009.</p>
<p>[10] X. Yuan, and S. Yan. Visual Classification with Multi-Task Joint Sparse Representation. In CVPR, 2010.</p>
<p>[11] X. Mei and H. Ling. Robust Visual Tracking and Vehicle Classification via Sparse Representation. PAMI, Vol.33, No. 11, pp.22592272, 2011.</p>
<p>[12] K.N. Tran, I.A. Kakadiaris, and S.K. Shah. Part-based motion descriptor image for human action recognition. Pattern Recognition, Vol.45, pp.2562-2572, 2012.</p>
<p>[13] I. Laptev. On space-time interest points. IJCV, Vol.64, No.2, pp.107123, 2005.</p>
<p>[14] P. Scovanner, S. Ali, M. Shah, ”A 3-dimensional sift descriptor and its application to action recognition,” In Proc. ACM Multimedia, pp. 357-360, 2007.</p>
<p>[15] Q. Qiu, Z. Jiang, and R. Chellappa. Sparse Dictionary-based Representation and Recognition of Action Attributes. In ICCV, 2011.</p>
<p>[16] H. Wang, A. Kl¨ aser, I. Laptev, C. Schmid, C. L. Liu. Action Recognition by Dense Trajectories. In CVPR, pp. 3 169-3 176, 2011.</p>
<p>[17] C. Schuldt, I. Laptev, and B. Caputo. Recognizing Human Actions: A Local SVM Approach. In ICPR, pp.32-36, 2004.</p>
<p>[18] M. D. Rodriguez, J. Ahmed, and M. Shah. Action MACH: a spatiotemporal maximum average correlation height filter for action recognition. In CVPR, 2008.</p>
<p>[19] M. Raptis, I. Kokkinos, and S. Soatto. Discovering Discriminative Action Parts from Mid-Level Video Representations. In CVPR, 2012.</p>
<p>[20] S. OHara, and B. A. Draper. Scalable Action Recognition with a Subspace Forest. In CVPR, 2012.</p>
<p>[21] S. Wang, Y. Yang, Z. Ma, X. Li, C. Pang, and A. G. Hauptmann. Action Recognition by Exploring Data Distribution and Feature Correlation. In CVPR, 2012.</p>
<p>[22] L. Yeffet, and L.Wolf. Local trinary patterns for human action recognition. In ICCV, 2009.</p>
<p>[23] H. Wang, M. M. Ullah, A. Kl¨ aser, I. Laptev, C. Schmid. Evaluation of local spatio-temporal features for action recognition. In BMVC, 2009.</p>
<p>[24] A. Kovashka, and K. Grauman. Learning a Hierarchy ofDiscriminative Space-Time Neighborhood Features for Human Action Recognition. In CVPR, 2010.</p>
<p>[25] Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In CVPR, pp. 3361-3368, 2011. 444222779</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
