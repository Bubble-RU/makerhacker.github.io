<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-245" href="../cvpr2013/cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">cvpr2013-245</a> <a title="cvpr-2013-245-reference" href="#">cvpr2013-245-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</h1>
<br/><p>Source: <a title="cvpr-2013-245-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shen_Layer_Depth_Denoising_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><br/>
<h2>reference text</h2><p>[1] H. S. Bhat and N. Kumar. On the derivation of the bayesian information criterion. Technical report, University of California, Merced, 2010.</p>
<p>[2] M. Camplani and L. Salgado. Efficient spatio-temporal hole filling strategy for kinect depth maps. Three-Dimensional Image Processing (3DIP) and Applications, 2012.</p>
<p>[3] J. Cho, S. Kim, Y. Ho, and K. Lee. Dynamic 3d human actor generation method using a time-of-flight depth camera. In IEEE Transactions on Consumer Electronics, volume 54, pages 1514–1521, 2008.</p>
<p>[4] J. Diebel and S. Thrun. An application of markov random fields to range sensing. In Advances in neural information processing systems, volume 18, pages 291–295, 2006.</p>
<p>[5] B. Freedman, A. Shpunt, M. Machline, and Y. Arieli. Depth mapping using projected patterns. Technical report, U.S. Patent No. 201001 18123, 2010.</p>
<p>[6] W. Freeman, E. Pasztor, and O. Carmichael. Learning lowlevel vision. In International journal of computer vision (IJCV), volume 40, pages 25–47. Springer, 2000.</p>
<p>[7] V. Garro, C. dal Mutto, P. Zanuttigh, and G. Cortelazzo. A novel interpolation scheme for range data with side information. In Conference for Visual Media Production (CVMP), 2009.</p>
<p>[8] K. Khoshelham. Accuracy analysis of kinect depth data. In ISPRS Workshop Laser Scanning, 2011.</p>
<p>[9] J. Kopf, M. F. Cohen, D. Lischinski, and M. Uyttendaele. Joint bilateral upsampling. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2007), 26(3), 2007.</p>
<p>[10] R. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim,</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]  A. Davison, P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. In IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2011. J. Park, H. Kim, Y. Tai, M. Brown, and I. Kweon. High quality depth map upsampling for 3d-tof cameras. In International Conference on Computer Vision (ICCV), 2011. PrimeSense Inc. Prime Sensor NITE 1.3 Algorithms notes, 2010. C. Rother, A. Blake, and V. Kolmogorov. Grabcut - interactive foreground extraction using iterated graph cuts. In In Proceedings of ACM SIGGRAPH, 2004. J. Shen, C. S. C., and J. Zhao. Virtual mirror by fusing multiple rgb-d cameras. Asia-Pacific Signal Information Processing Association Annual Summit and Conference (APSIPA), 2012. L. Torres-M e´ndez and G. Dudek. Reconstruction of 3d models from intensity images and partial depth. In Proceedings of the National Conference on Artificial Intelligence (AAAI), 2004. L. Wang, H. Jin, R. Yang, and M. Gong. Stereoscopic inpainting: Joint color and depth completion from stereo images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008. Q. Yang, R. Yang, J. Davis, and D. Nist e´r. Spatial-depth super resolution for range images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2007. 1 1 1 111119999933111</p>
<p>[2], results from Diebel. et al. [4], and our results. 1 1 1 1 1 19 9 94 2 2</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
