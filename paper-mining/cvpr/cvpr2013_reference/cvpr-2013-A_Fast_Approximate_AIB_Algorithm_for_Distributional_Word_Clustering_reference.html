<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-8" href="../cvpr2013/cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">cvpr2013-8</a> <a title="cvpr-2013-8-reference" href="#">cvpr2013-8-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</h1>
<br/><p>Source: <a title="cvpr-2013-8-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_A_Fast_Approximate_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><br/>
<h2>reference text</h2><p>[1] L. D. Baker and A. K. McCallum. Distributional clustering of words for text classification. SIGIR, pages 96–103, New York, NY, USA, 1998. ACM. 1</p>
<p>[2] R. Bekkerman, R. El-Yaniv, Y. Winter, and N. Tishby. On feature distributional clustering for text categorization. In SIGIR, pages 146–153, 2001. 1</p>
<p>[3] P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. Class-based n-gram models of natural language. Comput. Linguist., 18(4):467–479, Dec. 1992. 1</p>
<p>[4] I. S. Dhillon, S. Mallela, and R. Kumar. A divisive information-theoretic feature clustering algorithm for text classification. JMLR, 3: 1265–1287, 2003. 1 555666002  s0arEiCclonRt0eaf.180675 9F0Aa−stBI8A0svThe7ragn0umbe6or0factlu5senCradtl4wh0oc1s320n soicePyaruAm9. 08 9.1765432809FAa−stBI80sTh7Aevnruamgb60AoPcfnul5Cs0etardc4hw1o0s32 0isrcaneotlC0ERrif. 0 645 20398145FAa0−stBI30AveTrahgn25um0obreat2cf0ulnsPAerS1Cd5w0Lo−VsO1C075 eyunosicrPaAem03. 24863.05 ATvehran2gu50mAbPeor2nf0cAulSsetCr15Ld0−wVOoCs1705FAa−stBI 0 otaCir0ceslniEo0rRf. 056481205FAa−s tBIA−sveTragh10n4urmobeatfclnPusAeSrCdLw10o−3rVsOC7102icoeryauAnmP05.43 105AveTrhagn4umAbPeornfculAsSetCrdLw−1Vo0Or3Cs7FAa− tBIA1s02  aE0RrsliCta0noricfes0.1 01296578FaA−s6 tBIA−svTe5rahg numrobe4atfocnul2se03Nrdwosgr2up1x04noicePyaruAnemc0 s8. 9.876543FaA6−s tBIA−svT5hernagumbe4ArPocfnul2s0etN3rdwsgo u2p1x04  Figure 4. Comparison of classification error rate (left), mean Average Precision (right) on four datasets at each row: Caltech101, VOC07(4000 words), VOC07(32, 000 words), 20Newsgroups.</p>
<p>[5] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. 6</p>
<p>[6] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. CVIU, 106(1):59–70, 2007. 6</p>
<p>[7] B. Fulkerson, A. Vedaldi, and S. Soatto. Localizing objects with smart dictionaries. In ECCV, pages 179–192, Berlin, Heidelberg, 2008. Springer-Verlag. 1, 2, 6, 7</p>
<p>[8] K. Lang. Newsweeder: Learning to filter netnews. In ICML, pages 331–339, 1995. 6</p>
<p>[9] S. Lazebnik and M. Raginsky. Supervised learning of quantizer codebooks by information loss minimization. IEEE TPAMI, 31: 1294–1309, 2009. 1</p>
<p>[10] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. CVPR, pages 2169–2178, 2006. 6</p>
<p>[11] J. Liu and M. Shah. Scene modeling using co-clustering. In  ICCV, pages 1–7, 2007. 1</p>
<p>[12] J. Liu and M. Shah. Learning human actions via information maximization. CVPR, 0: 1–8, 2008. 1  osg)il0muIhcAt−olinufaM(rm−ev1 4602 8AvF aes−rgAtI8B0−acsumThleitnv6m0MbeIrolscfunetCra4l0dwcho1rs(Tai2n0gdt)− mr(M01fohtiaglnsIevtaumcA0−12345. 1450AF a−sv te8IBAr0−gesacTuhmnl6it0vbeMroIcflsutnCe4r0adlwcoh10s(T2edta)0 )−nvlaotcs(fMi0eumA−hg1Isrtuo−410286 4Avea3rF50g−seAtaBIc−u3sm0alTthvienM2u50Imolbser2fnc0luPsAeStCr1d50−wLVoOrsC107(Tani5gdt)0−4Momrli1fhtagn(slIevumcA012345 vFe0Aa−srgteBIAa−c30sumTalhtie2vn5Mu0mIbolesr2cf0nPulsAetSr15Cd0wL−oVrOs1C07(Te5dta)0 Itl−cuo(f1ar0ihMmvslec)−Agntos1− 804261 5AverFag−sAtc1BI04−umsalTihtvenMuIm1l0b3oesr fncPulAetS1Cr02Ld−woVOCs07(T1ranigdt)0−gneao)1fmhirl(soIMvtumcA210345vFeAar−s tge1BIAa0−4csumThaleitnv1M0b3IeolrscfunPetAr1S0Cd2wLo−rVsOC017(Tetda)0  cvaten0uiolr−g1smMulIA(−f)mhoti13205 7AverF a−6sgAteIBc−sum5alThtveinuMmIb4olrscfnu20et3Nrdwsgou2p(Trani1gdtx)04iaumcA− 1osvli)0fmhtragn(solIMe10234567AFvae6−srAtgBIe−ascuT5mhelnitvMb4Irosfculn2et30rNdwosgr2up(Tesdt1a)x04  Figure 5. Comparison of mutual information loss on training (left) and test (right) data on four datasets at each row: Caltech101, VOC07(4000 words), VOC07(32, 000 words), 20Newsgroups.</p>
<p>[13] L. Liu, L. Wang, and X. Liu. In defense of soft-assignment coding. In ICCV, pages 2486–2493, 2011. 6</p>
<p>[14] L. Liu, L. Wang, and C. Shen. A generalized probabilistic framework for compact codebook creation. In CVPR, pages 1537–1544, 2011. 1</p>
<p>[15] D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–1 10, 2004. 6</p>
<p>[16] F. Pereira, N. Tishby, and L. Lee. Distributional clustering of english words. ACL, pages 183–190, Stroudsburg, PA, USA, 1993. Association for Computational Linguistics. 1</p>
<p>[17] N. Slonim and N. Tishby. Agglomerative information bottleneck. In NIPS, pages 617–623, 1999. 1, 2</p>
<p>[18] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms. Software available at http://www.vlfeat.org/, 2008. 6</p>
<p>[19] L. Wang, L. Zhou, and C. Shen. A fast algorithm for creating a compact and discriminative visual codebook. In ECCV, pages 719–732, 2008. 1</p>
<p>[20] J. Winn, A. Criminisi, and T. Minka. Object categorization by learned universal visual dictionary. In ICCV, pages 1800–  1807, 2005. 1 555666113</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
