<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-320" href="../cvpr2013/cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">cvpr2013-320</a> <a title="cvpr-2013-320-reference" href="#">cvpr2013-320-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</h1>
<br/><p>Source: <a title="cvpr-2013-320-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wohlhart_Optimizing_1-Nearest_Prototype_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Paul Wohlhart, Martin Köstinger, Michael Donoser, Peter M. Roth, Horst Bischof</p><p>Abstract: The development of complex, powerful classifiers and their constant improvement have contributed much to the progress in many fields of computer vision. However, the trend towards large scale datasets revived the interest in simpler classifiers to reduce runtime. Simple nearest neighbor classifiers have several beneficial properties, such as low complexity and inherent multi-class handling, however, they have a runtime linear in the size of the database. Recent related work represents data samples by assigning them to a set of prototypes that partition the input feature space and afterwards applies linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper, we go a step beyond these approaches and purely focus on 1-nearest prototype classification, where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable, parameter free, avoids noise overfitting and, since during testing only comparisons to the derived prototypes are required, highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods, while even getting close to the results of more complex classifiers.</p><br/>
<h2>reference text</h2><p>[1] O. Boiman, E. Shechtman, and M. Irani. In defense of Nearest-Neighbor based image classification. In Proc. CVPR, 2008.</p>
<p>[2] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Trans. on Intelligent Systems and Technology, 2:27: 1–27:27, 2011.</p>
<p>[3] K. Crammer, R. Gilad-bachrach, A. Navot, and N. Tishby. Margin analysis of the LVQ algorithm. In Advances NIPS, 2002.</p>
<p>[4] D. M. Dunlavy, T. G. Kolda, and E. Acar. Poblano v1.0: A matlab toolbox for gradient-based optimization. Technical Report SAND2010-1422, Sandia National Laboratories, Albuquerque, NM and Livermore, CA, Mar. 2010.</p>
<p>[5] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. JMLR, 9: 1871–1874, 2008.</p>
<p>[6] P. W. Frey and D. J. Slate. Letter recognition using hollandstyle adaptive classifiers. Machine Learning, 6(2): 161–182, 1991.</p>
<p>[7] J. Hull. A database for handwritten text recognition research. IEEE Trans. on PAMI, 16(5):550–554, May 1994.</p>
<p>[8] T. Kohonen. Self-organization and associative memory. Springer-Verlag, New York, 1989.</p>
<p>[9] L. Ladicky and P. H. S. Torr. Locally linear support vector machines. In Proc. ICML, 2011.</p>
<p>[10] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(1 1):2278–2324, Nov 1998.</p>
<p>[11] S. Maji, A. C. Berg, and J. Malik. Classification using intersection kernel support vector machines is efficient. In Proc. CVPR, 2008.</p>
<p>[12] D. Ramanan and S. Baker. Local distance functions: A taxonomy, new algorithms, and an evaluation. IEEE Trans. on PAMI, 33:794–806, 2011.</p>
<p>[13] S. Seo and K. Obermayer. Soft learning vector quantization. Neural Computation, 2002.</p>
<p>[14] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In Proc. ICML, 2007.</p>
<p>[15] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. In Proc. CVPR, 2011.</p>
<p>[16] A. Vedaldi and A. Zisserman. Efficient additive kernels via explicit feature maps. IEEE Trans. on PAMI, 34(3), 2011.</p>
<p>[17] K. Weinberger and L. Saul. Distance metric learning for large margin nearest neighbor classification. JMLR, 10:207– 244, 2009.</p>
<p>[18] Z. Zhang, P. Sturgess, S. Sengupta, N. Crook, and P. H. S. Torr. Efficient discriminative learning of parametric nearest neighbor classifiers. In Proc. CVPR, 2012.</p>
<p>[19] X. Zhu, C. Vondrick, D. Ramanan, and C. C. Fowlkes. Do we need more training data or better models for object detection? In Proc. BMVC, 2012. 444666557</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
