<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-15" href="../cvpr2013/cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">cvpr2013-15</a> <a title="cvpr-2013-15-reference" href="#">cvpr2013-15-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</h1>
<br/><p>Source: <a title="cvpr-2013-15-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Welinder_A_Lazy_Mans_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Peter Welinder, Max Welling, Pietro Perona</p><p>Abstract: How many labeled examples are needed to estimate a classifier’s performance on a new dataset? We study the case where data is plentiful, but labels are expensive. We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier’s confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by reestimating the class-conditional confidence distributions.</p><br/>
<h2>reference text</h2><p>[1] P. N. Bennett. Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods. Technical report, Carnegie Mellon University, 2002. 3, 7</p>
<p>[2] P. N. Bennett and V. R. Carvalho. Online stratified sampling: evalutating classifiers at web-scale. In CIKM, 2010. 7</p>
<p>[3] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In ICML, 2009. 8</p>
<p>[4] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific and Statistical Computing, 16(5): 1190–1208, 1995. 5</p>
<p>[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In ICCV, 2005. 1, 2, 6</p>
<p>[6] S. Dasgupta and D. Hsu. Hierarchical sampling for active learning. In ICML, 2008. 8</p>
<p>[7] P. Doll a´r, C. Wojek, B. Schiele, and P. Perona. Pedestrian detection: An evaluation of the state of the art. PAMI, 99, 2011. 1, 2, 6</p>
<p>[8] G. Druck and A. McCallum. Toward interactive train-</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]  ing and evaluation. In CIKM, 2011. 7 A. Erkanli, M. Sung, E. J. Costello, and A. Angold. Bayesian semi-parametric ROC analysis. Statist. Med., 25:3905–3928, 2006. 3, 7 A. Frank and A. Asuncion. UCI machine learning repository, 2010. 6 J. Gu, S. Ghosal, and A. Roy. Bayesian bootstrap estimation of ROC curve. Statist. Med., 27:5407–5420, 2008. 3, 7 M. Hellmich, K. R. Abrams, and A. J. Sutton. Bayesian Approaches to Meta-analysis of ROC Curves. Med. Decis. Making, 19:252–264, 1999. 3, 7 A. Khosla, T. Zhou, T. Malisiewicz, A. Efros, and A. Torralba. Undoing the damage of dataset bias. In ECCV, 2012. 1 S. N. MacEachern. Estimating normal means with a conjugate style dirichlet process prior. Communications in Statistics B, 23(3):727–741, 1994. 5 R. M. Neal. MCMC using Hamiltonian dynamics. In S. Brooks, A. Gelman, G. L. Jones, , and X.-L. Meng, editors, Handbook of Markov Chain Monte Carlo, pages 113–162. Chapman & Hall / CRC Press, 2010. 5 K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning, 39(2/3): 103– 134, 2000. 2 J. C. Platt. Probabilistic outputs for support vector</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]  machines and comparisons to regularized likelihood methods. In A. J. Smola, P. Bartlett, B. Sch o¨lkopf, and D. Schuurmans, editors, Advances in Large Margin Classiers, pages 61–74. MIT Press, 1999. 7 C. Sawade, N. Landwehr, S. Bickel, and T. Scheffer. Active risk estimation. In ICML, 2010. 7, 8 C. Sawade, N. Landwehr, and T. Scheffer. Active Estimation of F-Measures. In NIPS, 2010. 7, 8 M. Seeger. Learning with labeled and unlabeled data. Technical report, University of Edinburgh, 2001. 2 X. Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin–Madison, 2005. 2 333222666977</p>
<br/>
<br/><br/><br/></body>
</html>
