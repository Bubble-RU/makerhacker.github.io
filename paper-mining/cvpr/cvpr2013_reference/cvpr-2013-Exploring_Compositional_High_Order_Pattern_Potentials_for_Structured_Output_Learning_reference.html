<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-156" href="../cvpr2013/cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">cvpr2013-156</a> <a title="cvpr-2013-156-reference" href="#">cvpr2013-156-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</h1>
<br/><p>Source: <a title="cvpr-2013-156-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Exploring_Compositional_High_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yujia Li, Daniel Tarlow, Richard Zemel</p><p>Abstract: When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs); we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss;and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.</p><br/>
<h2>reference text</h2><p>[1] Y. Boykov and M. Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in nd images. In ICCV, 2001. 2, 6</p>
<p>[2] S. M. A. Eslami, N. Heess, and J. Winn. The shape Boltzmann machine: a strong model of object shape. In CVPR, 2012. 3</p>
<p>[3] S. M. A. Eslami and C. Williams. A generative model for parts-based object segmentation. In NIPS, pages 100–107, 2012. 3</p>
<p>[4] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 2010. 2, 6</p>
<p>[5] V. Gulshan, C. Rother, A. Criminisi, A. Blake, and A. Zisserman. Geodesic star convexity for interactive image segmentation. In CVPR, 2010. 3</p>
<p>[6] T. Hazan and R. Urtasun. A primal-dual message-passing algorithm for approximated large scale structured prediction. In NIPS, 2010. 4</p>
<p>[7] X. He, R. Zemel, and M. Carreira-Perpinan. Multiscale conditional random fields for image labelling. In CVPR, 2004. 3</p>
<p>[8] G. Hinton. Training products of experts by minimizing contrastive</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]  divergence. Neural Computation, 2002. 3, 5 J. Kivinen and C. Williams. Multiple texture Boltzmann machines. In AISTATS, volume 22, 2012. 2 P. Kohli, L. Ladick` y, and P. Torr. Robust higher order potentials for enforcing label consistency. IJCV, 82(3), 2009. 1, 2 N. Komodakis. Efficient training for pairwise or higher order CRFs via dual decomposition. In CVPR, 2011. 3 N. Komodakis and N. Paragios. Beyond pairwise energies: Efficient optimization for higher-order MRFs. In CVPR, 2009. 2 P. Kr ¨ahenb u¨hl and V. Koltun. Efficient inference in fully connected CRFs with Gaussian edge potentials. In NIPS, 2012. 1 L. Ladicky, C. Russell, P. Kohli, and P. Torr. Inference methods for CRFs with co-occurrence statistics. IJCV, 2011. 2, 3 H. Larochelle and Y. Bengio. Classification using discriminative restricted Boltzmann machines. In ICML, 2008. 3 H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009. 4 V. Lempitsky, P. Kohli, C. Rother, and T. Sharp. Image segmentation with a bounding box prior. In ICCV, 2009. 3 N. LeRoux, N. Heess, J. Shotton, and J. Winn. Learning a generative model of images by factoring appearance and shape. Neural Computation, 2011. 3 M. Maire, P. Arbel ´aez, C. Fowlkes, and J. Malik. Using contours to detect and localize junctions in natural images. In CVPR, 2008. 2, 6, 7</p>
<p>[20] K. Miller, M. Kumar, B. Packer, D. Goodman, and D. Koller. Maxmargin min-entropy models. In AISTATS, 2012. 2</p>
<p>[21] V. Mnih, H. Larochelle, and G. Hinton. Conditional restricted Boltzmann machines for structured output prediction. In UAI, 2011. 3</p>
<p>[22] M. Norouzi, M. Ranjbar, and G. Mori. Stacks of convolutional restricted Boltzmann machines for shift-invariant feature learning. In CVPR, 2009. 4</p>
<p>[23] S. Nowozin and C. Lampert. Global connectivity potentials for random field models. In CVPR, 2009. 3</p>
<p>[24] A. Quattoni, S. Wang, L. Morency, M. Collins, and T. Darrell. Hid-</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30]</p>
<p>[31]</p>
<p>[32]</p>
<p>[33]</p>
<p>[34]</p>
<p>[35]  den conditional random fields. PAMI, 2007. 2 M. R. and G. Hinton. Learning to represent spatial transformations with factored higher-order Boltzmann machines. Neural Computation, 2010. 8 C. Rother, P. Kohli, W. Feng, and J. Jia. Minimizing sparse higher order energy functions of discrete variables. In CVPR, 2009. 1, 2, 3, 6, 7 A. Shekhovtsov, P. Kohli, and C. Rother. Curvature prior for MRFbased segmentation and shape inpainting. In Pattern Recognition, pages 41–51. Springer, 2012. 4 P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. In Parallel distributed processing. 1986. 3 K. Swersky, D. Tarlow, I. Sutskever, R. Salakhutdinov, R. Zemel, and R. Adams. Cardinality restricted Boltzmann machines. In NIPS, 2012. 4 Y. Tang, R. Salakhutdinov, and G. Hinton. Robust Boltzmann machines for recognition and denoising. In CVPR, 2012. 3 I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004. 2 S. Vicente, V. Kolmogorov, and C. Rother. Graph cut based image segmentation with connectivity priors. In CVPR, 2008. 3 S. Vicente, V. Kolmogorov, and C. Rother. Joint optimization of segmentation and appearance models. In ICCV. 2009. 2 O. Woodford, C. Rother, and V. Kolmogorov. A global perspective on MAP inference for low-level vision. In IJCV, 2009. 2 C. Yu and T. Joachims. Learning structural SVMs with latent variables. In ICML, 2009. 2 555666</p>
<br/>
<br/><br/><br/></body>
</html>
