<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-76" href="#">cvpr2013-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</h1>
<br/><p>Source: <a title="cvpr-2013-76-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bergamasco_Can_a_Fully_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>Reference: <a title="cvpr-2013-76-reference" href="../cvpr2013_reference/cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. [sent-4, score-0.816]
</p><p>2 The goal of any calibration method is to build such map by finding the optimal parameters for the model. [sent-18, score-0.416]
</p><p>3 In turn this results in tight coupling of models and calibration procedures, since any calibration approach is designed for a specific camera type. [sent-19, score-0.91]
</p><p>4 The simplest formalization of the imaging process is the pinhole camera. [sent-20, score-0.52]
</p><p>5 Also catadioptric systems have been widely covered in the literature, with a large selection of models and calibration methods [1, 13]. [sent-23, score-0.47]
</p><p>6 The most general imaging model, that associates an independent 3D ray to each pixel, would in principle be able to describe any kind of imaging system, regardless of the optical path that drives each ray to the sensitive elements of the device. [sent-25, score-0.816]
</p><p>7 However, the complete independence ofmillions ofrays makes its calibration a daunting task as each of them needs several 2D to 3D correspondences to be properly constrained. [sent-26, score-0.435]
</p><p>8 This problem was first addressed in [6], where unconstrained rays (here called raxels) are calibrated exploiting their intersections with a target (an encoded laptop monitor) that moves along a translating stage. [sent-27, score-0.654]
</p><p>9 This approach is somewhat limited by the fact that the pose of the calibration planes must be known (i. [sent-29, score-0.471]
</p><p>10 A method for the calibration of the general model and unknown poses is proposed in [18]. [sent-32, score-0.493]
</p><p>11 In general, the literature has deemed the unconstrained model and related calibration procedures a last resort to be adopted only when traditional approaches fail due to either geometrical or methodological issues. [sent-37, score-0.694]
</p><p>12 For this reason the pinhole model, augmented with a proper distortion correction, dominates the application landscape whenever its use is feasible. [sent-38, score-0.518]
</p><p>13 For this to be the case, the calibration must be both effective and reasonably easy to perform. [sent-41, score-0.389]
</p><p>14 In the following sections we briefly describe our generic model (which is indeed pretty standard) and we introduce a practical calibration method. [sent-42, score-0.447]
</p><p>15 The impact on the calibration accuracy with respect to the pin-  hole model is evaluated with a wide set of experiments. [sent-43, score-0.417]
</p><p>16 Imaging Model and Calibration In this paper we propose to use a fully unconstrained camera model even for central cameras. [sent-46, score-0.431]
</p><p>17 In the proposed model each pixel is associated with the light ray direction from the object to where the ray hits the optics, rendering completely irrelevant how the optics bend the light to hit the CCD. [sent-47, score-0.81]
</p><p>18 This ray can be formalized as a line in the Euclidean space which, in the unconstrained model, is independent on the lines assigned to the other pixels and completely free with respect to direction and position. [sent-48, score-0.536]
</p><p>19 Under these assumptions, pixels cease to hold a precise geometrical meaning and they become just indexes to the imaging rays, having completely hidden the path that the ray has to go through inside the optics to hit the right cell in the CCD. [sent-49, score-0.542]
</p><p>20 The ray associated with camera pixel ican be written as ri = (di , pi), where di, pi ∈ IR3 represent direction and position of the ray respectively (see Figure 1). [sent-51, score-0.89]
</p><p>21 z Any point x )i ann nthde d ray ri satisfies the parametric equation x = dit + pi for some t ∈ R. [sent-53, score-0.404]
</p><p>22 ing in several million parameters to be estimated for cur-  rent cameras, a dimensionality that is beyond the possibilities of the most commonly used calibration processes. [sent-55, score-0.416]
</p><p>23 One problem with the commonly used target-based calibration systems (be them chessboard-based, dot-based, or based on any other pattern) is that they provide sparse localization points, resulting in rather low numbers of per-pixel observations. [sent-57, score-0.389]
</p><p>24 We propose to solve this problem by providing dense localization on the target, thus resulting in one observation per pixel in each pose of the calibration target. [sent-59, score-0.483]
</p><p>25 This dense calibration target is obtained through the use of structured light pattens on a normal LCD display, allowing us to assign to each camera pixel the 2D coordinate in the target planar reference frame of the location where the ray associated to the pixel hits the target. [sent-60, score-1.368]
</p><p>26 Let index s range over the calibration shots, with Θs = (Rs , ts) we denote the pose parameters of the calibration target in shot s, transforming the target’s coordinate system onto the camera coordinates. [sent-63, score-1.29]
</p><p>27 The us, vs, and ns base vectors of the target coordinate system expressed in the camera coordinate system correspond to the first, second and third columns of Rs respectively, i. [sent-64, score-0.4]
</p><p>28 Further, let Cosi ∈ IR2 denote the code (target 2D location) measured at camera pixel i in shot s, while with Ce(ri |Θs) ∈ IR2 we denote the expected code at pixel i, given ray ri IaRnd target pose Θs (see Figure 1). [sent-67, score-0.846]
</p><p>29 Ignoring possible refraction effects on the monitor’s surface this corresponds simply to the surface coordinates (u, v) of the intersection between the ray and the target plane. [sent-68, score-0.885]
</p><p>30 Schema of the general camera model and calibration target described in this paper. [sent-75, score-0.717]
</p><p>31 Least Squares Formulation We express the calibration process as a generalized least squares problem [10]. [sent-83, score-0.535]
</p><p>32 In this context the main source of heteroscedasticity de-  rives from the directional correlation of code errors when rays hit the target plane at an angle. [sent-93, score-0.529]
</p><p>33 (5)  In the standard pinhole model the effect of eliminating the correlation of the errors is obtained by re-projecting the residuals onto the image plane, i. [sent-100, score-0.595]
</p><p>34 In this sense, normalizing over the inverse error covariance is as close as one can get to the minimization of the reprojection (or geometrical) error in an unconstrained system that loses any direct geometrical connection between rays and pixels. [sent-103, score-0.661]
</p><p>35 It is worth mentioning that the reprojection error in the pinhole model accounts for another source of heteroscedasticity, i. [sent-104, score-0.577]
</p><p>36 To optimize the least squares formulation efficiently, we make use of the conditional independence of the ray parameters r given the poses Θ and of the poses given the rays. [sent-110, score-0.654]
</p><p>37 We do this by performing a two-step optimization process in which we alternatively optimize all the ray parameters in parallel keeping the pose parameters fixed, and then optimize the poses keeping the rays fixed. [sent-111, score-0.682]
</p><p>38 In our experiments the optimization process is initialized with the normal pinhole model with polynomial radial distortion. [sent-115, score-0.607]
</p><p>39 As can be seen in Figure 1, given a ray r intersecting the target plane at 2D coordinate Ce, we can divide the residual ε = Ce − Co into the orthogonal vectors vε? [sent-120, score-0.593]
</p><p>40 ||2 + ||ε⊥||2  = εTΣ−1ε ,  (7)  thus the generalized least squares formulation with respect to the target coordinates corresponds to the standard linear least squares with respect to the 3D points associated with each ray. [sent-134, score-0.516]
</p><p>41 Estimation of the Poses  The second step in the alternating calibration process is the optimization of the target poses keeping the rays fixed. [sent-142, score-0.819]
</p><p>42 Also in this step we can make use of a conditional independence; in fact, with the rays fixed, the pose parameters from different shots become independent and can be optimized in parallel. [sent-143, score-0.439]
</p><p>43 Further, just like in the ray calibration step, we make use of the equivalence between the Mahalanobis distance over the 2D core errors Ce − Co with the 3laDn oEbuiscl diidsetaann deis otvanerc eth e be 2twDe ceonr eob esrreorrvsed C position awnitdh ray. [sent-144, score-0.702]
</p><p>44 In this situation the estimation of pose Θs reduces to the search for the rigid transformation that minimizes the distance between the (transformed) observations ΘsCois and the ray ri. [sent-145, score-0.393]
</p><p>45 The modification lies in how the closest points are sought: In our context instead of searching for the closest point in a single given 2D surface common for all the points, we select the closest point in the unique 1D ray associated with the given observation. [sent-148, score-0.52]
</p><p>46 Clearly, given the generality of the imaging model, the  optimization problem is not convex, thus we cannot guarantee a global optimum like in the case of the pinhole model. [sent-150, score-0.52]
</p><p>47 In [15] it was shown that refraction had a small but noticeable effect when calibrating using LCD displays, so we extended the least square formulation to incorporate Snell’s law of optical refraction in order to assess and correct its effects. [sent-155, score-0.603]
</p><p>48 According to Snell’s law, a ray r hitting the surface at an angle φ with the normal n, will be refracted inside the transparent layer at an angle satisfying sin = λ sin φ, hitting the reflective layer at target coordinates Ce + ΔCe with  ψ  ΔCe = r? [sent-157, score-0.749]
</p><p>49 s a non-linearity in both the ray and pose estimation that breaks the conditional independence assumption at the basis of our approach. [sent-161, score-0.412]
</p><p>50 We  solve this by adopting a fixed point approach, reiterating the least squares estimations (both in the ray and pose estimation phases) using the refraction shift ΔCe computed based on the previous rays and poses. [sent-162, score-0.914]
</p><p>51 Figure 2 shows the effect of the refraction parameters on the final root mean squared error (RMS) of the calibrated camera. [sent-163, score-0.496]
</p><p>52 From the plot we can see clearly that the minimum is attained in the refraction-less case (μ = 0 or λ = 0), thus pointing to a negligible effect of refraction for the unconstrained model as opposed to what was reported in [15] for the pinhole model. [sent-164, score-1.072]
</p><p>53 It must be said that our experiments with the pinhole model gave inconsistent results, exhibiting error reduction as reported by Schmalz et al. [sent-165, score-0.603]
</p><p>54 This can be explained by the fact that target shots are mostly frontal to the camera and thus the effect of refraction is mostly radial. [sent-167, score-0.785]
</p><p>55 Effect of refraction correction for different values of the refraction parameters. [sent-169, score-0.535]
</p><p>56 radial functions used for eliminating distortion with the pinhole model. [sent-170, score-0.61]
</p><p>57 Using more points to perform the calibration constrains the model more, resulting in no additional advantage. [sent-171, score-0.464]
</p><p>58 75mm glass layer (the front glass of a photo frame) in front of the LCD display in an attempt to produce a much stronger refraction effect. [sent-174, score-0.394]
</p><p>59 Even in this condition no effect could be measured both in the unconstrained and in the dense pinhole case. [sent-175, score-0.716]
</p><p>60 Working with the Unconstrained Camera By alternating the two estimation process we obtain the generalized least squares estimation of both rays and poses, and with that a full calibration of the unconstrained camera model. [sent-178, score-1.076]
</p><p>61 However, for the unconstrained model to become an effective alternative to the pinhole model several problems must be solved. [sent-179, score-0.735]
</p><p>62 In particular, if we want to use the model for high precision 3D reconstruction, we need at the very least an effective algorithm for stereo calibration as well as a way to interpolate rays at non-integer coordinates. [sent-180, score-0.678]
</p><p>63 Potentially we also need a wider set of geometrical and algorithmic tools that are either straightforward of well studied for the pinhole model. [sent-181, score-0.51]
</p><p>64 In fact, the parametric nature of the pinhole model offers a direct solution the interpolation problem, while there is a ample body of work on how to estimate the motion between two calibrated cameras. [sent-182, score-0.63]
</p><p>65 As a matter of fact, the pinhole model also allows for useful  Figure 3. [sent-183, score-0.484]
</p><p>66 However, arguably any measurement or reconstruction process can be reformulated based on only extrinsic calibration and ray interpolation, which incidentally is the minimal requirement to perform triangulation effectively. [sent-186, score-0.702]
</p><p>67 For the stereo calibration we adopt a quite crude approach: We take several shots of our active target with both cameras and use the modified ICP algorithm to estimate the poses for each camera. [sent-187, score-0.867]
</p><p>68 As usual we initialize the poses with the pinhole model to guarantee convergence. [sent-188, score-0.56]
</p><p>69 With the poses of the first camera at hand, we can construct a set of 3D points xis in the first camera’s coordinate system where ray ri intersects the target plane at shot s. [sent-189, score-0.94]
</p><p>70 To solve the ray interpolation problem, we generalize bilinear interpolation to the manifold of 3D lines. [sent-195, score-0.485]
</p><p>71 Root mean squared error between expected and observed codes as a function of the number of iterations of the calibration algorithm. [sent-199, score-0.498]
</p><p>72 Comparison of the error obtained with the pinhole model calibrated with a chessboard and dense target, and with our calibration approach for the unconstrained model. [sent-215, score-1.282]
</p><p>73 For the active target we used an LCD display with a resolution of 1280x1024 pixels, while we used a professional entry level 1 megapixel computer vision camera with variable focal length optics set close to the shortest available length in order to have noticeable, but not extreme distortion. [sent-216, score-0.409]
</p><p>74 The rays and poses where initialized performing a pinhole calibration using the OpenCV library [3] and adopting a 5th order polynomial model for the radial distortion. [sent-217, score-1.258]
</p><p>75 Figure 4 plots the root mean squared error between ex-  pected and observed codes as a function of the number of iterations of the calibration algorithm. [sent-218, score-0.528]
</p><p>76 While the iteration of the calibration procedure was performed on the training set, the computation of the error was performed on the test set by running only the pose estimation without changing the rays. [sent-219, score-0.5]
</p><p>77 The top figure plots the error averaged over all the pixels and poses, and clearly shows that the estimated model exhibits an order of magnitude lower error than the pinhole model, which is the initialization model ans thus the first entry in the plot. [sent-220, score-0.686]
</p><p>78 The leftmost image refers to the initial pinhole model, while the middle and right image refer to the model after 2 and 21 iterations of the calibration procedure. [sent-222, score-0.873]
</p><p>79 The plot is compared against the results obtained with a pinhole model calibrated with a standard chessboard pattern and using the same LCD-based active dense target used to calibrate the unconstrained model. [sent-247, score-1.057]
</p><p>80 We can see that the dense target offers a marginal advantage over the chessboard target for the pinhole model. [sent-248, score-0.843]
</p><p>81 However, while exhibiting large variance, the unconstrained model has a lower average RMS error even with as few as 4 shots, reaching an error when estimated with a sufficient number of shots that is approximately an order of magnitude lower than what can be obtained with the pinhole model. [sent-252, score-1.057]
</p><p>82 In order to assess the advantage that the unconstrained model offers over the pinhole one in high precision tasks, we performed a very basic 3D measurement task on a calibrated camera pair. [sent-253, score-0.948]
</p><p>83 Both cameras where calibrated with the pinhole model (both intrinsic and extrinsic parameters) using the OpenCV library [3], and using the proposed approaches for unconstrained camera and stereo calibration. [sent-254, score-0.977]
</p><p>84 With the calibration parameters at hand, we triangulated two known points on the calibration target in 20 different shots in the test set, and computed their distance as a function of their distance from the camera. [sent-255, score-1.193]
</p><p>85 The target points where, in both cases 620 target pixels apart and the spatial unit of the plot is target pixel width. [sent-257, score-0.646]
</p><p>86 From the plot we immediately see that with the pinhole model there is a correlation between depth and measured size, clear indication of an imperfect image formation model, and resulting in a relatively large overall variance of the measure. [sent-258, score-0.566]
</p><p>87 015% is well within the construction tolerances) and can be seen in the pinhole model as well. [sent-266, score-0.484]
</p><p>88 Discussion In this paper we investigated the use of an unconstrained camera model to calibrate central quasi-pinhole cameras for high precision measurement and reconstruction tasks, and provided an effective approach to perform the calibration. [sent-268, score-0.524]
</p><p>89 The basic ingredient for the calibration process is the use of a dense target which allows us to attain a favorable parameters to measurements ratio, guaranteeing a stable calibration with a limited number of shots, and thus rendering the pro1 1 13 3 39 9 957 5  Figure 7. [sent-269, score-0.973]
</p><p>90 In a pinhole model all the points would coincide. [sent-273, score-0.531]
</p><p>91 cess operationally not much more complex than standard pinhole calibration with sparse passive targets. [sent-275, score-0.845]
</p><p>92 The resulting model can successfully eliminate the spatial coherence of the error, resulting in more precise and repeatable measures than what is achieved with the pinhole model. [sent-276, score-0.539]
</p><p>93 In fact there is a clear indication that the estimated models are substantially different from a radially undistorted pinhole model. [sent-277, score-0.521]
</p><p>94 To this effect, for each pixel and its 4-neighborhood, we define a local pinhole as the point that has minimal sum of squared distances from the five rays. [sent-279, score-0.548]
</p><p>95 Clearly, this is only a preliminary analysis, and much work still needs to be done before the unconstrained model can substitute effectively the pinhole model. [sent-282, score-0.707]
</p><p>96 In particular, more principled approaches to stereo calibration are needed, as well as alternatives for those algorithms that rely on geometrical processes that do not have a counterpart on the unconstrained model. [sent-283, score-0.666]
</p><p>97 Accurate catadioptric calibration for real-time pose estimation of room-size environments. [sent-288, score-0.523]
</p><p>98 Straight lines have to be straight: automatic calibration and removal of distortion from scenes of structured enviroments. [sent-310, score-0.451]
</p><p>99 A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses. [sent-349, score-0.579]
</p><p>100 A versatile camera calibration technique for highaccuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. [sent-413, score-0.582]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pinhole', 0.456), ('calibration', 0.389), ('ray', 0.313), ('refraction', 0.246), ('unconstrained', 0.223), ('rays', 0.186), ('shots', 0.173), ('target', 0.168), ('camera', 0.132), ('heteroscedasticity', 0.109), ('radial', 0.092), ('lcd', 0.09), ('catadioptric', 0.081), ('ce', 0.079), ('calibrated', 0.077), ('poses', 0.076), ('squares', 0.073), ('interpolation', 0.069), ('cosi', 0.066), ('usvs', 0.066), ('imaging', 0.064), ('display', 0.062), ('distortion', 0.062), ('cameras', 0.061), ('exhibiting', 0.061), ('error', 0.058), ('co', 0.055), ('shot', 0.054), ('geometrical', 0.054), ('plot', 0.054), ('pose', 0.053), ('squared', 0.051), ('snell', 0.051), ('chessboard', 0.051), ('coordinate', 0.05), ('opencv', 0.05), ('central', 0.048), ('covariance', 0.047), ('points', 0.047), ('pi', 0.047), ('optics', 0.047), ('independence', 0.046), ('residuals', 0.046), ('ri', 0.044), ('monitor', 0.044), ('ditint', 0.044), ('echet', 0.044), ('lilienblum', 0.044), ('ntsst', 0.044), ('puv', 0.044), ('schmalz', 0.044), ('correction', 0.043), ('surface', 0.043), ('least', 0.043), ('pixel', 0.041), ('closest', 0.039), ('coordinates', 0.039), ('pinholes', 0.039), ('sturm', 0.038), ('di', 0.038), ('hit', 0.037), ('effect', 0.037), ('mahalanobis', 0.037), ('rms', 0.037), ('radially', 0.036), ('planar', 0.035), ('reprojection', 0.035), ('manifold', 0.034), ('andrea', 0.034), ('hitting', 0.034), ('effects', 0.033), ('phase', 0.033), ('orthogonal', 0.033), ('precision', 0.032), ('angle', 0.031), ('polynomial', 0.031), ('optical', 0.031), ('hits', 0.031), ('lay', 0.031), ('drives', 0.031), ('plots', 0.03), ('coherency', 0.03), ('generalized', 0.03), ('generic', 0.03), ('fact', 0.029), ('glass', 0.029), ('ccd', 0.029), ('plane', 0.029), ('onto', 0.028), ('model', 0.028), ('nts', 0.028), ('repeatable', 0.028), ('clearly', 0.028), ('formation', 0.028), ('layer', 0.028), ('precise', 0.027), ('scatter', 0.027), ('xis', 0.027), ('parameters', 0.027), ('situation', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="76-tfidf-1" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>2 0.3317273 <a title="76-tfidf-2" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>Author: Timothy Yau, Minglun Gong, Yee-Hong Yang</p><p>Abstract: In underwater imagery, the image formation process includes refractions that occur when light passes from water into the camera housing, typically through a flat glass port. We extend the existing work on physical refraction models by considering the dispersion of light, and derive new constraints on the model parameters for use in calibration. This leads to a novel calibration method that achieves improved accuracy compared to existing work. We describe how to construct a novel calibration device for our method and evaluate the accuracy of the method through synthetic and real experiments.</p><p>3 0.2680327 <a title="76-tfidf-3" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>4 0.24438307 <a title="76-tfidf-4" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>5 0.23521942 <a title="76-tfidf-5" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>6 0.22077711 <a title="76-tfidf-6" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>7 0.19629933 <a title="76-tfidf-7" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>8 0.19313869 <a title="76-tfidf-8" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>9 0.17145176 <a title="76-tfidf-9" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>10 0.16402377 <a title="76-tfidf-10" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>11 0.15255171 <a title="76-tfidf-11" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>12 0.13763253 <a title="76-tfidf-12" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>13 0.13643716 <a title="76-tfidf-13" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>14 0.1322051 <a title="76-tfidf-14" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>15 0.11450861 <a title="76-tfidf-15" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>16 0.10513283 <a title="76-tfidf-16" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>17 0.10166506 <a title="76-tfidf-17" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>18 0.10086729 <a title="76-tfidf-18" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>19 0.094511069 <a title="76-tfidf-19" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>20 0.093653813 <a title="76-tfidf-20" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.192), (2, -0.017), (3, 0.035), (4, -0.006), (5, -0.104), (6, -0.059), (7, -0.043), (8, 0.076), (9, 0.051), (10, -0.086), (11, 0.139), (12, 0.175), (13, -0.099), (14, -0.335), (15, 0.017), (16, 0.076), (17, 0.116), (18, -0.055), (19, 0.102), (20, 0.097), (21, -0.006), (22, -0.118), (23, -0.113), (24, -0.026), (25, 0.095), (26, 0.009), (27, 0.02), (28, -0.013), (29, -0.02), (30, 0.043), (31, -0.045), (32, 0.037), (33, 0.029), (34, 0.014), (35, 0.005), (36, -0.011), (37, -0.043), (38, -0.003), (39, 0.072), (40, -0.038), (41, -0.033), (42, 0.016), (43, -0.005), (44, -0.002), (45, -0.08), (46, -0.047), (47, 0.032), (48, 0.061), (49, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95284837 <a title="76-lsi-1" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>2 0.93532294 <a title="76-lsi-2" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>3 0.93234855 <a title="76-lsi-3" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>4 0.91789252 <a title="76-lsi-4" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>Author: Timothy Yau, Minglun Gong, Yee-Hong Yang</p><p>Abstract: In underwater imagery, the image formation process includes refractions that occur when light passes from water into the camera housing, typically through a flat glass port. We extend the existing work on physical refraction models by considering the dispersion of light, and derive new constraints on the model parameters for use in calibration. This leads to a novel calibration method that achieves improved accuracy compared to existing work. We describe how to construct a novel calibration device for our method and evaluate the accuracy of the method through synthetic and real experiments.</p><p>5 0.79063976 <a title="76-lsi-5" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>6 0.76778996 <a title="76-lsi-6" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>7 0.76138592 <a title="76-lsi-7" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>8 0.76054573 <a title="76-lsi-8" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>9 0.74863851 <a title="76-lsi-9" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>10 0.73826218 <a title="76-lsi-10" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>11 0.71611434 <a title="76-lsi-11" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>12 0.67352796 <a title="76-lsi-12" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>13 0.65549386 <a title="76-lsi-13" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>14 0.6326952 <a title="76-lsi-14" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>15 0.55954206 <a title="76-lsi-15" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>16 0.5237301 <a title="76-lsi-16" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>17 0.5112564 <a title="76-lsi-17" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>18 0.49789569 <a title="76-lsi-18" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>19 0.48710057 <a title="76-lsi-19" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>20 0.47196603 <a title="76-lsi-20" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.507), (16, 0.023), (26, 0.041), (33, 0.215), (67, 0.036), (69, 0.03), (87, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94388038 <a title="76-lda-1" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>2 0.93325967 <a title="76-lda-2" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>3 0.92937249 <a title="76-lda-3" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>same-paper 4 0.92521793 <a title="76-lda-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.91870105 <a title="76-lda-5" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>6 0.90390342 <a title="76-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.88754374 <a title="76-lda-7" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>8 0.88495028 <a title="76-lda-8" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>9 0.85440671 <a title="76-lda-9" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>10 0.85311592 <a title="76-lda-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.82994342 <a title="76-lda-11" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>12 0.80050874 <a title="76-lda-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.79361796 <a title="76-lda-13" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>14 0.78041291 <a title="76-lda-14" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>15 0.77899116 <a title="76-lda-15" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>16 0.7650848 <a title="76-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.76276076 <a title="76-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.75513774 <a title="76-lda-18" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>19 0.75333172 <a title="76-lda-19" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>20 0.75149971 <a title="76-lda-20" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
