<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-450" href="#">cvpr2013-450</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</h1>
<br/><p>Source: <a title="cvpr-2013-450-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Rubinstein_Unsupervised_Joint_Object_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Michael Rubinstein, Armand Joulin, Johannes Kopf, Ce Liu</p><p>Abstract: We present a new unsupervised algorithm to discover and segment out common objects from large and diverse image collections. In contrast to previous co-segmentation methods, our algorithm performs well even in the presence of significant amounts of noise images (images not containing a common object), as typical for datasets collected from Internet search. The key insight to our algorithm is that common object patterns should be salient within each image, while being sparse with respect to smooth transformations across images. We propose to use dense correspondences between images to capture the sparsity and visual variability of the common object over the entire database, which enables us to ignore noise objects that may be salient within their own images but do not commonly occur in others. We performed extensive numerical evaluation on es- tablished co-segmentation datasets, as well as several new datasets generated using Internet search. Our approach is able to effectively segment out the common object for diverse object categories, while naturally identifying images where the common object is not present.</p><p>Reference: <a title="cvpr-2013-450-reference" href="../cvpr2013_reference/cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Image datasets collected from Internet search vary considerably in their appearance, and typically include many noise images that do not contain the object of interest (a small subset of the car image dataset is shown in (a); the full dataset is available in the accompanying material). [sent-2, score-0.631]
</p><p>2 Our algorithm automatically discovers and segments out the common object (b). [sent-3, score-0.196]
</p><p>3 Note how no objects are discovered for noise images in (b). [sent-4, score-0.211]
</p><p>4 Most previous co-segmentation methods, in contrast, are designed for more homogeneous datasets in which every image contains the object of interest, and, therefore, their performance degrades in the presence of noise (c). [sent-5, score-0.24]
</p><p>5 Abstract We present a new unsupervised algorithm to discover and segment out common objects from large and diverse image collections. [sent-6, score-0.365]
</p><p>6 In contrast to previous co-segmentation methods, our algorithm performs well even in the presence of significant amounts of noise images (images not containing a common object), as typical for datasets collected from Internet search. [sent-7, score-0.36]
</p><p>7 The key insight to our algorithm is that common object patterns should be salient within each image, while being sparse with respect to smooth transformations across images. [sent-8, score-0.297]
</p><p>8 We propose to use dense correspondences between images to capture the sparsity and visual variability of the common object over the entire database, which enables us to ignore noise objects that may be salient within their own images but do not commonly occur in others. [sent-9, score-0.727]
</p><p>9 Our approach is able to effectively segment out the common object for diverse object categories, while naturally identifying images where the common object is not present. [sent-11, score-0.518]
</p><p>10 Introduction We consider the task of jointly segmenting multiple images containing a common object. [sent-13, score-0.238]
</p><p>11 The goal is to label each pixel in a set of images according to whether or not it belongs to the underlying common object, with no additional information on the images or the object class1 . [sent-14, score-0.332]
</p><p>12 While numerous co-segmentation methods have been proposed, they were shown to work well mostly on small datasets, namely MSRC and iCoseg, containing salient and similar objects. [sent-17, score-0.215]
</p><p>13 In fact, in most of the images in those datasets the foreground can be quite easily separated from the background based on each image alone (i. [sent-18, score-0.445]
</p><p>14 Not only do the objects in images downloaded from the Internet exhibit drastically different style, color, texture, shape, pose, size, location and view-point; but such image collections also contain many noise images—images which do not contain the object of interest at all. [sent-23, score-0.445]
</p><p>15 These challenges, as we demonstrate, pose great difficulties on existing cosegmentation techniques (Figure 1(c)). [sent-24, score-0.205]
</p><p>16 111999333977  In this paper, we propose a novel correspondence-based object discovery and co-segmentation algorithm that performs well even in the presence of many noise images. [sent-26, score-0.273]
</p><p>17 Our algorithm automatically discovers the common object among the majority of images and computes a binary object/background label mask for each image. [sent-27, score-0.303]
</p><p>18 Images that do not contain the common object are naturally handled by returning an empty labeling (Figure 1(b), Figure 2). [sent-28, score-0.2]
</p><p>19 Our algorithm is designed based on the assumption that pixels (features) belonging to the common object should be: (a) salient, i. [sent-29, score-0.221]
</p><p>20 Given an input image dataset, we build a large-scale graphical model connecting similar images, where dense pixel correspondences are used to capture the object’s visual variability. [sent-34, score-0.275]
</p><p>21 These correspondences between images allow us to separate the common object from the background and visual noise. [sent-35, score-0.516]
</p><p>22 Our algorithm produces state-of-the-art results on the established MSRC and iCoseg co-segmentation datasets2, and provides considerable improvement over previous methods on several new challenging Internet datasets containing rigid and non-rigid object categories. [sent-37, score-0.216]
</p><p>23 In a supervised setup, objects were treated as topics and images as documents, and generative models such as Latent Dirichlet Allocation (LDA) and Hierarchical Pitman-Yor (HPY) have been used to learn the distribution and segmentation of multiple classes simultaneously [24, 22]. [sent-45, score-0.186]
</p><p>24 Recently, PageRank [7] was used to discover regions of interest in a bounding box representation [10], and selfsimilarities were used to discover a common pattern in several images [ 1]. [sent-48, score-0.371]
</p><p>25 Although in these works no generative models were used to learn the distribution of visual objects, reliable matching and saliency are found to be helpful for object discovery. [sent-49, score-0.417]
</p><p>26 The notions of matching and saliency were also successfully applied by Fakor et al. [sent-50, score-0.349]
</p><p>27 [5], a work 1We note that while we call our method “unsupervised”, we do assume that the input image dataset contains a common visual category. [sent-51, score-0.167]
</p><p>28 done in parallel to ours, for unsupervised discovery of image categories. [sent-53, score-0.212]
</p><p>29 Since then, numerous methods were proposed to improve and refine the co-segmentation [16, 6, 2, 8], many of which work in the context of a pair of images with the exact same object [19, 16, 6] or require some form of user interaction [2, 4]. [sent-57, score-0.209]
</p><p>30 [25] introduced the notion of ”ob-  jectness” to the co-segmentation framework, showing that requiring the foreground segment to be an object often improves co-segmentation results significantly. [sent-63, score-0.296]
</p><p>31 Other methods were proposed to handle images which might not contain the common object, either implicitly [9] or explicitly [11]. [sent-65, score-0.191]
</p><p>32 While image annotations may facilitate object discovery and segmentation, image tags are often noisy, and bounding boxes or class labels are usually unavailable. [sent-69, score-0.249]
</p><p>33 , bN}, where for each image Ii and pixel xB B= = (x {,b by), bi (x) =}, 1 w ihnedrieca ftoesr efaorcehgr imouangde ( tIhe common object), and bi (x) = 0 indicates background (not the object) at location x. [sent-79, score-0.504]
</p><p>34 Recall our assumption that for an object of interest the foreground pixels should be salient, i. [sent-80, score-0.362]
</p><p>35 The saliency of a pixel or a region in an image can be defined in numerous ways and exten111999334088  uercSo12345 12345  aliencyNeighbor warpedS atchingM  Fnmiantoe Stgi ure2. [sent-88, score-0.371]
</p><p>36 The images are shown at the top row, with two images common to the two datasets – the face and horse images in columns 1and 2, respectively. [sent-91, score-0.493]
</p><p>37 Left: when adding to the two common images three images containing horses (columns 3 5), our algorithm successfully ,id reenspteificetsiv heolyr. [sent-92, score-0.384]
</p><p>38 se L as :th we common object and face as “onno i msea”g, resulting mina gtehes choonrstaeins being lrasebesl (ecdo as foreground and the face being labeled as background (bottom row). [sent-93, score-0.434]
</p><p>39 Right: when adding to the two common images three images containing faces, face is now recognized as common and horse as noise, and the algorithm labels the faces as foreground and the horse as background. [sent-94, score-0.804]
</p><p>40 In our experiments, we used an offthe-shelf saliency measure—Cheng et al. [sent-97, score-0.276]
</p><p>41 ’s Contrast-based Saliency [3]—that produced sufficiently good saliency esti-  mates for our purposes, but our formulation is not limited to a particular saliency measure and others can be used. [sent-98, score-0.552]
</p><p>42 [3] define the saliency of a pixel based on its color contrast to other pixels in the image (how different it is from the other pixels). [sent-100, score-0.446]
</p><p>43 Since high contrast to surrounding regions is usually a stronger evidence for saliency of a region than high contrast to far away regions, they weigh the contrast by thespatial distances in the image. [sent-101, score-0.276]
</p><p>44 To exploit the dataset structure and similarity between image regions, we need to establish reliable correspondences between pixels in different images. [sent-110, score-0.327]
</p><p>45 This enables us to determine a pixel as background even when it may be very salient within its own image. [sent-111, score-0.284]
</p><p>46 However, instead of establishing the correspondence between all pixels in a pair of images, as done by previous work, we solve and update the correspondences based on our estimation of the foreground regions. [sent-113, score-0.525]
</p><p>47 This helps in ignoring background clutter and ultimately improves the correspondence between foreground pixels (Figure 3). [sent-114, score-0.374]
</p><p>48 Formally, let wij denote the flow field from image Ii to  image Ij . [sent-115, score-0.198]
</p><p>49 Given the binary masks bi, bj, the SIFT flow objective function becomes E (wij ; bi , bj ) =  ? [sent-116, score-0.349]
</p><p>50 We then denote by W the set of all pixel correspondences in the dataset: W = ∪iN=1 ∪Ij ∈Ni wij . [sent-130, score-0.385]
</p><p>51 Tnhdeen ndcieffse irnen thcee bdeattwaseeetn: Wthis = objective function and the original SIFT flow [15] is that it encourages matching foreground pixels in image Ii with foreground pixels in image Ij . [sent-131, score-0.608]
</p><p>52 (c) Nearest neighbor ordering (bottom row; left to right) for the source image in (a), computed with a weighted Gist descriptor using the foreground estimates (top row). [sent-135, score-0.31]
</p><p>53 We use the foreground mask estimates to remove background clutter when computing correspondences (a), and to improve the retrieval of neighbor images (compared to (b), the ordering in (c) places right-facing horses first, followed by leftfacing horses, with the (noise) image of a person last). [sent-138, score-0.758]
</p><p>54 the contribution of this modification for establishing reliable correspondences between similar images. [sent-139, score-0.236]
</p><p>55 For small datasets, we can estimate the correspondences between any pair of images, however for large datasets such computation is clearly prohibitive. [sent-140, score-0.293]
</p><p>56 Therefore, we first find for each image Ii a set of similar images, Ni, based on global image statistics that are more efficient to compute, and estimate pixel correspondences with those images only. [sent-141, score-0.3]
</p><p>57 We use the Gist descriptor [ 17] in our implementation, and similarly modify it to account for the foreground estimates by giving lower weight in the descriptor to pixels labeled as background. [sent-143, score-0.324]
</p><p>58 Figure 3(b–c) demonstrate that better sorting of the images is achieved when using this weighted Gist descriptor, which in turn improves the set of images with which pixel correspondences are computed. [sent-144, score-0.358]
</p><p>59 We use the above saliency and matching terms to define the likelihood of a pixel label:  Φβ,isaliency(x) + λmatchΦimatch(x),  Φi(x) =? [sent-152, score-0.376]
</p><p>60 We would like the masks bi to be spatially consistent within each image, i. [sent-156, score-0.206]
</p><p>61 We would also like the labeling to be consistent between images, and so we add a term accounting for the inter-image compatibility between a pixel x in image Ii and its corresponding pixel y = x wij (x) in image Ij :  + Ψeijxt(x,y)  =  ? [sent-166, score-0.261]
</p><p>62 Finally, once we have an estimate of bi, we can learn the color histograms of the background and foreground regions of ? [sent-174, score-0.324]
</p><p>63 e aconndtr Hibu =tio ∪n of the pixel x to the foreground or background color model based on the segmentation estimate bi (x) :  Φciolor(x,  −loghbii(x)(x). [sent-182, score-0.579]
</p><p>64 By combining all the aforementioned terms, we obtain a cost function, E(B; W, H), for the segmentations B given the correspondences W and the color models H:  E(B;W,H) =i? [sent-184, score-0.301]
</p><p>65 Our algorithm alternates between optimizing the correspondences W (Equation 2), and the binary masks B (Equation 8). [sent-192, score-0.254]
</p><p>66 The algorithm then recomputes neighboring images and pixel correspondences based on the current foreground estimates, and the process is repeated for a few iterations until convergence (we typically used 5 − 10 iterations). [sent-197, score-0.492]
</p><p>67 Results We conducted extensive experiments to verify our approach, both on standard co-segmentation datasets and image collections downloaded from the Internet. [sent-205, score-0.207]
</p><p>68 We use two performance metrics: precision, P (the ratio of correctly labeled pixels, both foreground  and background), and Jaccard similarity, J (the intersection over union of the result and ground truth segmentations). [sent-211, score-0.192]
</p><p>69 Results on Co-segmentation datasets We report results for the MSRC dataset [23] (14 object classes; about 30 images per class) and iCoseg dataset [2] (30 classes; varying number of images per class), which have been widely used by previous work to evaluate co-segmentation performance. [sent-216, score-0.381]
</p><p>70 Both datasets include human-given segmentations that are used for the quantitative evaluation. [sent-217, score-0.181]
</p><p>71 when using the parameters above, and when setting λmatch = λext = 0, respectively), where the latter effectively reduces the method to segmenting every image independently using its saliency map and spatial regularization (combined in a Grabcut-style iterative optimization). [sent-220, score-0.327]
</p><p>72 Moreover, this simple algorithm—an off-the-shelf, low-level saliency measure combined with spatial regularization—which does not use co-segmentation, is sufficient to produce accurate results (and outperforms recent techniques; see below) on the standard cosegmentation datasets ! [sent-222, score-0.591]
</p><p>73 The reason is twofold: (a) all images in each visual category in those datasets contain the object of interest, and (b) for most of the images the foreground is quite easily separated from the  background based on its relative saliency alone. [sent-223, score-0.922]
</p><p>74 Our method outperforms theirs on all classes in MSRC and 9/16 of the classes in iCoseg (see supplementary material), and our average precision and Jaccard similarity are slightly better than theirs (Table 1). [sent-257, score-0.241]
</p><p>75 Results on Internet Datasets Using the Bing API, we automatically downloaded images for three queries with query expansion through Wikipedia: car (4, 347 images), horse (6, 381 images), and airplane (4, 542 images). [sent-261, score-0.529]
</p><p>76 Many objects are not very distinctive from the background in terms of color, but they were still successfully discovered due to good correspondences to other images. [sent-266, score-0.39]
</p><p>77 For car, some car parts are occasionally missing as they may be less salient within their image or not well aligned to other images. [sent-267, score-0.306]
</p><p>78 Similarly, for horse, the body of horses gets consistently discovered but sometimes legs are miss-  ing. [sent-268, score-0.162]
</p><p>79 More flexible transforms might be needed for establishing correspondences between horses. [sent-269, score-0.236]
</p><p>80 For airplane, saliency plays a more important role as the uniform skies always match best regardless of the transform. [sent-270, score-0.276]
</p><p>81 However the algorithm manages to correctly segment out airplanes even when they are less salient, and identifies noise images, such as that of plane cabins and jet engines, as background, since those have an overall worse matching to other images in the dataset. [sent-271, score-0.249]
</p><p>82 For qualitative evaluation, we collected partial human labels for each dataset using the LabelMe annotation toolbox [2 1] and a combination of volunteers and Mechanical Turk workers, resulting in 1, 306 car, 879 horse, and 561 airplane images labeled. [sent-272, score-0.319]
</p><p>83 Qualitative results for these datasets are shown in Figure 6 and the supplementary material. [sent-297, score-0.173]
</p><p>84 The performance on airplane is slightly better than horse and car as in many of the images the airplane can be easily segmented out from the uniform sky background. [sent-308, score-0.58]
</p><p>85 Image correspondences helped the most on the car dataset (+11% precision, +17% Jaccard similarity), probably because in many of the images the cars are not that salient, while they can be matched reliably to similar car images to be segmented correctly. [sent-309, score-0.67]
</p><p>86 We also compared our results with the same three state-of-the-art cosegmentation methods as in Section 4. [sent-311, score-0.205]
</p><p>87 We also compared to two baselines, one where all the pixels are classified as background (“Baseline 1”), and one where all pixels are classified as foreground (“Baseline 2”). [sent-315, score-0.405]
</p><p>88 The largest gain in precision by our method is on the airplane dataset, which has the highest noise level of these three datasets. [sent-317, score-0.282]
</p><p>89 False positives include a motorcycle and a headlight in the car dataset, and a tree in the horse dataset. [sent-322, score-0.24]
</p><p>90 The algorithm also fails occasionally to discover objects with unique views or background. [sent-324, score-0.172]
</p><p>91 Automatic discovery of cars, horses and airplanes downloaded from the Internet, containing 4, 347, 6, 381 and 4, 542 images, respectively. [sent-327, score-0.395]
</p><p>92 Notice how images that do not contain the object are labeled as background. [sent-329, score-0.168]
</p><p>93 The last row of each dataset shows some failure cases where no object was discovered or where the discovery is wrong or incomplete. [sent-330, score-0.347]
</p><p>94 For example, had a dataset of 100 car images contained 80 images of cars and 20 images of car wheels, then using K = 16 neighbor images by our algorithm may result in intra-group connections, relating images of cars to other images of cars and images of wheels with others alike. [sent-337, score-1.061]
</p><p>95 In such case the algorithm may not be able to infer that one category is more common than the other, and both cars and wheels would be segmented as foreground. [sent-338, score-0.247]
</p><p>96 Conclusion We explored automatic visual object discovery and segmentation from the Internet using one query of an object category. [sent-341, score-0.405]
</p><p>97 The common object often differs drastically in appearance, and a significant portion of the images may not contain the object at all. [sent-343, score-0.325]
</p><p>98 We model the sparsity and saliency properties of the common object, and construct a large-scale graphical model to jointly infer a binary mask for each image. [sent-345, score-0.415]
</p><p>99 We demonstrated improvement over existing cosegmentation techniques on standard co-segmentation datasets and several challenging Internet datasets. [sent-346, score-0.315]
</p><p>100 Using multiple segmentations to discover objects and their extent in image collections. [sent-493, score-0.198]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('internet', 0.311), ('saliency', 0.276), ('icoseg', 0.273), ('cosegmentation', 0.205), ('foreground', 0.192), ('correspondences', 0.183), ('msrc', 0.172), ('jaccard', 0.158), ('discovery', 0.143), ('wij', 0.143), ('airplane', 0.141), ('salient', 0.14), ('bi', 0.135), ('car', 0.121), ('horse', 0.119), ('datasets', 0.11), ('horses', 0.107), ('ciolor', 0.106), ('joulin', 0.103), ('gist', 0.097), ('armand', 0.094), ('discover', 0.092), ('common', 0.09), ('bj', 0.088), ('cars', 0.085), ('background', 0.085), ('rubinstein', 0.082), ('vicente', 0.08), ('precision', 0.078), ('ii', 0.078), ('ext', 0.075), ('wheels', 0.072), ('masks', 0.071), ('segmentations', 0.071), ('unsupervised', 0.069), ('object', 0.067), ('pixels', 0.064), ('supplementary', 0.063), ('sift', 0.063), ('noise', 0.063), ('segmentation', 0.061), ('rother', 0.059), ('pixel', 0.059), ('images', 0.058), ('nxi', 0.058), ('michael', 0.057), ('downloaded', 0.056), ('flow', 0.055), ('discovered', 0.055), ('establishing', 0.053), ('kim', 0.052), ('pagerank', 0.052), ('kuettel', 0.052), ('segmenting', 0.051), ('airplanes', 0.05), ('mask', 0.049), ('user', 0.048), ('ni', 0.048), ('color', 0.047), ('cheng', 0.046), ('occasionally', 0.045), ('dataset', 0.044), ('contain', 0.043), ('si', 0.043), ('ij', 0.043), ('ordering', 0.042), ('diverse', 0.042), ('accompanying', 0.042), ('engines', 0.042), ('neighbor', 0.042), ('collections', 0.041), ('matching', 0.041), ('comparisons', 0.04), ('interest', 0.039), ('discovers', 0.039), ('labels', 0.039), ('containing', 0.039), ('russell', 0.039), ('labelme', 0.038), ('row', 0.038), ('segment', 0.037), ('qualitative', 0.037), ('similarity', 0.036), ('yuen', 0.036), ('numerous', 0.036), ('dissimilar', 0.036), ('objects', 0.035), ('xing', 0.034), ('query', 0.034), ('pages', 0.034), ('descriptor', 0.034), ('supplemental', 0.034), ('equation', 0.034), ('noticed', 0.034), ('correspondence', 0.033), ('winn', 0.033), ('nearest', 0.033), ('visual', 0.033), ('classes', 0.032), ('successfully', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="450-tfidf-1" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>Author: Michael Rubinstein, Armand Joulin, Johannes Kopf, Ce Liu</p><p>Abstract: We present a new unsupervised algorithm to discover and segment out common objects from large and diverse image collections. In contrast to previous co-segmentation methods, our algorithm performs well even in the presence of significant amounts of noise images (images not containing a common object), as typical for datasets collected from Internet search. The key insight to our algorithm is that common object patterns should be salient within each image, while being sparse with respect to smooth transformations across images. We propose to use dense correspondences between images to capture the sparsity and visual variability of the common object over the entire database, which enables us to ignore noise objects that may be salient within their own images but do not commonly occur in others. We performed extensive numerical evaluation on es- tablished co-segmentation datasets, as well as several new datasets generated using Internet search. Our approach is able to effectively segment out the common object for diverse object categories, while naturally identifying images where the common object is not present.</p><p>2 0.3167831 <a title="450-tfidf-2" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>3 0.29185152 <a title="450-tfidf-3" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>4 0.29007846 <a title="450-tfidf-4" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>5 0.27659735 <a title="450-tfidf-5" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>6 0.26940697 <a title="450-tfidf-6" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>7 0.23424463 <a title="450-tfidf-7" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>8 0.18530862 <a title="450-tfidf-8" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>9 0.1767907 <a title="450-tfidf-9" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>10 0.17155512 <a title="450-tfidf-10" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>11 0.16324389 <a title="450-tfidf-11" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>12 0.15404925 <a title="450-tfidf-12" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>13 0.15345573 <a title="450-tfidf-13" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>14 0.14472355 <a title="450-tfidf-14" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>15 0.14028077 <a title="450-tfidf-15" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>16 0.13965815 <a title="450-tfidf-16" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>17 0.13349448 <a title="450-tfidf-17" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>18 0.13034314 <a title="450-tfidf-18" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>19 0.12960127 <a title="450-tfidf-19" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>20 0.12846506 <a title="450-tfidf-20" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.288), (1, -0.104), (2, 0.308), (3, 0.151), (4, 0.011), (5, -0.017), (6, -0.002), (7, -0.027), (8, -0.017), (9, 0.001), (10, 0.052), (11, 0.03), (12, 0.033), (13, 0.023), (14, 0.003), (15, -0.081), (16, 0.03), (17, -0.035), (18, 0.005), (19, -0.031), (20, 0.023), (21, -0.012), (22, 0.019), (23, -0.092), (24, 0.069), (25, -0.103), (26, 0.024), (27, 0.097), (28, -0.006), (29, -0.018), (30, 0.04), (31, -0.021), (32, -0.016), (33, -0.029), (34, 0.102), (35, -0.015), (36, -0.003), (37, 0.012), (38, 0.042), (39, -0.002), (40, 0.004), (41, 0.01), (42, 0.027), (43, 0.01), (44, -0.023), (45, -0.028), (46, 0.033), (47, 0.009), (48, 0.042), (49, -0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92110419 <a title="450-lsi-1" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>Author: Michael Rubinstein, Armand Joulin, Johannes Kopf, Ce Liu</p><p>Abstract: We present a new unsupervised algorithm to discover and segment out common objects from large and diverse image collections. In contrast to previous co-segmentation methods, our algorithm performs well even in the presence of significant amounts of noise images (images not containing a common object), as typical for datasets collected from Internet search. The key insight to our algorithm is that common object patterns should be salient within each image, while being sparse with respect to smooth transformations across images. We propose to use dense correspondences between images to capture the sparsity and visual variability of the common object over the entire database, which enables us to ignore noise objects that may be salient within their own images but do not commonly occur in others. We performed extensive numerical evaluation on es- tablished co-segmentation datasets, as well as several new datasets generated using Internet search. Our approach is able to effectively segment out the common object for diverse object categories, while naturally identifying images where the common object is not present.</p><p>2 0.83599126 <a title="450-lsi-2" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>3 0.83209288 <a title="450-lsi-3" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>4 0.82011944 <a title="450-lsi-4" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>5 0.81997621 <a title="450-lsi-5" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>6 0.80877924 <a title="450-lsi-6" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>7 0.80652922 <a title="450-lsi-7" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>8 0.79099029 <a title="450-lsi-8" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>9 0.76987672 <a title="450-lsi-9" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>10 0.72476643 <a title="450-lsi-10" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>11 0.71640426 <a title="450-lsi-11" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>12 0.68002993 <a title="450-lsi-12" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>13 0.60648495 <a title="450-lsi-13" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>14 0.60088158 <a title="450-lsi-14" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>15 0.58551615 <a title="450-lsi-15" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>16 0.57579726 <a title="450-lsi-16" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>17 0.57367873 <a title="450-lsi-17" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>18 0.56904346 <a title="450-lsi-18" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>19 0.56892157 <a title="450-lsi-19" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>20 0.55192471 <a title="450-lsi-20" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.174), (16, 0.03), (26, 0.056), (27, 0.068), (33, 0.314), (67, 0.088), (69, 0.047), (73, 0.019), (76, 0.015), (77, 0.012), (87, 0.07), (94, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98677909 <a title="450-lda-1" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>2 0.97427136 <a title="450-lda-2" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>3 0.97404712 <a title="450-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.97020984 <a title="450-lda-4" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>5 0.9701966 <a title="450-lda-5" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>6 0.96984351 <a title="450-lda-6" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>7 0.96921813 <a title="450-lda-7" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>8 0.96881831 <a title="450-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.96699321 <a title="450-lda-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.96424526 <a title="450-lda-10" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>11 0.96411568 <a title="450-lda-11" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>12 0.96325493 <a title="450-lda-12" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>13 0.96306247 <a title="450-lda-13" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>14 0.96154428 <a title="450-lda-14" href="./cvpr-2013-Tensor-Based_Human_Body_Modeling.html">426 cvpr-2013-Tensor-Based Human Body Modeling</a></p>
<p>15 0.96148831 <a title="450-lda-15" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>16 0.96128213 <a title="450-lda-16" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>17 0.96051759 <a title="450-lda-17" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>18 0.96014386 <a title="450-lda-18" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>19 0.96007514 <a title="450-lda-19" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>20 0.95985806 <a title="450-lda-20" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
