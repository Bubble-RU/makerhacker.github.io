<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-8" href="#">cvpr2013-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</h1>
<br/><p>Source: <a title="cvpr-2013-8-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_A_Fast_Approximate_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>Reference: <a title="cvpr-2013-8-reference" href="../cvpr2013_reference/cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. [sent-5, score-0.716]
</p><p>2 Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. [sent-6, score-0.494]
</p><p>3 Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. [sent-8, score-0.794]
</p><p>4 Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. [sent-9, score-0.136]
</p><p>5 Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve  more than 100 times speedup on large real data sets over the state-of-the-art method. [sent-10, score-0.152]
</p><p>6 It was first proposed in [3, 16] for word classification and then developed by [1] for document classification. [sent-13, score-0.381]
</p><p>7 The work in [1] summarizes the three key benefits of distributional word clustering as discovering semantic word groups, producing compact classification models and improving classification performance. [sent-14, score-0.998]
</p><p>8 Other work on studying distributional word clustering for text classification can be found in [2, 4]. [sent-15, score-0.642]
</p><p>9 Most, if not all, of the existing distributional word clustering algorithms use an objective function that can be explicitly or implicitly linked to the criterion of mutual information. [sent-16, score-0.642]
</p><p>10 Rooted in the Information Bottleneck (IB) framework, AIB conducts word clustering by maximally preserving the mutual information between words and class labels. [sent-18, score-0.721]
</p><p>11 Starting with each word as an individual cluster, AIB merges two words leading to the minimum loss of mutual information at each level. [sent-20, score-0.808]
</p><p>12 Accordingly, efficiently merging a large set of visual words to generate compact visual codebooks has attracted attention. [sent-26, score-0.319]
</p><p>13 Inspired by this finding, we propose an approach different from AIB and Fast-AIB, which evaluate the goodness of every pair of words by explicitly computing the mutual information loss. [sent-32, score-0.361]
</p><p>14 Instead, our algorithm evaluates the goodness of a pair of words by merely matching their ratios of word-class joint probabilities. [sent-33, score-0.324]
</p><p>15 And an explicit evalua-  tion of mutual information loss is only used to select the 555555446  best pair from a small set of good candidate word pairs. [sent-34, score-0.64]
</p><p>16 Also, once two words are merged, our algorithm can trivially obtain the good candidate words for the next level. [sent-35, score-0.414]
</p><p>17 By looking into this result, we think that this could be because the approximation in our algorithm, which only requires the information loss to be small rather than rigidly minimum as in AIB, produces a mild “regularization” effect. [sent-38, score-0.217]
</p><p>18 This seems to help enhance the robustness of AIB with respect to less reliable probability estimates, when the number of words is much larger than that of training samples. [sent-39, score-0.168]
</p><p>19 To verify the advantages of our algorithm, experimental study is conducted on three benchmark data sets on image recognition and text classification, with different number of words involved. [sent-40, score-0.264]
</p><p>20 Agglomerative Information Bottleneck [17] Let W = {w1, w2 , · · · , wn} denote a set of n words and let c bete W Wclas =s {lawbels, w,h··er·e , c =} 1, ·e ·n ·o , eC a. [sent-45, score-0.168]
</p><p>21 u Assuming tmhaatt towno b wetwordesen wp an andd wq are merged yin Ito(W a new? [sent-48, score-0.294]
</p><p>22 w Asorsud wpq, the word set becomes Wpq = {W \ {wp, wq}} ? [sent-49, score-0.329]
</p><p>23 (1)  By merging, the prior probability and the word-class joint probability of the new word wpq are defined as P(wpq) P(wpq, c)  = =  P(wp) + P(wq) P(wp, c) + P(wq, c) . [sent-53, score-0.443]
</p><p>24 When necessary, we w··il·l ,wnri aten dw oard wi in full as wi (ai, bi). [sent-56, score-0.532]
</p><p>25 Note that ri denotes the “class ratio” for each word, and it will play a fundamental role in our algorithm. [sent-57, score-0.143]
</p><p>26 This can be intuitively understood because merging operation is symmetrical with respect to two words. [sent-76, score-0.135]
</p><p>27 For given ap, bp, the function of mutual information loss in Eq. [sent-80, score-0.249]
</p><p>28 ∂ Δ I ∂ ( ba aq q , b q ) = 0 ⇐⇒abqq=abpp  (7)  Proof: From Eq. [sent-83, score-0.306]
</p><p>29 (6), it can be obtained that  ∂ΔI∂(aaqq,bq)= logaq((aapp++ a bqp)+(aq aq++ bq b)q). [sent-84, score-0.265]
</p><p>30 It is not difficult to verify that  (8)  aq((aapp++ a bqp)(+aq a+q+ bq b)q)= 1 ⇐⇒abqq=abpp. [sent-85, score-0.286]
</p><p>31 ationary points, the function ΔI indeed achieves its minimum value of zero, that is, no loss of mutual information. [sent-99, score-0.263]
</p><p>32 Geometrically, all stationary points  (aq, bq) reside on a straight line bq − rpaq = 0, where rp = bp/ap is the class ratio of word− wp. [sent-100, score-0.433]
</p><p>33 Therefore, the pair of words that leads to exact zero loss can be easily identified by simply comparing the class ratios of the two words, that is, whether rp = rq or not. [sent-102, score-0.419]
</p><p>34 However, this result is not as helpful as its first glance, because each word usually does not have an identical class ratio in practice. [sent-103, score-0.413]
</p><p>35 So, the goodness of a word pair may have to be evaluated by explicitly computing the incurred information loss ΔI, as done in AIB and Fast-AIB. [sent-104, score-0.639]
</p><p>36 The dashed red line, bq rpaq = 0, indicates the points where the minimum value of zero is achieved. [sent-120, score-0.34]
</p><p>37 Nevertheless, the above result motivates us to consider the following question: in addition to identifying the word 555555668  pair incurring exact zero information loss, whether a simple comparison of class ratios can be used to suggest good candidate word pairs to merge? [sent-121, score-0.835]
</p><p>38 If yes, this would bring significant computational advantage over the existing methods, where the incurred loss has to be explicitly evaluated for every possible pair of words to identify the optimal pair. [sent-122, score-0.477]
</p><p>39 To answer this question, we examine the relationship between |ri − rj | and ΔI(wi, wj). [sent-123, score-0.163]
</p><p>40 In specific, will a smaller t|rwi − rj r| le−ad r to| alnedss Δ ΔΔII((wwi , wj )? [sent-124, score-0.441]
</p><p>41 In the following, we use  |Trhe−ore rm| 2e adn tdo Corollary 1 to show: when only using the class ratio information of each word, for any given word wi, merging it with the word wj having the smallest |ri −rj | is indeed optimal. [sent-125, score-1.146]
</p><p>42 The optimality i sn gin t hthee s sense to |fr m−inrim|iz i-s ing the “maximum” information loss over all possible word wj(aj, bj) for which bj/aj = rj. [sent-126, score-0.476]
</p><p>43 wj is another word with class ratio rj, where ri ≤ rj ≤ r0 and r0 is a constant. [sent-129, score-0.997]
</p><p>44 No other information on the≤ ≤tw ro w≤ord rs is accessible except ri and rj . [sent-130, score-0.327]
</p><p>45 Let ΔImax (wi , wj) be the maximum information loss caused by merging wi to wj, ΔImax(wi,wj)  ? [sent-131, score-0.518]
</p><p>46 Twhheenre eΔ thIem saext t( Swi, = wj) is achie)ve|rd only wrhen rj = r0. [sent-133, score-0.163]
</p><p>47 addition, this conclusion is also true when ri ≥ rj ≥ r0. [sent-135, score-0.306]
</p><p>48 The set S corresponds to the triangle ΔAOB, which is the iTnhteerss eetct Sion co orrfe ethspreoen half-spaces inng tlhee Δ ΔfirAsOt quadrant: bj − riaj ≥ 0;  bj − r0aj ≤ 0;  aj  + bj  ≤ 1. [sent-140, score-0.259]
</p><p>49 (14)  Note that no tighter upper bound can be set for the last inequality because only the class ratio of each word is accessible. [sent-141, score-0.413]
</p><p>50 Recalling the proof of Theorem 1, it is known that for any stationary point of ΔI(wi, wj), there is  g = 0 ⇐⇒abjj=baii⇐⇒ rj= ri  (15)  and at this time ΔI(wi , wj) achieves its minimum value of zero. [sent-149, score-0.268]
</p><p>51 v = logbajj( abii++ a bjj) ≥ 0,  (16)  where “≥” is achieved because rj ≥ ri is given. [sent-155, score-0.328]
</p><p>52 This concludes that ΔImax (wi, wj ) can only be achieved on OA, which corresponds to wj with rj = r0. [sent-157, score-0.741]
</p><p>53 When the condition becomes ri ≥ rj ≥ r0, the above proof can still be applied except that ≥in rthis≥ case  g? [sent-158, score-0.352]
</p><p>54 v = logbajj( abii++ a bjj) ≤ 0,  (17)  ≤  where “≤” is achieved because rj ri is given. [sent-159, score-0.328]
</p><p>55 This concludes that ΔImax (wi, wj ) can only be achieved on OA? [sent-163, score-0.3]
</p><p>56 For any given word wi, in order to minimi≤zer Δ≤I·m·a·x≤ ≤(wri , wj) the optimal word wj must be either wi+1 or wi−1, one of the two neighbors of wi in L1. [sent-169, score-1.224]
</p><p>57 555555779  Through the above analysis, it can be seen that for any given word wi, we can find an “optimal” word wj to merge by simply comparing the class ratios of the n words. [sent-177, score-1.055]
</p><p>58 , the maximum information loss over all possible wj with class ratio equal to rj) rather than the loss actually incurred by merging the two words. [sent-181, score-0.833]
</p><p>59 This is the price that we pay for only using the class ratios to gain computational advantage. [sent-182, score-0.131]
</p><p>60 Instead, after  and wi+1 are identified, we will simply compare the loss actually incurred by them and pick the smaller one. [sent-185, score-0.219]
</p><p>61 It indicates that once two words are merged, the good candidate word pairs for the next level can be trivially obtained. [sent-187, score-0.575]
</p><p>62 Merging wi (a}i, i sbi) a and wi+1 (ai+1 , bi+1) produces a new word wi? [sent-190, score-0.595]
</p><p>63 This operation does not alter the order of words in the list L1. [sent-192, score-0.199]
</p><p>64 This is indeed true because it is trivial to verify t rhat≤ ≤ ∀a ri , bi, ai+1 , bi+1 > 0, wi−1  abii≤ ri? [sent-200, score-0.164]
</p><p>65 The above result shows that we only need to sort the n words once at the beginning, and the two neighbors of any newly generated word can be trivially identified. [sent-204, score-0.544]
</p><p>66 Recall that ΔI(wi, wj ) denotes the loss of mutual information actually incurred by merging words wi and wj . [sent-208, score-1.437]
</p><p>67 Given a set of n words {wi (ai, bi) }in=1 , sort them Gbaivseedn on stheteir o fcl nass w wraortidoss ri to obtai)n} the ordered list  L1 = {w1, w2 , · · · , wn}. [sent-210, score-0.342]
</p><p>68 Merge wi+1 into wi haned remove wi+1 vfraolmue eL i1n . [sent-218, score-0.266]
</p><p>69 As shown by Theorem 3, the order of words in L1 does not change, aTnhde othreemre 3fo,re th teh oer dtwero o neighbors oLf the new word wi can be trivially identified. [sent-225, score-0.81]
</p><p>70 For the convenience of presentation, all the words in L1 are re-numbered as w1, w2 , ·t ·i o· , wn by following t hLeir order in L1. [sent-226, score-0.221]
</p><p>71 Evaluate ΔI for merging the new word with its two neighbors in the list L1, respectively. [sent-228, score-0.465]
</p><p>72 To gain computational advantage, our algorithm identifies good candidate word pairs based on the worst-case loss instead of the loss actually incurred. [sent-249, score-0.649]
</p><p>73 Surprisingly, sometimes it could even  slightly improve the performance, especially when a large number of words are to be merged. [sent-251, score-0.194]
</p><p>74 Our explanation is that the approximation in our algorithm may implicitly introduce a mild “regularization” effect, which only requires the loss to be small rather than strictly minimum as in AIB and Fast-AIB. [sent-252, score-0.196]
</p><p>75 Specifically, when the number of words is large, the occurrence of each word in a set of training samples will become sparse. [sent-253, score-0.497]
</p><p>76 In the case of small sample size, minimizing ΔI based on the limited training data to determine the optimal word pair is prone to overfitting the samples, and the generated new words may not generalize well on test data. [sent-257, score-0.55]
</p><p>77 Our motivation for FA-AIB-s lies in that we want to test how far a simple comparison of the class ratios can go in appropriately clustering words. [sent-262, score-0.157]
</p><p>78 Instead of selecting the optimal pair (wi, wi+1) as the one having the minimum ΔI, FA-AIB-s simply selects the optimal pair as the one having the minimum Δr = |ri − ri+1 | . [sent-263, score-0.176]
</p><p>79 The computational load includes the time and memory spent in hierarchically merging n words to two word clusters only. [sent-271, score-0.686]
</p><p>80 The classification performance is evaluated by both classification error rate and the mean Average Precision (mAP), obtained by classifying the data with the words clustered at each level of the hierarchy. [sent-272, score-0.293]
</p><p>81 Also, the mutual information loss incurred by each algorithm in the clustering process is compared on both training and test data sets. [sent-273, score-0.405]
</p><p>82 Data sets One synthetic and three real data sets are used, where dif-  ×  ferent number (from 1000 to 60, 000) of words (or features) are clustered. [sent-280, score-0.244]
</p><p>83 Ap viziesu oalf vocabulary of 1000 words is created by applying k-means clustering to the extracted SIFT features. [sent-291, score-0.231]
</p><p>84 A visual vocabulary of 4000 words is generated in the same way as described in Caltech101 . [sent-300, score-0.168]
</p><p>85 Comparison of computational efficiency This experiment verifies on both synthetic and real data sets that our algorithms achieve higher computational efficiency. [sent-317, score-0.169]
</p><p>86 The x-axis is the number of words to be clustered and the y-axis is the time spent by each algorithm. [sent-319, score-0.211]
</p><p>87 Time cost on synthetic dataset (in second) # of words  Fast-AIB [7]  1000 5000 10000 20000 50000 100000  0. [sent-330, score-0.194]
</p><p>88 Time cost on three real datasets (in second)  Dataset  # of words  FastAIB [7]  FAAIB  FAAIB-s  Caltech101 VOC07 VOC07 20News-  1000 4000 32000 62061  0. [sent-349, score-0.168]
</p><p>89 Moreover, it even shows slightly better performance on Caltech101 and 20Newsgroups when the words are merged into a smaller number of word clusters. [sent-373, score-0.555]
</p><p>90 On most data sets, it does not incur significant performance degradation until the words are clustered to a small number of clusters. [sent-375, score-0.217]
</p><p>91 To gain insight into the clustering process, we also plot (in the logarithm of 10) the accumulative mutual information loss at each level of the hierarchy on both training and test sets in Figure 5. [sent-377, score-0.337]
</p><p>92 As expected, the loss monotonically increases with the progress of clustering. [sent-378, score-0.126]
</p><p>93 Comparatively, FA-AIB-s incurs a bit more loss as expected. [sent-381, score-0.151]
</p><p>94 On 20Newsgroups, Fast-AIB and FA-AIB also produce similar information loss on the training set, and FA-AIB-s incurs slightly more loss. [sent-383, score-0.198]
</p><p>95 It is worth mentioning that on 20Newsgroups, FA-AIB consistently produces lower information loss on the test set, which agrees well with its slightly better classification performance shown in Figure 4. [sent-384, score-0.225]
</p><p>96 Conclusion and future work  We propose a fast algorithm for agglomerative information bottleneck to do distributional word clustering. [sent-387, score-0.603]
</p><p>97 Instead of explicitly computing the mutual information loss incurred by merging each pair of words, our algorithm simply utilizes the class ratio of each word to find good candidate word pairs. [sent-388, score-1.251]
</p><p>98 The future work will systematically extend this idea to multi-class classification and to other information-theoretic methods where finding the minimum mutual information loss is intensively needed. [sent-390, score-0.336]
</p><p>99 Supervised learning of quantizer codebooks by information loss minimization. [sent-472, score-0.168]
</p><p>100 Comparison of mutual information loss on training (left) and test (right) data on four datasets at each row: Caltech101, VOC07(4000 words), VOC07(32, 000 words), 20Newsgroups. [sent-492, score-0.249]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aib', 0.38), ('word', 0.329), ('aq', 0.28), ('wj', 0.278), ('wi', 0.266), ('bq', 0.265), ('imax', 0.178), ('words', 0.168), ('rj', 0.163), ('wq', 0.161), ('distributional', 0.148), ('ri', 0.143), ('loss', 0.126), ('abii', 0.121), ('wpq', 0.114), ('merging', 0.105), ('mutual', 0.102), ('wp', 0.101), ('abqq', 0.101), ('bp', 0.098), ('incurred', 0.093), ('bj', 0.07), ('theorem', 0.067), ('bi', 0.063), ('clustering', 0.063), ('tbia', 0.06), ('agglomerative', 0.06), ('ratios', 0.056), ('corollary', 0.055), ('ap', 0.053), ('wn', 0.053), ('classification', 0.052), ('text', 0.05), ('aj', 0.049), ('trivially', 0.047), ('ratio', 0.046), ('proof', 0.046), ('bottleneck', 0.045), ('stationary', 0.044), ('ai', 0.043), ('aapp', 0.04), ('aapq', 0.04), ('aaqp', 0.04), ('abpp', 0.04), ('bbpq', 0.04), ('bbqp', 0.04), ('bjj', 0.04), ('logbajj', 0.04), ('rpaq', 0.04), ('rrat', 0.04), ('wmahxeirme', 0.04), ('goodness', 0.039), ('class', 0.038), ('computational', 0.037), ('bqp', 0.036), ('wollongong', 0.036), ('mild', 0.035), ('minimum', 0.035), ('apa', 0.033), ('merged', 0.032), ('candidate', 0.031), ('completes', 0.031), ('wor', 0.031), ('pair', 0.031), ('list', 0.031), ('complexity', 0.03), ('symmetrical', 0.03), ('merely', 0.03), ('incur', 0.028), ('twhe', 0.028), ('sno', 0.027), ('merges', 0.027), ('ns', 0.026), ('slightly', 0.026), ('synthetic', 0.026), ('pages', 0.026), ('ba', 0.026), ('ohef', 0.026), ('probabilities', 0.025), ('merge', 0.025), ('documents', 0.025), ('load', 0.025), ('sets', 0.025), ('compact', 0.025), ('sigir', 0.025), ('incurs', 0.025), ('verifies', 0.023), ('optimal', 0.022), ('oa', 0.022), ('simplified', 0.022), ('shall', 0.022), ('spent', 0.022), ('timing', 0.022), ('achieved', 0.022), ('coarser', 0.021), ('information', 0.021), ('verify', 0.021), ('clustered', 0.021), ('codebooks', 0.021), ('efficiency', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="8-tfidf-1" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>2 0.13699675 <a title="8-tfidf-2" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>3 0.12072351 <a title="8-tfidf-3" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>4 0.11258411 <a title="8-tfidf-4" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>Author: Quannan Li, Jiajun Wu, Zhuowen Tu</p><p>Abstract: Obtaining effective mid-level representations has become an increasingly important task in computer vision. In this paper, we propose a fully automatic algorithm which harvests visual concepts from a large number of Internet images (more than a quarter of a million) using text-based queries. Existing approaches to visual concept learning from Internet images either rely on strong supervision with detailed manual annotations or learn image-level classifiers only. Here, we take the advantage of having massive wellorganized Google and Bing image data; visual concepts (around 14, 000) are automatically exploited from images using word-based queries. Using the learned visual concepts, we show state-of-the-art performances on a variety of benchmark datasets, which demonstrate the effectiveness of the learned mid-level representations: being able to generalize well to general natural images. Our method shows significant improvement over the competing systems in image classification, including those with strong supervision.</p><p>5 0.099796318 <a title="8-tfidf-5" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>6 0.097484685 <a title="8-tfidf-6" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>7 0.09619277 <a title="8-tfidf-7" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>8 0.093117699 <a title="8-tfidf-8" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>9 0.092825174 <a title="8-tfidf-9" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>10 0.089607857 <a title="8-tfidf-10" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>11 0.082748622 <a title="8-tfidf-11" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>12 0.075307839 <a title="8-tfidf-12" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>13 0.075010523 <a title="8-tfidf-13" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>14 0.061015978 <a title="8-tfidf-14" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>15 0.060869139 <a title="8-tfidf-15" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>16 0.059025086 <a title="8-tfidf-16" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>17 0.056468762 <a title="8-tfidf-17" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>18 0.052560605 <a title="8-tfidf-18" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>19 0.049769357 <a title="8-tfidf-19" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>20 0.049676076 <a title="8-tfidf-20" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, -0.033), (2, -0.014), (3, 0.022), (4, 0.043), (5, 0.007), (6, -0.033), (7, -0.018), (8, -0.055), (9, 0.002), (10, 0.001), (11, 0.004), (12, -0.004), (13, -0.037), (14, -0.011), (15, -0.029), (16, 0.024), (17, 0.023), (18, 0.08), (19, -0.071), (20, 0.072), (21, -0.017), (22, 0.022), (23, -0.021), (24, 0.019), (25, 0.051), (26, -0.022), (27, 0.036), (28, -0.009), (29, -0.047), (30, 0.075), (31, 0.088), (32, -0.034), (33, 0.092), (34, 0.02), (35, 0.025), (36, -0.059), (37, 0.086), (38, -0.012), (39, -0.09), (40, -0.053), (41, -0.122), (42, -0.076), (43, -0.038), (44, -0.015), (45, 0.076), (46, -0.008), (47, -0.033), (48, 0.053), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95073092 <a title="8-lsi-1" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>2 0.78829122 <a title="8-lsi-2" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>Author: Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian</p><p>Abstract: The Inverse Document Frequency (IDF) is prevalently utilized in the Bag-of-Words based image search. The basic idea is to assign less weight to terms with high frequency, and vice versa. However, the estimation of visual word frequency is coarse and heuristic. Therefore, the effectiveness of the conventional IDF routine is marginal, and far from optimal. To tackle thisproblem, thispaper introduces a novel IDF expression by the use of Lp-norm pooling technique. . edu . cn qit i @ c s an . ut s a . edu ? ? ? ? ? ? ? ? Carefully designed, the proposed IDF takes into account the term frequency, document frequency, the complexity of images, as well as the codebook information. Optimizing the IDF function towards optimal balancing between TF and pIDF weights yields the so-called Lp-norm IDF (pIDF). WpIDe sFho wwe ithghatts sth yeie clodsnv tehnetio son-acla IlDleFd i Ls a special case of our generalized version, and two novel IDFs, i.e. the average IDF and the max IDF, can also be derived from our formula. Further, by counting for the term-frequency in each image, the proposed Lp-norm IDF helps to alleviate the viismuaalg we,o trhde b purrosptionseesds phenomenon. Our method is evaluated through extensive experiments on three benchmark datasets (Oxford 5K, Paris 6K and Flickr 1M). We report a performance improvement of as large as 27.1% over the baseline approach. Moreover, since the Lp-norm IDF is computed offline, no extra computation or memory cost is introduced to the system at all.</p><p>3 0.69485128 <a title="8-lsi-3" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>4 0.65356451 <a title="8-lsi-4" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>Author: Quannan Li, Jiajun Wu, Zhuowen Tu</p><p>Abstract: Obtaining effective mid-level representations has become an increasingly important task in computer vision. In this paper, we propose a fully automatic algorithm which harvests visual concepts from a large number of Internet images (more than a quarter of a million) using text-based queries. Existing approaches to visual concept learning from Internet images either rely on strong supervision with detailed manual annotations or learn image-level classifiers only. Here, we take the advantage of having massive wellorganized Google and Bing image data; visual concepts (around 14, 000) are automatically exploited from images using word-based queries. Using the learned visual concepts, we show state-of-the-art performances on a variety of benchmark datasets, which demonstrate the effectiveness of the learned mid-level representations: being able to generalize well to general natural images. Our method shows significant improvement over the competing systems in image classification, including those with strong supervision.</p><p>5 0.65271515 <a title="8-lsi-5" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>6 0.6362735 <a title="8-lsi-6" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>7 0.62133664 <a title="8-lsi-7" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>8 0.6185891 <a title="8-lsi-8" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>9 0.58588475 <a title="8-lsi-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.5838933 <a title="8-lsi-10" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>11 0.58202595 <a title="8-lsi-11" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>12 0.56692761 <a title="8-lsi-12" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>13 0.55751359 <a title="8-lsi-13" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>14 0.55588341 <a title="8-lsi-14" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>15 0.53616315 <a title="8-lsi-15" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>16 0.52616262 <a title="8-lsi-16" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>17 0.52310449 <a title="8-lsi-17" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>18 0.50659424 <a title="8-lsi-18" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>19 0.50076717 <a title="8-lsi-19" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>20 0.49784487 <a title="8-lsi-20" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.107), (16, 0.016), (26, 0.053), (33, 0.25), (50, 0.263), (67, 0.063), (69, 0.062), (80, 0.014), (87, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81621665 <a title="8-lda-1" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>2 0.8062079 <a title="8-lda-2" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>Author: Jian Dong, Wei Xia, Qiang Chen, Jianshi Feng, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In this paper, we introduce a subcategory-aware object classification framework to boost category level object classification performance. Motivated by the observation of considerable intra-class diversities and inter-class ambiguities in many current object classification datasets, we explicitly split data into subcategories by ambiguity guided subcategory mining. We then train an individual model for each subcategory rather than attempt to represent an object category with a monolithic model. More specifically, we build the instance affinity graph by combining both intraclass similarity and inter-class ambiguity. Visual subcategories, which correspond to the dense subgraphs, are detected by the graph shift algorithm and seamlessly integrated into the state-of-the-art detection assisted classification framework. Finally the responses from subcategory models are aggregated by subcategory-aware kernel regression. The extensive experiments over the PASCAL VOC 2007 and PASCAL VOC 2010 databases show the state-ofthe-art performance from our framework.</p><p>3 0.79605955 <a title="8-lda-3" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>4 0.79241669 <a title="8-lda-4" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>Author: Michalis Raptis, Leonid Sigal</p><p>Abstract: In this paper, we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework, where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and minefor hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive with the state of the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.</p><p>5 0.78852934 <a title="8-lda-5" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>Author: Yen H. Le, Uday Kurkure, Ioannis A. Kakadiaris</p><p>Abstract: Statistical shape models, such as Active Shape Models (ASMs), sufferfrom their inability to represent a large range of variations of a complex shape and to account for the large errors in detection of model points. We propose a novel method (dubbed PDM-ENLOR) that overcomes these limitations by locating each shape model point individually using an ensemble of local regression models and appearance cues from selected model points. Our method first detects a set of reference points which were selected based on their saliency during training. For each model point, an ensemble of regressors is built. From the locations of the detected reference points, each regressor infers a candidate location for that model point using local geometric constraints, encoded by a point distribution model (PDM). The final location of that point is determined as a weighted linear combination, whose coefficients are learnt from the training data, of candidates proposed from its ensemble ’s component regressors. We use different subsets of reference points as explanatory variables for the component regressors to provide varying degrees of locality for the models in each ensemble. This helps our ensemble model to capture a larger range of shape variations as compared to a single PDM. We demonstrate the advantages of our method on the challenging problem of segmenting gene expression images of mouse brain.</p><p>6 0.75622076 <a title="8-lda-6" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>7 0.74518442 <a title="8-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.74122691 <a title="8-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.74103791 <a title="8-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.73853153 <a title="8-lda-10" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>11 0.73841041 <a title="8-lda-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.73816627 <a title="8-lda-12" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>13 0.73812324 <a title="8-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.73771846 <a title="8-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.73771429 <a title="8-lda-15" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>16 0.73745775 <a title="8-lda-16" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>17 0.73736274 <a title="8-lda-17" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>18 0.73720062 <a title="8-lda-18" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>19 0.7368142 <a title="8-lda-19" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>20 0.73629248 <a title="8-lda-20" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
