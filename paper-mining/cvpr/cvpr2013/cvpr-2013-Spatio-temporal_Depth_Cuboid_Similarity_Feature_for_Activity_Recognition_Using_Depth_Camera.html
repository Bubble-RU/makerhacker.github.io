<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-407" href="#">cvpr2013-407</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</h1>
<br/><p>Source: <a title="cvpr-2013-407-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xia_Spatio-temporal_Depth_Cuboid_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>Reference: <a title="cvpr-2013-407-reference" href="../cvpr2013_reference/cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. [sent-6, score-0.244]
</p><p>2 In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. [sent-7, score-0.407]
</p><p>3 We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. [sent-8, score-0.372]
</p><p>4 Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. [sent-9, score-1.74]
</p><p>5 We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. [sent-10, score-0.153]
</p><p>6 Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. [sent-11, score-0.405]
</p><p>7 With the recent advent of the cost-effective Kinect, depth cameras have received a great deal of attention from researchers. [sent-22, score-0.278]
</p><p>8 The depth sensor has several advantages over visible light camera. [sent-24, score-0.278]
</p><p>9 Second, the depth camera can work in total darkness. [sent-27, score-0.278]
</p><p>10 These advantages lead to interesting research such as estimating human skeletons from  a single depth image [21]. [sent-29, score-0.341]
</p><p>11 The skeletons estimated from depth images are quite accurate under experimental settings and bring benefits to many applications including activity recognition, but the algorithm is limited at the same time. [sent-30, score-0.405]
</p><p>12 In this paper, we present algorithm for extracting STIPs from depth videos (DSTIPs) and describing local 3D depth cuboid using the Depth Cuboid Similarity Feature (DCSF). [sent-35, score-1.15]
</p><p>13 The DSTIP and DCSF can be effectively used to recognize activities without the dependence on skeleton tracking, thus they offer greater flexibility. [sent-36, score-0.217]
</p><p>14 Our contribution can be summarized as follows: first, we present an algorithm to extract DSTIPs which deals with the noise in depth videos. [sent-37, score-0.348]
</p><p>15 Second, we present DCSF as a descriptor for the 3D local cuboid in depth videos. [sent-38, score-0.859]
</p><p>16 Third, we show that this DSTIP+DCSF pipeline may be applied to recognize activity from depth videos, with no dependence on the skeletal joints information, motion segmentation, tracking, or denoising procedures. [sent-39, score-0.538]
</p><p>17 The widely used STIP detectors include the Harris3D detector [11], cuboid detector [5], and Hessian detector [28]. [sent-47, score-0.73]
</p><p>18 Although depth data has existed for several decades, the interest point detection and description has stayed at the level of describing static scenes or objects for a long time [24, 16, 23]. [sent-49, score-0.323]
</p><p>19 Until recently, a few spatial-temporal cuboid descriptors for depth videos were proposed. [sent-51, score-0.895]
</p><p>20 [4] build a Comparative Coding Descriptor(CCD) to describe the 3 3 3 depth cuboid by comparing the depth value tohfe th 3e × c3e n ×ter 3 point w cuitbho tihde b nearby 2ri6n points. [sent-53, score-1.078]
</p><p>21 [32] build Local Depth Pattern(LDP) by computing the  difference of the average depth values between the cells. [sent-55, score-0.278]
</p><p>22 In this paper, we propose DCSF as the descriptor for the spatio-temporal depth cuboid that describes the local ”appearances” in the depth video based on self-similarity concept. [sent-56, score-1.168]
</p><p>23 The self-similarity based feature has been proven to work well at object detection/retrieval and action detection on RGB data [20] and object recognition tasks on depth data [8, 9]. [sent-58, score-0.451]
</p><p>24 Activity recognition from depth videos Human activity or gesture recognition using depth images may be divided into two categories: algorithms based on low-level features, and algorithms based on high-level features. [sent-61, score-0.784]
</p><p>25 [13] sample a bag of 3D points from the depth maps to characterize a set of salient postures and employ an action graph to model the dynamics of the actions. [sent-63, score-0.432]
</p><p>26 Several researchers tried the STIP framework on depth videos. [sent-65, score-0.31]
</p><p>27 [14] use depth information to partition the space into layers, extract STIPs from RGB channels of each layer using a Harris3D detector, and use HOGHOF to describe the neighborhood of STIPs also in the RGB channel. [sent-67, score-0.3]
</p><p>28 [32, 7, 4] choose Harris3D detector [11] to extract STIPs from RGB or depth channels. [sent-68, score-0.362]
</p><p>29 [7] uses VFHCRH to describe the 2D depth image patch around the  STIPs, Zhao et al. [sent-70, score-0.303]
</p><p>30 [3 1] extract STIPs by calculating a response function from both depth and RGB channels and use the gradients along x, y, t directions as the descriptor. [sent-73, score-0.34]
</p><p>31 [27] combine joint location features and local occupancy features and employ a Fourier temporal pyramid to represent the temporal dynamics of the actions. [sent-76, score-0.152]
</p><p>32 [29] take the skeletal joint locations and vote them into 3D spatial bins and build posture words for action recognition. [sent-78, score-0.433]
</p><p>33 The second category usually gives better recognition rates, because the high-level skeletal information is well trained and greatly alleviates the difficulties. [sent-79, score-0.185]
</p><p>34 Spatio-Temporal Filtering First, a 2D Gaussian smoothing filter is applied on to the spatial dimensions:  Ds (x, y, t) = D(x, y, t)  ∗  g(x, y | σ)  (1)  where ∗ denotes convolution, D and Ds denote the origiwnahl depth veonloutmese caonndv tohluatt aiofnte,r D spatial filtering respectively. [sent-86, score-0.302]
</p><p>35 Then we apply a temporal filter along the t dimension: σ  Dst (x, y, t) = Ds (x, y, t) ∗ h(t | τ, ω)  ◦  s(x, y, t | τ) (3)  where Dst denotes the depth volume after spatio-temporal filtering. [sent-88, score-0.367]
</p><p>36 depth sequence a at cloocrareticotnio (nx f,u y, ctt)i. [sent-95, score-0.278]
</p><p>37 The reason we choose a correction function instead of using filters is based on the different nature of the noise in depth videos. [sent-189, score-0.376]
</p><p>38 One may divide the noise in depth videos into three categories: The first category of noise comes from the variation of the sensing device, which is evenly distributed throughout the entire image, the magnitude of which is comparatively small. [sent-190, score-0.497]
</p><p>39 The second category of noise occurs around the boundary of objects, the values jump from the depth of the background to the depth of the foreground, back and forth frequently. [sent-191, score-0.655]
</p><p>40 The third category of noise is the ”holes” that appear in the depth images, caused by special reflectance materials, fast movements, porous  surfaces, and other random effects. [sent-193, score-0.377]
</p><p>41 The flip of the signal caused by sensor noise usually happens much faster than human movements, and it can happen from once to dozens of times during the whole video. [sent-199, score-0.161]
</p><p>42 Interest Point Description Here we propose a descriptor for the local 3D cuboid centered at DSTIP. [sent-218, score-0.581]
</p><p>43 Note it is 3D instead of 4D because the depth image is a function of x and y, not all 3D points {x, y, z}, but it still provides useful information along the z {dixm,yen,zsi}o,n b. [sent-219, score-0.278]
</p><p>44 222888333644  Figure 3: Illustration of extracting DCSF from depth video 4. [sent-220, score-0.309]
</p><p>45 Adaptable supporting size We extract a 3D cuboid which contains the spatiotemporally windowed pixel values around the DSTIP. [sent-222, score-0.569]
</p><p>46 Considering objects appear smaller in the image at a farther distance, we design the cuboid size to be adaptable to the depth. [sent-223, score-0.659]
</p><p>47 We define the spatial size of the cuboid to be proportional to the scale at which it was detected and inversely proportional to the depth at which it locates:  Δ(xi)= Δy(i)= σdL(i)  (10)  where σ is the scale at which the i-th cuboid was detected. [sent-224, score-1.322]
</p><p>48 Notice that we do not take the depth pixel value at the interest point D(xi, yi, ti) as d(i) , because the DSTIP sometimes lands at the edge of body parts. [sent-226, score-0.382]
</p><p>49 Instead, we compute the minimum non-zero depth value in the 2τ time interval round the location (xi, yi, ti), i. [sent-227, score-0.302]
</p><p>50 The side length of the temporal dimension of a cuboid is simply defined as:  Δt(i)  = 2τ  (11)  4. [sent-240, score-0.561]
</p><p>51 Depth cuboid similarity feature  Different from RGB data, depth data lacks texture, and is inherently noisy. [sent-242, score-0.826]
</p><p>52 We define the block as containing 1 1 1to nxy nxy nt vdoefxienels. [sent-247, score-0.599]
</p><p>53 t We compute a histogram of the depth pixels contained in each block, normalize them to make the total value of every histogram to be 1. [sent-248, score-0.348]
</p><p>54 hp(n)h(qn)  (12)  ×  which describes the depth relationship of the two blocks. [sent-250, score-0.278]
</p><p>55 Note in this definition, the length of the feature depends on nxy and nt only, it does not relate to the actual size of the cuboid which offers greater freedom for the interest point detection and the cuboid extraction process. [sent-252, score-1.455]
</p><p>56 Varying spatialsize from 1 1 to nxy nxy gives nxy (nxy 1) (2nxy 1si)z/e6 f possibilities, varying temporal-size from− −1 1to) nt gives nt (nt + 1)/2 possibilities. [sent-254, score-0.946]
</p><p>57 nt2nx3y/6, and the total length of the DCSF  To reduce computational cost, we use integral histograms [17] to compute the depth histograms rapidly. [sent-256, score-0.278]
</p><p>58 We quantize the depth pixels into M bins, M = (dmax dmin)/Δd, where Δd is chosen according to the spatial level of movements to recognize, e. [sent-257, score-0.351]
</p><p>59 We intentionally do not rotate the cuboid itself to retain the direction of the movements so that 222888333755  we can distinguish between actions such as stand up and sit down. [sent-275, score-0.728]
</p><p>60 Cuboid codebook Inspired by the successful bag-of-words approach at RGB image classification and retrieval, we build a cuboid codebook by clustering the DCSF using K-means algorithm with Euclidean distance. [sent-280, score-0.566]
</p><p>61 Thus, each depth sequence can be represented as a bag-of-codewords from the codebook. [sent-282, score-0.278]
</p><p>62 These bag-of-codewords describe what’s happening in the depth sequences in a simple yet powerful way. [sent-283, score-0.278]
</p><p>63 We use a histogram of the cuboid prototypes as the action descriptor and SVM [3] for classification with histogram intersection kernel: K(a, b) = Σin=1min(ai, bi) , ai ≥ 0, bi ≥ 0 5. [sent-287, score-0.808]
</p><p>64 Mining discriminative feature pool  (16)  Not all the cuboid prototypes give the same level of discrimination among different actions, some cuboids may be related with movements that do not offer good discrimination among different actions, e. [sent-289, score-0.758]
</p><p>65 the cuboid prototypes by their F-scores and select features with high F-scores. [sent-303, score-0.561]
</p><p>66 In our experiments, small improvement is observed by deleting 1-2% cuboid prototypes. [sent-306, score-0.522]
</p><p>67 We compare our algorithm with stateof-the-art methods on activity recognition algorithms from depth videos [13, 27, 30, 25, 26]. [sent-310, score-0.477]
</p><p>68 The depth image is clean, there are no background objects, and the subjects appear at the same depth to the camera. [sent-317, score-0.612]
</p><p>69 On this dataset, we take σ = 5, σ = T/27 and T/17 (T denotes the duration of the action sequence) and Np = 160 for DSTIP extraction, and take the number of voxels for each cuboid to be nxy = 4, nt = 2. [sent-318, score-1.114]
</p><p>70 We fix the cuboid spatial size Δx = Δy = 6σ because all actions take place at the same depth. [sent-319, score-0.626]
</p><p>71 Our algorithm outperforms the algorithms based on 3D silhouette features [13], skeletal joint features [30, 27] and local occupancy patterns[25, 26]. [sent-322, score-0.173]
</p><p>72 (This can happen in action type: sit still, read books, write on paper and use laptop). [sent-330, score-0.151]
</p><p>73 ’s algorithm [13] cannot work without segmenting out the human subjects from the depth image, which is not a trivial work considering the human appears at different depths and interacts with objects. [sent-332, score-0.377]
</p><p>74 We take σ = 5, τ = T/17, Np = 500 for DSTIP extraction and take the number of voxels for each cuboid to be nxy = 4, nt = 3 . [sent-337, score-0.968]
</p><p>75 For the Cuboid descriptor, we use a fixed cuboid size Δx = Δy = 6σ, because it does not handle different sizes. [sent-347, score-0.522]
</p><p>76 For the HOG descriptor, we incorporate the adaptable cuboid size and take nxy = 6, nt = 4 and use 4-bin histograms of gradient orientations, which is the best parameter for HOG on this dataset. [sent-348, score-0.956]
</p><p>77 Our DCSF descriptor performs significantly better than the Cuboid descriptor or gradient based descriptor even with adaptable cuboid size. [sent-349, score-0.788]
</p><p>78 We also compared our DSTIP detector with widely used detectors in RGB images, including the Harris3D detector [11] and Cuboid detector [5]. [sent-351, score-0.208]
</p><p>79 Figure 5 shows the STIPs extracted by the Cuboid detector and our DSTIP detector when take the STIPs at local maximum with the top 50,100,200,300,500,800 response values. [sent-353, score-0.192]
</p><p>80 For the tool to work, we smooth and scale the depth pixels to 0-255. [sent-359, score-0.278]
</p><p>81 of voxels for a cuboid nxy, nt, support region L, and codebook size k. [sent-367, score-0.594]
</p><p>82 Each action was collected from 10 different persons each performing the actions 3 times. [sent-372, score-0.194]
</p><p>83 nd W taek tea kthee σ σnu =mb 5e,r1 0of, voxels for each cuboid to be nxy = 4, nt = 2. [sent-377, score-0.889]
</p><p>84 There is no skeleton information recorded so skeleton feature based algorithms [27, 30] cannot be applied onto it. [sent-454, score-0.182]
</p><p>85 On this dataset, we tried another method in which we take the 3D point clouds of the whole body in each frame and map it to a posture word. [sent-455, score-0.191]
</p><p>86 Then each action is represented by a sequence of posture words and we classify upon that (we  Test OneTest TwoCross Subject  DSTIP+DCSF93. [sent-456, score-0.218]
</p><p>87 By this means, our algorithm is a more general approach to processing depth videos and recognizing activities, which may also be used for a 222888334088  wider variety of settings, e. [sent-468, score-0.35]
</p><p>88 Discussion and Conclusions This paper presents algorithm to extract DSTIPs from depth videos and to calculate descriptors for the local 3D depth cuboid around the DSTIPs. [sent-472, score-1.22]
</p><p>89 The descriptor can be used to recognize actions of either humans or animals with no dependence on skeleton information or preprocessing  like human detection, motion segmentation, tracking, or even image denoising or hole-filling. [sent-473, score-0.333]
</p><p>90 As shown in the experiment, when skeletal joint information is available, the DCSF can be concatenated with the joint features to bring more accurate recognition results. [sent-477, score-0.186]
</p><p>91 Additionally, the STIP locations extracted from the depth videos and RGB videos can be combined or filtered to provide more discriminate interest point locations and thus render better recognition performance. [sent-480, score-0.548]
</p><p>92 Human daily action analysis with multi-view and color-depth data. [sent-509, score-0.15]
</p><p>93 Rgbd-hudaact: A color-depth video database for human daily activity recognition. [sent-572, score-0.195]
</p><p>94 Unsupervised learning of human action categories using spatial-temporal words. [sent-578, score-0.152]
</p><p>95 Real-time human pose recognition in parts from a single depth image. [sent-619, score-0.341]
</p><p>96 Interest point detection in depth images through scale-space surface analysis. [sent-631, score-0.278]
</p><p>97 Stop: Space-time occupancy patterns for 3d action recognition from depth map sequences. [sent-647, score-0.47]
</p><p>98 Mining actionlet ensemble for action recognition with depth cameras. [sent-663, score-0.425]
</p><p>99 View invariant human action  recognition using histograms of 3d joints. [sent-676, score-0.181]
</p><p>100 Combing rgb and depth map features for human activity recognition. [sent-693, score-0.519]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cuboid', 0.522), ('dcsf', 0.363), ('dstip', 0.322), ('depth', 0.278), ('nxy', 0.25), ('stips', 0.223), ('dstips', 0.161), ('action', 0.118), ('rgb', 0.109), ('posture', 0.1), ('skeletal', 0.099), ('activity', 0.098), ('stip', 0.096), ('adaptable', 0.089), ('skeleton', 0.078), ('actions', 0.076), ('movements', 0.073), ('videos', 0.072), ('msrdaily', 0.072), ('nt', 0.067), ('detector', 0.062), ('hoghof', 0.06), ('descriptor', 0.059), ('np', 0.055), ('flips', 0.054), ('activities', 0.052), ('voxels', 0.05), ('correction', 0.05), ('noise', 0.048), ('dst', 0.047), ('interest', 0.045), ('occupancy', 0.045), ('rusu', 0.045), ('pages', 0.044), ('ldp', 0.04), ('narf', 0.04), ('aggarwal', 0.04), ('xia', 0.04), ('response', 0.04), ('temporal', 0.039), ('prototypes', 0.039), ('iros', 0.037), ('lop', 0.036), ('postures', 0.036), ('histogram', 0.035), ('human', 0.034), ('bins', 0.033), ('fpfh', 0.033), ('sit', 0.033), ('block', 0.032), ('recognize', 0.032), ('tried', 0.032), ('daily', 0.032), ('body', 0.031), ('movement', 0.031), ('dependence', 0.031), ('video', 0.031), ('subjects', 0.031), ('gives', 0.031), ('signal', 0.03), ('icra', 0.029), ('recognition', 0.029), ('mm', 0.029), ('joint', 0.029), ('skeletons', 0.029), ('take', 0.028), ('duration', 0.027), ('locates', 0.027), ('texas', 0.027), ('volume', 0.026), ('locations', 0.026), ('feature', 0.026), ('category', 0.026), ('flip', 0.026), ('appear', 0.025), ('around', 0.025), ('magnitude', 0.025), ('collects', 0.025), ('bar', 0.025), ('hp', 0.025), ('discrimination', 0.025), ('behavior', 0.024), ('offer', 0.024), ('cuboids', 0.024), ('stand', 0.024), ('renders', 0.024), ('ti', 0.024), ('denotes', 0.024), ('interval', 0.024), ('noises', 0.023), ('hardly', 0.023), ('descriptors', 0.023), ('preprocessing', 0.023), ('ds', 0.023), ('farther', 0.023), ('happens', 0.023), ('extraction', 0.023), ('codebook', 0.022), ('detectors', 0.022), ('extract', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="407-tfidf-1" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>2 0.42094848 <a title="407-tfidf-2" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>Author: Hao Jiang, Jianxiong Xiao</p><p>Abstract: We propose a novel linear method to match cuboids in indoor scenes using RGBD images from Kinect. Beyond depth maps, these cuboids reveal important structures of a scene. Instead of directly fitting cuboids to 3D data, we first construct cuboid candidates using superpixel pairs on a RGBD image, and then we optimize the configuration of the cuboids to satisfy the global structure constraints. The optimal configuration has low local matching costs, small object intersection and occlusion, and the cuboids tend to project to a large region in the image; the number of cuboids is optimized simultaneously. We formulate the multiple cuboid matching problem as a mixed integer linear program and solve the optimization efficiently with a branch and bound method. The optimization guarantees the global optimal solution. Our experiments on the Kinect RGBD images of a variety of indoor scenes show that our proposed method is efficient, accurate and robust against object appearance variations, occlusions and strong clutter.</p><p>3 0.24929547 <a title="407-tfidf-3" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><p>4 0.24290441 <a title="407-tfidf-4" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>5 0.20862515 <a title="407-tfidf-5" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><p>6 0.20310767 <a title="407-tfidf-6" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>7 0.16168894 <a title="407-tfidf-7" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>8 0.14563735 <a title="407-tfidf-8" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>9 0.13856664 <a title="407-tfidf-9" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>10 0.13823162 <a title="407-tfidf-10" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>11 0.13503623 <a title="407-tfidf-11" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>12 0.13304092 <a title="407-tfidf-12" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>13 0.13121334 <a title="407-tfidf-13" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>14 0.13021582 <a title="407-tfidf-14" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>15 0.12616992 <a title="407-tfidf-15" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>16 0.12405162 <a title="407-tfidf-16" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>17 0.12297186 <a title="407-tfidf-17" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>18 0.11994789 <a title="407-tfidf-18" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>19 0.11518624 <a title="407-tfidf-19" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>20 0.11307342 <a title="407-tfidf-20" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.214), (1, 0.06), (2, 0.015), (3, -0.096), (4, -0.221), (5, -0.027), (6, -0.101), (7, 0.109), (8, -0.039), (9, -0.052), (10, -0.037), (11, -0.122), (12, 0.011), (13, 0.124), (14, 0.018), (15, -0.065), (16, -0.152), (17, 0.086), (18, -0.017), (19, -0.006), (20, 0.017), (21, 0.029), (22, -0.012), (23, -0.007), (24, 0.068), (25, 0.062), (26, -0.019), (27, 0.015), (28, -0.029), (29, 0.004), (30, -0.045), (31, 0.135), (32, 0.113), (33, -0.083), (34, -0.015), (35, -0.067), (36, -0.007), (37, 0.113), (38, -0.029), (39, -0.024), (40, 0.009), (41, -0.04), (42, 0.078), (43, 0.097), (44, -0.077), (45, -0.041), (46, 0.051), (47, 0.052), (48, -0.019), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94364029 <a title="407-lsi-1" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>2 0.82063973 <a title="407-lsi-2" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><p>3 0.77241415 <a title="407-lsi-3" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>Author: Simon Hadfield, Richard Bowden</p><p>Abstract: Action recognition in unconstrained situations is a difficult task, suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent emergence of 3D data, both in broadcast content, and commercial depth sensors, provides the possibility to overcome this issue. This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use ofthe 3D data, andfive new interestpoint detection strategies are alsoproposed, that extend to the 3D data. Our evaluation compares all 4 feature descriptors, using 7 different types of interest point, over a variety of threshold levels, for the Hollywood3D dataset. We make the dataset including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community.</p><p>4 0.72445047 <a title="407-lsi-4" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>Author: Hao Jiang, Jianxiong Xiao</p><p>Abstract: We propose a novel linear method to match cuboids in indoor scenes using RGBD images from Kinect. Beyond depth maps, these cuboids reveal important structures of a scene. Instead of directly fitting cuboids to 3D data, we first construct cuboid candidates using superpixel pairs on a RGBD image, and then we optimize the configuration of the cuboids to satisfy the global structure constraints. The optimal configuration has low local matching costs, small object intersection and occlusion, and the cuboids tend to project to a large region in the image; the number of cuboids is optimized simultaneously. We formulate the multiple cuboid matching problem as a mixed integer linear program and solve the optimization efficiently with a branch and bound method. The optimization guarantees the global optimal solution. Our experiments on the Kinect RGBD images of a variety of indoor scenes show that our proposed method is efficient, accurate and robust against object appearance variations, occlusions and strong clutter.</p><p>5 0.71572548 <a title="407-lsi-5" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>6 0.69033772 <a title="407-lsi-6" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>7 0.66378075 <a title="407-lsi-7" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>8 0.63566571 <a title="407-lsi-8" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>9 0.6229853 <a title="407-lsi-9" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>10 0.59909463 <a title="407-lsi-10" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>11 0.58262247 <a title="407-lsi-11" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>12 0.54744881 <a title="407-lsi-12" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>13 0.53756285 <a title="407-lsi-13" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>14 0.51716304 <a title="407-lsi-14" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>15 0.50445294 <a title="407-lsi-15" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>16 0.49906114 <a title="407-lsi-16" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>17 0.488262 <a title="407-lsi-17" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>18 0.46650392 <a title="407-lsi-18" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>19 0.46461549 <a title="407-lsi-19" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>20 0.45472014 <a title="407-lsi-20" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.137), (16, 0.022), (26, 0.041), (28, 0.012), (33, 0.249), (39, 0.011), (40, 0.206), (67, 0.08), (69, 0.053), (72, 0.029), (87, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88076913 <a title="407-lda-1" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>Author: Rui Caseiro, Pedro Martins, João F. Henriques, Fátima Silva Leite, Jorge Batista</p><p>Abstract: In the past few years there has been a growing interest on geometric frameworks to learn supervised classification models on Riemannian manifolds [31, 27]. A popular framework, valid over any Riemannian manifold, was proposed in [31] for binary classification. Once moving from binary to multi-class classification thisparadigm is not valid anymore, due to the spread of multiple positive classes on the manifold [27]. It is then natural to ask whether the multi-class paradigm could be extended to operate on a large class of Riemannian manifolds. We propose a mathematically well-founded classification paradigm that allows to extend the work in [31] to multi-class models, taking into account the structure of the space. The idea is to project all the data from the manifold onto an affine tangent space at a particular point. To mitigate the distortion induced by local diffeomorphisms, we introduce for the first time in the computer vision community a well-founded mathematical concept, so-called Rolling map [21, 16]. The novelty in this alternate school of thought is that the manifold will be firstly rolled (without slipping or twisting) as a rigid body, then the given data is unwrapped onto the affine tangent space, where the classification is performed.</p><p>same-paper 2 0.85191286 <a title="407-lda-2" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>3 0.85160393 <a title="407-lda-3" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>4 0.84622669 <a title="407-lda-4" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>5 0.82393289 <a title="407-lda-5" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>6 0.82378012 <a title="407-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.8205688 <a title="407-lda-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.81741649 <a title="407-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.8167761 <a title="407-lda-9" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>10 0.8156749 <a title="407-lda-10" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>11 0.81516021 <a title="407-lda-11" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>12 0.81417263 <a title="407-lda-12" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>13 0.8134107 <a title="407-lda-13" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>14 0.81139678 <a title="407-lda-14" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>15 0.81038606 <a title="407-lda-15" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>16 0.8102321 <a title="407-lda-16" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>17 0.8099969 <a title="407-lda-17" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>18 0.80985337 <a title="407-lda-18" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>19 0.80942178 <a title="407-lda-19" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>20 0.80906528 <a title="407-lda-20" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
