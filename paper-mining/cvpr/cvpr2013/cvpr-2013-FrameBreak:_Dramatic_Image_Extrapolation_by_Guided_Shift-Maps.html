<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-177" href="#">cvpr2013-177</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</h1>
<br/><p>Source: <a title="cvpr-2013-177-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_FrameBreak_Dramatic_Image_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>Reference: <a title="cvpr-2013-177-reference" href="../cvpr2013_reference/cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps Yinda Zhang  Jianxiong Xiao  Natoiofn Sailn Uganpivore rsity Masosafc Theucshent osl Iongsyti ute  James Hays  Brown University  Ping Tan  Natoiofn Sailn Uganpivore rsity  of a panorama image of the same scene category (top right). [sent-1, score-0.473]
</p><p>2 The input image is roughly aligned with the guide image as shown with the  dashed red bounding box. [sent-2, score-0.476]
</p><p>3 Abstract We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. [sent-3, score-0.669]
</p><p>4 The extrapolation problem is formulated in the shift-map image synthesis framework. [sent-5, score-0.258]
</p><p>5 We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. [sent-6, score-0.57]
</p><p>6 Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. [sent-7, score-0.639]
</p><p>7 To handle this in-  crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. [sent-10, score-0.194]
</p><p>8 In the computational domain, numerous texture synthesis and image completion techniques can modestly extend the apparent field of view (FOV) of an image by propagating textures outward from the boundary. [sent-15, score-0.315]
</p><p>9 However, no existing technique can significantly extrapolate a photo because this requires implicit or explicit knowledge of scene layout. [sent-16, score-0.228]
</p><p>10 [29] introduced the first large-scale database of panoramic photographs and demonstrated the ability to align typical photographs with panoramic scene models. [sent-18, score-0.457]
</p><p>11 Specifically, we seek to extrapolate the FOV of an input image using a panoramic image of the same scene category. [sent-20, score-0.432]
</p><p>12 The input to our system  is an image (Figure 1, left) roughly registered with a guide image (Figure 1, top). [sent-22, score-0.513]
</p><p>13 Our algorithm extrapolates the original input image to a panorama as shown in the output image on the bottom right. [sent-24, score-0.353]
</p><p>14 The extrapolated result keeps the scene specific structure of the guide image, e. [sent-25, score-0.442]
</p><p>15 At the same time, its visual elements should all come from the original input image so that it appears to be a panorama image captured at the same viewpoint. [sent-28, score-0.353]
</p><p>16 Essentially, we need to learn the shared scene structure from the guide panorama and apply it to the input image to create a novel panorama. [sent-29, score-0.795]
</p><p>17 We approach this FOV extrapolation as a constrained 1 1 1 1 1 176 671 9 9  texture synthesis problem and address it under the framework of shift-map image editing [25]. [sent-30, score-0.367]
</p><p>18 We assume that panorama images can be synthesized by combining multiple shifted versions of a small image region with limited FOV. [sent-31, score-0.475]
</p><p>19 Under this model, a panorama is fully determined by that region and a shift-map which defines a translation vector at each pixel. [sent-32, score-0.453]
</p><p>20 We learn such a shift map from a guide panorama and then use it to constrain the extrapolation of a limited FOV input image. [sent-33, score-0.92]
</p><p>21 Such a guided shift-map can capture scene structures that are not present in the small image region, and ensures that the synthesized result adheres to the layout of the guide image. [sent-34, score-0.68]
</p><p>22 Our approach relies on understanding and reusing the long range self-similarity of the guide image. [sent-35, score-0.389]
</p><p>23 Because a panoramic scene typically contains surfaces, boundaries, and objects at multiple orientations and scales, it is difficult to sufficiently characterize the self-similarity using only patch translations. [sent-36, score-0.256]
</p><p>24 Next, we combine these intermediate results together with a graph optimization similar to photomontage [1]. [sent-41, score-0.259]
</p><p>25 Rendering techniques rely on panoramic environment maps to realistically illuminate objects in scenes. [sent-49, score-0.208]
</p><p>26 Left:wecapturescnestruc ebythe  motion of individual image patches according to self-similarity in the guide image. [sent-57, score-0.468]
</p><p>27 Because they do not model image texture in general, these methods cannot convincingly synthesize large missing regions. [sent-61, score-0.245]
</p><p>28 Example based texture synthesis methods such as [9, 8] are inherently image extrapolation methods because they iteratively copy patches from known regions to unknown areas. [sent-64, score-0.491]
</p><p>29 [19] extrapolate image boundaries by texture synthesis to fill the boundaries of panoramic mosaics. [sent-71, score-0.622]
</p><p>30 Poleg and Peleg [24] extrapolate individual, non-overlapping photographs in order to compose them into a panorama. [sent-72, score-0.218]
</p><p>31 These methods might extrapolate individual images by as much as 50% of their size, but we aim to synthesize outputs which have 500% the field of view of input photos. [sent-73, score-0.399]
</p><p>32 Like all of these methods, our approach relies on information from external images to guide the image completion or extrapolation. [sent-81, score-0.473]
</p><p>33 However, our singular guide scene is provided as input and we do not directly copy content from it, but rather learn and recreate its layout. [sent-82, score-0.625]
</p><p>34 (a) and (b) are the guide image and the input image respectively. [sent-85, score-0.434]
</p><p>35 To address this challenging problem, we assume a guide image  Ig with desirable FOV is known, and Ii is roughly registered to Igi (the “interior” region of Ig). [sent-91, score-0.507]
</p><p>36 Our goal is to synthesize the exterior of I according to Ii and Ig. [sent-93, score-0.267]
</p><p>37 Intuitively, we need to learn the transformation between Igi and Ig, and apply it to Ii to synthesize I. [sent-97, score-0.296]
</p><p>38 Following this idea, as illustrated in Figure 2, for each pixel q in the exterior region of the guide image, we first find a pixel p in the interior region, such that the two patches centered at q and p are most similar. [sent-99, score-0.821]
</p><p>39 This matching suggests that the pixel q in the guide image can be generated by transferring p with a transformation M(q), i. [sent-101, score-0.576]
</p><p>40 c oWored can efi ondf qsu acfhte a ttrraannssffoorrmmeadtion for each pixel of the guide image by brute force search. [sent-106, score-0.459]
</p><p>41 as To improve the synthesis quality, we can further adopt the texture optimization [20] technique. [sent-108, score-0.272]
</p><p>42 For each grid point, we copy a patch of pixels from Ii centered at its matched  position, as the blue and green boxes shown in Figure 2. [sent-110, score-0.208]
</p><p>43 Texture optimization iterates between two steps to synthesize the image I. [sent-112, score-0.22]
</p><p>44 Second, it copies the matched patches over and averages the overlapped patches to update the image. [sent-114, score-0.198]
</p><p>45 This is largely because this baseline method is overly sensitive to the registration between the input and the guide image. [sent-117, score-0.641]
</p><p>46 In most cases, we can only hope to have a rough registration such that the alignment is semantically plausible but not geometrically perfect. [sent-118, score-0.234]
</p><p>47 For example, in the theater example shown in Figure 3, the registration provides a rough overlap between regions of chairs and regions of screen. [sent-119, score-0.275]
</p><p>48 Such misalignment leads to improper results when the simple baseline method attempts to strictly recreate the geometric relationships observed in the guide image. [sent-121, score-0.498]
</p><p>49 Our generalized shift-map To handle the fact that registration is necessarily inexact, we do not directly copy transformations computed from Ig according to the registration of Ii and Ig. [sent-124, score-0.478]
</p><p>50 Instead, we formulate a graph optimization to choose an optimal transformation at each pixel of I. [sent-125, score-0.264]
</p><p>51 Ed(·) is the data term to measure the consistency of the patch icsen ttheere dda taat q ramnd t q ◦m Meas(uq)re ein th tehe c guide image Ig. [sent-132, score-0.433]
</p><p>52 Ien p oattchher cwenortedrse, dw ahte qn atnhed qda ◦ta M Mter(qm) i ns s thmeal glu, tidhee pixel q in the guide image Ig can be synthesized by copying the pixel at q ◦ M(q). [sent-133, score-0.666]
</p><p>53 Since we expect I have the to same scene slt artuc qtu ◦re M as Ig (a Snindc Iei wise registered twoi thha Igi), iet is therefore reasonable to apply the same copy to synthesize q in I. [sent-134, score-0.399]
</p><p>54 [12]  further narrowed down M(q) to a small set of representative translations M obtained by analyzing the input image. [sent-148, score-0.23]
</p><p>55 Specifically, a tsra Mnsl oatbitoani nMed w byil a abnea present hine t ihnep representative translation set only if many image patches can find a good match by that translation. [sent-149, score-0.256]
</p><p>56 So we estimate such a set from the guide image Ig, and apply it to synthesize the result I from the input Ii, as shown in Figure 5. [sent-152, score-0.613]
</p><p>57 As our set of representative translations M is computed from the guide image, we call our approach Mthe is guided shift-map tmheeth goudid. [sent-154, score-0.69]
</p><p>58 [6] introduced more general transformations such as rotation, scaling and reflection for image synthesis. [sent-157, score-0.231]
</p><p>59 Left:inheguidemage,thegrenpatchesvotefora  common shift vector, because they all can find a good match (blue ones) with this shift vector; Right: The red rectangle is the output image canvas. [sent-166, score-0.203]
</p><p>60 The yellow rectangle represents the input image shifted by a vector voted by the green patches in the guide image. [sent-167, score-0.672]
</p><p>61 As shown in Figure 4, we first fix the rotation, scaling and reflection parameters and solve an optimal translation map. [sent-171, score-0.243]
</p><p>62 Guided shift-map at bottom level We represent a transformation T by three parameters r, s, m for rotation, scaling, and reflection respectively. [sent-175, score-0.194]
</p><p>63 Building representative translations As observed in [12], while applying shift-map image editing, it is preferable to limit these shift vectors to a small set of predetermined representative translations. [sent-186, score-0.341]
</p><p>64 So we use Ig to build a set of permissible translation vectors and apply them to synthesize I from Ii. [sent-187, score-0.285]
</p><p>65 For each pixel q in the exterior of Ig, we search for its K nearest neighbors from the interior Igi transformed by T, and choose only those whose distance is within a fixed threshold. [sent-188, score-0.244]
</p><p>66 For efficiency consideration, we choose the top 50 candidate translations to form the set of representative translations MT. [sent-193, score-0.299]
</p><p>67 For the K nearest neighbor search, we measure the sim-  ×  1 1 1 1 1 17 7 74 2 2  transformation Ti, we compute a best translation at each pixel by the guided shift-map method to generate ITi . [sent-195, score-0.409]
</p><p>68 ilarity between two patches according to color and gradient layout using 32 32 color patches and 3 1-dimensional HOG [l1ay0o] ufetuatsuirnegs,3 respectively. [sent-197, score-0.193]
</p><p>69 Graph optimization We choose a translation vector at each pixel from the candidate set MT by minimizing tahte e graph energy Equation d1i waiteth s tehte M guidance condition M(q) ∈ MT for any pixel q. [sent-199, score-0.323]
</p><p>70 For any translation M ∈ MT, the input image Ii is first transformed by oTn (w Mhic ∈h iMs not shown in Figure 5 for clarity), and then shifted according to M. [sent-201, score-0.192]
</p><p>71 At each pixel, we need to choose an optimal transformation T (and its associated shift vector computed by the guided shift-map). [sent-212, score-0.318]
</p><p>72 The data term at a pixel q evaluates its synthesis quality under the transformation T(q). [sent-219, score-0.352]
</p><p>73 N(q) Here, MT(q) is the optimal translation vector selected for the pixel q under the transformation T. [sent-224, score-0.293]
</p><p>74 EdT(·) and EsT(·, ·) are the data term and smoothness terms of t(·h)e guide sh(·i,ft·-) map method under the transformation T. [sent-225, score-0.542]
</p><p>75 Given an input image Ii, we find a suitable Ig from the SUN360 panorama database [28] of the same scene category as Ii or we use an image search engine. [sent-236, score-0.406]
</p><p>76 In the theater example, although rough registration aligns semantically similar regions to the guide image Ig, directly applying the offset vectors computed in Ig to the I generates poor results. [sent-242, score-0.664]
</p><p>77 In comparison, our method synthesizes correct regions of chair and wall by accommodating the perspective-based scaling between exterior and interior in the MT. [sent-243, score-0.234]
</p><p>78 parts of the tree in the exterior region of the guide image match to patches in the sky in the interior region due to the similarity of patch feature (both HOG and color). [sent-252, score-0.797]
</p><p>79 As a result, part of the tree region is synthesized with the color of sky in the baseline method. [sent-253, score-0.218]
</p><p>80 Our method can avoid this problem by choosing the most representative motion vectors in the guide image and thus avoid such outliers. [sent-254, score-0.46]
</p><p>81 While PatchMatch allows an almost perfect reconstruction of the guide image from its interior region, the resulting self-similarity field does not produce plausible extrapolations of the input image. [sent-258, score-0.62]
</p><p>82 In general, as more transformations are allowed, reconstruction of the guide image itself strictly improves (Equation 1), but the likelihood that these best transformations generalize to an-  other scene decreases. [sent-259, score-0.63]
</p><p>83 Robustness to registration errors Our method requires the input image to be registered to a subregion of the guide image. [sent-265, score-0.661]
</p><p>84 We randomly shift the manually registered input image for 5–20% of the image width (600 pixels). [sent-268, score-0.209]
</p><p>85 Our method is robust to moderate registration errors, as we optimize the transformations with the graph optimization. [sent-273, score-0.278]
</p><p>86 Some patches in the foliage are matched to patches in the water in the guide image when the HOG feature is not used. [sent-278, score-0.587]
</p><p>87 Panorama Synthesis When Ig is a panoramic image, our method can synthesize Ii to a panorama. [sent-284, score-0.338]
</p><p>88 However, synthesizing a whole panorama at once requires a large offset vector space for voting to find representative translations. [sent-285, score-0.379]
</p><p>89 Also the size of MT has to be much larger in order to cover the whole panorama image domain. [sent-286, score-0.308]
</p><p>90 To solve this problem, we first divide the panoramic guide image Ig into several sub-images with smaller 1 1 1 1 1 17 7 76 4 4  Figure 7. [sent-288, score-0.548]
</p><p>91 For example, for the sub-image Ig1, we find representative translations by matching patches in Ig1 to Igr. [sent-298, score-0.264]
</p><p>92 Finally, we combine all these intermediate results to a full panorama by photomontage, which involves another  graph cut optimization. [sent-300, score-0.385]
</p><p>93 Figure 8 shows more panorama results for outdoor, indoor, and street scenes. [sent-304, score-0.342]
</p><p>94 On the right hand side of each input image are the guide image (upper image) and the synthesized result (lower image). [sent-306, score-0.521]
</p><p>95 In all the panorama synthesis experiments, the 360◦ of panorama is divided into 12 sub-images with uniformly sampled viewing direction from 0◦ ∼ 360◦. [sent-307, score-0.781]
</p><p>96 Conclusion We present the first study of the problem of extrapolating the field-of-view of a given image with a wide-angle guide image of the same scene category. [sent-314, score-0.488]
</p><p>97 We design a novel guided shift-map image synthesis method. [sent-315, score-0.281]
</p><p>98 The guide image generates a set of allowable transformations. [sent-316, score-0.431]
</p><p>99 The graph optimization chooses an optimal transformation for each pixel to synthesize the result. [sent-317, score-0.443]
</p><p>100 Our method can extrapolate an image to a panorama and is successfully demonstrated on various scenes. [sent-319, score-0.483]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('guide', 0.389), ('panorama', 0.308), ('ig', 0.294), ('fov', 0.187), ('synthesize', 0.179), ('extrapolate', 0.175), ('synthesis', 0.165), ('panoramic', 0.159), ('registration', 0.148), ('photomontage', 0.141), ('transformation', 0.117), ('guided', 0.116), ('translations', 0.114), ('ii', 0.109), ('translation', 0.106), ('mt', 0.096), ('transformations', 0.094), ('extrapolation', 0.093), ('copy', 0.088), ('exterior', 0.088), ('igi', 0.087), ('synthesized', 0.087), ('interior', 0.086), ('shift', 0.085), ('kaneva', 0.085), ('theater', 0.085), ('completion', 0.084), ('registered', 0.079), ('patches', 0.079), ('reflection', 0.077), ('siggraph', 0.075), ('representative', 0.071), ('pixel', 0.07), ('texture', 0.066), ('hays', 0.066), ('hog', 0.06), ('scaling', 0.06), ('baseline', 0.059), ('fill', 0.057), ('extrapolations', 0.056), ('intraub', 0.056), ('natoiofn', 0.056), ('poleg', 0.056), ('rsity', 0.056), ('sailn', 0.056), ('uganpivore', 0.056), ('rotation', 0.054), ('scene', 0.053), ('photograph', 0.052), ('recreate', 0.05), ('voted', 0.05), ('copying', 0.05), ('darabi', 0.05), ('environment', 0.049), ('seams', 0.046), ('bertalmio', 0.046), ('drucker', 0.046), ('arch', 0.046), ('extrapolating', 0.046), ('input', 0.045), ('xiao', 0.045), ('equation', 0.045), ('patch', 0.044), ('plausible', 0.044), ('conquer', 0.044), ('edt', 0.044), ('drew', 0.044), ('holes', 0.043), ('editing', 0.043), ('photographs', 0.043), ('rough', 0.042), ('dashed', 0.042), ('allowable', 0.042), ('narrow', 0.041), ('optimization', 0.041), ('memory', 0.041), ('shifted', 0.041), ('intermediate', 0.041), ('incoherent', 0.04), ('whyte', 0.04), ('matched', 0.04), ('region', 0.039), ('panoramas', 0.039), ('efros', 0.038), ('james', 0.037), ('ed', 0.036), ('graph', 0.036), ('patchmatch', 0.036), ('kopf', 0.036), ('pixels', 0.036), ('smoothness', 0.036), ('yellow', 0.035), ('pritch', 0.035), ('layout', 0.035), ('street', 0.034), ('es', 0.034), ('observers', 0.034), ('rectangle', 0.033), ('sky', 0.033), ('avidan', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="177-tfidf-1" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>2 0.15892604 <a title="177-tfidf-2" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>Author: Christian Richardt, Yael Pritch, Henning Zimmer, Alexander Sorkine-Hornung</p><p>Abstract: We present a solution for generating high-quality stereo panoramas at megapixel resolutions. While previous approaches introduced the basic principles, we show that those techniques do not generalise well to today’s high image resolutions and lead to disturbing visual artefacts. As our first contribution, we describe the necessary correction steps and a compact representation for the input images in order to achieve a highly accurate approximation to the required ray space. Our second contribution is a flow-based upsampling of the available input rays which effectively resolves known aliasing issues like stitching artefacts. The required rays are generated on the fly to perfectly match the desired output resolution, even for small numbers of input images. In addition, the upsampling is real-time and enables direct interactive control over the desired stereoscopic depth effect. In combination, our contributions allow the generation of stereoscopic panoramas at high output resolutions that are virtually free of artefacts such as seams, stereo discontinuities, vertical parallax and other mono-/stereoscopic shape distortions. Our process is robust, and other types of multiperspective panoramas, such as linear panoramas, can also benefit from our contributions. We show various comparisons and high-resolution results.</p><p>3 0.12543267 <a title="177-tfidf-3" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>4 0.12487312 <a title="177-tfidf-4" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>5 0.10780361 <a title="177-tfidf-5" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>6 0.10649253 <a title="177-tfidf-6" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>7 0.10050742 <a title="177-tfidf-7" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>8 0.092182979 <a title="177-tfidf-8" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>9 0.090567522 <a title="177-tfidf-9" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>10 0.088203348 <a title="177-tfidf-10" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>11 0.080963254 <a title="177-tfidf-11" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>12 0.075730458 <a title="177-tfidf-12" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>13 0.074709684 <a title="177-tfidf-13" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>14 0.074528478 <a title="177-tfidf-14" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>15 0.074104071 <a title="177-tfidf-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.073073715 <a title="177-tfidf-16" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>17 0.072135948 <a title="177-tfidf-17" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>18 0.068354197 <a title="177-tfidf-18" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>19 0.064505219 <a title="177-tfidf-19" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>20 0.063468821 <a title="177-tfidf-20" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, 0.06), (2, 0.012), (3, 0.031), (4, 0.029), (5, -0.001), (6, -0.021), (7, 0.002), (8, -0.015), (9, -0.027), (10, 0.023), (11, 0.046), (12, 0.015), (13, -0.028), (14, 0.046), (15, -0.122), (16, 0.023), (17, -0.001), (18, 0.086), (19, 0.026), (20, 0.001), (21, 0.004), (22, 0.003), (23, -0.087), (24, 0.018), (25, -0.079), (26, 0.013), (27, -0.052), (28, 0.049), (29, 0.021), (30, -0.061), (31, -0.077), (32, 0.029), (33, 0.029), (34, 0.008), (35, -0.122), (36, -0.138), (37, 0.079), (38, -0.085), (39, 0.02), (40, -0.01), (41, 0.027), (42, -0.005), (43, -0.022), (44, 0.049), (45, 0.005), (46, -0.026), (47, -0.062), (48, -0.066), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93923599 <a title="177-lsi-1" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>2 0.82298535 <a title="177-lsi-2" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>3 0.70317143 <a title="177-lsi-3" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>4 0.70077169 <a title="177-lsi-4" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>5 0.69856858 <a title="177-lsi-5" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>6 0.68876779 <a title="177-lsi-6" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>7 0.6596452 <a title="177-lsi-7" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>8 0.65695649 <a title="177-lsi-8" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>9 0.60492611 <a title="177-lsi-9" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>10 0.5788936 <a title="177-lsi-10" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>11 0.56567699 <a title="177-lsi-11" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>12 0.55944788 <a title="177-lsi-12" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>13 0.54295129 <a title="177-lsi-13" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>14 0.53970462 <a title="177-lsi-14" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>15 0.53242946 <a title="177-lsi-15" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>16 0.51671594 <a title="177-lsi-16" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>17 0.51358294 <a title="177-lsi-17" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>18 0.51291102 <a title="177-lsi-18" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>19 0.51083255 <a title="177-lsi-19" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>20 0.50854993 <a title="177-lsi-20" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.112), (16, 0.031), (26, 0.047), (33, 0.219), (65, 0.313), (67, 0.057), (69, 0.043), (72, 0.011), (87, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84583855 <a title="177-lda-1" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>Author: Ajay Kumar, Cyril Kwong</p><p>Abstract: In order to avail the benefits of higher user convenience, hygiene, and improved accuracy, contactless 3D fingerprint recognition techniques have recently been introduced. One of the key limitations of these emerging 3D fingerprint technologies to replace the conventional 2D fingerprint system is their bulk and high cost, which mainly results from the use of multiple imaging cameras or structured lighting employed in these systems. This paper details the development of a contactless 3D fingerprint identification system that uses only single camera. We develop a new representation of 3D finger surface features using Finger Surface Codes and illustrate its effectiveness in matching 3D fingerprints. Conventional minutiae representation is extended in 3D space to accurately match the recovered 3D minutiae. Multiple 2D fingerprint images (with varying illumination profile) acquired to build 3D fingerprints can themselves be used recover 2D features for further improving 3D fingerprint identification and has been illustrated in this paper. The experimental results are shown on a database of 240 client fingerprints and confirm the advantages of the single camera based 3D fingerprint identification.</p><p>2 0.79102802 <a title="177-lda-2" href="./cvpr-2013-Efficient_3D_Endfiring_TRUS_Prostate_Segmentation_with_Globally_Optimized_Rotational_Symmetry.html">139 cvpr-2013-Efficient 3D Endfiring TRUS Prostate Segmentation with Globally Optimized Rotational Symmetry</a></p>
<p>Author: Jing Yuan, Wu Qiu, Eranga Ukwatta, Martin Rajchl, Xue-Cheng Tai, Aaron Fenster</p><p>Abstract: Segmenting 3D endfiring transrectal ultrasound (TRUS) prostate images efficiently and accurately is of utmost importance for the planning and guiding 3D TRUS guided prostate biopsy. Poor image quality and imaging artifacts of 3D TRUS images often introduce a challenging task in computation to directly extract the 3D prostate surface. In this work, we propose a novel global optimization approach to delineate 3D prostate boundaries using its rotational resliced images around a specified axis, which properly enforces the inherent rotational symmetry of prostate shapes to jointly adjust a series of 2D slicewise segmentations in the global 3D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard, we propose a novel coupled continuous max-flow model, which not only provides a powerful mathematical tool to analyze the proposed optimization problem but also amounts to a new and efficient duality-basedalgorithm. Ex- tensive experiments demonstrate that the proposed method significantly outperforms the state-of-art methods in terms ofefficiency, accuracy, reliability and less user-interactions, and reduces the execution time by a factor of 100.</p><p>3 0.78692526 <a title="177-lda-3" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>Author: Adam Herout, István Szentandrási, Michal Zachariáš, Markéta Dubská, Rudolf Kajan</p><p>Abstract: We introduce here an improved design of the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution.</p><p>same-paper 4 0.76817936 <a title="177-lda-4" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>5 0.75419152 <a title="177-lda-5" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>6 0.73033082 <a title="177-lda-6" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>7 0.72397131 <a title="177-lda-7" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>8 0.71565348 <a title="177-lda-8" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>9 0.64896381 <a title="177-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.64782494 <a title="177-lda-10" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>11 0.64709216 <a title="177-lda-11" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>12 0.64325976 <a title="177-lda-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.6429854 <a title="177-lda-13" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>14 0.64274657 <a title="177-lda-14" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>15 0.64266139 <a title="177-lda-15" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>16 0.64243388 <a title="177-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.64233047 <a title="177-lda-17" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>18 0.64205807 <a title="177-lda-18" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>19 0.64181149 <a title="177-lda-19" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>20 0.64161491 <a title="177-lda-20" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
