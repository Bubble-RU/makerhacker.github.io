<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>402 cvpr-2013-Social Role Discovery in Human Events</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-402" href="#">cvpr2013-402</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>402 cvpr-2013-Social Role Discovery in Human Events</h1>
<br/><p>Source: <a title="cvpr-2013-402-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ramanathan_Social_Role_Discovery_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>Reference: <a title="cvpr-2013-402-reference" href="../cvpr2013_reference/cvpr-2013-Social_Role_Discovery_in_Human_Events_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 yao , Abstract We deal with the problem of recognizing social roles played by people in an event. [sent-2, score-1.122]
</p><p>2 Social roles are governed by human interactions, and form a fundamental component of human event description. [sent-3, score-0.715]
</p><p>3 We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. [sent-4, score-0.737]
</p><p>4 Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. [sent-5, score-1.946]
</p><p>5 We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. [sent-6, score-0.663]
</p><p>6 We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. [sent-7, score-1.707]
</p><p>7 Our ability to comprehend human relations stands fundamental to our survival, development and social life. [sent-11, score-0.607]
</p><p>8 We understand such relationships in terms of social roles assumed by people, and tend to describe events using these roles. [sent-12, score-1.018]
</p><p>9 Typically, social roles answer semantic queries like, “Who is doing what in an event? [sent-15, score-0.954]
</p><p>10 While the tasks of identifying the action and detecting the person are widely studied in computer vision, the problem of role assignment is relatively new and equally interesting. [sent-17, score-0.556]
</p><p>11 Social role discovery derives motivation from the field of “Role Theory” [2] in sociology, which observes that people behave in predictable ways based on their social roles. [sent-18, score-1.098]
</p><p>12 This shows that knowing the role of a person can help determine his/her interactions with the environment and vice-versa. [sent-19, score-0.572]
</p><p>13 Social roles act as identities for the individuals and can help us describe the event in terms of these roles. [sent-26, score-0.625]
</p><p>14 Also, the knowledge of social roles can help determine the interesting segments of social event footages [7] and sports videos. [sent-29, score-1.726]
</p><p>15 The definition of social roles is event specific, and can sometimes be abstract such as, people “helping”, “visiting” or “residing” in a nursing home [13], making role identification a difficult human task. [sent-30, score-1.732]
</p><p>16 Recognizing these difficulties, we formulate the problem of social role discovery in a weakly supervised framework. [sent-33, score-1.033]
</p><p>17 Given a set of videos belonging to a social event without training labels for the people in the videos, we group them into different social roles. [sent-34, score-1.536]
</p><p>18 The event label acts as the weak annotation in our setting, restricting the discovered roles to be event specific. [sent-35, score-0.857]
</p><p>19 The problem is amply challenging due to the wide variation in appearance, scale, location and scene context of a role across different videos as seen in Fig. [sent-36, score-0.506]
</p><p>20 1, it is difficult to determine roles by observing people individually. [sent-39, score-0.56]
</p><p>21 Rather, social role discovery is an attempt to identify people based on their interactions in an event. [sent-40, score-1.182]
</p><p>22 In order to solve this problem of weakly supervised  role assignment, we propose a Conditional Random Field (CRF) to capture inter-role interaction cues, and develop 222444777533  ? [sent-42, score-0.653]
</p><p>23 The different roles in each event are marked by the colors noted in the last column. [sent-177, score-0.667]
</p><p>24 Further, to evaluate the model performance, we introduce a novel YouTube social roles dataset in Sec. [sent-180, score-0.954]
</p><p>25 1, accompanied by event specific ground truth role annotations for the people in the videos. [sent-182, score-0.779]
</p><p>26 We also provide role annotations for a subset of videos from two events of the TRECVID MED-1 1 [1] event kits, and test our model performance on these videos. [sent-184, score-0.735]
</p><p>27 Experiments on these datasets show that our method achieves encouraging performance in weakly supervised social role assignment. [sent-185, score-0.997]
</p><p>28 Related Work Socially aware video and image analysis Recent works on social network construction and interaction understanding is relevant to our work on social role recognition. [sent-187, score-1.715]
</p><p>29 [5] uses scene context and visual concept attributes to build social relation network. [sent-190, score-0.557]
</p><p>30 [23] also builds a social role network based on their co-occurrence of movie characters in different scenes. [sent-191, score-1.004]
</p><p>31 [20] studied the problem of face recognition in social context. [sent-194, score-0.536]
</p><p>32 Social Interaction in Action Recognition Another related line of work has been the use of social interaction to aid group action recognition [14, 3, 6]. [sent-195, score-0.773]
</p><p>33 [18] also uses social grouping to help multi target tracking. [sent-197, score-0.536]
</p><p>34 [10] uses social context in group photos to make better prediction of human attributes and scene semantics. [sent-198, score-0.628]
</p><p>35 Although the above works capture social interactions in some form, they do not explicitly identify the roles assumed by people during a social event. [sent-202, score-1.716]
</p><p>36 Role Recognition Recently, [7, 13] used social roles to predict group activities. [sent-203, score-0.98]
</p><p>37 They used training labels 222444777644  to learn role assignments based on spatio-temporal interaction between players. [sent-207, score-0.627]
</p><p>38 However, in our work we are not provided role annotations, and we wish to discover interactionbased roles automatically by studying different instances of an event. [sent-208, score-0.826]
</p><p>39 Our Approach We define social role discovery as a weakly supervised problem, where the training role labels for the people in the videos are not available. [sent-211, score-1.648]
</p><p>40 We are only provided the event label for each video, and the number of roles to be discovered in an event. [sent-212, score-0.625]
</p><p>41 Social roles are not only decided by person specific descriptors, but also by the interaction between people. [sent-214, score-0.732]
</p><p>42 Hence, any model used to discover social roles should be capable of incorporating this information. [sent-215, score-0.978]
</p><p>43 In our approach, every event has a reference role, and the interaction of any person with this reference role is most significant. [sent-217, score-1.109]
</p><p>44 One instance of the reference role is assumed to be present in every video belonging to the event class. [sent-222, score-0.748]
</p><p>45 Model Formulation We present a CRF model which accounts for the reference role interaction with other roles in a video. [sent-226, score-1.105]
</p><p>46 As illustrated, to capture person specific social cues, we extract unary features (Ψu) from each human track, describing spatio-temporal activity, human appearance and human-object interaction. [sent-229, score-0.811]
</p><p>47 Similarly, to represent interaction based social cues, pairwise features (Ψp) describing proxemic touch codes, and spatial proximity are extracted. [sent-230, score-0.879]
</p><p>48 Our CRF model uses these features to perform weakly supervised social role recognition. [sent-231, score-0.997]
</p><p>49 Let Pv be the set of people in a video v and siv be the social role assigned to a person piv ∈ Pv. [sent-232, score-1.368]
</p><p>50 We want to assign social roles, and jointly learn model∈ weights by maximizing the log likelihood of the CRF shown in Eq. [sent-233, score-0.536]
</p><p>51 m is the index of the reference role in the video v. [sent-246, score-0.541]
</p><p>52 where mE denotes the reference role in the event E, and the person holding the reference role in v. [sent-250, score-1.301]
</p><p>53 1, sE is the complete social role assignment to all people in the event, and Zv is the log-partition function for the video v. [sent-257, score-1.157]
</p><p>54 Note that the model only considers interaction of different roles with the reference role, in accordance with our assumption, and every video is assumed to contain one person playing this reference role. [sent-259, score-0.982]
</p><p>55 Unary Features The unary feature Ψu captures role specific social cues extracted from human tracks, and their interaction with the event environment. [sent-264, score-1.462]
</p><p>56 Object Interaction Feature ΨuOI: The interaction of a person with the event environment plays a key role in determining his/ her role. [sent-271, score-0.887]
</p><p>57 : These features capture two important social aspects of a person, representing gender and clothing. [sent-277, score-0.575]
</p><p>58 Pairwise Interaction Features Human interaction forms an important basis for social role definitions. [sent-285, score-1.112]
</p><p>59 : The proxemic interaction of two people provides interesting insights regarding the relation between roles in an event such as the touch-code between a “parent” and the “birthday child”. [sent-290, score-1.059]
</p><p>60 1 arises due to the correlation between different social roles and the coupling introduced by Zv. [sent-304, score-0.954]
</p><p>61 We also introduce a variational approximation to the social role probability distribution in a video, with similar dependencies as the original model. [sent-308, score-0.959]
</p><p>62 3, where sv denotes the role assignment to all people in the video v. [sent-310, score-0.621]
</p><p>63 is a factor giving the probability of a person being assigned the reference role in the video. [sent-318, score-0.617]
</p><p>64 ψv is a set of |Pv | factors, where ψ(vi) is the secondary role probability fm |Patr|ix f fcotro orst,h werh people in the video, when piv is assigned the reference role. [sent-319, score-0.8]
</p><p>65 This variational approximation ofthe social role probability, retains the dependencies in our original structure. [sent-322, score-0.959]
</p><p>66 It represents one predominant reference role, with secondary role assignments dependent on this reference role. [sent-323, score-0.701]
</p><p>67 In every video v, the person pvm with the highest value of φv is assigned the reference role, forming a reference role cluster. [sent-337, score-0.817]
</p><p>68 The corresponding variational probability ψ(vm) is used to assign secondary roles to other people in the video. [sent-338, score-0.663]
</p><p>69 We enforce a lower and upper bound on the number of people assigned to a secondary role cluster in the event. [sent-339, score-0.632]
</p><p>70 This acts a lose prior on the number of people in each role cluster. [sent-341, score-0.551]
</p><p>71 Datasets YouTube Social Roles Most publicly available video datasets are not suitable for evaluating the social role assignment task, since they do not cover a good range of peo-  ple donning different roles in specific social events. [sent-347, score-1.987]
</p><p>72 In an attempt to evaluate our method, we collected a set of YouTube videos under 4 social events. [sent-348, score-0.605]
</p><p>73 To facilitate easy evaluation, we annotate every person in our dataset with the relevant social roles. [sent-351, score-0.64]
</p><p>74 Some videos have stray individuals not annotated with any specific social role and are called as “others”. [sent-352, score-1.007]
</p><p>75 Within each social event, there is wide variation in event settings as seen from the sample video frames in Fig. [sent-354, score-0.789]
</p><p>76 This diversity in scenarios, with the same underlying interactions between different roles is an interesting characteristic of the dataset, and makes the task amply challenging. [sent-359, score-0.534]
</p><p>77 TRECVID Social Roles Among publicly available datasets, the TRECVID-MED1 1event kits [1] have two social event classes birthday and wedding. [sent-360, score-0.968]
</p><p>78 Some videos were cropped to include only the parts showing relevant social events. [sent-363, score-0.605]
</p><p>79 Due to the weakly supervised nature of the problem, we do not have a direct mapping between role clusters and groundtruth role labels. [sent-379, score-0.862]
</p><p>80 To facilitate easy comparison with different baselines, the role clusters obtained from a method are each mapped to one ofthe human defined roles, maximizing the total correct role assignments in an event. [sent-380, score-0.861]
</p><p>81 e A r roalen,d aonmd tpheer stornue i prior ho fv secondary roles is used to assign roles to other people in the video. [sent-389, score-1.042]
</p><p>82 This confirms our belief that, human interactions are informative for role recognition. [sent-415, score-0.513]
</p><p>83 This demonstrates the value in explicitly modeling interaction between role pairs, instead of using interaction as a context feature. [sent-419, score-0.789]
</p><p>84 Sample frames from videos are shown, where our full model identified the correct (a) “bride” (green box), “groom”(red box) roles in wedding and (b) “presenter” (green box), “recipient” (red box) roles in award function. [sent-425, score-1.155]
</p><p>85 The column corresponding to the reference role cluster chosen by our algorithm is highlighted in each matrix. [sent-436, score-0.519]
</p><p>86 The average purities of the reference role clusters are 0. [sent-437, score-0.512]
</p><p>87 We observe that the model is able to cluster the roles better in the wedding event, as seen in Fig. [sent-441, score-0.602]
</p><p>88 To study this interaction, we visualize the marginals of the spatial relationship of different roles with the reference role (“groom”) cluster in the YouTube wedding dataset, in Fig. [sent-444, score-1.119]
</p><p>89 “friends” are difficult to distinguish from “guests” in the TRECVID birthday dataset, where we observed both roles to exhibit low interaction with the reference role. [sent-449, score-0.881]
</p><p>90 The column corresponding to the reference role cluster chosen by our model is highlighted in each event. [sent-456, score-0.519]
</p><p>91 In order to evaluate the latent reference role assignment in our model, we compare performances with a control setting which randomly chooses the reference role in each video. [sent-460, score-1.039]
</p><p>92 82% for the YouTube social roles dataset with this choice ofreference role, justifying the need to model it as a latent variable. [sent-462, score-0.986]
</p><p>93 80% for the wedding event, which has more role classes than the other events leading to increased randomness in the choice of reference role in each video. [sent-464, score-1.086]
</p><p>94 Conclusion We proposed to recognize social roles from human event videos in a weakly supervised setting, and designed a CRF to model the inter-role interactions along with person specific unary features. [sent-466, score-1.621]
</p><p>95 As a next step, our approach can be extended to perform simultaneous event classification along with role discovery. [sent-470, score-0.591]
</p><p>96 It is also noted that our method is not robust to noisy and fragmented reference role tracking, due to the inherent assumption of one reference role per video. [sent-471, score-1.012]
</p><p>97 Learning relations among movie characters: A social network perspective. [sent-492, score-0.621]
</p><p>98 Marginal of the position of a role relative to the reference (“groom”), estimated by our model is shown for YouTube wedding videos. [sent-600, score-0.655]
</p><p>99 Seeing people in social context: recognizing people and social relation-  [26] J. [sent-645, score-1.382]
</p><p>100 a 4tion,  ysis from the perspective of social networks. [sent-665, score-0.536]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('social', 0.536), ('roles', 0.418), ('role', 0.384), ('event', 0.207), ('interaction', 0.192), ('birthday', 0.16), ('wedding', 0.16), ('people', 0.142), ('groom', 0.129), ('bride', 0.115), ('reference', 0.111), ('person', 0.104), ('proxemic', 0.1), ('interactions', 0.084), ('youtube', 0.082), ('trecvid', 0.081), ('piv', 0.081), ('presenter', 0.081), ('crf', 0.078), ('award', 0.07), ('videos', 0.069), ('kits', 0.065), ('secondary', 0.064), ('unary', 0.063), ('siv', 0.057), ('pst', 0.053), ('proxemics', 0.05), ('recipient', 0.05), ('assignment', 0.049), ('weakly', 0.049), ('cake', 0.048), ('sjv', 0.048), ('events', 0.047), ('video', 0.046), ('human', 0.045), ('pvm', 0.043), ('gender', 0.039), ('variational', 0.039), ('movie', 0.038), ('discovery', 0.036), ('amply', 0.032), ('bridesmaids', 0.032), ('distributor', 0.032), ('envamente', 0.032), ('groomsmen', 0.032), ('guests', 0.032), ('justifying', 0.032), ('methodbirthday', 0.032), ('occupation', 0.032), ('pceiaolp', 0.032), ('peesr', 0.032), ('priest', 0.032), ('rleol', 0.032), ('sociology', 0.032), ('assignments', 0.031), ('activities', 0.03), ('touch', 0.03), ('footages', 0.029), ('osfo', 0.029), ('bangpeng', 0.029), ('supervised', 0.028), ('helping', 0.028), ('annotations', 0.028), ('zv', 0.027), ('group', 0.026), ('relations', 0.026), ('tractable', 0.026), ('recognizing', 0.026), ('clothing', 0.025), ('characters', 0.025), ('acts', 0.025), ('activity', 0.025), ('discover', 0.024), ('cluster', 0.024), ('tracks', 0.023), ('inference', 0.023), ('box', 0.023), ('noted', 0.022), ('marginals', 0.022), ('network', 0.021), ('pairwise', 0.021), ('context', 0.021), ('labels', 0.02), ('full', 0.02), ('initialized', 0.02), ('gallagher', 0.02), ('marked', 0.02), ('failure', 0.019), ('track', 0.019), ('action', 0.019), ('scores', 0.019), ('confusion', 0.018), ('pv', 0.018), ('specific', 0.018), ('assigned', 0.018), ('understand', 0.017), ('lan', 0.017), ('clusters', 0.017), ('cues', 0.017), ('distinguished', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="402-tfidf-1" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>2 0.35223502 <a title="402-tfidf-2" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>3 0.21563119 <a title="402-tfidf-3" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>4 0.19812562 <a title="402-tfidf-4" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>5 0.15826614 <a title="402-tfidf-5" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>6 0.15287368 <a title="402-tfidf-6" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>7 0.15174016 <a title="402-tfidf-7" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>8 0.12907794 <a title="402-tfidf-8" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>9 0.11486498 <a title="402-tfidf-9" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>10 0.10339929 <a title="402-tfidf-10" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>11 0.10300253 <a title="402-tfidf-11" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>12 0.09349966 <a title="402-tfidf-12" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>13 0.091923036 <a title="402-tfidf-13" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>14 0.079794347 <a title="402-tfidf-14" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>15 0.077557996 <a title="402-tfidf-15" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>16 0.074195184 <a title="402-tfidf-16" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>17 0.07380119 <a title="402-tfidf-17" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>18 0.072292574 <a title="402-tfidf-18" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>19 0.072100073 <a title="402-tfidf-19" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>20 0.072093658 <a title="402-tfidf-20" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.146), (1, -0.064), (2, -0.01), (3, -0.1), (4, -0.034), (5, 0.014), (6, -0.064), (7, -0.019), (8, 0.036), (9, 0.057), (10, 0.072), (11, -0.059), (12, 0.043), (13, -0.0), (14, -0.02), (15, 0.037), (16, 0.056), (17, 0.128), (18, -0.003), (19, -0.218), (20, -0.122), (21, 0.065), (22, -0.004), (23, 0.005), (24, -0.009), (25, -0.026), (26, 0.024), (27, -0.109), (28, -0.006), (29, -0.122), (30, 0.062), (31, -0.025), (32, 0.012), (33, -0.103), (34, 0.024), (35, 0.038), (36, 0.05), (37, 0.117), (38, 0.112), (39, 0.132), (40, 0.138), (41, -0.065), (42, -0.108), (43, 0.0), (44, 0.248), (45, 0.006), (46, -0.123), (47, -0.04), (48, 0.018), (49, 0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97262555 <a title="402-lsi-1" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>2 0.87969333 <a title="402-lsi-2" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>3 0.6817174 <a title="402-lsi-3" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>4 0.5869 <a title="402-lsi-4" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>5 0.55061287 <a title="402-lsi-5" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>6 0.54717082 <a title="402-lsi-6" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>7 0.49402237 <a title="402-lsi-7" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>8 0.48305944 <a title="402-lsi-8" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>9 0.47653821 <a title="402-lsi-9" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>10 0.39281487 <a title="402-lsi-10" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>11 0.38253939 <a title="402-lsi-11" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>12 0.37395087 <a title="402-lsi-12" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>13 0.34933192 <a title="402-lsi-13" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>14 0.33627442 <a title="402-lsi-14" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>15 0.32864064 <a title="402-lsi-15" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>16 0.31949019 <a title="402-lsi-16" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>17 0.30728874 <a title="402-lsi-17" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>18 0.30722526 <a title="402-lsi-18" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>19 0.30526671 <a title="402-lsi-19" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>20 0.28423381 <a title="402-lsi-20" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.081), (16, 0.012), (26, 0.051), (28, 0.017), (33, 0.217), (67, 0.075), (69, 0.071), (77, 0.332), (87, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7901265 <a title="402-lda-1" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>2 0.78930581 <a title="402-lda-2" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>3 0.78406799 <a title="402-lda-3" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>4 0.71990788 <a title="402-lda-4" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>Author: Tim Matthews, Mark S. Nixon, Mahesan Niranjan</p><p>Abstract: We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pairwise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence ofattributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.</p><p>5 0.71975654 <a title="402-lda-5" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>6 0.68228555 <a title="402-lda-6" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>7 0.67998117 <a title="402-lda-7" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>8 0.63622814 <a title="402-lda-8" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>9 0.62194157 <a title="402-lda-9" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>10 0.61867863 <a title="402-lda-10" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>11 0.61304194 <a title="402-lda-11" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>12 0.61230272 <a title="402-lda-12" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>13 0.60963553 <a title="402-lda-13" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>14 0.60590142 <a title="402-lda-14" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>15 0.60557956 <a title="402-lda-15" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>16 0.60498899 <a title="402-lda-16" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>17 0.60490084 <a title="402-lda-17" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>18 0.60460263 <a title="402-lda-18" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>19 0.60441172 <a title="402-lda-19" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>20 0.60411441 <a title="402-lda-20" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
