<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 cvpr-2013-Adaptive Compressed Tomography Sensing</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-35" href="#">cvpr2013-35</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 cvpr-2013-Adaptive Compressed Tomography Sensing</h1>
<br/><p>Source: <a title="cvpr-2013-35-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Barkan_Adaptive_Compressed_Tomography_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Oren Barkan, Jonathan Weill, Amir Averbuch, Shai Dekel</p><p>Abstract: One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms.</p><p>Reference: <a title="cvpr-2013-35-reference" href="../cvpr2013_reference/cvpr-2013-Adaptive_Compressed_Tomography_Sensing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 i l Abstract One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. [sent-4, score-0.144]
</p><p>2 We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. [sent-5, score-0.518]
</p><p>3 The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. [sent-6, score-0.633]
</p><p>4 The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. [sent-7, score-0.332]
</p><p>5 We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms. [sent-8, score-0.519]
</p><p>6 Introduction In the last decade, several studies have shown that radiation exposure during CT scanning is a significant factor in raising the total public risk of cancer deaths [1,20,21]. [sent-10, score-0.145]
</p><p>7 It’s meant to ensure that “… CT dose factors are kept to a point where risk is minimized for maximum diagnostic benefit. [sent-12, score-0.221]
</p><p>8 ", where the dose can be determined by the product of the CT tube current and the time the patient is exposed to radiation (see [2] for an overview). [sent-15, score-0.324]
</p><p>9 Currently, there are several state-ofthe-art technologies that attempt to achieve dose reduction. [sent-16, score-0.221]
</p><p>10 This paper describes an adaptive acquisition model that  is superior, in the CT image quality, to existing limited angle, non-adaptive acquisition methods and in theory may allow minimal and optimal dosage levels. [sent-21, score-0.683]
</p><p>11 The method can be considered a significant generalization of existing twostep adaptive acquisition methods [7,8] and can potentially use the same hardware configurations that are capable of changing their geometric configuration and acquisition protocols on-the-fly [9]. [sent-22, score-0.569]
</p><p>12 Observe that adaptive acquisition should not be confused with adaptive reconstruction. [sent-23, score-0.487]
</p><p>13 In the latter, the acquisition model is a non-adaptive uniform sampling scheme, where over a discrete set of pre-determined angles, line projections are computed at equal intervals. [sent-24, score-0.704]
</p><p>14 In this setup, the adaptive elements, if exist, are part of the post-acquisition reconstruction step. [sent-25, score-0.197]
</p><p>15 The outline of the algorithm is as follows: First, the system projects the object with an extreme low dose according to a uniform predetermined pattern and reconstructs an initial low quality image. [sent-26, score-0.327]
</p><p>16 Then, the system iterates by incorporating the newly added line projections in order to obtain a refined approximation of the object's image. [sent-28, score-0.514]
</p><p>17 The algorithm continues to iterate between estimation of locations of significant features, adaptive acquisition and reconstruction until a convergence criterion is met. [sent-29, score-0.414]
</p><p>18 The goal is to converge to a high quality reconstruction using a minimal number of rays (line projections). [sent-30, score-0.103]
</p><p>19 The mathematical theory of [5] quantifies, in the setup of CT, the geometric ‘structure’ of the image and how fast a Ridgelet approximation converges to the image. [sent-32, score-0.149]
</p><p>20 Our algorithm, whose goal is to acquire an unknown image, regards the adaptive Ridgelet approximation of the image as the ‘optimal’ benchmark and is designed to match its performance. [sent-33, score-0.234]
</p><p>21 This approach has strong ties with the 222111999533  waveform analysis presented in [10] that allowed the authors to classify singularities and quantify the ‘stability’ of limited angle tomography. [sent-34, score-0.123]
</p><p>22 Indeed, although in our work we limit the number of line projections, but do not limit the angles, the fundamental understanding of the relationship between a function’s edge singularities and its Radon representation, as explained in [10], is at the core of our algorithm (see Figure 3. [sent-35, score-0.232]
</p><p>23 We show in the experimental results section that for the same number of line projections, our algorithm yields higher image reconstruction quality, when compared with known limited angle, non-adaptive acquisition algorithms. [sent-37, score-0.446]
</p><p>24 Section 3 describes in detail our adaptive acquisition algorithm. [sent-39, score-0.352]
</p><p>25 As we shall see in Section 3, in our case, the sparsity is due to the fact that each row of A is associated with weighted integration over a digital line in the image I and therefore a vector of weights. [sent-110, score-0.2]
</p><p>26 Each weight value corresponds to a pixel in Iand determined by the amount of intersection between the analytic line and the pixel itself. [sent-111, score-0.143]
</p><p>27 As a result, only weights that are located in entries associated with the pixels of the digital line have non-zero values. [sent-112, score-0.164]
</p><p>28 We note that even if we use a more accurate model based interpolation, where the line is given more significant width, the matrix  A would remain sparse. [sent-117, score-0.164]
</p><p>29 In this work we focus on the 2D model and in future work we plan to investigate whether in the 3D case our smaller adaptive sampling set can be stored in memory or computed on-the-fly. [sent-122, score-0.21]
</p><p>30 In applications, this means that the Ridgelet transform can be computed by the application of the Radon transform at a given angle, followed by 1D fast wavelet transform [15]. [sent-184, score-0.197]
</p><p>31 We find that Ridgelets are the right mathematical tool in the setup of CT, because the acquisition device is not able to capture, through its sampling process, well localized functionals such as Curvelet coefficients. [sent-185, score-0.357]
</p><p>32 From approximation theoretical perspective, the mathematical foundation of our adaptive algorithm follows the framework of characterizing the images by the appropriate function smoothness spaces and then providing an estimate for the order of convergence. [sent-187, score-0.272]
</p><p>33 ,  of their absolute values and denote the n-term adaptive approximation to f by  fn  ? [sent-322, score-0.198]
</p><p>34 1/ 2 , certain  assumptions on the input function, not only the convergence of the adaptive approximation is ensured, but its rate is also estimated. [sent-347, score-0.198]
</p><p>35 The outcome the theory is that the approximation rate of an adaptive Ridgelet approximation depends on the smoothness of the function in a given Ridgelet smoothness space. [sent-348, score-0.319]
</p><p>36 As we shall see in Section 3, our adaptive acquisition method follows the adaptive Ridgelet approximation to the image Ias a model. [sent-349, score-0.586]
</p><p>37 , Then we use these coefficients in order to select the next set of line projections that are considered as best candidates to project I with, in the subsequent iteration. [sent-351, score-0.469]
</p><p>38 We then ask, how many line projections are needed as rows in the matrix A , such that the image of Figure 3. [sent-354, score-0.433]
</p><p>39 1) contains only 8 rows associated with 8 line projections. [sent-360, score-0.187]
</p><p>40 This is achieved by selecting the unique four pairs of line projections that 222111999755  are the immediate neighbors of each of the four lines associated with the edges of the white square. [sent-366, score-0.459]
</p><p>41 1(b) and (c) show the locations of the line projections and the reconstructed image, respectively. [sent-368, score-0.418]
</p><p>42 The moral of this example, which correlates well with the theory in [5], is that during the acquisition process, we should try to adaptively sample the line projections that are  aligned and centered around the edges of the image. [sent-369, score-0.647]
</p><p>43 Initialization: We create an initial sampling matrix A using a relatively small uniform set of line projections and sample the image I to obtain an initial observations vector y . [sent-374, score-0.523]
</p><p>44 The number of the initial line projections is  determined relative to the image size. [sent-375, score-0.406]
</p><p>45 256 , we measured 8 equally spaced line integrals at eight uniformly spaced angles, which generates a total of 64 initial measurements that are about 0. [sent-377, score-0.318]
</p><p>46 TV Minimization: The inputs to this step are: an updated sampling matrix A (with new additional rows that correspond to the newly acquired line projections) , an observations vector y and the previous approximation as the initial guess. [sent-384, score-0.391]
</p><p>47 This speeds up this step in the algorithm, but in some cases, its effect on the next analysis step implies that more line projections are needed to be acquired in order to achieve the same reconstruction quality. [sent-392, score-0.48]
</p><p>48 In any case, our adaptive acquisition process terminates  U? [sent-393, score-0.352]
</p><p>49 %  if one of the following conditions: or number of rows in A # L , holds, where % is a predetermined threshold and L is a limit on the total number of line projections that is permitted to be acquired. [sent-408, score-0.452]
</p><p>50 of Ridgelet coefficients by the application of Radon transform followed by the application of wavelet transform, as shown in (2. [sent-411, score-0.187]
</p><p>51 In practice, we realize that if we choose the number of angles to be a quarter of the image length, then our sampling scheme is sufficiently dense for high quality reconstruction, but not too much as to lead to subsequent unnecessary acquisition. [sent-414, score-0.14]
</p><p>52 256 , we compute the Ridgelet coefficients for only 64 uniformly spaced angles, ? [sent-416, score-0.136]
</p><p>53 For our experimental results, we applied the univariate discrete Haar wavelet [15] transform at each of the 64 angles to the 256 computed line projections. [sent-424, score-0.316]
</p><p>54 In this case, the discrete sampling of  Ridgelet coefficients is controlled by the pairs ( ? [sent-450, score-0.151]
</p><p>55 Adaptive Sampling of New Line Projections: The analysis of the Ridgelet coefficients , computed at step 3, enables us to decide who are the new line projections that are added to A as new rows. [sent-472, score-0.469]
</p><p>56 222111999866  we choose these line projections to be associated with the M largest Ridgelet coefficients that have not yet been marked as sampled by the algorithm. [sent-478, score-0.516]
</p><p>57 The goal of the selected line projections is to approximate (2. [sent-479, score-0.389]
</p><p>58 3, we see an illustration of the support of the Haar Ridgelet function (dashed lines) and the associated two line projections (inner lines) within its support. [sent-486, score-0.41]
</p><p>59 Now, we look closer at the implication of using only two line projections to approximate the value of the Haar Ridgelet. [sent-488, score-0.389]
</p><p>60 In this case, the two values of the line projections that we acquire are RI ? [sent-500, score-0.425]
</p><p>61 Hence, the ATA algorithm can be summarized as follows: ATA (A , , I , M , L , ,% ) M,  Input: A - Initial sampling matrix , I Input image, M Number of the Ridgelet coefficients subset considered in each iteration. [sent-525, score-0.153]
</p><p>62 Find the M Ridgelet coefficients that have the largest absolute values that have not been sampled yet. [sent-560, score-0.106]
</p><p>63 For each of the newly found Ridgelet coefficients: add new rows to A associated with line  projections, whose sampling approximates the value of the Ridgelet coefficient on the image I . [sent-563, score-0.282]
</p><p>64 :Line tgralsthawer acquiredprasignfcant  Ridgelet coefficient: The external dashed lines correspond to the support of the Ridgelet and the inner lines are the sampled line projections In Figure 3. [sent-582, score-0.479]
</p><p>65 4 (a) and (b) show the uniform acquisition pattern described in the initialization step and the resulted first approximation image , respectively. [sent-588, score-0.307]
</p><p>66 4 (c), (e) and (g) show the newly sampled line projections associated with the next M largest unsampled Ridgelet coefficients in iterations 0, 1 and 2. [sent-591, score-0.587]
</p><p>67 4 (b)), the algorithm finds from the Ridgelet analysis that it should first acquire line projections associated with Ridgelet coefficients from coarse resolution as seen in Figs. [sent-605, score-0.526]
</p><p>68 4 (f)), Ridgelet coefficients from finer resolution become significant and the line projections associated with them are acquired as seen in Fig. [sent-610, score-0.519]
</p><p>69 In summary, the ATA algorithm attempts to acquire only line projections that are around and aligned with edge singularities that are ordered by resolution. [sent-613, score-0.476]
</p><p>70 4: Adaptive acquisition of the Ellipse image: Iterations of newly added projection lines and approximations U? [sent-618, score-0.292]
</p><p>71 Experimental Results In this section we compare the ATA algorithm with known limited angle (non-adaptive) methods and also examine the quality of the estimate for the significant Ridgelet coefficients of the image I produced by our algorithm. [sent-622, score-0.193]
</p><p>72 We show that for a given number of line projections measured on the image I ATA produces a , significantly better approximation to I The experiments . [sent-623, score-0.452]
</p><p>73 m line projections (regardless of the target limit), which are m equally spaced line integrals over the angles 0, ? [sent-647, score-0.681]
</p><p>74 Non Adaptive Equally Spaced (NAS): We used equally spaced rotations and a fixed number of line projections at each angle such that the total number of line projections matched the prescribed budget. [sent-655, score-0.905]
</p><p>75 Specifically, m / 2 (equally spaced) line projections were acquired over the angles 0, ? [sent-657, score-0.465]
</p><p>76 In this mode, we uniformly select lines in the Fourier domain of the image and use Fourier coefficients on these lines as the entries of the sampling matrix A. [sent-667, score-0.217]
</p><p>77 Specifically, m Fourier coefficients were taken on the lines associated with the angles 0, ? [sent-668, score-0.18]
</p><p>78 Adaptive Tomography Acquisition (ATA): Our proposed adaptive algorithm (see Section 3). [sent-677, score-0.135]
</p><p>79 2 we see that the ATA algorithm achieves perfect reconstruction using the smallest number of line projections, while the uniform limited angle, non-adaptive acquisition algorithms (NAS  Figure 4. [sent-685, score-0.473]
</p><p>80 1: PSNR comparison between non-adaptive and adaptive acquisition methods for the reconstruction of the 'Shepp-Logan' image. [sent-686, score-0.414]
</p><p>81 3 shows a comparison between PSNR values of methods 2-5 for different numbers of line projections for the 'Shepp-Logan' image. [sent-689, score-0.389]
</p><p>82 Next, we show results with simulated low dose as in [3]. [sent-691, score-0.285]
</p><p>83 For a selected parameter of incident photon count ! [sent-692, score-0.138]
</p><p>84 4 we see a comparison of ATA and NAS using dose simulation for the Shepp-Logan image. [sent-712, score-0.221]
</p><p>85 We see that the image quality produced by ATA is higher for a smaller number of line projections. [sent-713, score-0.184]
</p><p>86 5 shows a clear advantage of ATA over the compared methods on the 'Zubal Head' image under a dose simulation. [sent-715, score-0.221]
</p><p>87 6, we see a plot of PSNR reconstruction values at various simulated dose levels for the Shepp Logan image. [sent-717, score-0.365]
</p><p>88 2: PSNR comparison between non-adaptive and adaptive acquisition methods for the reconstruction of the 'Zubal-head' image. [sent-719, score-0.414]
</p><p>89 100  0  500  1000 1500 Number of line projections  2000  2500  Figure 4. [sent-720, score-0.389]
</p><p>90 We note that currently the running times of the ATA algorithm, simulated on Matlab, are about 7-10 slower than the non-adaptive methods (NAS, NAF) for the same number of line projections. [sent-722, score-0.207]
</p><p>91 This relates to the choice of M , the number of new line projections introduced at each iteration. [sent-723, score-0.389]
</p><p>92 For a given number of line projections L , the choice M ? [sent-724, score-0.389]
</p><p>93 4: PSNR comparison between ATA and NAS for the reconstruction of the 'Shepp-Logan' image at simulated incident photon count ! [sent-727, score-0.264]
</p><p>94 5: PSNR comparison between ATA and NAS for the reconstruction of the 'Zubal-head' image at simulated incident photon count ! [sent-731, score-0.264]
</p><p>95 6: PSNR comparison between the ATA algorithm and NAS after their application to the 'Shepp-Logan' image for various simulated incident photon counts . [sent-734, score-0.179]
</p><p>96 Conclusion and Future Work In this paper we propose a mathematical model for adaptive CT acquisition whose theoretical goal is to radically reduce dosage levels, while maintaining high quality reconstruction. [sent-745, score-0.516]
</p><p>97 We presented numerical simulations that demonstrate the potential of the mathematical model of adaptive acquisition and compared our results to known limited angle, non adaptive acquisition methods. [sent-746, score-0.811]
</p><p>98 We plan to explore an  efficient updating scheme that allows the current approximated image to be modified solely by the newly acquired line projections and by the previous approximated image. [sent-752, score-0.505]
</p><p>99 Furthermore, it should be interesting to test other TV solvers such as [19] and see if they (or a modified version of them) are better suited to the adaptive scheme proposed in this paper. [sent-754, score-0.18]
</p><p>100 “MBIR aims to outshine ASIR for sharpness, CT dose reduction”, article in AuntMinnie, 18th May, 2010. [sent-781, score-0.221]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ridgelet', 0.713), ('ata', 0.28), ('projections', 0.246), ('dose', 0.221), ('acquisition', 0.217), ('line', 0.143), ('adaptive', 0.135), ('ct', 0.117), ('tomography', 0.106), ('nas', 0.103), ('psnr', 0.097), ('ridgelets', 0.083), ('coefficients', 0.08), ('radon', 0.077), ('photon', 0.077), ('dosage', 0.066), ('naf', 0.066), ('oracle', 0.065), ('simulated', 0.064), ('approximation', 0.063), ('reconstruction', 0.062), ('wavelet', 0.062), ('tv', 0.06), ('radiation', 0.059), ('spaced', 0.056), ('haar', 0.055), ('sampling', 0.052), ('singularities', 0.051), ('crtf', 0.05), ('fbp', 0.05), ('angle', 0.048), ('angles', 0.047), ('cancer', 0.045), ('transform', 0.045), ('newly', 0.043), ('figs', 0.041), ('quality', 0.041), ('mathematical', 0.041), ('incident', 0.038), ('shall', 0.036), ('acquire', 0.036), ('sensing', 0.034), ('brenner', 0.033), ('helical', 0.033), ('logan', 0.033), ('mbir', 0.033), ('muinutv', 0.033), ('lines', 0.032), ('acquired', 0.029), ('reconstructed', 0.029), ('iterations', 0.028), ('fourier', 0.028), ('uniform', 0.027), ('compressive', 0.027), ('crt', 0.027), ('tomographic', 0.026), ('functionals', 0.026), ('sampled', 0.026), ('bivariate', 0.024), ('risks', 0.024), ('ii', 0.024), ('solver', 0.024), ('theory', 0.024), ('limited', 0.024), ('solvers', 0.024), ('equipped', 0.024), ('flux', 0.023), ('count', 0.023), ('rows', 0.023), ('scanning', 0.023), ('equally', 0.023), ('ak', 0.023), ('non', 0.023), ('edition', 0.023), ('exposed', 0.023), ('integrals', 0.023), ('plan', 0.023), ('noise', 0.022), ('au', 0.022), ('matrix', 0.021), ('patient', 0.021), ('modified', 0.021), ('ri', 0.021), ('associated', 0.021), ('setup', 0.021), ('predetermined', 0.021), ('minimization', 0.02), ('simulations', 0.019), ('discrete', 0.019), ('limit', 0.019), ('iterates', 0.019), ('ellipse', 0.018), ('levels', 0.018), ('exposure', 0.018), ('initial', 0.017), ('edges', 0.017), ('smoothness', 0.017), ('multiresolution', 0.017), ('selective', 0.017), ('theoretical', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="35-tfidf-1" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Amir Averbuch, Shai Dekel</p><p>Abstract: One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms.</p><p>2 0.099678315 <a title="35-tfidf-2" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>Author: Jirí Filip, Radomír Vávra, Michal Haindl, Pavel Žid, Mikuláš Krupika, Vlastimil Havran</p><p>Abstract: In this paper we introduce unique publicly available dense anisotropic BRDF data measurements. We use this dense data as a reference for performance evaluation of the proposed BRDF sparse angular sampling and interpolation approach. The method is based on sampling of BRDF subspaces at fixed elevations by means of several adaptively-represented, uniformly distributed, perpendicular slices. Although this proposed method requires only a sparse sampling of material, the interpolation provides a very accurate reconstruction, visually and computationally comparable to densely measured reference. Due to the simple slices measurement and method’s robustness it allows for a highly accurate acquisition of BRDFs. This in comparison with standard uniform angular sampling, is considerably faster yet uses far less samples.</p><p>3 0.07756865 <a title="35-tfidf-3" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>Author: Giuseppe Ottaviano, Pushmeet Kohli</p><p>Abstract: Traditional video compression methods obtain a compact representation for image frames by computing coarse motion fields defined on patches of pixels called blocks, in order to compensate for the motion in the scene across frames. This piecewise constant approximation makes the motion field efficiently encodable, but it introduces block artifacts in the warped image frame. In this paper, we address the problem of estimating dense motion fields that, while accurately predicting one frame from a given reference frame by warping it with the field, are also compressible. We introduce a representation for motion fields based on wavelet bases, and approximate the compressibility of their coefficients with a piecewise smooth surrogate function that yields an objective function similar to classical optical flow formulations. We then show how to quantize and encode such coefficients with adaptive precision. We demonstrate the effectiveness of our approach by com- paring its performance with a state-of-the-art wavelet video encoder. Experimental results on a number of standard flow and video datasets reveal that our method significantly outperforms both block-based and optical-flow-based motion compensation algorithms.</p><p>4 0.067685112 <a title="35-tfidf-4" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>5 0.056794971 <a title="35-tfidf-5" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<p>Author: Won Hwa Kim, Moo K. Chung, Vikas Singh</p><p>Abstract: The analysis of 3-D shape meshes is a fundamental problem in computer vision, graphics, and medical imaging. Frequently, the needs of the application require that our analysis take a multi-resolution view of the shape ’s local and global topology, and that the solution is consistent across multiple scales. Unfortunately, the preferred mathematical construct which offers this behavior in classical image/signal processing, Wavelets, is no longer applicable in this general setting (data with non-uniform topology). In particular, the traditional definition does not allow writing out an expansion for graphs that do not correspond to the uniformly sampled lattice (e.g., images). In this paper, we adapt recent results in harmonic analysis, to derive NonEuclidean Wavelets based algorithms for a range of shape analysis problems in vision and medical imaging. We show how descriptors derived from the dual domain representation offer native multi-resolution behavior for characterizing local/global topology around vertices. With only minor modifications, the framework yields a method for extracting interest/key points from shapes, a surprisingly simple algorithm for 3-D shape segmentation (competitive with state of the art), and a method for surface alignment (without landmarks). We give an extensive set of comparison results on a large shape segmentation benchmark and derive a uniqueness theorem for the surface alignment problem.</p><p>6 0.054583572 <a title="35-tfidf-6" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>7 0.054189418 <a title="35-tfidf-7" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>8 0.044587478 <a title="35-tfidf-8" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>9 0.043284226 <a title="35-tfidf-9" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>10 0.043085966 <a title="35-tfidf-10" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>11 0.039945006 <a title="35-tfidf-11" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>12 0.039794479 <a title="35-tfidf-12" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>13 0.039632514 <a title="35-tfidf-13" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>14 0.037799969 <a title="35-tfidf-14" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>15 0.036844071 <a title="35-tfidf-15" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>16 0.036305368 <a title="35-tfidf-16" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>17 0.036046553 <a title="35-tfidf-17" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>18 0.035656814 <a title="35-tfidf-18" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>19 0.034769449 <a title="35-tfidf-19" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>20 0.034712441 <a title="35-tfidf-20" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.093), (1, 0.049), (2, -0.019), (3, 0.041), (4, 0.006), (5, -0.001), (6, -0.01), (7, -0.029), (8, -0.01), (9, 0.003), (10, -0.01), (11, -0.001), (12, -0.006), (13, -0.052), (14, -0.004), (15, 0.005), (16, 0.056), (17, 0.008), (18, 0.034), (19, 0.034), (20, -0.005), (21, 0.013), (22, -0.013), (23, -0.002), (24, 0.006), (25, -0.014), (26, -0.042), (27, -0.068), (28, 0.011), (29, -0.015), (30, -0.014), (31, 0.039), (32, -0.001), (33, 0.012), (34, -0.056), (35, 0.008), (36, -0.002), (37, -0.004), (38, -0.048), (39, 0.014), (40, 0.007), (41, 0.001), (42, 0.003), (43, 0.021), (44, -0.042), (45, -0.026), (46, -0.005), (47, -0.003), (48, -0.056), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9208203 <a title="35-lsi-1" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Amir Averbuch, Shai Dekel</p><p>Abstract: One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms.</p><p>2 0.65841609 <a title="35-lsi-2" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>Author: Ramakrishna Kakarala, Prabhu Kaliamoorthi, Vittal Premachandran</p><p>Abstract: We show that bilateral symmetry plane estimation for three-dimensional (3-D) shapes may be carried out accurately, and efficiently, in the spherical harmonic domain. Our methods are valuable for applications where spherical harmonic expansion is already employed, such as 3-D shape registration, morphometry, and retrieval. We show that the presence of bilateral symmetry in the 3-D shape is equivalent to a linear phase structure in the corresponding spherical harmonic coefficients, and provide algorithms for estimating the orientation of the symmetry plane. The benefit of using spherical harmonic phase is that symmetry estimation reduces to matching a compact set of descriptors, without the need to solve a correspondence problem. Our methods work on point clouds as well as large-scale mesh models of 3-D shapes.</p><p>3 0.58768022 <a title="35-lsi-3" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>Author: Shireen Y. Elhabian, Aly A. Farag</p><p>Abstract: Conventional subspace construction approaches suffer from the need of “large-enough ” image ensemble rendering numerical methods intractable. In this paper, we propose an analytic formulation for low-dimensional subspace construction in which shading cues lie while preserving the natural structure of an image sample. Using the frequencyspace representation of the image irradiance equation, the process of finding such subspace is cast as establishing a relation between its principal components and that of a deterministic set of basis functions, termed as irradiance harmonics. Representing images as matrices further lessen the number of parameters to be estimated to define a bilinear projection which maps the image sample to a lowerdimensional bilinear subspace. Results show significant impact on dimensionality reduction with minimal loss of information as well as robustness against noise.</p><p>4 0.58249003 <a title="35-lsi-4" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>Author: Laurent Sifre, Stéphane Mallat</p><p>Abstract: An affine invariant representation is constructed with a cascade of invariants, which preserves information for classification. A joint translation and rotation invariant representation of image patches is calculated with a scattering transform. It is implemented with a deep convolution network, which computes successive wavelet transforms and modulus non-linearities. Invariants to scaling, shearing and small deformations are calculated with linear operators in the scattering domain. State-of-the-art classification results are obtained over texture databases with uncontrolled viewing conditions.</p><p>5 0.57809395 <a title="35-lsi-5" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>Author: Jirí Filip, Radomír Vávra, Michal Haindl, Pavel Žid, Mikuláš Krupika, Vlastimil Havran</p><p>Abstract: In this paper we introduce unique publicly available dense anisotropic BRDF data measurements. We use this dense data as a reference for performance evaluation of the proposed BRDF sparse angular sampling and interpolation approach. The method is based on sampling of BRDF subspaces at fixed elevations by means of several adaptively-represented, uniformly distributed, perpendicular slices. Although this proposed method requires only a sparse sampling of material, the interpolation provides a very accurate reconstruction, visually and computationally comparable to densely measured reference. Due to the simple slices measurement and method’s robustness it allows for a highly accurate acquisition of BRDFs. This in comparison with standard uniform angular sampling, is considerably faster yet uses far less samples.</p><p>6 0.56184512 <a title="35-lsi-6" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>7 0.55865282 <a title="35-lsi-7" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>8 0.55441892 <a title="35-lsi-8" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>9 0.55321819 <a title="35-lsi-9" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>10 0.54413801 <a title="35-lsi-10" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>11 0.5395323 <a title="35-lsi-11" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>12 0.53938681 <a title="35-lsi-12" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>13 0.53576064 <a title="35-lsi-13" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>14 0.53276527 <a title="35-lsi-14" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>15 0.52740186 <a title="35-lsi-15" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>16 0.52350187 <a title="35-lsi-16" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>17 0.52313328 <a title="35-lsi-17" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>18 0.52242446 <a title="35-lsi-18" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>19 0.5209983 <a title="35-lsi-19" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>20 0.51287538 <a title="35-lsi-20" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.128), (16, 0.048), (26, 0.051), (29, 0.227), (33, 0.201), (47, 0.013), (67, 0.075), (69, 0.041), (77, 0.011), (80, 0.012), (87, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8378877 <a title="35-lda-1" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>2 0.83131254 <a title="35-lda-2" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>Author: Rui Li, Edward H. Adelson</p><p>Abstract: Sensing surface textures by touch is a valuable capability for robots. Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface texxtures. The GelSight sensor, when pressed against a surfacce, delivers a height map. This can be treated as an image, aand processed using the tools of visual texture analysis. WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. Our system can correctly categorize materials fromm this database with high accuracy. This suggests that the GGelSight sensor can be useful for material recognition by roobots.</p><p>same-paper 3 0.8045103 <a title="35-lda-3" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Amir Averbuch, Shai Dekel</p><p>Abstract: One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms.</p><p>4 0.80046242 <a title="35-lda-4" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>5 0.75922173 <a title="35-lda-5" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>6 0.75483257 <a title="35-lda-6" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>7 0.74566418 <a title="35-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.74076962 <a title="35-lda-8" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>9 0.73922062 <a title="35-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.73874539 <a title="35-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.73850685 <a title="35-lda-11" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>12 0.73821104 <a title="35-lda-12" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>13 0.73494661 <a title="35-lda-13" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>14 0.73452783 <a title="35-lda-14" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>15 0.7331422 <a title="35-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.73280704 <a title="35-lda-16" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>17 0.73201346 <a title="35-lda-17" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>18 0.73197097 <a title="35-lda-18" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>19 0.73186141 <a title="35-lda-19" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>20 0.73057956 <a title="35-lda-20" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
