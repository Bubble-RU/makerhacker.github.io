<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-12" href="#">cvpr2013-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</h1>
<br/><p>Source: <a title="cvpr-2013-12-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Antunes_A_Global_Approach_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>Reference: <a title="cvpr-2013-12-reference" href="../cvpr2013_reference/cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 pt s  Abstract This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. [sent-4, score-0.738]
</p><p>2 These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. [sent-5, score-0.183]
</p><p>3 We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. [sent-6, score-0.235]
</p><p>4 In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent. [sent-8, score-0.169]
</p><p>5 Introduction  A set of parallel lines in the scene project into a pencil of lines intersecting in the so-called vanishing point (VP). [sent-10, score-0.344]
</p><p>6 The VP is the image of the point at infinity where the parallel lines intersect and encodes their common direction. [sent-11, score-0.078]
</p><p>7 In the case of man-made environments, the sets of parallel lines are usually orthogonal to each other, and the detection of the corresponding VPs enables to accomplish different tasks. [sent-12, score-0.266]
</p><p>8 The automatic detection of VPs using sparse edges [6] or edge gradients [19] is a problem of multi-model fitting where the models are line pencils. [sent-14, score-0.218]
</p><p>9 It is in general a ”chicken-and-egg” problem because we neither know the number and parameters of the models (the vanishing points), nor the edges that belong to each model (the mem-  (a)ManhatanWorld(b)Multipleorthogonaltriplets Figure 1. [sent-15, score-0.208]
</p><p>10 In [1], Antone and Teller suggests to carry the VP detection using ExpectationMaximization (EM) with the E-step computing the probability distributions of the input lines passing through the hypothesized VPs, and the M-step refining the VP models by maximizing the likelihood of the observed data. [sent-20, score-0.123]
</p><p>11 However, the process is iterative and requires a good initial estimate that is typically accomplished by clustering the edges assuming a world dominated by either 3 (Manhattan) [1, 12] or 5 (Atlanta) [19] mutually orthogonal vanishing directions (VDs). [sent-22, score-0.524]
</p><p>12 Finally, Tardif has recently proposed a new image-based consistency metric to be used with J-Linkage for clustering the edges into pencils of lines [21]. [sent-24, score-0.228]
</p><p>13 The works above perform the separate estimation of the VPs in the image, which, in many cases, is followed by  1 1 13 3 3 3 3 46 4  grouping the result into directions that are mutually orthogonal [21]. [sent-26, score-0.288]
</p><p>14 In this case, the VP detection is no longer a problem of multiple model fitting, but the problem of fitting a single triplet of mutually orthogonal VPs in the presence of edges that are outliers. [sent-28, score-0.468]
</p><p>15 The disadvantages of this type of approach are that additional VDs that might exist are passed undetected, and the methods cannot handle images with more than one set of Manhattan-world directions for which multi-model fitting is again required (see Fig. [sent-30, score-0.099]
</p><p>16 Contributions This article addresses the problem of detecting VPs in uncalibrated images using either edges or line segments, and (given the intrinsic calibration) the problem of grouping the detection results into sets of mutually orthogonal VDs. [sent-34, score-0.429]
</p><p>17 Our article goes towards this direction and formulates for the first time the detection of VPs as an Uncapacited Facility Location (UFL) problem [14] that can be solved using a local message passing approach [14, 15]. [sent-37, score-0.159]
</p><p>18 i nM tahney G pariuorss iwanor sphere uaflatetre back-projecting the edges and VPs [3, 1, 12, 4, 19]. [sent-45, score-0.102]
</p><p>19 hHeo gwe-ever, and in order to avoid iterative non-linear minimization, he works with the maximum orthogonal distances to the edge endpoints rather than considering the mean distance to all points. [sent-48, score-0.233]
</p><p>20 We show that this minimization problem can be solved in closed-form and propose new functions D(e, v) and W(S) that improve tohpeo osev nereawll fitting rness Dult(se ,wvh)il aen keeping computation tractable. [sent-49, score-0.105]
</p><p>21 A global solution for detecting multiple sets of mutually orthogonal VDs: The existing methods for detecting mutually orthogonal VDs assume that the image depicts a single Manhattan-world configuration [17, 4]. [sent-51, score-0.552]
</p><p>22 1(b) showing more than one group of mutually orthogonal directions. [sent-53, score-0.254]
</p><p>23 The multi-model fitting is solved in a global manner by casting the problem as an Hierarchical Facility Location (HFL) problem [8]. [sent-55, score-0.088]
</p><p>24 Finally, the orthogonal image distance between a point q and a line lis given by  1)T,  d⊥(q,l) =? [sent-68, score-0.219]
</p><p>25 2 presents the FL and HFL problems, and shows how they can be solved using a message passing approach; Sec. [sent-70, score-0.103]
</p><p>26 4 concerns the detection of multiple orthogonal VP triplets; and finally, the experimental results are presented in Sec. [sent-72, score-0.204]
</p><p>27 The Facility Location Problems This section briefly introduces the problems of Uncapacited Facility Location (UFL) and Hierarchical Facility 1 1 13 3 3 3 3 57 5  Location (HFL) that play a key role in the global approaches for detecting VPs and clustering mutually orthogonal VDs. [sent-75, score-0.304]
</p><p>28 formulated the simultaneous segmentation of registered 2D images and 3D points as a hierarchical exemplar-based clustering instance [7], a problem that is closely related to UFL, and that was solved using a greedy bottom-up affinity propagation approach [23]. [sent-78, score-0.099]
</p><p>29 Uncapacited Facility Location (UFL) problem Suppose that you need to open a set of facilities vj0 to serve N customers ei ∈ E whose locations are known. [sent-85, score-0.2]
</p><p>30 Given a set V0 comprising M∈0E possible facility laorceatkinoonws,n t. [sent-86, score-0.273]
</p><p>31 M=i0jc o1,≥xnyi0sj txr=a∈i0jn1,t{ ∀e0 in,sj1u}re,s∀tih,jaech (u1s)-  tomer is assigned to exactly one facility, while inequality of the last constraint guarantees that each customer is only served by facilities that were opened. [sent-101, score-0.101]
</p><p>32 Thefirstlayer 0  l = 0 corresponds to the UFL problem, where the customers ei need to be served by the facilities vj0. [sent-111, score-0.228]
</p><p>33 Given a set of potential Ml facility locations Vl at layer l, the cost vjl : Vl → R for opening ttyhe l facility vjl, and the cost cljk : Vl−1 ×Vl → R for the facility vkl supplying the facility vjl−1, the goal of HFL is to find the vector x = {x0. [sent-115, score-1.287]
</p><p>34 1 are that (i) if a facility vjl−1 is closed in layer l−1, then vjl−1 will not need to be stocked by a storage facility vjl, whereas (ii) if a facility vjl−1 is open, then it must be stocked by a facility in the next layer l. [sent-133, score-1.206]
</p><p>35 Solving the UFL and HFL problems using the  max-sum algorithm In [15, 14] Lazic et al show how to solve the UFL problem using a local message passing approach. [sent-138, score-0.08]
</p><p>36 Algorithm for vanishing point detection This section shows that the detection and estimation of VPs can be formulated as an instance of the UFL problem discussed in Sec. [sent-151, score-0.189]
</p><p>37 Such formulation requires defining a consistency metric D(ei, vj0) that measures the consistency of an edge ei with a putative VP vj0, and a function W(S, w) that, given a subset ofedges S, computes the most likely vanishing point v. [sent-154, score-0.452]
</p><p>38 Vanishing point detection as a UFL problem Let ei ∈ E with i= 1. [sent-157, score-0.146]
</p><p>39 iNs Nto b assign hto e deagech e edge ei a oVmP vj0 ∈ V0 using as few unique VP models as possible. [sent-162, score-0.16]
</p><p>40 This mul∈ti-m Vodel fitting problem can be casted as an instance of the UFL problem as follows: consider that the edges ei are the customers and the putative VPs vj0 are the facilities. [sent-163, score-0.326]
</p><p>41 the consistency between ei and vj0, and let vj0 be the cost for adding vj0 in the final VP assignment. [sent-165, score-0.192]
</p><p>42 In the case of no time limitations, V0 can comprise all the point ein catesrese ocfti noons t mb eetw leimeint pairs o,f V lines li, lj fitting every possible pair of image edges ei, ej, respectively. [sent-170, score-0.232]
</p><p>43 The consistency function D(e, v) Given an edge ei, comprising Pi points ek with k = 1. [sent-175, score-0.125]
</p><p>44 Pi, and a putative VP vj0, the objective is to find a cost function D(ei , vj0) that evaluates how well a line lj in the pencil centered in vj0 can fit the edge points ek. [sent-178, score-0.276]
</p><p>45 We propose to determine the line lj that minimizes the sum of the squares of the geometric distances to the points, and use the root mean value of this sum as the client-facility cost ci0j . [sent-179, score-0.094]
</p><p>46 Any line lj going through vj0 can be parametrized as follows lj(λ)  ∼  (1 − λ)[a]×vj0 + λ[b]×v0j ,  with a, b being any two points non-collinear with vj0, and λ being a free parameter. [sent-181, score-0.099]
</p><p>47 For the sake of convenience, the points a, b are typically chosen as being the endpoints of a line segment orthogonal to the edge ei and passing through its midpoint. [sent-182, score-0.453]
</p><p>48 P=i1d2⊥(ek, lj(λ)  From the formula for the orthogonal distance d⊥, it comes  after some algebraic manipulations that  k? [sent-184, score-0.238]
</p><p>49 Given the particular arrangement between a, b, and the edge ei we choose the root λ0 that is closest to 0. [sent-208, score-0.16]
</p><p>50 Taking into account the formula of the orthogonal distance, it comes after some algebraic manipulations that W(S) =  mvin  vTQv  subject to :  vT p = 1  with  Q =i? [sent-222, score-0.238]
</p><p>51 where wi is the length of each edge ei, and p = (0 0 Remark that the purpose of the constraint is to assure that v3 = 1 complies with the formula for computing the orthogonal distance d⊥. [sent-224, score-0.237]
</p><p>52 Detection of multiple orthogonal triplets We assume in this section that a set of VPs has already been extracted using any type of VP detection approach e. [sent-242, score-0.298]
</p><p>53 3, and the objective is to detect multiple mutually orthogonal directions in the scene. [sent-245, score-0.304]
</p><p>54 Given the intrinsic calibration matrix K, two VPs vj0 and vk0 are orthogonal if the following relation is verified  vj0Tωv0k  = 0,  (4)  where ω = K−TK−1 is the image of the absolute conic [10]. [sent-247, score-0.205]
</p><p>55 Let the set vm1 = {v0j, vk0, vl0} be a mutually orthogonal triplet, meaning that each pair of VPs in vm1 verifies Eq. [sent-248, score-0.254]
</p><p>56 2): at the bottom layer l= 0 we have the edges ei and the VPs vj0, and at the top  layer l= 1we have the orthogonal triplets vk1. [sent-252, score-0.523]
</p><p>57 For solving (ii), we add the vanishing groups vm1 = {v0j} containing a single VP to V1 whenever there is no other VP in Vin0i whose VD makes up an angle in the 1 1 13 3 3 3 4 80 8  ×  range [2π−θ, 2π+θ] with the VD of vj0. [sent-260, score-0.184]
</p><p>58 The costs vm1 for sets vm1 containing a single VP are always less than for orthogonal triplets, keeping these VPs in the final labeling. [sent-261, score-0.229]
</p><p>59 Finally, the issue (iii) is solved by noting that since we construct each orthogonal triplet vm1 individually, we can keep track of similar VPs in vm1 after the HFL labeling. [sent-262, score-0.273]
</p><p>60 For each VP vj, we generate a pencil of N line segments in the image, which are sampled into a discrete set of points ek with k = 1. [sent-280, score-0.166]
</p><p>61 Each set ei has a  length between 20 to 200 pixels. [sent-284, score-0.125]
</p><p>62 3 compares four different consistency metrics for quantifying ci0j for the UFL clustering method: (i) UFL+D - our measure D(ei, vj0) described in Eq. [sent-287, score-0.081]
</p><p>63 3 using all the points in ei, (2) UFL+D2 - the same measure D(ei , vj0) using only the two end points the edge ei; (3) UFL+Tar. [sent-288, score-0.073]
</p><p>64 D - the consistency metric of Tardif described in [21], and (4) UFL+Gauss - operate on the Gaussian sphere by analyzing the angle between the normal to the line li and vj0. [sent-289, score-0.13]
</p><p>65 The performance of the three metrics operating in the image plane are similar for low noise, but our metric D(ei, vj0), which uses all the points in ei being clearly the top-performer for higher noise magnitudes. [sent-291, score-0.164]
</p><p>66 The consistency metric D(ei , vj0) operating on the two end points of li performs slightly worse, but with a high increase in computational efficiency (less input data). [sent-292, score-0.092]
</p><p>67 By taking this results in consideration, we decided to select UFL+D2 for measuring the consistency between edges and VPs, being a good trade-off between accuracy and computational efficiency. [sent-293, score-0.114]
</p><p>68 Given a cluster S containing N lines li, we need to compute a benet tae crl uVsPte ers Stim coantitoanin. [sent-294, score-0.081]
</p><p>69 inAgs iNn tl ihnee previous experiment, we randomly generate a pencil S containing N lines, sam4  1  W, N=30  ed(g)lan1620481T W2a ,rN . [sent-295, score-0.092]
</p><p>70 Accuracy of the estimation of VPs given a pencil of N lines. [sent-297, score-0.073]
</p><p>71 ple the lines into a discrete set of points, perturb the points using Gaussian noise of different magnitudes, and then fit a line li to these points in the least-squares sense. [sent-299, score-0.136]
</p><p>72 A careful analysis of the graphic shows that our VP estimator provides better estimates for the same pencil of lines, being considerably more robust to the noise level. [sent-308, score-0.073]
</p><p>73 In order to handle possible outlier edges detected in the images, we added the empty sets v0 and v1 to both UFL and HFL, which have the facility  v0,1  costs =0 and constant connections costs. [sent-314, score-0.361]
</p><p>74 Experiments in YUD using the supplied lines  mao%gesif143690152870 0 012345678aegsfmio%154321098760 0 024681024TOau1r6dsfi degrees  %  (a) Cumul. [sent-317, score-0.079]
</p><p>75 Cumulative consistency error computed using our D for the three groups of ground truth edges (belonging to orthogonal VDs). [sent-331, score-0.315]
</p><p>76 For each image we compute the root-mean-square (rms) consistency error across all lines fitting the estimated VPs; GT corresponds to the ground truth VPs provided by YUD. [sent-332, score-0.18]
</p><p>77 Regarding the clustering of the lines, our approach shows some improvements, having in 92% of the images less than  2% of the lines wrongly labeled. [sent-344, score-0.09]
</p><p>78 Given the initial set of VPs obtained using the UFL algorithm, the objective now is to detect the manhattan directions, or similarly, a single rotation. [sent-347, score-0.106]
</p><p>79 [4], and (2) the rotation obtained using the three most orthogonal VDs of Tardif after fitting a perfect orthogonal frame to these VPs [6]. [sent-349, score-0.431]
</p><p>80 Despite of the close performance in terms of estimating the three orthogonal VDs, our method is computational more efficient, running more than 50 times faster than Bazin. [sent-352, score-0.183]
</p><p>81 Experiments in YUD using extracted edges In this section, we test our HFL algorithm for detecting the Manhattan frame in the YUD, but using edges extracted trough Tardif’s detector [21] instead of the line segments supplied by the database (see Fig. [sent-355, score-0.217]
</p><p>82 The left images show the extracted edges (orange), while the detection results are shown on the right. [sent-365, score-0.082]
</p><p>83 In  each example (row) we detected 2 groups of orthogonal triplets with the blue VD in common. [sent-369, score-0.295]
</p><p>84 8(b), we identified two different mutually orthogonal triplets (remark that for the analysis in Fig. [sent-377, score-0.348]
</p><p>85 7, the orthogonal triplet with more lines was automatically selected), one corresponding to the Manhattan frame and the other is due to the squares on the floor. [sent-378, score-0.312]
</p><p>86 We also identified that both orthogonal frames share the same vertical direction. [sent-379, score-0.183]
</p><p>87 Experiments in scenes containing multiple orthogonal triplets This section shows experiments on real images containing more than one orthogonal triplet of VDs. [sent-382, score-0.565]
</p><p>88 (Row 1) extracted edges (left) and clustering obtained using UFL (right); (Row 2) 3 groups of orthogonal triplets were detected using HFL, the 2 on the left have the blue VD in common shown in Fig. [sent-384, score-0.384]
</p><p>89 We run Tardif’s edge detector for obtaining the input edges for our UFL and HFL algorithms. [sent-387, score-0.096]
</p><p>90 We are able to handle high-resolution images containing many edges, detecting simultaneously both multiple orthogonal triplets as well as single VDs. [sent-389, score-0.318]
</p><p>91 Our approach mistakenly assigns the edges on the roof to one orthogonal triplet, but this issue can be a consequence of either a poor estimation of the focal length or an ineffective tuning of the facility costs for HFL. [sent-393, score-0.544]
</p><p>92 Conclusions We presented an automatic and global approach for the detection of VPs and mutual orthogonal VDs. [sent-395, score-0.204]
</p><p>93 The core of the framework is the formulation of these multi-model fitting problems as UFL and HFL instances, which are solved using a message passing approach. [sent-396, score-0.168]
</p><p>94 Globally optimal line clustering and vanishing point estimation in manhattan world. [sent-432, score-0.301]
</p><p>95 Efficient edge-based methods for estimating manhattan frames in urban imagery. [sent-448, score-0.106]
</p><p>96 An automatic approach for camera calibration from vanishing points. [sent-468, score-0.169]
</p><p>97 Solving the uncapacitated facility location problem using message passing algorithms. [sent-500, score-0.375]
</p><p>98 Optimal estimation of vanishing points in a manhattan world. [sent-520, score-0.256]
</p><p>99 A new approach for vanishing point detection in architectural environments. [sent-524, score-0.168]
</p><p>100 Non-iterative approach for fast and accurate vanishing point detection. [sent-541, score-0.147]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ufl', 0.52), ('vps', 0.461), ('vp', 0.309), ('hfl', 0.287), ('facility', 0.273), ('orthogonal', 0.183), ('vanishing', 0.147), ('tardif', 0.131), ('ei', 0.125), ('vds', 0.121), ('vjl', 0.109), ('triplets', 0.094), ('manhattan', 0.09), ('pencil', 0.073), ('yud', 0.073), ('mutually', 0.071), ('triplet', 0.067), ('fitting', 0.065), ('lines', 0.062), ('edges', 0.061), ('ekekt', 0.055), ('uncapacited', 0.055), ('consistency', 0.053), ('vd', 0.053), ('bazin', 0.045), ('lj', 0.044), ('sphere', 0.041), ('cilk', 0.041), ('lazic', 0.041), ('message', 0.04), ('passing', 0.04), ('putative', 0.039), ('facilities', 0.039), ('frey', 0.037), ('vl', 0.037), ('customers', 0.036), ('givoni', 0.036), ('line', 0.036), ('article', 0.035), ('edge', 0.035), ('directions', 0.034), ('customer', 0.034), ('atlanta', 0.032), ('delong', 0.032), ('layer', 0.03), ('ilj', 0.029), ('opening', 0.028), ('served', 0.028), ('clustering', 0.028), ('antone', 0.027), ('antunes', 0.027), ('stocked', 0.027), ('vdp', 0.027), ('vtqv', 0.027), ('costs', 0.027), ('coimbra', 0.024), ('pencils', 0.024), ('solved', 0.023), ('isack', 0.022), ('tretyak', 0.022), ('detecting', 0.022), ('ransac', 0.022), ('calibration', 0.022), ('location', 0.022), ('detection', 0.021), ('operating', 0.02), ('null', 0.02), ('solver', 0.02), ('segments', 0.02), ('containing', 0.019), ('tvhe', 0.019), ('points', 0.019), ('comes', 0.019), ('michel', 0.019), ('formula', 0.019), ('pi', 0.018), ('groups', 0.018), ('ek', 0.018), ('supplied', 0.017), ('manipulations', 0.017), ('remark', 0.017), ('atn', 0.017), ('minimization', 0.017), ('urban', 0.016), ('objective', 0.016), ('differentiating', 0.016), ('infinity', 0.016), ('magenta', 0.016), ('hierarchical', 0.015), ('endpoints', 0.015), ('cast', 0.015), ('messages', 0.015), ('jl', 0.015), ('hypotheses', 0.015), ('acknowledge', 0.015), ('argued', 0.015), ('affinity', 0.014), ('magnitudes', 0.014), ('mk', 0.014), ('cost', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="12-tfidf-1" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>2 0.4582476 <a title="12-tfidf-2" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>3 0.17766005 <a title="12-tfidf-3" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>4 0.13162427 <a title="12-tfidf-4" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>5 0.053047646 <a title="12-tfidf-5" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>Author: Srikumar Ramalingam, Jaishanker K. Pillai, Arpit Jain, Yuichi Taguchi</p><p>Abstract: Junctions are strong cues for understanding the geometry of a scene. In this paper, we consider the problem of detecting junctions and using them for recovering the spatial layout of an indoor scene. Junction detection has always been challenging due to missing and spurious lines. We work in a constrained Manhattan world setting where the junctions are formed by only line segments along the three principal orthogonal directions. Junctions can be classified into several categories based on the number and orientations of the incident line segments. We provide a simple and efficient voting scheme to detect and classify these junctions in real images. Indoor scenes are typically modeled as cuboids and we formulate the problem of the cuboid layout estimation as an inference problem in a conditional random field. Our formulation allows the incorporation of junction features and the training is done using structured prediction techniques. We outperform other single view geometry estimation methods on standard datasets.</p><p>6 0.047236215 <a title="12-tfidf-6" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>7 0.045794167 <a title="12-tfidf-7" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>8 0.044745497 <a title="12-tfidf-8" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>9 0.043679103 <a title="12-tfidf-9" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>10 0.040068053 <a title="12-tfidf-10" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>11 0.037782453 <a title="12-tfidf-11" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>12 0.036717243 <a title="12-tfidf-12" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>13 0.036027931 <a title="12-tfidf-13" href="./cvpr-2013-Nonlinearly_Constrained_MRFs%3A_Exploring_the_Intrinsic_Dimensions_of_Higher-Order_Cliques.html">308 cvpr-2013-Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques</a></p>
<p>14 0.035286743 <a title="12-tfidf-14" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>15 0.034912243 <a title="12-tfidf-15" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>16 0.034162007 <a title="12-tfidf-16" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>17 0.033503767 <a title="12-tfidf-17" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>18 0.033297792 <a title="12-tfidf-18" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>19 0.033031173 <a title="12-tfidf-19" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>20 0.032946948 <a title="12-tfidf-20" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.09), (1, 0.037), (2, 0.017), (3, 0.01), (4, 0.03), (5, -0.015), (6, 0.004), (7, -0.006), (8, -0.03), (9, 0.008), (10, 0.038), (11, 0.035), (12, 0.011), (13, -0.037), (14, -0.055), (15, -0.004), (16, 0.049), (17, 0.107), (18, -0.027), (19, 0.015), (20, 0.012), (21, 0.034), (22, -0.028), (23, -0.001), (24, 0.11), (25, -0.057), (26, -0.067), (27, -0.082), (28, -0.012), (29, 0.069), (30, -0.051), (31, 0.11), (32, -0.068), (33, 0.248), (34, -0.155), (35, 0.172), (36, -0.078), (37, 0.018), (38, 0.114), (39, 0.109), (40, 0.151), (41, -0.176), (42, 0.039), (43, 0.102), (44, -0.028), (45, -0.004), (46, -0.099), (47, 0.153), (48, -0.065), (49, -0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93198234 <a title="12-lsi-1" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>2 0.92135096 <a title="12-lsi-2" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>3 0.66584045 <a title="12-lsi-3" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>4 0.64365542 <a title="12-lsi-4" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>Author: Srikumar Ramalingam, Jaishanker K. Pillai, Arpit Jain, Yuichi Taguchi</p><p>Abstract: Junctions are strong cues for understanding the geometry of a scene. In this paper, we consider the problem of detecting junctions and using them for recovering the spatial layout of an indoor scene. Junction detection has always been challenging due to missing and spurious lines. We work in a constrained Manhattan world setting where the junctions are formed by only line segments along the three principal orthogonal directions. Junctions can be classified into several categories based on the number and orientations of the incident line segments. We provide a simple and efficient voting scheme to detect and classify these junctions in real images. Indoor scenes are typically modeled as cuboids and we formulate the problem of the cuboid layout estimation as an inference problem in a conditional random field. Our formulation allows the incorporation of junction features and the training is done using structured prediction techniques. We outperform other single view geometry estimation methods on standard datasets.</p><p>5 0.56635141 <a title="12-lsi-5" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>6 0.43592867 <a title="12-lsi-6" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>7 0.40718549 <a title="12-lsi-7" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>8 0.39924294 <a title="12-lsi-8" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>9 0.36962685 <a title="12-lsi-9" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>10 0.34659508 <a title="12-lsi-10" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>11 0.3182303 <a title="12-lsi-11" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>12 0.31206691 <a title="12-lsi-12" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>13 0.29590079 <a title="12-lsi-13" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>14 0.27666116 <a title="12-lsi-14" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>15 0.26858896 <a title="12-lsi-15" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>16 0.26826206 <a title="12-lsi-16" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>17 0.2591933 <a title="12-lsi-17" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>18 0.25459838 <a title="12-lsi-18" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>19 0.24541312 <a title="12-lsi-19" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>20 0.24206471 <a title="12-lsi-20" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.102), (16, 0.028), (26, 0.036), (28, 0.011), (29, 0.011), (33, 0.166), (45, 0.063), (49, 0.268), (59, 0.012), (66, 0.019), (67, 0.034), (69, 0.056), (87, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72235471 <a title="12-lda-1" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>2 0.7010265 <a title="12-lda-2" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>Author: Fei Chen, Huimin Yu, Roland Hu, Xunxun Zeng</p><p>Abstract: In this paper we introduce a new shape-driven approach for object segmentation. Given a training set of shapes, we first use deep Boltzmann machine to learn the hierarchical architecture of shape priors. This learned hierarchical architecture is then used to model shape variations of global and local structures in an energetic form. Finally, it is applied to data-driven variational methods to perform object extraction of corrupted data based on shape probabilistic representation. Experiments demonstrate that our model can be applied to dataset of arbitrary prior shapes, and can cope with image noise and clutter, as well as partial occlusions.</p><p>3 0.66373688 <a title="12-lda-3" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>4 0.63450533 <a title="12-lda-4" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>5 0.61251134 <a title="12-lda-5" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>6 0.61161667 <a title="12-lda-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.6101957 <a title="12-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.60934651 <a title="12-lda-8" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>9 0.60928887 <a title="12-lda-9" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>10 0.60759252 <a title="12-lda-10" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>11 0.60721111 <a title="12-lda-11" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>12 0.60611677 <a title="12-lda-12" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>13 0.60600424 <a title="12-lda-13" href="./cvpr-2013-Bayesian_Grammar_Learning_for_Inverse_Procedural_Modeling.html">57 cvpr-2013-Bayesian Grammar Learning for Inverse Procedural Modeling</a></p>
<p>14 0.60434192 <a title="12-lda-14" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>15 0.60421628 <a title="12-lda-15" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>16 0.60243267 <a title="12-lda-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.60187507 <a title="12-lda-17" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>18 0.60149294 <a title="12-lda-18" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>19 0.60131556 <a title="12-lda-19" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>20 0.60123914 <a title="12-lda-20" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
