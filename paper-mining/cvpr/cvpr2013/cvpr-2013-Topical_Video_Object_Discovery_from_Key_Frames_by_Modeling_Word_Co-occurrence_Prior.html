<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-434" href="#">cvpr2013-434</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</h1>
<br/><p>Source: <a title="cvpr-2013-434-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhao_Topical_Video_Object_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>Reference: <a title="cvpr-2013-434-reference" href="../cvpr2013_reference/cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 s g Abstract A topical video object refers to an object that is frequently highlighted in a video. [sent-3, score-0.962]
</p><p>2 We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. [sent-7, score-1.655]
</p><p>3 Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. [sent-8, score-0.567]
</p><p>4 Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. [sent-10, score-0.938]
</p><p>5 Introduction With the prevalence of video recording devices and the far reach of online social video sharing, we are now making more videos than ever before. [sent-13, score-0.347]
</p><p>6 The videos usually contain a number of topical objects, which refer to objects that are frequently highlighted in the video, e. [sent-14, score-0.874]
</p><p>7 It is of great interests to automatically discover topical objects in videos efficiently as they are essential to the understanding and summarization of the video contents. [sent-17, score-1.079]
</p><p>8 One potential approach to automatically discover video objects is using frequent pattern mining [6]. [sent-18, score-0.57]
</p><p>9 Although significant progress has been made along this path [21] [19], it is still a challenge to discover topical objects in videos automatically using frequent pattern mining methods. [sent-19, score-1.176]
</p><p>10 This makes the frequent  item set mining with video data to be very difficult with the ambiguity of visual items and visual vocabularies. [sent-23, score-0.503]
</p><p>11 To mitigate this challenge, several methods have been proposed to discover topical objects in images and videos [21] [17] [14] [15] [23]. [sent-24, score-0.938]
</p><p>12 For example, Zhao and Yuan [23] have proposed to discover topical objects in videos by considering the correlation of visual items via cohesive sub-graph mining. [sent-26, score-1.085]
</p><p>13 It has demonstrated effectiveness in finding one topical object, but it can only find multiple video objects one by one. [sent-27, score-0.891]
</p><p>14 It can discover multiple objects simultaneously while each object is one topic discovered by LDA model in a top-down manner. [sent-30, score-0.57]
</p><p>15 However, the computational cost will be too high if LDA model is directly leveraged to discover video object, as one second video contains dozens of frames. [sent-31, score-0.422]
</p><p>16 One possible mitigation is to discover the video object from selected key frames only. [sent-32, score-0.438]
</p><p>17 To effectively address the issue of limited training samples, we propose a new topic model which explicitly incorporates a word co-occurrence prior using a Gauss-Markov network over the topic-word distribution in LDA. [sent-34, score-0.559]
</p><p>18 This data-driven word co-occurrence prior can effectively regularize the topic model to learn effectively from limited samples. [sent-36, score-0.543]
</p><p>19 The parameter of the word co-occurrence prior is obtained by analyzing the spatialtemporal word co-occurrence information. [sent-40, score-0.627]
</p><p>20 After that, the topical objects are discovered by the proposed LDA-WCP model. [sent-41, score-0.852]
</p><p>21 First, by using the multiple segmentation and the bag-of-words representation, our method is able to cope with the variant shapes and appearance of the topical video objects. [sent-43, score-0.84]
</p><p>22 Second, through the proposed LDA-WCP model, our method can discover multiple topical objects simultaneously. [sent-44, score-0.89]
</p><p>23 Last but not least, by incorporating the word co-occurrence prior, the proposed LDAWCP model can successfully discover more instances of the topical video objects. [sent-45, score-1.322]
</p><p>24 Related Works Most existing visual object discovery methods fall into one of the two categories: bottom-up based methods that find visual objects from bottom-up, and generative model based methods that find objects through top-down reasoning. [sent-48, score-0.437]
</p><p>25 These methods first translate each image into a collection of “visual words” and then discover the common object through frequently co-occurring words mining [21] [19] [20] [22]. [sent-50, score-0.44]
</p><p>26 Zhao and Yuan characterize all key frames using an affinity graph of all visual features and find the topical object by cohesive sub-graph mining [23]. [sent-59, score-1.071]
</p><p>27 Liu and Chen [8] show promising video object discovery results by combining pLSA with Probabilistic Data Association (PDA) filter based motion model. [sent-66, score-0.366]
</p><p>28 Some other works explore different priors over the topic proportion such as using logistic normal prior [3] or Dirichlet tree prior [1] to develop correlated topic models. [sent-76, score-0.55]
</p><p>29 LDA with Word Co-occurrence Prior To discover topical objects from videos, visual features are extracted from key frames and clustered into visual words first. [sent-80, score-1.179]
</p><p>30 After that we obtain the word co-occurrence prior by analyzing the spatial-temporal word co-occurrence information. [sent-82, score-0.61]
</p><p>31 Each word wdn is associated with a latent topic zdn according to the doc-  ×  ument specific topic proportion vector θd, whose prior is Dirichlet with parameter α. [sent-118, score-1.067]
</p><p>32 The word wdn is sampled from the topic word distribution parameterized by a K V matrix β? [sent-119, score-0.839]
</p><p>33 For each of the Nd word in document d: Choose a topic zdn ∼ Multinomial(θd); Choose a word wdn ∼ Multinomial(βzdn ). [sent-125, score-1.056]
</p><p>34 For each document d, the joint distribution of a topic mixture θd, a set of Nd topics z, and a set of Nd words w is given by:  ? [sent-126, score-0.458]
</p><p>35 Take the video data as an example, a topical object may contain unique patterns composed of multiple cooccurrence features. [sent-148, score-0.942]
</p><p>36 Besides, the video objects may be small and hidden in the cluttered background, these cooccurrence features can provide highly discriminative information to differentiate the topical object from the background clutter. [sent-149, score-0.955]
</p><p>37 This is technically achieved by placing a Markovian smoothness prior p(β) over the topic-word distributions β, which encourages  two words to be categorized into the same topic if there is strong co-occurrence between them. [sent-151, score-0.421]
</p><p>38 In video object discovery, the visual words belonging to the same object co-occur frequently in the video, as shown in Figure 2. [sent-152, score-0.393]
</p><p>39 Therefore, more instances of this object will be categorized to the same topic even when some instances contain the noisy visual words from other objects or the background. [sent-154, score-0.671]
</p><p>40 m(βij− βim)2,  (2)  where Πj represents the words which have co-occurrence with word wj and ? [sent-161, score-0.437]
</p><p>41 m is the co-occurrence weight between word wm and word wj . [sent-162, score-0.66]
</p><p>42 E(βij ) is the co-occurrence evidence for word j within topic i. [sent-163, score-0.469]
</p><p>43 The parameter σi captures the global word co-occurrence smoothness oftopic iand enforces different degrees of smoothness in each topic in order to better adapt the model to the data. [sent-164, score-0.52]
</p><p>44 The larger the parameter σi is, the stronger word co-occurrence is incorporated in topic i. [sent-165, score-0.486]
</p><p>45 The estimation of word co-occurrence prior is introduced in Sec. [sent-166, score-0.342]
</p><p>46 Parameter Estimation for LDA-WCP The influence of word co-occurrence prior is adjusted through the strength parameter σ when estimating values of β. [sent-194, score-0.359]
</p><p>47 iis the Lagrange multipliers for constraint The word co-occurrence prior is included in the objective ? [sent-222, score-0.342]
</p><p>48 The overall algorithm is summarized in algorithm 1:  Algorithm 1 The EM algorithm for LDA-WCP model 1 2 3 4 5 6  7 8 9 10 11  input: The corpus D and word co-occurrence prior p(β). [sent-299, score-0.391]
</p><p>49 output : The topic document matrix γ and the topic word matrix β. [sent-300, score-0.736]
</p><p>50 9 update β with word co-occurrence prior by solving Eq. [sent-304, score-0.342]
</p><p>51 The Word  Co-occurrence  Prior for Videos  For video corpus, we can obtain the word co-occurrence prior by considering the spatial-temporal co-occurrence of words. [sent-307, score-0.502]
</p><p>52 To estimate the word co-occurrence prior, we find the k nearest neighbors for each visual word wj in each video frame Il first. [sent-309, score-0.851]
</p><p>53 The neighbors in frame Il are selected according to their spatial distances with word wj and denote the selected nearest neighbor set in frame Il as . [sent-310, score-0.488]
</p><p>54 Second, we obtain the global neighbor set Πj for each visual word wj by assembling the nearest neighbors of word wj in all frames: Πj = ··· , (14)  Πlj  {Πj1,  Πlj, ··· ,ΠjF}. [sent-311, score-0.761]
</p><p>55 Then we count the number of occurrence of each visual word wmin the neighbor set Πj : N(wm) = |{wm :  wm ∈ Πj}|. [sent-312, score-0.387]
</p><p>56 (15)  After that, we select the top k visual words according to the numbers of their occurrences in the neighbor set Πj and denote the selected visual word set as Π? [sent-313, score-0.511]
</p><p>57 In this section, we show how to obtain the word cooccurrence prior by simply checking their co-occurrence  frequency in the whole video. [sent-324, score-0.381]
</p><p>58 It is important to note that the word co-occurrence prior can also be obtained by considering the frequent pattern mining algorithms [6], or by employing the human knowledge. [sent-325, score-0.599]
</p><p>59 Evaluation To evaluate our approach, we test it on challenging videos for topical object discovery. [sent-327, score-0.789]
</p><p>60 Video Datasets In the first experiment, we discover video objects from fourteen video sequences downloaded from YouTube. [sent-331, score-0.473]
</p><p>61 We test our method on the video sequences one by one, and try to find one topical object from each video. [sent-333, score-0.865]
</p><p>62 Most of the videos have the well-defined topical objects, e. [sent-334, score-0.747]
</p><p>63 It is possible that one video frame contains multiple objects and some video frames contain only one topical object. [sent-338, score-1.152]
</p><p>64 The segment with normal color contains the discovered topical object, while the segments highlighted by the green color correspond to the background region. [sent-348, score-0.899]
</p><p>65 The red bounding boxes indicate the ground truth position of the topical objects and the frames without bounding boxes contain non instances of  topical objects. [sent-349, score-1.683]
</p><p>66 To employ LDA-WCP model, the word co-occurrence prior is estimated by using the top 10 neighbors for each visual word as shown in Sec. [sent-355, score-0.675]
</p><p>67 To quantify the performance of the proposed approach, we manually labeled the ground truth bounding boxes of the instances oftopical objects in each video frame. [sent-357, score-0.365]
</p><p>68 After obtaining a pool of segments from all key frames, object topics are discovered using the proposed LDA-WCP model. [sent-363, score-0.361]
</p><p>69 The more instances of the ground truth object in one topic, the higher the supportiveness of this topic is. [sent-366, score-0.368]
</p><p>70 Single Video Object Discovery Many videos contain a single topical object, e. [sent-370, score-0.768]
</p><p>71 Figure 3 shows some sample results of video object discovery by LDA-WCP model. [sent-374, score-0.366]
</p><p>72 In the video sequences, the topical objects are subject to variations introduced by partial occlusions, scale, viewpoint and lighting condition changes. [sent-375, score-0.874]
</p><p>73 It is possible that some frames contain multiple instances of video objects and some frames do not contain any video Table 1. [sent-376, score-0.603]
</p><p>74 Numbers of topical frames and instances of two topical objects in each video sequence. [sent-377, score-1.707]
</p><p>75 On average, each video have 42 keyframes and the proposed method can correctly discover 16 instances from total 19 instances of topical object. [sent-381, score-1.145]
</p><p>76 Figure 4 shows sample results of multiple topical object discovery. [sent-390, score-0.741]
</p><p>77 For one video, we show two discovered topical objects and each row shows the result of one topical object. [sent-391, score-1.534]
</p><p>78 It can be seen that the proposed approach can categorize the instances of different topical object to different topics, even when one video frame contains multiple types of topical objects. [sent-392, score-1.712]
</p><p>79 ), the ground truth number of topical object instances (INo. [sent-395, score-0.815]
</p><p>80 ) and the corrected detected number of topical object instances (CNo. [sent-396, score-0.815]
</p><p>81 Averagely, the proposed method can correctly discover 3 1instances from total 36 instances of two topical objects. [sent-399, score-0.913]
</p><p>82 These results show that the proposed approach performs well for discovering multiple topical objects from videos simultaneously. [sent-400, score-0.837]
</p><p>83 Comparison with Other Approaches We compare our video object discovery method with two other methods: (1) LDA based approach and (2) sub-graph mining approach. [sent-403, score-0.501]
</p><p>84 (b)  (d) The instances of two discovered topical objects are given. [sent-406, score-0.943]
</p><p>85 The LDA-WCP model categorized more instances of one topical object to the same topic than the LDA model. [sent-408, score-1.046]
</p><p>86 After obtaining a pool of segments from all key frames, object topics are discovered using LDA following the work in [2]. [sent-412, score-0.361]
</p><p>87 To find the topical object using sub-graph mining approach, each key frame is segmented multiple times as our method first. [sent-415, score-0.986]
</p><p>88 As shown in Figure 5(a), our proposed approach outperforms both LDA approach and sub-graph mining approach in terms of the F-measure for single topical object discovery, with an average score of 0. [sent-419, score-0.859]
</p><p>89 We further compare the number ofdiscovered topical object instances by LDA model and the proposed LDA-WCP model. [sent-430, score-0.815]
</p><p>90 Figure 6(a) shows the discovered instance numbers of single video object and Figure 6(b) shows the discovered instance numbers of multiple objects. [sent-431, score-0.474]
</p><p>91 It can be seen that LDA-WCP model can categorize more instances of one object to the same topic than LDA model. [sent-432, score-0.352]
</p><p>92 By incorporating the word co-occurrence prior, LDA-WCP model encourages the words to be categorized to the same topic if there is strong co-occurrence prior between them. [sent-433, score-0.672]
</p><p>93 This implies that LDA-WCP model makes the learned topics more interpretable by considering both the bag-of-words information and the word co-occurrence prior. [sent-434, score-0.379]
</p><p>94 These comparisons clearly show the advantages of the proposed video object discovery technique. [sent-435, score-0.366]
</p><p>95 Conclusion  Video object discovery is a challenging problem due to the possibly large object variations, the complicated dependencies between visual items and the prohibitive computational cost to explore all the candidate set. [sent-437, score-0.345]
</p><p>96 (a) shows the single object discovery performance of our approach (Proposed), LDA approach (LDA) [14] and sub-graph mining approach (Subgraph Mining) [23]. [sent-442, score-0.36]
</p><p>97 tegrates the word co-occurrence prior and the bag-of-words information in a unified way. [sent-444, score-0.342]
</p><p>98 3 Single object discovery  Multiple objects discovery  ertsnaucbmNI12 305 0 12PL3rDoAp45se6d7Vi8eo910 12 314AvgeamuNecntsIbr6543210 0 1PL2rDoAp3s4edV5ie6o78910Avg  (a) (b) Figure 6. [sent-528, score-0.459]
</p><p>99 The number of discovered video object instances by LDA (LDA) and the proposed LDA-WCP (Proposed). [sent-529, score-0.393]
</p><p>100 (a) shows the number of single video object and (b) shows the number of multiple video objects. [sent-530, score-0.341]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('topical', 0.682), ('lda', 0.289), ('word', 0.268), ('topic', 0.201), ('discovery', 0.183), ('ij', 0.157), ('zdn', 0.151), ('video', 0.141), ('discover', 0.14), ('mining', 0.135), ('discovered', 0.119), ('dirichlet', 0.105), ('wdn', 0.102), ('words', 0.099), ('topics', 0.092), ('instances', 0.091), ('frequent', 0.084), ('prior', 0.074), ('jv', 0.073), ('wj', 0.07), ('ldawcp', 0.068), ('document', 0.066), ('videos', 0.065), ('frames', 0.06), ('allocation', 0.055), ('wm', 0.054), ('yuan', 0.054), ('latent', 0.053), ('objects', 0.051), ('cohesive', 0.05), ('corpus', 0.049), ('segments', 0.049), ('visual', 0.045), ('il', 0.044), ('object', 0.042), ('key', 0.039), ('cooccurrence', 0.039), ('frame', 0.039), ('variational', 0.035), ('dnp', 0.034), ('oftopical', 0.034), ('supportiveness', 0.034), ('thematic', 0.034), ('items', 0.033), ('segmented', 0.032), ('highlighted', 0.031), ('zhao', 0.031), ('gruber', 0.03), ('stevens', 0.03), ('starbucks', 0.03), ('categorized', 0.03), ('iwj', 0.028), ('log', 0.028), ('multinomial', 0.028), ('subgraph', 0.028), ('boxes', 0.027), ('bound', 0.027), ('plsa', 0.026), ('logo', 0.026), ('nanyang', 0.024), ('frequently', 0.024), ('transaction', 0.023), ('icdm', 0.023), ('discovering', 0.022), ('blei', 0.022), ('contain', 0.021), ('bounding', 0.021), ('nd', 0.02), ('item', 0.02), ('neighbor', 0.02), ('neighbors', 0.02), ('collections', 0.02), ('generative', 0.02), ('philbin', 0.02), ('pool', 0.02), ('pattern', 0.019), ('russell', 0.019), ('considering', 0.019), ('categorize', 0.018), ('iv', 0.018), ('numbers', 0.018), ('lj', 0.018), ('segment', 0.018), ('affinity', 0.018), ('clustered', 0.018), ('coherent', 0.017), ('smoothness', 0.017), ('multiple', 0.017), ('parameter', 0.017), ('selected', 0.016), ('incorporates', 0.016), ('cao', 0.016), ('efficacy', 0.016), ('spatially', 0.016), ('resolutions', 0.016), ('maximizing', 0.016), ('cut', 0.016), ('liu', 0.015), ('itemsets', 0.015), ('markovian', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="434-tfidf-1" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>2 0.13699675 <a title="434-tfidf-2" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>3 0.12744962 <a title="434-tfidf-3" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>4 0.1244192 <a title="434-tfidf-4" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>5 0.11135226 <a title="434-tfidf-5" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>Author: Yun Jiang, Hema Koppula, Ashutosh Saxena</p><p>Abstract: For scene understanding, one popular approach has been to model the object-object relationships. In this paper, we hypothesize that such relationships are only an artifact of certain hidden factors, such as humans. For example, the objects, monitor and keyboard, are strongly spatially correlated only because a human types on the keyboard while watching the monitor. Our goal is to learn this hidden human context (i.e., the human-object relationships), and also use it as a cue for labeling the scenes. We present Infinite Factored Topic Model (IFTM), where we consider a scene as being generated from two types of topics: human configurations and human-object relationships. This enables our algorithm to hallucinate the possible configurations of the humans in the scene parsimoniously. Given only a dataset of scenes containing objects but not humans, we show that our algorithm can recover the human object relationships. We then test our algorithm on the task ofattribute and object labeling in 3D scenes and show consistent improvements over the state-of-the-art.</p><p>6 0.10672724 <a title="434-tfidf-6" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>7 0.10664477 <a title="434-tfidf-7" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>8 0.10193618 <a title="434-tfidf-8" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>9 0.10033306 <a title="434-tfidf-9" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>10 0.099556103 <a title="434-tfidf-10" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>11 0.098183766 <a title="434-tfidf-11" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>12 0.096475907 <a title="434-tfidf-12" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>13 0.091241129 <a title="434-tfidf-13" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>14 0.089909181 <a title="434-tfidf-14" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>15 0.088482149 <a title="434-tfidf-15" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>16 0.082581758 <a title="434-tfidf-16" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>17 0.072591007 <a title="434-tfidf-17" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>18 0.068545736 <a title="434-tfidf-18" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>19 0.065756157 <a title="434-tfidf-19" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>20 0.064920813 <a title="434-tfidf-20" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, -0.045), (2, 0.012), (3, -0.031), (4, 0.003), (5, 0.016), (6, -0.02), (7, 0.014), (8, -0.044), (9, 0.02), (10, 0.038), (11, -0.035), (12, 0.027), (13, -0.007), (14, -0.009), (15, -0.031), (16, 0.044), (17, 0.05), (18, 0.018), (19, -0.099), (20, 0.017), (21, -0.021), (22, 0.085), (23, -0.077), (24, 0.008), (25, -0.001), (26, -0.016), (27, 0.065), (28, -0.012), (29, -0.055), (30, 0.058), (31, 0.022), (32, -0.08), (33, 0.049), (34, 0.113), (35, 0.068), (36, -0.053), (37, 0.056), (38, -0.03), (39, -0.137), (40, 0.057), (41, -0.065), (42, -0.151), (43, -0.019), (44, -0.128), (45, 0.058), (46, -0.084), (47, -0.053), (48, -0.029), (49, -0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93285978 <a title="434-lsi-1" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>2 0.75575852 <a title="434-lsi-2" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>3 0.67275894 <a title="434-lsi-3" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>4 0.66771197 <a title="434-lsi-4" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>5 0.65790701 <a title="434-lsi-5" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>6 0.63465083 <a title="434-lsi-6" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>7 0.62206048 <a title="434-lsi-7" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>8 0.6134944 <a title="434-lsi-8" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>9 0.60321873 <a title="434-lsi-9" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>10 0.5843721 <a title="434-lsi-10" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>11 0.57653534 <a title="434-lsi-11" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>12 0.56632835 <a title="434-lsi-12" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>13 0.49614877 <a title="434-lsi-13" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>14 0.49250746 <a title="434-lsi-14" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>15 0.47924209 <a title="434-lsi-15" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>16 0.46954393 <a title="434-lsi-16" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>17 0.46190292 <a title="434-lsi-17" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>18 0.45794654 <a title="434-lsi-18" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>19 0.44845146 <a title="434-lsi-19" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>20 0.44811016 <a title="434-lsi-20" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.128), (16, 0.022), (26, 0.063), (28, 0.013), (33, 0.265), (67, 0.083), (69, 0.039), (76, 0.209), (80, 0.011), (87, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88628215 <a title="434-lda-1" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>Author: Erik Ask, Olof Enqvist, Fredrik Kahl</p><p>Abstract: This paper is concerned with model fitting in the presence of noise and outliers. Previously it has been shown that the number of outliers can be minimized with polynomial complexity in the number of measurements. This paper improves on these results in two ways. First, it is shown that for a large class of problems, the statistically more desirable truncated L2-norm can be optimized with the same complexity. Then, with the same methodology, it is shown how to transform multi-model fitting into a purely combinatorial problem—with worst-case complexity that is polynomial in the number of measurements, though exponential in the number of models. We apply our framework to a series of hard registration and stitching problems demonstrating that the approach is not only of theoretical interest. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers for fitting problems with lowdimensional models.</p><p>2 0.87617177 <a title="434-lda-2" href="./cvpr-2013-Tensor-Based_Human_Body_Modeling.html">426 cvpr-2013-Tensor-Based Human Body Modeling</a></p>
<p>Author: Yinpeng Chen, Zicheng Liu, Zhengyou Zhang</p><p>Abstract: In this paper, we present a novel approach to model 3D human body with variations on both human shape and pose, by exploring a tensor decomposition technique. 3D human body modeling is important for 3D reconstruction and animation of realistic human body, which can be widely used in Tele-presence and video game applications. It is challenging due to a wide range of shape variations over different people and poses. The existing SCAPE model [4] is popular in computer vision for modeling 3D human body. However, it considers shape and pose deformations separately, which is not accurate since pose deformation is persondependent. Our tensor-based model addresses this issue by jointly modeling shape and pose deformations. Experimental results demonstrate that our tensor-based model outperforms the SCAPE model quite significantly. We also apply our model to capture human body using Microsoft Kinect sensors with excellent results.</p><p>same-paper 3 0.87007004 <a title="434-lda-3" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>4 0.86913145 <a title="434-lda-4" href="./cvpr-2013-Procrustean_Normal_Distribution_for_Non-rigid_Structure_from_Motion.html">341 cvpr-2013-Procrustean Normal Distribution for Non-rigid Structure from Motion</a></p>
<p>Author: Minsik Lee, Jungchan Cho, Chong-Ho Choi, Songhwai Oh</p><p>Abstract: Non-rigid structure from motion is a fundamental problem in computer vision, which is yet to be solved satisfactorily. The main difficulty of the problem lies in choosing the right constraints for the solution. In this paper, we propose new constraints that are more effective for non-rigid shape recovery. Unlike the other proposals which have mainly focused on restricting the deformation space using rank constraints, our proposal constrains the motion parameters so that the 3D shapes are most closely aligned to each other, which makes the rank constraints unnecessary. Based on these constraints, we define a new class ofprobability distribution called the Procrustean normal distribution and propose a new NRSfM algorithm, EM-PND. The experimental results show that the proposed method outperforms the existing methods, and it works well even if there is no temporal dependence between the observed samples.</p><p>5 0.85571921 <a title="434-lda-5" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>Author: Hua Wang, Feiping Nie, Heng Huang, Chris Ding</p><p>Abstract: To better understand, search, and classify image and video information, many visual feature descriptors have been proposed to describe elementary visual characteristics, such as the shape, the color, the texture, etc. How to integrate these heterogeneous visual features and identify the important ones from them for specific vision tasks has become an increasingly critical problem. In this paper, We propose a novel Sparse Multimodal Learning (SMML) approach to integrate such heterogeneous features by using the joint structured sparsity regularizations to learn the feature importance of for the vision tasks from both group-wise and individual point of views. A new optimization algorithm is also introduced to solve the non-smooth objective with rigorously proved global convergence. We applied our SMML method to five broadly used object categorization and scene understanding image data sets for both singlelabel and multi-label image classification tasks. For each data set we integrate six different types of popularly used image features. Compared to existing scene and object cat- egorization methods using either single modality or multimodalities of features, our approach always achieves better performances measured.</p><p>6 0.85508895 <a title="434-lda-6" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>7 0.83235693 <a title="434-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.8290906 <a title="434-lda-8" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>9 0.82781011 <a title="434-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.82766265 <a title="434-lda-10" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>11 0.82696986 <a title="434-lda-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.82565761 <a title="434-lda-12" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>13 0.8245852 <a title="434-lda-13" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>14 0.8245042 <a title="434-lda-14" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>15 0.8239193 <a title="434-lda-15" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>16 0.82350779 <a title="434-lda-16" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>17 0.82329631 <a title="434-lda-17" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>18 0.8226468 <a title="434-lda-18" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>19 0.8221724 <a title="434-lda-19" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>20 0.82192516 <a title="434-lda-20" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
