<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-110" href="#">cvpr2013-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</h1>
<br/><p>Source: <a title="cvpr-2013-110-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bao_Dense_Object_Reconstruction_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>Reference: <a title="cvpr-2013-110-reference" href="../cvpr2013_reference/cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. [sent-2, score-0.969]
</p><p>2 We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. [sent-4, score-1.256]
</p><p>3 We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. [sent-5, score-0.317]
</p><p>4 Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. [sent-6, score-0.301]
</p><p>5 We propose a framework for semantic dense reconstruction that learns a category-level shape prior, which is used with weighted warping and refinement mechanisms to reconstruct regularized, high-quality 3D shapes. [sent-29, score-0.641]
</p><p>6 This paper presents a framework for dense 3D reconstruction that overcomes the drawbacks of traditional MVS by leveraging semantic information in the form of object detection and shape priors learned from a database of training images and 3D shapes. [sent-31, score-0.592]
</p><p>7 We postulate in Section 3 that while object instances within a category might have very different shapes and appearances, they share certain similarities at a semantic level. [sent-40, score-0.312]
</p><p>8 We model semantic similarity as a shape prior, which consists of a set of automatically learned anchor points across several instances, along with a learned mean shape that captures the –  11111222226666624422  shared commonality of the entire category. [sent-42, score-1.379]
</p><p>9 In the learning phase (Section 4), the anchor points encode attributes such as frequency, appearance and location similarity of features across instances. [sent-44, score-0.798]
</p><p>10 Based on matched anchor points, the shape prior for a category is determined by a series of weighted thin-plate spline (TPS) warps over the scans of training objects. [sent-46, score-1.094]
</p><p>11 Our reconstruction phase (Section 5) starts with a point cloud obtained by applying a structure-from-motion (SFM) or MVS system to images of an unseen instance (with a shape different from training objects). [sent-47, score-0.434]
</p><p>12 This guides the process of matching anchor points shown by green stars in right panel in Figure 2 – between the learned prior and the test object’s SFM point cloud, followed by a warping of the prior shape in order to closely resemble the true shape. [sent-49, score-1.326]
</p><p>13 Finer details not captured by the shape prior may be recovered by a refinement step, using guidance from SFM or MVS output. [sent-50, score-0.295]
</p><p>14 The refinement combines confidence scores from anchor points and photoconsistency in order to produce a regularized, high quality output shape. [sent-51, score-0.909]
</p><p>15 This paper provides a framework to augment traditional multiview stereo (MVS) reconstruction methods with semantic information. [sent-56, score-0.318]
</p><p>16 A set of example shapes is used by active shape models (ASM) to encode patterns of variability, thereby ensuring a fitted shape consistent with deformations observed in training [8]. [sent-63, score-0.389]
</p><p>17 Subsequent works on statistical shape analysis [10] allow nonrigid TPS warps between shapes [5], but often require landmark identification and initial rigid alignment based on point distributions, which is not feasible for general scenes [24]. [sent-66, score-0.393]
</p><p>18 We use semantic information, namely object detection for localization and anchor point matching, to overcome those drawbacks. [sent-67, score-0.841]
</p><p>19 Learned anchor points yield confidence scores, which guide our deformation process through a weighted TPS [26]. [sent-68, score-0.797]
</p><p>20 Morphable models in 3D demonstrate realistic shape recovery, but are limited to categories like faces with low shape variation that can be accurately modeled with a linear PCA basis [4]. [sent-69, score-0.32]
</p><p>21 By exploiting semantics in the form of object detection and anchor point matching, we handle both greater shape variation and noisy, incomplete, image-based MVS inputs. [sent-72, score-0.929]
</p><p>22 Determining correspondence across instances with varying shape is a key step in shape matching. [sent-74, score-0.418]
</p><p>23 The demands on correspondences for 3D reconstruction are far higher than 2D shape matching competing factors like high localization accuracy, stringent outlier rejection and good density are all crucial to obtaining a high quality dense reconstruction. [sent-79, score-0.384]
</p><p>24 However, the complexity of 3D shapes and the accuracy demands of 3D reconstruction necessitate far greater control over the deformation process, so we consider it advantageous to compute priors in the mesh space. [sent-86, score-0.324]
</p><p>25 Our Model We assume that for each object category, there exists a  prior that consists of a 3D mean shape S∗ that captures the commonality of shapes across all instances and a set of anchor points A that captures similarities between subsets of instances. [sent-88, score-1.321]
</p><p>26 The shape of any particular object Si is a transformation of S∗, plus specific details Δi not shared by other instances: Si = T({S∗, A}, θi) + Δi,  (1)  where T is a warping (transformation) function and θi is the warping parameter that is unique to each object instance. [sent-89, score-0.506]
</p><p>27 We leverage on certain reliable features associated with the shape prior, which we call anchor points. [sent-93, score-0.848]
</p><p>28 Anchor points form the backbone of our framework, since they are representative of object shape and the relative importance of different object structures. [sent-94, score-0.322]
</p><p>29 Anchor points with high weights, ω, are considered stable in terms of location and appearance, and thus, more representative of object shape across instances. [sent-95, score-0.311]
</p><p>30 1, we detail the mechanism of learning anchor points from training data. [sent-98, score-0.768]
</p><p>31 In particular, prior work on shape matching [2, 19] has demonstrated inspiring results using regularized thin-plate spline (TPS) transformations [5] to capture deformations. [sent-101, score-0.315]
</p><p>32 i}, i= 1, · · · , n, be two sets of anchor points for object ins{taxnc}es Oi = =an 1d, ·O··? [sent-103, score-0.809]
</p><p>33 Semantic information of this nature is determined automatically in our framework by the anchor point learning mechanism. [sent-123, score-0.728]
</p><p>34 To incorporate semantic information from anchor points, in the form of a weight matrix W = diag(ω1 , · · · , ωn), we use an extension of TPS [26]: (K + nλW−1)β + Φα = x? [sent-124, score-0.76]
</p><p>35 Details specific to each object that are not captured in the shape prior are recovered by a refinement step. [sent-128, score-0.336]
</p><p>36 This refinement is used in both mean shape learning and during reconstruction of a particular test object. [sent-129, score-0.414]
</p><p>37 To refine a shape Si (a mesh) towards shape Sj , we compute displacements for vertices in Si. [sent-130, score-0.346]
</p><p>38 The vertices of the refined shape are obtained as pik + dik and it inherits the connectivity of Si. [sent-149, score-0.415]
</p><p>39 This is because the above mechanism can be used, with minor changes, for both mean shape learning with the shape Sj being a mesh and for reconstruction with Sj being the oriented point cloud output of MVS, as elaborated in Sections 4. [sent-151, score-0.644]
</p><p>40 Learning Reconstruction Priors For each object category, we use a set of object instances {On} to learn a mean shape S∗ and a set of anchor points A. [sent-155, score-1.118]
</p><p>41 They also serve as the initial{izSatio}n for the anchor point learning, as described in the following. [sent-160, score-0.728]
</p><p>42 Learning Anchor Points An anchor point, A = {Γ, χ, ω}, consists of a feature vector Γ that describes appearance, t,hωe} 3D location χ with respect to the mean shape and a scalar weight ω. [sent-163, score-0.888]
</p><p>43 For cars, most anchor points are located around wheels and body corners since those parts are shared across instances. [sent-167, score-0.798]
</p><p>44 For fruits, anchor points are distributed around the stem and bottom. [sent-168, score-0.768]
</p><p>45 We also show image patches associated with the features of a few example anchor points. [sent-170, score-0.688]
</p><p>46 For an anchor point A, if V are the indices of objects across which the corresponding SFMV points are matched and Ωi are the indices of images of Oi where A is visible, the corresponding feature vector is: Γ = {{fkii }ki∈Ωi }i∈V. [sent-172, score-0.901]
</p><p>47 Then, the location for the anchor point is  χj=|V1|? [sent-175, score-0.728]
</p><p>48 (7)  The weight ω reflects “importance” of an anchor point. [sent-177, score-0.688]
</p><p>49 We consider an anchor point important if it appears across many instances, with low position and appearance variance. [sent-178, score-0.758]
</p><p>50 In contrast to applications like shape matching, the quality of dense reconstruction is greatly affected by the order and extent of deformations. [sent-189, score-0.326]
</p><p>51 Thus, the learned anchor point weights ω are crucial to the success of dense reconstruction. [sent-190, score-0.833]
</p><p>52 The key precursor to learning anchor points is matching 3D points across instances, which is far from trivial. [sent-193, score-0.91]
</p><p>53 Such points usually dominate an SFM point cloud, but do not generalize across instances  Algorithm 1 Learning anchor points Set Parameters δf, δp. [sent-195, score-0.986]
</p><p>54 endU wphdailtee  Output: denser anchor point  set  A. [sent-213, score-0.728]
</p><p>55 Learned shape prior and anchor points for keyboard category. [sent-215, score-1.018]
</p><p>56 since they do not correspond to the object shape, thus, may not be anchor point candidates. [sent-220, score-0.769]
</p><p>57 Moreover, the density of anchor points cannot be too low, since they guide the deformation process that computes the mean shape and fits it to the 3D point cloud. [sent-221, score-1.063]
</p><p>58 To ensure the robustness of anchor point matching and good density, we propose an iterative algorithm, detailed in Algorithm 1. [sent-222, score-0.76]
</p><p>59 The distribution and weights of the learned anchor points are visualized in Figure 3 and 4. [sent-223, score-0.837]
</p><p>60 Mean Shape Construction The learned anchor points are used to compute a mean shape for an object category. [sent-226, score-1.052]
</p><p>61 Recall that we have a mapping from the set of anchor points to each instance in the training set. [sent-227, score-0.797]
</p><p>62 Thus, we can warp successive shapes closer to a mean shape using the anchor points. [sent-228, score-1.041]
</p><p>63 The mean shape is constructed by combining these aligned and warped shapes of different instances. [sent-229, score-0.351]
</p><p>64 In our experiments, we use the weighted number of commonly matched anchor points as the similarity cue. [sent-232, score-0.831]
</p><p>65 We combine the warped shapes T(Ssican) following the order of merging successive branches, to eventually obtain a single shape S∗, which represents the commonality of all training instances. [sent-238, score-0.348]
</p><p>66 The mean shape learning procedure  is shown for a subset of the car dataset in Fig. [sent-240, score-0.304]
</p><p>67 Note that S∗ is computed by using the warped training examples, where the warping maps the 3D locations of learned anchor points. [sent-242, score-0.907]
</p><p>68 Thus, the prior shape is always aligned with the anchor points. [sent-243, score-0.925]
</p><p>69 Two shapes aligned by anchor points are eventually combined into a single one using displacement vectors computed by minimizing (5). [sent-245, score-0.863]
</p><p>70 Semantic Reconstruction with Shape Priors Given a number of images of an object O, we can reconstruct its 3D shape by warping the learned prior shape S∗ based on the estimated θ and by recovering Δ in (1) subsequently. [sent-249, score-0.614]
</p><p>71 The reconstruction consists of three steps: matching anchor points, warping by anchor points, and refinement. [sent-250, score-1.658]
</p><p>72 Accurately recovering warp parameters θ requires accurate matches between anchor points in S∗ and SFM points in Ssfm. [sent-251, score-0.942]
</p><p>73 Initial Alignment It is conventional in shape modeling literature to compute shape alignments using Procrustes analysis or ICP [8]. [sent-255, score-0.32]
</p><p>74 Matching anchor points from leaned model (left) to new object (right). [sent-266, score-0.809]
</p><p>75 Since we also know those for the shape prior, we can use a rigid transformation to coarsely align the prior shape and its anchor points to fit the SFM point cloud of the object. [sent-276, score-1.303]
</p><p>76 The initial alignment for a car reconstruction is shown in Figure 6. [sent-277, score-0.321]
</p><p>77 Reconstruction Given a set of images I an object with unknown shape of S, we use standard SFM to recover the 3D point cloud Ssfm. [sent-281, score-0.316]
</p><p>78 Our goal is to use the mean shape S∗ to produce a dense reconstruction that closely resembles S. [sent-282, score-0.366]
</p><p>79 Since the initial alignment uses the object’s location, pose and scale, anchor points are likely to be aligned to 3D locations in the vicinity of their true matches. [sent-284, score-0.881]
</p><p>80 Thus, the burden of identifying the point in Ssfm that corresponds to an anchor point in S∗ is reduced to a local search. [sent-285, score-0.768]
</p><p>81 We use HOG features to match anchor points to SFM points. [sent-286, score-0.768]
</p><p>82 Examples of robust anchor point matches from our algorithm are shown in Figure 7. [sent-288, score-0.762]
</p><p>83 11111222226666668866  Algorithm 2 Matching anchor points Set parameters δ1δ2η. [sent-289, score-0.768]
</p><p>84 Warping of the shape prior with the learned anchor points matched to SFM points using Algorithm 2. [sent-340, score-1.165]
</p><p>85 Note that while the shape prior represents the commonality of all instances, anchor point-based warping recovers coarse aspects of instance-specific shape, such as the back geometry of Car 2. [sent-341, score-1.082]
</p><p>86 Assume S∗ is the shape prior after the initial alignment of Section 5. [sent-343, score-0.298]
</p><p>87 We use the above matches between anchor points in S∗ and SFM points in Ssfm to estimate parameters θ for the weighted TPS warping (4) and obtain S? [sent-345, score-1.002]
</p><p>88 Notice that, this warping not only reduces the alignment error from the initial detection-based alignment, it also deforms the prior to fit the actual shape of the object. [sent-347, score-0.418]
</p><p>89 This refined shape is the final output of our dense reconstruction framework. [sent-361, score-0.326]
</p><p>90 The efficacy of using anchor points and their learned weights can be demonstrated by Table 2. [sent-386, score-0.837]
</p><p>91 Using anchor points can greatly reduce the reconstruction error compared to only using object detection for alignment. [sent-387, score-0.939]
</p><p>92 Learning anchor point weights further enhances the reconstruction accuracy. [sent-388, score-0.884]
</p><p>93 RGD: Rigidly align mean shape to test object using matched anchor points. [sent-399, score-1.041]
</p><p>94 WP: Align and warp mean shape using matched anchor points (without refinement). [sent-400, score-1.091]
</p><p>95 RGD: Rigidly align the mean shape to a test object by using matched anchor points. [sent-416, score-1.041]
</p><p>96 WP: Align and warp the mean shape by using matched anchor points (Section 5. [sent-417, score-1.091]
</p><p>97 In contrast, we successfully learn meaningful semantic priors across shape variations and use them in our reconstruction, to produce the much higher quality reconstructions in (d), that closely resemble the ground truth (e). [sent-427, score-0.366]
</p><p>98 Discussion and Future Work We have presented a comprehensive framework for dense object reconstruction that uses data-driven semantic priors to recover shape in situations unfavorable to traditional MVS. [sent-429, score-0.522]
</p><p>99 Our learned priors, combined with robust anchor point matching and refinement mechanisms, are shown to produce visually high quality and quantitatively accurate results. [sent-430, score-0.887]
</p><p>100 Evaluating shape correspondence for statistical shape analysis: A benchmark study. [sent-599, score-0.32]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('anchor', 0.688), ('mvs', 0.327), ('sfm', 0.205), ('pik', 0.172), ('shape', 0.16), ('tps', 0.136), ('reconstruction', 0.13), ('warping', 0.12), ('car', 0.104), ('ssifm', 0.092), ('refinement', 0.084), ('points', 0.08), ('cloud', 0.075), ('fruits', 0.074), ('semantic', 0.072), ('shapes', 0.069), ('instances', 0.068), ('commonality', 0.063), ('matched', 0.063), ('alignment', 0.061), ('stars', 0.061), ('warp', 0.06), ('dik', 0.057), ('photoconsistency', 0.057), ('priors', 0.057), ('warped', 0.056), ('sj', 0.056), ('pkj', 0.055), ('ssfm', 0.055), ('multiview', 0.053), ('prior', 0.051), ('bk', 0.051), ('rear', 0.05), ('pj', 0.05), ('align', 0.049), ('reconstructions', 0.047), ('furukawa', 0.044), ('psr', 0.043), ('learned', 0.043), ('oi', 0.042), ('pages', 0.042), ('fruit', 0.041), ('object', 0.041), ('point', 0.04), ('mean', 0.04), ('keyboard', 0.039), ('mesh', 0.039), ('reconstruct', 0.039), ('regularized', 0.038), ('holes', 0.038), ('stereo', 0.037), ('fkii', 0.037), ('nki', 0.037), ('rgd', 0.037), ('spoiler', 0.037), ('ssican', 0.037), ('warps', 0.037), ('dense', 0.036), ('curless', 0.034), ('matches', 0.034), ('spline', 0.034), ('specularities', 0.033), ('plj', 0.033), ('matching', 0.032), ('reconstructed', 0.032), ('similarities', 0.031), ('category', 0.031), ('scans', 0.03), ('chui', 0.03), ('nik', 0.03), ('across', 0.03), ('cars', 0.03), ('bao', 0.029), ('instance', 0.029), ('morphable', 0.029), ('pji', 0.029), ('schematic', 0.029), ('deformation', 0.029), ('diffuse', 0.028), ('inset', 0.027), ('gallup', 0.027), ('poisson', 0.027), ('drawbacks', 0.027), ('seitz', 0.027), ('si', 0.026), ('aligned', 0.026), ('nec', 0.026), ('scan', 0.026), ('traditional', 0.026), ('density', 0.026), ('vertices', 0.026), ('weights', 0.026), ('initial', 0.026), ('michigan', 0.025), ('pauly', 0.025), ('rigidly', 0.024), ('scanner', 0.024), ('mi', 0.024), ('closer', 0.024), ('unique', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="110-tfidf-1" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>2 0.25705382 <a title="110-tfidf-2" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>3 0.17774113 <a title="110-tfidf-3" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>4 0.11021759 <a title="110-tfidf-4" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>Author: Zsolt Sánta, Zoltan Kato</p><p>Abstract: A novel correspondence-less approach is proposed to find a thin plate spline map between a pair of deformable 3D objects represented by triangular surface meshes. The proposed method works without landmark extraction and feature correspondences. The aligning transformation is found simply by solving a system of nonlinear equations. Each equation is generated by integrating a nonlinear function over the object’s domains. We derive recursive formulas for the efficient computation of these integrals. Based on a series of comparative tests on a large synthetic dataset, our triangular mesh-based algorithm outperforms state of the art methods both in terms of computing time and accuracy. The applicability of the proposed approach has been demonstrated on the registration of 3D lung CT volumes.</p><p>5 0.1100532 <a title="110-tfidf-5" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>Author: Armand Joulin, Sing Bing Kang</p><p>Abstract: An anaglyph is a single image created by selecting complementary colors from a stereo color pair; the user can perceive depth by viewing it through color-filtered glasses. We propose a technique to reconstruct the original color stereo pair given such an anaglyph. We modified SIFT-Flow and use it to initially match the different color channels across the two views. Our technique then iteratively refines the matches, selects the good matches (which defines the “anchor” colors), and propagates the anchor colors. We use a diffusion-based technique for the color propagation, and added a step to suppress unwanted colors. Results on a variety of inputs demonstrate the robustness of our technique. We also extended our method to anaglyph videos by using optic flow between time frames.</p><p>6 0.1052368 <a title="110-tfidf-6" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>7 0.10337775 <a title="110-tfidf-7" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>8 0.10172585 <a title="110-tfidf-8" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>9 0.097364001 <a title="110-tfidf-9" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>10 0.095129862 <a title="110-tfidf-10" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>11 0.085891291 <a title="110-tfidf-11" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>12 0.085328124 <a title="110-tfidf-12" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>13 0.084903836 <a title="110-tfidf-13" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>14 0.083035596 <a title="110-tfidf-14" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>15 0.078424312 <a title="110-tfidf-15" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>16 0.07818611 <a title="110-tfidf-16" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>17 0.077221826 <a title="110-tfidf-17" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>18 0.075531855 <a title="110-tfidf-18" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>19 0.071567804 <a title="110-tfidf-19" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>20 0.069823191 <a title="110-tfidf-20" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.188), (1, 0.083), (2, 0.003), (3, 0.023), (4, 0.048), (5, -0.038), (6, -0.027), (7, 0.015), (8, -0.01), (9, -0.045), (10, -0.008), (11, 0.045), (12, -0.034), (13, 0.014), (14, 0.059), (15, -0.123), (16, -0.016), (17, 0.063), (18, 0.009), (19, 0.03), (20, -0.046), (21, -0.03), (22, 0.044), (23, 0.001), (24, 0.0), (25, -0.059), (26, 0.007), (27, -0.032), (28, 0.024), (29, 0.029), (30, -0.071), (31, -0.092), (32, -0.058), (33, -0.051), (34, 0.085), (35, 0.042), (36, -0.056), (37, -0.056), (38, -0.02), (39, -0.069), (40, -0.008), (41, 0.07), (42, -0.045), (43, -0.074), (44, 0.062), (45, -0.082), (46, -0.041), (47, 0.024), (48, -0.134), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92861521 <a title="110-lsi-1" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>2 0.70061082 <a title="110-lsi-2" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>Author: Ming Zeng, Jiaxiang Zheng, Xuan Cheng, Xinguo Liu</p><p>Abstract: This paper presents a method for quasi-rigid objects modeling from a sequence of depth scans captured at different time instances. As quasi-rigid objects, such as human bodies, usually have shape motions during the capture procedure, it is difficult to reconstruct their geometries. We represent the shape motion by a deformation graph, and propose a model-to-partmethod to gradually integrate sampled points of depth scans into the deformation graph. Under an as-rigid-as-possible assumption, the model-to-part method can adjust the deformation graph non-rigidly, so as to avoid error accumulation in alignment, which also implicitly achieves loop-closure. To handle the drift and topological error for the deformation graph, two algorithms are introduced. First, we use a two-stage registration to largely keep the rigid motion part. Second, in the step of graph integration, we topology-adaptively integrate new parts and dynamically control the regularization effect of the deformation graph. We demonstrate the effectiveness and robustness of our method by several depth sequences of quasi-rigid objects, and an application in human shape modeling.</p><p>3 0.63362688 <a title="110-lsi-3" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>Author: Ravi Garg, Anastasios Roussos, Lourdes Agapito</p><p>Abstract: This paper offers the first variational approach to the problem of dense 3D reconstruction of non-rigid surfaces from a monocular video sequence. We formulate nonrigid structure from motion (NRSfM) as a global variational energy minimization problem to estimate dense low-rank smooth 3D shapes for every frame along with the camera motion matrices, given dense 2D correspondences. Unlike traditional factorization based approaches to NRSfM, which model the low-rank non-rigid shape using a fixed number of basis shapes and corresponding coefficients, we minimize the rank of the matrix of time-varying shapes directly via trace norm minimization. In conjunction with this low-rank constraint, we use an edge preserving total-variation regularization term to obtain spatially smooth shapes for every frame. Thanks to proximal splitting techniques the optimization problem can be decomposed into many point-wise sub-problems and simple linear systems which can be easily solved on GPU hardware. We show results on real sequences of different objects (face, torso, beating heart) where, despite challenges in tracking, illumination changes and occlusions, our method reconstructs highly deforming smooth surfaces densely and accurately directly from video, without the need for any prior models or shape templates.</p><p>4 0.63048518 <a title="110-lsi-4" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>Author: Yen H. Le, Uday Kurkure, Ioannis A. Kakadiaris</p><p>Abstract: Statistical shape models, such as Active Shape Models (ASMs), sufferfrom their inability to represent a large range of variations of a complex shape and to account for the large errors in detection of model points. We propose a novel method (dubbed PDM-ENLOR) that overcomes these limitations by locating each shape model point individually using an ensemble of local regression models and appearance cues from selected model points. Our method first detects a set of reference points which were selected based on their saliency during training. For each model point, an ensemble of regressors is built. From the locations of the detected reference points, each regressor infers a candidate location for that model point using local geometric constraints, encoded by a point distribution model (PDM). The final location of that point is determined as a weighted linear combination, whose coefficients are learnt from the training data, of candidates proposed from its ensemble ’s component regressors. We use different subsets of reference points as explanatory variables for the component regressors to provide varying degrees of locality for the models in each ensemble. This helps our ensemble model to capture a larger range of shape variations as compared to a single PDM. We demonstrate the advantages of our method on the challenging problem of segmenting gene expression images of mouse brain.</p><p>5 0.62271434 <a title="110-lsi-5" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>Author: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar</p><p>Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In thispaper wepresent an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.</p><p>6 0.60533732 <a title="110-lsi-6" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>7 0.60341644 <a title="110-lsi-7" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>8 0.59596121 <a title="110-lsi-8" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>9 0.59442008 <a title="110-lsi-9" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>10 0.59332699 <a title="110-lsi-10" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>11 0.58773851 <a title="110-lsi-11" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>12 0.58564883 <a title="110-lsi-12" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>13 0.58023947 <a title="110-lsi-13" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>14 0.57959688 <a title="110-lsi-14" href="./cvpr-2013-Procrustean_Normal_Distribution_for_Non-rigid_Structure_from_Motion.html">341 cvpr-2013-Procrustean Normal Distribution for Non-rigid Structure from Motion</a></p>
<p>15 0.57647896 <a title="110-lsi-15" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>16 0.56899005 <a title="110-lsi-16" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>17 0.56643981 <a title="110-lsi-17" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>18 0.56606179 <a title="110-lsi-18" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>19 0.5572992 <a title="110-lsi-19" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>20 0.54859197 <a title="110-lsi-20" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.113), (16, 0.036), (26, 0.073), (33, 0.242), (36, 0.227), (67, 0.038), (69, 0.049), (87, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86873335 <a title="110-lda-1" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>2 0.8492763 <a title="110-lda-2" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>same-paper 3 0.84258777 <a title="110-lda-3" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>4 0.82384145 <a title="110-lda-4" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>5 0.82171196 <a title="110-lda-5" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>Author: Dong Wang, Huchuan Lu, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods, the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution, which can be solved efficiently. Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.</p><p>6 0.80410367 <a title="110-lda-6" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>7 0.79964304 <a title="110-lda-7" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>8 0.78957695 <a title="110-lda-8" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>9 0.78361493 <a title="110-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.77833521 <a title="110-lda-10" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>11 0.77829194 <a title="110-lda-11" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>12 0.77787912 <a title="110-lda-12" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>13 0.77755868 <a title="110-lda-13" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>14 0.7763266 <a title="110-lda-14" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>15 0.77593499 <a title="110-lda-15" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>16 0.77551603 <a title="110-lda-16" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>17 0.77547532 <a title="110-lda-17" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>18 0.77509636 <a title="110-lda-18" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>19 0.77471423 <a title="110-lda-19" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>20 0.77429575 <a title="110-lda-20" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
