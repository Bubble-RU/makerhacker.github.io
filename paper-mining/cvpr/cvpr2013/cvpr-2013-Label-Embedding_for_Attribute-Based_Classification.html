<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>241 cvpr-2013-Label-Embedding for Attribute-Based Classification</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-241" href="#">cvpr2013-241</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>241 cvpr-2013-Label-Embedding for Attribute-Based Classification</h1>
<br/><p>Source: <a title="cvpr-2013-241-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Akata_Label-Embedding_for_Attribute-Based_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>Reference: <a title="cvpr-2013-241-reference" href="../cvpr2013_reference/cvpr-2013-Label-Embedding_for_Attribute-Based_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. [sent-2, score-0.375]
</p><p>2 The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. [sent-4, score-0.267]
</p><p>3 The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e. [sent-6, score-0.761]
</p><p>4 A solution to zero-shot learning which has recently gained in popularity in the computer vision community consists in introducing an intermediate space A referred to as attribute layer [16, 8]. [sent-14, score-0.361]
</p><p>5 age embedding (left): how to extract suitable features from an image? [sent-17, score-0.307]
</p><p>6 We focus on label embedding (right): how to embed class labels in a Euclidean space? [sent-18, score-0.501]
</p><p>7 We use attributes as side information for the label embedding and measure the “compatibility”’ between the embedded inputs and outputs with a function F. [sent-19, score-0.753]
</p><p>8 As an example, ifthe classes correspond to animals, possible attributes include “has paws”, “has stripes” or “is black”. [sent-21, score-0.357]
</p><p>9 To classify a new image, its attributes are predicted using the learned classifiers and the attribute scores are combined into class-level scores. [sent-23, score-0.596]
</p><p>10 In other words,  since attribute classifiers are learned independently of the end-task they might be optimal at predicting attributes but not necessarily at predicting classes. [sent-27, score-0.596]
</p><p>11 which can perform zeroshot prediction if no labeled samples are available for some classes, but which can also leverage new labeled samples for these classes as they become available. [sent-30, score-0.419]
</p><p>12 Third, while attributes can be a useful source of prior information, other sources of information could be leveraged for zeroshot learning. [sent-32, score-0.41]
</p><p>13 Indeed, images of classes which are close in a semantic hierarchy are usually more similar than images of classes which are far [6]. [sent-34, score-0.284]
</p><p>14 This paper proposes such a solution by making use of the label embedding framework. [sent-38, score-0.395]
</p><p>15 We underline that, while there is an abundant literature in the computer vision community on image embedding (how to describe an image? [sent-39, score-0.307]
</p><p>16 ) much less work has been devoted in  comparison to label embedding in the Y space (how to describe a class? [sent-40, score-0.425]
</p><p>17 We embed each class y ∈ Y in the space of attribute vectors and thus refer to our approach as Attribute Label Embedding (ALE). [sent-42, score-0.395]
</p><p>18 Third, the label embedding framework is generic and not restricted to attributes. [sent-50, score-0.395]
</p><p>19 Related Work We now review related work on attributes, zero-shot learning and label embedding (three research areas which  strongly overlap) with an emphasis on the latter. [sent-58, score-0.436]
</p><p>20 It has been proposed to improve the standard DAP model to take into account the correlation between attributes or between attributes and classes [38, 39, 43, 19]. [sent-62, score-0.652]
</p><p>21 Zero-shot learning Zero-shot learning requires the ability to transfer knowledge from classes for which we have training data to classes for which we do not. [sent-74, score-0.299]
</p><p>22 Possible sources of prior information  ×  include attributes [16, 8, 25, 28, 27], semantic class taxonomies [27, 22] or text features [25, 28, 27]. [sent-75, score-0.452]
</p><p>23 It is unclear, however, how such an embedding could be extrapolated to the case of generic visual categories. [sent-79, score-0.307]
</p><p>24 Label embedding In computer vision, a vast amount of work has been devoted to input embedding, i. [sent-83, score-0.337]
</p><p>25 This includes works on patch encoding (see [4] for a recent comparison), on kernel-based methods [32] with a recent focus on explicit embeddings [20, 35], on dimensionality reduction [32] and on compression [13, 30, 36]. [sent-86, score-0.281]
</p><p>26 Provided that the embedding function ϕ is chosen correctly, label embedding can be an effective way to share parameters between classes. [sent-88, score-0.702]
</p><p>27 While this taxonomy is valid fot both input θ and output embeddings ϕ, we focus here on output embeddings. [sent-91, score-0.353]
</p><p>28 A possible strategy consists in learning directly an embedding from the input to the output (or from the output to the input) as is the case of re888882111200888  gression [25]. [sent-98, score-0.426]
</p><p>29 This setting is particularly relevant when little training data is available, as side information and the derived embeddings can compensate for the lack of data. [sent-106, score-0.357]
</p><p>30 In our work, we focus on embeddings derived from side information but we also consider the case where they are learned from labeled data, using side information as a prior. [sent-110, score-0.46]
</p><p>31 Learning with attributes as label embedding Given a training set S = {(xn, yn) , n = 1. [sent-112, score-0.693]
</p><p>32 In machine learning, a common strategy is to use embedding functions θ :X → and ϕ : Y → for the inputs and outputs and then to learn on the transformed input/output pairs. [sent-118, score-0.386]
</p><p>33 We then explain how to leverage attributes to compute label embeddings. [sent-122, score-0.389]
</p><p>34 Finally, we show that the label embedding framework is generic enough to accommodate for other sources of side information. [sent-124, score-0.575]
</p><p>35 It is generally assumed that F is linear in some combined feature embedding of inputs/outputs ψ(x, y) : F(x, y; w) = w? [sent-129, score-0.307]
</p><p>36 ψ(x, y) (2) and that the joint embedding ψ can be written as the tensor product between the image embedding θ : X → = RD and the label embedding ϕ : Y → = RE: ψ(x, y) = θ(x) ⊗ ϕ(y) (3) and ψ(x, y) : RD RE → RDE. [sent-130, score-1.009]
</p><p>37 Attribute label embedding We now consider the problem of computing label embeddings ϕA from attributes which we refer to as Attribute Label Embedding (ALE). [sent-144, score-1.005]
</p><p>38 , C} and that we have a set of E attributes A = {ai , i= 1. [sent-150, score-0.265]
</p><p>39 We also assume that we are provided with an association measure ρy,i between each attribute ai and each class y. [sent-154, score-0.36]
</p><p>40 In this work, we focus on binary relevance although one advantage of the label embedding framework is that it can easily accommodate real-valued relevances. [sent-156, score-0.443]
</p><p>41 We embed class y in the E-dim attribute space as follows:  ϕA(y) = [ρy,1,  . [sent-157, score-0.395]
</p><p>42 ,  ρy,E]  (7)  and denote ΦA the E C matrix of attribute embeddings which stacks the individual ϕA(y) ’s. [sent-160, score-0.567]
</p><p>43 We note that in equation (4) the image and label embeddings play symmetric roles. [sent-161, score-0.345]
</p><p>44 Also, in the case where attributes are redundant, it might be advantageous to decorrelate them. [sent-165, score-0.284]
</p><p>45 We will study the effect of attribute decorrelation in our experiments. [sent-171, score-0.312]
</p><p>46 The simplest learning strategy is to maximize directly the compatibility between the input and output embeddings N1 ? [sent-175, score-0.405]
</p><p>47 Therefore, we draw inspiration from the WSABIE algorithm [41] which learns jointly image and label embeddings from data to optimize classification accuracy. [sent-179, score-0.408]
</p><p>48 The crucial difference between WSABIE and ALE is the fact that the latter uses attributes as side information. [sent-180, score-0.332]
</p><p>49 In what follows, Φ is the matrix which stacks the embeddings ϕ(y). [sent-183, score-0.278]
</p><p>50 In WSABIE, the label embedding space dimensionality is a parameter to tune. [sent-198, score-0.419]
</p><p>51 In such a case, we want to learn the class embeddings using as prior information ΦA. [sent-210, score-0.368]
</p><p>52 Beyond attributes While attributes make sense in the label embedding framework, we note that label embedding is more general and can accommodate for other sources of side information. [sent-218, score-1.5]
</p><p>53 The hierarchy embedding ϕH (y) can be defined as the C dimensional vector:  ξy,z  ϕH(y) = [ξy,1,  . [sent-221, score-0.387]
</p><p>54 (11)  We later refer to this embedding as Hierarchy Label Embed-  ding (HLE) and we compare ϕA and ϕH as sources of prior information in our experiments. [sent-225, score-0.398]
</p><p>55 In the case where classes are not organized in a tree structure but form a graph, then other types of embeddings could be used, for instance by performing a kernel PCA on the commute time kernel [29]. [sent-226, score-0.391]
</p><p>56 Different embeddings can be easily combined in the label embedding framework, e. [sent-227, score-0.652]
</p><p>57 through simple concatenation of the different embeddings or through more complex operations such as a CCA of the embeddings. [sent-229, score-0.289]
</p><p>58 Each class was annotated with 85 attributes by 10 students [24] and the result was binarized. [sent-243, score-0.317]
</p><p>59 Hence, there is a significant difference in the number and quality of attributes between the two datasets. [sent-255, score-0.265]
</p><p>60 What is the best way to encode/normalize the attribute embeddings? [sent-273, score-0.289]
</p><p>61 How do attributes compare to a class hierarchy as prior information? [sent-276, score-0.423]
</p><p>62 The first baseline is Ridge Regression (RR) which was used in [25] to map input features to output attribute labels. [sent-280, score-0.338]
</p><p>63 Comparison of different attribute embeddings: {0, 1} embedding, {− 1, +1} embedding and mean-centered embedding, with and without ? [sent-295, score-0.596]
</p><p>64 For these experiments, the attribute vectors are encoded in a binary fashion (using {0, 1}) and ? [sent-299, score-0.308]
</p><p>65 We experiment with a {0, 1} embedding, a {−1, +1} embedding and a meancentered embedding (i. [sent-307, score-0.614]
</p><p>66 Underlying the {0, 1} embedding is the assumption that the presence of the same attribute in two classes should contribute to their similarity, but not its ab-  sence3. [sent-310, score-0.707]
</p><p>67 Underlying the {−1, 1} embedding is the assumption that the presence or the absence of the same attribute in two classes should contribute equally to their similarity. [sent-311, score-0.707]
</p><p>68 For instance, if an attribute appears in almost all classes, then in the mean-centered embedding, its absence will contribute more to the similarity than its presence4. [sent-313, score-0.308]
</p><p>69 In what follows, we make use of the simple {0, 1} embedding with ? [sent-320, score-0.307]
</p><p>70 In DAP, given a new image x, we assign it to the class y with 3Here we assume a dot-product similarity between attribute embeddings which is consistent with our linear compatibility function (4). [sent-325, score-0.656]
</p><p>71 Our DAP results on AWA are lower than those reported in [16] because we use only half of the data to train the attribute classifiers. [sent-342, score-0.308]
</p><p>72 Right 2 columns: attribute prediction accuracy (AUC in %) on the 85 AWA and 312 CUB attributes. [sent-343, score-0.352]
</p><p>73 p(ae  = ρy,e|x)  (12)  e= 1  where ρy,e is the association measure between attribute ae and class y, and p(ae = 1|x) is the probability that image x contains attribute e. [sent-345, score-0.668]
</p><p>74 We train for each attribute one linear classifier on the FVs. [sent-346, score-0.289]
</p><p>75 We use a (regularized) logistic loss which provides an attribute classification accuracy similar to the SVM but with the added benefit that its output is already a probability. [sent-347, score-0.352]
</p><p>76 Hence, our approach seems to be more beneficial when the attribute quality is higher. [sent-349, score-0.289]
</p><p>77 In ALE, each column of W can be interpreted as an attribute classifier and θ(x)? [sent-352, score-0.289]
</p><p>78 However, one major difference with DAP is that we do not optimize for attribute classification accuracy. [sent-354, score-0.352]
</p><p>79 We therefore measured the attribute prediction accuracy of DAP and ALE. [sent-356, score-0.352]
</p><p>80 As expected, the attribute prediction accuracy of DAP is higher than that of our approach. [sent-359, score-0.352]
</p><p>81 Thus, our learned attribute classifiers should still be interpretable. [sent-363, score-0.331]
</p><p>82 Classification accuracy on AWA (left) and CUB (right) as a function of the label embedding dimensionality. [sent-366, score-0.395]
</p><p>83 We compare the baseline which uses all attributes, with an SVD dimensionality reduction and a sampling of attributes (we report the mean and standard deviation over 10 samplings). [sent-367, score-0.309]
</p><p>84 Comparison of attributes (ALE) and hierarchies (HLE) for label embedding. [sent-377, score-0.381]
</p><p>85 We explore two different techniques: Singular Value Decomposition (SVD) and attribute sampling. [sent-382, score-0.289]
</p><p>86 From these experiments, we can conclude that there is a significant amount of correlation between attributes and that the output space dimensionality can be significantly reduced with little accuracy loss. [sent-389, score-0.348]
</p><p>87 As expected, SVD outperforms a random sampling of the attribute dimensions. [sent-396, score-0.289]
</p><p>88 As mentioned earlier, while attributes can be a useful source of prior information to embed classes, other sources exist. [sent-398, score-0.41]
</p><p>89 For each attribute we show the images ranked highest. [sent-413, score-0.289]
</p><p>90 We explore different alternatives such as the concatenation of the embeddings or performing CCA on the embeddings. [sent-418, score-0.289]
</p><p>91 On AWA, the combina-  tion performs better than attributes or the hierarchy alone while on CUB, there is no improvement through the combination, certainly because the hierarchy adds little additional information. [sent-424, score-0.425]
</p><p>92 We compare ALE with WSABIE [41] which performs label embedding and therefore “shares” samples be-  tween classes but does not use prior information. [sent-453, score-0.553]
</p><p>93 One advantage of WSABIE with respect to ALE is that the embedding space dimensionality can be tuned, thus giving more flexibility when larger amounts of training data become available. [sent-457, score-0.364]
</p><p>94 As an example, ALE with 2 training samples performs on par with WSABIE with 20 training samples, showing that attributes can compensate for limited training data. [sent-460, score-0.423]
</p><p>95 We compare three embedding techniques: ALE (attributes only), HLE (hierarchy only), AHLE (attributes and hierarchy). [sent-465, score-0.307]
</p><p>96 Second, our model can leverage labeled training data (if available) to update the label embedding, using the attribute embedding as a prior. [sent-482, score-0.8]
</p><p>97 Third, the label embedding famework is not restricted to attributes and can accommodate other sources of prior information such as class taxonomies. [sent-483, score-0.851]
</p><p>98 In the few-shots setting, we showed improvements with respect to WSABIE, which learns the label embedding from labeled data but does not leverage prior information. [sent-485, score-0.504]
</p><p>99 Learning to detect unseen object classes by between-class attribute transfer. [sent-596, score-0.381]
</p><p>100 A joint learning framework for attribute models and object descriptions. [sent-613, score-0.33]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wsabie', 0.376), ('cub', 0.338), ('awa', 0.319), ('embedding', 0.307), ('attribute', 0.289), ('attributes', 0.265), ('ale', 0.257), ('embeddings', 0.257), ('dap', 0.249), ('classes', 0.092), ('ahle', 0.089), ('label', 0.088), ('hierarchy', 0.08), ('cca', 0.073), ('yn', 0.072), ('hle', 0.069), ('side', 0.067), ('svd', 0.066), ('sources', 0.065), ('ovr', 0.063), ('prediction', 0.063), ('compatibility', 0.058), ('embed', 0.054), ('zeroshot', 0.054), ('class', 0.052), ('ranking', 0.051), ('weston', 0.05), ('accommodate', 0.048), ('xn', 0.048), ('ssvm', 0.047), ('labeled', 0.047), ('objective', 0.042), ('learning', 0.041), ('samples', 0.04), ('fv', 0.039), ('multiclass', 0.038), ('taxonomy', 0.038), ('auc', 0.037), ('leverage', 0.036), ('dataindependent', 0.036), ('fvs', 0.036), ('wintoh', 0.036), ('xrce', 0.036), ('classification', 0.034), ('training', 0.033), ('rank', 0.033), ('learn', 0.033), ('ecml', 0.032), ('concatenation', 0.032), ('intermediate', 0.031), ('correlation', 0.03), ('ridge', 0.03), ('devoted', 0.03), ('lear', 0.029), ('fisher', 0.029), ('output', 0.029), ('optimize', 0.029), ('hierarchies', 0.028), ('mahajan', 0.026), ('perronnin', 0.026), ('animals', 0.026), ('prior', 0.026), ('inputs', 0.026), ('larochelle', 0.025), ('dimensionality', 0.024), ('kulkarni', 0.024), ('wordnet', 0.024), ('taxonomies', 0.024), ('hsu', 0.024), ('mensink', 0.024), ('rohrbach', 0.024), ('decorrelation', 0.023), ('amit', 0.022), ('rr', 0.022), ('learned', 0.022), ('wah', 0.021), ('kernel', 0.021), ('stark', 0.021), ('comparatively', 0.021), ('stacks', 0.021), ('principled', 0.02), ('anchez', 0.02), ('strategy', 0.02), ('baseline', 0.02), ('branson', 0.02), ('semantic', 0.02), ('classifiers', 0.02), ('france', 0.02), ('ae', 0.019), ('douze', 0.019), ('half', 0.019), ('verbeek', 0.019), ('advantageous', 0.019), ('nips', 0.019), ('association', 0.019), ('contribute', 0.019), ('par', 0.019), ('fashion', 0.019), ('regularized', 0.018), ('vedaldi', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="241-tfidf-1" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>2 0.36487374 <a title="241-tfidf-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.22966821 <a title="241-tfidf-3" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>4 0.22280677 <a title="241-tfidf-4" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>5 0.2042508 <a title="241-tfidf-5" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>6 0.20250143 <a title="241-tfidf-6" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>7 0.20110162 <a title="241-tfidf-7" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>8 0.19115734 <a title="241-tfidf-8" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>9 0.18065017 <a title="241-tfidf-9" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>10 0.15909271 <a title="241-tfidf-10" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>11 0.15463108 <a title="241-tfidf-11" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>12 0.1541459 <a title="241-tfidf-12" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>13 0.14656073 <a title="241-tfidf-13" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>14 0.14281867 <a title="241-tfidf-14" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>15 0.11091744 <a title="241-tfidf-15" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>16 0.10978945 <a title="241-tfidf-16" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>17 0.10683892 <a title="241-tfidf-17" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>18 0.10406834 <a title="241-tfidf-18" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<p>19 0.10111265 <a title="241-tfidf-19" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>20 0.097601034 <a title="241-tfidf-20" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, -0.127), (2, -0.059), (3, -0.016), (4, 0.133), (5, 0.124), (6, -0.29), (7, 0.057), (8, 0.052), (9, 0.21), (10, -0.058), (11, 0.103), (12, -0.078), (13, -0.005), (14, 0.053), (15, 0.044), (16, -0.077), (17, -0.049), (18, -0.036), (19, 0.09), (20, -0.004), (21, 0.003), (22, -0.016), (23, 0.022), (24, 0.026), (25, -0.002), (26, -0.028), (27, 0.044), (28, -0.021), (29, 0.005), (30, -0.003), (31, -0.011), (32, 0.029), (33, -0.031), (34, -0.019), (35, -0.01), (36, -0.005), (37, -0.016), (38, 0.018), (39, 0.042), (40, -0.023), (41, 0.017), (42, 0.021), (43, -0.022), (44, -0.024), (45, -0.014), (46, -0.014), (47, 0.052), (48, 0.015), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95807832 <a title="241-lsi-1" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>same-paper 2 0.9400838 <a title="241-lsi-2" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>3 0.92089325 <a title="241-lsi-3" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>4 0.86920995 <a title="241-lsi-4" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>5 0.86760652 <a title="241-lsi-5" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>6 0.86574024 <a title="241-lsi-6" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>7 0.85250318 <a title="241-lsi-7" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>8 0.81303418 <a title="241-lsi-8" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>9 0.76107413 <a title="241-lsi-9" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>10 0.74261642 <a title="241-lsi-10" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>11 0.70843422 <a title="241-lsi-11" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>12 0.68041295 <a title="241-lsi-12" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>13 0.64426816 <a title="241-lsi-13" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>14 0.5828703 <a title="241-lsi-14" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>15 0.51000178 <a title="241-lsi-15" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>16 0.5091207 <a title="241-lsi-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.47398087 <a title="241-lsi-17" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>18 0.42305344 <a title="241-lsi-18" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>19 0.40526125 <a title="241-lsi-19" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>20 0.38328567 <a title="241-lsi-20" href="./cvpr-2013-Fine-Grained_Crowdsourcing_for_Fine-Grained_Recognition.html">174 cvpr-2013-Fine-Grained Crowdsourcing for Fine-Grained Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.094), (15, 0.23), (16, 0.03), (26, 0.042), (28, 0.018), (33, 0.316), (67, 0.057), (69, 0.03), (72, 0.01), (77, 0.01), (80, 0.014), (87, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8933984 <a title="241-lda-1" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<p>Author: Peter Ochs, Alexey Dosovitskiy, Thomas Brox, Thomas Pock</p><p>Abstract: Natural image statistics indicate that we should use nonconvex norms for most regularization tasks in image processing and computer vision. Still, they are rarely used in practice due to the challenge to optimize them. Recently, iteratively reweighed ?1 minimization has been proposed as a way to tackle a class of non-convex functions by solving a sequence of convex ?2-?1 problems. Here we extend the problem class to linearly constrained optimization of a Lipschitz continuous function, which is the sum of a convex function and a function being concave and increasing on the non-negative orthant (possibly non-convex and nonconcave on the whole space). This allows to apply the algorithm to many computer vision tasks. We show the effect of non-convex regularizers on image denoising, deconvolution, optical flow, and depth map fusion. Non-convexity is particularly interesting in combination with total generalized variation and learned image priors. Efficient optimization is made possible by some important properties that are shown to hold.</p><p>2 0.88491631 <a title="241-lda-2" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>Author: Thoma Papadhimitri, Paolo Favaro</p><p>Abstract: We investigate the problem of reconstructing normals, albedo and lights of Lambertian surfaces in uncalibrated photometric stereo under the perspective projection model. Our analysis is based on establishing the integrability constraint. In the orthographicprojection case, it is well-known that when such constraint is imposed, a solution can be identified only up to 3 parameters, the so-called generalized bas-relief (GBR) ambiguity. We show that in the perspective projection case the solution is unique. We also propose a closed-form solution which is simple, efficient and robust. We test our algorithm on synthetic data and publicly available real data. Our quantitative tests show that our method outperforms all prior work of uncalibrated photometric stereo under orthographic projection.</p><p>same-paper 3 0.86829448 <a title="241-lda-3" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>4 0.85838288 <a title="241-lda-4" href="./cvpr-2013-All_About_VLAD.html">38 cvpr-2013-All About VLAD</a></p>
<p>Author: unkown-author</p><p>Abstract: The objective of this paper is large scale object instance retrieval, given a query image. A starting point of such systems is feature detection and description, for example using SIFT. The focus of this paper, however, is towards very large scale retrieval where, due to storage requirements, very compact image descriptors are required and no information about the original SIFT descriptors can be accessed directly at run time. We start from VLAD, the state-of-the art compact descriptor introduced by J´ egou et al. [8] for this purpose, and make three novel contributions: first, we show that a simple change to the normalization method significantly improves retrieval performance; second, we show that vocabulary adaptation can substantially alleviate problems caused when images are added to the dataset after initial vocabulary learning. These two methods set a new stateof-the-art over all benchmarks investigated here for both mid-dimensional (20k-D to 30k-D) and small (128-D) descriptors. Our third contribution is a multiple spatial VLAD representation, MultiVLAD, that allows the retrieval and local- ization of objects that only extend over a small part of an image (again without requiring use of the original image SIFT descriptors).</p><p>5 0.85517418 <a title="241-lda-5" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>Author: Dong Yi, Zhen Lei, Stan Z. Li</p><p>Abstract: Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper, we propose a novel method for pose robust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-ofthe-art methods significantly, meanwhile, the method works well on LFW.</p><p>6 0.82691896 <a title="241-lda-6" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>7 0.82684982 <a title="241-lda-7" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>8 0.82679904 <a title="241-lda-8" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>9 0.82670224 <a title="241-lda-9" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>10 0.82638812 <a title="241-lda-10" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>11 0.82582033 <a title="241-lda-11" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>12 0.82577819 <a title="241-lda-12" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>13 0.82572746 <a title="241-lda-13" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>14 0.82528633 <a title="241-lda-14" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>15 0.82514405 <a title="241-lda-15" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>16 0.82513899 <a title="241-lda-16" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>17 0.82502085 <a title="241-lda-17" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>18 0.82500225 <a title="241-lda-18" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>19 0.82497966 <a title="241-lda-19" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>20 0.82490051 <a title="241-lda-20" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
