<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-388" href="#">cvpr2013-388</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</h1>
<br/><p>Source: <a title="cvpr-2013-388-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Semi-supervised_Learning_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yang Yang, Guang Shu, Mubarak Shah</p><p>Abstract: We propose a novel approach to boost the performance of generic object detectors on videos by learning videospecific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-bytracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto encoders. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability; second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.</p><p>Reference: <a title="cvpr-2013-388-reference" href="../cvpr2013_reference/cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We propose a novel approach to boost the performance of generic object detectors on videos by learning videospecific features using a deep neural network. [sent-4, score-0.529]
</p><p>2 We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. [sent-7, score-0.544]
</p><p>3 In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto encoders. [sent-8, score-1.041]
</p><p>4 Therefore, how to adapt a learned generic detector to the images in a specific video taken under different visual conditions becomes a very important problem to be explored. [sent-21, score-0.499]
</p><p>5 Several authors [2, 9, 10] propose to improve detection and tracking simultaneously through de-  tection by tracking and vice versa. [sent-23, score-0.308]
</p><p>6 The detection results serve as a cue to build the tracking results, and the detection component maybe further improved by the result of trackers through online learning. [sent-24, score-0.341]
</p><p>7 However, online retraining of the detector is usually hard due to the less training samples and expensive due to the model complexity. [sent-27, score-0.316]
</p><p>8 In this paper, we propose to improve the detection results of a generic detector on a video by refining the detection scores in an offline fashion, without requiring any trajectory information nor annotation from the video. [sent-28, score-0.607]
</p><p>9 All detected visual examples are collected to form the candidate detection pools using both positive and negative examples pertaining to the target video. [sent-30, score-0.357]
</p><p>10 Since selections of the right features plays an important role in object detection, we argue that, the classical handcrafted features, such as HOG, SIFT, may not be universally suitable and discriminative enough to every type of video. [sent-31, score-0.372]
</p><p>11 Hence, unlike other proposed methods, which are built on using hand-designed features, we learn the good features directly from the raw pixels of the video itself. [sent-33, score-0.369]
</p><p>12 In order to learn discriminative and compact features, we 111666445088  propose a new feature learning method using a deep neural network based on auto encoders. [sent-34, score-1.041]
</p><p>13 Moreover, we learn a discriminative feature hierarchy from local patches to global images. [sent-36, score-0.541]
</p><p>14 The proposed method is presented in section 3 in this order: Preliminary, generative feature learning, discriminative feature learning and learning higher levels. [sent-40, score-0.878]
</p><p>15 One is detection by tracking [2, 9, 10], which use the trajectory  information to help improving detection results and the improved detection can be used backward to improve tracking. [sent-46, score-0.384]
</p><p>16 Another is detection by detection [14, 34, 20, 24, 22, 18, 23, 3 1] and most methods in this category treat this as a semisupervised problem and try to propagate the label to new examples correctly. [sent-47, score-0.368]
</p><p>17 Authors in [3 1] used HOG feature with tree coding in a non-parametric detector adaptation method. [sent-48, score-0.321]
</p><p>18 Notice that the object appearance in video frames should share some regularities among each other, which could be used for discriminative classification. [sent-53, score-0.303]
</p><p>19 Our work intend to learn the good features directly from the raw pixels of a video. [sent-54, score-0.338]
</p><p>20 However, in our case since we have the label information(the confidence scores from the detector), and since examples from video frames are highly correlated , we would  like to use the label information to directly learn a more discriminative feature set for better classification. [sent-58, score-0.76]
</p><p>21 Therefore, we need to learn the features generatively and discriminatively. [sent-59, score-0.425]
</p><p>22 The authors in [35] proposed a single level hybrid learning method for incremental feature learning. [sent-63, score-0.364]
</p><p>23 In this paper, we propose a new feature learning method using a deep neural network based on auto encoders with invariance design. [sent-64, score-0.594]
</p><p>24 We learn three levels of discriminative features from local to global by optimizing both discriminative and generative properties of the features simultaneously. [sent-65, score-1.092]
</p><p>25 First we apply an original detector on a video to get a substantial amount of candidate detections for rescoring. [sent-68, score-0.311]
</p><p>26 Those detections are initially labeled as confident-positive, confident-negative or hard examples by their confidences. [sent-69, score-0.289]
</p><p>27 Then, we re-score the hard examples by training a classifier using the learned features. [sent-71, score-0.378]
</p><p>28 After the rescoring, a small number of hard examples with high confidence are  moved into the confident-positive or confident negative sets for next iteration of feature learning. [sent-72, score-0.518]
</p><p>29 We repeat the above steps until no hard samples become confident ones. [sent-73, score-0.292]
</p><p>30 To learn a set of representative features, we propose to use both a supervised and an unsupervised objective based on auto-encoders[28]. [sent-76, score-0.324]
</p><p>31 Further, we learn the feature hierarchies from local to global by increasing the receptive field size (the 2D patch size). [sent-78, score-0.402]
</p><p>32 In the following part of this section, we will start with introducing the preliminaries about auto-encoders, then move to unsupervised generative feature learning using autoencoders and discriminative feature learning. [sent-81, score-0.976]
</p><p>33 The confident (positive and negative) and hard examples are first collected based on the confidence  scores given by the object detector. [sent-95, score-0.466]
</p><p>34 Then the feature hierarchies and classifiers are learned from the confident examples and used for re-scoring the hard samples. [sent-96, score-0.71]
</p><p>35 The hard samples with high confidence scores are included into the confident examples iteratively until no hard examples become confident ones. [sent-97, score-0.77]
</p><p>36 Preliminaries: Auto-encoders We start by describing the algorithm for our basic learning module, based on the auto-encoders[28], an unsupervised learning architecture used to pre-train deep networks. [sent-100, score-0.481]
</p><p>37 Suppose we have N randomly sampled local patches x(i) ∈ RD from the training set (the dark blue nodes in figure 2)∈, to learn features from them, the conventional auto-encoders attempts to reconstruct the data by minimizing the following loss function: ? [sent-101, score-0.478]
</p><p>38 To make the learned features invariant to local transformation, we further impose a second layer on the top of the auto-encoders by hard coded weights V which pools from several adjacent neurons h, as shown in figure 2 the red nodes. [sent-124, score-0.506]
</p><p>39 Hence, the loss function with the second layer pooling unit for unsupervised generative feature learning is as below: ? [sent-126, score-0.823]
</p><p>40 This design of pooling units is very similar with Independent Subspace Analysis(ISA) [13] and has the advantage of being able to learn overcomplete hidden representations. [sent-159, score-0.33]
</p><p>41 Discriminative Feature Learning The generative feature learning methods intend to learn the features or filters W by minimizing the reconstruction 111666555200  Figure 2: The neural network architecture for learning features at one level. [sent-163, score-1.289]
</p><p>42 The red nodes are the pooling units pooling a non-overlap pair of feature responses(subspace is 2). [sent-166, score-0.319]
</p><p>43 The green node is the classification label which are used for discriminative feature learning. [sent-167, score-0.468]
</p><p>44 [Best viewed in color] error and learn a set of redundant overcomplete features. [sent-168, score-0.289]
</p><p>45 However, a good generative property does not necessarily implies a good discriminative ability. [sent-169, score-0.512]
</p><p>46 In the experiments, we found out that by randomly picking out some filters learned in a generative way, the classification performance does not drop. [sent-170, score-0.61]
</p><p>47 Notice that we have the collected confident set with labeled images. [sent-173, score-0.294]
</p><p>48 The filters W are now not only learned from reconstructing the input x, but also a classifier predicting the label c from the representation p. [sent-175, score-0.385]
</p><p>49 A discriminative objective function computes an average classification loss between the actual label c ∈ [0, 1]K and the predicted label c? [sent-176, score-0.438]
</p><p>50 To maintain the discriminative property, we can enforce the loss function at the image level instead of each local patch. [sent-197, score-0.397]
</p><p>51 We perform average pooling on the image from the feature maps of the local patches and the loss function can be modified as:  E? [sent-198, score-0.397]
</p><p>52 To avoid from overfitting by the discriminative loss function, we further combine the discriminative and generative loss function to learn the discriminative features as follows: E = Egen + βE? [sent-208, score-1.37]
</p><p>53 In the rest of the paper, for simplicity, we will call the features learned by equation 7 as discriminative(hybrid) ones and equation 4 as generative ones. [sent-213, score-0.63]
</p><p>54 Learning Higher Levels We learn the features W from small image patches (small receptive field size) sampled from the confident labeled images at the beginning as the first level. [sent-216, score-0.625]
</p><p>55 To learn the higher-level features, we adopt a convolutional neural network architecture [12, 15] that progressively makes use of auto-encoders as sub-units as shown in Figure 3. [sent-219, score-0.479]
</p><p>56 We learn the first level filters by minimizing equation 7 on small input patches. [sent-221, score-0.394]
</p><p>57 Then we use the learned m filters to convolve with a larger region of the input image to obtain m feature maps. [sent-222, score-0.392]
</p><p>58 We can therefore extract local patches from these locally-invariant multidimensional feature maps and feed them to another level which is also implemented by auto-encoders. [sent-224, score-0.303]
</p><p>59 Each feature map is then pooled from a 4 4 pixel non-overlapping grid to generate the pooled map. [sent-227, score-0.407]
</p><p>60 f Tirhste le covenlc aist tnhaatti:o tnhe o f e tahteur peoso ilne dth me aspec osenrdv elesv aesl are learned from the pooled local feature maps, instead of the larger local patches from the input images. [sent-230, score-0.532]
</p><p>61 [Best viewed in color]  trained greedily layerwise in the same manner as other algorithms proposed in the deep learning literature[1 1]. [sent-231, score-0.306]
</p><p>62 According to the detector confidence, we divide all detections into two groups: the  ×  ones with confident above a threshold are the positive examples; and the rest are hard examples and will be classified later. [sent-245, score-0.594]
</p><p>63 The first two levels are learned from 8 8 pixel wise patches from the input image and the first level pooled feature map respectively. [sent-249, score-0.576]
</p><p>64 The third level is learned directly from the second level pooled feature maps. [sent-250, score-0.599]
</p><p>65 A linear SVM classifier is trained from the confident examples on the learned image representation and used for re-scoring the hard examples. [sent-255, score-0.614]
</p><p>66 In the following subsections, we first report our human detection performance, then compare the generative and discriminative features quantitatively and qualitatively. [sent-260, score-0.764]
</p><p>67 The features at each level are then analyzed based on the detection performance and we report our horse detection results in the end. [sent-261, score-0.55]
</p><p>68 Figure 5: The 100 filters learned discriminatively from four human detection datasets(a. [sent-274, score-0.522]
</p><p>69 [Best viewed in  color] posed method improves the generic offline detector’s results 3-10% on the four benchmark human detection datasets. [sent-280, score-0.391]
</p><p>70 Further, we compared our learned feature with the classical hand-designed HOG feature. [sent-281, score-0.305]
</p><p>71 One of the advantage of our method is that it learns a specific discriminative compact set of features from the data itself, instead of using the combination of different classical hand-designed features. [sent-285, score-0.453]
</p><p>72 We show the final learned discriminative features from each dataset in figure 5. [sent-286, score-0.494]
</p><p>73 (a)-(d) are the corresponding learned first level features from TownCenter, ParkingLot, PETS09 and CAVIAR. [sent-287, score-0.358]
</p><p>74 In contrast to boosting, which selects the good features from a pre-defined feature pool, our method dynamically selects and learns the good features from the raw pixels. [sent-291, score-0.368]
</p><p>75 Generative As described previously, the features learned in a generative manner are usually over-complete, which are good for reconstruction, but are not necessarily effective for recognition. [sent-295, score-0.552]
</p><p>76 Hence, we propose to directly learn the discriminative features for particular video by adding the discriminative loss function. [sent-296, score-0.855]
</p><p>77 We train the classifier and the filters at the same time to find out which features are good for recognition. [sent-297, score-0.286]
</p><p>78 To qualitatively visualize the differences of the two set of features, we show the features generatively learned from TownCenter and CAVIER dataset in figure 6. [sent-298, score-0.446]
</p><p>79 Comparing figure 5(a) with 6(a), interestingly, the color information is more emphasized by the discriminatively learned features. [sent-300, score-0.316]
</p><p>80 Comparing figure 5(d) with 6(b), the color information is now more emphasized by the generative learned feature, since most of the negative examples are the colored background and most of the people are wearing dark clothes. [sent-301, score-0.665]
</p><p>81 To quantitatively measure the generatively and discriminatively learned features, we compute the Average Precision (shown in table 1 third and fourth row) of the two sets of features on the four dataset. [sent-302, score-0.538]
</p><p>82 On average, the discriminative learned features are 2% better than the generative learned features. [sent-303, score-0.939]
</p><p>83 Further, we show that the discriminative learned feature set is more compact than the generative learned feature. [sent-304, score-1.02]
</p><p>84 We compute the AP by increasing the learned features at each level from 40 to 800, the re111666555533  CTeonwtenrPaLrkoitngPE0T9SCAVIAR  DBiGsacesHrneimelOirnaGnetaiv[t7ie]ve98 95642. [sent-305, score-0.358]
</p><p>85 The second row is using the the same re-scoring process but HOG feature without our feature learning algorithm. [sent-315, score-0.29]
</p><p>86 The fourth row is the generatively learned feature results. [sent-317, score-0.446]
</p><p>87 Overall, our proposed algorithm improves the detection results of the generic object detector by 3 − 10% and the HOG features by 3%. [sent-318, score-0.478]
</p><p>88 Figure 6: The 100 filters learned generatively from Town-  center(a) and CAVIAR(b) dataset. [sent-328, score-0.464]
</p><p>89 Compared with the corresponding discriminative filters (a) and (d) in figure5, the generative features are quite different especially in color. [sent-329, score-0.744]
</p><p>90 Interestingly, we found that the discriminative features reach the highest average precision when 400 features are learned. [sent-331, score-0.508]
</p><p>91 Whereas, the generative learned features do need a large set of over-complete features to capture enough discriminative information. [sent-333, score-0.886]
</p><p>92 Figure 7: The average precision of discriminative and generative method over different number of feature. [sent-371, score-0.579]
</p><p>93 This means the discriminative one is more compact than the generative one. [sent-373, score-0.593]
</p><p>94 Overall, our proposed algorithm improves the detection results of the generic object detector by 7% and the HOG by 5-6%. [sent-381, score-0.371]
</p><p>95 The generic offline trained horse detector performs averagely 57% on the dataset, whereas, our approach achieves significantly better results than the original detector. [sent-390, score-0.492]
</p><p>96 Conclusion and Future Work We propose to learn a feature hierarchies directly from the raw pixels in a particular video to improve a generic detector. [sent-393, score-0.562]
</p><p>97 We consider the discriminative property of features, 111666555644  and simultaneously learn discriminative and reconstructive features by using both a supervised and an unsupervised objective. [sent-394, score-0.924]
</p><p>98 The future work will be incrementally learn the discriminative set of features instead of a fixed size of feature set. [sent-396, score-0.58]
</p><p>99 Unsupervised feature learning and deep learning: A review and new perspectives. [sent-427, score-0.329]
</p><p>100 Semi-supervised learning of compact document representations with deep networks. [sent-544, score-0.303]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('generative', 0.285), ('discriminative', 0.227), ('confident', 0.195), ('generatively', 0.179), ('learned', 0.16), ('detector', 0.154), ('pooled', 0.15), ('softmax', 0.149), ('deep', 0.146), ('egen', 0.144), ('learn', 0.139), ('horse', 0.136), ('unsupervised', 0.132), ('filters', 0.125), ('towncenter', 0.121), ('generic', 0.109), ('detection', 0.108), ('feature', 0.107), ('features', 0.107), ('pooling', 0.106), ('hard', 0.097), ('auto', 0.094), ('neural', 0.091), ('level', 0.091), ('activation', 0.091), ('wth', 0.086), ('hierarchies', 0.084), ('detections', 0.081), ('compact', 0.081), ('network', 0.08), ('loss', 0.079), ('hog', 0.077), ('video', 0.076), ('learning', 0.076), ('convolutional', 0.075), ('receptive', 0.072), ('ap', 0.072), ('redundant', 0.068), ('patches', 0.068), ('precision', 0.067), ('examples', 0.067), ('eae', 0.066), ('grabner', 0.065), ('online', 0.065), ('tracking', 0.06), ('adaptation', 0.06), ('pools', 0.06), ('byproduct', 0.057), ('caviar', 0.057), ('collected', 0.055), ('classifier', 0.054), ('color', 0.054), ('supervised', 0.053), ('offline', 0.052), ('confidence', 0.052), ('emphasized', 0.052), ('shu', 0.052), ('extensive', 0.052), ('architecture', 0.051), ('discriminatively', 0.05), ('iarpa', 0.049), ('ranzato', 0.049), ('hybrid', 0.049), ('javed', 0.048), ('chart', 0.048), ('node', 0.048), ('pedestrian', 0.048), ('le', 0.047), ('dark', 0.047), ('taylor', 0.047), ('raw', 0.047), ('shah', 0.046), ('hidden', 0.046), ('label', 0.046), ('setups', 0.045), ('intend', 0.045), ('labeled', 0.044), ('weights', 0.044), ('progressively', 0.043), ('viewed', 0.043), ('preliminaries', 0.042), ('four', 0.042), ('roth', 0.042), ('authors', 0.041), ('trained', 0.041), ('bengio', 0.041), ('classification', 0.04), ('simultaneously', 0.039), ('semisupervised', 0.039), ('equation', 0.039), ('overcomplete', 0.039), ('layer', 0.038), ('efficacy', 0.038), ('blue', 0.038), ('classical', 0.038), ('dis', 0.038), ('maps', 0.037), ('human', 0.037), ('person', 0.037), ('dictionaries', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="388-tfidf-1" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>Author: Yang Yang, Guang Shu, Mubarak Shah</p><p>Abstract: We propose a novel approach to boost the performance of generic object detectors on videos by learning videospecific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-bytracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto encoders. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability; second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.</p><p>2 0.25745201 <a title="388-tfidf-2" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>Author: Guang Shu, Afshin Dehghan, Mubarak Shah</p><p>Abstract: We propose an approach to improve the detection performance of a generic detector when it is applied to a particular video. The performance of offline-trained objects detectors are usually degraded in unconstrained video environments due to variant illuminations, backgrounds and camera viewpoints. Moreover, most object detectors are trained using Haar-like features or gradient features but ignore video specificfeatures like consistent colorpatterns. In our approach, we apply a Superpixel-based Bag-of-Words (BoW) model to iteratively refine the output of a generic detector. Compared to other related work, our method builds a video-specific detector using superpixels, hence it can handle the problem of appearance variation. Most importantly, using Conditional Random Field (CRF) along with our super pixel-based BoW model, we develop and algorithm to segment the object from the background . Therefore our method generates an output of the exact object regions instead of the bounding boxes generated by most detectors. In general, our method takes detection bounding boxes of a generic detector as input and generates the detection output with higher average precision and precise object regions. The experiments on four recent datasets demonstrate the effectiveness of our approach and significantly improves the state-of-art detector by 5-16% in average precision.</p><p>3 0.21774904 <a title="388-tfidf-3" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>Author: Pramod Sharma, Ram Nevatia</p><p>Abstract: In this work, we present a novel and efficient detector adaptation method which improves the performance of an offline trained classifier (baseline classifier) by adapting it to new test datasets. We address two critical aspects of adaptation methods: generalizability and computational efficiency. We propose an adaptation method, which can be applied to various baseline classifiers and is computationally efficient also. For a given test video, we collect online samples in an unsupervised manner and train a randomfern adaptive classifier . The adaptive classifier improves precision of the baseline classifier by validating the obtained detection responses from baseline classifier as correct detections or false alarms. Experiments demonstrate generalizability, computational efficiency and effectiveness of our method, as we compare our method with state of the art approaches for the problem of human detection and show good performance with high computational efficiency on two different baseline classifiers.</p><p>4 0.193424 <a title="388-tfidf-4" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>5 0.16624577 <a title="388-tfidf-5" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>6 0.16195424 <a title="388-tfidf-6" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>7 0.16164485 <a title="388-tfidf-7" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>8 0.15578055 <a title="388-tfidf-8" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>9 0.14826721 <a title="388-tfidf-9" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>10 0.14378123 <a title="388-tfidf-10" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>11 0.14258195 <a title="388-tfidf-11" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>12 0.14079343 <a title="388-tfidf-12" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>13 0.14058141 <a title="388-tfidf-13" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>14 0.13931736 <a title="388-tfidf-14" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>15 0.13861643 <a title="388-tfidf-15" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>16 0.13579203 <a title="388-tfidf-16" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>17 0.1324943 <a title="388-tfidf-17" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>18 0.12955584 <a title="388-tfidf-18" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>19 0.12646672 <a title="388-tfidf-19" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>20 0.12609008 <a title="388-tfidf-20" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.321), (1, -0.123), (2, -0.047), (3, -0.019), (4, 0.041), (5, 0.039), (6, 0.087), (7, 0.003), (8, 0.01), (9, 0.005), (10, -0.086), (11, -0.136), (12, 0.061), (13, -0.14), (14, 0.012), (15, 0.036), (16, -0.079), (17, -0.027), (18, 0.081), (19, -0.015), (20, 0.033), (21, -0.065), (22, -0.025), (23, -0.086), (24, -0.115), (25, 0.09), (26, 0.016), (27, 0.005), (28, 0.002), (29, 0.033), (30, -0.07), (31, -0.003), (32, 0.01), (33, -0.044), (34, 0.055), (35, 0.114), (36, -0.007), (37, 0.026), (38, 0.045), (39, -0.047), (40, -0.002), (41, -0.106), (42, 0.039), (43, -0.06), (44, -0.093), (45, 0.011), (46, 0.01), (47, -0.032), (48, 0.07), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96295935 <a title="388-lsi-1" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>Author: Yang Yang, Guang Shu, Mubarak Shah</p><p>Abstract: We propose a novel approach to boost the performance of generic object detectors on videos by learning videospecific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-bytracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto encoders. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability; second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.</p><p>2 0.84626704 <a title="388-lsi-2" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>3 0.74696004 <a title="388-lsi-3" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>Author: Pramod Sharma, Ram Nevatia</p><p>Abstract: In this work, we present a novel and efficient detector adaptation method which improves the performance of an offline trained classifier (baseline classifier) by adapting it to new test datasets. We address two critical aspects of adaptation methods: generalizability and computational efficiency. We propose an adaptation method, which can be applied to various baseline classifiers and is computationally efficient also. For a given test video, we collect online samples in an unsupervised manner and train a randomfern adaptive classifier . The adaptive classifier improves precision of the baseline classifier by validating the obtained detection responses from baseline classifier as correct detections or false alarms. Experiments demonstrate generalizability, computational efficiency and effectiveness of our method, as we compare our method with state of the art approaches for the problem of human detection and show good performance with high computational efficiency on two different baseline classifiers.</p><p>4 0.74353766 <a title="388-lsi-4" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>Author: Rodrigo Benenson, Markus Mathias, Tinne Tuytelaars, Luc Van_Gool</p><p>Abstract: The current state of the art solutions for object detection describe each class by a set of models trained on discovered sub-classes (so called “components ”), with each model itself composed of collections of interrelated parts (deformable models). These detectors build upon the now classic Histogram of Oriented Gradients+linear SVM combo. In this paper we revisit some of the core assumptions in HOG+SVM and show that by properly designing the feature pooling, feature selection, preprocessing, and training methods, it is possible to reach top quality, at least for pedestrian detections, using a single rigid component. We provide experiments for a large design space, that give insights into the design of classifiers, as well as relevant information for practitioners. Our best detector is fully feed-forward, has a single unified architecture, uses only histograms of oriented gradients and colour information in monocular static images, and improves over 23 other methods on the INRIA, ETHand Caltech-USA datasets, reducing the average miss-rate over HOG+SVM by more than 30%.</p><p>5 0.74151367 <a title="388-lsi-5" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Complex real-world signals, such as images, contain discriminative structures that differ in many aspects including scale, invariance, and data channel. While progress in deep learning shows the importance of learning features through multiple layers, it is equally important to learn features through multiple paths. We propose Multipath Hierarchical Matching Pursuit (M-HMP), a novel feature learning architecture that combines a collection of hierarchical sparse features for image classification to capture multiple aspects of discriminative structures. Our building blocks are MI-KSVD, a codebook learning algorithm that balances the reconstruction error and the mutual incoherence of the codebook, and batch orthogonal matching pursuit (OMP); we apply them recursively at varying layers and scales. The result is a highly discriminative image representation that leads to large improvements to the state-of-the-art on many standard benchmarks, e.g., Caltech-101, Caltech-256, MITScenes, Oxford-IIIT Pet and Caltech-UCSD Bird-200.</p><p>6 0.74102986 <a title="388-lsi-6" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>7 0.73603368 <a title="388-lsi-7" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>8 0.71045995 <a title="388-lsi-8" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>9 0.70412314 <a title="388-lsi-9" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>10 0.69264168 <a title="388-lsi-10" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>11 0.68215334 <a title="388-lsi-11" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>12 0.67586505 <a title="388-lsi-12" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>13 0.67221034 <a title="388-lsi-13" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>14 0.6691848 <a title="388-lsi-14" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>15 0.65057403 <a title="388-lsi-15" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>16 0.65024316 <a title="388-lsi-16" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>17 0.64964002 <a title="388-lsi-17" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>18 0.64051574 <a title="388-lsi-18" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>19 0.63775921 <a title="388-lsi-19" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>20 0.63760173 <a title="388-lsi-20" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.108), (16, 0.015), (26, 0.046), (28, 0.015), (33, 0.332), (37, 0.02), (67, 0.111), (69, 0.058), (80, 0.181), (87, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96473485 <a title="388-lda-1" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>2 0.95303208 <a title="388-lda-2" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>3 0.9452312 <a title="388-lda-3" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: In this paper we consider the challenging problem of articulated human pose estimation in still images. We observe that despite high variability of the body articulations, human motions and activities often simultaneously constrain the positions of multiple body parts. Modelling such higher order part dependencies seemingly comes at a cost of more expensive inference, which resulted in their limited use in state-of-the-art methods. In this paper we propose a model that incorporates higher order part dependencies while remaining efficient. We achieve this by defining a conditional model in which all body parts are connected a-priori, but which becomes a tractable tree-structured pictorial structures model once the image observations are available. In order to derive a set of conditioning variables we rely on the poselet-based features that have been shown to be effective for people detection but have so far found limited application for articulated human pose estimation. We demon- strate the effectiveness of our approach on three publicly available pose estimation benchmarks improving or being on-par with state of the art in each case.</p><p>4 0.9353016 <a title="388-lda-4" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>5 0.93467486 <a title="388-lda-5" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>6 0.92084765 <a title="388-lda-6" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>same-paper 7 0.91660011 <a title="388-lda-7" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>8 0.91284907 <a title="388-lda-8" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>9 0.90718162 <a title="388-lda-9" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>10 0.90459138 <a title="388-lda-10" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>11 0.9029454 <a title="388-lda-11" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>12 0.90026003 <a title="388-lda-12" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>13 0.89964515 <a title="388-lda-13" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>14 0.89922082 <a title="388-lda-14" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>15 0.8984673 <a title="388-lda-15" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>16 0.89568388 <a title="388-lda-16" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>17 0.89520055 <a title="388-lda-17" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>18 0.89494258 <a title="388-lda-18" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>19 0.89456105 <a title="388-lda-19" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>20 0.89450502 <a title="388-lda-20" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
