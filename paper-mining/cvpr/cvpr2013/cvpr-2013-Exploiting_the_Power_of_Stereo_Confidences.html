<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 cvpr-2013-Exploiting the Power of Stereo Confidences</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-155" href="#">cvpr2013-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 cvpr-2013-Exploiting the Power of Stereo Confidences</h1>
<br/><p>Source: <a title="cvpr-2013-155-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Pfeiffer_Exploiting_the_Power_2013_CVPR_paper.pdf">pdf</a></p><p>Author: David Pfeiffer, Stefan Gehrig, Nicolai Schneider</p><p>Abstract: Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste- riori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level.</p><p>Reference: <a title="cvpr-2013-155-reference" href="../cvpr2013_reference/cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. [sent-7, score-0.442]
</p><p>2 While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. [sent-8, score-1.058]
</p><p>3 Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. [sent-9, score-0.644]
</p><p>4 In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. [sent-11, score-1.15]
</p><p>5 Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. [sent-12, score-1.16]
</p><p>6 This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste-  riori estimation process. [sent-14, score-0.842]
</p><p>7 We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level. [sent-16, score-0.882]
</p><p>8 In recent years, stereo algorithms and applications have matured significantly spawning products in fields ranging from industrial automation over gaming up to driver assistance systems. [sent-19, score-0.476]
</p><p>9 The underlying stereo algorithms and their properties are well understood, at least for the current real-time algorithms, typically approaches based on correlation [20] or semi-global matching (SGM) [10]. [sent-20, score-0.352]
</p><p>10 that compare stereo algorithms on a 100 % density level are available [19], also for the automotive domain [8]. [sent-23, score-0.394]
</p><p>11 The computation of stereo confidences has only recently been researched in more detail. [sent-24, score-0.551]
</p><p>12 Hu and Mordohai [12] performed an excellent review ofknown stereo confidence metrics comparing them to ground truth scenes on a pixel level. [sent-25, score-0.872]
</p><p>13 In related work on confidence estimation for stereo or optical flow computation, the so called sparsification plots are established as the main method to show the effectiveness of the considered confidence metric. [sent-26, score-1.187]
</p><p>14 This procedure gives a good impression with respect to how well the confidence helps reducing the average error of the disparity map when the least confident values are removed. [sent-27, score-0.682]
</p><p>15 However, no explicit use of both the disparity map and the confidence map in further processing has been reported so far. [sent-28, score-0.644]
</p><p>16 The main objective is to robustly extract free space and obstacle information from dense disparity maps and to represent the results in a compact and simple fashion. [sent-30, score-0.33]
</p><p>17 This paper extends our Bayesian approach [17] to use stereo confidence cues. [sent-36, score-0.706]
</p><p>18 The idea is that each disparity mea222999777  surement is given an individual probability to be an outlier. [sent-37, score-0.328]
</p><p>19 The effectiveness of this procedure is evaluated on a large sequence data base containing different adverse scenarios for the stereo sensor setup. [sent-40, score-0.564]
</p><p>20 To round things off, the performance is also compared against the straightforward way of using sparsification on the disparity map. [sent-41, score-0.443]
</p><p>21 The main contribution of this paper is the first-time fully probabilistic usage of stereo confidences along with the disparity map. [sent-42, score-0.813]
</p><p>22 Moreover, we introduce modified stereo confidence metrics suited for global stereo algorithms, and link confidence values to disparity outlier probabilities. [sent-43, score-1.952]
</p><p>23 The paper is organized as follows: Section 2 describes related work to the field of stereo confidence estimation. [sent-44, score-0.706]
</p><p>24 We limit ourselves to references that inspired our confidence metrics. [sent-45, score-0.354]
</p><p>25 In addition, work that makes use of stereo confidences in subsequent processing is analyzed. [sent-46, score-0.523]
</p><p>26 Besides stereo confidence we also review work on 3D intermediate representations. [sent-47, score-0.706]
</p><p>27 Section 3 encourages our selection of stereo confidence metrics and their modifications for our applica-  tion. [sent-48, score-0.816]
</p><p>28 The resulting confidence values are mapped to outlier probabilities which is described in Section 4. [sent-49, score-0.494]
</p><p>29 In Section 5, the Stixel World is introduced, followed by the extension to use stereo confidences, also for further applications, in the subsequent Sections 6 and 7. [sent-50, score-0.352]
</p><p>30 Related Work Stereo confidence computation has recently attracted rising attention [4, 9, 12]. [sent-54, score-0.354]
</p><p>31 So far, most work on stereo confidences focused on local stereo approaches. [sent-55, score-0.875]
</p><p>32 [9] applied some of these confidence metrics to SGM. [sent-57, score-0.464]
</p><p>33 In [12], Hu and Mordohai provide an extensive review of existing stereo confidence metrics, again using local correlation as the underlying stereo method. [sent-58, score-1.058]
</p><p>34 To this end, the disparity values are sorted according to their confidence values. [sent-60, score-0.644]
</p><p>35 Subsequently, those depth measurements with the lowest confidence are dropped and a new error metric is calculated for the remaining pixels. [sent-61, score-0.418]
</p><p>36 Milella and Siegwart [15] explicitly compute stereo confidence and eliminate less confident matches for the use in an iterated closest point (ICP) algorithm for ego-motion estimation. [sent-66, score-0.706]
</p><p>37 [26] also compute  stereo confidence and eliminate less reliable matches by thresholding on the confidence. [sent-68, score-0.706]
</p><p>38 In addition, the confidence value is used as a weight in plane fitting for 3D reconstruction. [sent-69, score-0.354]
</p><p>39 the precision of a stereo measurement) has been incorporated several times into occupancy grid approaches where obstacles are mapped onto a grid structure (e. [sent-72, score-0.455]
</p><p>40 and our approach make explicit use of a detailed sensor model by pre-  cisely taking the measurement noise and outlier characteristic of the particular sensor setup into account, till now, no approach has taken advantage of stereo confidence cues. [sent-86, score-1.09]
</p><p>41 Stereo Confidence Metrics The stereo confidence metrics introduced in [12] have been investigated in conjunction with a local stereo method. [sent-88, score-1.168]
</p><p>42 In addition, we perform the well-known leftright consistency (LRC) check that was shown to be very effective for high stereo densities [9]. [sent-93, score-0.352]
</p><p>43 The lower rows visualize the particular stereo confidence cues which is b) LC, c) PKRN, and d) MLM. [sent-109, score-0.749]
</p><p>44 The brighter a pixel is, the higher is the confidence that the depth measurement is correct. [sent-110, score-0.437]
</p><p>45 We exclude very similar, adjacent costs to not penalize disparity results around half integer values. [sent-116, score-0.315]
</p><p>46 Although the PKRN modifications slightly violate the confidence ordering of the original metric, they have the following advantages over the original counterpart: The rare case of a singularity with a denominator of zero is avoided. [sent-120, score-0.354]
</p><p>47 (3)  The costs C+ and C− are adjacent to the optimal disparity Cmin. [sent-127, score-0.315]
</p><p>48 The results of both the stereo matching and the confidence metrics are illustrated in Figure 2. [sent-129, score-0.816]
</p><p>49 For this purpose, two different situations are shown that pose quite a challenge for the stereo matching. [sent-130, score-0.352]
</p><p>50 The first one exhibits strong textural patterns caused by scattered sunlight in the windshield which clearly misleads the stereo estimation. [sent-131, score-0.409]
</p><p>51 For practical reasons, all confidence metrics are scaled and bound to the interval [0 . [sent-133, score-0.464]
</p><p>52 Using Confidences as Outlier Probabilities Before turning to the core topic ofthis section it is important to establish a common conception of confidence metrics as discussed in this work. [sent-139, score-0.464]
</p><p>53 Instead of simply computing the disparity measurement d for a pixel, we assume the used stereo scheme to output pairs of values (d, c), with d ∈ D and c ∈ [0, 1] . [sent-140, score-0.7]
</p><p>54 D = [0, 127] , and c is the corresponding confidence value of d. [sent-144, score-0.354]
</p><p>55 d It is to be expected that this value c strongly depends on various aspects: Foremost the confidence metric itself (i. [sent-148, score-0.393]
</p><p>56 PKRN, LC, or MLM), the used stereo scheme (in our case SGM), and the corresponding parameter choice for that particular stereo scheme. [sent-150, score-0.704]
</p><p>57 Since we intend to use confidences in a probabilistic framework for Stixel computation, a mapping from the particular confidence metric to an actual outlier probability is required. [sent-151, score-0.741]
</p><p>58 It works with the same type of sensor data and stereo algorithm that we later run our vision algorithms on. [sent-162, score-0.445]
</p><p>59 Similar to using the ground truth data, the underlying idea is  straightforward: a human inspector annotates regions in the stereo map using the binary labels “inlier” and ”outlier”. [sent-163, score-0.408]
</p><p>60 ch eNmoete ( tohra stt tehreiso parameter c bheoi dcoen) e b outn liys independent of the used confidence metric. [sent-166, score-0.354]
</p><p>61 The right side of Figure 3 shows the obtained confidence mapping p (o | c). [sent-179, score-0.391]
</p><p>62 The dashed line is the expected ground profile and the disparity measurement vector is marked using purple. [sent-229, score-0.406]
</p><p>63 8  Figure 3: The left figure shows the confidence distribution with respect to the manual labeling of the disparity values (training data) into “inlier” and ”outlier”. [sent-235, score-0.644]
</p><p>64 The Stixel computation is formulated as a MAP estimation problem, this way ensuring to obtain the best segmentation result for the given stereo input. [sent-245, score-0.352]
</p><p>65 Modeling all segments as piecewise planar surfaces simplifies the function set fn to linear functions: object segments are assumed to have a constant disparity while ground segments follow the disparity gradient of the ground  surface. [sent-255, score-0.642]
</p><p>66 Since this paper discusses how to efficiently take confidence cues into account, particular emphasis is put on the data term. [sent-264, score-0.397]
</p><p>67 y for a single disparity mea|s surement dv at image row coordinate v to belong to a possible Stixel segment sn. [sent-291, score-0.411]
</p><p>68 333000111  numerous  false positives (mostly red)  on  the ground surface caused by  stereo  matching. [sent-298, score-0.527]
</p><p>69 Using Confidences for Stixel Computation For obtaining  a more measurement  specific  outlier  model, stereo confidence cues are used. [sent-300, score-0.947]
</p><p>70 As discussed in Section 4, these confidence cues are not used directly but are mapped to an outlier probability. [sent-301, score-0.537]
</p><p>71 Equation 9) is straightforward: Instead of processing the plain disparity measurement dv in PD, the tuple (dv ,pv) is used which is the disparity measurement dv along with the corresponding outlier probability pv. [sent-304, score-1.002]
</p><p>72 That decision is for two reasons: Firstly, when not providing stereo confidence cues, using pv = 0 yields the original sensor model of Equation 9. [sent-313, score-0.832]
</p><p>73 Secondly and more crucial for our application, in awareness that the stereo confidence cue might not always be correct (i. [sent-314, score-0.706]
</p><p>74 No false positives are visible for the confidence version whereas many false positives occurred in the original version. [sent-322, score-0.642]
</p><p>75 Using Confidences for Other Applications Confidences are helpful for further applications driven by stereo vision. [sent-324, score-0.352]
</p><p>76 The following popular stereo-based tasks are easily extended to use confidence cues: Occupancy map generation: The disparities are triangulated and registered in a map. [sent-325, score-0.401]
</p><p>77 com/ground-truth-stixel-dataset 333000222  stereo  –  –  –  confidence  cues  in the  sensor  model allows  to remove  nearly all false positives while the detection  rate  is kept high. [sent-346, score-1.035]
</p><p>78 Base line: SGM stereo and Stixels are computed according to [17]. [sent-347, score-0.352]
</p><p>79 Sparsification: SGM and the proposed confidence metrics are computed. [sent-349, score-0.464]
</p><p>80 A manually optimized confidence threshold is applied for discarding all depth measurements with a lower confidence from the disparity map. [sent-350, score-1.023]
</p><p>81 Stixels with confidences: SGM and the proposed confidence metrics are computed. [sent-352, score-0.464]
</p><p>82 Subsequently, we transfer both the disparity and the confidence map to the Stixel engine as described in Section 6. [sent-353, score-0.644]
</p><p>83 Any differences purely result from using the confidence metrics and the way how they are taken into account. [sent-356, score-0.464]
</p><p>84 When using sparsification on the disparity input, the number of false positives is reduced to 637 with LC, 758 with PKRN, and 648 with MLM. [sent-362, score-0.561]
</p><p>85 The results when using confidence cues as suggested are as follows: On the large data set we obtain 360 frames with false positives when using LC, 719 with PKRN, and 301 in case of MLM. [sent-372, score-0.574]
</p><p>86 Problems with repetitive structures, a known shortcoming of local stereo methods that is detected with PKRN and MLM, are rarely observed when using SGM. [sent-380, score-0.352]
</p><p>87 In conclusion, exploiting stereo confidences throughout the whole processing chain clearly proves to have a positive effect. [sent-381, score-0.523]
</p><p>88 Conclusions and Outlook In this contribution, we presented an improvement of the state-of-the-art 3D Stixel intermediate representation by exploiting stereo confidence information in a probabilistic fashion. [sent-385, score-0.706]
</p><p>89 It is shown that the intuitive approach to sparsify the disparity maps based on confidence allows to reduce the false positive rate by a factor of three. [sent-386, score-0.777]
</p><p>90 The same holds true for integrating confidence information into the subsequent Stixel processing step. [sent-391, score-0.354]
</p><p>91 Also, when using Stixels with motion information, the identical concept can  be applied for using optical flow confidence information. [sent-394, score-0.354]
</p><p>92 78 890 127415  Table 1: For evaluating our extension of the Stixel computation scheme, we considered three different stereo confidence metrics and compared against both the base line approach of Pfeiffer et al. [sent-400, score-0.912]
</p><p>93 and the straight-forward way of using sparsification of the disparity map. [sent-401, score-0.417]
</p><p>94 A stereo confidence metric using single view imagery with comparison to five alternative approaches. [sent-424, score-0.745]
</p><p>95 Sensor integration for robot navigation: Combining sonar and stereo range data in a gridbased representation. [sent-430, score-0.352]
</p><p>96 A real-time low-power stereo vision engine using semi-global matching. [sent-443, score-0.352]
</p><p>97 Analysis of KITTI data for stereo analysis with stereo confidence measures. [sent-456, score-1.058]
</p><p>98 Accurate and efficient stereo processing by semi-global matching and mutual information. [sent-460, score-0.352]
</p><p>99 Calculating dense disparity maps from color stereo images, an efficient implementation. [sent-512, score-0.642]
</p><p>100 A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. [sent-534, score-0.352]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stixel', 0.457), ('confidence', 0.354), ('stereo', 0.352), ('disparity', 0.29), ('pkrn', 0.248), ('mlm', 0.191), ('confidences', 0.171), ('outlier', 0.14), ('sparsification', 0.127), ('stixels', 0.114), ('metrics', 0.11), ('lc', 0.106), ('sensor', 0.093), ('false', 0.084), ('dv', 0.083), ('gehrig', 0.078), ('vnt', 0.076), ('occupancy', 0.075), ('sgm', 0.074), ('base', 0.069), ('badino', 0.068), ('positives', 0.06), ('driver', 0.06), ('inlier', 0.059), ('measurement', 0.058), ('vnb', 0.057), ('windshield', 0.057), ('pd', 0.053), ('pfeiffer', 0.051), ('daimler', 0.051), ('franke', 0.051), ('pout', 0.051), ('world', 0.05), ('rate', 0.049), ('germany', 0.048), ('disparities', 0.047), ('cues', 0.043), ('gallup', 0.042), ('automotive', 0.042), ('icvs', 0.042), ('free', 0.04), ('sn', 0.039), ('metric', 0.039), ('clmcin', 0.038), ('cmmilnm', 0.038), ('cpmkinrn', 0.038), ('daimle', 0.038), ('ege', 0.038), ('fpframes', 0.038), ('milella', 0.038), ('minneapolis', 0.038), ('planetary', 0.038), ('pomuint', 0.038), ('rover', 0.038), ('sindelfingen', 0.038), ('surement', 0.038), ('utterly', 0.038), ('impression', 0.038), ('mapping', 0.037), ('june', 0.037), ('rain', 0.035), ('assistance', 0.034), ('lrc', 0.034), ('haeusler', 0.034), ('usa', 0.034), ('frames', 0.033), ('pv', 0.033), ('belgium', 0.031), ('aodha', 0.031), ('hirschm', 0.031), ('ground', 0.031), ('icp', 0.03), ('mac', 0.03), ('gaming', 0.03), ('traffic', 0.029), ('october', 0.029), ('researched', 0.028), ('providence', 0.028), ('indoors', 0.028), ('benenson', 0.028), ('obstacles', 0.028), ('weather', 0.027), ('mordohai', 0.027), ('corridor', 0.027), ('line', 0.027), ('straightforward', 0.026), ('san', 0.026), ('dl', 0.026), ('subsequently', 0.026), ('italy', 0.026), ('findings', 0.025), ('adverse', 0.025), ('unsolved', 0.025), ('francisco', 0.025), ('safe', 0.025), ('truth', 0.025), ('po', 0.025), ('scenarios', 0.025), ('costs', 0.025), ('depth', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="155-tfidf-1" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>Author: David Pfeiffer, Stefan Gehrig, Nicolai Schneider</p><p>Abstract: Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste- riori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level.</p><p>2 0.50374079 <a title="155-tfidf-2" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>3 0.20858274 <a title="155-tfidf-3" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>Author: Yuichi Takeda, Shinsaku Hiura, Kosuke Sato</p><p>Abstract: In this paper we propose a novel depth measurement method by fusing depth from defocus (DFD) and stereo. One of the problems of passive stereo method is the difficulty of finding correct correspondence between images when an object has a repetitive pattern or edges parallel to the epipolar line. On the other hand, the accuracy of DFD method is inherently limited by the effective diameter of the lens. Therefore, we propose the fusion of stereo method and DFD by giving different focus distances for left and right cameras of a stereo camera with coded apertures. Two types of depth cues, defocus and disparity, are naturally integrated by the magnification and phase shift of a single point spread function (PSF) per camera. In this paper we give the proof of the proportional relationship between the diameter of defocus and disparity which makes the calibration easy. We also show the outstanding performance of our method which has both advantages of two depth cues through simulation and actual experiments.</p><p>4 0.19235854 <a title="155-tfidf-4" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>5 0.18122736 <a title="155-tfidf-5" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>Author: Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang</p><p>Abstract: This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants, a new tree structure, ”Segment-Tree ”, is proposed for non-local matching cost aggregation. Conceptually, the segment-tree is constructed in a three-step process: first, the pixels are grouped into a set of segments with the reference color or intensity image; second, a tree graph is created for each segment; and in the final step, these independent segment graphs are linked to form the segment-tree structure. In practice, this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights, our method introduces some ’non-local’ decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities, and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree within two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable to previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore, the tree structure can be refined with the estimated disparities, which leads to consistent scene segmentation and significantly better aggregation results.</p><p>6 0.14350784 <a title="155-tfidf-6" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>7 0.12116648 <a title="155-tfidf-7" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>8 0.11973462 <a title="155-tfidf-8" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>9 0.11803633 <a title="155-tfidf-9" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>10 0.11556447 <a title="155-tfidf-10" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>11 0.11075393 <a title="155-tfidf-11" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>12 0.098829016 <a title="155-tfidf-12" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>13 0.092864916 <a title="155-tfidf-13" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>14 0.089287832 <a title="155-tfidf-14" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>15 0.085124105 <a title="155-tfidf-15" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>16 0.080172986 <a title="155-tfidf-16" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>17 0.072653852 <a title="155-tfidf-17" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>18 0.072397709 <a title="155-tfidf-18" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>19 0.07093899 <a title="155-tfidf-19" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>20 0.068614602 <a title="155-tfidf-20" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, 0.123), (2, 0.024), (3, 0.013), (4, 0.03), (5, -0.061), (6, -0.004), (7, 0.014), (8, 0.012), (9, 0.076), (10, -0.0), (11, 0.036), (12, 0.193), (13, 0.014), (14, -0.099), (15, 0.054), (16, -0.19), (17, -0.169), (18, 0.108), (19, -0.051), (20, -0.123), (21, 0.101), (22, 0.213), (23, 0.203), (24, -0.103), (25, -0.115), (26, -0.008), (27, -0.091), (28, 0.05), (29, 0.011), (30, 0.011), (31, 0.092), (32, -0.183), (33, 0.009), (34, 0.019), (35, 0.038), (36, -0.045), (37, 0.06), (38, 0.016), (39, -0.039), (40, 0.098), (41, 0.013), (42, 0.062), (43, 0.002), (44, -0.071), (45, 0.062), (46, 0.044), (47, 0.015), (48, -0.061), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97182739 <a title="155-lsi-1" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>Author: David Pfeiffer, Stefan Gehrig, Nicolai Schneider</p><p>Abstract: Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste- riori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level.</p><p>2 0.93768519 <a title="155-lsi-2" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>3 0.76344782 <a title="155-lsi-3" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>Author: Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang</p><p>Abstract: This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants, a new tree structure, ”Segment-Tree ”, is proposed for non-local matching cost aggregation. Conceptually, the segment-tree is constructed in a three-step process: first, the pixels are grouped into a set of segments with the reference color or intensity image; second, a tree graph is created for each segment; and in the final step, these independent segment graphs are linked to form the segment-tree structure. In practice, this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights, our method introduces some ’non-local’ decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities, and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree within two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable to previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore, the tree structure can be refined with the estimated disparities, which leads to consistent scene segmentation and significantly better aggregation results.</p><p>4 0.72265339 <a title="155-lsi-4" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>Author: Yuichi Takeda, Shinsaku Hiura, Kosuke Sato</p><p>Abstract: In this paper we propose a novel depth measurement method by fusing depth from defocus (DFD) and stereo. One of the problems of passive stereo method is the difficulty of finding correct correspondence between images when an object has a repetitive pattern or edges parallel to the epipolar line. On the other hand, the accuracy of DFD method is inherently limited by the effective diameter of the lens. Therefore, we propose the fusion of stereo method and DFD by giving different focus distances for left and right cameras of a stereo camera with coded apertures. Two types of depth cues, defocus and disparity, are naturally integrated by the magnification and phase shift of a single point spread function (PSF) per camera. In this paper we give the proof of the proportional relationship between the diameter of defocus and disparity which makes the calibration easy. We also show the outstanding performance of our method which has both advantages of two depth cues through simulation and actual experiments.</p><p>5 0.6582424 <a title="155-lsi-5" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>Author: Carl Olsson, Johannes Ulén, Yuri Boykov</p><p>Abstract: It is commonly believed that higher order smoothness should be modeled using higher order interactions. For example, 2nd order derivatives for deformable (active) contours are represented by triple cliques. Similarly, the 2nd order regularization methods in stereo predominantly use MRF models with scalar (1D) disparity labels and triple clique interactions. In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels, e.g. tangent planes. This general paradigm has been criticized due to perceived computational complexity of optimization in higher-dimensional label space. Contrary to popular beliefs, we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization problems with (nearly) submodular pairwise interactions. Our theoretical and experimental re- sults demonstrate advantages over state-of-the-art methods for 2nd order smoothness stereo. 1</p><p>6 0.56806332 <a title="155-lsi-6" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>7 0.56667352 <a title="155-lsi-7" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>8 0.46428162 <a title="155-lsi-8" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>9 0.43840563 <a title="155-lsi-9" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>10 0.42513514 <a title="155-lsi-10" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>11 0.41743386 <a title="155-lsi-11" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>12 0.41317242 <a title="155-lsi-12" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>13 0.37845036 <a title="155-lsi-13" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>14 0.36090457 <a title="155-lsi-14" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>15 0.33089972 <a title="155-lsi-15" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>16 0.31789681 <a title="155-lsi-16" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>17 0.30168894 <a title="155-lsi-17" href="./cvpr-2013-Discrete_MRF_Inference_of_Marginal_Densities_for_Non-uniformly_Discretized_Variable_Space.html">128 cvpr-2013-Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space</a></p>
<p>18 0.29733664 <a title="155-lsi-18" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>19 0.28754336 <a title="155-lsi-19" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>20 0.2838324 <a title="155-lsi-20" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.104), (16, 0.061), (26, 0.034), (28, 0.025), (33, 0.191), (67, 0.041), (69, 0.058), (86, 0.224), (87, 0.145), (97, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7923587 <a title="155-lda-1" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>Author: David Pfeiffer, Stefan Gehrig, Nicolai Schneider</p><p>Abstract: Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste- riori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level.</p><p>2 0.76866931 <a title="155-lda-2" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>Author: Jingchen Liu, Peter Carr, Robert T. Collins, Yanxi Liu</p><p>Abstract: We employ hierarchical data association to track players in team sports. Player movements are often complex and highly correlated with both nearby and distant players. A single model would require many degrees of freedom to represent the full motion diversity and could be difficult to use in practice. Instead, we introduce a set of Game Context Features extracted from noisy detections to describe the current state of the match, such as how the players are spatially distributed. Our assumption is that players react to the current situation in only a finite number of ways. As a result, we are able to select an appropriate simplified affinity model for each player and time instant using a random decisionforest based on current track and game contextfeatures. Our context-conditioned motion models implicitly incorporate complex inter-object correlations while remaining tractable. We demonstrate significant performance improvements over existing multi-target tracking algorithms on basketball and field hockey sequences several minutes in duration and containing 10 and 20 players respectively.</p><p>3 0.74386621 <a title="155-lda-3" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>4 0.74181581 <a title="155-lda-4" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>5 0.73785013 <a title="155-lda-5" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>6 0.73773718 <a title="155-lda-6" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>7 0.7372449 <a title="155-lda-7" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>8 0.73617363 <a title="155-lda-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.73471707 <a title="155-lda-9" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>10 0.73190027 <a title="155-lda-10" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>11 0.72960252 <a title="155-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.72922081 <a title="155-lda-12" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>13 0.7286827 <a title="155-lda-13" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>14 0.72779918 <a title="155-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.72771859 <a title="155-lda-15" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>16 0.72131747 <a title="155-lda-16" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>17 0.72103727 <a title="155-lda-17" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>18 0.7193538 <a title="155-lda-18" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>19 0.71917909 <a title="155-lda-19" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>20 0.71763074 <a title="155-lda-20" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
