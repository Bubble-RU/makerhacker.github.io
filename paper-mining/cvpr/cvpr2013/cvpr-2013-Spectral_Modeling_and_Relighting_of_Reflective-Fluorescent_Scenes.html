<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-409" href="#">cvpr2013-409</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</h1>
<br/><p>Source: <a title="cvpr-2013-409-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lam_Spectral_Modeling_and_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Antony Lam, Imari Sato</p><p>Abstract: Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally, we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.</p><p>Reference: <a title="cvpr-2013-409-reference" href="../cvpr2013_reference/cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp , Abstract Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. [sent-3, score-0.683]
</p><p>2 Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. [sent-4, score-0.671]
</p><p>3 All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. [sent-5, score-0.526]
</p><p>4 In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. [sent-6, score-1.143]
</p><p>5 We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. [sent-7, score-0.67]
</p><p>6 Introduction  Hyperspectral reflectance data has been used for highly accurate spectral relighting of scenes under arbitrary illumination and has benefited many applications ranging from archiving for cultural e-heritage to the design of consumer products. [sent-10, score-0.741]
</p><p>7 In fact, previous research has demonstrated the necessity of using spectral reflectance for accurate relighting [7, 16, 24]. [sent-11, score-0.647]
</p><p>8 Thus there have been many methods for imaging the spectral reflectance of scenes [3, 4, 10, 14, 21]. [sent-12, score-0.478]
</p><p>9 While previous methods for measuring spectral reflectance have been successful in accurately predicting the color of objects under arbitrary illumination, they all make the same assumption that fluorescence is absent from the scene. [sent-13, score-0.702]
</p><p>10 In fact, Barnard showed through intensive studies on color constancy algorithms, that fluorescent surfaces are common and present in 20% of randomly constructed scenes [2]. [sent-15, score-0.508]
</p><p>11 composite object with both fluorescent and reflective components dramatically changes under different illumination in computer graphics [7]. [sent-20, score-0.624]
</p><p>12 More importantly, reflective and fluorescent components interact with illuminants differently. [sent-21, score-0.644]
</p><p>13 Reflective surfaces emit light at the same wavelength as the light source but fluorescent surfaces will first absorb incident light and then emit at longer wavelengths–a phenomenon known as Stokes shift [17, 19]. [sent-22, score-1.195]
</p><p>14 We start by detailing exactly how reflectance and fluorescence are different. [sent-26, score-0.576]
</p><p>15 Then using the reflectance data, we extract two components of fluorescence called the emission and excitation which are crucial for accurate modeling and relighting of fluorescent surfaces. [sent-29, score-1.896]
</p><p>16 Essentially, the excitation models how incoming light is absorbed and the emission  models how light is emitted. [sent-30, score-0.844]
</p><p>17 Relighting under arbitrary illumination can then be done based on the recovered reflective and fluorescent components. [sent-31, score-0.587]
</p><p>18 111444555200  We also note that to our knowledge, we are the first to introduce a method for modeling and relighting of scenes with both reflective and fluorescent components based on well established theories and observations of the physical behaviors of said components. [sent-32, score-0.901]
</p><p>19 Our consideration of fluorescence provides much better predictions of how colors appear when relighted in comparison to only considering the reflective component. [sent-33, score-0.454]
</p><p>20 Introduce the very different behaviors between reflectance and fluorescence and show the need for explicit joint modeling of the two phenomenon. [sent-35, score-0.576]
</p><p>21 Show that fluorescent components can be modeled using sets of spectral basis vectors that well represent the excitation and emission of fluorescent materials. [sent-39, score-1.617]
</p><p>22 For example, Maloney and Wan-  dell used a three-channel camera for spectral reflectance recovery under ambient light [10]. [sent-45, score-0.54]
</p><p>23 Tominaga later introduced the use of a multi-channel camera for spectral reflectance illuminant recovery by sequentially placing band-pass filters in front of a monochromatic camera [21]. [sent-46, score-0.545]
</p><p>24 Other researchers proposed the use of active illumination for spectral reflectance recovery. [sent-48, score-0.431]
</p><p>25 One notable paper aimed to overcome the complexity by approximating fluorescent effects through a data driven simplified model for relighting [2]. [sent-55, score-0.673]
</p><p>26 However, their model does not properly account for exactly how reflectance and fluorescence would interact with incoming light. [sent-56, score-0.627]
</p><p>27 On the other hand, research in fluorometry details proper procedures for measuring the color of fluorescent materials in the spectral domain using optical devices [20]. [sent-57, score-0.552]
</p><p>28 For example, bispectral methods are well established approaches to measuring the spectral distribution of the fluorescence as a function of incident and outgoing wavelengths for a single point. [sent-58, score-0.725]
</p><p>29 acquire bispectral bidirectional reflectance and reradiation distribution functions (BRRDF) of fluorescent objects using a sampling strategy on spheres with fluorescent paint [6]. [sent-62, score-1.237]
</p><p>30 They demonstrated the effectiveness of modeling and rendering fluorescent objects but their method requires precise bispectral and bidirectional measurements of object appearance, and is thus not suitable for modeling and relighting an entire scene. [sent-63, score-0.746]
</p><p>31 Regarding the separation of reflective and fluorescent components, Alterman et al. [sent-64, score-0.553]
</p><p>32 proposed separating the ap-  pearance of each fluorescent dye from a mixture by unmixing multiplexed images [1]. [sent-65, score-0.478]
</p><p>33 In Zhang and Sato [26], a detailed model for reflectance and fluorescence was presented and used to accurately separate reflective and fluorescent components from scenes. [sent-66, score-1.186]
</p><p>34 These methods successfully separate fluorescent components, but do not fully model the reflective and fluorescent components of a scene and so cannot be used for spectral relighting. [sent-67, score-1.116]
</p><p>35 proposed a method for estimating the emission spectra of fluorescence using multi-spectral images taken under two ordinary light sources. [sent-69, score-0.968]
</p><p>36 However, they assume that fluorescent emission is constant for all excitation wavelengths and thus cannot accurately predict the brightness of fluorescent components under varying illumination [22]. [sent-70, score-1.831]
</p><p>37 Reflectance and Fluorescence As discussed, illuminated reflective surfaces emit light at the same wavelength as the light source while fluorescent surfaces absorb incident light and then emit at longer wavelengths. [sent-72, score-1.372]
</p><p>38 The appearance of a reflective-fluorescent surface point at wavelength λo illuminated at wavelength λi can be expressed as a linear 111444555311  combination of the reflective and fluorescent components. [sent-76, score-1.203]
</p><p>39 P(λo, λi) = PR(λo, λi) + PF(λo, λi) (1) where PR(λo, λi) and PF (λo, λi) are the reflective and fluorescent terms computed from information on the surface point’s physical properties and illuminant I wavelength at λi. [sent-77, score-0.95]
</p><p>40 As mentioned, reflectance emits light at the same wavelength as the illuminant so it’s model is expressed as PR(λo, λi) = R(λo)I(λi)δ(λo − λi) (2) where R(λo) is the reflectance at wavelength λo and I(λi) is the illuminant at wavelength λi. [sent-78, score-1.784]
</p><p>41 2a shows examples of excitation and emission spectra for one fluorescent dye over the visible spectrum. [sent-85, score-1.294]
</p><p>42 Also, the emission being to the right of the excitation is an example of Stokes shift. [sent-86, score-0.61]
</p><p>43 Up to this point, we have only described reflectance and fluorescence under narrowband illumination. [sent-87, score-0.665]
</p><p>44 In the case of wideband illumination, the emitted light at wavelength λo  is expressed as a sum over all illumination wavelengths λi. [sent-88, score-0.827]
</p><p>45 Thus in the absence of the reflective component R(λo)I(λo), the fluorescent emission spectrum would only be scaled by the amount of energy from the light source and how it interacts with the excitation. [sent-99, score-1.108]
</p><p>46 We accomplish this by imaging the three components using various combinations  (a) Emission Capture  (b) Excitation Capture at Different Wavelengths  Figure 3: Capture of fluorescent components for a single point. [sent-105, score-0.554]
</p><p>47 Reflectance Capture: Due to Stokes shift, a fluorescent point illuminated at wavelength λi will generally emit light at longer wavelengths λo. [sent-109, score-1.204]
</p><p>48 Emission Capture: The illuminant only changes the scaling of the emission spectrum and so the shape of the emission is invariant to incident light. [sent-111, score-0.901]
</p><p>49 Thus if we fix an illuminant at λi, the emission could be observed for any wavelength λo > λi. [sent-112, score-0.729]
</p><p>50 For a reflective-fluorescent point, no reflectance would be observed at λo since reflectance only emits at the same wavelength as the illuminant. [sent-114, score-0.942]
</p><p>51 3a shows an emission illuminated by a narrowband light. [sent-116, score-0.511]
</p><p>52 Excitation Capture: We can observe the emission at a wavelength λo while we varying the illuminant wavelength λi for λi < λo. [sent-117, score-1.009]
</p><p>53 This would allow us to observe how much different wavelengths of light would rescale the emission which by definition is the excitation spectrum. [sent-118, score-1.034]
</p><p>54 In general, a scene consists of multiple fluorescent materials so it is difficult to fix λi or λo to observe all fluorescent spectra. [sent-121, score-0.888]
</p><p>55 recovery  based  on  fluorescent  com-  111444555422  rowband light source at wavelength λ and capture the scene  using a camera equipped with a narrowband filter that only allows wavelength λ through, we can capture the reflectance of the scene at λ. [sent-129, score-1.62]
</p><p>56 Provided this process is repeated for all wavelengths, the full reflectance spectra could be obtained. [sent-132, score-0.574]
</p><p>57 In the next subsection, we derive a method that can make use of real-world statistics on spectra to accurately estimate a full spectrum given only a few sparse values of the spectrum at different wavelengths. [sent-134, score-0.464]
</p><p>58 Recovering Full Reflectance Sparse Wavelengths  Spectra  using  It is well known reflectance spectra from various domains such as Munsell colors and natural scenes can be treated as vectors and represented compactly using 6-8 principal components derived from real world statistics [8, 9, 11, 15]. [sent-137, score-0.645]
</p><p>59 (5)  The full spectrum r is accurately estimated through a selection of good sparse wavelengths to use. [sent-183, score-0.428]
</p><p>60 The key to getting a good approximation of the full spectrum r from sparse measurements is to select sparse wavelengths so that the pseudoinverse in Eq. [sent-184, score-0.454]
</p><p>61 Randomly select one wavelength s ∈ S and one wavelength o ∈y sOe laencdt swap athveeilre sngett memberships. [sent-200, score-0.576]
</p><p>62 Fluorescent Emission Capture Let us start from the simplest case where a scene con-  sists of a homogeneous fluorescent material. [sent-212, score-0.442]
</p><p>63 3, its emission spectrum can be easily obtained by illumination at an appropriate narrowband wavelength λi and capture of its emitted light for all wavelengths λo > λi. [sent-214, score-1.274]
</p><p>64 Fortunately, the laborious capture of all wavelengths is unnecessary because similar to the reflectance case, we have found that emission spectra can also benefit from our sparse wavelength algorithm described in Sec. [sent-218, score-1.53]
</p><p>65 We have found that a large collection of emission spectra from the McNamara and Boswell Fluorescence Spectra Dataset [13] can be well represented using 12 principal components. [sent-221, score-0.597]
</p><p>66 In addition, we tested the 12 eigenvectors ability 111444555533  to reconstruct spectra obtained from a real fluorescent chart with 17 colors using the following error metric  ? [sent-223, score-0.787]
</p><p>67 It can be seen that both the McNamra and Boswell Dataset (used to derive the basis) and the fluorescent chart show low errors. [sent-230, score-0.516]
</p><p>68 Thus our observed emission spectrum can be well represented as a linear combina? [sent-231, score-0.431]
</p><p>69 1 to utilize our method for recovering full emission spectra using only sparsely captured wavelengths. [sent-240, score-0.639]
</p><p>70 Furthermore, not all fluorescent materials necessarily excite at the same wavelength. [sent-242, score-0.446]
</p><p>71 To overcome these difficulties, we first subtract out reflectance and then propose an alternative imaging approach for emission capture. [sent-243, score-0.747]
</p><p>72 We utilize wideband light instead so that all emission spectra in the scene can be simultaneously excited by the same illuminant. [sent-244, score-0.824]
</p><p>73 4 which describes the appearance of a surface point at wavelength λo illuminated by wideband light I. [sent-247, score-0.578]
</p><p>74 Note that although we obtain the emission up to a scaling factor kex, the constant kex will be canceled during relighting as described in Sec. [sent-254, score-0.666]
</p><p>75 This is because similar to the homogeneous fluorescent scene, we can estimate Em(λo) up to the scaling factor kex by representing Em(λo) using a basis. [sent-257, score-0.468]
</p><p>76 Also, similar to the emission case, we have found that 12 principal components can represent 99% of the energy in excitation spectra from the MacNamara and Boswell Fluorescence Spectra Dataset  well. [sent-268, score-0.89]
</p><p>77 We also show in Table 1 that reconstruction errors on the both the McNamara and Boswell Dataset and the fluorescent color chart were low. [sent-269, score-0.538]
</p><p>78 So we can express the observed excitation as a linear combination of basis vectors Ex(λi)Em(λo) = Em(λo) σnbn (λo) and use sparse wavelengths derived from a? [sent-270, score-0.609]
</p><p>79 Unfortunately, a scene can still contain many different emission spectra that may not even overlap meaning that we cannot obtain all excitation spectra from the scene by observing only one wavelength λo. [sent-273, score-1.414]
</p><p>80 Also, similar to emission spectra we can express excitation as a linear combination of basis vectors. [sent-287, score-0.893]
</p><p>81 m could be employed to capture the full excitation spectra from images captured at only a few wavelengths. [sent-290, score-0.544]
</p><p>82 In our results, we first show that our method for estimating a full spectrum using sparse measurements at key wavelengths is accurate by comparing against brute force capture of all wavelengths. [sent-358, score-0.522]
</p><p>83 We then show that our estimated R, Em, and Ex spectra from sparse wavelength imaging yields highly accurate color relighting of scenes. [sent-359, score-0.881]
</p><p>84 1 was then used to find six sparse wavelengths for accurate recovery of full reflectance spectra. [sent-366, score-0.661]
</p><p>85 i0ta6t4ion Mean Errors  Best CaseWorst Case  on  Figure 4: Brute force versus sparse wavelength imaging of spectra on color charts. [sent-382, score-0.658]
</p><p>86 The mean errors between sparse wavelength and brute force imaging are shown at the top. [sent-384, score-0.436]
</p><p>87 Since we image from 420 nm to 700 nm in increments of 10 nm, the brute force capture of all such wavelengths would require 29 images each for capturing R, Em, and Ex. [sent-385, score-0.443]
</p><p>88 Accurate reflectance estimation is crucial for our method because subsequent steps in capturing the fluorescent components depend on accurately subtracting out reflectance. [sent-394, score-0.793]
</p><p>89 Relighting Scenes We continue our quantitative analysis on the color chart scene by relighting it under three illuminants and show that the predicted colors are very close to ground truth images. [sent-401, score-0.449]
</p><p>90 chart xy chromaticities truth, red considers both green is relighting only relighting is incorrect in  Specifically, we rendered RGB images under blue, green,  and the CIE D250 light. [sent-413, score-0.593]
</p><p>91 Ground TruthRelightedRelighted with Fluorescence  Reflectance Only  Figure 7: Relighting results for a scene with fluorescent and non-fluorescent objects under blue, green, and D250 lights. [sent-421, score-0.442]
</p><p>92 When illuminated with blue light, relighting the scene using only the reflective component results in many fluorescent colors appearing as black. [sent-425, score-0.935]
</p><p>93 In the D250 illuminated images, the reflectance only case is darker due to the lack of fluorescent emission. [sent-427, score-0.804]
</p><p>94 The fluorescent objects on the other hand, show obvious improvement with our method. [sent-431, score-0.423]
</p><p>95 Conclusion We detailed the very different ways reflectance and fluorescence behave concerning emission of light and showed the need for explicit consideration of fluorescence. [sent-433, score-1.038]
</p><p>96 We also proposed a sparse wavelength imaging method that was successfully applied to the capture of the reflective, emission  and excitation components of entire scenes. [sent-434, score-1.04]
</p><p>97 By applying sparse wavelength imaging to fluorescence, we also showed that fluorescent spectra can be characterized by basis vectors. [sent-435, score-1.072]
</p><p>98 Finally, we are, to the best of our knowledge, the first to model all aspects of relighting reflective-fluorescent scenes using established theories on the physical behavior of reflectance and fluorescence. [sent-436, score-0.624]
</p><p>99 Evaluation of linear models of surface spectral reflectance with small numbers of parameters. [sent-507, score-0.419]
</p><p>100 Spectral estimation of fluorescent objects using visible lights and an imaging device. [sent-596, score-0.48]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fluorescent', 0.423), ('emission', 0.354), ('reflectance', 0.313), ('wavelengths', 0.284), ('wavelength', 0.28), ('fluorescence', 0.263), ('excitation', 0.256), ('relighting', 0.25), ('spectra', 0.243), ('reflective', 0.13), ('light', 0.108), ('wideband', 0.1), ('illuminant', 0.095), ('chart', 0.093), ('ex', 0.092), ('narrowband', 0.089), ('spectral', 0.084), ('spectrum', 0.077), ('illuminated', 0.068), ('em', 0.067), ('imaging', 0.057), ('bispectral', 0.056), ('boswell', 0.056), ('kex', 0.045), ('macbeth', 0.045), ('mcnamara', 0.045), ('relight', 0.045), ('brute', 0.043), ('emit', 0.041), ('basis', 0.04), ('nbn', 0.04), ('tb', 0.039), ('components', 0.037), ('multiplexed', 0.037), ('illuminants', 0.037), ('stokes', 0.037), ('josa', 0.036), ('illumination', 0.034), ('cr', 0.034), ('kexem', 0.033), ('relighted', 0.033), ('condition', 0.031), ('tominaga', 0.03), ('sparse', 0.029), ('colors', 0.028), ('multispectral', 0.027), ('force', 0.027), ('capture', 0.027), ('sato', 0.026), ('band', 0.025), ('switzerland', 0.024), ('sparsely', 0.024), ('scenes', 0.024), ('materials', 0.023), ('rgb', 0.023), ('nm', 0.023), ('subtract', 0.023), ('color', 0.022), ('alterman', 0.022), ('egrw', 0.022), ('emissions', 0.022), ('kem', 0.022), ('kemex', 0.022), ('maloney', 0.022), ('reradiation', 0.022), ('xcitation', 0.022), ('surface', 0.022), ('surfaces', 0.022), ('nov', 0.021), ('emitted', 0.021), ('phenomenon', 0.021), ('incident', 0.021), ('theories', 0.02), ('emits', 0.02), ('hyperspectral', 0.02), ('accurately', 0.02), ('scene', 0.019), ('fortunately', 0.019), ('camera', 0.018), ('full', 0.018), ('incoming', 0.018), ('cultural', 0.018), ('hullin', 0.018), ('cic', 0.018), ('munsell', 0.018), ('archiving', 0.018), ('nii', 0.018), ('dye', 0.018), ('pf', 0.018), ('blue', 0.017), ('recovery', 0.017), ('canceled', 0.017), ('monochrome', 0.017), ('measurements', 0.017), ('constancy', 0.017), ('established', 0.017), ('interact', 0.017), ('tunable', 0.016), ('would', 0.016), ('swap', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="409-tfidf-1" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>Author: Antony Lam, Imari Sato</p><p>Abstract: Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally, we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.</p><p>2 0.15356928 <a title="409-tfidf-2" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>Author: Zhenglong Zhou, Zhe Wu, Ping Tan</p><p>Abstract: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple, which consists of only a digital camera and a handheld light source. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distributionfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments, the captured shapes are accurate to 0.3 millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9%.</p><p>3 0.13240395 <a title="409-tfidf-3" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>Author: Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials, the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals, and that the intensity distribution of an intensity profile conveys information about the reflectance properties, when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations, we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.</p><p>4 0.1310007 <a title="409-tfidf-4" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>5 0.12405393 <a title="409-tfidf-5" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>Author: Timothy Yau, Minglun Gong, Yee-Hong Yang</p><p>Abstract: In underwater imagery, the image formation process includes refractions that occur when light passes from water into the camera housing, typically through a flat glass port. We extend the existing work on physical refraction models by considering the dispersion of light, and derive new constraints on the model parameters for use in calibration. This leads to a novel calibration method that achieves improved accuracy compared to existing work. We describe how to construct a novel calibration device for our method and evaluate the accuracy of the method through synthetic and real experiments.</p><p>6 0.10643428 <a title="409-tfidf-6" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>7 0.085279092 <a title="409-tfidf-7" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>8 0.072129957 <a title="409-tfidf-8" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>9 0.069687009 <a title="409-tfidf-9" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>10 0.068498611 <a title="409-tfidf-10" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>11 0.066614538 <a title="409-tfidf-11" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>12 0.061710246 <a title="409-tfidf-12" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>13 0.059232809 <a title="409-tfidf-13" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>14 0.057355184 <a title="409-tfidf-14" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>15 0.053866085 <a title="409-tfidf-15" href="./cvpr-2013-Whitened_Expectation_Propagation%3A_Non-Lambertian_Shape_from_Shading_and_Shadow.html">466 cvpr-2013-Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow</a></p>
<p>16 0.052604955 <a title="409-tfidf-16" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>17 0.05061765 <a title="409-tfidf-17" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>18 0.050228499 <a title="409-tfidf-18" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>19 0.045240041 <a title="409-tfidf-19" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>20 0.041451842 <a title="409-tfidf-20" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.072), (1, 0.092), (2, -0.006), (3, 0.047), (4, -0.008), (5, -0.059), (6, -0.066), (7, 0.026), (8, 0.022), (9, -0.012), (10, -0.05), (11, -0.072), (12, -0.025), (13, -0.057), (14, -0.013), (15, 0.079), (16, 0.089), (17, -0.061), (18, -0.002), (19, -0.051), (20, 0.047), (21, 0.024), (22, -0.021), (23, -0.008), (24, 0.003), (25, 0.025), (26, 0.026), (27, 0.001), (28, -0.008), (29, 0.009), (30, 0.011), (31, -0.069), (32, 0.017), (33, -0.038), (34, 0.005), (35, 0.008), (36, -0.019), (37, 0.033), (38, 0.009), (39, -0.019), (40, -0.101), (41, 0.015), (42, -0.018), (43, 0.007), (44, 0.001), (45, -0.044), (46, -0.029), (47, -0.007), (48, 0.003), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93658888 <a title="409-lsi-1" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>Author: Antony Lam, Imari Sato</p><p>Abstract: Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally, we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.</p><p>2 0.73616147 <a title="409-lsi-2" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>3 0.71083337 <a title="409-lsi-3" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>Author: Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials, the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals, and that the intensity distribution of an intensity profile conveys information about the reflectance properties, when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations, we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.</p><p>4 0.69168234 <a title="409-lsi-4" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>Author: Zhe Wu, Ping Tan</p><p>Abstract: Under unknown directional lighting, the uncalibrated Lambertian photometric stereo algorithm recovers the shape of a smooth surface up to the generalized bas-relief (GBR) ambiguity. We resolve this ambiguity from the halfvector symmetry, which is observed in many isotropic materials. Under this symmetry, a 2D BRDF slice with low-rank structure can be obtained from an image, if the surface normals and light directions are correctly recovered. In general, this structure is destroyed by the GBR ambiguity. As a result, we can resolve the ambiguity by restoring this structure. We develop a simple algorithm of auto-calibration from separable homogeneous specular reflection of real images. Compared with previous methods, this method takes a holistic approach to exploiting reflectance symmetry and produces superior results.</p><p>5 0.66785884 <a title="409-lsi-5" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>Author: Zhenglong Zhou, Zhe Wu, Ping Tan</p><p>Abstract: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple, which consists of only a digital camera and a handheld light source. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distributionfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments, the captured shapes are accurate to 0.3 millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9%.</p><p>6 0.62905788 <a title="409-lsi-6" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>7 0.59067076 <a title="409-lsi-7" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>8 0.58322227 <a title="409-lsi-8" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>9 0.54601461 <a title="409-lsi-9" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>10 0.54267365 <a title="409-lsi-10" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>11 0.52940571 <a title="409-lsi-11" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>12 0.52657855 <a title="409-lsi-12" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>13 0.50488985 <a title="409-lsi-13" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>14 0.49008927 <a title="409-lsi-14" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>15 0.47493538 <a title="409-lsi-15" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>16 0.46454614 <a title="409-lsi-16" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>17 0.46111199 <a title="409-lsi-17" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>18 0.45997187 <a title="409-lsi-18" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>19 0.43276957 <a title="409-lsi-19" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>20 0.42719856 <a title="409-lsi-20" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.071), (16, 0.068), (26, 0.022), (33, 0.16), (67, 0.023), (69, 0.043), (87, 0.063), (90, 0.42)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.6722343 <a title="409-lda-1" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>Author: Antony Lam, Imari Sato</p><p>Abstract: Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally, we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.</p><p>2 0.5135048 <a title="409-lda-2" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>Author: Feng Zhou, Fernando De_la_Torre</p><p>Abstract: Graph matching (GM) is a fundamental problem in computer science, and it has been successfully applied to many problems in computer vision. Although widely used, existing GM algorithms cannot incorporate global consistence among nodes, which is a natural constraint in computer vision problems. This paper proposes deformable graph matching (DGM), an extension of GM for matching graphs subject to global rigid and non-rigid geometric constraints. The key idea of this work is a new factorization of the pair-wise affinity matrix. This factorization decouples the affinity matrix into the local structure of each graph and the pair-wise affinity edges. Besides the ability to incorporate global geometric transformations, this factorization offers three more benefits. First, there is no need to compute the costly (in space and time) pair-wise affinity matrix. Second, it provides a unified view of many GM methods and extends the standard iterative closest point algorithm. Third, it allows to use the path-following optimization algorithm that leads to improved optimization strategies and matching performance. Experimental results on synthetic and real databases illustrate how DGM outperforms state-of-the-art algorithms for GM. The code is available at http : / / human s en s ing . c s . cmu .edu / fgm.</p><p>3 0.50004095 <a title="409-lda-3" href="./cvpr-2013-Graph-Laplacian_PCA%3A_Closed-Form_Solution_and_Robustness.html">191 cvpr-2013-Graph-Laplacian PCA: Closed-Form Solution and Robustness</a></p>
<p>Author: Bo Jiang, Chris Ding, Bio Luo, Jin Tang</p><p>Abstract: Principal Component Analysis (PCA) is a widely used to learn a low-dimensional representation. In many applications, both vector data X and graph data W are available. Laplacian embedding is widely used for embedding graph data. Wepropose a graph-Laplacian PCA (gLPCA) to learn a low dimensional representation of X that incorporates graph structures encoded in W. This model has several advantages: (1) It is a data representation model. (2) It has a compact closed-form solution and can be efficiently computed. (3) It is capable to remove corruptions. Extensive experiments on 8 datasets show promising results on image reconstruction and significant improvement on clustering and classification.</p><p>4 0.44605994 <a title="409-lda-4" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>Author: Lili Tao, Bogdan J. Matuszewski</p><p>Abstract: In this paper, a novel approach based on a non-linear manifold learning technique is proposed to recover 3D nonrigid structures from 2D image sequences captured by a single camera. Most ofthe existing approaches assume that 3D shapes can be accurately modelled in a linear subspace. These techniques perform well when the deformations are relatively small or simple, but fail when more complex deformations need to be recovered. The non-linear deformations are often observed in highly flexible objects for which the use of the linear model is impractical. A specific type of shape variations might be governed by only a small number of parameters, therefore can be wellrepresented in a low dimensional manifold. We learn a nonlinear shape prior using diffusion maps method. The key contribution in this paper is the introduction of the shape prior that constrain the reconstructed shapes to lie in the learned manifold. The proposed methodology has been validated quantitatively and qualitatively on 2D points sequences projected from the 3D motion capture data and real 2D video sequences. The comparisons oftheproposed man- ifold based method against several state-of-the-art techniques are shown on different types of deformable objects.</p><p>5 0.42767254 <a title="409-lda-5" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>6 0.42240006 <a title="409-lda-6" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>7 0.42111161 <a title="409-lda-7" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>8 0.42096514 <a title="409-lda-8" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>9 0.41995132 <a title="409-lda-9" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>10 0.4193534 <a title="409-lda-10" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>11 0.41863135 <a title="409-lda-11" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>12 0.41847149 <a title="409-lda-12" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>13 0.41830486 <a title="409-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.41683233 <a title="409-lda-14" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>15 0.4167594 <a title="409-lda-15" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>16 0.41673526 <a title="409-lda-16" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>17 0.41628417 <a title="409-lda-17" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>18 0.41597098 <a title="409-lda-18" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>19 0.41569942 <a title="409-lda-19" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>20 0.41554368 <a title="409-lda-20" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
