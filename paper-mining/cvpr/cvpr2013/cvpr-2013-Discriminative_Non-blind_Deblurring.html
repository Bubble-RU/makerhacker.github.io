<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 cvpr-2013-Discriminative Non-blind Deblurring</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-131" href="#">cvpr2013-131</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>131 cvpr-2013-Discriminative Non-blind Deblurring</h1>
<br/><p>Source: <a title="cvpr-2013-131-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Schmidt_Discriminative_Non-blind_Deblurring_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>Reference: <a title="cvpr-2013-131-reference" href="../cvpr2013_reference/cvpr-2013-Discriminative_Non-blind_Deblurring_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 † Microsoft Research Cambridge  Abstract Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. [sent-4, score-1.081]
</p><p>2 Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. [sent-5, score-0.571]
</p><p>3 One key challenge is that the blur kernel in use at test time is not known in advance. [sent-8, score-0.496]
</p><p>4 From this analysis, we derive a discriminative model cascade for image deblurring. [sent-10, score-0.256]
</p><p>5 Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. [sent-11, score-0.324]
</p><p>6 We train our model by loss minimization and use synthetically generated blur kernels to generate training data. [sent-12, score-0.786]
</p><p>7 Image deblurring has thus been an active area of research, starting with the pioneering work of Lucy [21] and Richardson [23]. [sent-18, score-0.53]
</p><p>8 Recent work has predominantly focused on blind deblurring, particularly on estimating the blur from images (stationary and non-stationary). [sent-19, score-0.52]
</p><p>9 Yet, this is an important problem since most blind deblurring approaches separate the problem into blur estimation and non-blind deblurring (theoretically justified by Levin et al. [sent-21, score-1.58]
</p><p>10 To this date, most approaches rely on the classical Lucy-Richardson algorithm as non-blind deblurring component [e. [sent-23, score-0.53]
</p><p>11 In this paper we introduce a discriminative non-blind image deblurring approach for arbitrary photographic input images and arbitrary blurs. [sent-30, score-0.69]
</p><p>12 To the best of our knowledge,  this is the first time discriminative deblurring has been attempted. [sent-31, score-0.601]
</p><p>13 the observed image is the result of convolving the unknown original image with a blur kernel (+ noise), but our approach is not limited to this setup and can be extended to non-uniform image blurs. [sent-35, score-0.533]
</p><p>14 Since it is in general not feasible to train a specialized model for every image blur, it is necessary to train a model that outputs a deblurred image given an arbitrary input image and blur kernel. [sent-37, score-0.687]
</p><p>15 We address this by effectively parametrizing our discriminative model with the blur kernel. [sent-38, score-0.514]
</p><p>16 We address this using a model cascade based on regression tree fields [15], which first predicts a relatively crude estimate that removes dominant image blur and is refined further in later stages. [sent-40, score-0.811]
</p><p>17 We use synthetically generated blur kernels to overcome this limitation. [sent-42, score-0.691]
</p><p>18 While previous non-blind deblurring approaches have for the most part either been very fast but 666000224  with inferior performance, or slow but with high-quality results [e. [sent-44, score-0.53]
</p><p>19 25], our approach delivers state-of-the-art deblurring performance with an efficient inference method that allows deblurring even higher-resolution images. [sent-46, score-1.06]
</p><p>20 In image deblurring, denoising and other restoration applications, sparse image priors are frequently used for regularization [e. [sent-49, score-0.274]
</p><p>21 I)n =the N case Kofx n,σon-blind deconvolution, we have Kx = k ⊗ x, where K is the bdleucro mnvaotrluixti otnha,t corresponds xto = convolving thhee image sw tihteh a blur kernel k. [sent-61, score-0.533]
</p><p>22 The image noise is assumed to be pixelindependent additive white Gaussian noise with variance σ2. [sent-62, score-0.216]
</p><p>23 , MAP estimation) by introducing (independent) auxiliary/latent variables zjc for each filter and image clique, such that the prior is re? [sent-66, score-0.265]
</p><p>24 Updating z based on p(z|y, x) is easy, since all zjc are independent:  p(z|x,y)  ∝  ? [sent-112, score-0.208]
</p><p>25 odtohmingfiebldu,t 666000335  Therefore μx|y,z∗ and Σx|y,z∗ are the mean and covariance parameters yo,zf a multivariate normal distribution defined on the whole image, chosen through so as to hopefully lead to good deblurring results. [sent-152, score-0.559]
</p><p>26 Non-blind image deblurring is more difficult than image denoising, and it might be difficult to directly regress suitable model parameters. [sent-169, score-0.685]
</p><p>27 Then, in the generative approach one can think of zjc as modulating pairwise potentials: reducing smoothness constraints in case of large image derivatives of the output image x, and imposing smoothness otherwise. [sent-171, score-0.249]
</p><p>28 But in the case of deblurring, the image content in y is shifted and combined with other parts of the image, depending on a blur kernel that is different for each image. [sent-175, score-0.496]
</p><p>29 We believe this is one of the reasons why discriminative non-blind deblurring approaches had not been attempted before. [sent-177, score-0.601]
</p><p>30 In a standard half-quadratic approach (top), each zjc can only be updated via Eq. [sent-181, score-0.208]
</p><p>31 In the proposed discriminative cascade (bottom), one can use arbitrary pferaotpuoresse do fd tihscer image over larger areas (large white circles) to find model parameters Θ(i) and via regression. [sent-183, score-0.318]
</p><p>32 Discriminative model cascade To build a discriminative model for deblurring, we draw inspiration from the iterative refinement of z in halfquadratic regularization. [sent-187, score-0.455]
</p><p>33 We start with an educated guess of the Gaussian model parameters, regressed from the input image, to obtain a restored image x(1) , which is less corrupted than the original input image. [sent-188, score-0.271]
</p><p>34 We can then use this as an intermediate result to help regress refined Gaussian model parameters, in order to obtain a better restored image x(2) , etc. [sent-189, score-0.272]
</p><p>35 Image denoising and other restoration tasks may also benefit from such a model cascade and repeated refinement of the auxiliary variables; we do not consider this here, however. [sent-192, score-0.417]
</p><p>36 As discussed above, a standard generative halfquadratic approach updates each zjc only based on the local clique of the current estimate of the restored image (cf. [sent-194, score-0.505]
</p><p>37 In a discriminative approach, we can regress the parameters based on arbitrary local and global properties of the input image as well as the current estimate of the re-  Θ(i)  666000446  stored image (see Fig. [sent-197, score-0.254]
</p><p>38 The proposed discriminative cascade is also related to the active random field [2], which is a multi-stage approach for image denoising that is trained discriminatively. [sent-203, score-0.345]
</p><p>39 Gaussian CRF for Deblurring As we have seen, a discriminative alternative to halfquadratic MAP estimation is conceptually attractive, but also challenging due to the need of regressing local image models from the blurred input image y. [sent-206, score-0.348]
</p><p>40 lO Gnaeu challenge i np devising such a model is that we cannot train a different model for every blur matrix K; this difficulty may in fact be the reason why no such approach exists to date. [sent-208, score-0.503]
</p><p>41 (t h6)a;t tthhee blur is not used as an input feature to the regressor. [sent-221, score-0.439]
</p><p>42 However, for deblurring this is not feasible, and it is crucial to incorporate a blur component into the model to adapt to arbitrary blurs. [sent-225, score-1.005]
</p><p>43 We extend these previous RTF-based approaches to our setting by (a) incorporating the blur likelihood for non-blind image deblurring into the prediction as outlined in Eqs. [sent-248, score-1.007]
</p><p>44 (6) and (7), and (b) by assembling multiple RTFs into a model cascade that iteratively refines the prediction (see Sec. [sent-249, score-0.22]
</p><p>45 Second, the model parameters of arbitrary pairwise Gaussian potentials (with full mean and covariance) are regressed from the input image, whereas [27] restrict their parameterization to diagonal weighting of filter responses. [sent-254, score-0.269]
</p><p>46 Since capturing image pairs of blurred and clean images is difficult, one possible avenue is to synthesize training data by blurring clean images with realistic blurs. [sent-262, score-0.296]
</p><p>47 Unfortunately, existing databases [16, 20] only supply a relatively limited number of blur kernels, and moreover serve also for testing. [sent-263, score-0.414]
</p><p>48 We address this problem by generating realistic-looking blur kernels by sampling random 3D trajectories using a simple linear motion model; the obtained trajectories are projected and rasterized to random square kernel sizes in the range from 5 5 up to 27 27 pixels (see Fig. [sent-265, score-0.679]
</p><p>49 r Wealhi slteic i tk weronuellds through more accurate models of camera shake motion3, we find that these synthetic kernels already allow to generalize well to unseen real blur (cf. [sent-268, score-0.706]
</p><p>50 We synthetically generate blurred images by convolving each clean image with an artificially generated blur kernel, and subsequently add pixel-independent Gaussian noise (using standard deviations σ = 2. [sent-271, score-0.83]
</p><p>51 2, it is difficult to directly regress good  local image models from the blurred input image. [sent-279, score-0.287]
</p><p>52 Therefore, we employ a cascade of RTF models, where each subsequent model stage uses the output of all previous models as features for the regression (see Fig. [sent-280, score-0.396]
</p><p>53 We train the first stage of the cascade with minimal conditioning on the input image to avoid overfitting. [sent-282, score-0.303]
</p><p>54 The parameters of the unary and pairwise potentials are only linearly regressed from the pixels in the respective cliques (plus a constant pseudo-input); we do not use a regression tree. [sent-283, score-0.246]
</p><p>55 We train this model with 200 pairs of blurred and clean images, which is ample since there are only few model parameters. [sent-285, score-0.26]
</p><p>56 3We think that on average these synthetic blur kernels more challenging than typical real ones. [sent-287, score-0.597]
</p><p>57 While we do not expect excellent results from RTF1 , it is able to remove the dominant blur from the input image (cf. [sent-291, score-0.466]
</p><p>58 6) and makes it much easier for subsequent  ××  RTF stages to regress good potentials for the CRF. [sent-294, score-0.276]
</p><p>59 However, we use a different filter bank here, the 16 generatively trained 5 5 fdiliftefersre nfrot mfil tehre b arencken hte Fields-of-Experts mveoldye trl aoinf [e1d0 5]; we found these filters to outperform other filter banks we have tried, including those used in [14]4. [sent-298, score-0.245]
</p><p>60 An interesting property of our model cascade is that it  yields a deblurred image after every stage, not only at the end. [sent-311, score-0.281]
</p><p>61 Even if a deep cascade was trained, at test time we can trade off computational resources versus quality of the deblurred image by stopping after a certain stage (cf. [sent-312, score-0.368]
</p><p>62 when training and testing is carried out with perfect blur kernels. [sent-319, score-0.449]
</p><p>63 proach by training the model to deal with imperfect blur kernels. [sent-329, score-0.478]
</p><p>64 This is important for blind deblurring, where the estimated blur kernels mostly contain some errors. [sent-330, score-0.731]
</p><p>65 Please note that images and kernels are always kept strictly separate for training and testing in all experiments. [sent-332, score-0.218]
</p><p>66 We trained a six-stage RTF prediction cascade as described in Sec. [sent-334, score-0.232]
</p><p>67 Training images have been blurred synthetically with 1% additive white Gaussian noise (σ = 2. [sent-336, score-0.371]
</p><p>68 While we used artifical blur kernels to generate our training data, the test images from [25] have been created with the realistic kernels from [20]. [sent-340, score-0.87]
</p><p>69 The blur kernels used for deblurring are slightly perturbed from the ground truth to mimic kernel estimation errors, but the perturbation is somewhat minor here. [sent-341, score-1.234]
</p><p>70 ground truth) blur kernels, as usual for non-blind deblurring, our approach achieves excellent results. [sent-363, score-0.441]
</p><p>71 our model has been trained on artificially generated blur kernels (Fig. [sent-369, score-0.706]
</p><p>72 Blind deblurring approaches often produce kernel estimates with substantial errors, which can cause ringing artifacts in the restored image [cf. [sent-372, score-0.772]
</p><p>73 To train our model for this task, we experimented with adding noise to the ground truth kernels and also used estimated kernels for training. [sent-375, score-0.554]
</p><p>74 [19] as a benchmark, which provides several kernel estimates besides blurred and ground truth images for 32 test instances, as well as deblurring results with the various kernel estimates. [sent-377, score-0.901]
</p><p>75 Since the amount of noise in these blurred images is significantly lower than in the benchmark of [25], we only added Gaussian noise with σ = 0. [sent-378, score-0.321]
</p><p>76 2 show that training with ground truth kernels leads to subpar performance when kernels estimates are used at test time. [sent-382, score-0.472]
</p><p>77 Adding noise to the ground  truth kernels for training leads to improved results of RTF1 with estimated kernels at test time, but performance of our second stage model RTF2 already deteriorates; hence those noisy kernels are not an ideal proxy for real kernel estimates. [sent-383, score-0.914]
</p><p>78 However, we achieve superior results by training our model with a mix of perfect and estimated kernels (obtained with the method of Xu and Jia [30]), i. [sent-384, score-0.304]
</p><p>79 for half of the synthetically blurred training images we use an estimated kernel instead of the ground truth kernel5. [sent-386, score-0.4]
</p><p>80 Compared to the deblurred images from [19] (which used the 5Here, we trained RTF1 and RTF2 with the same 200 images as it was time-consuming to obtain good enough kernel estimates for training. [sent-387, score-0.265]
</p><p>81 666000779  those in the two rightmost columns: we derived the noisy ground truth (GT) kernels from the provided GT kernels, and estimated kernels with [30]. [sent-388, score-0.419]
</p><p>82 The last row shows the average performance of deblurring results provided by [19] (using the non-blind approach of [18]). [sent-389, score-0.53]
</p><p>83 For the kernel estimates of [19]  (4th column),  we used the “free energy with diagonal covariance approximation”  algorithm in the filter domain. [sent-390, score-0.214]
</p><p>84 non-blind approach of [18]), we achieve substantial performance improvements for deblurring with estimated kernels of up to 0. [sent-391, score-0.741]
</p><p>85 [16] to demonstrate results on realistic higher-resolution images; these images may substantially violate our model’s stationary blur and Gaussian noise assumptions (which can deteriorate performance [cf. [sent-406, score-0.625]
</p><p>86 The overall best performing blind deblurring approach in this benchmark is the one by Xu and Jia [30] despite making a stationary blur assumption, i. [sent-409, score-1.126]
</p><p>87 the same blur kernel is used in all parts of the image. [sent-411, score-0.496]
</p><p>88 We use the provided kernel estimates by [30] from the benchmark dataset, but with our non-blind method to obtain the deblurred image (treating color channels R, G, and B independently). [sent-412, score-0.259]
</p><p>89 While Xu  6The result might not be fully comparable, since the blur kernel estimation and non-blind method from [6] may have been used. [sent-415, score-0.496]
</p><p>90 7Theoretically, in the absence of noise non-blind deblurring can be solved exactly without any regularization by inverting the blur matrix. [sent-416, score-1.071]
</p><p>91 [16] for each combination of 4 test images and 12 blur kernels. [sent-418, score-0.414]
</p><p>92 We use the provided blur kernel estimates of [30] with our RTF2 model for non-blind deblurring. [sent-419, score-0.571]
</p><p>93 3), showing the result of our RTF2 model (right) given the blurred image (left) and the kernel estimates by [30]. [sent-422, score-0.293]
</p><p>94 trained with a mix of ground truth and estimated kernels (using [30]), and additive Gaussian noise with σ = 0. [sent-430, score-0.417]
</p><p>95 The first stage RTF1 removes dominant blur from the image (c), but artifacts remain. [sent-434, score-0.53]
</p><p>96 The blur kernel is shown at the upper left of (b), scaled and resized for better visualization. [sent-438, score-0.496]
</p><p>97 Summary  and Conclusions  From a novel analysis of common half-quadratic regularization, we introduced – to the best of our knowledge – the first discriminative non-blind deblurring method. [sent-441, score-0.601]
</p><p>98 Our proposed cascade model is based on regression tree fields at each stage, which are trained by loss minimization on training data generated with synthesized blur kernels. [sent-442, score-0.859]
</p><p>99 Our approach is not limited to image deblurring and can readily be extended to other image restoration applications in the future. [sent-444, score-0.648]
</p><p>100 Motion-aware noise filtering for deblurring of noisy and blurry images. [sent-625, score-0.605]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('deblurring', 0.53), ('blur', 0.414), ('zjc', 0.208), ('kernels', 0.183), ('rtf', 0.171), ('cascade', 0.156), ('blurred', 0.136), ('regress', 0.126), ('restoration', 0.118), ('halfquadratic', 0.116), ('ktk', 0.116), ('blind', 0.106), ('deblurred', 0.096), ('regression', 0.095), ('synthetically', 0.094), ('fjtx', 0.092), ('stage', 0.091), ('restored', 0.089), ('cj', 0.086), ('kernel', 0.082), ('jancsary', 0.079), ('shake', 0.078), ('denoising', 0.077), ('crf', 0.077), ('gaussian', 0.077), ('noise', 0.075), ('levin', 0.072), ('discriminative', 0.071), ('kty', 0.069), ('ohler', 0.069), ('rtfs', 0.069), ('regressed', 0.063), ('potentials', 0.063), ('stages', 0.062), ('filter', 0.057), ('realistic', 0.055), ('schmidt', 0.052), ('regressor', 0.052), ('regularization', 0.052), ('clique', 0.051), ('deconvolution', 0.05), ('bank', 0.049), ('psnr', 0.049), ('gsms', 0.046), ('uwe', 0.046), ('estimates', 0.046), ('fields', 0.045), ('tree', 0.044), ('generatively', 0.041), ('trained', 0.041), ('stationary', 0.041), ('fj', 0.041), ('generative', 0.041), ('substantially', 0.04), ('corrupted', 0.04), ('artificially', 0.039), ('fergus', 0.039), ('krishnan', 0.038), ('tappen', 0.037), ('kx', 0.037), ('auxiliary', 0.037), ('nowozin', 0.037), ('convolving', 0.037), ('additive', 0.036), ('benchmark', 0.035), ('training', 0.035), ('prediction', 0.035), ('clean', 0.035), ('expressive', 0.034), ('generalizes', 0.034), ('jia', 0.034), ('posterior', 0.033), ('geman', 0.033), ('durand', 0.033), ('arbitrary', 0.032), ('camera', 0.031), ('train', 0.031), ('corruption', 0.03), ('white', 0.03), ('mix', 0.029), ('iterative', 0.029), ('model', 0.029), ('covariance', 0.029), ('likelihood', 0.028), ('refined', 0.028), ('estimated', 0.028), ('regressors', 0.028), ('priors', 0.027), ('excellent', 0.027), ('xu', 0.027), ('subsequent', 0.025), ('input', 0.025), ('deep', 0.025), ('truth', 0.025), ('cliques', 0.025), ('inspiration', 0.025), ('mrf', 0.025), ('artifacts', 0.025), ('date', 0.024), ('dagm', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="131-tfidf-1" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>2 0.59192854 <a title="131-tfidf-2" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>3 0.49294853 <a title="131-tfidf-3" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>Author: Li Xu, Shicheng Zheng, Jiaya Jia</p><p>Abstract: We show in this paper that the success of previous maximum a posterior (MAP) based blur removal methods partly stems from their respective intermediate steps, which implicitly or explicitly create an unnatural representation containing salient image structures. We propose a generalized and mathematically sound L0 sparse expression, together with a new effective method, for motion deblurring. Our system does not require extra filtering during optimization and demonstrates fast energy decreasing, making a small number of iterations enough for convergence. It also provides a unifiedframeworkfor both uniform andnon-uniform motion deblurring. We extensively validate our method and show comparison with other approaches with respect to convergence speed, running time, and result quality.</p><p>4 0.49291316 <a title="131-tfidf-4" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: Motion blur frequently occurs in dense 3D reconstruction using a single moving camera, and it degrades the quality of the 3D reconstruction. To handle motion blur caused by rapid camera shakes, we propose a blur-aware depth reconstruction method, which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. Motion blur is dependent on 3D geometry, thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. The estimated depth is then converted intopixel-wise blur kernels, and non-uniform motion blur is easily removed with low computational cost. The obtained blur kernel is depth-dependent, thus it effectively addresses scene-depth variation, which is a challenging problem in conventional non-uniform deblurring methods.</p><p>5 0.44433734 <a title="131-tfidf-5" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>6 0.42456988 <a title="131-tfidf-6" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>7 0.40353554 <a title="131-tfidf-7" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>8 0.25940481 <a title="131-tfidf-8" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>9 0.20004231 <a title="131-tfidf-9" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>10 0.19433153 <a title="131-tfidf-10" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>11 0.12941945 <a title="131-tfidf-11" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>12 0.12675087 <a title="131-tfidf-12" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>13 0.10658139 <a title="131-tfidf-13" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>14 0.086246312 <a title="131-tfidf-14" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>15 0.082651146 <a title="131-tfidf-15" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>16 0.082174122 <a title="131-tfidf-16" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>17 0.080928527 <a title="131-tfidf-17" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>18 0.078697234 <a title="131-tfidf-18" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>19 0.077436157 <a title="131-tfidf-19" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>20 0.077166073 <a title="131-tfidf-20" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.201), (1, 0.19), (2, -0.054), (3, 0.172), (4, -0.136), (5, 0.626), (6, 0.11), (7, -0.005), (8, 0.04), (9, -0.024), (10, -0.001), (11, -0.035), (12, -0.037), (13, 0.008), (14, -0.042), (15, 0.041), (16, 0.096), (17, -0.004), (18, 0.01), (19, -0.007), (20, 0.014), (21, 0.003), (22, 0.006), (23, 0.077), (24, -0.028), (25, -0.018), (26, 0.009), (27, 0.028), (28, -0.007), (29, 0.008), (30, 0.009), (31, 0.012), (32, -0.014), (33, 0.01), (34, 0.03), (35, 0.029), (36, -0.01), (37, -0.028), (38, 0.024), (39, 0.039), (40, -0.021), (41, 0.033), (42, 0.018), (43, 0.011), (44, 0.009), (45, 0.023), (46, -0.009), (47, -0.004), (48, 0.018), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95543724 <a title="131-lsi-1" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>same-paper 2 0.94366241 <a title="131-lsi-2" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>3 0.92544019 <a title="131-lsi-3" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>4 0.91436243 <a title="131-lsi-4" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>5 0.90750164 <a title="131-lsi-5" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>Author: Yi Zhang, Keigo Hirakawa</p><p>Abstract: We propose a notion of double discrete wavelet transform (DDWT) that is designed to sparsify the blurred image and the blur kernel simultaneously. DDWT greatly enhances our ability to analyze, detect, and process blur kernels and blurry images—the proposed framework handles both global and spatially varying blur kernels seamlessly, and unifies the treatment of blur caused by object motion, optical defocus, and camera shake. To illustrate the potential of DDWT in computer vision and image processing, we develop example applications in blur kernel estimation, deblurring, and near-blur-invariant image feature extraction.</p><p>6 0.86856687 <a title="131-lsi-6" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>7 0.79280037 <a title="131-lsi-7" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>8 0.75979507 <a title="131-lsi-8" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>9 0.74825239 <a title="131-lsi-9" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>10 0.70519221 <a title="131-lsi-10" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>11 0.64309502 <a title="131-lsi-11" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>12 0.38177344 <a title="131-lsi-12" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>13 0.37909314 <a title="131-lsi-13" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>14 0.37113294 <a title="131-lsi-14" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>15 0.35501999 <a title="131-lsi-15" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>16 0.28799215 <a title="131-lsi-16" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>17 0.27674156 <a title="131-lsi-17" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>18 0.26961395 <a title="131-lsi-18" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>19 0.25944665 <a title="131-lsi-19" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>20 0.25917724 <a title="131-lsi-20" href="./cvpr-2013-Kernel_Methods_on_the_Riemannian_Manifold_of_Symmetric_Positive_Definite_Matrices.html">238 cvpr-2013-Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.26), (16, 0.015), (17, 0.138), (26, 0.058), (33, 0.252), (57, 0.013), (67, 0.049), (69, 0.035), (87, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93228227 <a title="131-lda-1" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>2 0.93019789 <a title="131-lda-2" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>3 0.92791718 <a title="131-lda-3" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>4 0.9246462 <a title="131-lda-4" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>Author: unkown-author</p><p>Abstract: We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the “right” frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.</p><p>5 0.92407715 <a title="131-lda-5" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>6 0.92369848 <a title="131-lda-6" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>7 0.9229908 <a title="131-lda-7" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>8 0.91522366 <a title="131-lda-8" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>9 0.91512012 <a title="131-lda-9" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>10 0.9132641 <a title="131-lda-10" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>same-paper 11 0.90772694 <a title="131-lda-11" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>12 0.90483296 <a title="131-lda-12" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>13 0.89603734 <a title="131-lda-13" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>14 0.88986945 <a title="131-lda-14" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>15 0.88951641 <a title="131-lda-15" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>16 0.88758081 <a title="131-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.88530344 <a title="131-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.87880743 <a title="131-lda-18" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>19 0.87733847 <a title="131-lda-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.87430418 <a title="131-lda-20" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
