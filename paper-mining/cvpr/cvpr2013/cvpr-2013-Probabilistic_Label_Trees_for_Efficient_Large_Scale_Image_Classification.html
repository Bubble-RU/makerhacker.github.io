<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-340" href="#">cvpr2013-340</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</h1>
<br/><p>Source: <a title="cvpr-2013-340-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Liu_Probabilistic_Label_Trees_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>Reference: <a title="cvpr-2013-340-reference" href="../cvpr2013_reference/cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. [sent-4, score-1.127]
</p><p>2 In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. [sent-5, score-0.802]
</p><p>3 This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy. [sent-6, score-0.779]
</p><p>4 Introduction In this paper, we present an improved probabilistic model for distinguishing between large numbers of categories with a label tree model. [sent-8, score-0.738]
</p><p>5 Thus, if there are K possible classes, assigning a label to a new feature vector x will require the computation of K dot products between x and the vectors defining the classifier. [sent-12, score-0.449]
</p><p>6 introduce the label tree model for reducing the complexity of recognition in a problem with a larger number of classes. [sent-17, score-0.639]
</p><p>7 In the label tree model, a feature vector is assigned a label by traversing a tree. [sent-18, score-0.935]
</p><p>8 At each node visited, the classifier computes the dot product between the Ohad Shamir, Ce Liu Microsoft Research, New England {ohadsh , cel iu} @mi cros o ft . [sent-19, score-0.542]
</p><p>9 This tree structure causes the classification complexity to grow logarithmically, rather than linearly, with the number of classes. [sent-21, score-0.488]
</p><p>10 We show how a recursive process learns the tree parameters. [sent-25, score-0.458]
</p><p>11 From a broader perspective, formulating the label tree in a probabilistic framework provides a straight-forward avenue for integrating more complex, accurate classification models into the label tree framework. [sent-28, score-1.442]
</p><p>12 Overview of a Label Tree This section will briefly review how a label tree operates and previous work on learning the parameters of the tree. [sent-31, score-0.717]
</p><p>13 Following previous work in [1, 8], we will focus on a label tree that uses linear classifiers at each node of the tree. [sent-32, score-1.028]
</p><p>14 Algorithm 1 Classifying a test example with the label tree algorithm. [sent-33, score-0.662]
</p><p>15 Input: Test example x, label tree parameters T,σ,l 1: Initialize s to the root node  = ∅ do s ← acr∈gσm(sa)xwc? [sent-34, score-1.118]
</p><p>16 x  2: while σ(s)  3:  4: end while 5: Assign the label l(s) to the test example  Following the notation in [8], a label tree is a tree with nodes V and edges E, such that the tree T = (V, E). [sent-35, score-1.889]
</p><p>17 Every child node, c, is associated with a vector of weights wc that are used to choose which child node will be visited during the classification process. [sent-37, score-1.186]
</p><p>18 Each leaf node is also associated 888884444433111  with a label, l(s), for a node s, that specifies the label that should be assigned to the example if that node s is reached as a leaf node. [sent-38, score-1.881]
</p><p>19 Except for the leaf nodes, each node is treated identically. [sent-40, score-0.627]
</p><p>20 At a node s, assuming it is not a leaf node, classification scores are computed for every child node of s using the weights wc for the child node c. [sent-41, score-2.119]
</p><p>21 The child node with the highest classification score becomes the next node to be visited. [sent-42, score-1.067]
</p><p>22 As described above, if the classification algorithm arrives at a leaf node, then the test example is assigned the label l(s), where s is the leaf node visited. [sent-43, score-1.293]
</p><p>23 Learning the label tree parameters requires finding the classifier weights for each node and the labels for the leaf nodes. [sent-47, score-1.394]
</p><p>24 The efficiency benefits of the tree structure will be maximized when the examples or classes are balanced among the leaf nodes. [sent-48, score-0.979]
</p><p>25 build a randomized tree at test time, and each sample traverse the tree from the root to single leaf node which corresponds to a single class. [sent-50, score-1.58]
</p><p>26 Conditional Probability Tree(CPT) [11] adopts a similar idea as [3], but they build the tree at training time based on optimizing conditional probability. [sent-51, score-0.54]
</p><p>27 propose to learn a label embedding tree for multi-class tasks [1]. [sent-55, score-0.668]
</p><p>28 In this method, the tree structure is determined by the spectral clustering of the confusion matrix, which is generated by a one-against-all classifier. [sent-56, score-0.423]
</p><p>29 Then they learn the tree classifier based on a tree based loss function that outperforms independent nodes optimization. [sent-57, score-1.105]
</p><p>30 In addition, label embedding is integrated into the tree framework to further boost their accuracy and speed. [sent-58, score-0.669]
</p><p>31 propose a margin-based model that finds the label tree parameters with two alternating opti-  mization steps. [sent-60, score-0.703]
</p><p>32 In the first step, the system learns the linear classifiers that choose the child node to be visited by maximizing a multi-class margin score. [sent-61, score-0.761]
</p><p>33 The list of classes assigned to each node is determined in a second optimization that attempts to create a balanced tree by assigning classes to different child nodes. [sent-63, score-1.406]
</p><p>34 Learning a Probabilistic Label Tree We propose a probabilistic approach for learning the label tree parameters. [sent-67, score-0.779]
</p><p>35 Each node, s, in the probabilistic label tree is associated with a categorical probability distribution p[y|S] where y dweitnhote as c ctahtee lgaobreicl aolf pthroe bteabsti elixtyam dpislter iabnudti oSn nde pn[yo|teSs] t wheh eevreen yt that the inference process has arrived at node s. [sent-69, score-1.431]
</p><p>36 This categorical distribution can be combined with a probabilistic classifier defined by the classification vectors at each node to compute the probability of a particular label being assigned to the test example, given the feature vector x. [sent-70, score-1.214]
</p><p>37 Defining the Learning Criterion Defining the label tree as a probabilistic model makes it natural to take a maximum likelihood approach to learning  the parameters. [sent-73, score-0.88]
</p><p>38 At the root node, the classification vectors are applied to the feature vector x to determine which child node should be visited. [sent-85, score-0.793]
</p><p>39 The final label of the example is then determined by the categorical distribution associated with the child node. [sent-86, score-0.783]
</p><p>40 Recursively Expanding the Model  If there are K classes and each node has n children, then the tree will need at least logn K levels of child nodes so that there is at least one leaf node for each possible class. [sent-89, score-1.979]
</p><p>41 Having fewer leaf nodes than classes will guarantee ambiguous results as some leaf nodes will be forced to represent two classes. [sent-90, score-0.935]
</p><p>42 If c1 repr(e1)se bnyts tehceu crshivildel yno edxep cahndosineng eata cthhe p f[iyr|stS level and c2 represents a child node of c1, a two level model would have the form p(y|x) =  ? [sent-92, score-0.698]
</p><p>43 Building the Tree Stagewise Given unlimited computing resources, the label tree parameters could be found by expanding Equation (3) to the desired number of levels and optimizing the log of Equation (3) with a continuous optimization algorithm. [sent-101, score-0.897]
</p><p>44 Returning to the probability at the root node in Equation (1), once the classifier vectors and parameters of the categorical distributions p[y|Sc] have been found, Jensen’s cInaeteqguoarli tcya lc danis btrieb uaptipolinesd p t[oy |thSe log of Equation (1) to compute a lower bound on p(y|x). [sent-104, score-0.901]
</p><p>45 σ(r) the categorical distribution at each of the child nodes can be expanded recursively to have classifiers and a new set of child nodes. [sent-111, score-1.122]
</p><p>46 This decouples the  child nodes during training and makes it possible to independently learn the parameters for each of the child nodes. [sent-117, score-0.842]
</p><p>47 This also shows that learning the parameters of the label tree can be viewed as a stagewise lower-bound maximization of the log likelihood function for the classification problem. [sent-118, score-0.931]
</p><p>48 Final Algorithm for Learning a Probabilistic Label Tree The process of learning the label tree can be viewed as a recursive expansion of nodes. [sent-120, score-0.801]
</p><p>49 Starting with the root node, each non-leaf node is expanded into a branch node by iteratively performing: (1)Learning the maximum likelihood classifiers based on the categorical distribution of each child node (2) Learning the categorical distribution associated with each child node. [sent-121, score-2.623]
</p><p>50 The overall training process is more easily specified by defining a general loss for expanding a node s, given N training pairs of the form (yi, xi), expressed as:  L =i? [sent-125, score-0.539]
</p><p>51 Formal Specification of the Algorithm Our formal specification of the algorithm for constructing the label tree will involve two sets offunctions. [sent-134, score-0.701]
</p><p>52 First, we will define ψ(s) to be the set of nodes that must be traversed before arriving at node s, or the path to s. [sent-135, score-0.523]
</p><p>53 Figure 1(a) shows the categorical distribution at a branch node in the second level of a T6,4 tree that has four levels, not counting the root node. [sent-141, score-1.333]
</p><p>54 Figure 1(b) - 1(d) show several of the six child nodes of this branch node. [sent-142, score-0.611]
</p><p>55 The table in (a) shows a portion of the categorical distribution at a branch node in the second level of a T6,4 tree that has four levels, excluding the root node. [sent-145, score-1.333]
</p><p>56 The tables in (b), (c), and (d) show the categorical distributions learned for three of the child nodes. [sent-146, score-0.544]
</p><p>57 The probability in these child nodes is more concentrated on a subset of the classes than in the parent node. [sent-147, score-0.6]
</p><p>58 In this result, the tree has not been trained with the pruning techniques in Section. [sent-148, score-0.477]
</p><p>59 Algorithm 2 Algorithm for Learning Node Parameters Input: N training pairs (yi,xi), maximum number of levels, L, branching factor B, weight α Test example x, label tree parameters T, σ, l 1: for l= 0 . [sent-151, score-0.798]
</p><p>60 Balanced Trees and Efficiency Increasing classification efficiency is the primary motivation behind the label tree model. [sent-169, score-0.729]
</p><p>61 The number of dot products depends on the number of leaf nodes, which is itself dependent on how balanced the tree is. [sent-171, score-1.019]
</p><p>62 A perfect balancing of the probabilities for each class across the nodes of the tree would minimize the number of leaf nodes needed. [sent-172, score-1.022]
</p><p>63 However, in most natural applications, it is reasonable to assume that the la-  bel distribution can be well approximated by a reasonablybalanced label tree, where the leaf nodes distribution concentrate on individual classes. [sent-176, score-0.758]
</p><p>64 A simple way to demonstrate this is to show that if the data distribution indeed corresponds to a balanced label tree, then the maximum-likelihood approach would learn a similar balanced tree. [sent-177, score-0.555]
</p><p>65 Then as the dataset size increases, the structure and weights of the label tree learned by our algorithm converges to those of the true label tree Proof. [sent-189, score-1.304]
</p><p>66 It is enough to show that we can perfectly reconstruct the root node of the tree - the reconstruction of its child nodes and other nodes in the tree would follow in a  similar way by induction. [sent-190, score-1.933]
</p><p>67 1, learning the label tree parameters at each node consists of two alternating steps: learning the classifier weights, then learning the categorical distribution. [sent-273, score-1.476]
</p><p>68 Eliminating Samples During Training  In [8], each node is assigned a specific set of classes during the training process. [sent-279, score-0.517]
</p><p>69 An advantage of this approach is that only the training examples from those classes need to be considered when learning the classifier parameters for that node and children of the node. [sent-280, score-0.693]
</p><p>70 In the probabilistic model proposed here, the learning criterion depends on the probability that each example arrives at the node where the parameters are being learned. [sent-281, score-0.664]
</p><p>71 In practice, this probability is often quite small, but still non-zero, so the learning could require processing all of the training examples at every node in the label tree. [sent-282, score-0.733]
</p><p>72 To increase the speed of training, the training examples used at a node are pruned to eliminate examples that have a very low probability of reaching the node. [sent-283, score-0.501]
</p><p>73 Fixing the Number of Leaf Nodes Following [8], the label tree is constructed with a fixed number of levels. [sent-287, score-0.639]
</p><p>74 Ideally, the learning system would be able to find a perfectly balanced tree and each leaf node would correspond to one class. [sent-288, score-1.282]
</p><p>75 In practice, it is difficult to find a perfectly balanced tree, so the number of leaf nodes must be determined individually for each branch in the next to last level of the tree. [sent-289, score-0.778]
</p><p>76 A number of training examples could be assigned to a leaf node with very low probability, so a natural criterion is to assign one leaf per class for the set of classes that account for some percentage, such as 90%, of the probability in the categorical distribution. [sent-290, score-1.396]
</p><p>77 While this threshold could be adjusted to achieve a desired number of leaf-nodes, we found it easier to directly control this number by imposing a hard cap on the number of leaf nodes per branch node. [sent-292, score-0.668]
</p><p>78 at68e74hasm  children per node when branching and n levels. [sent-309, score-0.465]
</p><p>79 The accuracy of maximum likelihood (ML) is consistently better than  the hard label partition with similar speedup. [sent-315, score-0.442]
</p><p>80 The tree denoted by Tm, n has m children per node when branching and n levels, not including the root node. [sent-338, score-0.972]
</p><p>81 The trees trained using maximum likelihood produce competitive results while requiring 18 to 30 times less dot products at test time. [sent-353, score-0.439]
</p><p>82 In our probabilistic approach, the training process maintains the probability of each sample reaching a particular node when optimizing that node’s parameters. [sent-359, score-0.579]
</p><p>83 In contrast, the learning approach in [8] uses a partition matrix that is rounded so that all examples from a class either reach the node or do not. [sent-360, score-0.47]
</p><p>84 We refer to this as hard label partitioning because every class, and all of its examples, are assigned a hard binary label describing whether they can reach a node. [sent-361, score-0.618]
</p><p>85 To explore whether this style of partitioning can produce improved results, we evaluated the combination ofour probabilistic system with strategy used in [8] to measure whether hard label partitioning is a superior strategy for learning the label tree. [sent-363, score-0.725]
</p><p>86 For numerical reasons, classes not assigned to a node are given a very small probability. [sent-366, score-0.478]
</p><p>87 The result is  a two-stage alternating algorithm that consists of learning a linear classifier in the form of a multinomial logistic regression classifier, then using the classifications from that classifier to learn the partition matrix. [sent-367, score-0.455]
</p><p>88 As the second row of Table 1 shows, training the tree just using the maximum likelihood criterion consistently outperforms this approach. [sent-371, score-0.598]
</p><p>89 Exploring the Accuracy-Efficiency Trade-off  One of the weaknesses of the classification procedure in a label tree is that the classifier must make a hard choice to decide which branch to follow. [sent-380, score-0.978]
</p><p>90 The maximum likelihood method has higher classification accuracy with less average dot products needed at test time . [sent-420, score-0.448]
</p><p>91 Again, the tree trained with the maximum likelihood approach has consistently higher accuracy. [sent-454, score-0.552]
</p><p>92 The final classification is found by adding together the categorical distributions at each of the leaf nodes that the classification  algorithms reaches. [sent-458, score-0.847]
</p><p>93 The green curves in Figures 2 and 3 show the relationship in different tree models between the average number of dotproducts needed to classify a sample and the resulting accuracy. [sent-460, score-0.502]
</p><p>94 These curves is shorter because we found that as the ambiguity limits increased, the number of classes at deep levels of the tree increased dramatically and the time required to train the system became untenable. [sent-464, score-0.659]
</p><p>95 In both figures, the tree learned with the probabilistic approach provides better accuracy for a similar average number of dot products. [sent-465, score-0.671]
</p><p>96 It is also important to note that creating this curve with our model does not require that tree parameters be retrained. [sent-466, score-0.494]
</p><p>97 On the other hand, using the optimization with ambiguity constraints requires that the tree be re-trained for different parameters. [sent-468, score-0.48]
</p><p>98 Conclusion In this work, we propose a probabilistic label tree frame-  work to accelerate large scale classification problems. [sent-470, score-0.803]
</p><p>99 Our experiments show that learning a label tree in this fashion can improve recognition accuracy with comparable speedups to previous work. [sent-472, score-0.71]
</p><p>100 Fast and balanced: Efficient label tree learning for large scale object recognition. [sent-550, score-0.68]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tree', 0.423), ('node', 0.358), ('child', 0.286), ('leaf', 0.269), ('sc', 0.24), ('categorical', 0.227), ('label', 0.216), ('nodes', 0.165), ('branch', 0.16), ('balanced', 0.128), ('dot', 0.119), ('wc', 0.113), ('probabilistic', 0.099), ('multinomial', 0.092), ('trees', 0.088), ('expansion', 0.086), ('root', 0.084), ('products', 0.08), ('xewi', 0.073), ('expanding', 0.069), ('classes', 0.067), ('classifier', 0.065), ('classification', 0.065), ('beygelzimer', 0.065), ('likelihood', 0.064), ('conference', 0.062), ('stochastic', 0.061), ('children', 0.061), ('logistic', 0.057), ('ambiguity', 0.057), ('equation', 0.054), ('probability', 0.054), ('distribution', 0.054), ('assigned', 0.053), ('levels', 0.053), ('visited', 0.052), ('hard', 0.049), ('dotproducts', 0.049), ('wrc', 0.049), ('conditional', 0.049), ('deng', 0.048), ('partition', 0.046), ('branching', 0.046), ('log', 0.045), ('langford', 0.043), ('maximized', 0.042), ('learning', 0.041), ('vlfeat', 0.04), ('stagewise', 0.04), ('arrives', 0.04), ('expanded', 0.039), ('training', 0.039), ('branches', 0.039), ('parameters', 0.037), ('bengio', 0.037), ('maximum', 0.037), ('partitioning', 0.035), ('recursive', 0.035), ('criterion', 0.035), ('specification', 0.034), ('defining', 0.034), ('recursively', 0.034), ('curve', 0.034), ('system', 0.034), ('regression', 0.033), ('ter', 0.032), ('pages', 0.032), ('classifiers', 0.031), ('distributions', 0.031), ('ieee', 0.03), ('accuracy', 0.03), ('needed', 0.03), ('optimizing', 0.029), ('descent', 0.029), ('unbalanced', 0.029), ('perfectly', 0.029), ('learn', 0.029), ('formal', 0.028), ('concentrated', 0.028), ('trained', 0.028), ('level', 0.027), ('traversing', 0.027), ('alternating', 0.027), ('llc', 0.027), ('chooses', 0.027), ('weights', 0.026), ('denominator', 0.026), ('pruning', 0.026), ('liblinear', 0.025), ('induces', 0.025), ('found', 0.025), ('efficiency', 0.025), ('examples', 0.025), ('held', 0.025), ('iu', 0.024), ('accuracies', 0.024), ('imagenet', 0.024), ('attempts', 0.024), ('test', 0.023), ('gao', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="340-tfidf-1" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>2 0.20478144 <a title="340-tfidf-2" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>Author: Fang Wang, Yi Li</p><p>Abstract: Simple tree models for articulated objects prevails in the last decade. However, it is also believed that these simple tree models are not capable of capturing large variations in many scenarios, such as human pose estimation. This paper attempts to address three questions: 1) are simple tree models sufficient? more specifically, 2) how to use tree models effectively in human pose estimation? and 3) how shall we use combined parts together with single parts efficiently? Assuming we have a set of single parts and combined parts, and the goal is to estimate a joint distribution of their locations. We surprisingly find that no latent variables are introduced in the Leeds Sport Dataset (LSP) during learning latent trees for deformable model, which aims at approximating the joint distributions of body part locations using minimal tree structure. This suggests one can straightforwardly use a mixed representation of single and combined parts to approximate their joint distribution in a simple tree model. As such, one only needs to build Visual Categories of the combined parts, and then perform inference on the learned latent tree. Our method outperformed the state of the art on the LSP, both in the scenarios when the training images are from the same dataset and from the PARSE dataset. Experiments on animal images from the VOC challenge further support our findings.</p><p>3 0.20029026 <a title="340-tfidf-3" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>4 0.14976852 <a title="340-tfidf-4" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>Author: Matthieu Guillaumin, Luc Van_Gool, Vittorio Ferrari</p><p>Abstract: Pairwise discrete energies defined over graphs are ubiquitous in computer vision. Many algorithms have been proposed to minimize such energies, often concentrating on sparse graph topologies or specialized classes of pairwise potentials. However, when the graph is fully connected and the pairwise potentials are arbitrary, the complexity of even approximate minimization algorithms such as TRW-S grows quadratically both in the number of nodes and in the number of states a node can take. Moreover, recent applications are using more and more computationally expensive pairwise potentials. These factors make it very hard to employ fully connected models. In this paper we propose a novel, generic algorithm to approximately minimize any discrete pairwise energy function. Our method exploits tractable sub-energies to filter the domain of the function. The parameters of the filter are learnt from instances of the same class of energies with good candidate solutions. Compared to existing methods, it efficiently handles fully connected graphs, with many states per node, and arbitrary pairwise potentials, which might be expensive to compute. We demonstrate experimentally on two applications that our algorithm is much more efficient than other generic minimization algorithms such as TRW-S, while returning essentially identical solutions.</p><p>5 0.14654516 <a title="340-tfidf-5" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>6 0.14558104 <a title="340-tfidf-6" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>7 0.13813567 <a title="340-tfidf-7" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>8 0.12972802 <a title="340-tfidf-8" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>9 0.12951012 <a title="340-tfidf-9" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>10 0.12698241 <a title="340-tfidf-10" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>11 0.12337229 <a title="340-tfidf-11" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>12 0.12229931 <a title="340-tfidf-12" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>13 0.1124096 <a title="340-tfidf-13" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>14 0.11173765 <a title="340-tfidf-14" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>15 0.10686615 <a title="340-tfidf-15" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>16 0.10560046 <a title="340-tfidf-16" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>17 0.10389797 <a title="340-tfidf-17" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>18 0.098046765 <a title="340-tfidf-18" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>19 0.094750002 <a title="340-tfidf-19" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>20 0.092082404 <a title="340-tfidf-20" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, -0.051), (2, -0.014), (3, 0.006), (4, 0.097), (5, 0.03), (6, 0.04), (7, 0.062), (8, -0.06), (9, -0.005), (10, -0.004), (11, 0.07), (12, -0.048), (13, 0.022), (14, -0.052), (15, 0.042), (16, -0.053), (17, -0.048), (18, 0.112), (19, -0.109), (20, -0.039), (21, 0.071), (22, -0.053), (23, 0.047), (24, -0.004), (25, 0.13), (26, 0.095), (27, 0.031), (28, 0.02), (29, 0.21), (30, -0.182), (31, 0.084), (32, 0.025), (33, -0.003), (34, 0.096), (35, 0.037), (36, -0.042), (37, -0.13), (38, 0.034), (39, -0.035), (40, -0.07), (41, 0.074), (42, -0.116), (43, 0.033), (44, 0.048), (45, 0.017), (46, -0.104), (47, 0.011), (48, 0.016), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9740026 <a title="340-lsi-1" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>2 0.78172994 <a title="340-lsi-2" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>3 0.70272559 <a title="340-lsi-3" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>4 0.69527841 <a title="340-lsi-4" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>Author: Ning Zhu, Albert C.S. Chung</p><p>Abstract: In this paper, we propose a graph-based method for 3D vessel tree structure segmentation based on a new tubularity Markov tree model ( TMT), which works as both new energy function and graph construction method. With the help of power-watershed implementation [7], a global optimal segmentation can be obtained with low computational cost. Different with other graph-based vessel segmentation methods, the proposed method does not depend on any skeleton and ROI extraction method. The classical issues of the graph-based methods, such as shrinking bias and sensitivity to seed point location, can be solved with the proposed method thanks to vessel data fidelity obtained with TMT. The proposed method is compared with some classical graph-based image segmentation methods and two up-to-date 3D vessel segmentation methods, and is demonstrated to be more accurate than these methods for 3D vessel tree segmentation. Although the segmentation is done without ROI extraction, the computational cost for the proposed method is low (within 20 seconds for 256*256*144 image).</p><p>5 0.63969827 <a title="340-lsi-5" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>6 0.57892352 <a title="340-lsi-6" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>7 0.57757163 <a title="340-lsi-7" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>8 0.57484049 <a title="340-lsi-8" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>9 0.56994659 <a title="340-lsi-9" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>10 0.5625422 <a title="340-lsi-10" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>11 0.55632514 <a title="340-lsi-11" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>12 0.53768069 <a title="340-lsi-12" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>13 0.52111667 <a title="340-lsi-13" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>14 0.51406914 <a title="340-lsi-14" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>15 0.5119614 <a title="340-lsi-15" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>16 0.48725832 <a title="340-lsi-16" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>17 0.48019201 <a title="340-lsi-17" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>18 0.47779483 <a title="340-lsi-18" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>19 0.4765406 <a title="340-lsi-19" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>20 0.47258791 <a title="340-lsi-20" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.098), (16, 0.031), (26, 0.07), (33, 0.343), (67, 0.085), (69, 0.038), (71, 0.113), (87, 0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97353542 <a title="340-lda-1" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>2 0.95820701 <a title="340-lda-2" href="./cvpr-2013-Universality_of_the_Local_Marginal_Polytope.html">448 cvpr-2013-Universality of the Local Marginal Polytope</a></p>
<p>Author: unkown-author</p><p>Abstract: We show that solving the LP relaxation of the MAP inference problem in graphical models (also known as the minsum problem, energy minimization, or weighted constraint satisfaction) is not easier than solving any LP. More precisely, any polytope is linear-time representable by a local marginal polytope and any LP can be reduced in linear time to a linear optimization (allowing infinite weights) over a local marginal polytope.</p><p>3 0.94892424 <a title="340-lda-3" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>4 0.94803238 <a title="340-lda-4" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>5 0.94775438 <a title="340-lda-5" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>Author: Peter Welinder, Max Welling, Pietro Perona</p><p>Abstract: How many labeled examples are needed to estimate a classifier’s performance on a new dataset? We study the case where data is plentiful, but labels are expensive. We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier’s confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by reestimating the class-conditional confidence distributions.</p><p>6 0.94755507 <a title="340-lda-6" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>7 0.94671458 <a title="340-lda-7" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>8 0.94593275 <a title="340-lda-8" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>9 0.94472623 <a title="340-lda-9" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>10 0.94455385 <a title="340-lda-10" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>11 0.94445813 <a title="340-lda-11" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>12 0.94347417 <a title="340-lda-12" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>13 0.94325078 <a title="340-lda-13" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>14 0.94319117 <a title="340-lda-14" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>15 0.9422226 <a title="340-lda-15" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>16 0.9420318 <a title="340-lda-16" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>17 0.94188011 <a title="340-lda-17" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>18 0.94187558 <a title="340-lda-18" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>19 0.9415319 <a title="340-lda-19" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>20 0.94145834 <a title="340-lda-20" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
