<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-444" href="#">cvpr2013-444</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</h1>
<br/><p>Source: <a title="cvpr-2013-444-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yu_Unconstrained_Monocular_3D_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>Reference: <a title="cvpr-2013-444-reference" href="../cvpr2013_reference/cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk l  Abstract This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. [sent-9, score-0.707]
</p><p>2 We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. [sent-11, score-1.355]
</p><p>3 Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. [sent-12, score-0.77]
</p><p>4 A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts. [sent-16, score-0.562]
</p><p>5 Introduction  3D human pose estimation (HPE) has been a longstanding challenge in computer vision. [sent-18, score-0.586]
</p><p>6 Contemporary methods commonly approach 3D pose estimation as a regression or a manifold learning scenario, features are embedded to a parametrised 3D pose space. [sent-20, score-1.016]
</p><p>7 Additional priors are needed to optimise a correct pose from multiple hypotheses. [sent-22, score-0.453]
</p><p>8 These priors are crucial to pose estimation, however, they also require a much controlled environment to capture compatible data: clean background segmentation, a calibrated multi-camera network, and depth sensor. [sent-23, score-0.441]
</p><p>9 Furthermore, if there are changes to the imaging environment, the whole pose estimator has to be retrained, making the pose estimation algorithm not scalable. [sent-24, score-0.885]
</p><p>10 In this paper, we present a new method that incorporates action detection and 2D part-based pose estimation techniques for realistic, video-based 3D pose estimation. [sent-25, score-1.524]
</p><p>11 Firstly, we combine action detection with 3D pose estimation to utilise the strong spatiotemporal structures of actions. [sent-27, score-1.134]
</p><p>12 The analysis of human pose and action are two closely interrelated areas in computer vision. [sent-28, score-1.047]
</p><p>13 using actions to help pose analysis, is still an aspect that many methods have overlooked. [sent-33, score-0.447]
</p><p>14 By detecting an atomic action in video, a strong 3D pose prior per each frame is obtained. [sent-35, score-0.982]
</p><p>15 In addition to kinematic constraints, action determines the temporal structure of a series of poses. [sent-36, score-0.55]
</p><p>16 For instance, in figure 1, action detection simultaneously estimates the action category and the space-time location of an action, supporting pose estimation within the action’s time span. [sent-37, score-1.646]
</p><p>17 Secondly, we apply 2D part-based pose estimation techniques to infer 3D articulated poses. [sent-39, score-0.604]
</p><p>18 The regression forests estimate joint positions in 3D from detected 2D parts, which are combined with action detection to optimise 3D pose estimation. [sent-48, score-1.43]
</p><p>19 While some methods yield a point estimate in the pose space, our approach outputs joint positions as probability distributions in 3D space. [sent-50, score-0.476]
</p><p>20 Part-based 2D pose analysis has been studied for decades, examples of early approaches include e. [sent-53, score-0.398]
</p><p>21 3D human pose estimation is more complicated than its 2D counterpart due to occlusions and high dimensionality. [sent-57, score-0.586]
</p><p>22 To estimate 3D pose from video data, various techniques have been proposed, e. [sent-58, score-0.428]
</p><p>23 Approaches for traditional 3D human pose estimation are discussed in [21]. [sent-61, score-0.586]
</p><p>24 Cross-modality approach had not been explored until [18], where 2D face and hand detectors were used to infer a simple 3D pose of upper body. [sent-62, score-0.429]
</p><p>25 More recent techniques of human pose estimation are discussed below according to the techniques or representations used. [sent-63, score-0.646]
</p><p>26 Holistic shapes, silhouettes in particular, are common features for 3D pose estimation. [sent-65, score-0.481]
</p><p>27 Several methods recognise 3D human poses from depth images, using techniques such as point cloud matching [6, 35] and random forest [30, 17]. [sent-68, score-0.407]
</p><p>28 A common approach to resolve the pose ambiguity is to maximise the field of view by capturing multiple images simultaneously using calibrated cameras, e. [sent-73, score-0.441]
</p><p>29 As mentioned previously, the integration of action and pose, in particular action for pose, is still a largely unexplored area. [sent-78, score-1.1]
</p><p>30 We seek to investigate the feasibility of using action detection to facilitate 3D human pose estimation in uncontrolled and monocular videos. [sent-79, score-1.365]
</p><p>31 Early approaches that combines action and pose constrains include [36] and [22]. [sent-80, score-0.948]
</p><p>32 The closest work to this idea is [34] that uses action recognition to assist a multiview 3D HPE algorithm. [sent-81, score-0.55]
</p><p>33 Separate regression models are trained; After an action is recognised, poses are inferred using the model of the action class estimated. [sent-82, score-1.404]
</p><p>34 Whilst action recognition is applied, the inputs are still images captured from a controlled, multi-camera environment. [sent-83, score-0.55]
</p><p>35 Instead, we perform action detection in video, by which we exploit the spatiotemporal structure of actions in addition to action class labels, to infer 3D poses. [sent-84, score-1.312]
</p><p>36 Recent advances in 2D human pose estimation methods, particularly in uncontrolled environments, have inspired a resurgence of part-based approaches for 3D pose estimation. [sent-90, score-1.015]
</p><p>37 Method Figure 3 describes the graphical model of our 3D human pose estimation framework: action detection is performed to yield the rough 3D pose estimates, then cross-modality regression forests with the estimated action classes are applied to refine the 3D pose estimations. [sent-96, score-2.801]
</p><p>38 3D coordinates of all NJ joints, are learnt in the action detection forest, one joint location is estimated per crossmodality regression forest. [sent-99, score-0.847]
</p><p>39 Part-based Feature Extraction  ×  In order to perform 3D pose estimation in different backgrounds and scenarios, input features are extracted using a 2D part-based model. [sent-104, score-0.487]
</p><p>40 A 3D human pose is represented by the scale-normalised coordinates of the NJ joints detected by the Kinect sensor, an articulated model of NJ = 15 joints is used in this work. [sent-112, score-0.692]
</p><p>41 Every feature vector in the training dataset is assigned to the 3D pose detected and one of the C action categories, according to its corresponding frame and video. [sent-113, score-1.012]
</p><p>42 The corresponding class label and corresponding 3D pose are defined as A =  {apqr |apqr n∈d 1 c,o . [sent-116, score-0.433]
</p><p>43 The action detection forest, D, performs action categorisation ea andct o3Dn pose clustering Dsi-, multaneously. [sent-127, score-1.557]
</p><p>44 In each leaf node, the 3D pose vectors U are lctlaansesoiufiesdly . [sent-128, score-0.398]
</p><p>45 Lastly, every leave node stores the votes that are required in Hough-voting for action detection during testing. [sent-134, score-0.703]
</p><p>46 (1)  The first term, Ha(·), is the information gain measure used in standard classif(ic·)at,io isn t hfeor einsfto [r7m] a(htieonre gfoairn na cmtieoansu ucrleas ussifeidcation); the second term measures the improvement in 3D pose coherence when a split is performed ? [sent-136, score-0.433]
</p><p>47 1  where Ψ(·) = log(det(·)) and Σ(Unc) is the covariance wmahetrriex Ψof( ·th)e PCA-compressed d3 DΣ pose vectors Unc of the mn-tahtr inxo dofe h(le, r CdeAn-octoem thpere lseseftd a 3nDd right split respectively) and the c-th action class. [sent-140, score-0.983]
</p><p>48 These measures are weighted by ω that describes the class purity of a node as ω  = max(|Anc |/|An |) − min(|Anc |/|An |) , c  (3)  ci  where An denotes the action labels of training data in node n haenrde AAnc the action labels of node n and class c. [sent-141, score-1.338]
</p><p>49 The nfirs atn quality term in (1) optimises action classification performance while the second term optimises pose clustering performance within a node. [sent-142, score-1.076]
</p><p>50 Once the learning is completed, the class posterior of a leaf node is obtained by:  ˆn  P(a = c|ˆ n) = |Aˆ nc|/|A nˆ |  (4)  The distribution of 3D poses, given a action class-label c, is modelled by a Gaussian N(μ(Uˆ nc) , Σ(U nˆc)). [sent-145, score-0.682]
</p><p>51 X Split fwuhnecrtieon Y canidsid tahete sj are generated cino othrdei same way as action detection using the feature vector x. [sent-160, score-0.609]
</p><p>52 Although action class posteriors can be computed in the terminal nodes, it is not modelled in this method as the action detection forest D provides a better action recognition rate that helps the localisation accuracies of R(j). [sent-161, score-2.012]
</p><p>53 Testing  μ(Yˆ( ncj)  Video snippet, the basic unit required for action detection [25], is a short sequence excerpted from the testing video, centere? [sent-167, score-0.65]
</p><p>54 Action detection forest D performs action classification on St. [sent-193, score-0.706]
</p><p>55 The posterior of snippet action class at ntim thee et k i-st hde tfrienee odf as  P(a = c|St,D) =Nk,D? [sent-195, score-0.689]
</p><p>56 A Hough-based voting scheme is designed for action detection. [sent-198, score-0.55]
</p><p>57 Hence, all frames I can vote for a 3D pose at time t, by applying temporal offsets δ to the votes obtained from XSt . [sent-203, score-0.471]
</p><p>58 n eAd 3frDo pose αt is hence modelled by NJ independent Gaussians with respect to its joints. [sent-222, score-0.439]
</p><p>59 Estimation of current 3D pose by the regression forest, βt, is performed on perframe basis. [sent-225, score-0.529]
</p><p>60 , Nh}, the set of pose estimates for class c {isx ret|uir n=ed 1 by t. [sent-229, score-0.433]
</p><p>61 Three-dimensional human poses are estimated globally via action detection, and locally by the joint regression forests. [sent-237, score-0.996]
</p><p>62 Existing public 3D pose datasets are inadequate to justify the main objectives of the proposed approach. [sent-250, score-0.428]
</p><p>63 In the first part, pose estimation accuracy was evaluated quantitatively with ground truth and current state-of-the-arts, in 3D and 2D respectively. [sent-262, score-0.487]
</p><p>64 elbowRhandRhip kneRfoAtverag Figure 6: 3D joint localisation errors based on ground truth pose from Kinect sensor Quantitative Evaluation. [sent-271, score-0.682]
</p><p>65 9527ia4vfdcbweloanvpxcdea21tionby  (left) action detection forest, and (right) cross-modality regression forest 333666444644  Er1342o0 r(Hpeixadls)NeckTorsLhou. [sent-291, score-0.837]
</p><p>66 Besides the combined pose estimation Θ, we also evaluated each of the forests alone, and compared it with the latest 2D HPE algorithms [9] and [33]. [sent-318, score-0.616]
</p><p>67 In order to cope with actions performed in different speeds, testing videos are preprocessed by normalising with respect to their action speeds estimated from the first 25 frames of the videos. [sent-319, score-0.669]
</p><p>68 The action detection for-  est achieves excellent accuracy, as it has been optimised for classification during learning, the video-based input, snippet, also provides temporal cues that improve classification. [sent-322, score-0.609]
</p><p>69 The proposed framework showed promising results, by extending the flexibility of [33], the proposed method showed high robustness in 3D pose estimation and outperformed both state-of-the-arts in the 2D tests. [sent-327, score-0.526]
</p><p>70 While some classes reported significant improvements after combining the results of action detection and pose regression, e. [sent-330, score-1.007]
</p><p>71 The KTH [1] and Weizmann [2] dataset were used in the experiments as they shared action categories with the APE dataset. [sent-339, score-0.55]
</p><p>72 The experimental results have demonstrated high feasibility in the idea of using action detection to estimate 3D poses under challenging conditions. [sent-346, score-0.821]
</p><p>73 Coupling the outputs from the random forests, the 3D pose estimation accuracy is further enhanced. [sent-347, score-0.487]
</p><p>74 Red ellipsoids represent the confidence region of pose estimation, Λ, in equation (13). [sent-350, score-0.438]
</p><p>75 Sample (o) and (p) shows the wrong pose estimations when the 2D body part detector fails. [sent-351, score-0.456]
</p><p>76 a global pose estimation and the corresponding class label. [sent-352, score-0.522]
</p><p>77 Meanwhile, errors in the initial global estimation, due to the differences among individual action patterns, are corrected locally by regression forests, which improves the accuracy of the final pose estimation as shown in figure 6. [sent-353, score-1.202]
</p><p>78 Conclusions The challenging problem of 3D human pose estimation is discussed in this paper. [sent-367, score-0.586]
</p><p>79 While traditional methods for 3D human pose estimation emphasise accuracy over their compatibility with realistic applications, we present a novel practical approach without using any scene-dependent constraints. [sent-368, score-0.586]
</p><p>80 We investigate the new area of using action for pose estimation. [sent-369, score-0.948]
</p><p>81 The proposed method combines human action detection and deformable part model-based 2D human pose estimation to estimate 3D poses from unconstrained, monocular videos. [sent-370, score-1.534]
</p><p>82 We suggest that the collaboration between the techniques in human action and pose analysis will be beneficial to both areas of computer vision research in the coming future. [sent-374, score-1.077]
</p><p>83 Pictorial structures revisited: People detection and articulated pose estimation. [sent-399, score-0.513]
</p><p>84 A datadriven approach for real-time full body pose reconstruction from a depth camera. [sent-414, score-0.499]
</p><p>85 2d articulated human pose estimation and retrieval in (almost) unconstrained still images. [sent-433, score-0.702]
</p><p>86 Inferring 3d body pose from silhouettes using activity manifold learning. [sent-439, score-0.539]
</p><p>87 Estimating human shape and pose from a single image. [sent-474, score-0.497]
</p><p>88 Real-time upper body detection and 3d pose estimation in monoscopic images. [sent-500, score-0.604]
</p><p>89 Semi-supervised learning of joint density models for human pose estimation. [sent-507, score-0.575]
</p><p>90 Joint pose estimation and action recognition in image graphs. [sent-533, score-1.037]
</p><p>91 Fast human pose detection using randomized hierarchical cascades of rejectors. [sent-546, score-0.556]
</p><p>92 Action snippets: How many frames does human action recognition require? [sent-551, score-0.649]
</p><p>93 Loose-limbed people: Estimating 3d human pose and motion using non-parametric belief propagation. [sent-567, score-0.497]
</p><p>94 Single image 3d human pose estimation from noisy observations. [sent-575, score-0.586]
</p><p>95 An efficient branchand-bound algorithm for optimal human pose estimation. [sent-582, score-0.497]
</p><p>96 The vitruvian manifold: Inferring dense correspondences for one-shot human pose estimation. [sent-590, score-0.497]
</p><p>97 Mining actionlet ensemble for action recognition with depth cameras. [sent-597, score-0.593]
</p><p>98 Coupled action recognition and pose estimation from multiple views. [sent-614, score-1.037]
</p><p>99 Accurate 3d pose estimation from a single depth image. [sent-622, score-0.53]
</p><p>100 Real-time action recognition by spatiotemporal semantic and structural forest. [sent-630, score-0.588]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('action', 0.55), ('pose', 0.398), ('ape', 0.214), ('hpe', 0.186), ('poses', 0.138), ('regression', 0.131), ('localisation', 0.13), ('forests', 0.129), ('dpm', 0.11), ('snippet', 0.104), ('human', 0.099), ('forest', 0.097), ('estimation', 0.089), ('silhouettes', 0.083), ('nj', 0.081), ('evaw', 0.078), ('upqr', 0.078), ('joint', 0.078), ('feasibility', 0.074), ('st', 0.073), ('kinect', 0.067), ('jc', 0.065), ('monocular', 0.065), ('optimises', 0.064), ('uk', 0.061), ('unconstrained', 0.06), ('detection', 0.059), ('unc', 0.058), ('imperial', 0.058), ('body', 0.058), ('pictorial', 0.057), ('articulated', 0.056), ('xij', 0.056), ('node', 0.056), ('optimise', 0.055), ('baak', 0.052), ('forestours', 0.052), ('snippets', 0.052), ('tcj', 0.052), ('tjt', 0.052), ('xpqr', 0.052), ('xst', 0.052), ('ypqr', 0.052), ('actions', 0.049), ('hp', 0.049), ('wave', 0.048), ('weizmann', 0.047), ('un', 0.047), ('apqr', 0.046), ('nise', 0.046), ('ac', 0.045), ('depth', 0.043), ('balan', 0.043), ('anc', 0.043), ('maximise', 0.043), ('sensor', 0.042), ('nh', 0.042), ('testing', 0.041), ('modelled', 0.041), ('ellipsoids', 0.04), ('pco', 0.04), ('kinematics', 0.04), ('joints', 0.04), ('flexibility', 0.039), ('recognising', 0.038), ('kth', 0.038), ('holistic', 0.038), ('parts', 0.038), ('spatiotemporal', 0.038), ('unseen', 0.038), ('votes', 0.038), ('environments', 0.038), ('deformable', 0.037), ('cam', 0.037), ('split', 0.035), ('class', 0.035), ('vote', 0.035), ('xti', 0.034), ('uller', 0.034), ('bend', 0.034), ('errors', 0.034), ('nc', 0.034), ('cambridge', 0.034), ('frame', 0.034), ('seidel', 0.033), ('tthioen', 0.032), ('infer', 0.031), ('uncontrolled', 0.031), ('normalised', 0.031), ('kim', 0.031), ('ha', 0.031), ('techniques', 0.03), ('sigal', 0.03), ('whilst', 0.03), ('justify', 0.03), ('detected', 0.03), ('videos', 0.029), ('coordinates', 0.029), ('gall', 0.028), ('acquire', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="444-tfidf-1" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>2 0.46717244 <a title="444-tfidf-2" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>3 0.36608607 <a title="444-tfidf-3" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>4 0.32550254 <a title="444-tfidf-4" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>Author: Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</p><p>Abstract: How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What defines these spatiotemporal patches is their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore, these patches can be used as a discriminative vocabulary for action classification where they demonstrate stateof-the-art performance on UCF50 and Olympics datasets.</p><p>5 0.30879226 <a title="444-tfidf-5" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>Author: Chao-Yeh Chen, Kristen Grauman</p><p>Abstract: We propose an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process. Using unlabeled video containing various human activities, the system first learns how body pose tends to change locally in time. Then, given a small number of labeled static images, it uses that model to extrapolate beyond the given exemplars and generate “synthetic ” training examples—poses that could link the observed images and/or immediately precede or follow them in time. In this way, we expand the training set without requiring additional manually labeled examples. We explore both example-based and manifold-based methods to implement our idea. Applying our approach to recognize actions in both images and video, we show it enhances a state-of-the-art technique when very few labeled training examples are available.</p><p>6 0.29758632 <a title="444-tfidf-6" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>7 0.27644655 <a title="444-tfidf-7" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>8 0.2764329 <a title="444-tfidf-8" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>9 0.26451296 <a title="444-tfidf-9" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>10 0.26250696 <a title="444-tfidf-10" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>11 0.26015204 <a title="444-tfidf-11" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>12 0.25719506 <a title="444-tfidf-12" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>13 0.25144967 <a title="444-tfidf-13" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>14 0.2272476 <a title="444-tfidf-14" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>15 0.22237794 <a title="444-tfidf-15" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>16 0.20870082 <a title="444-tfidf-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.18544263 <a title="444-tfidf-17" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>18 0.18470289 <a title="444-tfidf-18" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>19 0.18159749 <a title="444-tfidf-19" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>20 0.17479014 <a title="444-tfidf-20" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.303), (1, -0.075), (2, -0.013), (3, -0.299), (4, -0.352), (5, -0.041), (6, 0.013), (7, 0.147), (8, 0.017), (9, -0.212), (10, -0.12), (11, 0.169), (12, -0.115), (13, 0.096), (14, -0.113), (15, -0.003), (16, -0.044), (17, -0.087), (18, -0.003), (19, 0.137), (20, 0.041), (21, -0.009), (22, -0.11), (23, 0.091), (24, -0.035), (25, -0.05), (26, -0.021), (27, -0.014), (28, 0.033), (29, 0.008), (30, 0.005), (31, -0.005), (32, -0.077), (33, 0.005), (34, -0.007), (35, 0.044), (36, 0.038), (37, -0.027), (38, 0.067), (39, 0.033), (40, -0.032), (41, 0.003), (42, -0.025), (43, -0.008), (44, 0.035), (45, 0.061), (46, -0.022), (47, 0.022), (48, 0.021), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96657133 <a title="444-lsi-1" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>2 0.90848649 <a title="444-lsi-2" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>3 0.83467138 <a title="444-lsi-3" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>Author: Michalis Raptis, Leonid Sigal</p><p>Abstract: In this paper, we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework, where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and minefor hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive with the state of the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.</p><p>4 0.77713829 <a title="444-lsi-4" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>5 0.76391941 <a title="444-lsi-5" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>6 0.75336564 <a title="444-lsi-6" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>7 0.69961929 <a title="444-lsi-7" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>8 0.67328721 <a title="444-lsi-8" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>9 0.66437924 <a title="444-lsi-9" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>10 0.63669789 <a title="444-lsi-10" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>11 0.62346166 <a title="444-lsi-11" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>12 0.6094501 <a title="444-lsi-12" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>13 0.60501838 <a title="444-lsi-13" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>14 0.60087878 <a title="444-lsi-14" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>15 0.59527361 <a title="444-lsi-15" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>16 0.58939564 <a title="444-lsi-16" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>17 0.58834743 <a title="444-lsi-17" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>18 0.58550447 <a title="444-lsi-18" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>19 0.583709 <a title="444-lsi-19" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>20 0.58214045 <a title="444-lsi-20" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.188), (10, 0.127), (16, 0.024), (26, 0.034), (33, 0.291), (39, 0.015), (40, 0.014), (67, 0.098), (69, 0.035), (80, 0.017), (87, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90071034 <a title="444-lda-1" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>2 0.88936692 <a title="444-lda-2" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>Author: Kevin Tang, Rahul Sukthankar, Jay Yagnik, Li Fei-Fei</p><p>Abstract: The ubiquitous availability of Internet video offers the vision community the exciting opportunity to directly learn localized visual concepts from real-world imagery. Unfortunately, most such attempts are doomed because traditional approaches are ill-suited, both in terms of their computational characteristics and their inability to robustly contend with the label noise that plagues uncurated Internet content. We present CRANE, a weakly supervised algorithm that is specifically designed to learn under such conditions. First, we exploit the asymmetric availability of real-world training data, where small numbers of positive videos tagged with the concept are supplemented with large quantities of unreliable negative data. Second, we ensure that CRANE is robust to label noise, both in terms of tagged videos that fail to contain the concept as well as occasional negative videos that do. Finally, CRANE is highly parallelizable, making it practical to deploy at large scale without sacrificing the quality of the learned solution. Although CRANE is general, this paper focuses on segment annotation, where we show state-of-the-art pixel-level segmentation results on two datasets, one of which includes a training set of spatiotemporal segments from more than 20,000 videos.</p><p>3 0.88671398 <a title="444-lda-3" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><p>4 0.87378883 <a title="444-lda-4" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>Author: Michele Fenzi, Laura Leal-Taixé, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models, i.e., regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset [17] and the Pointing’04 face dataset [8]. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.</p><p>5 0.87192416 <a title="444-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.87169158 <a title="444-lda-6" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>7 0.86935651 <a title="444-lda-7" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>8 0.86889172 <a title="444-lda-8" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>9 0.86872822 <a title="444-lda-9" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>10 0.86729944 <a title="444-lda-10" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>11 0.86722034 <a title="444-lda-11" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>12 0.86668497 <a title="444-lda-12" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>13 0.8666172 <a title="444-lda-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.86659545 <a title="444-lda-14" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>15 0.86612362 <a title="444-lda-15" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>16 0.86590326 <a title="444-lda-16" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>17 0.8656984 <a title="444-lda-17" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>18 0.86541629 <a title="444-lda-18" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>19 0.86463314 <a title="444-lda-19" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>20 0.86461425 <a title="444-lda-20" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
