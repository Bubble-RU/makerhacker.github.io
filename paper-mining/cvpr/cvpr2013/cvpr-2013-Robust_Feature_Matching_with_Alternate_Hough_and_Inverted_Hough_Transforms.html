<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-361" href="#">cvpr2013-361</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</h1>
<br/><p>Source: <a title="cvpr-2013-361-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Robust_Feature_Matching_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>Reference: <a title="cvpr-2013-361-reference" href="../cvpr2013_reference/cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. [sent-2, score-0.418]
</p><p>2 Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. [sent-3, score-0.722]
</p><p>3 In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. [sent-4, score-0.603]
</p><p>4 On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. [sent-5, score-0.611]
</p><p>5 Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. [sent-8, score-0.46]
</p><p>6 Introduction Establishing correspondences among two or more images has attracted great attention in the field of computer vision. [sent-11, score-0.361]
</p><p>7 Despite the great applicability, at least two difficulties hinder the advance in establishing correspondences of high quality. [sent-13, score-0.412]
</p><p>8 (Top) We project correspondences into the transformation space, and distinguish correct (red) and wrong (black) correspondences by their densities. [sent-18, score-0.929]
</p><p>9 (Bottom) Potential (green) correspondences are incrementally inferred by exploring density distributions of transformations grouped by BPLRs [17]. [sent-19, score-0.645]
</p><p>10 They often work on a pre-selected, small subset of correspondence candidates, and result in low recall. [sent-22, score-0.147]
</p><p>11 Our approach is developed upon the insight that nearby features on the same object typically share similar homographies if they are matched correctly. [sent-24, score-0.192]
</p><p>12 It follows that their homographies tend to gather together in the transformation space. [sent-25, score-0.205]
</p><p>13 Besides, each wrong matching is usually wrong in its own way. [sent-26, score-0.133]
</p><p>14 It implies that the density of each correspondence in the transformation space can verify its correctness. [sent-27, score-0.304]
</p><p>15 We leverage this property and cast the task of feature matching into a density estimation problem. [sent-28, score-0.225]
</p><p>16 Specifically, we identify correct correspondences by comparing the densities among mutually exclusive correspon-  dences, i. [sent-29, score-0.473]
</p><p>17 On the other hand, it is allowed to dynamically recommend potential correspondences by exploring the density distributions of locally grouped features. [sent-32, score-0.619]
</p><p>18 The proposed approach carries out Hough transform and inverted Hough transform alternately to establish robust feature correspondences. [sent-34, score-0.455]
</p><p>19 First, every correspondence candidate is projected into the Hough space 222777666200  spanned by the transformations. [sent-36, score-0.172]
</p><p>20 Only the correspondences associated to features within the same BPLR are considered in Hough voting. [sent-38, score-0.361]
</p><p>21 In this way, mutual verification with relevant correspondences boosts the precision of matching. [sent-39, score-0.566]
</p><p>22 Furthermore, it makes the complexity of geometric checking independent to the number of correspondences, and leads to one order speed-up in matching. [sent-40, score-0.072]
</p><p>23 The inverted Hough transform recommends each feature additional transformations by investigating density distributions of nearby features covered by the same BPLR. [sent-42, score-0.631]
</p><p>24 These transformations enable the dynamical construction of potential correspondences. [sent-43, score-0.111]
</p><p>25 It allows relevant features to propagate their transformations to each other until consistency is reached. [sent-44, score-0.174]
</p><p>26 Related Work The literature of feature correspondence is quite extensive. [sent-49, score-0.19]
</p><p>27 Our review focuses on those that are relevant to the development of the proposed approach. [sent-50, score-0.057]
</p><p>28 Point-to-point matching with local feature descriptors is a principal way for correspondence problems. [sent-52, score-0.257]
</p><p>29 One way to address matching ambiguity with additional geometric checking is to cast feature correspondence as a graph matching problem. [sent-58, score-0.438]
</p><p>30 By defining an objective function based on both photometric similarity and pairwise geometric compatibility between correspondences, promising results via graph matching have been demonstrated [11, 12, 18, 33]. [sent-59, score-0.096]
</p><p>31 As mentioned in [20], graph matching is sensitive to corrupt cor-  respondences and outliers. [sent-61, score-0.09]
</p><p>32 Research efforts on clusteringbased mechanisms have been made to handle unconstrained matching cases. [sent-64, score-0.067]
</p><p>33 Bottom-up clustering can integrate locally adaptive constraints to aggregates coherent bundles of matches. [sent-65, score-0.078]
</p><p>34 [8] carry out object-based image matching via hierarchical agglomerative clustering. [sent-67, score-0.067]
</p><p>35 RANSAC [14], a geometric verification model, can be incorporated with local descriptors to enhance the performance. [sent-75, score-0.116]
</p><p>36 [32] treat each correspondence as a voter, and maintain an affinity matrix to encode how these correspondences vote each other according to their compatibilities. [sent-77, score-0.508]
</p><p>37 Tolias and Avrithis [26] offer a variant of Hough transform for multi-object matching. [sent-79, score-0.096]
</p><p>38 They rank the correspondences by adopting the mech-  anism of pyramid match [15]. [sent-80, score-0.361]
</p><p>39 Their method evenly quantizes the transformation space for fast matching. [sent-81, score-0.084]
</p><p>40 However, the transformations of correct correspondences often distribute irregularly. [sent-82, score-0.513]
</p><p>41 Our approach is a voting-based system, and can be distinguished by the advantage that the complexity of Hough voting for each feature is independent to the number of correspondences. [sent-84, score-0.153]
</p><p>42 Furthermore, it dynamically enriches correspondences, and overcomes the low recall problem caused by working on a pre-selected, small subset of initial correspondences. [sent-85, score-0.065]
</p><p>43 Most feature correspondence methods work with a small subset of pre-selected correspondences. [sent-87, score-0.19]
</p><p>44 , [11, 13], propagate individual matches to nearby regions based on local appearance, but their performances heavily depend on the quality of initial matching. [sent-91, score-0.111]
</p><p>45 [7] develop a region-growing algorithm to distinguish correct and incorrect correspondences. [sent-93, score-0.118]
</p><p>46 [10] instead describe a progressive graph matching framework to enrich initial matching. [sent-95, score-0.139]
</p><p>47 However, the yielded correspondences by their approach are biased to the density of features, and may be noisy due to diverse feature distributions in the two matched images. [sent-96, score-0.525]
</p><p>48 In contrast, our method works on feature bundles guided by BPLRs, so the concerted transformations with high probability are transferred  × ×  through mutually relevant features. [sent-97, score-0.424]
</p><p>49 It turns out that the information can be propagated more efficiently and the resulting candidates of correspondences are much more targeted. [sent-98, score-0.447]
</p><p>50 {Tvhe} region and the= =ce {nvter} }of feature vi ∈ VP ∪ VQ are denoted by Si and xi, respectively. [sent-101, score-0.102]
</p><p>51 The appearance of vi is described by feature vector ui, and its orientation θi is estimated by a dominant orientation in the gradient histogram [23]. [sent-102, score-0.102]
</p><p>52 O Vur goal is to find as many as possible correct correspondences in C. [sent-104, score-0.427]
</p><p>53 Transformation space  The local shape and the position of feature vi can be described by a 3 3 matrix T(vi), which specifies an affine strcarnibsefodrm by yo af vi ×w 3ith m regards (tov the normalized patch [23]:  T(vi) =? [sent-107, score-0.161]
</p><p>54 Given a feature pair viP ∈ VP and ∈ VQ, the relative transformation Hii? [sent-112, score-0.127]
</p><p>55 can be considered as a point in the 6-dimensional transformation space. [sent-125, score-0.084]
</p><p>56 projects xjP around For a pair of correspondences mii? [sent-152, score-0.361]
</p><p>57 , they are considered compatible if the corresponding homographies are similar. [sent-154, score-0.146]
</p><p>58 (5)  Note that it is symmetric and is used to compute the dis-  tances among correspondences in the transformation space. [sent-168, score-0.47]
</p><p>59 deon ce C o r espondenc e H omography HHoough transform for homography  verification  transform for correspondence recommendation Figure 2. [sent-171, score-0.543]
</p><p>60 The Proposed Approach Features with compatible geometric configurations are mutually dependent in matching. [sent-174, score-0.1]
</p><p>61 We investigate feature dependence via BPLR detector [17], and cast feature matching as a density estimation problem. [sent-175, score-0.268]
</p><p>62 The proposed approach carries out this idea by alternate Hough and inverted Hough voting. [sent-176, score-0.262]
</p><p>63 While the former discovers the consistent homographies by projecting correspondences into the transformation space, the latter incrementally recommends potential correspondences driven by the concerted homographies. [sent-177, score-1.212]
</p><p>64 Then the Hough and inverted Hough transforms for feature matching are introduced, respectively. [sent-180, score-0.279]
</p><p>65 Initial correspondence candidates Our approach starts from the construction of initial correspondence candidates. [sent-183, score-0.372]
</p><p>66 For each feature viP ∈ IP, we find  {vQik}rk=1  ×  its r potential matchings in IQ according to their appearance similarity agnsd { vwit}h the constraint that none of the r matchings highly overlap. [sent-184, score-0.219]
</p><p>67 c Wtioitnh d {iv iQikd}ekrd= b1y, the set of initial correspondences associated with viP is Mi  = {miik = (vPi,vQik,Hiik)}kr=1,  (6)  viQk. [sent-191, score-0.394]
</p><p>68 where Hiik is the relative transformation from viP to This process is repeated for each feature in IP. [sent-192, score-0.127]
</p><p>69 Then the set of initial correspondences is constructed by N? [sent-193, score-0.394]
</p><p>70 It contains many corrupted matchings zsien |cMe |th =ere r e ×xi Nsts at most one correct correspondence in each Mi. [sent-199, score-0.301]
</p><p>71 In complex matching ctaosrkrse,c itt ciso usually dtehen case th eaact only a small subset of correct correspondences in C is included in M. [sent-200, score-0.519]
</p><p>72 e Ethme precision woef correspondences decreases rapidly when r is larger than 5. [sent-202, score-0.417]
</p><p>73 207 out of 222 correct correspondences in M are identified via Hough  (contours)  voting. [sent-207, score-0.427]
</p><p>74 e (sb d) Henoouteg hth veo ctoinrgre acnt correspondences  wdeitthec StIeFdT by 0b7ot hou approaches. [sent-209, score-0.361]
</p><p>75 by only Hough voting and the nearest SIFT searching, respectively. [sent-210, score-0.11]
</p><p>76 bcayn odnildyat Heso  augndh vleoatdins gto a anddd thiteio nneaal 1e3st0 S  (= T3 s3e7a − 207)  eRcetd c aonrrde cyan leinncese are Mthe caorerr iedcetn correspondences  (c) Inverted Hough voting. [sent-211, score-0.361]
</p><p>77 It recommends  rceosrpreecctti correspondences  147  (= 369 −222)  correct  (green lines) gd. [sent-212, score-0.52]
</p><p>78 Hough transform for homography verification The goal at this stage is to detect the correct correspondences in M, which is either the initial correspondence sdeetn or sth ien e Mnri,ch wehdi shet by itthhee following stage. [sent-216, score-0.863]
</p><p>79 s Wpoen idnevnecsetigate the property that the transformations of correct correspondences are concerted while those of incorrect correspondences are different in their own ways. [sent-217, score-1.044]
</p><p>80 Hough voting for homography verification is employed since it can handle a high percentage of incorrect correspondences and detect correct correspondences via density estimation. [sent-218, score-1.159]
</p><p>81 Specifically, the relative transformation of each correspondence is treated as a point in Hough space, and it is considered as a hypothesis about the underlying homography of interest. [sent-219, score-0.304]
</p><p>82 Despite its robustness, Hough transform is developed upon the assumption that the hypotheses are a sum of independent votes, and thereby neglects the spatial dependence among features. [sent-220, score-0.096]
</p><p>83 As pointed out in [3 1], choosing proper voters is critical in Hough transform, especially when voters are dependent. [sent-221, score-0.17]
</p><p>84 We are inspired by the fact that nearby features on the same object are mutually dependent, and group relevant correspondences via BPLR detector [17], which re-  spects object boundary and captures the local shape of an object. [sent-222, score-0.511]
</p><p>85 It turns out that the performance of Hough voting is remarkably boosted. [sent-223, score-0.11]
</p><p>86 Furthermore, only relevant, small-size correspondences are involved in density estimation, instead of the whole M. [sent-224, score-0.434]
</p><p>87 We then cluster features relevant to viP by checking if they reside in at least one common BPLR, i. [sent-233, score-0.1]
</p><p>88 (8)  We assume that the grouped features with high probability undergo similar transformations in matching. [sent-236, score-0.134]
</p><p>89 It follows that the correspondences relevant to viP in Hough voting can be collected by R(viP) =  ? [sent-237, score-0.528]
</p><p>90 (6), there exists at most one correct correspondence in Mi. [sent-241, score-0.213]
</p><p>91 Hough voting as well as voters R(viP) are adopted eto in pick the most plausible correspondence associated with feature viP. [sent-242, score-0.418]
</p><p>92 Specifically, it is accomplished by normalized kernel density estimation (KDE):  mi∗i? [sent-243, score-0.102]
</p><p>93 (10), zbautti oitn i tse required ivn comparing d aefnfescittie thse across feature points. [sent-249, score-0.068]
</p><p>94 The procedure of correspondence selection is repeated for each feature in image IP. [sent-250, score-0.19]
</p><p>95 It results in NP selected correspondences M∗ = We then sort them according oton dtheneicre ass Msocia=te {d mdens}ities in Eq. [sent-251, score-0.399]
</p><p>96 (10), and return the top correspondences by a proper threshold. [sent-252, score-0.361]
</p><p>97 An example of the verification results by Hough voting is shown in Figure 3b. [sent-254, score-0.197]
</p><p>98 While Hough transform identifies correct correspondences M∗ M and boosts the precision in matching, tdheen goal Mof in⊆ve Mrted a Hough ttrsan thsefo rpmre ciiss oton e innri mcha Mchi so tthhaet gtohea lroe cfal inl can bde Hinocuregahs tedra. [sent-259, score-0.67]
</p><p>99 sTfoherm locally eclnursitcehre Md fe saotures by BPLRs have consensus transformations and can assist each other in finding plausible correspondences. [sent-260, score-0.147]
</p><p>100 We investigate this property and develop the inverted Hough transform, which allows grouped features to propagate their homographies to each other and recommends each feature  ⊆  concerted correspondences by exploring the propagated homographies. [sent-261, score-1.077]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hough', 0.508), ('vip', 0.397), ('correspondences', 0.361), ('bplr', 0.171), ('bplrs', 0.171), ('hii', 0.171), ('inverted', 0.169), ('correspondence', 0.147), ('concerted', 0.142), ('homographies', 0.121), ('viq', 0.114), ('voting', 0.11), ('mii', 0.11), ('transform', 0.096), ('recommends', 0.093), ('verification', 0.087), ('transformations', 0.086), ('mjj', 0.085), ('voters', 0.085), ('vpj', 0.085), ('transformation', 0.084), ('vq', 0.083), ('homography', 0.073), ('density', 0.073), ('vp', 0.069), ('matching', 0.067), ('correct', 0.066), ('matchings', 0.063), ('vi', 0.059), ('xjq', 0.057), ('relevant', 0.057), ('carries', 0.051), ('bundles', 0.05), ('djj', 0.05), ('iq', 0.049), ('grouped', 0.048), ('nearby', 0.047), ('vpi', 0.047), ('mutually', 0.046), ('candidates', 0.045), ('taiwan', 0.044), ('recommendation', 0.044), ('feature', 0.043), ('checking', 0.043), ('alternate', 0.042), ('cast', 0.042), ('propagated', 0.041), ('enrich', 0.039), ('oton', 0.038), ('ip', 0.034), ('plausible', 0.033), ('initial', 0.033), ('wrong', 0.033), ('dynamically', 0.032), ('precision', 0.031), ('propagate', 0.031), ('boosts', 0.03), ('mi', 0.03), ('accomplished', 0.029), ('geometric', 0.029), ('establishing', 0.028), ('locally', 0.028), ('incorrect', 0.028), ('exploring', 0.028), ('cho', 0.025), ('compatible', 0.025), ('spanned', 0.025), ('tuhree', 0.025), ('extracte', 0.025), ('tances', 0.025), ('gbn', 0.025), ('mcha', 0.025), ('wtioitnh', 0.025), ('tthea', 0.025), ('oitn', 0.025), ('cme', 0.025), ('dtehen', 0.025), ('enrichment', 0.025), ('nsts', 0.025), ('stfoherm', 0.025), ('agnsd', 0.025), ('manifest', 0.025), ('sinica', 0.025), ('voter', 0.025), ('vqi', 0.025), ('woef', 0.025), ('yacov', 0.025), ('potential', 0.025), ('corrupted', 0.025), ('incrementally', 0.025), ('distinguish', 0.024), ('matched', 0.024), ('distributions', 0.024), ('tdheen', 0.023), ('respondences', 0.023), ('hai', 0.023), ('toor', 0.023), ('sip', 0.023), ('hinder', 0.023), ('cech', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="361-tfidf-1" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>2 0.34294868 <a title="361-tfidf-2" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>Author: Tao Wang, Xuming He, Nick Barnes</p><p>Abstract: Wepropose a structuredHough voting methodfor detecting objects with heavy occlusion in indoor environments. First, we extend the Hough hypothesis space to include both object location and its visibility pattern, and design a new score function that accumulates votes for object detection and occlusion prediction. In addition, we explore the correlation between objects and their environment, building a depth-encoded object-context model based on RGB-D data. Particularly, we design a layered context representation and .barne s }@ nict a . com .au (a)(b)(c) (d)(e)(f) allow image patches from both objects and backgrounds voting for the object hypotheses. We demonstrate that using a data-driven 2.1D representation we can learn visual codebooks with better quality, and more interpretable detection results in terms of spatial relationship between objects and viewer. We test our algorithm on two challenging RGB-D datasets with significant occlusion and intraclass variation, and demonstrate the superior performance of our method.</p><p>3 0.23091722 <a title="361-tfidf-3" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>Author: Qiang Hao, Rui Cai, Zhiwei Li, Lei Zhang, Yanwei Pang, Feng Wu, Yong Rui</p><p>Abstract: 3D model-based object recognition has been a noticeable research trend in recent years. Common methods find 2D-to-3D correspondences and make recognition decisions by pose estimation, whose efficiency usually suffers from noisy correspondences caused by the increasing number of target objects. To overcome this scalability bottleneck, we propose an efficient 2D-to-3D correspondence filtering approach, which combines a light-weight neighborhoodbased step with a finer-grained pairwise step to remove spurious correspondences based on 2D/3D geometric cues. On a dataset of 300 3D objects, our solution achieves ∼10 times speed improvement over the baseline, with a comparable recognition accuracy. A parallel implementation on a quad-core CPU can run at ∼3fps for 1280× 720 images.</p><p>4 0.17729081 <a title="361-tfidf-4" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>5 0.10844426 <a title="361-tfidf-5" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>6 0.10724881 <a title="361-tfidf-6" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>7 0.10265818 <a title="361-tfidf-7" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>8 0.099719323 <a title="361-tfidf-8" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>9 0.09787894 <a title="361-tfidf-9" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>10 0.091940865 <a title="361-tfidf-10" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>11 0.075906716 <a title="361-tfidf-11" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>12 0.075466931 <a title="361-tfidf-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.070161872 <a title="361-tfidf-13" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>14 0.070160821 <a title="361-tfidf-14" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>15 0.069965757 <a title="361-tfidf-15" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>16 0.06915395 <a title="361-tfidf-16" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>17 0.067204297 <a title="361-tfidf-17" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>18 0.064702563 <a title="361-tfidf-18" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>19 0.062199328 <a title="361-tfidf-19" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>20 0.061422013 <a title="361-tfidf-20" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, 0.041), (2, 0.014), (3, 0.009), (4, 0.04), (5, -0.017), (6, -0.011), (7, -0.027), (8, -0.017), (9, -0.045), (10, 0.004), (11, 0.076), (12, 0.05), (13, -0.018), (14, 0.034), (15, -0.202), (16, -0.005), (17, 0.058), (18, 0.08), (19, -0.026), (20, 0.046), (21, -0.067), (22, 0.031), (23, -0.004), (24, 0.125), (25, -0.163), (26, -0.083), (27, -0.049), (28, 0.04), (29, 0.051), (30, -0.064), (31, -0.066), (32, 0.026), (33, 0.028), (34, 0.155), (35, -0.005), (36, -0.005), (37, 0.079), (38, 0.188), (39, 0.107), (40, 0.0), (41, 0.051), (42, 0.089), (43, 0.021), (44, 0.023), (45, 0.065), (46, 0.063), (47, 0.084), (48, 0.128), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96196496 <a title="361-lsi-1" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>2 0.76462507 <a title="361-lsi-2" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>Author: Qiang Hao, Rui Cai, Zhiwei Li, Lei Zhang, Yanwei Pang, Feng Wu, Yong Rui</p><p>Abstract: 3D model-based object recognition has been a noticeable research trend in recent years. Common methods find 2D-to-3D correspondences and make recognition decisions by pose estimation, whose efficiency usually suffers from noisy correspondences caused by the increasing number of target objects. To overcome this scalability bottleneck, we propose an efficient 2D-to-3D correspondence filtering approach, which combines a light-weight neighborhoodbased step with a finer-grained pairwise step to remove spurious correspondences based on 2D/3D geometric cues. On a dataset of 300 3D objects, our solution achieves ∼10 times speed improvement over the baseline, with a comparable recognition accuracy. A parallel implementation on a quad-core CPU can run at ∼3fps for 1280× 720 images.</p><p>3 0.66934603 <a title="361-lsi-3" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>4 0.62250382 <a title="361-lsi-4" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>Author: Victor Fragoso, Matthew Turk</p><p>Abstract: We present SWIGS, a Swift and efficient Guided Sampling method for robust model estimation from image feature correspondences. Our method leverages the accuracy of our new confidence measure (MR-Rayleigh), which assigns a correctness-confidence to a putative correspondence in an online fashion. MR-Rayleigh is inspired by Meta-Recognition (MR), an algorithm that aims to predict when a classifier’s outcome is correct. We demonstrate that by using a Rayleigh distribution, the prediction accuracy of MR can be improved considerably. Our experiments show that MR-Rayleigh tends to predict better than the often-used Lowe ’s ratio, Brown’s ratio, and the standard MR under a range of imaging conditions. Furthermore, our homography estimation experiment demonstrates that SWIGS performs similarly or better than other guided sampling methods while requiring fewer iterations, leading to fast and accurate model estimates.</p><p>5 0.55661738 <a title="361-lsi-5" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>6 0.5236569 <a title="361-lsi-6" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>7 0.51772743 <a title="361-lsi-7" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>8 0.51457208 <a title="361-lsi-8" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>9 0.48195806 <a title="361-lsi-9" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>10 0.47806099 <a title="361-lsi-10" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>11 0.47060129 <a title="361-lsi-11" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>12 0.461624 <a title="361-lsi-12" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>13 0.44366547 <a title="361-lsi-13" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>14 0.43936265 <a title="361-lsi-14" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>15 0.43697897 <a title="361-lsi-15" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>16 0.4286038 <a title="361-lsi-16" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>17 0.4252626 <a title="361-lsi-17" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>18 0.42028761 <a title="361-lsi-18" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>19 0.41427431 <a title="361-lsi-19" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>20 0.4043546 <a title="361-lsi-20" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.282), (10, 0.116), (16, 0.084), (26, 0.036), (33, 0.25), (67, 0.029), (69, 0.042), (76, 0.011), (87, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8184554 <a title="361-lda-1" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>2 0.76425731 <a title="361-lda-2" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>3 0.7419917 <a title="361-lda-3" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>Author: Go Irie, Dong Liu, Zhenguo Li, Shih-Fu Chang</p><p>Abstract: Despite significant progress, most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. However, Web images are often associated with text data which may carry substantial information regarding image semantics, and may be exploited for visual dictionary learning. This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering, in addition to information of image descriptors. Existing co-clustering methods are not optimal for this problem because they ignore the structure of image descriptors in the continuous space, which is crucial for capturing visual characteristics of images. We propose a novel Bayesian co-clustering model to jointly estimate the underlying distributions of the continuous image descriptors as well as the relationship between such distributions and the textual words through a unified Bayesian inference. Extensive experiments on image categorization and retrieval have validated the substantial value of the proposed joint modeling in improving visual dictionary learning, where our model shows superior performance over several recent methods.</p><p>4 0.71640921 <a title="361-lda-4" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>5 0.71406299 <a title="361-lda-5" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>Author: Wei Li, Xiaogang Wang</p><p>Abstract: In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person reidentification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsityinducing norm and information theoretical regularization. . cuhk . edu .hk (a) Camera view A (b) Camera view B This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.</p><p>6 0.71188724 <a title="361-lda-6" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>7 0.71125388 <a title="361-lda-7" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>8 0.71102303 <a title="361-lda-8" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>9 0.71101403 <a title="361-lda-9" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>10 0.70961773 <a title="361-lda-10" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>11 0.70927536 <a title="361-lda-11" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>12 0.70805061 <a title="361-lda-12" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>13 0.70794725 <a title="361-lda-13" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>14 0.70767301 <a title="361-lda-14" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>15 0.70723569 <a title="361-lda-15" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>16 0.70687646 <a title="361-lda-16" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>17 0.70649064 <a title="361-lda-17" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>18 0.70539206 <a title="361-lda-18" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>19 0.70506257 <a title="361-lda-19" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>20 0.70499814 <a title="361-lda-20" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
