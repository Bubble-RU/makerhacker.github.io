<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-195" href="#">cvpr2013-195</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</h1>
<br/><p>Source: <a title="cvpr-2013-195-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hu_HDR_Deghosting_How_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>Reference: <a title="cvpr-2013-195-reference" href="../cvpr2013_reference/cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration. [sent-3, score-0.401]
</p><p>2 The limited dynamic range of most imaging sensors often fails to capture the irradiance range visible to the human eye in common real-world scenes. [sent-6, score-0.229]
</p><p>3 A relatively cheap way to address this limitation is to capture a stack of differently exposed pictures of the same scene and merge them, effectively extending the captured range [18, 6]. [sent-7, score-0.419]
</p><p>4 However, because the merging process assumes that the pixels of the  different images are aligned, any motion—either due to the motion of the camera or to anything moving in the scene— will cause ghosting artifacts (if the motion is large) or blurring artifacts (if the motion is small). [sent-8, score-0.406]
</p><p>5 A common approach to address the artifacts due to the camera motion is to first register the low-dynamic-range (LDR) images, a task complicated by the dramatic changes in brightness across the stack, since most registration algorithms rely on the brightness constancy assumption [3 1]. [sent-9, score-0.396]
</p><p>6 [12] address the brightness changes by binarizing each exposure and determining the optimal translation and rotation, respectively. [sent-11, score-0.555]
</p><p>7 [26] compute the gradient map for each exposure and find a similarity transformation in the Fourier domain. [sent-13, score-0.515]
</p><p>8 Our approach allows gathering data from the images in the stack even for regions that are severely under- or over-exposed in the reference, a main limitation of many state-of-the-art approaches. [sent-19, score-0.353]
</p><p>9 camera is static, or that a global registration of the background can be performed. [sent-20, score-0.124]
</p><p>10 [7] model the exposure change and determine patches that might contain moving objects by counting the pixels that deviate from the predicted behavior. [sent-22, score-0.654]
</p><p>11 Raman and Chaudhuri [23] follow a similar idea, but they model the intensity change and detect the motion in irregular patches obtained by grouping pixels into super-pixels. [sent-23, score-0.204]
</p><p>12 These algorithms pay for the reduction of motion artifacts with a potentially reduced dynamic range, as they drop data that does not follow the registration of the background. [sent-24, score-0.273]
</p><p>13 [12] detect pixels that would cause ghosting based on the variance and entropy across the exposure stack. [sent-27, score-0.598]
</p><p>14 [10] use a weight that emphasizes well-exposed pixels and a second weight that enforces consistency across spatial and exposure domains. [sent-31, score-0.563]
</p><p>15 Zhang and Cham [29] propose to weight the pixel using local gradients across the exposure stack as a measure of consistency. [sent-32, score-0.851]
</p><p>16 While computationally efficient, these approaches have the drawback that they downweight or completely ignore pixels of moving objects except, possibly, in one of the images. [sent-33, score-0.125]
</p><p>17 More sophisticated methods attempt to establish dense correspondences between the reference image and the other images in the stack. [sent-35, score-0.268]
</p><p>18 However, standard optical flow algorithms [2] rely on the brightness constancy assumption, which is always violated, by construction, in the case of exposure stacks. [sent-36, score-0.555]
</p><p>19 [13] boost the image intensity to compensate for this and use a standard optical flow to refine the correspondence mapping initialized by a global registration. [sent-38, score-0.173]
</p><p>20 [24] 1 converts each image into a linear space inverting the camera response function, and selects an image as the reference for the final HDR image. [sent-44, score-0.276]
</p><p>21 Using a variant of PatchMatch, they reconstruct an HDR image which maximizes the similarity with the reference image at the pixel level while minimizing the bidirectional similarity metric with the remaining images. [sent-45, score-0.282]
</p><p>22 In general, methods dealing with non-rigid scene motion fall in one of two categories, each with its limitations: • Algorithms that do not define a reference image and incorporate de-ghosting dienf itnhee ade rfeinfeirtieonnc eo ifm mthaeg pixel weights. [sent-46, score-0.319]
</p><p>23 •  Approaches requiring the definition of a reference image. [sent-48, score-0.24]
</p><p>24 However, while it capitalizes on the benefits of selecting a reference image (producing a consistent image [7]) it also enables us to recover from the other images the regions that contain clipped pixels (either too dark or too bright) in the reference image. [sent-53, score-0.726]
</p><p>25 In a nutshell, from each source image S in the stack we attempt to build a new image that looks as if it was taken at the same time as the reference R, but with the exposure settings of S. [sent-54, score-1.177]
</p><p>26 For the areas where R provides sufficient detail, the process is driven by the reference image to ensure consistency. [sent-55, score-0.301]
</p><p>27 For the remaining areas, we use other constraints, as reliable direct registration becomes impossible, and we rely mostly on the information in the source image. [sent-56, score-0.139]
</p><p>28 To get  consistent results even when parts of the scene are moving, we ensure that the boundaries of the saturated regions are consistent with both R and S. [sent-57, score-0.382]
</p><p>29 Our contribution is a novel method for generating a registered stack from a set of mis-aligned images of dynamic scenes, similar to Hu et al. [sent-58, score-0.423]
</p><p>30 However, as opposed to their work, our algorithm can be applied to generic non-linearized exposure stacks, and is also capable of dealing with large saturated regions in the reference image, even under large camera motion or scene object displacements. [sent-61, score-1.21]
</p><p>31 Besides, our method propagates both intensity and gradient information in the reconstruction process, so we can preserve more detail from the exposure stacks. [sent-62, score-0.585]
</p><p>32 Method Our algorithm works by first selecting the image with the highest number of well-exposed pixels to be the reference image R [13, 7]. [sent-64, score-0.288]
</p><p>33 Then, for each source image S in the stack, it synthesizes a new image L (the latent image) that looks like the reference image R, only exposed like S. [sent-65, score-0.502]
</p><p>34 First, where the reference R is properly exposed, L has image content that is geometrically compatible with R. [sent-67, score-0.274]
</p><p>35 In Figure 1, where the reference R is the middle exposure, this means for instance that the arms of the woman in the latent images L must appear in the same location as they appear in R. [sent-68, score-0.374]
</p><p>36 If the reference had been the darkest image (top row in Figure 1), the areas posing these difficulties would have been the dark areas, where details are lost due to clipping. [sent-70, score-0.365]
</p><p>37 For each source image S in the stack, we want to synthesize the latent image L we would have if we had captured it at the same time as the reference image R, but with the same exposure settings as used to capture S. [sent-72, score-0.965]
</p><p>38 τ is an intensity mapping function accounting for how the pixel values change under the exposure change. [sent-73, score-0.701]
</p><p>39 , 5 (ordered by exposure time); if the reference image is 3, we first register 2 and 4 to the reference 3, then 2 acts as the reference for 1, and 4 acts as the reference for 5. [sent-80, score-1.526]
</p><p>40 The reference R is on the left (red), the source S is on the right (blue), and we want to create a latent image L in the center (green) so the shapes of objects in L look like they do in R, except that they have the luminance range of S. [sent-82, score-0.448]
</p><p>41 We first initialize L by applying a color mapping function τ to R, where τ is initialized using the intensity histograms of the images [8], and is later refined as L is updated. [sent-83, score-0.173]
</p><p>42 If the reference patch PiR is not clipped, that is, it is mostly mid-tones and does not contain too dark or bright pixels, PatchMatch looks for a match from S. [sent-85, score-0.524]
</p><p>43 However, if PiR is clipped, neither the color mapping τ, nor direct registration is reliable. [sent-86, score-0.162]
</p><p>44 In this case we modify the PatchMatch to find a patch PiS that could plausibly match PiR: pixels in PiS should match the pixels in PiR that are not clipped, and the rest ofthe pixels in PiS would clip under the current τ. [sent-87, score-0.349]
</p><p>45 As we progress, the intensity mapping function τ is updated  and refined based on the dense correspondence. [sent-89, score-0.144]
</p><p>46 To avoid a bad local minimum and to better synthesize clipped areas, these processes are executed iteratively using a coarse-tofine schedule. [sent-90, score-0.212]
</p><p>47 Two-picture Synthesis Algorithm We wish to synthesize the latent image L that looks just as if R was taken using the exposure setting of S: in other words, L should be consistent with R everywhere in geometry. [sent-94, score-0.716]
</p><p>48 [5], but we account for a generic intensity mapping function τ: Cr(L,R,τ) = ? [sent-99, score-0.144]
</p><p>49 In addition to boosting the details of the texture [1, 21], using gradients helps to compensate for exposure changes [30]. [sent-109, score-0.515]
</p><p>50 The intensity mapping function τ describes how the RGB values change from the reference to the source image. [sent-110, score-0.435]
</p><p>51 , where PiS is a p p patch centered at iin image S (same for PiL and L)i san ad p u×(ip) maps patches i ant tL i itno itmhea corresponding patches in S, see Figure 2. [sent-117, score-0.168]
</p><p>52 We operate in the RGB color space and only search over translations, which makes the updates of L faster but does not lower the quality of our results, given the expected changes in an exposure stack. [sent-121, score-0.515]
</p><p>53 The optimal solution can therefore be reduced to finding the nearest-neighbor patches in S for each patch PiL. [sent-132, score-0.119]
</p><p>54 1 and 2, and summing over the pixels  ×  in the patches rather than over than over the patches themselves, Eq. [sent-136, score-0.146]
</p><p>55 si Bmailsaicra pixels i ins S and the patch in τ(R), while ∇T denotes the weighted average othfe thpea gradients. [sent-140, score-0.118]
</p><p>56 n(i)wu(j)S(i + u(j))], (6) where wτ (i) and wu (i) reflect the confidence of the intensity mapping function τ(·) and the geometric mapping u(·) fsoitry pixel i ,n ? [sent-145, score-0.295]
</p><p>57 The intensity mapping function τ, which describes how the RGB values change from the reference to the source im-  age, cannot be accurate across the whole range, due to saturation and under-exposure. [sent-153, score-0.435]
</p><p>58 For example, if S was captured with a shorter exposure time (darker) than R, and if the top of the range in the domain of R is saturated, τ will be flat in that area, thus not providing any relevant information; all the useful information for registration and HDR image  creation is in S. [sent-154, score-0.701]
</p><p>59 The opposite may be true when S was captured with a longer exposure time, see inset, where red bands show the range in which the mapping τ is not reliable. [sent-155, score-0.633]
</p><p>60 However, consider an area that is saturated in R and assume that we are working with an S that is darker, and therefore better exposed. [sent-166, score-0.353]
</p><p>61 In such regions, τ(PiR) is not reliable and we want to relax the requirement that patches from S have to match, or we would reject all the patches in that area. [sent-167, score-0.135]
</p><p>62 On the other hand, if a patch in S is so dark that it wouldn’t possibly become saturated in R we also don’t want to allow its use. [sent-168, score-0.524]
</p><p>63 In this way, the clipped areas of R in L can be reasonably synthesized using the information from S. [sent-171, score-0.166]
</p><p>64 In the third and last step, given the existing L, we need to re-estimate the intensity mapping function (IMF) τ (Eq. [sent-174, score-0.144]
</p><p>65 Second, 1 1 1 1 1 16 6 6 64 4  in addition to the hard monotonicity constraint, we also require the function to be within [0, 1] , and be convex (or concave) if the exposure time of R is longer (or shorter) than that of S. [sent-181, score-0.515]
</p><p>66 When moving from a level to a finer one, three variables need to be propagated; we transfer τ as is, and linearly interpolate the mapping u. [sent-188, score-0.116]
</p><p>67 Otherwise, it should be initialized using the source image S (using the mapping u derived from the previous level). [sent-191, score-0.154]
</p><p>68 When the reference image is reasonably well-exposed everywhere, our method produces very similar results as Hu et al. [sent-201, score-0.288]
</p><p>69 However, when part of the reference is saturated, as in Figure 5, Hu et al. [sent-202, score-0.288]
</p><p>70 discard valuable information from the shorter exposure (first row, middle image); our method, on the other hand, successfully captures all the available information in the synthesized latent image (second row, middle image). [sent-203, score-0.79]
</p><p>71 Figure 6 shows another case with a large saturated region. [sent-207, score-0.353]
</p><p>72 are not caused by the tonemapping algorithm, rather they are artifacts of their registration algorithm. [sent-219, score-0.155]
</p><p>73 In our result (bottom, rightmost image in Figure 6) the sky is more faithful to the original images and no artifacts are introduced. [sent-220, score-0.16]
</p><p>74 As we mentioned in the previous section, we attempt to preserve as much information as possible from the exposure stack by using both the intensity and the gradients in our reconstruction. [sent-221, score-0.907]
</p><p>75 Figure 7 shows an extreme case of a stack comprising only two images, with a region that is saturated in both images, demonstrating one of the limitations of our method. [sent-225, score-0.647]
</p><p>76 Our method can register the images correctly despite selecting a reference image that has a completely saturated sky. [sent-227, score-0.679]
</p><p>77 However, since the sun is saturated in both images, our algorithm fills in the saturated sun using non-saturated pixels from S. [sent-228, score-0.834]
</p><p>78 Notice that the sky is almost completely saturated, causing their algorithm to disregard useful information in the short exposure (top row, middle image), and leading to poor quality in the fusion result (top right). [sent-246, score-0.724]
</p><p>79 Sen’s algorithm is designed to work on linear exposure stacks. [sent-247, score-0.515]
</p><p>80 For this non-linear stack, a reliable estimation of the camera response function would require acquiring a stack of registered images. [sent-248, score-0.33]
</p><p>81 With the same reference frame our algorithm can synthesize a novel image which is completely consistent with the reference, and also captures all the details of the sky (bottom row, middle image). [sent-250, score-0.492]
</p><p>82 This directly reflects in the high quality of our exposure fusion result (bottom row, rightmost image). [sent-251, score-0.585]
</p><p>83 The first column shows the original images in the stack, the middle exposure is selected as the reference. [sent-255, score-0.602]
</p><p>84 , we first linearize the original images and use the linearized exposure stacks as the input. [sent-257, score-0.612]
</p><p>85 For example, the blurred sky in the saturated region and the halo around the dome are unexpected. [sent-260, score-0.45]
</p><p>86 Note that the halo in the reconstructed shorter exposure is not caused by tone mapping but the errors in HDR reconstruction. [sent-261, score-0.74]
</p><p>87 For the tone mapped HDR image (top right), the reconstructed sky is not natural for the saturated region in the reference. [sent-262, score-0.463]
</p><p>88 Our algorithm can synthesize an image (bottom middle) that is completely consistent with the reference and also preserves as much information as possible from the whole exposure stack. [sent-263, score-0.865]
</p><p>89 The original images (left) are dramatically separated in terms of exposure time: the areas that are correctly exposed in one are barely visible in the other. [sent-267, score-0.657]
</p><p>90 An interesting feature of this stack is that the region around the sun is saturated in both images. [sent-268, score-0.687]
</p><p>91 Note that the longer exposure, which we selected as the reference (left bottom), is completely saturated in the sky; our algorithm attempts to synthesize the saturated region in the source image from other pixels in the same image, thus effectively removing the sun (middle top). [sent-269, score-1.195]
</p><p>92 The last column shows the exposure fusion result for the standard patch size (top) and for the larger patches (bottom). [sent-271, score-0.666]
</p><p>93 Conclusions We have presented a novel method to generate a perfectly aligned stack from a set of images of a dynamic scene, captured with a hand-held camera. [sent-273, score-0.375]
</p><p>94 Four previous methods can deal with both the camera and scene object motion at the same time: Kang et al. [sent-274, score-0.121]
</p><p>95 It successfully deals with large saturated regions in the reference image, which is the most common limitation for algorithms that select a reference frame. [sent-280, score-0.862]
</p><p>96 Ghost removal in high  [15]  [16] [17]  [18]  [19]  [20] [21] [22] [23] [24]  [25]  [26]  dynamic range images. [sent-389, score-0.125]
</p><p>97 Being ‘undigital’ with digital cameras: Extending dynamic range by combining differently exposed pictures. [sent-420, score-0.206]
</p><p>98 A perceptual framework for contrast processing of high dynamic range images. [sent-427, score-0.125]
</p><p>99 Image registration for multi-exposure high dynamic range image acquisition. [sent-472, score-0.213]
</p><p>100 Fast, robust image registration for compositing high-dynamic range photographs from handheld exposures. [sent-484, score-0.132]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exposure', 0.515), ('saturated', 0.353), ('stack', 0.294), ('reference', 0.24), ('hdr', 0.234), ('sen', 0.188), ('patchmatch', 0.107), ('clipped', 0.105), ('zimmer', 0.105), ('pir', 0.097), ('gallo', 0.095), ('pis', 0.091), ('registration', 0.088), ('middle', 0.087), ('hacohen', 0.084), ('dynamic', 0.081), ('exposed', 0.081), ('synthesize', 0.075), ('kang', 0.074), ('mapping', 0.074), ('wexler', 0.073), ('hu', 0.072), ('mantiuk', 0.071), ('plausibly', 0.071), ('intensity', 0.07), ('patch', 0.07), ('bright', 0.069), ('stacks', 0.069), ('artifacts', 0.067), ('dark', 0.064), ('darabi', 0.063), ('areas', 0.061), ('siggraph', 0.056), ('tone', 0.055), ('sky', 0.055), ('shorter', 0.054), ('goldman', 0.053), ('source', 0.051), ('register', 0.051), ('looks', 0.049), ('patches', 0.049), ('et', 0.048), ('pixels', 0.048), ('mertens', 0.047), ('raman', 0.047), ('tico', 0.047), ('tomaszewska', 0.047), ('latent', 0.047), ('range', 0.044), ('halo', 0.042), ('tzimiropoulos', 0.042), ('moving', 0.042), ('pixel', 0.042), ('sun', 0.04), ('brightness', 0.04), ('shechtman', 0.04), ('drucker', 0.039), ('duke', 0.039), ('pulli', 0.039), ('rightmost', 0.038), ('want', 0.037), ('motion', 0.037), ('dramatic', 0.037), ('courtesy', 0.037), ('pil', 0.037), ('cr', 0.036), ('camera', 0.036), ('completely', 0.035), ('isp', 0.035), ('ghosting', 0.035), ('wu', 0.035), ('jacobs', 0.034), ('content', 0.034), ('synthesizes', 0.034), ('radiance', 0.032), ('heo', 0.032), ('match', 0.032), ('bad', 0.032), ('fusion', 0.032), ('nvidia', 0.031), ('irradiance', 0.031), ('coarsest', 0.031), ('chances', 0.03), ('everywhere', 0.03), ('kopf', 0.03), ('ct', 0.03), ('fourier', 0.03), ('severely', 0.03), ('jun', 0.03), ('barnes', 0.03), ('regions', 0.029), ('initialized', 0.029), ('impossible', 0.029), ('luminance', 0.029), ('bottom', 0.029), ('imaging', 0.029), ('rgb', 0.028), ('linearized', 0.028), ('plausible', 0.028), ('attempt', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="195-tfidf-1" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>2 0.11073313 <a title="195-tfidf-2" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: Motion blur frequently occurs in dense 3D reconstruction using a single moving camera, and it degrades the quality of the 3D reconstruction. To handle motion blur caused by rapid camera shakes, we propose a blur-aware depth reconstruction method, which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. Motion blur is dependent on 3D geometry, thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. The estimated depth is then converted intopixel-wise blur kernels, and non-uniform motion blur is easily removed with low computational cost. The obtained blur kernel is depth-dependent, thus it effectively addresses scene-depth variation, which is a challenging problem in conventional non-uniform deblurring methods.</p><p>3 0.10780361 <a title="195-tfidf-3" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>4 0.088014543 <a title="195-tfidf-4" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>5 0.086840674 <a title="195-tfidf-5" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>Author: Yen H. Le, Uday Kurkure, Ioannis A. Kakadiaris</p><p>Abstract: Statistical shape models, such as Active Shape Models (ASMs), sufferfrom their inability to represent a large range of variations of a complex shape and to account for the large errors in detection of model points. We propose a novel method (dubbed PDM-ENLOR) that overcomes these limitations by locating each shape model point individually using an ensemble of local regression models and appearance cues from selected model points. Our method first detects a set of reference points which were selected based on their saliency during training. For each model point, an ensemble of regressors is built. From the locations of the detected reference points, each regressor infers a candidate location for that model point using local geometric constraints, encoded by a point distribution model (PDM). The final location of that point is determined as a weighted linear combination, whose coefficients are learnt from the training data, of candidates proposed from its ensemble ’s component regressors. We use different subsets of reference points as explanatory variables for the component regressors to provide varying degrees of locality for the models in each ensemble. This helps our ensemble model to capture a larger range of shape variations as compared to a single PDM. We demonstrate the advantages of our method on the challenging problem of segmenting gene expression images of mouse brain.</p><p>6 0.082817085 <a title="195-tfidf-6" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>7 0.080203474 <a title="195-tfidf-7" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>8 0.079530649 <a title="195-tfidf-8" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>9 0.07690604 <a title="195-tfidf-9" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>10 0.074700877 <a title="195-tfidf-10" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>11 0.072924055 <a title="195-tfidf-11" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>12 0.070683695 <a title="195-tfidf-12" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>13 0.069971383 <a title="195-tfidf-13" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>14 0.069491021 <a title="195-tfidf-14" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>15 0.0665906 <a title="195-tfidf-15" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>16 0.063508779 <a title="195-tfidf-16" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>17 0.061650224 <a title="195-tfidf-17" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>18 0.06148601 <a title="195-tfidf-18" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>19 0.059138093 <a title="195-tfidf-19" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>20 0.057951976 <a title="195-tfidf-20" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.091), (2, 0.007), (3, 0.034), (4, -0.014), (5, 0.025), (6, -0.009), (7, -0.01), (8, 0.004), (9, 0.005), (10, 0.012), (11, 0.032), (12, 0.026), (13, -0.02), (14, 0.041), (15, -0.067), (16, 0.013), (17, -0.041), (18, 0.058), (19, 0.01), (20, -0.001), (21, 0.015), (22, 0.006), (23, -0.09), (24, -0.006), (25, -0.063), (26, -0.02), (27, -0.036), (28, 0.034), (29, -0.005), (30, -0.056), (31, -0.066), (32, 0.034), (33, 0.003), (34, -0.004), (35, -0.053), (36, -0.049), (37, 0.005), (38, -0.051), (39, 0.004), (40, -0.001), (41, 0.004), (42, -0.005), (43, -0.046), (44, 0.051), (45, 0.015), (46, -0.048), (47, -0.048), (48, -0.054), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94127947 <a title="195-lsi-1" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>2 0.87086469 <a title="195-lsi-2" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>3 0.70955515 <a title="195-lsi-3" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>4 0.68051773 <a title="195-lsi-4" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>5 0.66997886 <a title="195-lsi-5" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>6 0.6558314 <a title="195-lsi-6" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>7 0.65185028 <a title="195-lsi-7" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>8 0.6368773 <a title="195-lsi-8" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>9 0.63176203 <a title="195-lsi-9" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>10 0.62150377 <a title="195-lsi-10" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>11 0.61257887 <a title="195-lsi-11" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>12 0.60855711 <a title="195-lsi-12" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>13 0.59301275 <a title="195-lsi-13" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>14 0.58920002 <a title="195-lsi-14" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>15 0.58556259 <a title="195-lsi-15" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>16 0.57965863 <a title="195-lsi-16" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>17 0.57346147 <a title="195-lsi-17" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>18 0.5711019 <a title="195-lsi-18" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>19 0.56968737 <a title="195-lsi-19" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>20 0.56776386 <a title="195-lsi-20" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.144), (13, 0.036), (16, 0.052), (21, 0.174), (26, 0.044), (33, 0.23), (65, 0.042), (67, 0.041), (69, 0.034), (80, 0.011), (87, 0.071), (96, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91159445 <a title="195-lda-1" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>2 0.87531984 <a title="195-lda-2" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>same-paper 3 0.86207366 <a title="195-lda-3" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>4 0.85648966 <a title="195-lda-4" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>Author: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar</p><p>Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In thispaper wepresent an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.</p><p>5 0.84013325 <a title="195-lda-5" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>6 0.82672018 <a title="195-lda-6" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>7 0.82619584 <a title="195-lda-7" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>8 0.82596171 <a title="195-lda-8" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>9 0.82482833 <a title="195-lda-9" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>10 0.82452703 <a title="195-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.82384408 <a title="195-lda-11" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>12 0.82374328 <a title="195-lda-12" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>13 0.82315397 <a title="195-lda-13" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>14 0.82289803 <a title="195-lda-14" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>15 0.82260871 <a title="195-lda-15" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>16 0.8223443 <a title="195-lda-16" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>17 0.82150644 <a title="195-lda-17" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>18 0.82146335 <a title="195-lda-18" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>19 0.82136399 <a title="195-lda-19" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>20 0.82134843 <a title="195-lda-20" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
