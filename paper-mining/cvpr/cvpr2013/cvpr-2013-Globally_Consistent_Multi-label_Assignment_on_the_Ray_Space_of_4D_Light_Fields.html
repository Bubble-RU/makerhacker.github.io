<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-188" href="#">cvpr2013-188</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</h1>
<br/><p>Source: <a title="cvpr-2013-188-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wanner_Globally_Consistent_Multi-label_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>Reference: <a title="cvpr-2013-188-reference" href="../cvpr2013_reference/cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Globally Consistent Multi-Label Assignment on the Ray Space of 4D Light Fields Sven Wanner Christoph Straehle Bastian Goldluecke Heidelberg Collaboratory for Image Processing  Abstract Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. [sent-1, score-0.981]
</p><p>2 For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. [sent-2, score-0.039]
</p><p>3 The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. [sent-3, score-0.05]
</p><p>4 In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. [sent-4, score-0.631]
</p><p>5 It is thus possible to consistently optimize label assignment over all views simultaneously. [sent-5, score-0.237]
</p><p>6 As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms. [sent-6, score-0.999]
</p><p>7 Introduction  Recent developments in light field acquisition systems [2, 18, 19, 20] strengthen the prediction that we might soon enter an age of light field photography [17]. [sent-8, score-0.956]
</p><p>8 Since compared to a single image, light fields increase the content captured of a scene by directional information, they require an adaption of established algorithms in image processing and computer vision as well as the development of completely novel techniques. [sent-9, score-0.426]
</p><p>9 In this work, we develop methods for training classifiers on features of a light field, and for consistently optimizing label assignments to rays in a global variational framework. [sent-10, score-0.687]
</p><p>10 Here, the ray space of the light field is considered four-dimensional, parametrized by the two points of intersection of a ray with two parallel planes, so that the light field can be considered as a collection of planar views, see figures 2 and 3. [sent-11, score-2.123]
</p><p>11 Due to the planar sampling, 3D points are projected onto lines in cross-sections of the light field called epi-polarplane images. [sent-12, score-0.575]
</p><p>12 In recent works, it was shown that robust disparity reconstruction is possible by analyzing this line struc-  Input serscrib les ingleviewlabelingrayspacelabeling  Figure 1. [sent-13, score-0.2]
</p><p>13 Multi-label segmentation with light field features and disparity-consistent regularization across ray space leads to results which are superior to single-view labeling. [sent-14, score-1.143]
</p><p>14 In contrast to traditional stereo matching, no correspondence search is required, and floating-point precision disparity data can be reconstructed at a very small cost. [sent-16, score-0.2]
</p><p>15 From the point of view of segmentation, this means that in light fields, we not only have access to the color of a pixel and information about the neighboring image texture. [sent-17, score-0.419]
</p><p>16 Instead, we can assume that disparity is readily available as a possible additional feature. [sent-18, score-0.253]
</p><p>17 As long as the inter-class variety of imaged objects is high and the intraclass variation is low, state of the art classifiers can easily discriminate different objects. [sent-20, score-0.033]
</p><p>18 However, separating for example background and foreground leafs poses a more difficult task, see figure 1. [sent-21, score-0.035]
</p><p>19 However, for a classifier which also has geometry based features available, similar looking objects are getting readily distinguishable if their geometric features are separable. [sent-23, score-0.136]
</p><p>20 In the following, we will show that light fields are ideally suited for image segmentation. [sent-24, score-0.426]
</p><p>21 One reason is that geometry is an inherent characteristic of a light field, and thus we can use disparity as a very helpful additional feature. [sent-25, score-0.624]
</p><p>22 The rich structure becomes visible when one stacks all images along a line of view points on top of each other and considers a cut through this stack (denoted by the green border above). [sent-30, score-0.127]
</p><p>23 The 2D image in the plane of the cut is called an epipolar plane image (EPI). [sent-31, score-0.651]
</p><p>24 Contributions In this work, we leverage the intrinsic geometry of 4D light fields to overcome problems of classical segmentation of single images. [sent-32, score-0.515]
</p><p>25 Typical examples are different objects with similar texture properties or identical objects on different spatial positions which cannot be discriminated by a classifier without geometry based features. [sent-33, score-0.082]
</p><p>26 First, we show that ray space based features enable a classifier to distinguish between objects similar in appearance. [sent-35, score-0.565]
</p><p>27 Second, we propose a variational multi-label optimization framework which makes use of the ray space regularizers in a related work [12] in order to ob-  tain a consistent labeling over the complete ray space of the light field. [sent-36, score-1.767]
</p><p>28 To our knowledge, this is the first labeling framework which is designed to work on ray space. [sent-37, score-0.582]
</p><p>29 Third, we will provide all of our data sets online together with ground truth information for label assignments and depth where available, and thus establish the first benchmark data set for light field analysis. [sent-38, score-0.676]
</p><p>30 Light field structure and parametrization This paper builds upon our light field regularization framework, which is introduced in a related work [12]. [sent-40, score-0.692]
</p><p>31 Ray space A 4D light field or Lumigraph is defined on a ray space R, tAhe 4 sDet l gofh rays passing through tsw doe planes nΠ a arandy sΩp aicne eR R3,, where each ray can be uniquely identified by its two intersection points. [sent-43, score-1.996]
</p><p>32 For the sake of simplicity, we assume that both planes are parallel with distance f > 0, and equipped with 2D coordinate systems which are compatible in the sense that the base vectors are parallel and the origins lie on a line orthogonal to both planes. [sent-44, score-0.262]
</p><p>33 The parametrization for ray space we choose is slightly different from the standard one for a Lumigraph [13], and  inspired by [4]. [sent-45, score-0.636]
</p><p>34 A ray R[s, t, x, y] is given by a point (s, t) ∈ Π and (x, y) ∈ R2. [sent-46, score-0.528]
</p><p>35 The twist is that (x, y) is not a c,to)or ∈din Πate an pair ,iny )Ω ∈ (a Rs in the Lumigraph parametrization), but in the local coordinate system of the pinhole projection through (s, t) with image plane in Ω. [sent-47, score-0.341]
</p><p>36 This means that R[s, t, 0, 0] is the ray which passes through the focal point (s, t) and the center of projection in the image plane, i. [sent-48, score-0.572]
</p><p>37 We assume that the coordinate system of the pinhole view is chosen such that x is aligned with s and y is aligned with t, respectively. [sent-52, score-0.177]
</p><p>38 Light fields and epipolar plane images A light field L can now simply be defined as a function on ray space, either scalar or vector-valued for gray scale or color, respectively. [sent-53, score-1.557]
</p><p>39 Of particular interest are the images which emerge when ray space is restricted to a 2D plane. [sent-54, score-0.565]
</p><p>40 Note that Ls∗ ,t∗ is the image of the pinhole view with center of projection (s∗ , t∗). [sent-56, score-0.221]
</p><p>41 The images Ly∗ ,t∗ and Lx∗ are called epipolar plane images. [sent-57, score-0.485]
</p><p>42 They can be interpreted as horizontal or vertical cuts through a horizontal or vertical stack of ,s∗  the views in the light field, see figure 2, and have an interesting structure which seems to consist mainly of straight lines. [sent-58, score-0.473]
</p><p>43 The slope of the lines is linked to disparity, and determines correct regularization, as we will review now. [sent-59, score-0.039]
</p><p>44 Each camera location (s, t) in the view point plane Π yields a different pinhole view of the scene. [sent-62, score-0.422]
</p><p>45 The two thick dashed black lines are orthogonal to both planes, and their intersection with the plane Ω marks the origins of the two different (x, y)-coordinate systems for the views (s1, t) and (s2 , t), respectively. [sent-63, score-0.387]
</p><p>46 Consistent functions on ray space The planar camera movement leads to a linear dependency between the change of the view point and projection coordinates in the epipolar image plane. [sent-64, score-1.176]
</p><p>47 The rate of change depends on the depth of the scene point being projected, and is called the disparity. [sent-65, score-0.041]
</p><p>48 This dependency leads to the  characteristic structure of epipolar plane images we have observed, since it implies that the projection of a 3D scene point in epipolar plane image space is a line. [sent-66, score-1.157]
</p><p>49 Previous works showed that this enables a very robust estimation of disparity on a light field, since line patterns can be detected without computing correspondences [9, 23]. [sent-67, score-0.54]
</p><p>50 In segmentation problems, when one wants to label rays according to e. [sent-68, score-0.224]
</p><p>51 the visible object class, the unknown function on ray space ultimately reflects a property of scene points. [sent-70, score-0.565]
</p><p>52 In consequence, all the rays which view the same scene point have to be assigned the same function value. [sent-71, score-0.174]
</p><p>53 Equivalent to this is to demand that the function must be consistent with the structure on the epipolar plane images. [sent-72, score-0.523]
</p><p>54 In particular, except at depth discontinuities, the value of such a function is not allowed to change in the direction of the epipolar lines, which are induced by the disparity field. [sent-73, score-0.56]
</p><p>55 Regularization on ray space The above considerations give rise to a regularizer Jλμ (U) for vector-valued functions U : R → Rn on ray space. [sent-74, score-1.29]
</p><p>56 It can vbeec wtorri-tvteanlu as th fuen sum osf U Ucon :tr Rib →ution Rs for the regularizers on all epipolar plane images as well as all the views, Jλμ(U) = μJxs(U) + μJyt(U) + λJst(U)  with Jxs(U) =? [sent-75, score-0.653]
</p><p>57 JV(Us∗,t∗) d(s∗,t∗), Jyt(U)  (2)  where the anisotropic regularizers Jρ act on 2D epipolar plane images, and are defined such that they encourage smoothing in the direction of the epipolar lines. [sent-78, score-0.976]
</p><p>58 This way, they enforce consistency of the function U with the epipolar plane image structure. [sent-79, score-0.523]
</p><p>59 The spatial regularizer JV encodes the label transition costs, as we will explore in more detail in the next section. [sent-81, score-0.284]
</p><p>60 Finally, the constants λ > 0 and μ > 0 are user-defined and adjust the amount of regularization on the separate views and epipolar plane images, respectively. [sent-82, score-0.615]
</p><p>61 Optimal label assignment on ray space In this section, we introduce the new variational labeling framework on ray space. [sent-84, score-1.376]
</p><p>62 Its design is based on the representation of labels with indicator functions [6, 16, 25], which leads to a convex optimization problem. [sent-85, score-0.197]
</p><p>63 We can use the efficient optimization framework presented in [12] to obtain a globally optimal solution to the convex problem, however, as usual we need to project back to indicator functions and only end up within a (usually small) posterior bound of the optimum. [sent-86, score-0.161]
</p><p>64 The variational multi-label problem Let Γ be the (discrete) set of labels, then to each label γ ∈ Γ we assign a binary tfeu)n scettio onf uγ : R, t →en t{o0 ,e 1ac}h w lahbicelh γ γta ∈ke Γs the value one if and only if a ray Ris → assigned twhhei clahb tealk γ. [sent-87, score-0.735]
</p><p>65 Since the assignment must be unique, the set of indicator functions must satisfy the simplex constraint  ? [sent-88, score-0.266]
</p><p>66 (3)  Arbitrary spatially varying label cost functions cγ can be defined, which penalize the assignment of γ to a ray R ∈ R dweifthin tehde, cwohsitc cγ p(Ren)a ≥iz e0 . [sent-91, score-0.734]
</p><p>67 This implies that the labeling is encouraged to be consistent with the epipolar plane structure of the light field to be labeled. [sent-95, score-1.035]
</p><p>68 The spatial regularizer JV needs to enforce the label transition costs. [sent-96, score-0.322]
</p><p>69 For the remainder of this work, we choose a simple weighted Potts penalizer [24]  JV(Us∗,t∗) :=21γ? [sent-97, score-0.04]
</p><p>70 Ωg |(Duγ)s∗t∗| d(x,y),  (4)  where g is a spatially varying transition cost. [sent-99, score-0.051]
</p><p>71 Since the total variation of a binary function equals the length of the interface between the zero and one level set due to the co-area formula [11], the factor 1/2 leads to the desired penaliza-  tion. [sent-100, score-0.076]
</p><p>72 Rather, 1 1 10 0 01 1 1 3 1  we can use any of the more sophisticated regularizers proposed in the literature [6, 16], for example truncated linear penalization, Euclidean label distances, Huber TV or the Mumford-Shah regularizer. [sent-102, score-0.218]
</p><p>73 An overview as well as further specializations tailored to vector-valued label spaces can be found in [22]. [sent-103, score-0.09]
</p><p>74 The space of binary functions over which one needs to optimize is not convex, since convex combinations of binary functions are usually not binary. [sent-104, score-0.197]
</p><p>75 We resort to a convex relaxation, which with the above conventions can now be written as  arUg∈mCin⎧⎨Jλμ(U) +γ? [sent-105, score-0.085]
</p><p>76 Rcγuγd(x,y,s,t)⎬⎫,  (5)  where C is ⎩the convex set of functions U = (uγ⎭ : R → w[0,h 1])γ∈Γ sw thheich c satisfy etht eo simplex ncson Ustr =ain (tu (3). [sent-107, score-0.274]
</p><p>77 : RAft →er optimization, the solution of (5) needs to be projected back onto the space of binary functions. [sent-108, score-0.072]
</p><p>78 An exception is the two-label case, where we indeed achieve global optimality via thresholding, since  the anisotropic total variation also satisfies a co-area formula [25]. [sent-110, score-0.084]
</p><p>79 Optimization Note that according to (2), the full regularizer Jλμ which is defined on 4D ray space decomposes into a sum of 2D regularizers on the epipolar plane images and individual views, respectively. [sent-111, score-1.321]
</p><p>80 While solving a single saddle point problem for the full regularizer would require too much memory, it is feasible to iteratively compute independent descent steps for the data term and regularizer components. [sent-112, score-0.286]
</p><p>81 Aside from the data term, the main difference here is the simplex constraint set for the primal variable U. [sent-114, score-0.095]
</p><p>82 We enforce it with Lagrange multipliers in the proximity operators of the regularizer components, which can be easily integrated into the primal-dual algorithm [7]. [sent-115, score-0.181]
</p><p>83 On our system equipped with an nVidia GTX 580 GPU, optimization takes about 1. [sent-117, score-0.043]
</p><p>84 5 seconds per label in Γ and per million rays in R, i. [sent-118, score-0.185]
</p><p>85 about 5 minutes for our rendered data smetilsl oifn nt rheay rse isnul Rt ,fo ir. [sent-120, score-0.072]
</p><p>86 r oIfu only tehree d re dsautlat for one single view (i. [sent-123, score-0.079]
</p><p>87 the center one) is required, computation can be restricted to view points located in a cross with that specific view at the center. [sent-125, score-0.158]
</p><p>88 The result will usually be very close to the optimization over the complete ray space. [sent-126, score-0.528]
</p><p>89 While this compromise forfeits some information in the data, it leads to significant speeds ups, for our rendered  data sets to about 30 seconds. [sent-127, score-0.075]
</p><p>90 Local class probabilities We calculate the unary potentials cγ in (5) from the negative log-likelihoods of the local class probabilities, cγ  (R) = −logp? [sent-129, score-0.042]
</p><p>91 ∈ [0, 1] for the label γ, conditioned on a local featu? [sent-138, score-0.09]
</p><p>92 r v(R) ∈ oRr|tFh| for each ray R ∈ R, are ao lbotacianle fed by training a (clRa)ssi ∈fi Rer on a user-provided partial labeling of the center view. [sent-140, score-0.582]
</p><p>93 As features, we use a combination of color, Laplace operator of the view, intensity standard deviation in a neighborhood, Eigenvalues of the Hessian and the disparity computed on several scales. [sent-141, score-0.2]
</p><p>94 While our framework allows the use of arbitrary classifiers, we specialize in this paper to a Random Forest [5]. [sent-142, score-0.035]
</p><p>95 Random Forests make use of bagging to reduce variance and avoid overfitting. [sent-144, score-0.063]
</p><p>96 A decision forest is built from a number n of trees, which are each trained from a random subset of the available training samples. [sent-145, score-0.033]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ray', 0.528), ('light', 0.34), ('epipolar', 0.319), ('disparity', 0.2), ('plane', 0.166), ('regularizer', 0.143), ('regularizers', 0.128), ('jv', 0.127), ('lumigraph', 0.119), ('field', 0.118), ('pinhole', 0.098), ('rays', 0.095), ('simplex', 0.095), ('label', 0.09), ('planes', 0.088), ('fields', 0.086), ('views', 0.085), ('jst', 0.079), ('jxs', 0.079), ('jyt', 0.079), ('view', 0.079), ('variational', 0.077), ('parametrization', 0.071), ('ly', 0.065), ('bagging', 0.063), ('assignment', 0.062), ('origins', 0.061), ('indicator', 0.055), ('labeling', 0.054), ('functions', 0.054), ('readily', 0.053), ('convex', 0.052), ('assignments', 0.052), ('transition', 0.051), ('geometry', 0.05), ('stack', 0.048), ('potts', 0.047), ('regularization', 0.045), ('anisotropic', 0.044), ('projection', 0.044), ('planar', 0.043), ('equipped', 0.043), ('probabilities', 0.042), ('depth', 0.041), ('formula', 0.04), ('penalizer', 0.04), ('fuen', 0.04), ('injected', 0.04), ('scettio', 0.04), ('arandy', 0.04), ('epi', 0.04), ('gofh', 0.04), ('mum', 0.04), ('rib', 0.04), ('solutio', 0.04), ('straehle', 0.04), ('strengthen', 0.04), ('thheich', 0.04), ('vecto', 0.04), ('wanner', 0.04), ('rendered', 0.039), ('lines', 0.039), ('segmentation', 0.039), ('consistent', 0.038), ('enforce', 0.038), ('thefirst', 0.037), ('ups', 0.037), ('ution', 0.037), ('wepresent', 0.037), ('collaboratory', 0.037), ('aicne', 0.037), ('bastian', 0.037), ('goldluecke', 0.037), ('sven', 0.037), ('tsw', 0.037), ('space', 0.037), ('intersection', 0.036), ('leads', 0.036), ('dependency', 0.036), ('establish', 0.035), ('parallel', 0.035), ('doe', 0.035), ('leafs', 0.035), ('mcin', 0.035), ('specialize', 0.035), ('rer', 0.035), ('projected', 0.035), ('characteristic', 0.034), ('forest', 0.033), ('christoph', 0.033), ('featu', 0.033), ('oifn', 0.033), ('ssi', 0.033), ('distinguishable', 0.033), ('twist', 0.033), ('conventions', 0.033), ('etht', 0.033), ('classifiers', 0.033), ('discriminated', 0.032), ('penalization', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="188-tfidf-1" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>2 0.69602066 <a title="188-tfidf-2" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>3 0.2680327 <a title="188-tfidf-3" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>4 0.23101841 <a title="188-tfidf-4" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>5 0.2027481 <a title="188-tfidf-5" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>6 0.18582433 <a title="188-tfidf-6" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>7 0.1771903 <a title="188-tfidf-7" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>8 0.1583268 <a title="188-tfidf-8" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>9 0.14015688 <a title="188-tfidf-9" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>10 0.13638103 <a title="188-tfidf-10" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>11 0.12994993 <a title="188-tfidf-11" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>12 0.11672396 <a title="188-tfidf-12" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>13 0.11668807 <a title="188-tfidf-13" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>14 0.11666376 <a title="188-tfidf-14" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>15 0.11391145 <a title="188-tfidf-15" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>16 0.11314696 <a title="188-tfidf-16" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>17 0.11225509 <a title="188-tfidf-17" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>18 0.10824358 <a title="188-tfidf-18" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>19 0.10193984 <a title="188-tfidf-19" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>20 0.1001964 <a title="188-tfidf-20" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.183), (1, 0.23), (2, 0.013), (3, 0.059), (4, 0.021), (5, -0.1), (6, -0.077), (7, 0.041), (8, 0.018), (9, 0.093), (10, -0.025), (11, 0.115), (12, 0.256), (13, -0.068), (14, -0.4), (15, 0.172), (16, -0.027), (17, -0.032), (18, 0.043), (19, 0.074), (20, -0.012), (21, 0.085), (22, 0.112), (23, 0.017), (24, -0.008), (25, 0.082), (26, 0.036), (27, -0.023), (28, 0.003), (29, -0.061), (30, 0.037), (31, -0.055), (32, -0.003), (33, -0.071), (34, 0.066), (35, -0.05), (36, -0.018), (37, -0.022), (38, 0.009), (39, -0.025), (40, -0.042), (41, 0.062), (42, 0.016), (43, -0.056), (44, 0.053), (45, -0.052), (46, -0.062), (47, -0.0), (48, 0.124), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96812117 <a title="188-lsi-1" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>same-paper 2 0.96689409 <a title="188-lsi-2" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>3 0.73940068 <a title="188-lsi-3" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>4 0.71738464 <a title="188-lsi-4" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>5 0.68883592 <a title="188-lsi-5" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>Author: Yu Ji, Jinwei Ye, Jingyi Yu</p><p>Abstract: Transparent gas flows are difficult to reconstruct: the refractive index field (RIF) within the gas volume is uneven and rapidly evolving, and correspondence matching under distortions is challenging. We present a novel computational imaging solution by exploiting the light field probe (LFProbe). A LF-probe resembles a view-dependent pattern where each pixel on the pattern maps to a unique ray. By . ude l. edu observing the LF-probe through the gas flow, we acquire a dense set of ray-ray correspondences and then reconstruct their light paths. To recover the RIF, we use Fermat’s Principle to correlate each light path with the RIF via a Partial Differential Equation (PDE). We then develop an iterative optimization scheme to solve for all light-path PDEs in conjunction. Specifically, we initialize the light paths by fitting Hermite splines to ray-ray correspondences, discretize their PDEs onto voxels, and solve a large, over-determined PDE system for the RIF. The RIF can then be used to refine the light paths. Finally, we alternate the RIF and light-path estimations to improve the reconstruction. Experiments on synthetic and real data show that our approach can reliably reconstruct small to medium scale gas flows. In particular, when the flow is acquired by a small number of cameras, the use of ray-ray correspondences can greatly improve the reconstruction.</p><p>6 0.65765572 <a title="188-lsi-6" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>7 0.64707112 <a title="188-lsi-7" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>8 0.64618248 <a title="188-lsi-8" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>9 0.62321591 <a title="188-lsi-9" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>10 0.60488862 <a title="188-lsi-10" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>11 0.57933432 <a title="188-lsi-11" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>12 0.55639863 <a title="188-lsi-12" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>13 0.55412918 <a title="188-lsi-13" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>14 0.53137857 <a title="188-lsi-14" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>15 0.46781313 <a title="188-lsi-15" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>16 0.43698198 <a title="188-lsi-16" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>17 0.43355373 <a title="188-lsi-17" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>18 0.43206862 <a title="188-lsi-18" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>19 0.42875579 <a title="188-lsi-19" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>20 0.42848513 <a title="188-lsi-20" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.161), (16, 0.038), (22, 0.119), (26, 0.042), (33, 0.254), (47, 0.011), (67, 0.035), (69, 0.041), (70, 0.016), (87, 0.129), (96, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92605937 <a title="188-lda-1" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>Author: Stefan Harmeling, Michael Hirsch, Bernhard Schölkopf</p><p>Abstract: We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition. A real world experiment shows that we can super-resolve beyond the Rayleigh limit.</p><p>same-paper 2 0.91534412 <a title="188-lda-2" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>3 0.91416895 <a title="188-lda-3" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>4 0.91079843 <a title="188-lda-4" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>Author: Steve Branson, Oscar Beijbom, Serge Belongie</p><p>Abstract: unkown-abstract</p><p>5 0.90327388 <a title="188-lda-5" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>Author: Sagi Katz, Ayellet Tal</p><p>Abstract: Point sets are the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way.</p><p>6 0.9020378 <a title="188-lda-6" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>7 0.8963424 <a title="188-lda-7" href="./cvpr-2013-Is_There_a_Procedural_Logic_to_Architecture%3F.html">228 cvpr-2013-Is There a Procedural Logic to Architecture?</a></p>
<p>8 0.89436322 <a title="188-lda-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.89182717 <a title="188-lda-9" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>10 0.88850081 <a title="188-lda-10" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>11 0.88796216 <a title="188-lda-11" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>12 0.88790888 <a title="188-lda-12" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>13 0.88700074 <a title="188-lda-13" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>14 0.88684547 <a title="188-lda-14" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>15 0.88679212 <a title="188-lda-15" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>16 0.88663274 <a title="188-lda-16" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>17 0.88575852 <a title="188-lda-17" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>18 0.8857097 <a title="188-lda-18" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>19 0.88550138 <a title="188-lda-19" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>20 0.88404775 <a title="188-lda-20" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
