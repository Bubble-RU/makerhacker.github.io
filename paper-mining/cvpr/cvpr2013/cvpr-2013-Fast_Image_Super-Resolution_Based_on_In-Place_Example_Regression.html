<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-166" href="#">cvpr2013-166</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</h1>
<br/><p>Source: <a title="cvpr-2013-166-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Fast_Image_Super-Resolution_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>Reference: <a title="cvpr-2013-166-reference" href="../cvpr2013_reference/cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. [sent-2, score-0.585]
</p><p>2 Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. [sent-3, score-0.229]
</p><p>3 Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. [sent-4, score-0.253]
</p><p>4 Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. [sent-5, score-0.408]
</p><p>5 The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain  diverse textures and they are potentially contaminated by noise or compression artifacts. [sent-6, score-0.495]
</p><p>6 Such image priors range from simple analytical “smoothness” priors to more sophisticated statistical priors learned from natural images [16, 5, 7, 19, 10]. [sent-10, score-0.197]
</p><p>7 As images contain strong discontinuities, such as edges and corners, the simple “smoothness” prior will result in ringing, jaggy and blurring artifacts. [sent-14, score-0.193]
</p><p>8 Therefore, more sophisticated statistical priors learned from natural images are exs cohen} @ adobe . [sent-15, score-0.138]
</p><p>9 Alternatively, example-based nonparametric methods [7, 14, 2, 17] were used to predict the missing frequency band of the upsampled image, by relating the image pixels at two spatial scales using a universal set of training example patch pairs. [sent-18, score-0.725]
</p><p>10 , local image structures tend to recur within and across different image scales [4, 1, 21], and image super-resolution can be regularized based on these self-similar examples instead of some external database [4, 9, 6]. [sent-24, score-0.225]
</p><p>11 They show that the local selfsimilarity assumption for natural images holds better for small upscaling factors and the patch search can be conducted in a restricted local region, allowing a very fast practical implementation. [sent-28, score-0.871]
</p><p>12 In this paper, we refine the local self-similarity [6, 21] by in-place self-similarity, by proving that, for a query patch in the upper scale image, patch matching can be restricted to its origin location in the lower scale image. [sent-29, score-0.933]
</p><p>13 g, cellphone 1 1 10 0 05 5 579 7  cameras) or compression artifacts (e. [sent-32, score-0.273]
</p><p>14 We propose a new fast super-resolution algorithm based on regression on in-place examples, which, for the first time, leverages the two fundamental super-resolution approaches of learning from externalexamples and learning from self-examples. [sent-36, score-0.329]
</p><p>15 We prove that patch matching across different image scales with small scaling factors is in-place, which refines and validates the recently proposed local selfsimilarity theoretically. [sent-38, score-0.634]
</p><p>16 We can easily extend our algorithm to handle noisy input images by combining regression results on multiple in-place examples. [sent-40, score-0.289]
</p><p>17 Section 3 presents our robust regression model for super-resolution. [sent-44, score-0.289]
</p><p>18 Preliminaries and Notations  ××  This work focuses on upscaling an input image X0 which contains some high-frequency content that we can borrow for image super-resolution, i. [sent-48, score-0.347]
</p><p>19 We use bolded lower case x0 and x to denote a a image patches sampled from X0 and Xan, respectively, a an×d y0 maandg y atot cdheenso steam a ×le a image patches sampled from Y0 and Ya , respectively. [sent-53, score-0.229]
</p><p>20 Regression on In-place Examples  In this section, we present our super-resolution algorithm based on learning the in-place example regression by referring to an external database. [sent-57, score-0.408]
</p><p>21 The Image Super-resolution Scheme Similar to [6], we first describe our overall image upscaling scheme in Figure 1, which is based on in-place examples and first-order regression that will be discussed shortly. [sent-60, score-0.696]
</p><p>22 For each patch y of the upsampled low-frequency band image Y , we find its in-place match y0 from the low-frequency band Y0, and then perform a first-order regression on x0 to estimate the desired patch x for target X. [sent-64, score-1.321]
</p><p>23 The input image is denoted as X0 ∈ RK1 ×K2, from which we obtain its low-frequency band image Y0 ∈ RK1×K2 by a low-pass Gaussian filtering. [sent-65, score-0.186]
</p><p>24 We upsample X∈0 using bicubic interpolation by a factor of s to get Y ∈ RsK1 ×sK2. [sent-66, score-0.196]
</p><p>25 For each image patch y (a a) from the image Y at locaFotiron e (cxh, y im), we pfiantcdh i tys in-place example y0 g(ae ×Y a at) around its origin coordinates (xr, yr) in image Y0(, aw ×her ae) xr = ? [sent-69, score-0.49]
</p><p>26 a{ ×y0 a, )x f0r}o mco Xnstitutes a low- and high-resolution prior example pair }fro cmon nwsthiitucthe we apply a first-order regression model to estimate the high-  +  +  ×  resolution image patch x for y. [sent-81, score-0.62]
</p><p>27 We repeat this procedure for overlapping patches of Y , and the final high-resolution image X is generated by aggregation all the recovered highresolution image patches. [sent-82, score-0.181]
</p><p>28 For large upscaling factors, we iteratively repeat the above upscaling step by multiple times, each with a constant scaling factor of s. [sent-83, score-0.854]
</p><p>29 These local singular primitives are more invariant to scale changes, i. [sent-88, score-0.201]
</p><p>30 , an upper scale image contains singular structures similar to those in its lower spatial scale [9]. [sent-90, score-0.295]
</p><p>31 Furthermore, we expect a singular structure in the upper scale image will have a similar structure in its origin location on the lower spatial scale. [sent-91, score-0.277]
</p><p>32 To evaluate such local scale invariance, we compute the matching error between a 5 5 query patch y at (x, y) in Y and 49 patches centering a ×ro u5n qdu e(xryr, p pyart)c hin y image ,Yy0) on tYhe a Berkeley Segmentation Dataset [11]. [sent-92, score-0.528]
</p><p>33 Figures 2 plots the average matching errors for different scaling factor s, where “blue” denotes small matching error and “red” denotes large matching error. [sent-93, score-0.34]
</p><p>34 Matching errors of each patch y in an upper-scale image with its in-place neighbors in the lower-scale image for different scaling factors. [sent-97, score-0.399]
</p><p>35 s  center (xr, yr) on average, and the smaller the scaling factor, the lower the matching error, and the more concentrated the area with lower matching error. [sent-98, score-0.303]
</p><p>36 Therefore, for a small scaling factor, finding a similar match for a query patch y could be extremely localized on Y0. [sent-99, score-0.482]
</p><p>37 The local scale invariance indicates that we could efficiently find a similar example y0 for y, and thus the corresponding high-resolution patch x0, from which we can infer the high-frequency information of y. [sent-101, score-0.378]
</p><p>38 For each patch y of size a a (a > 2) from upsampled image Y ea cath l poacatcthio yn (oxf, s yiz)e, containing only one singular primitive, the location (x0, y0) of a close match y0 in Y0 for y will be at most one pixel away from y ’s origin location (x/s, y/s) on Y0, i. [sent-104, score-0.56]
</p><p>39 , |x0 x/s| < 1and |y0 y/s| < 1, given the scaling factor s −< xa//s(a| <− 1 2). [sent-106, score-0.16]
</p><p>40 a −  −  Therefore, the search region for y on Y0 is in-place, and  is called an in-place example for patch y. [sent-107, score-0.294]
</p><p>41 Based on the in-place example pair {y0 , x0}, we perform a first-order regression atom epsletim paatier {thye high-resolution rinmfo arm fiartsito-nor dfeorr patch y in the following section. [sent-108, score-0.583]
</p><p>42 In-place Example Regression The patch-based single image super-resolution problem can be viewed as a regression problem, i. [sent-111, score-0.289]
</p><p>43 , finding a mapping function f from the low-resolution patch space to the target high-resolution patch space. [sent-113, score-0.644]
</p><p>44 However, learning this regression function turns out to be extremely difficult due to the ill-posed nature of super-resolution; proper regularizations or good image priors are need to constrain the solution space. [sent-114, score-0.388]
</p><p>45 22) ≈x0 + ∇fT(y0)(y − y0),  x =f(y)  (1)  2This is a reasonable assumption, as one will not expect a dramatic change in the high-resolution patch for a minor change in the lowresolution image patch, especially in the case of small scaling factors. [sent-118, score-0.399]
</p><p>46 , cn} sampled from the luoews- ornes aol suettio onf patch space. [sent-125, score-0.294]
</p><p>47 Discussions In previous example-based super-resolution works [7, 6], the high-resolution image patch x is obtained by transferring the high-frequency component from the best prior example pair {y0, x0} to the low-resolution image patch y, aim. [sent-133, score-0.625]
</p><p>48 th Caot highfrequency component transfer is an approximate of the firstorder regression model by setting the derivative function ∇f to be the identity matrix. [sent-148, score-0.431]
</p><p>49 1, which does not account for error compensation and thus has larger approximation errors. [sent-156, score-0.15]
</p><p>50 The results are obtained by recovering overlapping low-resolution image patches which −  −x  3Here, x, y, x0 and y0 are in their vectorized form 4The prior in-place example pairs are found based on {yi }im=1 only discuTssheed p brieofror ine. [sent-158, score-0.216]
</p><p>51 Because the zero-order approximation has large approximation errors, the overlapping pixel predictions do not agree with each other. [sent-164, score-0.197]
</p><p>52 However, the algorithm still requires a large number of training examples in order to approximate f well, resulting in expensive computations for practical applications. [sent-169, score-0.147]
</p><p>53 To reduce the regression variance, we can perform regression on each of them and combine the results by a weighted average. [sent-174, score-0.578]
</p><p>54 By aggregating the multiple regression results, our algorithm can handle different image degradations well in practical applications. [sent-196, score-0.341]
</p><p>55 It is worthy to note that our formulation only uses regression results on extremely localized in-place examples in a lower spatial scale, which is different from that of the non-local means algorithm [1] that operates on raw image patches in a much larger spatial window at the same spatial scale. [sent-197, score-0.59]
</p><p>56 Prediction RMSEs for different approaches on testing patches and images for one upscaling step (1. [sent-199, score-0.442]
</p><p>57 Although simple interpolation methods result in artifacts along the discontinuities, they perform well on smooth regions. [sent-208, score-0.27]
</p><p>58 This observation suggests that we only need to process the textured regions with our super-resolution model, while leaving the large smooth regions to simple and fast interpolation techniques. [sent-209, score-0.191]
</p><p>59 To differentiate smooth and textured regions, we do SVD on the gradient matrix of a local image patch, and calculate the singular values s1 ≥ s2 ≥ 0, which represent the energies in the dominant loc≥al gradient wanhdic edge orersieenntta tthioen e. [sent-210, score-0.209]
</p><p>60 Parameters We choose patch size a = 5 and iterative scaling factor s = 1. [sent-216, score-0.454]
</p><p>61 The low-frequency band Y of the target high-resolution image is approximated by bicubic interpolation from X0. [sent-218, score-0.327]
</p><p>62 The low-frequency band Y0 of the input image X0 is obtained by a low-pass Gaussian filtering with a standard deviation of 0. [sent-219, score-0.186]
</p><p>63 The image patches of Y are processed with overlapping pixels, which are simply averaged to get the final result. [sent-221, score-0.136]
</p><p>64 1 1 10 0 06 6 602 0  ×  frequency transfer egres ion  Figure 4. [sent-224, score-0.135]
</p><p>65 Training To train the regression model, we start from a collection of high-resolution natural images {Xi}ir=1 from tchoel Berkeley Segmentation nD naatatuseratl [ i1m m1a]. [sent-227, score-0.333]
</p><p>66 g eTsh {eX corresponding lower spatial scale images {X0i}ir=1 are generated by blurring arn sdp downsampling tehse { high-resolution images by a factor of s. [sent-228, score-0.223]
</p><p>67 The two low-frequency band image sets {Yi}ir=1 and {Y0i}ir=1 are generated as described above. [sent-229, score-0.186]
</p><p>68 {WYe }then randomly sample image patch pairs from {Yi}ir=1 Wande {hXeni} rira=n1d otom loyb staamin ptlhee mlowag-e a pnadt high-resolution patch pairs {Xxi} , yi}im=1, and meanwhile get the corresponding inplace matching image pairs {xi0, yi0}im=1 from {X0i}im=1 apnladc {eY m0ia}trci=h1in . [sent-230, score-0.849]
</p><p>69 In our experiments, we find that the learned regression model can be very com-  ×  pact; using 27 = 128 anchor points already suffice for our purpose. [sent-236, score-0.379]
</p><p>70 Regression Evaluations  To measure the regression accuracy from the recovery perspective, we first conduct quantitative comparisons for different methods in terms of RMSE. [sent-239, score-0.33]
</p><p>71 Table 1 reports the results on both testing patches and synthetic images taken from [11] for one upscaling step (1. [sent-240, score-0.48]
</p><p>72 As shown, zeroforrodmer regression performs nthge s wtepors (1t d. [sent-242, score-0.289]
</p><p>73 Frequency transfer performs much better than zero-order regression due to the error compensation for Δx by Δy. [sent-245, score-0.415]
</p><p>74 5 In Figure 4, we show super-resolution results (3 ) on the “Mayan aurrceh 4i,te wcteur seh”o image rw-ritehs frequency utrltasn(s f3e×r )a ondn our regression model, respectively. [sent-247, score-0.37]
</p><p>75 By comparison, our regression estimation is more accurate and thus can preserve more sharp details compared with frequency transfer. [sent-248, score-0.491]
</p><p>76 5It is worthy to note that the algorithms discussed here are based on multiple iterative upscaling steps for large scaling factors. [sent-249, score-0.511]
</p><p>77 Therefore, we only make comparisons for one upscaling step to keep the experiment clean. [sent-252, score-0.388]
</p><p>78 However, it also creates small artifacts across the image due to the fact that some unique patches to the image cannot be well represented by the universal dictionary, and the algorithm is also much slower than ours. [sent-263, score-0.315]
</p><p>79 The algorithms of [9] and [6] are based on selfexamples, which tend to create artificially sharp edges and artifacts due to the insufficient number of matched selfexamples. [sent-264, score-0.384]
</p><p>80 In contrast, by leveraging learning from external examples and learning from self-examples, our algo-  ××  rithm can produce sharp details without noticeable visual artifacts. [sent-265, score-0.338]
</p><p>81 , ghost artifacts along the cheek in “kid”, jaggy artifacts on the long edge in “chip”, and artifacts in the camera area in “cameraman”. [sent-271, score-0.565]
</p><p>82 [18] are generally a little burry and they contain many small artifacts across the image upon a closer look. [sent-273, score-0.167]
</p><p>83 , pupil in “kid”, as it is using frequency transfer to generate high-resolution patches. [sent-279, score-0.135]
</p><p>84 In comparison, our algorithm is able to recover local texture details as well as sharp edges without sacrificing the naturalness. [sent-280, score-0.177]
</p><p>85 Most previous super-resolution works focus on synthetic test examples with simple edge structures, but the large body of natural images typically contain diverse textures and rich fine structures. [sent-281, score-0.238]
</p><p>86 , images captured by low cost sensors are typically contaminated by some amount of sensor noise and internet images with JPEG compression artifacts. [sent-292, score-0.317]
</p><p>87 By averaging the regression results on multiple in-place self-examples, our algorithm can naturally handle noisy inputs. [sent-294, score-0.289]
</p><p>88 Figure 7 shows one more set of super-resolution results (3 ) on real-world images that are corrupted uwtiitohn e riethseurlt sensor onnois ree or compression a thrtai-t  ×  facts. [sent-295, score-0.153]
</p><p>89 As shown, the algorithms in [9] and [6] cannot distinguish noise from the signal and thus enhance both, resulting in magnified noise artifacts, while our algorithm almost completely eliminates the noise and at the same time preserves sharp image structures. [sent-296, score-0.298]
</p><p>90 Computational Efficiency With fast in-place matching and selective patch processing, our algorithm is much faster than Glasner’s algorithm [9], is at least one order of magnitude faster than Yang’s algorithm [18], and is comparable with Freedman’s algorithm [6]. [sent-298, score-0.431]
</p><p>91 Conclusions  In this paper, we propose a robust first-order regression model for image super-resolution based on justified in-place self-similarity. [sent-305, score-0.289]
</p><p>92 Taking advantage of the in-place examples, we can learn a fast and robust regression function for the otherwise ill-posed inverse mapping from low- to high-resolution patches. [sent-307, score-0.385]
</p><p>93 On the other hand, by learning from an external training database, the regression model can overcome the  problem of insufficient number of self-examples for matching. [sent-308, score-0.448]
</p><p>94 Compared with previous example-based approaches, our new approach is more accurate and can produce natural looking results with sharp details. [sent-309, score-0.203]
</p><p>95 In many practical applications where images are contaminated by noise or compression artifacts, our robust formulation is of particular importance. [sent-310, score-0.322]
</p><p>96 The results of [18] contain small artifacts along edges (best perceived in zoomed PDF). [sent-314, score-0.27]
</p><p>97 Imag and video upscaling  [7]  [8]  [9]  [10] [11]  [12]  [13]  [14]  [15]  from local self-examples. [sent-360, score-0.347]
</p><p>98 In order to get sharp results for singular structure (p, q), we want to match (p, q) with (p? [sent-445, score-0.243]
</p><p>99 Given a patch y (a a) from the low-frequency band image nY a ace pnattecrhed y ya t( a(x ×, y a),) fsrhoomwn t hine Figure 8, we assume it has a singular structure centered at (p, q). [sent-477, score-0.637]
</p><p>100 This point has a shift from the patch center by p = x + tx and q = y + ty (|tx |, |ty | < a/2). [sent-478, score-0.294]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('upscaling', 0.347), ('patch', 0.294), ('regression', 0.289), ('band', 0.186), ('artifacts', 0.167), ('cameraman', 0.149), ('freedman', 0.148), ('chip', 0.145), ('singular', 0.122), ('sharp', 0.121), ('external', 0.119), ('kid', 0.107), ('compression', 0.106), ('contaminated', 0.105), ('scaling', 0.105), ('fattal', 0.096), ('patches', 0.095), ('anchor', 0.09), ('xr', 0.084), ('yr', 0.081), ('frequency', 0.081), ('im', 0.079), ('approximation', 0.078), ('bicubic', 0.077), ('glasner', 0.077), ('ir', 0.075), ('origin', 0.072), ('inplace', 0.072), ('naturalness', 0.072), ('upsampled', 0.072), ('compensation', 0.072), ('conference', 0.069), ('superresolution', 0.066), ('jaggy', 0.064), ('interpolation', 0.064), ('matching', 0.06), ('examples', 0.06), ('regressions', 0.059), ('worthy', 0.059), ('thye', 0.059), ('upscale', 0.059), ('noise', 0.059), ('mapping', 0.056), ('selfsimilarity', 0.056), ('edges', 0.056), ('factor', 0.055), ('cohen', 0.055), ('transfer', 0.054), ('universal', 0.053), ('textures', 0.053), ('derivative', 0.053), ('practical', 0.052), ('priors', 0.051), ('proving', 0.051), ('downsampling', 0.049), ('society', 0.049), ('textured', 0.048), ('sharpness', 0.048), ('dictionary', 0.048), ('extremely', 0.048), ('sensor', 0.047), ('zoomed', 0.047), ('structures', 0.046), ('highresolution', 0.045), ('upsampling', 0.045), ('yi', 0.045), ('ime', 0.044), ('scale', 0.044), ('natural', 0.044), ('diverse', 0.043), ('adobe', 0.043), ('pairs', 0.043), ('ft', 0.042), ('refines', 0.041), ('comparisons', 0.041), ('overlapping', 0.041), ('validates', 0.04), ('fast', 0.04), ('invariance', 0.04), ('insufficient', 0.04), ('aw', 0.04), ('smooth', 0.039), ('lower', 0.039), ('missing', 0.039), ('factors', 0.038), ('produce', 0.038), ('realworld', 0.038), ('synthetic', 0.038), ('runs', 0.037), ('prior', 0.037), ('smoothness', 0.037), ('super', 0.037), ('selective', 0.037), ('yang', 0.036), ('transactions', 0.036), ('blurring', 0.036), ('ya', 0.035), ('primitives', 0.035), ('query', 0.035), ('approximate', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="166-tfidf-1" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>2 0.18762812 <a title="166-tfidf-2" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>Author: Maria Zontak, Inbar Mosseri, Michal Irani</p><p>Abstract: Recurrence of small clean image patches across different scales of a natural image has been successfully used for solving ill-posed problems in clean images (e.g., superresolution from a single image). In this paper we show how this multi-scale property can be extended to solve ill-posed problems under noisy conditions, such as image denoising. While clean patches are obscured by severe noise in the original scale of a noisy image, noise levels drop dramatically at coarser image scales. This allows for the unknown hidden clean patches to “naturally emerge ” in some coarser scale of the noisy image. We further show that patch recurrence across scales is strengthened when using directional pyramids (that blur and subsample only in one direction). Our statistical experiments show that for almost any noisy image patch (more than 99%), there exists a “good” clean version of itself at the same relative image coordinates in some coarser scale of the image.This is a strong phenomenon of noise-contaminated natural images, which can serve as a strong prior for separating the signal from the noise. Finally, incorporating this multi-scale prior into a simple denoising algorithm yields state-of-the-art denois- ing results.</p><p>3 0.17994674 <a title="166-tfidf-3" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>Author: unkown-author</p><p>Abstract: We tackle the problem of jointly increasing the spatial resolution and apparent measurement accuracy of an input low-resolution, noisy, and perhaps heavily quantized depth map. In stark contrast to earlier work, we make no use of ancillary data like a color image at the target resolution, multiple aligned depth maps, or a database of highresolution depth exemplars. Instead, we proceed by identifying and merging patch correspondences within the input depth map itself, exploiting patchwise scene self-similarity across depth such as repetition of geometric primitives or object symmetry. While the notion of ‘single-image ’ super resolution has successfully been applied in the context of color and intensity images, we are to our knowledge the first to present a tailored analogue for depth images. Rather than reason in terms of patches of 2D pixels as others have before us, our key contribution is to proceed by reasoning in terms of patches of 3D points, with matched patch pairs related by a respective 6 DoF rigid body motion in 3D. In support of obtaining a dense correspondence field in reasonable time, we introduce a new 3D variant of Patch- Match. A third contribution is a simple, yet effective patch upscaling and merging technique, which predicts sharp object boundaries at the target resolution. We show that our results are highly competitive with those of alternative techniques leveraging even a color image at the target resolution or a database of high-resolution depth exemplars.</p><p>4 0.12793395 <a title="166-tfidf-4" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>Author: Dong Chen, Xudong Cao, Fang Wen, Jian Sun</p><p>Abstract: Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. This prevents further exploration of the use of a highdimensional feature. In this paper, we study the performance of a highdimensional feature. We first empirically show that high dimensionality is critical to high performance. A 100K-dim feature, based on a single-type Local Binary Pattern (LBP) descriptor, can achieve significant improvements over both its low-dimensional version and the state-of-the-art. We also make the high-dimensional feature practical. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.</p><p>5 0.1263562 <a title="166-tfidf-5" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>Author: Xiaogang Chen, Sing Bing Kang, Jie Yang, Jingyi Yu</p><p>Abstract: Patch-based methods such as Non-Local Means (NLM) and BM3D have become the de facto gold standard for image denoising. The core of these approaches is to use similar patches within the image as cues for denoising. The operation usually requires expensive pair-wise patch comparisons. In this paper, we present a novel fast patch-based denoising technique based on Patch Geodesic Paths (PatchGP). PatchGPs treat image patches as nodes and patch differences as edge weights for computing the shortest (geodesic) paths. The path lengths can then be used as weights of the smoothing/denoising kernel. We first show that, for natural images, PatchGPs can be effectively approximated by minimum hop paths (MHPs) that generally correspond to Euclidean line paths connecting two patch nodes. To construct the denoising kernel, we further discretize the MHP search directions and use only patches along the search directions. Along each MHP, we apply a weightpropagation scheme to robustly and efficiently compute the path distance. To handle noise at multiple scales, we conduct wavelet image decomposition and apply PatchGP scheme at each scale. Comprehensive experiments show that our approach achieves comparable quality as the state-of-the-art methods such as NLM and BM3D but is a few orders of magnitude faster.</p><p>6 0.12300441 <a title="166-tfidf-6" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>7 0.10788668 <a title="166-tfidf-7" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>8 0.10547522 <a title="166-tfidf-8" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>9 0.10277064 <a title="166-tfidf-9" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>10 0.10206003 <a title="166-tfidf-10" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>11 0.10172585 <a title="166-tfidf-11" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>12 0.10094873 <a title="166-tfidf-12" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>13 0.098697267 <a title="166-tfidf-13" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>14 0.093091287 <a title="166-tfidf-14" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>15 0.08954794 <a title="166-tfidf-15" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>16 0.089458145 <a title="166-tfidf-16" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>17 0.089357436 <a title="166-tfidf-17" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>18 0.089140035 <a title="166-tfidf-18" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>19 0.088862985 <a title="166-tfidf-19" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>20 0.08841683 <a title="166-tfidf-20" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.219), (1, 0.028), (2, -0.035), (3, 0.099), (4, -0.002), (5, 0.055), (6, -0.007), (7, -0.006), (8, -0.005), (9, -0.066), (10, -0.011), (11, 0.035), (12, 0.01), (13, 0.009), (14, 0.055), (15, -0.073), (16, -0.054), (17, -0.057), (18, 0.133), (19, 0.023), (20, 0.063), (21, 0.126), (22, -0.052), (23, -0.152), (24, -0.034), (25, -0.005), (26, -0.097), (27, -0.145), (28, 0.037), (29, -0.099), (30, -0.088), (31, -0.136), (32, 0.042), (33, 0.034), (34, -0.026), (35, 0.061), (36, 0.007), (37, -0.061), (38, -0.108), (39, 0.001), (40, 0.049), (41, 0.031), (42, 0.011), (43, 0.015), (44, 0.045), (45, 0.061), (46, -0.002), (47, -0.003), (48, 0.002), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96195108 <a title="166-lsi-1" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>2 0.92396837 <a title="166-lsi-2" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>Author: Xiaogang Chen, Sing Bing Kang, Jie Yang, Jingyi Yu</p><p>Abstract: Patch-based methods such as Non-Local Means (NLM) and BM3D have become the de facto gold standard for image denoising. The core of these approaches is to use similar patches within the image as cues for denoising. The operation usually requires expensive pair-wise patch comparisons. In this paper, we present a novel fast patch-based denoising technique based on Patch Geodesic Paths (PatchGP). PatchGPs treat image patches as nodes and patch differences as edge weights for computing the shortest (geodesic) paths. The path lengths can then be used as weights of the smoothing/denoising kernel. We first show that, for natural images, PatchGPs can be effectively approximated by minimum hop paths (MHPs) that generally correspond to Euclidean line paths connecting two patch nodes. To construct the denoising kernel, we further discretize the MHP search directions and use only patches along the search directions. Along each MHP, we apply a weightpropagation scheme to robustly and efficiently compute the path distance. To handle noise at multiple scales, we conduct wavelet image decomposition and apply PatchGP scheme at each scale. Comprehensive experiments show that our approach achieves comparable quality as the state-of-the-art methods such as NLM and BM3D but is a few orders of magnitude faster.</p><p>3 0.91823858 <a title="166-lsi-3" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>Author: Maria Zontak, Inbar Mosseri, Michal Irani</p><p>Abstract: Recurrence of small clean image patches across different scales of a natural image has been successfully used for solving ill-posed problems in clean images (e.g., superresolution from a single image). In this paper we show how this multi-scale property can be extended to solve ill-posed problems under noisy conditions, such as image denoising. While clean patches are obscured by severe noise in the original scale of a noisy image, noise levels drop dramatically at coarser image scales. This allows for the unknown hidden clean patches to “naturally emerge ” in some coarser scale of the noisy image. We further show that patch recurrence across scales is strengthened when using directional pyramids (that blur and subsample only in one direction). Our statistical experiments show that for almost any noisy image patch (more than 99%), there exists a “good” clean version of itself at the same relative image coordinates in some coarser scale of the image.This is a strong phenomenon of noise-contaminated natural images, which can serve as a strong prior for separating the signal from the noise. Finally, incorporating this multi-scale prior into a simple denoising algorithm yields state-of-the-art denois- ing results.</p><p>4 0.80177677 <a title="166-lsi-4" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>Author: Wangmeng Zuo, Lei Zhang, Chunwei Song, David Zhang</p><p>Abstract: Image denoising is a classical yet fundamental problem in low level vision, as well as an ideal test bed to evaluate various statistical image modeling methods. One of the most challenging problems in image denoising is how to preserve the fine scale texture structures while removing noise. Various natural image priors, such as gradient based prior, nonlocal self-similarity prior, and sparsity prior, have been extensively exploited for noise removal. The denoising algorithms based on these priors, however, tend to smooth the detailed image textures, degrading the image visual quality. To address this problem, in this paper we propose a texture enhanced image denoising (TEID) method by enforcing the gradient distribution of the denoised image to be close to the estimated gradient distribution of the original image. A novel gradient histogram preservation (GHP) algorithm is developed to enhance the texture structures while removing noise. Our experimental results demonstrate that theproposed GHP based TEID can well preserve the texture features of the denoised images, making them look more natural.</p><p>5 0.78040105 <a title="166-lsi-5" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>Author: Ran Margolin, Ayellet Tal, Lihi Zelnik-Manor</p><p>Abstract: What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns, others on the colors, and several add high-level cues and priors. We propose a simple, yet powerful, algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.</p><p>6 0.7627818 <a title="166-lsi-6" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>7 0.70134336 <a title="166-lsi-7" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>8 0.6725949 <a title="166-lsi-8" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>9 0.62496328 <a title="166-lsi-9" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>10 0.61743712 <a title="166-lsi-10" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>11 0.60894573 <a title="166-lsi-11" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>12 0.59098667 <a title="166-lsi-12" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>13 0.5646528 <a title="166-lsi-13" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>14 0.5595938 <a title="166-lsi-14" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>15 0.55040395 <a title="166-lsi-15" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>16 0.54928088 <a title="166-lsi-16" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>17 0.54715556 <a title="166-lsi-17" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>18 0.53408802 <a title="166-lsi-18" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>19 0.52597696 <a title="166-lsi-19" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>20 0.52213657 <a title="166-lsi-20" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.117), (16, 0.025), (26, 0.032), (28, 0.016), (33, 0.296), (39, 0.026), (56, 0.206), (67, 0.058), (69, 0.045), (87, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89429712 <a title="166-lda-1" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>Author: Yi Zhang, Keigo Hirakawa</p><p>Abstract: We propose a notion of double discrete wavelet transform (DDWT) that is designed to sparsify the blurred image and the blur kernel simultaneously. DDWT greatly enhances our ability to analyze, detect, and process blur kernels and blurry images—the proposed framework handles both global and spatially varying blur kernels seamlessly, and unifies the treatment of blur caused by object motion, optical defocus, and camera shake. To illustrate the potential of DDWT in computer vision and image processing, we develop example applications in blur kernel estimation, deblurring, and near-blur-invariant image feature extraction.</p><p>same-paper 2 0.86978716 <a title="166-lda-2" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>3 0.86688912 <a title="166-lda-3" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>Author: Wongun Choi, Yu-Wei Chao, Caroline Pantofaru, Silvio Savarese</p><p>Abstract: Visual scene understanding is a difficult problem interleaving object detection, geometric reasoning and scene classification. We present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the 3D Geometric Phrase Model which captures the semantic and geometric relationships between objects whichfrequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings from a single image, while also improving individual object detections.</p><p>4 0.85309386 <a title="166-lda-4" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>5 0.85185283 <a title="166-lda-5" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>Author: Byung-soo Kim, Shili Xu, Silvio Savarese</p><p>Abstract: In this paper we focus on the problem of detecting objects in 3D from RGB-D images. We propose a novel framework that explores the compatibility between segmentation hypotheses of the object in the image and the corresponding 3D map. Our framework allows to discover the optimal location of the object using a generalization of the structural latent SVM formulation in 3D as well as the definition of a new loss function defined over the 3D space in training. We evaluate our method using two existing RGB-D datasets. Extensive quantitative and qualitative experimental results show that our proposed approach outperforms state-of-theart as methods well as a number of baseline approaches for both 3D and 2D object recognition tasks.</p><p>6 0.85047609 <a title="166-lda-6" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>7 0.84993941 <a title="166-lda-7" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>8 0.84802788 <a title="166-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.84755981 <a title="166-lda-9" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>10 0.84679544 <a title="166-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>11 0.84661877 <a title="166-lda-11" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>12 0.84617007 <a title="166-lda-12" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>13 0.8460567 <a title="166-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.84591764 <a title="166-lda-14" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>15 0.84549838 <a title="166-lda-15" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>16 0.84546447 <a title="166-lda-16" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>17 0.84535933 <a title="166-lda-17" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>18 0.84511083 <a title="166-lda-18" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>19 0.84480572 <a title="166-lda-19" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>20 0.84473085 <a title="166-lda-20" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
