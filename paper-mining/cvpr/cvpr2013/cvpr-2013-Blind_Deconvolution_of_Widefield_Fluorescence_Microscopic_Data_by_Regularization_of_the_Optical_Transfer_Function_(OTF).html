<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-65" href="#">cvpr2013-65</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</h1>
<br/><p>Source: <a title="cvpr-2013-65-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Keuper_Blind_Deconvolution_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>Reference: <a title="cvpr-2013-65-reference" href="../cvpr2013_reference/cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uni Abstract With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. [sent-2, score-0.532]
</p><p>2 Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. [sent-3, score-0.34]
</p><p>3 These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). [sent-4, score-0.128]
</p><p>4 For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. [sent-5, score-0.426]
</p><p>5 As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF. [sent-9, score-1.997]
</p><p>6 Introduction To analyze living cells, widefield fluorescence microscopy plays an important role, because it is prevalently available and, compared to confocal microscopy, has advantages concerning temporal resolution and phototoxicity. [sent-11, score-0.7]
</p><p>7 In contrast to confocal laser scanning microscopy, where the sample is scanned point by point, the 3D information of the specimen is recorded in a stack of 2D images. [sent-12, score-0.413]
</p><p>8 Since the whole specimen is illuminated for every image, light is al-  ways recorded from in-focus and from out-of-focus planes. [sent-13, score-0.372]
</p><p>9 The recorded out-of-focus light also determines the impulse response, the point-spread function (PSF) of the imaging system. [sent-15, score-0.11]
</p><p>10 The image formation of fluorescence microscopes can be approximated by a convolution ofthe original specimen function s with the PSF h and a voxelwise with the PSF makes the data hard to process, the aim is to correct for the effect of the PSF by a deconvolution technique. [sent-16, score-0.741]
</p><p>11 (1)  The objective function o is the recorded image data. [sent-18, score-0.097]
</p><p>12 As for widefield microscopy the convolution of the signal with the PSF makes the data hard to process, the aim is to correct for the effect of the PSF by a deconvolution technique. [sent-19, score-0.884]
</p><p>13 In principle, the PSF can be determined for each combination of objective, filters and light path in a specific microscope by a calibration procedure. [sent-20, score-0.105]
</p><p>14 However, it depends on many parameters, is subjected to changes in the recording system like thermal expansions, aberrations, or the optical properties of the recorded specimen. [sent-21, score-0.199]
</p><p>15 Therefore, it is beneficial to use the calibration outcome only as an initial estimate for blind image deconvolution, where the PSF h is refined while estimating the specimen s in a joint optimization procedure. [sent-22, score-0.373]
</p><p>16 222111777977  Deconvolution of 3D data from widefield microscopy is particularly hard because a part of the information is lost in the recording even if the perfect PSF was given [23]. [sent-23, score-0.608]
</p><p>17 Due to the so-called missing cone of the widefield recording system, certain spatial frequencies simply can not be captured. [sent-24, score-0.515]
</p><p>18 Related Work Image deconvolution in general is a vast topic with applications in the fields of microscopic image reconstruction, motion deblurring, or the deblurring of astronomic images. [sent-28, score-0.446]
</p><p>19 We refer to [23] for a survey on several deconvolution methods in widefield microscopy. [sent-30, score-0.723]
</p><p>20 In [3] several nonlinear deconvolution methods as the LucyRichardson method [17, 22], the Maximum Likelihood Expectation Maximization (MLEM) method [26], and a maximum entropy method [19] are described in detail. [sent-31, score-0.333]
</p><p>21 For the deconvolution of widefield microscopic data, MLEM deconvolution methods have proven to be very efficient [23, 10, 4, 14]. [sent-33, score-1.139]
</p><p>22 However, they require regularization of both the specimen function and the deconvolution kernel. [sent-34, score-0.661]
</p><p>23 Common methods use prior models on the specimen function such as the Tikhonov-Miller penalizer [30] or Total Variation (TV) regularization [5], enforcing smoothness in the deconvolved data and thus reducing noise. [sent-36, score-0.328]
</p><p>24 For the regularization of the reconstructed PSF, usually some specific prior knowledge on the image formation is used. [sent-38, score-0.096]
</p><p>25 In [11], an adaptive image deconvolution algorithm (AIDA) is presented: a blind deconvolution framework for 2D and 3D data. [sent-44, score-0.777]
</p><p>26 The basic assumption is that the PSF of the optical system is approximately known and either given in the form of the optical transfer function (OTF) or as several OTFs (computed for example from given PSF simulations), of which the true OTF is expected to be a linear combination. [sent-45, score-0.11]
</p><p>27 During the deconvolution process, the PSF estimation  (a)Airypatern(b)SimulatedPSF(xz-section)using[16, ]. [sent-46, score-0.333]
</p><p>28 As [14], [20] use the PSF model of [29] in the blind MLEM deconvolution framework. [sent-53, score-0.444]
</p><p>29 All the before mentioned deconvolution methods describe the PSF in the spatial domain. [sent-55, score-0.333]
</p><p>30 This has major disadvan-  tages in the case ofwidefield microscopy: the widefield PSF has an infinitely large support in the spatial domain and is not smooth (compare figure 2), such that discrete sampling of the PSF in the spatial domain leads to artifacts. [sent-56, score-0.51]
</p><p>31 In this paper, we present a regularization method of the deconvolution kernel, that is based on imposing constraints on the PSF in the frequency domain, where it is easy to describe and well localized. [sent-57, score-0.44]
</p><p>32 We thus propose to use a regularization of the Fourier transform of the deconvolution kernel, employing its frequency domain properties. [sent-58, score-0.469]
</p><p>33 Physical Model The PSF of a fluorescence microscope depends on several parameters of the recording system such as the Numerical Aperture, the emission wavelength λ, and the position of the recorded object. [sent-60, score-0.418]
</p><p>34 The Numerical Aperture is defined as NA = nimm · sin Θ, where nimm is the refractive index of the immersion m·seidniuΘm,, w ahnerde 2 nΘ is the angular aperture of the objective. [sent-61, score-0.09]
</p><p>35 Most importantly, the finite lens aperture introduces diffraction ring patterns in the recorded xy-sections [23], the so called Airy pattern (see figure 2(a)), that limits the resolution of the recording system. [sent-62, score-0.24]
</p><p>36 Because of the Airy pattern, the PSF of widefield microscopes does not have a compact support [6, 1] and usually has values of an unnegligible range in the whole image domain Ω. [sent-63, score-0.477]
</p><p>37 [29, 6, 1] or its Fourier domain equivalent, the optical transfer function (OTF) [27, 21, 24]. [sent-66, score-0.098]
</p><p>38 While other deconvolution methods focus on modeling the PSF, we show the advantages of working with the OTF. [sent-68, score-0.333]
</p><p>39 The OTF support of a widefield microscope is depicted in figure 3. [sent-71, score-0.497]
</p><p>40 The recorded light field is assumed to be nearly monochromatic with constant wavelength λ. [sent-72, score-0.166]
</p><p>41 Since only the light from a limited angle Θ can be recorded (figure 3(b)), the recorded information is limited to a “cap” of the spherical shell in the Fourier space (figure 3(c)). [sent-74, score-0.248]
</p><p>42 Blind  Maximum  Likelihood  Expectation  Maximization Deconvolution In the following, we briefly sketch the blind Maximum Likelihood Expectation Maximization Deconvolution (MLEM) algorithm. [sent-87, score-0.111]
</p><p>43 Since photon noise is Poisson distributed [2], the recorded objective function o(x) at voxel x can be viewed as a sample of a Poisson distribution with mean (s ∗ h) (x). [sent-90, score-0.136]
</p><p>44 Since the intensity at each vtroixbeulti oinn o i tsh d mraewann (frso m∗ h an independent P inotiesnssointy process, the objective function o given the specimen s and the PSF  ? [sent-91, score-0.281]
</p><p>45 Illustration of the support of the widefield OTF [9, 21]. [sent-141, score-0.424]
</p><p>46 (a) The spherical shell on which the Fourier transform of the monochromatic light field amplitudes is non-zero. [sent-142, score-0.112]
</p><p>47 (c) The spherical cap containing the recorded amplitude information. [sent-144, score-0.169]
</p><p>48 the specimen function update equivalent to the Lucy-Richardson deconvolution, has some desired properties such as conservation of the positivity of the radiant flux [3]. [sent-163, score-0.262]
</p><p>49 Rk can be denoised with any denoising function, such as wavelet denoising [28] or median filtering [14]. [sent-168, score-0.104]
</p><p>50 This allows for the computation o×f 3th×e r meseidduiaanl denoised objective function ¯o = sk ∗ hˆk + R¯k, which replaces o in the update scheme (5) and ∗(6). [sent-170, score-0.106]
</p><p>51 Regularization  In the blind deconvolution setting, the above ML formulation is ill-posed. [sent-173, score-0.444]
</p><p>52 (9)  exp(−λsPs (s))  and  p(h)  =  exp(−λhPh (h)) are general prior probability functeixopns( fλor the specimen function and the deconvolution kernel respectively. [sent-176, score-0.633]
</p><p>53 TV Regularization in the Frequency Domain For widefield recordings, we know that the PSF is not smooth, such that a regularization of the kernel by imposing smoothness is not reasonable. [sent-208, score-0.494]
</p><p>54 We therefore enforce a limited support by shrinking all values outside the support region to zero  [10]. [sent-227, score-0.087]
</p><p>55 To make sure that valid frequencies are not cut off, we compute the largest theoretically possible OTF support for a widefield microscope according to [21]. [sent-228, score-0.541]
</p><p>56 The OTF is largest for the smallest possible emission wavelength and the largest possible sin(Θ) where Θ is the angle of the maximum cone of light that can enter the objective lens (see section 2, figure 3). [sent-229, score-0.185]
</p><p>57 Values outside this support are set to zero −in every iteration of the deconvolution update. [sent-232, score-0.386]
</p><p>58 For minimizing the resulting functional with TV regularization of the kernel in the frequency domain (KFTV), we compute the gradient with the calculus of variations. [sent-239, score-0.174]
</p><p>59 If this array is too small, the reconstructed PSF can have non-zero values at the array boundaries such that zeropadding for the convolutions introduces artifacts. [sent-274, score-0.1]
</p><p>60 In our implementation, we initially extend the array by padding it with zeros, assuming that the initial PSF estimate drops to zero at the array boundaries. [sent-275, score-0.089]
</p><p>61 During the deconvolution process, we update the PSF in the whole, extended array, even though we know that this might again lead to aliasing effects after applying a circular convolution, if the PSF grows too large. [sent-276, score-0.333]
</p><p>62 Experiments and Results We conducted experiments on synthetic data as well as on real microscopic recordings. [sent-279, score-0.107]
</p><p>63 counts for different scalings of  the data and has previously been used for the evaluation of state-of-the-art deconvolution methods (e. [sent-300, score-0.333]
</p><p>64 It consists of two widefield microscopic recordings with different levels of Poisson noise of a synthetic HeLa cell nucleus (sim. [sent-306, score-0.685]
</p><p>65 KIP [15] uses a simple intensity regularization on the reconstructed PSF preventing the reconstructed kernel from collapsing. [sent-311, score-0.164]
</p><p>66 Residual denoising (RD) has been used to regularize the specimen function. [sent-313, score-0.297]
</p><p>67 As expected, a kernel regularization in the frequency domain has clear advantages over the simple KIP [15] baseline, both visually and in terms of RMSE. [sent-314, score-0.174]
</p><p>68 The development of the RMSE of the reconstructed specimen function over 2The dataset uni -fre iburg  . [sent-318, score-0.316]
</p><p>69 KFTV blind deconvolution results on the synthetic datasets sim. [sent-329, score-0.468]
</p><p>70 The reduced blur in z-direction obtained by OTF mask and KFTV shows the advantage of constraining the kernel function in the frequency domain. [sent-335, score-0.127]
</p><p>71 [14] perform blind MLEM deconvolution with RD in the specimen function and in the kernel update. [sent-348, score-0.744]
</p><p>72 Toy data used in [14] and the deconvolution results with the different methods AIDA [11], kernel PCA [14], and our proposed method KFTV. [sent-360, score-0.371]
</p><p>73 For the comparison to [14] and [11], we also use RD for the specimen function and the kernel update. [sent-367, score-0.3]
</p><p>74 Drosophila S2 Cell Recordings  Real widefield microscopic recordings were taken from fixed samples of DAPI stained Drosophila S2 cell nuclei. [sent-375, score-0.601]
</p><p>75 Blind deconvolution results on a Drosophila S2 cell nucleus recording shown in the central xy-section (top row) and the yz-section indicated by the yellow line (bottom row). [sent-387, score-0.486]
</p><p>76 The average of the bead recordings contains less noise than the single bead recordings and can therefore be used as initial PSF estimate. [sent-391, score-0.29]
</p><p>77 The identical Drosophila S2 cell nuclei were recorded with a spinning disk confocal microscope for comparison. [sent-392, score-0.37]
</p><p>78 The spinning disk recording was taken with a voxel size of 0. [sent-393, score-0.168]
</p><p>79 very small, this data can be used as pseudo ground truth to evaluate the deconvolution result from the widefield data. [sent-399, score-0.723]
</p><p>80 The whole dataset consists of 22 recorded nuclei that were cropped from five original recordings into separate volumes of about 100 100 100 voxels before tsheep adraetceon vvoolluumtieosn. [sent-401, score-0.217]
</p><p>81 o T abheo ucte n1t0r0al × ×sl 1ic0e0 a ×nd 1 an vxozx-seelsct iboenfo roef the spinning disk recording of one nucleus after registration is shown in figure 9(a), the same views of the widefield recording are displayed in figure 9(b). [sent-402, score-0.681]
</p><p>82 As expected, the results with the TV-regularized specimen function are smoother than the results with RD, such that very fine structures in the central xy-plane are not restored. [sent-406, score-0.262]
</p><p>83 In terms of RMSE, the TV regularized OTF mask yields better results than the residual denoised OTF mask. [sent-407, score-0.098]
</p><p>84 With KFTV, residual denoising is sufficient to suppress noise in the reconstructed specimen function, yielding clearer structures in the reconstructed specimen and a lower RMSE. [sent-408, score-0.675]
</p><p>85 To evaluate the influence of the parameter choice on the real dataset, we have computed the KFTV deconvolution with RD for different parameters λh on the recording from 9(b). [sent-409, score-0.413]
</p><p>86 KFTV results on the 22 Drosophila S2 cell nucleus recordings in terms of RMSE. [sent-412, score-0.168]
</p><p>87 The resulting RMSE with λh = 4 on the whole dataset of 22 nuclei with RD and TV regularization of the specimen function is given in figure 10. [sent-414, score-0.372]
</p><p>88 With KFTV and a TV-regularized specimen function, the average RMSE could be decreased from 23. [sent-415, score-0.262]
</p><p>89 Conclusion We have presented a blind deconvolution method for widefield microscopic data that exploits the frequency domain properties of the widefield PSF. [sent-420, score-1.377]
</p><p>90 Accounting for the fact that the widefield OTF has a limited support and varies smoothly within this support, the presented KFTV method is based on a TV prior on the OTF values and a mask indicating the maximally possible OTF support. [sent-421, score-0.452]
</p><p>91 KFTV outperforms state-of-the-art deconvolution methods both visually and in terms of RMSE on two evaluation datasets, while it is independent from specific PSF simulation methods. [sent-422, score-0.333]
</p><p>92 On a new dataset of real widefield microscopic recordings showing Drosophila S2 cell nuclei, KFTV also shows convincing results. [sent-423, score-0.601]
</p><p>93 Fast regularization technique for expectation maximization algorithm for optical sectioning microscopy. [sent-446, score-0.15]
</p><p>94 Richardson-Lucy algorithm with total variation regularization for 3D confocal microscope deconvolution. [sent-463, score-0.212]
</p><p>95 Sevenfold improvement of axial resolution in 3D widefield microscopy using two objective lenses. [sent-498, score-0.547]
</p><p>96 Aida: an adaptive image deconvolution algorithm with application to multi-frame and three-dimensional data. [sent-517, score-0.333]
</p><p>97 Blind image deconvolution using machine learning for three-dimensional microscopy. [sent-536, score-0.333]
</p><p>98 Parametric blind deconvolution: a robust method for the simultaneous estimation of image and blur. [sent-568, score-0.111]
</p><p>99 Comparison of widefield/deconvolution and confocal microscopy in three-dimensional imaging. [sent-622, score-0.211]
</p><p>100 Efficient superresolution restoration algorithms using maximum a posteriori estimations with applicationto fluorescence microscopy. [sent-677, score-0.099]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('psf', 0.421), ('widefield', 0.39), ('deconvolution', 0.333), ('otf', 0.312), ('rmse', 0.287), ('kftv', 0.282), ('specimen', 0.262), ('microscopy', 0.138), ('blind', 0.111), ('kip', 0.108), ('mlem', 0.108), ('fluorescence', 0.099), ('recordings', 0.095), ('microscopic', 0.083), ('drosophila', 0.081), ('recording', 0.08), ('recorded', 0.078), ('confocal', 0.073), ('microscope', 0.073), ('regularization', 0.066), ('tv', 0.059), ('fourier', 0.057), ('aida', 0.054), ('sk', 0.053), ('emission', 0.052), ('snr', 0.044), ('nuclei', 0.044), ('spinning', 0.044), ('optical', 0.041), ('frequency', 0.041), ('rd', 0.04), ('bead', 0.04), ('hph', 0.04), ('jmlem', 0.04), ('kam', 0.04), ('amplitude', 0.04), ('nucleus', 0.04), ('psfs', 0.04), ('kernel', 0.038), ('aperture', 0.036), ('airy', 0.036), ('wavelength', 0.036), ('residual', 0.036), ('array', 0.035), ('denoising', 0.035), ('support', 0.034), ('denoised', 0.034), ('cell', 0.033), ('shell', 0.033), ('sps', 0.033), ('poisson', 0.033), ('light', 0.032), ('hk', 0.031), ('deblurring', 0.03), ('reconstructed', 0.03), ('domain', 0.029), ('mask', 0.028), ('transfer', 0.028), ('infinitely', 0.028), ('spherical', 0.027), ('agard', 0.027), ('kenig', 0.027), ('keuper', 0.027), ('nimm', 0.027), ('verveer', 0.027), ('multiplicative', 0.027), ('div', 0.027), ('lens', 0.025), ('disk', 0.025), ('rk', 0.024), ('synthetic', 0.024), ('cap', 0.024), ('iburg', 0.024), ('conical', 0.024), ('fftw', 0.024), ('microscopes', 0.024), ('otfs', 0.024), ('frequencies', 0.024), ('numerical', 0.023), ('convolution', 0.023), ('biomedical', 0.023), ('displayed', 0.022), ('expectation', 0.022), ('freiburg', 0.022), ('aberrations', 0.022), ('cone', 0.021), ('maximization', 0.021), ('diffraction', 0.021), ('isbi', 0.021), ('blur', 0.02), ('cut', 0.02), ('penalized', 0.02), ('biological', 0.02), ('noise', 0.02), ('kz', 0.02), ('monochromatic', 0.02), ('voxel', 0.019), ('objective', 0.019), ('zero', 0.019), ('tomography', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="65-tfidf-1" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>2 0.34059647 <a title="65-tfidf-2" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>Author: James Gregson, Felix Heide, Matthias B. Hullin, Mushfiqur Rouf, Wolfgang Heidrich</p><p>Abstract: We present a novel stochastic framework for non-blind deconvolution based on point samples obtained from random walks. Unlike previous methods that must be tailored to specific regularization strategies, the new Stochastic Deconvolution method allows arbitrary priors, including nonconvex and data-dependent regularizers, to be introduced and tested with little effort. Stochastic Deconvolution is straightforward to implement, produces state-of-the-art results and directly leads to a natural boundary condition for image boundaries and saturated pixels.</p><p>3 0.17521016 <a title="65-tfidf-3" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>4 0.16149221 <a title="65-tfidf-4" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>5 0.14727588 <a title="65-tfidf-5" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>6 0.1308137 <a title="65-tfidf-6" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>7 0.11492939 <a title="65-tfidf-7" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>8 0.097436741 <a title="65-tfidf-8" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>9 0.083080702 <a title="65-tfidf-9" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>10 0.082308248 <a title="65-tfidf-10" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>11 0.077436157 <a title="65-tfidf-11" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>12 0.068498611 <a title="65-tfidf-12" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>13 0.068280622 <a title="65-tfidf-13" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>14 0.065019958 <a title="65-tfidf-14" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>15 0.047438346 <a title="65-tfidf-15" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>16 0.045694735 <a title="65-tfidf-16" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>17 0.04439608 <a title="65-tfidf-17" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>18 0.043774754 <a title="65-tfidf-18" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>19 0.042808641 <a title="65-tfidf-19" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<p>20 0.042176586 <a title="65-tfidf-20" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.083), (1, 0.097), (2, -0.023), (3, 0.076), (4, -0.047), (5, 0.171), (6, 0.01), (7, -0.015), (8, 0.009), (9, -0.001), (10, -0.004), (11, -0.012), (12, 0.002), (13, -0.047), (14, -0.06), (15, 0.03), (16, 0.054), (17, -0.017), (18, 0.049), (19, 0.031), (20, -0.028), (21, 0.048), (22, -0.016), (23, 0.024), (24, -0.007), (25, 0.012), (26, 0.009), (27, -0.061), (28, 0.015), (29, 0.008), (30, 0.066), (31, -0.008), (32, -0.085), (33, -0.043), (34, -0.014), (35, -0.058), (36, -0.002), (37, -0.011), (38, -0.018), (39, -0.004), (40, -0.021), (41, 0.013), (42, -0.026), (43, -0.069), (44, 0.005), (45, 0.017), (46, -0.003), (47, 0.001), (48, -0.035), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93976629 <a title="65-lsi-1" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>2 0.87538332 <a title="65-lsi-2" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>Author: James Gregson, Felix Heide, Matthias B. Hullin, Mushfiqur Rouf, Wolfgang Heidrich</p><p>Abstract: We present a novel stochastic framework for non-blind deconvolution based on point samples obtained from random walks. Unlike previous methods that must be tailored to specific regularization strategies, the new Stochastic Deconvolution method allows arbitrary priors, including nonconvex and data-dependent regularizers, to be introduced and tested with little effort. Stochastic Deconvolution is straightforward to implement, produces state-of-the-art results and directly leads to a natural boundary condition for image boundaries and saturated pixels.</p><p>3 0.76060331 <a title="65-lsi-3" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>4 0.75746131 <a title="65-lsi-4" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>5 0.71164799 <a title="65-lsi-5" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>Author: Li Xu, Shicheng Zheng, Jiaya Jia</p><p>Abstract: We show in this paper that the success of previous maximum a posterior (MAP) based blur removal methods partly stems from their respective intermediate steps, which implicitly or explicitly create an unnatural representation containing salient image structures. We propose a generalized and mathematically sound L0 sparse expression, together with a new effective method, for motion deblurring. Our system does not require extra filtering during optimization and demonstrates fast energy decreasing, making a small number of iterations enough for convergence. It also provides a unifiedframeworkfor both uniform andnon-uniform motion deblurring. We extensively validate our method and show comparison with other approaches with respect to convergence speed, running time, and result quality.</p><p>6 0.70628899 <a title="65-lsi-6" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>7 0.70193052 <a title="65-lsi-7" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>8 0.69402808 <a title="65-lsi-8" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>9 0.61039829 <a title="65-lsi-9" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>10 0.48346734 <a title="65-lsi-10" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>11 0.45884085 <a title="65-lsi-11" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>12 0.45729703 <a title="65-lsi-12" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>13 0.42890218 <a title="65-lsi-13" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>14 0.3946352 <a title="65-lsi-14" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>15 0.39307052 <a title="65-lsi-15" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<p>16 0.38348162 <a title="65-lsi-16" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>17 0.36707518 <a title="65-lsi-17" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>18 0.36621302 <a title="65-lsi-18" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>19 0.36106351 <a title="65-lsi-19" href="./cvpr-2013-Auxiliary_Cuts_for_General_Classes_of_Higher_Order_Functionals.html">51 cvpr-2013-Auxiliary Cuts for General Classes of Higher Order Functionals</a></p>
<p>20 0.35559487 <a title="65-lsi-20" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.151), (16, 0.028), (26, 0.043), (33, 0.154), (57, 0.017), (67, 0.028), (69, 0.026), (79, 0.318), (80, 0.011), (87, 0.063), (90, 0.022), (96, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73063713 <a title="65-lda-1" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>Author: Margret Keuper, Thorsten Schmidt, Maja Temerinac-Ott, Jan Padeken, Patrick Heun, Olaf Ronneberger, Thomas Brox</p><p>Abstract: With volumetric data from widefield fluorescence microscopy, many emerging questions in biological and biomedical research are being investigated. Data can be recorded with high temporal resolution while the specimen is only exposed to a low amount of phototoxicity. These advantages come at the cost of strong recording blur caused by the infinitely extended point spread function (PSF). For widefield microscopy, its magnitude only decays with the square of the distance to the focal point and consists of an airy bessel pattern which is intricate to describe in the spatial domain. However, the Fourier transform of the incoherent PSF (denoted as Optical Transfer Function (OTF)) is well localized and smooth. In this paper, we present a blind -fre iburg .de Figure 1. As for widefield microscopy the convolution ofthe signal deconvolution method that improves results of state-of-theart deconvolution methods on widefield data by exploiting the properties of the widefield OTF.</p><p>2 0.62712723 <a title="65-lda-2" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>Author: Ming Zeng, Jiaxiang Zheng, Xuan Cheng, Xinguo Liu</p><p>Abstract: This paper presents a method for quasi-rigid objects modeling from a sequence of depth scans captured at different time instances. As quasi-rigid objects, such as human bodies, usually have shape motions during the capture procedure, it is difficult to reconstruct their geometries. We represent the shape motion by a deformation graph, and propose a model-to-partmethod to gradually integrate sampled points of depth scans into the deformation graph. Under an as-rigid-as-possible assumption, the model-to-part method can adjust the deformation graph non-rigidly, so as to avoid error accumulation in alignment, which also implicitly achieves loop-closure. To handle the drift and topological error for the deformation graph, two algorithms are introduced. First, we use a two-stage registration to largely keep the rigid motion part. Second, in the step of graph integration, we topology-adaptively integrate new parts and dynamically control the regularization effect of the deformation graph. We demonstrate the effectiveness and robustness of our method by several depth sequences of quasi-rigid objects, and an application in human shape modeling.</p><p>3 0.57153922 <a title="65-lda-3" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>4 0.57104754 <a title="65-lda-4" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>5 0.56792253 <a title="65-lda-5" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>6 0.56729454 <a title="65-lda-6" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>7 0.56488609 <a title="65-lda-7" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>8 0.56425583 <a title="65-lda-8" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>9 0.56333232 <a title="65-lda-9" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>10 0.5623123 <a title="65-lda-10" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>11 0.56213915 <a title="65-lda-11" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>12 0.56083858 <a title="65-lda-12" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>13 0.55661952 <a title="65-lda-13" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>14 0.55635279 <a title="65-lda-14" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>15 0.55526447 <a title="65-lda-15" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>16 0.55480158 <a title="65-lda-16" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>17 0.55135971 <a title="65-lda-17" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>18 0.55111545 <a title="65-lda-18" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>19 0.55054617 <a title="65-lda-19" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>20 0.54979706 <a title="65-lda-20" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
