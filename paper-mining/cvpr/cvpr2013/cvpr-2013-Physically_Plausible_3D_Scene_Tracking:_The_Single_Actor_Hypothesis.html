<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-331" href="#">cvpr2013-331</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</h1>
<br/><p>Source: <a title="cvpr-2013-331-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kyriazis_Physically_Plausible_3D_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>Reference: <a title="cvpr-2013-331-reference" href="../cvpr2013_reference/cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. [sent-5, score-0.326]
</p><p>2 Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. [sent-6, score-1.125]
</p><p>3 We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. [sent-7, score-0.747]
</p><p>4 This work focuses on a scenario, where the hand of a human actor interacts with a number of objects placed on a table. [sent-12, score-0.507]
</p><p>5 A key observation is that in such a scenario, the human hand is the single actor and scene state changes can be attributed to the actions of the human hand and their induced consequences. [sent-16, score-0.908]
</p><p>6 Thus, we model the dynamics of a hand interacting with a physical world. [sent-18, score-0.771]
</p><p>7 Moreover, 999  (a)(b)(c) Figure 1: The exploitation of the single actor hypothesis through physics modelling, allows physically plausi-  ble, heuristic-free 3D tracking of hand-object interactions. [sent-19, score-0.796]
</p><p>8 (b), (c) By searching for hand motion only, we are able to track the 3D state of the entire scene. [sent-21, score-0.523]
</p><p>9 The state can be overt (partially visible hand and objects (b)) or even covert (totally occluded objects like the ball-inside-the-cup (c)). [sent-22, score-0.438]
</p><p>10 we make a distinction between active and passive entities, to come up with effective, physically plausible interpretations of scenes, exhibiting complex hand-object(s) interaction. [sent-23, score-0.327]
</p><p>11 We monitor the 3D state of the scene by means of tracking (Fig. [sent-24, score-0.391]
</p><p>12 The objective function is a quantification of the discrepancy between a given hypothesis over the scene state and observations, and is parameterized over a hand motion between two successive instants in time. [sent-26, score-0.792]
</p><p>13 A hypothesized hand motion is simulated in a physics-based simulation environment that reflects the latest state of the scene, as it has been tracked up to that point in time. [sent-27, score-0.709]
</p><p>14 The simulated handobject(s) interaction yields an expectation over the appearance of such a hypothesis, that regards both the hand and the object(s). [sent-28, score-0.492]
</p><p>15 A comparison of this expectation to actual observations quantifies the compatibility of the hand motion hypothesis to the data. [sent-29, score-0.648]
</p><p>16 A highly preferable hypothesis is one that explains (a) where the hand is in the new tracking frame and (b) the consequences of its interaction with the scene, as those are reflected in the observations. [sent-30, score-0.677]
</p><p>17 The expectation and comparison mechanisms are implemented as  a forward model that accounts for the dynamics and the appearance of a scene. [sent-31, score-0.403]
</p><p>18 This model is turned into an inference mechanism over the physical state of the scene by means of black-box optimization. [sent-32, score-0.368]
</p><p>19 Relevant work Our work aims at deriving physically plausible interpretations of the interaction of a human with the environment. [sent-37, score-0.371]
</p><p>20 [9] used the notion of physical stability to hypothesize physically plausible 3D scene interpretations. [sent-48, score-0.512]
</p><p>21 Several other approaches consider dynamics explicitly, but restrict understanding to either the actor or a single object, only. [sent-49, score-0.492]
</p><p>22 [23] modeled the dynamics of the golf swing motion to track golf swings in 3D from a single camera. [sent-52, score-0.539]
</p><p>23 [24] fused motion planning and contact dynamics to track humans from multiple cameras and a ground assumption. [sent-55, score-0.481]
</p><p>24 Duff and Wyatt [8] used physical simulation and search heuristics to track a fast moving ball, despite occlusions and for the 2D case. [sent-61, score-0.415]
</p><p>25 In previous work [13], we performed 3D motion estimation for a bouncing ball, from a single camera and despite severe occlusions by exploiting dynamics modelling. [sent-62, score-0.421]
</p><p>26 Ye and Liu [26] synthesized physically plausible hand movements, from pour or absent hand observations, that explained the manipulation of objects with known trajectories from a hand whose rough lo-  cation was also known. [sent-63, score-1.041]
</p><p>27 There are also approaches that go beyond abstractions of dynamics while considering ensembles of entities rather than entities in isolation. [sent-64, score-0.524]
</p><p>28 Salzmann and Urtasun [22] approached the problem of 3D tracking by attributing motion of parts to net forces that act upon them at each tracking frame. [sent-67, score-0.56]
</p><p>29 In previous work we tracked the constellation of a hand and an object from multiple cameras [17], and the full articulation of two interacting hands from a RGBD sensor [18], all in 3D, by employing synthetic 3D models. [sent-72, score-0.426]
</p><p>30 While a hand transports an object, we do not directly perceive the hand touching it. [sent-84, score-0.5]
</p><p>31 Our recently proposed method in [17] demonstrates multicamera-based joint hand-object tracking that is performed based on two criteria: (a) the appearance of the hand-object ensemble matches the observations and (b) the 111000 hand does not share the same space as the object. [sent-88, score-0.537]
</p><p>32 The laws of physics guarantee that a ball that is trapped between a cup and the table, has to travel inside the cup being moved by a hand. [sent-103, score-0.523]
</p><p>33 Instead, within our framework and as long as the single actor hypothesis holds, tracking scenes of different cardinalities does not alter the dimensionality of the problem. [sent-107, score-0.562]
</p><p>34 Methodology Dynamics, as a rich and powerful modelling tool, consti-  tutes an excellent framework where the single actor hypothesis is naturally expressed. [sent-109, score-0.462]
</p><p>35 Additionally, it is powerful because the predictive power of dynamics is the most elaborate reflection of how entities interact in a truly physical world. [sent-111, score-0.554]
</p><p>36 A hand motion is sought that best explains the evolution of a scene between two consecutive time instants t and t 1. [sent-113, score-0.536]
</p><p>37 Hand motion is parameterized as the transition from a reference hand pose ht (e. [sent-114, score-0.63]
</p><p>38 As new observations arrive, a new tracking frame is defined, for which the tracking solution is established by a hypothesizeand-test fashion, driven by Particle Swarm Optimization (PSO) [11]. [sent-117, score-0.486]
</p><p>39 Hypotheses of hand motion are tested in a physics-based simulation environment and the outlook of the induced scene state is rendered into maps that are comparable to the observations. [sent-118, score-0.782]
</p><p>40 The sought solution is a physically plausible scene interpretation that is most compatible with the observations. [sent-120, score-0.336]
</p><p>41 Forward model We use a forward model that regards the physical state of a scene and its appearance, as observed by a camera. [sent-123, score-0.492]
</p><p>42 Given a hand motion, forward modelling produces two different outputs. [sent-127, score-0.422]
</p><p>43 First, through dynamics simulation, it updates the poses and velocities of objects, as these have been altered due to the hypothesized hand motion. [sent-128, score-0.569]
</p><p>44 Second, the resulting scene state is rendered so that a direct comparison between hypotheses and actual observations is possible. [sent-129, score-0.429]
</p><p>45 All entities are represented in a dynamics simulator (Bullet [6]). [sent-133, score-0.455]
</p><p>46 Entities are essentially represented as 3D shapes with inertia tensors, masses, friction and restitution coefficients. [sent-134, score-0.332]
</p><p>47 Still, the selected simulator can generate realistic dynamic behaviour, which is the key in extracting physically plausible scene interpretations. [sent-139, score-0.413]
</p><p>48 The collision spheres (green) inside the hand model give it physical substance. [sent-152, score-0.772]
</p><p>49 The hand is able to change the state of the scene by means of forces that are the result of its accelerated surface contacting the surface of the objects. [sent-156, score-0.495]
</p><p>50 We approximate the effective surface of the hand by a compound of spheres that are strategically inscribed at various locations inside the 3D volume of the hand’s structure (Fig. [sent-157, score-0.394]
</p><p>51 If sk is the k-th sphere of the collision model and its 3D position is given through the application of the kinematics function Kk (h) for a hand pose h, then for a hand motion from ht to ht+1, sk is given a velocity vk = (Kk (ht+1) − Kk (ht)) /Δt. [sent-159, score-1.273]
</p><p>52 The spheres of the hand’s collision model are not allowed to rotate, so that all tangential collision energy is transferred to the colliding object and is not spent on the rotation of the spheres, too. [sent-161, score-0.548]
</p><p>53 The collisions among the spheres are ignored so as to better approximate the flexibility of the hand’s surface, by accounting for the whole hand collision model as a union rather as a collection of independent entities. [sent-163, score-0.596]
</p><p>54 By modulating the mass and friction coefficient of the spheres the hand becomes less/more capable of manipulating heavy or slippery objects. [sent-164, score-0.631]
</p><p>55 No gravitational force is assigned to the spheres as it is assumed to  always be eliminated by the torques of the hand joints. [sent-165, score-0.482]
</p><p>56 For a hand motion, a given state of the rest of the scene and a time step, simulation of dynamics is responsible for evolving the scene into a new, physically plausible state. [sent-166, score-1.199]
</p><p>57 The hand spheres bare kinetic energy and transfer that energy, through collision, to other objects. [sent-167, score-0.459]
</p><p>58 Dynamics simulation is responsible for applying collision checking, force direction estimation and preservation of energy and momentum in order to transform the old scene state to the new one. [sent-168, score-0.539]
</p><p>59 (2), si is the collision shape, mi is the mass, Ii is the inertia tensor, Fi is the friction coefficient, Ri is the restitution coefficient, p? [sent-175, score-0.534]
</p><p>60 2  Appearance model  Every hand motion hypothesis yields a new expectation over the physical state of the scene. [sent-182, score-0.842]
</p><p>61 A hypothesis scores well when its simulated expectations over the scene evolution match the new observations well. [sent-184, score-0.414]
</p><p>62 For a hand motion hypothesis h (c), a synthetic depth map Idr is rendered (e). [sent-201, score-0.636]
</p><p>63 Inference In order to infer total state change from new observations  we formulate an optimization problem, which we solve for the hand motion alone. [sent-207, score-0.553]
</p><p>64 All scene changes are attributed to hand intervention, which, in optimization terms, amounts to 27 parameters. [sent-208, score-0.336]
</p><p>65 Thus, at any tracking iteration at time t, given (a) the hand position ht and (b) the state of the scene St, we seek for a new hand pose ht+1 defined as ht+1 ? [sent-209, score-1.122]
</p><p>66 (5) ht+1 must be such that the motion of the hand from ht to ht+1 best explains the observed evolution of the scene. [sent-211, score-0.584]
</p><p>67 Function E defines a penalty to be minimized over hand motion hypotheses. [sent-214, score-0.359]
</p><p>68 However, we need to penalize for hand motions that contain inter-penetrations of distinct hand subparts (e. [sent-224, score-0.5]
</p><p>69 CM (h) where function CM provides the collision check pairs for the sub-parts, sk is the k-th collision element and function PD computes pair-wise penetration depth, that is com-  puted by the simulator. [sent-229, score-0.49]
</p><p>70 The data term D combines equations (1), (3) and (4) to quantify the difference between the observation of a scene and the expected outcome of a hand motion hypothesis: D (h? [sent-230, score-0.445]
</p><p>71 Every invocation involves 3D rendering and dynamics simulation, both being computationally demanding tasks. [sent-262, score-0.341]
</p><p>72 GPU architectures are used in order to accelerate rendering and multicore CPU architectures are exploited for the acceleration of dynamics simulation for each PSO generation. [sent-263, score-0.486]
</p><p>73 A hypothesis of a new  hand pose ht amounts to a relative motion with respect to 1 13 3 ht−1 . [sent-274, score-0.737]
</p><p>74 The hypothesis that optimally explains the evolution of a scene in terms of appearance is dubbed as the tracking solution for the current frame. [sent-283, score-0.469]
</p><p>75 The scene state that accompanies the winning hypothesis replaces St−1 for the next tracking frame. [sent-284, score-0.577]
</p><p>76 We used GPU threads for 3D rendering and objective evaluation and CPU threads  ×  for dynamics simulation. [sent-287, score-0.341]
</p><p>77 Acquisition was performed at a 30fps rate and therefore the dynamics simulation time interval was set to Δt = 1/30s. [sent-291, score-0.421]
</p><p>78 The mass of the hand model collision spheres was set to 1 and the friction factor to 10. [sent-298, score-0.833]
</p><p>79 Quantitative evaluation For the problem of scene tracking and when a hand is involved, it is very difficult to acquire ground truth information. [sent-303, score-0.535]
</p><p>80 A human hand grasped a cup firmly, lifted it and moved it around in various angles. [sent-433, score-0.448]
</p><p>81 In a sequence of 500 frames, the hand grasped the cup firmly in the last 370 frames (see the 1st column of Fig. [sent-434, score-0.492]
</p><p>82 By construction, the pose of the hand was correlated with the pose of the cup. [sent-436, score-0.336]
</p><p>83 We tracked this scene and thus gained access to the inferred poses of the hand and the cup. [sent-437, score-0.336]
</p><p>84 In the synthetic experiments even low budgets suffice for adequately accurate tracking of both the hand and the object. [sent-459, score-0.514]
</p><p>85 The (correct) motion of all objects was inferred as a consequence ofthe hand’s motion that best explained the observations in total. [sent-471, score-0.347]
</p><p>86 The first experiment considered a hand and a cup (2nd column of Fig. [sent-476, score-0.409]
</p><p>87 At this footage the hand picked up the cup and put it back on the table in an upside-down orientation. [sent-478, score-0.409]
</p><p>88 In the second experiment a hand lifted and manipulated a plastic bowling ball that was barely graspable due to its size (3rd column of Fig. [sent-481, score-0.351]
</p><p>89 Given enough friction, our hand modelling was able to explain the lifting and manipulation of the object. [sent-483, score-0.349]
</p><p>90 Even when almost the entire hand was occluded by the ball we came up with plausible hypotheses. [sent-484, score-0.471]
</p><p>91 One cup trapped the ball and was moved around, moving the ball inside it and pushing other cups when in its way. [sent-489, score-0.429]
</p><p>92 The hand pushed an empty cup, which in turn pushed the cup containing the ball. [sent-491, score-0.409]
</p><p>93 As the hand shuffled the cups intense occlusions occurred that did not prevent our framework from maintaining plausible hypotheses about the 3D position and orientation of the fully/partially occluded hand, cups and of the truly invisible ball. [sent-492, score-0.642]
</p><p>94 This scenario challenged both the dynamics modelling and the optimization module, because stacking of generic geometry is indeed a difficult problem for dynamics simulators to handle stably, which in turn yields an erratic behaviour in the objective function. [sent-496, score-0.651]
</p><p>95 Summary In this work we enabled the efficient 3D tracking of complex scenes by exploiting the single actor hypothesis. [sent-499, score-0.415]
</p><p>96 To achieve this, we proposed the use of a dynamics model (physics simulator) and a appearance model (3D rendering) as a powerful, combined forward model, that is turned  into an inference mechanism by means of black-box optimization. [sent-500, score-0.349]
</p><p>97 A natural extension of this work would be to consider a wider observation horizon in order to tackle cases where the hand is not constantly observed to manipulate objects but only initiates motion by passing kinetic energy. [sent-504, score-0.465]
</p><p>98 Notably, the single actor hypothesis does not constrain the actor to be single but only that all source of state change is directly and efficiently modelled: it can also regard the extension to two  active hands, an active body or even active objects, etc. [sent-508, score-0.685]
</p><p>99 Efficient model-based 3d tracking of hand articulations using kinect. [sent-613, score-0.449]
</p><p>100 Full dof tracking of a hand interacting with an object by modeling occlusions and physical constraints. [sent-620, score-0.73]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dynamics', 0.276), ('hand', 0.25), ('actor', 0.216), ('collision', 0.202), ('tracking', 0.199), ('ht', 0.188), ('physical', 0.176), ('friction', 0.161), ('cup', 0.159), ('pso', 0.149), ('hypothesis', 0.147), ('simulation', 0.145), ('spheres', 0.144), ('physically', 0.13), ('plausible', 0.12), ('motion', 0.109), ('state', 0.106), ('physics', 0.104), ('entities', 0.102), ('ball', 0.101), ('modelling', 0.099), ('kyriazis', 0.097), ('observations', 0.088), ('bullet', 0.087), ('permanence', 0.087), ('restitution', 0.087), ('sk', 0.086), ('scene', 0.086), ('rendered', 0.086), ('inertia', 0.084), ('interaction', 0.081), ('observability', 0.081), ('idr', 0.077), ('ido', 0.077), ('oikonomidis', 0.077), ('simulator', 0.077), ('mass', 0.076), ('forward', 0.073), ('rgbd', 0.071), ('interacting', 0.069), ('cups', 0.068), ('budgets', 0.065), ('ilo', 0.065), ('kinetic', 0.065), ('rendering', 0.065), ('hypotheses', 0.063), ('articulation', 0.062), ('velocity', 0.059), ('track', 0.058), ('swarm', 0.058), ('budget', 0.058), ('simulated', 0.056), ('expectation', 0.054), ('instants', 0.054), ('forces', 0.053), ('regards', 0.051), ('brubaker', 0.051), ('golf', 0.048), ('cviu', 0.048), ('popovi', 0.046), ('hands', 0.045), ('yielded', 0.045), ('depth', 0.044), ('abstractions', 0.044), ('blackbox', 0.044), ('delamarre', 0.044), ('duff', 0.044), ('firmly', 0.044), ('gbs', 0.044), ('gravitational', 0.044), ('ilr', 0.044), ('kjellstrom', 0.044), ('papadourakis', 0.044), ('torques', 0.044), ('vondrak', 0.044), ('wyatt', 0.044), ('tensors', 0.043), ('pose', 0.043), ('hypothesized', 0.043), ('objects', 0.041), ('parameterized', 0.04), ('interpretations', 0.04), ('urtasun', 0.04), ('particle', 0.039), ('st', 0.039), ('accompanies', 0.039), ('bmva', 0.039), ('effortlessly', 0.039), ('nikolaos', 0.039), ('grasped', 0.039), ('contact', 0.038), ('entity', 0.038), ('orientation', 0.037), ('evolution', 0.037), ('fleet', 0.037), ('triangular', 0.037), ('exhibiting', 0.037), ('dd', 0.036), ('occlusions', 0.036), ('game', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="331-tfidf-1" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>2 0.17448376 <a title="331-tfidf-2" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>Author: Vineet Gandhi, Remi Ronfard</p><p>Abstract: We introduce a generative model for learning person and costume specific detectors from labeled examples. We demonstrate the model on the task of localizing and naming actors in long video sequences. More specifically, the actor’s head and shoulders are each represented as a constellation of optional color regions. Detection can proceed despite changes in view-point and partial occlusions. We explain how to learn the models from a small number of labeled keyframes or video tracks, and how to detect novel appearances of the actors in a maximum likelihood framework. We present results on a challenging movie example, with 81% recall in actor detection (coverage) and 89% precision in actor identification (naming).</p><p>3 0.16213487 <a title="331-tfidf-3" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>Author: Bo Zheng, Yibiao Zhao, Joey C. Yu, Katsushi Ikeuchi, Song-Chun Zhu</p><p>Abstract: In this paper, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.</p><p>4 0.16198784 <a title="331-tfidf-4" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>5 0.15815909 <a title="331-tfidf-5" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>Author: Cheng Li, Kris M. Kitani</p><p>Abstract: We address the task of pixel-level hand detection in the context of ego-centric cameras. Extracting hand regions in ego-centric videos is a critical step for understanding handobject manipulation and analyzing hand-eye coordination. However, in contrast to traditional applications of hand detection, such as gesture interfaces or sign-language recognition, ego-centric videos present new challenges such as rapid changes in illuminations, significant camera motion and complex hand-object manipulations. To quantify the challenges and performance in this new domain, we present a fully labeled indoor/outdoor ego-centric hand detection benchmark dataset containing over 200 million labeled pixels, which contains hand images taken under various illumination conditions. Using both our dataset and a publicly available ego-centric indoors dataset, we give extensive analysis of detection performance using a wide range of local appearance features. Our analysis highlights the effectiveness of sparse features and the importance of modeling global illumination. We propose a modeling strategy based on our findings and show that our model outperforms several baseline approaches.</p><p>6 0.13617802 <a title="331-tfidf-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.12587032 <a title="331-tfidf-7" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>8 0.12380564 <a title="331-tfidf-8" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>9 0.11654726 <a title="331-tfidf-9" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>10 0.11255006 <a title="331-tfidf-10" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>11 0.10999618 <a title="331-tfidf-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.10276655 <a title="331-tfidf-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.10269926 <a title="331-tfidf-13" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>14 0.10220528 <a title="331-tfidf-14" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>15 0.099508569 <a title="331-tfidf-15" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>16 0.097533628 <a title="331-tfidf-16" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>17 0.096899576 <a title="331-tfidf-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.094326675 <a title="331-tfidf-18" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>19 0.093959987 <a title="331-tfidf-19" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>20 0.091531925 <a title="331-tfidf-20" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, 0.09), (2, 0.013), (3, -0.091), (4, -0.045), (5, -0.049), (6, 0.083), (7, -0.044), (8, 0.065), (9, 0.104), (10, -0.027), (11, 0.003), (12, -0.058), (13, 0.069), (14, 0.033), (15, -0.006), (16, 0.027), (17, 0.099), (18, -0.011), (19, 0.028), (20, 0.06), (21, 0.001), (22, -0.008), (23, 0.04), (24, -0.015), (25, 0.024), (26, 0.029), (27, -0.05), (28, -0.063), (29, -0.032), (30, -0.054), (31, -0.007), (32, -0.001), (33, 0.001), (34, -0.034), (35, -0.014), (36, 0.034), (37, 0.051), (38, 0.027), (39, -0.014), (40, -0.027), (41, 0.038), (42, 0.048), (43, 0.049), (44, 0.057), (45, 0.059), (46, 0.037), (47, -0.056), (48, 0.022), (49, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96045756 <a title="331-lsi-1" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>2 0.82414865 <a title="331-lsi-2" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>3 0.79697978 <a title="331-lsi-3" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>4 0.74742961 <a title="331-lsi-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.68495041 <a title="331-lsi-5" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>6 0.67564428 <a title="331-lsi-6" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>7 0.63736695 <a title="331-lsi-7" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>8 0.63662934 <a title="331-lsi-8" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>9 0.63075429 <a title="331-lsi-9" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>10 0.62227911 <a title="331-lsi-10" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>11 0.61427486 <a title="331-lsi-11" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>12 0.61237985 <a title="331-lsi-12" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>13 0.60995144 <a title="331-lsi-13" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>14 0.60459936 <a title="331-lsi-14" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>15 0.59518987 <a title="331-lsi-15" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>16 0.58182871 <a title="331-lsi-16" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>17 0.58164775 <a title="331-lsi-17" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>18 0.56130707 <a title="331-lsi-18" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>19 0.55353385 <a title="331-lsi-19" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>20 0.55268645 <a title="331-lsi-20" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.116), (16, 0.044), (26, 0.083), (33, 0.209), (47, 0.241), (67, 0.065), (69, 0.074), (87, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83730191 <a title="331-lda-1" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>2 0.79650819 <a title="331-lda-2" href="./cvpr-2013-Towards_Efficient_and_Exact_MAP-Inference_for_Large_Scale_Discrete_Computer_Vision_Problems_via_Combinatorial_Optimization.html">436 cvpr-2013-Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization</a></p>
<p>Author: Jörg Hendrik Kappes, Markus Speth, Gerhard Reinelt, Christoph Schnörr</p><p>Abstract: Discrete graphical models (also known as discrete Markov random fields) are a major conceptual tool to model the structure of optimization problems in computer vision. While in the last decade research has focused on fast approximative methods, algorithms that provide globally optimal solutions have come more into the research focus in the last years. However, large scale computer vision problems seemed to be out of reach for such methods. In this paper we introduce a promising way to bridge this gap based on partial optimality and structural properties of the underlying problem factorization. Combining these preprocessing steps, we are able to solve grids of size 2048 2048 in less than 90 seconds. On the hitherto unsolva2b04le8 C×h2i0ne4s8e character dataset of Nowozin et al. we obtain provably optimal results in 56% of the instances and achieve competitive runtimes on other recent benchmark problems. While in the present work only generalized Potts models are considered, an extension to general graphical models seems to be feasible.</p><p>3 0.78759879 <a title="331-lda-3" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>4 0.76662046 <a title="331-lda-4" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>Author: Payman Yadollahpour, Dhruv Batra, Gregory Shakhnarovich</p><p>Abstract: This paper introduces a two-stage approach to semantic image segmentation. In the first stage a probabilistic model generates a set of diverse plausible segmentations. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. While our proposed approach already achieves state-of-the-art results (48.1%) on the challenging VOC 2012 dataset, our machine and human analyses suggest that even larger gains are possible with such an approach.</p><p>5 0.73188597 <a title="331-lda-5" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>Author: Bojan Pepikj, Michael Stark, Peter Gehler, Bernt Schiele</p><p>Abstract: Despite the success of recent object class recognition systems, the long-standing problem of partial occlusion remains a major challenge, and a principled solution is yet to be found. In this paper we leave the beaten path of methods that treat occlusion as just another source of noise instead, we include the occluder itself into the modelling, by mining distinctive, reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular, we evaluate and compare models that range from standard object class detectors to hierarchical, part-based representations of occluder/occludee pairs. In an extensive evaluation we derive insights that can aid further developments in tackling the occlusion challenge. –</p><p>6 0.72883654 <a title="331-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.72540027 <a title="331-lda-7" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>8 0.7219975 <a title="331-lda-8" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>9 0.72135299 <a title="331-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.7210201 <a title="331-lda-10" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>11 0.72060865 <a title="331-lda-11" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>12 0.72003776 <a title="331-lda-12" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>13 0.71888494 <a title="331-lda-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.7171154 <a title="331-lda-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.71668136 <a title="331-lda-15" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>16 0.716582 <a title="331-lda-16" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>17 0.71656507 <a title="331-lda-17" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>18 0.71556967 <a title="331-lda-18" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>19 0.71489561 <a title="331-lda-19" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>20 0.71412891 <a title="331-lda-20" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
