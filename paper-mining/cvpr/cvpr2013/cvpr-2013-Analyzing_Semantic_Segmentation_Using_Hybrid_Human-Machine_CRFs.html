<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-43" href="#">cvpr2013-43</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</h1>
<br/><p>Source: <a title="cvpr-2013-43-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Mottaghi_Analyzing_Semantic_Segmentation_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>Reference: <a title="cvpr-2013-43-reference" href="../cvpr2013_reference/cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. [sent-5, score-0.974]
</p><p>2 In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. [sent-6, score-0.43]
</p><p>3 Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. [sent-7, score-0.38]
</p><p>4 One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! [sent-9, score-0.657]
</p><p>5 Clearly, other image understanding tasks like object detection [10], scene recognition [38], contextual reasoning among objects [29], and pose estimation [39] can aid semantic segmentation. [sent-14, score-0.581]
</p><p>6 Studies have shown that humans can effectively leverage contextual information from the entire scene to recognize objects in low resolution images that can not be recognized in isolation [35]. [sent-16, score-0.487]
</p><p>7 Recent works [ 12, 40, 16, 23], have thus pushed on holistic scene understanding models for among other things, improved semantic segmentation. [sent-18, score-0.529]
</p><p>8 A holistic scene understanding approach to semantic segmentation consists of a conditional random field (CRF) model that jointly reasons about: (a) classification of local patches (segmentation), (b) object detection, (c) shape analysis, (d) scene recognition and (e) contextual reasoning. [sent-36, score-0.914]
</p><p>9 In this paper we analyze the relative importance of each of these components by building an array of hybrid human-machine CRFs where each component is performed by a machine (default), or replaced by human subjects or ground truth, or is removed all together (top). [sent-37, score-0.658]
</p><p>10 333 111444113  We analyze the recent and most comprehensive holistic scene understanding model of Yao et al. [sent-45, score-0.4]
</p><p>11 It is a conditional random field (CRF) that models the interplay between segmentation and a variety of components such as local super-pixel appearance, object detection, scene recognition, shape analysis, class co-occurrence, and compatibility of classes with scene categories. [sent-47, score-0.491]
</p><p>12 To gain insights into the relative importance of these different factors or tasks, we isolate each one, and substitute a machine with a human for that task, keeping the rest of the model intact (Figure 1). [sent-48, score-0.398]
</p><p>13 Hence, the use of human subjects in our studies is key, as it gives us a feasible point of what can be done. [sent-53, score-0.454]
</p><p>14 However when plugged into the holistic model, human potentials provide a significant boost in performance. [sent-58, score-0.86]
</p><p>15 Excited by this insight, we conducted a thorough analysis of the human generated super-pixel potentials to identify precisely how they differ from existing machine potentials. [sent-63, score-0.756]
</p><p>16 Our analysis inspired a rather simple modification of the machine potentials which resulted in a significant increase of 2. [sent-64, score-0.61]
</p><p>17 Related Work Holistic Scene Understanding: The key motivation behind holistic scene understanding, going back to the seminal 1Of course, ground truth segmentation annotations are themselves generated by humans, but by viewing the whole image and leveraging information from the entire scene. [sent-69, score-0.372]
</p><p>18 In this paper, orthogonal to these advances, we propose the use of human subjects to understand the relative  ××  importance of various recognition tasks in aiding semantic segmentation. [sent-85, score-0.639]
</p><p>19 In contrast, we are interested in semantic segmentation which involves identifying the semantic category of each pixel in the image. [sent-90, score-0.451]
</p><p>20 sk Ins that closely mimic existing holistic computational models for semantic segmentation in order to identify bottlenecks, and better guide future research efforts. [sent-101, score-0.413]
</p><p>21 In contrast, in this work, we are inter-  ested in systematically analyzing the roles played by several high- and mid-level tasks such as grouping, shape analysis, scene recognition, object detection and contextual interactions in holistic scene understanding models for semantic segmentation. [sent-104, score-0.935]
</p><p>22 gies of the human studies and machine experiments, as well as the findings and insights are all novel. [sent-157, score-0.518]
</p><p>23 This allows us to conveniently replace the machine potentials with human responses: after all, we cannot quite require humans to be submodular! [sent-166, score-0.915]
</p><p>24 The problem of holistic scene understanding is formulated as that of inference in a CRF. [sent-169, score-0.367]
</p><p>25 The random field contains variables representing the class labels of image segments at two levels in a segmentation hierarchy: super-pixels and larger segments. [sent-170, score-0.463]
</p><p>26 The segments and super-segments reason about the semantic class labels to be assigned to each pixel in the image. [sent-174, score-0.446]
</p><p>27 A shape prior is associated with these nodes encouraging segments that respect this prior to take on corresponding class labels. [sent-179, score-0.355]
</p><p>28 Before we provide details about how the various machine potentials are computed, we first discuss the dataset we work with to ground further descriptions. [sent-193, score-0.566]
</p><p>29 The contextual interactions are also quite skewed [7] making it less interesting for holistic scene understanding. [sent-203, score-0.43]
</p><p>30 Machine CRF Potentials We now describe the machine potentials we employed. [sent-214, score-0.566]
</p><p>31 Segments and super-segments: We utilize UCM [1] to create our segments and super-segments as it returns a small number of segments that tend to respect the true object boundaries well. [sent-216, score-0.544]
</p><p>32 We use the output of the modified TextonBoost [33] in [20] to get pixel-wise potentials and average those within the segments and super-segments to get the unary potentials. [sent-221, score-0.727]
</p><p>33 Following [18], we connect the two levels via a pairwise Pn potential that encourages segments and super-segments to take the same label. [sent-222, score-0.438]
</p><p>34 We also employ pairwise potentials between zi and zk that capture cooccurance statistics of pairs of classes. [sent-224, score-0.605]
</p><p>35 A binary variable bi is used for each detection and it is connected to the binary class variable, zci , where ci is the class of the detector that fired for the i−th hypothesis. [sent-229, score-0.357]
</p><p>36 Shape: Shape potentials are incorporated in the model by connecting the binary detection variables bi to all segments xj inside the detection’s bounding box. [sent-230, score-0.91]
</p><p>37 Scene and scene-class co-occurrence: We train a classifier [38] to predict each of the scene types, and use its confidence to form the unitary potential for the scene variable. [sent-234, score-0.449]
</p><p>38 The scene node connects to each binary class variable zi  via a pairwise potential which is defined based on the cooccurance statistics of the training data, i. [sent-235, score-0.568]
</p><p>39 More than 500 subjects participated in our studies that involved ∼ 300, 000 crowd-sourced tasks, making tiehes rtheastuil tnsv ooblvtaeidne ∼d likely 0to0 b cer fairly ostuarbcleed across a daifkfienrgent sampling of subjects. [sent-246, score-0.36]
</p><p>40 Segments and Super-segments: The study involves having human subjects classify segments into one of the semantic categories. [sent-247, score-0.747]
</p><p>41 However, showing all the information that the machine uses to human subjects would lead to nearly 100% classification accuracy by the subjects, leaving us with little insights to gain. [sent-254, score-0.66]
</p><p>42 More importantly, a 200 x 200 window occupies nearly 60% of the image, resulting in humans potentially using holistic scene understanding while classifying the segments. [sent-255, score-0.524]
</p><p>43 To this goal, the discrepancy in information shown to humans and machines is not a concern, as long as humans are not shown more information than the machine has access to. [sent-259, score-0.533]
</p><p>44 showing subjects a collection of segments and asking them to click on all the ones likely to belong to a certain class, or allowing a subject to select only one category per segment, etc. [sent-262, score-0.583]
</p><p>45 Our experiment involved having subjects label all segments and super-segments from the MSRC dataset containing more than 500 pixels. [sent-265, score-0.497]
</p><p>46 Figure 4 shows examples of segmentations obtained by assigning each segment to the  class with most human votes. [sent-272, score-0.38]
</p><p>47 Assigning each segment to the class with the highest number of human votes achieves an accuracy of 72. [sent-274, score-0.421]
</p><p>48 The C dimensional human unary potential for a (super)segment is proportional to the number of times subjects selected each class, normalized to sum to 1. [sent-281, score-0.627]
</p><p>49 We set the potentials for the unlabeled (smaller than 500 pixels) (super)segments to be uniform. [sent-282, score-0.414]
</p><p>50 For all pairs of categories, we then ask subjects which category is more likely to occur in an image from the collection. [sent-284, score-0.369]
</p><p>51 We build the class unary potentials by counting how often each class was preferred over all other classes. [sent-285, score-0.648]
</p><p>52 Class-Class Co-occurrence: To obtain the human cooccurrence potentials we ask subjects the following question for all triplets of categories {zi , zj , zk}: “Which scentioarnio f oisr more likely fto c occur eins an image? [sent-292, score-0.968]
</p><p>53 We use th)e, wChhiocwh- gLiviue algorithm on tchois-o mccautrrirxe,n as was used in [40] on the class co-occurrence potentials to obtain the tree structure, where the edges connect highly cooccurring nodes. [sent-300, score-0.493]
</p><p>54 As a crude proxy, we showed subjects images inside ground truth object bounding boxes and asked them to recognize the object. [sent-304, score-0.398]
</p><p>55 Shape: We showed 5 subjects the segment boundaries in the ground truth object bounding boxes along with its category label and contextual information from the rest of the scene. [sent-307, score-0.674]
</p><p>56 Using the interface of [14], subjects were asked to trace a subset of the segment boundaries to match their expected shape of the object. [sent-309, score-0.488]
</p><p>57 This shows that humans can not decipher the shape of an object from the UCM segment boundaries better than an automatic approach. [sent-314, score-0.408]
</p><p>58 Scene Unary: We ask human subjects to classify an image into one of the 21 scene categories used in [40] (see Figure 2). [sent-316, score-0.557]
</p><p>59 Subjects were allowed to select 5We showed subjects contextual information around the bounding box because without it humans were unable to recognize the object category reliably using only the boundaries of the segments in the box (54% accuracy). [sent-320, score-1.051]
</p><p>60 Humans clearly outperform the machine at scene recognition, but the question of interest is whether this will translate to improved semantic segmentation performance. [sent-345, score-0.501]
</p><p>61 Scene-Class Co-occurrence: Similar to the class-class experiment, subjects were asked which object category is more likely to be present in the scene. [sent-346, score-0.396]
</p><p>62 Ground-truth Potentials: In addition to human potentials (which provide a feasible point), we are also interested in establishing an upper-bound on the effect each subtask can have on segmentation performance. [sent-349, score-0.707]
</p><p>63 We do so by introduc-  ing ground truth (GT) potentials into the model. [sent-350, score-0.414]
</p><p>64 For segments and super-segments we simply set the value of the potential to be 1for the segment GT label and 0 otherwise, similarly for scene and class unary potentials. [sent-352, score-0.864]
</p><p>65 Experiments with Human-Machine CRFs We now describe the results of inserting the human potentials in the CRF model. [sent-356, score-0.573]
</p><p>66 We also investigated how plugging in GT potentials or discarding certain tasks all together affects segmentation performance on the MSRC dataset. [sent-357, score-0.604]
</p><p>67 Class presence, class-class co-occurrence, and the sceneclass potentials have negligible impact on the performance of semantic segmentation. [sent-367, score-0.575]
</p><p>68 GT shape also improves performance, but as discussed earlier, we find that humans are unable to instantiate this potential using the UCM segment boundaries. [sent-370, score-0.541]
</p><p>69 One human potential that does improve performance is the unitary segment potential. [sent-372, score-0.51]
</p><p>70 This is quite striking since human labeling accuracy of segments was substantially worse than machine’s (72. [sent-373, score-0.47]
</p><p>71 Intrigued by this, we performed detailed analysis to identify properties of the human potential that are leading to this boost in performance. [sent-379, score-0.418]
</p><p>72 Resultant insights provided us concrete guidance to improve machine potentials and hence state-of-the-art accuracies. [sent-380, score-0.653]
</p><p>73 Scale: We noticed that the machine did not have access to the scale of the segments while humans did. [sent-382, score-0.576]
</p><p>74 So we added a feature that captured the size of a segment relative to the image and re-trained the unary machine potentials. [sent-383, score-0.37]
</p><p>75 Over-fitting: The machine segment unaries are trained on the same images as the CRF parameters, potentially leading to over-fitting. [sent-387, score-0.436]
</p><p>76 333 111444668  Ranking of the correct label: It is clear that the highest ranked label of the human potential is wrong more often than the highest ranked label of the machine potential (hence the lower accuracy of the former outside the model). [sent-395, score-0.772]
</p><p>77 But we wondered if perhaps even when wrong, the human potential gave a high enough score to the correct label making it revivable when used in the CRF, while the machine was more “blatantly” wrong. [sent-396, score-0.552]
</p><p>78 We found that among the misclassified segments, the rank of the correct label using human potentials was 4. [sent-397, score-0.612]
</p><p>79 Uniform potentials for small segments: Recall that we did not have human subjects label the segments smaller than 500 pixels and assigned a uniform potential to those segments. [sent-400, score-1.241]
</p><p>80 We suspected that ignoring the small (likely to be misclassified) segments may give the human potential an advantage in the model. [sent-402, score-0.567]
</p><p>81 So we replaced the machine potentials for small segments with a uniform distribution over the categories. [sent-403, score-0.844]
</p><p>82 As a follow-up, we also weighted the machine potentials by the size of the corresponding segment. [sent-406, score-0.566]
</p><p>83 We also replicated the sparsity of human potentials in the machine potentials, but this did not improve performance by much (77. [sent-415, score-0.725]
</p><p>84 Complementarity: To get a deeper understanding as to why human segment potentials significantly increase performance when used in the model, we performed a variety of additional CRF experiments with hybrid potentials. [sent-417, score-0.861]
</p><p>85 These included having human (H) or machine (M) potentials for segments (S) or super-segments (SS) or both, with or without the Pn potential in the model. [sent-418, score-1.133]
</p><p>86 The last two rows correspond to the case where both human and machine segment potentials are used together at the same level. [sent-420, score-0.867]
</p><p>87 But when the human and machine potentials are placed at different levels in the model (rows 3 and 4), not having a Pn potential (and thus losing connection between the two levels) significantly hurts performance. [sent-422, score-0.896]
</p><p>88 This indicates that even though human potentials are not more accurate than machine potentials, when both human and machine potentials interact, there is a significant boost in performance, demonstrating the complementary nature of the two. [sent-423, score-1.554]
</p><p>89 So we hypothesized that the types of mistakes that the machine and humans make may be different. [sent-446, score-0.411]
</p><p>90 The resultant  confusion matrix was more similar to that of human subjects (Figure 7(c)). [sent-456, score-0.49]
</p><p>91 We re-computed the segment unaries and plugged them into the model in addition to the original unaries that used large windows. [sent-458, score-0.503]
</p><p>92 Notice that the improvement provided by the entire CRF model over the original machine segment unaries alone was 3% (from 74. [sent-467, score-0.436]
</p><p>93 While a fairly straightforward change in the training of machine unaries lead to this improvement in performance, we note that the insight to do so was provided by our use of humans to “debug” the state-of-the-art model. [sent-470, score-0.451]
</p><p>94 9% of segments, while humans assign different labels to 12% of the segments within a supersegment. [sent-473, score-0.394]
</p><p>95 GT labels for segments when plugged into the CRF provide an accuracy of 94% (and not 100% because deci-  sions are made at the segment level which are not perfect). [sent-478, score-0.497]
</p><p>96 Plugging in human potentials for all the components gives us an accuracy of 89. [sent-481, score-0.614]
</p><p>97 Our analysis hinges on the use of human subjects to produce the different potentials in the model. [sent-489, score-0.794]
</p><p>98 One of our findings was that human responses to local segments in isolation, while being less accurate than machines’, provide complementary information that the CRF model can effectively exploit. [sent-491, score-0.537]
</p><p>99 We explored various avenues to precisely characterize this complementary nature, which resulted in a novel machine potential that significantly improves accuracy over  the state-of-art. [sent-492, score-0.455]
</p><p>100 Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation. [sent-771, score-0.404]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('potentials', 0.414), ('msrc', 0.254), ('segments', 0.237), ('subjects', 0.221), ('crf', 0.221), ('potential', 0.171), ('human', 0.159), ('humans', 0.157), ('holistic', 0.153), ('machine', 0.152), ('segment', 0.142), ('unaries', 0.142), ('semantic', 0.13), ('scene', 0.12), ('gt', 0.103), ('mistakes', 0.102), ('segmentation', 0.099), ('textonboost', 0.098), ('understanding', 0.094), ('contextual', 0.093), ('insights', 0.087), ('zj', 0.083), ('class', 0.079), ('plugged', 0.077), ('unary', 0.076), ('isolation', 0.076), ('zi', 0.074), ('studies', 0.074), ('resultant', 0.07), ('aiding', 0.069), ('zk', 0.065), ('ucm', 0.064), ('tasks', 0.06), ('crfs', 0.058), ('category', 0.057), ('boost', 0.057), ('ask', 0.057), ('bounding', 0.052), ('hybrid', 0.052), ('cooccurance', 0.052), ('slew', 0.052), ('asked', 0.05), ('detection', 0.05), ('variables', 0.048), ('things', 0.048), ('responses', 0.048), ('stuff', 0.048), ('complementary', 0.047), ('efforts', 0.046), ('findings', 0.046), ('mask', 0.046), ('resulted', 0.044), ('room', 0.044), ('involvement', 0.043), ('barrow', 0.043), ('rivest', 0.043), ('roles', 0.042), ('yao', 0.042), ('replaced', 0.041), ('pn', 0.041), ('recognize', 0.041), ('binary', 0.041), ('accuracy', 0.041), ('isolated', 0.04), ('confusion', 0.04), ('author', 0.04), ('label', 0.039), ('shape', 0.039), ('unitary', 0.038), ('weakest', 0.038), ('machines', 0.037), ('bi', 0.036), ('boundaries', 0.036), ('interested', 0.035), ('likely', 0.034), ('oliva', 0.034), ('arbelaez', 0.034), ('fidler', 0.034), ('asking', 0.034), ('object', 0.034), ('parikh', 0.034), ('clas', 0.033), ('analyze', 0.033), ('quite', 0.033), ('hazan', 0.033), ('reasons', 0.032), ('unable', 0.032), ('saxena', 0.032), ('pushed', 0.032), ('incorporated', 0.032), ('box', 0.031), ('identify', 0.031), ('variable', 0.031), ('plugging', 0.031), ('making', 0.031), ('impact', 0.031), ('shotton', 0.03), ('encourages', 0.03), ('access', 0.03), ('reliably', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="43-tfidf-1" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>2 0.24813338 <a title="43-tfidf-2" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>Author: Yujia Li, Daniel Tarlow, Richard Zemel</p><p>Abstract: When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs); we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss;and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.</p><p>3 0.23076862 <a title="43-tfidf-3" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>Author: Sanja Fidler, Abhishek Sharma, Raquel Urtasun</p><p>Abstract: We are interested in holistic scene understanding where images are accompanied with text in the form of complex sentential descriptions. We propose a holistic conditional random field model for semantic parsing which reasons jointly about which objects are present in the scene, their spatial extent as well as semantic segmentation, and employs text as well as image information as input. We automatically parse the sentences and extract objects and their relationships, and incorporate them into the model, both via potentials as well as by re-ranking candidate detections. We demonstrate the effectiveness of our approach in the challenging UIUC sentences dataset and show segmentation improvements of 12.5% over the visual only model and detection improvements of 5% AP over deformable part-based models [8].</p><p>4 0.21916112 <a title="43-tfidf-4" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>Author: Neill D.F. Campbell, Kartic Subr, Jan Kautz</p><p>Abstract: Conditional Random Fields (CRFs) are used for diverse tasks, ranging from image denoising to object recognition. For images, they are commonly defined as a graph with nodes corresponding to individual pixels and pairwise links that connect nodes to their immediate neighbors. Recent work has shown that fully-connected CRFs, where each node is connected to every other node, can be solved efficiently under the restriction that the pairwise term is a Gaussian kernel over a Euclidean feature space. In this paper, we generalize the pairwise terms to a non-linear dissimilarity measure that is not required to be a distance metric. To this end, we propose a density estimation technique to derive conditional pairwise potentials in a nonparametric manner. We then use an efficient embedding technique to estimate an approximate Euclidean feature space for these potentials, in which the pairwise term can still be expressed as a Gaussian kernel. We demonstrate that the use of non-parametric models for the pairwise interactions, conditioned on the input data, greatly increases expressive power whilst maintaining efficient inference.</p><p>5 0.21831279 <a title="43-tfidf-5" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>Author: Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun</p><p>Abstract: In this paper we are interested in how semantic segmentation can help object detection. Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional HOG filters as well as a set of novel segmentation features. Thus our model “blends ” between the detector and segmentation models. Since our features can be computed very efficiently given the segments, we maintain the same complexity as the original DPM [14]. We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector [12] on all classes, achieving 13% higher average AP. When employing the parts, we outperform the original DPM [14] in 19 out of 20 classes, achieving an improvement of 8% AP. Furthermore, we outperform the previous state-of-the-art on VOC’10 test by 4%.</p><p>6 0.19698168 <a title="43-tfidf-6" href="./cvpr-2013-A_Principled_Deep_Random_Field_Model_for_Image_Segmentation.html">24 cvpr-2013-A Principled Deep Random Field Model for Image Segmentation</a></p>
<p>7 0.17781401 <a title="43-tfidf-7" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>8 0.17147966 <a title="43-tfidf-8" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>9 0.16275071 <a title="43-tfidf-9" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>10 0.15789954 <a title="43-tfidf-10" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>11 0.15764707 <a title="43-tfidf-11" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>12 0.15061529 <a title="43-tfidf-12" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>13 0.14735027 <a title="43-tfidf-13" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>14 0.14308287 <a title="43-tfidf-14" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>15 0.14156234 <a title="43-tfidf-15" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>16 0.14058991 <a title="43-tfidf-16" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>17 0.13759448 <a title="43-tfidf-17" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>18 0.13349448 <a title="43-tfidf-18" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>19 0.12730154 <a title="43-tfidf-19" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>20 0.12557933 <a title="43-tfidf-20" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.282), (1, -0.049), (2, 0.06), (3, -0.068), (4, 0.149), (5, 0.057), (6, 0.049), (7, 0.192), (8, -0.091), (9, -0.013), (10, 0.143), (11, -0.031), (12, -0.061), (13, 0.054), (14, -0.079), (15, 0.142), (16, 0.075), (17, 0.076), (18, -0.029), (19, -0.05), (20, -0.025), (21, -0.045), (22, 0.05), (23, 0.045), (24, 0.01), (25, -0.079), (26, -0.02), (27, 0.117), (28, -0.063), (29, -0.15), (30, -0.059), (31, -0.092), (32, -0.074), (33, 0.044), (34, 0.017), (35, 0.022), (36, -0.035), (37, 0.019), (38, -0.082), (39, 0.176), (40, -0.042), (41, 0.092), (42, 0.031), (43, 0.054), (44, 0.006), (45, -0.022), (46, -0.068), (47, 0.017), (48, -0.037), (49, -0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95373279 <a title="43-lsi-1" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>2 0.82253164 <a title="43-lsi-2" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>Author: Sanja Fidler, Abhishek Sharma, Raquel Urtasun</p><p>Abstract: We are interested in holistic scene understanding where images are accompanied with text in the form of complex sentential descriptions. We propose a holistic conditional random field model for semantic parsing which reasons jointly about which objects are present in the scene, their spatial extent as well as semantic segmentation, and employs text as well as image information as input. We automatically parse the sentences and extract objects and their relationships, and incorporate them into the model, both via potentials as well as by re-ranking candidate detections. We demonstrate the effectiveness of our approach in the challenging UIUC sentences dataset and show segmentation improvements of 12.5% over the visual only model and detection improvements of 5% AP over deformable part-based models [8].</p><p>3 0.77982736 <a title="43-lsi-3" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>Author: Payman Yadollahpour, Dhruv Batra, Gregory Shakhnarovich</p><p>Abstract: This paper introduces a two-stage approach to semantic image segmentation. In the first stage a probabilistic model generates a set of diverse plausible segmentations. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. While our proposed approach already achieves state-of-the-art results (48.1%) on the challenging VOC 2012 dataset, our machine and human analyses suggest that even larger gains are possible with such an approach.</p><p>4 0.75550902 <a title="43-lsi-4" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>5 0.75470114 <a title="43-lsi-5" href="./cvpr-2013-A_Principled_Deep_Random_Field_Model_for_Image_Segmentation.html">24 cvpr-2013-A Principled Deep Random Field Model for Image Segmentation</a></p>
<p>Author: Pushmeet Kohli, Anton Osokin, Stefanie Jegelka</p><p>Abstract: We discuss a model for image segmentation that is able to overcome the short-boundary bias observed in standard pairwise random field based approaches. To wit, we show that a random field with multi-layered hidden units can encode boundary preserving higher order potentials such as the ones used in the cooperative cuts model of [11] while still allowing for fast and exact MAP inference. Exact inference allows our model to outperform previous image segmentation methods, and to see the true effect of coupling graph edges. Finally, our model can be easily extended to handle segmentation instances with multiple labels, for which it yields promising results.</p><p>6 0.70454556 <a title="43-lsi-6" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>7 0.70280379 <a title="43-lsi-7" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>8 0.68942374 <a title="43-lsi-8" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>9 0.67605597 <a title="43-lsi-9" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>10 0.66498739 <a title="43-lsi-10" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>11 0.64910448 <a title="43-lsi-11" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>12 0.63334054 <a title="43-lsi-12" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>13 0.61734015 <a title="43-lsi-13" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>14 0.60377061 <a title="43-lsi-14" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>15 0.60201824 <a title="43-lsi-15" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>16 0.5957883 <a title="43-lsi-16" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>17 0.58995301 <a title="43-lsi-17" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>18 0.58963197 <a title="43-lsi-18" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>19 0.58093536 <a title="43-lsi-19" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>20 0.57299161 <a title="43-lsi-20" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.131), (16, 0.021), (26, 0.083), (33, 0.295), (67, 0.103), (69, 0.059), (72, 0.011), (76, 0.011), (80, 0.013), (87, 0.092), (94, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95083177 <a title="43-lda-1" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>Author: Hu Ding, Branislav Stojkovic, Ronald Berezney, Jinhui Xu</p><p>Abstract: Computing accurate and robust organizational patterns of chromosome territories inside the cell nucleus is critical for understanding several fundamental genomic processes, such as co-regulation of gene activation, gene silencing, X chromosome inactivation, and abnormal chromosome rearrangement in cancer cells. The usage of advanced fluorescence labeling and image processing techniques has enabled researchers to investigate interactions of chromosome territories at large spatial resolution. The resulting high volume of generated data demands for high-throughput and automated image analysis methods. In this paper, we introduce a novel algorithmic tool for investigating association patterns of chromosome territories in a population of cells. Our method takes as input a set of graphs, one for each cell, containing information about spatial interaction of chromosome territories, and yields a single graph that contains essential information for the whole population and stands as its structural representative. We formulate this combinato- rial problem as a semi-definite programming and present novel techniques to efficiently solve it. We validate our approach on both artificial and real biological data; the experimental results suggest that our approach yields a nearoptimal solution, and can handle large-size datasets, which are significant improvements over existing techniques.</p><p>2 0.94928235 <a title="43-lda-2" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>3 0.94663095 <a title="43-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.94100296 <a title="43-lda-4" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>Author: Bojan Pepikj, Michael Stark, Peter Gehler, Bernt Schiele</p><p>Abstract: Despite the success of recent object class recognition systems, the long-standing problem of partial occlusion remains a major challenge, and a principled solution is yet to be found. In this paper we leave the beaten path of methods that treat occlusion as just another source of noise instead, we include the occluder itself into the modelling, by mining distinctive, reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular, we evaluate and compare models that range from standard object class detectors to hierarchical, part-based representations of occluder/occludee pairs. In an extensive evaluation we derive insights that can aid further developments in tackling the occlusion challenge. –</p><p>same-paper 5 0.93891686 <a title="43-lda-5" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>6 0.93851721 <a title="43-lda-6" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>7 0.93798411 <a title="43-lda-7" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>8 0.93778569 <a title="43-lda-8" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>9 0.93683225 <a title="43-lda-9" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>10 0.93631768 <a title="43-lda-10" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>11 0.93579358 <a title="43-lda-11" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>12 0.93541789 <a title="43-lda-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.93532187 <a title="43-lda-13" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>14 0.93500513 <a title="43-lda-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.93281603 <a title="43-lda-15" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>16 0.93239343 <a title="43-lda-16" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>17 0.93184894 <a title="43-lda-17" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>18 0.93128902 <a title="43-lda-18" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>19 0.93090057 <a title="43-lda-19" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>20 0.93080884 <a title="43-lda-20" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
