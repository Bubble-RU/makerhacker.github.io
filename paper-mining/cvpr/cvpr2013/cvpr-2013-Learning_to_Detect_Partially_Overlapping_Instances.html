<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>264 cvpr-2013-Learning to Detect Partially Overlapping Instances</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-264" href="#">cvpr2013-264</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>264 cvpr-2013-Learning to Detect Partially Overlapping Instances</h1>
<br/><p>Source: <a title="cvpr-2013-264-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Arteta_Learning_to_Detect_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Carlos Arteta, Victor Lempitsky, J. Alison Noble, Andrew Zisserman</p><p>Abstract: The objective of this work is to detect all instances of a class (such as cells or people) in an image. The instances may be partially overlapping and clustered, and hence quite challenging for traditional detectors, which aim at localizing individual instances. Our approach is to propose a set of candidate regions, and then select regions based on optimizing a global classification score, subject to the constraint that the selected regions are non-overlapping. Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. For example, our detector can pick a region containing two or three object instances, while assigning such region an appropriate label. We show that this formulation can be learned within the structured output SVM framework, and that the inference in such model can be accomplished using dynamic programming on a tree structured region graph. Furthermore, the learning only requires weak annotations – a dot on each instance. The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on quite disparate data sets: fluorescence microscopy images and UCSD pedestrians.</p><p>Reference: <a title="cvpr-2013-264-reference" href="../cvpr2013_reference/cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Alison Noble1 Andrew Zisserman1 1Department of Engineering Science, University of Oxford, UK 2Skolkovo Institute of Science and Technology, Russia  Abstract The objective of this work is to detect all instances of a class (such as cells or people) in an image. [sent-2, score-0.297]
</p><p>2 The instances may be partially overlapping and clustered, and hence quite challenging for traditional detectors, which aim at localizing individual instances. [sent-3, score-0.255]
</p><p>3 Our approach is to propose a set of candidate regions, and then select regions based on optimizing a global classification score, subject to the constraint that the selected regions are non-overlapping. [sent-4, score-0.394]
</p><p>4 Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. [sent-5, score-0.436]
</p><p>5 For example, our detector can pick a region containing two or three object instances, while assigning such region an appropriate label. [sent-6, score-0.392]
</p><p>6 We show that this formulation can be learned within the structured output SVM framework, and that the inference in such model can be accomplished using dynamic programming on a tree structured region graph. [sent-7, score-0.448]
</p><p>7 The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on quite  disparate data sets: fluorescence microscopy images and UCSD pedestrians. [sent-9, score-0.555]
</p><p>8 crowds of pedestrians, or animal and plant populations) and within the microscopy domain (cells of in-vitro cultures and developing embryos, blood samples, histopathology images, etc. [sent-13, score-0.183]
</p><p>9 Such detection can be based on a sliding window or Hough transform, followed by an appropriate non-maxima suppression procedure [3, 8, 14], stochastic fitting of interacting particles or object models [9, 10, 24], or region-based detection [2, 18, 19]. [sent-17, score-0.222]
</p><p>10 The second class contains the methods that avoid the detection of individual instances but instead perform analysis based on local or global texture and appearance descriptors, e. [sent-18, score-0.353]
</p><p>11 by recovering the overall realvalued count of objects in the scene [5, 12, 16, 22] or by estimating the local real-valued density of the objects in each location of interest [11, 15]. [sent-20, score-0.263]
</p><p>12 For the high-density images, however, detection-based analysis may fail badly, especially when the amount ofoverlap and inter-occlusion between objects makes the detection of individual instances hard or impossible even for human experts. [sent-26, score-0.35]
</p><p>13 The analysis in this case is essentially reduced to texture matching between the test image and the training set, which may be feasible even when individual instances are not distinguishable. [sent-28, score-0.207]
</p><p>14 an image from a surveillance camera may contain multiple individual pedestrians but also few groups of people which are hard to segment from each other [7]. [sent-33, score-0.277]
</p><p>15 Likewise, a microscopy image may contain both regions of low and high cell density (sometimes corresponding to different morphological parts or different tissues). [sent-34, score-0.541]
</p><p>16 The learning in our model is performed based on weak annotation (red dots) and is driven by an instance count loss. [sent-40, score-0.191]
</p><p>17 Similarly to our initial approach [2], the parsing process is based on an efficient and exact inference procedure that detects a set of non-overlapping extremal regions delivering a maximum to the parsing functional. [sent-47, score-0.672]
</p><p>18 The learning is performed in a structured SVM framework and optimizes the  (convex upper bound on the) counting loss. [sent-48, score-0.249]
</p><p>19 to choose the groups of the smallest size whenever objects are discernable, as this strategy tends to provide the highest counting accuracy. [sent-51, score-0.279]
</p><p>20 We conduct a set of experiments with real and synthetic fluorescence microscopy images, as well a surveillance data from the UCSD pedestrian dataset. [sent-52, score-0.51]
</p><p>21 For all datasets, the proposed method outperformed other detection methods, including a considerable improvement over the baseline [2], and is comparable with the methods that are trained to count (and do not perform detection). [sent-54, score-0.31]
</p><p>22 Our main contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. [sent-57, score-0.436]
</p><p>23 Thus rather than trying to reason about the boundary and part assignment between several tightly overlapping regions [3, 8, 14, 21], tuples ofobjects are detected as a whole, making the object detection process more resilient to strong object overlap. [sent-58, score-0.505]
</p><p>24 Instead, we follow the observation [18] that good object support hypotheses can be provided by extremal regions of the image, for example MSER [17] (Figure 1-b). [sent-65, score-0.69]
</p><p>25 These regions are well suited to biomedical data [2] and text detection [19]. [sent-66, score-0.324]
</p><p>26 As an additional contribution, we extend the applicability of this approach by using extremal regions of a derived image (rather than the input image itself). [sent-67, score-0.627]
</p><p>27 For example, we use the extremal regions of a soft background difference image to generate detection hypotheses for a surveillance image stream (whereas extremal regions of the input images themselves would provide a poor hypotheses set). [sent-68, score-1.476]
</p><p>28 Our computational model is based on our previous work [2] that also used non-overlapping extremal regions. [sent-73, score-0.446]
</p><p>29 Whilst that initial model achieves good results on those biomedical datasets where objects are clearly discernable from each other as extremal regions, it struggles to achieve high recall when that is not the case (i. [sent-74, score-0.627]
</p><p>30 when for some object X, any extremal region containing X also contains another overlapping object Y; in this case [2] has no hope of detecting both X and Y as they have to be detected as separate extremal regions). [sent-76, score-1.183]
</p><p>31 The Model For an input image I containing multiple instances of an object acnlas ins p(ustom imea gofe Iwh ciocnht may g be m overlapping) we owfa annt to automatically detect the instances and provide an estimate of their location. [sent-82, score-0.369]
</p><p>32 We start by generating a pool of N nested regions, such that for each pair of regions Ri and Rj in the pool, these regions are either nested (i. [sent-83, score-0.679]
</p><p>33 In ⊂the R simplest case, a pool can comprise extre∩mRal regions no tfh thee s min-put image (i. [sent-86, score-0.3]
</p><p>34 io Muso ways, ecrraelalty,ing a new map I where higher-value regions correspond to higher probabilities oerfe an object’s presence. [sent-90, score-0.181]
</p><p>35 oTrhrees pool toof candidate regions can then be generated as a set of extremal regions in the transformed image I. [sent-91, score-0.959]
</p><p>36 Oionncse i tnh teh pool nosff noermsteedd regions i. [sent-92, score-0.3]
</p><p>37 s generated, each region is scored using a set of classifiers that evaluate the similar-  ity of such region to each of D classes, where each class signifies the integer number of instances of the object that the region contains (i. [sent-93, score-0.696]
</p><p>38 Given the scores of the classifiers, an inference procedure selects a non-overlapping subset of regions, and assigns each selected region in the subset a class label, thus indicating the number of objects that our system believes this region represents. [sent-97, score-0.44]
</p><p>39 The choice of the region subset and the class labels are driven by the optimization process that simply maximizes the total classifier score corresponding to selected regions and class labels subject to the non-overlap constraint. [sent-98, score-0.463]
</p><p>40 More specifically, let Vi (d) denote the classifier score of a region Ri for class d (the higher the score, the more this region looks like a typical region containing d object centroids). [sent-99, score-0.558]
</p><p>41 N}, where yi = 0 means that the region Ri {isy n|oit =sele 1c. [sent-104, score-0.189]
</p><p>42 y ∈ Y  (1)  This maximization of (1) can be performed exactly and efficiently using dynamic programming (since the region pool has a tree structure this follows from the nestedness property of the regions). [sent-123, score-0.564]
</p><p>43 Learning the model The model for the evaluation of the regions can learn from weak annotations, i. [sent-127, score-0.181]
</p><p>44 Such learning is driven by an instance count loss (IC-loss) (2) that penalizes all deviations from the one-to-one correspondences between annotation dots and the selected regions (Figure 1). [sent-130, score-0.511]
</p><p>45 LetS udjip now w bee th haev neum Mbe trra oinfi dnogt sim caognetasin Ied in the region , and Nj be the total number of dots in Ij . [sent-132, score-0.237]
</p><p>46 1  Here, the first term penalizes the deviations between the assigned class label of the selected regions and the true number of dots inside of it. [sent-141, score-0.376]
</p><p>47 h Tehde (ul anstcovered) dots for the yj configuration under the non-overlap  dij  yij  constraint, and thus penalize false negatives (missed detections). [sent-144, score-0.602]
</p><p>48 Assuming that the properties of each region are characterized by the feature vector we set the classification scores to be linear functions of these feature vectors: (d) = (wd · ), where wd is the parameter vector for the dth class, a·nd f has the same dimensionality as the feature vector. [sent-145, score-0.205]
</p><p>49 However, when considering the possibility of regions containing multiple objects, we must take into account the increasing intraclass variability (e. [sent-181, score-0.244]
</p><p>50 of region shape) for higher-order classes that would bias the labels assigned to the regions towards low-order classes. [sent-183, score-0.378]
</p><p>51 In order to counterbalance such effect, we use a re-scaled penalization based on the true number of dots inside the region . [sent-184, score-0.393]
</p><p>52 Intuitively, assigning a class 7 to a region that contains 6 instances is not as bad as assigning a class 3 to a region with 2 instances, thus it is not penalized so hard. [sent-185, score-0.602]
</p><p>53 zero-loss) region configurations can be consistent with such annotation (Figure 1c,e). [sent-191, score-0.186]
</p><p>54 The maximization of (1) can be performed exactly and efficiently by exploiting the nestedness property of the region pool. [sent-219, score-0.345]
</p><p>55 Indeed, one can consider a tree-structured model, where each node  corresponds to a region and where parent-child links correspond to the nestedness property. [sent-220, score-0.29]
</p><p>56 Namely, the node Rj becomes a parent of the node Ri if Rj is the smallest region in the pool that Ri strictly belongs to. [sent-221, score-0.267]
</p><p>57 In this way, because of the nestedness, the region pool can be organized into a forest. [sent-222, score-0.267]
</p><p>58 (i)  i  =0  i  where p(i) maps region Ri to the number of its parent region (p(i) = 0 for root regions in the forest), Wi (d, d) = 0,  =  Wi(d, 0) = Vi(d), Wi(0, d > 0) = −∞, and Wi(d1, d2 d1) = −∞ as long as (d02, d> >0. [sent-230, score-0.477]
</p><p>59 For each selected region Ri we run k-means with k = yi on the image coordinates of all pixels in that region, thus obtaining an estimate for the set of centroids of individual objects. [sent-237, score-0.32]
</p><p>60 The positive training examples for the binary classifier wd consist of all regions in the training images that contain d dots. [sent-240, score-0.238]
</p><p>61 Experiments and Results To show the performance and generality of the method presented, results are reported for two different tasks: cell detection in microscopy images (Figure 2) and pedestrians detection in surveillance videos (Figure 3). [sent-244, score-0.674]
</p><p>62 Our primary metric is mean absolute counting error, which measures the absolute mismatch in the number of objects in an image between the output and the GT. [sent-246, score-0.241]
</p><p>63 Cell Detection Detecting cells in microscopy images is a challenging task in many real applications. [sent-253, score-0.261]
</p><p>64 We have selected two datasets to show the applicability of our method for this scenario: a synthetic and a real dataset of fluorescence microscopy. [sent-255, score-0.206]
</p><p>65 2  Table 1: Accuracy for the synthetic cell dataset and components evaluation. [sent-303, score-0.19]
</p><p>66 The high cell confluency in the synthetic cell dataset [15] poses a difficult challenge for detection algorithms due to very high cell overlap. [sent-304, score-0.547]
</p><p>67 Therefore, it is expected that counting algorithms such as [11, 15] would outperform detection methods. [sent-305, score-0.288]
</p><p>68 Nonetheless, our method is able to produce a comparable mean counting error (MCE), while providing estimates of object localization evaluated with precision and recall. [sent-306, score-0.225]
</p><p>69 regions without nested regions in the pool) nested within a given region. [sent-314, score-0.56]
</p><p>70 This last descriptor often indicates the presence of individual objects existing inside the region being encoded. [sent-315, score-0.32]
</p><p>71 The synthetic dataset of flourescence microscopy from [15] consists of 200 images generated with [13], divided in half for testing and training, with an average number of 171 64 cells per  ±  333222333422  Figure 2: (best viewed in color) Results for our method on fluorescence microscopy datasets. [sent-317, score-0.735]
</p><p>72 The output images show the selected regions, colourcoded according to the estimated number of objects inside of it (green=1, blue=2, purple=3, yellow=4, cyan=5, red=7), also indicated with digits omitting class 1for clarity. [sent-319, score-0.188]
</p><p>73 Moreover, we compare to the counting methods [11, 15] and the detection method [3]. [sent-326, score-0.288]
</p><p>74 As expected, the counting algorithms can outperform the detection methods in cases of very high object overlap such as this synthetic cells dataset. [sent-327, score-0.495]
</p><p>75 The baseline [2], restricted to one object per extremal regions, cannot cope with the level of object clustering in this dataset and thus performs poorly. [sent-329, score-0.574]
</p><p>76 The proposed method outperforms the two previous methods both in terms of the detection accuracy and the counting accuracy. [sent-342, score-0.288]
</p><p>77 In general, the proposed method outperformed both competitors, both in terms of detection accuracy and, more substantially, in terms of the counting error. [sent-344, score-0.288]
</p><p>78 Pedestrian detection We apply our method to detect and count pedestrians in the UCSD surveillance camera dataset [6]. [sent-348, score-0.416]
</p><p>79 Extremal regions are collected from (c) the soft background difference image (see text), and a portion of those regions is shown over the original image (d). [sent-352, score-0.362]
</p><p>80 The method selects non-overlapping regions (e) and estimates the number of instances of the object that the region contains, which allows the prediction of the location of the individual instances. [sent-353, score-0.568]
</p><p>81 Digits indicate the estimated number of instances inside the region,  and green regions correspond to single objects. [sent-354, score-0.374]
</p><p>82 The pedestrians frequently occlude each other and are imaged at a very low resolution (the furthest pedestrians are just a few pixels tall). [sent-356, score-0.21]
</p><p>83 All this makes detection very hard for this dataset, and although a number of counting methods have been evaluated on it, to the best of our knowledge, we are the first to run detection algorithms. [sent-357, score-0.383]
</p><p>84 As pedestrians can correspond to both dark and bright regions, we cannot use the extremal regions of the input images. [sent-358, score-0.732]
</p><p>85 Instead, to generate the tree of regions for this data, we computed the background image using a simple median filtering of a sparsely sampled set. [sent-359, score-0.211]
</p><p>86 For each frame, we then simply compute the absolute value of the difference with the background and look for extremal regions in this difference image. [sent-360, score-0.627]
</p><p>87 To reduce the number of candidate regions to a  few hundreds, we applied a mild Gaussian smoothing to the difference image (σ = 1pixel). [sent-361, score-0.213]
</p><p>88 The counting accuracy of our detection method is comparable with the accuracy of methods that are trained to count and are not able to estimate the locations ofindividual pedestrians (even for singletons). [sent-368, score-0.514]
</p><p>89 For this dataset, we have observed that the method produced classes 1 to 5, indicating that discerning individual instances was harder than in the case of the real cell images. [sent-369, score-0.387]
</p><p>90 In terms of the detection accuracy, the proposed method has also achieved an improvement over the baseline [2] (Table 4). [sent-370, score-0.189]
</p><p>91 35 Table 3: Mean absolute errors for people counting in the surveillance video [6]. [sent-398, score-0.258]
</p><p>92 Our detection method approaches the counting accuracy of the counting methods, while outperforming the baseline detection [2] in all splits. [sent-400, score-0.64]
</p><p>93 Depending on the difficulty of the detection task, the model has the flexibility to choose groups of variable sizes (including individual instances if the task is easy). [sent-413, score-0.34]
</p><p>94 The use of the model is particular attractive for biomedical images, where it considerably outperforms the baseline [2] that can only predict individual instances all the time. [sent-418, score-0.319]
</p><p>95 Thanks to the presented generalization of the region pool generation process, we could also apply the model to object detection in surveillance imagery, obtaining good detection accuracy despite low resolution. [sent-419, score-0.554]
</p><p>96 One of the limitations of the proposed method appears when the instances become even denser than in the considered datasets and a higher number of classes is needed to parse such images. [sent-421, score-0.187]
</p><p>97 Finally, it is worth noting that all that is required of the candidate regions is that they are nested. [sent-427, score-0.213]
</p><p>98 Thus, although we have used extremal regions for candidates, they could instead be generated by hierarchical image segmentation, e. [sent-428, score-0.627]
</p><p>99 On the detection of multiple object instances using Hough transforms. [sent-454, score-0.265]
</p><p>100 Computational framework for simulating fluorescence microscope images with cell populations. [sent-525, score-0.278]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('extremal', 0.446), ('yij', 0.231), ('counting', 0.193), ('microscopy', 0.183), ('regions', 0.181), ('region', 0.148), ('fluorescence', 0.147), ('nestedness', 0.142), ('instances', 0.138), ('cell', 0.131), ('yj', 0.129), ('count', 0.121), ('pool', 0.119), ('tuples', 0.117), ('dij', 0.111), ('pedestrians', 0.105), ('penalization', 0.101), ('ucsd', 0.101), ('nested', 0.099), ('detection', 0.095), ('hj', 0.095), ('rj', 0.095), ('ri', 0.094), ('dji', 0.093), ('dots', 0.089), ('discernable', 0.085), ('flourescence', 0.085), ('singletons', 0.085), ('crowd', 0.084), ('fij', 0.083), ('cells', 0.078), ('mcj', 0.076), ('individual', 0.069), ('surveillance', 0.065), ('baseline', 0.064), ('nj', 0.063), ('rij', 0.063), ('centroids', 0.062), ('synthetic', 0.059), ('splits', 0.057), ('wd', 0.057), ('arteta', 0.057), ('ayxj', 0.057), ('bernardis', 0.057), ('downscale', 0.057), ('fiaschi', 0.057), ('lapping', 0.057), ('tthhiiss', 0.057), ('wwoorrkk', 0.057), ('pedestrian', 0.056), ('structured', 0.056), ('inside', 0.055), ('maximization', 0.055), ('vij', 0.052), ('class', 0.051), ('loss', 0.05), ('classes', 0.049), ('overlapping', 0.048), ('biomedical', 0.048), ('objects', 0.048), ('wi', 0.047), ('postprocessing', 0.047), ('upscale', 0.047), ('density', 0.046), ('inference', 0.045), ('dilation', 0.044), ('mser', 0.044), ('accomplished', 0.043), ('configuration', 0.042), ('yi', 0.041), ('hough', 0.04), ('barinova', 0.039), ('annotation', 0.038), ('overlap', 0.038), ('programming', 0.038), ('groups', 0.038), ('fj', 0.038), ('lempitsky', 0.037), ('vi', 0.037), ('badly', 0.037), ('intensities', 0.036), ('dot', 0.035), ('auxiliary', 0.034), ('digits', 0.034), ('assigning', 0.033), ('annotations', 0.033), ('dynamic', 0.032), ('possibility', 0.032), ('object', 0.032), ('driven', 0.032), ('purple', 0.032), ('candidate', 0.032), ('containing', 0.031), ('classifiers', 0.031), ('hundred', 0.031), ('hypotheses', 0.031), ('monitoring', 0.031), ('detect', 0.03), ('improvement', 0.03), ('tree', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="264-tfidf-1" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>Author: Carlos Arteta, Victor Lempitsky, J. Alison Noble, Andrew Zisserman</p><p>Abstract: The objective of this work is to detect all instances of a class (such as cells or people) in an image. The instances may be partially overlapping and clustered, and hence quite challenging for traditional detectors, which aim at localizing individual instances. Our approach is to propose a set of candidate regions, and then select regions based on optimizing a global classification score, subject to the constraint that the selected regions are non-overlapping. Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. For example, our detector can pick a region containing two or three object instances, while assigning such region an appropriate label. We show that this formulation can be learned within the structured output SVM framework, and that the inference in such model can be accomplished using dynamic programming on a tree structured region graph. Furthermore, the learning only requires weak annotations – a dot on each instance. The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on quite disparate data sets: fluorescence microscopy images and UCSD pedestrians.</p><p>2 0.17482588 <a title="264-tfidf-2" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>Author: Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah</p><p>Abstract: We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark con- trast to datasets usedfor existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.</p><p>3 0.15949681 <a title="264-tfidf-3" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>4 0.13940483 <a title="264-tfidf-4" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>5 0.11548999 <a title="264-tfidf-5" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>Author: Byung-Woo Hong, Zhaojin Lu, Ganesh Sundaramoorthi</p><p>Abstract: In this work, we address the multi-label Mumford-Shah problem, i.e., the problem of jointly estimating a partitioning of the domain of the image, and functions defined within regions of the partition. We create algorithms that are efficient, robust to undesirable local minima, and are easy-toimplement. Our algorithms are formulated by slightly modifying the underlying statistical model from which the multilabel Mumford-Shah functional is derived. The advantage of this statistical model is that the underlying variables: the labels and thefunctions are less coupled than in the original formulation, and the labels can be computed from the functions with more global updates. The resulting algorithms can be tuned to the desired level of locality of the solution: from fully global updates to more local updates. We demonstrate our algorithm on two applications: joint multi-label segmentation and denoising, and joint multi-label motion segmentation and flow estimation. We compare to the stateof-the-art in multi-label Mumford-Shah problems and show that we achieve more promising results.</p><p>6 0.10668455 <a title="264-tfidf-6" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>7 0.10542698 <a title="264-tfidf-7" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>8 0.10510716 <a title="264-tfidf-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.10202605 <a title="264-tfidf-9" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>10 0.10159196 <a title="264-tfidf-10" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>11 0.099377714 <a title="264-tfidf-11" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>12 0.095935427 <a title="264-tfidf-12" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>13 0.094872423 <a title="264-tfidf-13" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>14 0.092196755 <a title="264-tfidf-14" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>15 0.08876095 <a title="264-tfidf-15" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>16 0.086277343 <a title="264-tfidf-16" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>17 0.085821129 <a title="264-tfidf-17" href="./cvpr-2013-Bilinear_Programming_for_Human_Activity_Recognition_with_Unknown_MRF_Graphs.html">62 cvpr-2013-Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</a></p>
<p>18 0.08578717 <a title="264-tfidf-18" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>19 0.085135505 <a title="264-tfidf-19" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>20 0.084858432 <a title="264-tfidf-20" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.241), (1, -0.033), (2, 0.03), (3, -0.042), (4, 0.079), (5, 0.026), (6, 0.052), (7, 0.037), (8, -0.019), (9, 0.011), (10, -0.014), (11, -0.036), (12, 0.042), (13, -0.086), (14, 0.023), (15, 0.006), (16, -0.025), (17, 0.06), (18, 0.058), (19, -0.029), (20, -0.022), (21, 0.077), (22, -0.126), (23, 0.031), (24, 0.019), (25, -0.039), (26, -0.059), (27, -0.034), (28, 0.062), (29, -0.044), (30, 0.038), (31, 0.062), (32, -0.026), (33, 0.108), (34, 0.013), (35, -0.118), (36, 0.06), (37, -0.039), (38, 0.074), (39, -0.056), (40, -0.08), (41, 0.097), (42, -0.103), (43, -0.001), (44, -0.03), (45, -0.002), (46, 0.09), (47, 0.023), (48, 0.043), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90028024 <a title="264-lsi-1" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>Author: Carlos Arteta, Victor Lempitsky, J. Alison Noble, Andrew Zisserman</p><p>Abstract: The objective of this work is to detect all instances of a class (such as cells or people) in an image. The instances may be partially overlapping and clustered, and hence quite challenging for traditional detectors, which aim at localizing individual instances. Our approach is to propose a set of candidate regions, and then select regions based on optimizing a global classification score, subject to the constraint that the selected regions are non-overlapping. Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. For example, our detector can pick a region containing two or three object instances, while assigning such region an appropriate label. We show that this formulation can be learned within the structured output SVM framework, and that the inference in such model can be accomplished using dynamic programming on a tree structured region graph. Furthermore, the learning only requires weak annotations – a dot on each instance. The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on quite disparate data sets: fluorescence microscopy images and UCSD pedestrians.</p><p>2 0.80468971 <a title="264-lsi-2" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>3 0.78195143 <a title="264-lsi-3" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>Author: Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah</p><p>Abstract: We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark con- trast to datasets usedfor existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.</p><p>4 0.749008 <a title="264-lsi-4" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>5 0.69666326 <a title="264-lsi-5" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>6 0.63548756 <a title="264-lsi-6" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>7 0.58415854 <a title="264-lsi-7" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>8 0.58300263 <a title="264-lsi-8" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>9 0.58122659 <a title="264-lsi-9" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>10 0.55708957 <a title="264-lsi-10" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>11 0.5517841 <a title="264-lsi-11" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>12 0.54321319 <a title="264-lsi-12" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>13 0.54064214 <a title="264-lsi-13" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>14 0.53745317 <a title="264-lsi-14" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>15 0.53316367 <a title="264-lsi-15" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>16 0.52697337 <a title="264-lsi-16" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>17 0.51658338 <a title="264-lsi-17" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>18 0.5104472 <a title="264-lsi-18" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>19 0.50940001 <a title="264-lsi-19" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>20 0.50860685 <a title="264-lsi-20" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.117), (16, 0.032), (26, 0.046), (28, 0.016), (33, 0.249), (37, 0.227), (67, 0.077), (69, 0.057), (87, 0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83925378 <a title="264-lda-1" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>Author: Carlos Arteta, Victor Lempitsky, J. Alison Noble, Andrew Zisserman</p><p>Abstract: The objective of this work is to detect all instances of a class (such as cells or people) in an image. The instances may be partially overlapping and clustered, and hence quite challenging for traditional detectors, which aim at localizing individual instances. Our approach is to propose a set of candidate regions, and then select regions based on optimizing a global classification score, subject to the constraint that the selected regions are non-overlapping. Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. For example, our detector can pick a region containing two or three object instances, while assigning such region an appropriate label. We show that this formulation can be learned within the structured output SVM framework, and that the inference in such model can be accomplished using dynamic programming on a tree structured region graph. Furthermore, the learning only requires weak annotations – a dot on each instance. The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on quite disparate data sets: fluorescence microscopy images and UCSD pedestrians.</p><p>2 0.81361985 <a title="264-lda-2" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>Author: Katerina Fragkiadaki, Han Hu, Jianbo Shi</p><p>Abstract: Human pose detectors, although successful in localising faces and torsos of people, often fail with lower arms. Motion estimation is often inaccurate under fast movements of body parts. We build a segmentation-detection algorithm that mediates the information between body parts recognition, and multi-frame motion grouping to improve both pose detection and tracking. Motion of body parts, though not accurate, is often sufficient to segment them from their backgrounds. Such segmentations are crucialfor extracting hard to detect body parts out of their interior body clutter. By matching these segments to exemplars we obtain pose labeled body segments. The pose labeled segments and corresponding articulated joints are used to improve the motion flow fields by proposing kinematically constrained affine displacements on body parts. The pose-based articulated motion model is shown to handle large limb rotations and displacements. Our algorithm can detect people under rare poses, frequently missed by pose detectors, showing the benefits of jointly reasoning about pose, segmentation and motion in videos.</p><p>3 0.79664111 <a title="264-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.79437172 <a title="264-lda-4" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>Author: Guang Shu, Afshin Dehghan, Mubarak Shah</p><p>Abstract: We propose an approach to improve the detection performance of a generic detector when it is applied to a particular video. The performance of offline-trained objects detectors are usually degraded in unconstrained video environments due to variant illuminations, backgrounds and camera viewpoints. Moreover, most object detectors are trained using Haar-like features or gradient features but ignore video specificfeatures like consistent colorpatterns. In our approach, we apply a Superpixel-based Bag-of-Words (BoW) model to iteratively refine the output of a generic detector. Compared to other related work, our method builds a video-specific detector using superpixels, hence it can handle the problem of appearance variation. Most importantly, using Conditional Random Field (CRF) along with our super pixel-based BoW model, we develop and algorithm to segment the object from the background . Therefore our method generates an output of the exact object regions instead of the bounding boxes generated by most detectors. In general, our method takes detection bounding boxes of a generic detector as input and generates the detection output with higher average precision and precise object regions. The experiments on four recent datasets demonstrate the effectiveness of our approach and significantly improves the state-of-art detector by 5-16% in average precision.</p><p>5 0.79351604 <a title="264-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.78907567 <a title="264-lda-6" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>7 0.78787225 <a title="264-lda-7" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>8 0.78712326 <a title="264-lda-8" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>9 0.7870934 <a title="264-lda-9" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>10 0.78667468 <a title="264-lda-10" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>11 0.78654194 <a title="264-lda-11" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>12 0.78590852 <a title="264-lda-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.78585523 <a title="264-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.78485793 <a title="264-lda-14" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>15 0.78480881 <a title="264-lda-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.78476149 <a title="264-lda-16" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>17 0.7846275 <a title="264-lda-17" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>18 0.78455794 <a title="264-lda-18" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>19 0.78452438 <a title="264-lda-19" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>20 0.78410691 <a title="264-lda-20" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
