<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 cvpr-2013-Alternating Decision Forests</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-39" href="#">cvpr2013-39</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>39 cvpr-2013-Alternating Decision Forests</h1>
<br/><p>Source: <a title="cvpr-2013-39-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Schulter_Alternating_Decision_Forests_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>Reference: <a title="cvpr-2013-39-reference" href="../cvpr2013_reference/cvpr-2013-Alternating_Decision_Forests_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  UK  Abstract This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. [sent-7, score-0.275]
</p><p>2 Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. [sent-10, score-0.407]
</p><p>3 Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth. [sent-13, score-0.196]
</p><p>4 They all approximate Bayes Decision Rule known to be the optimal classifier via minimizing a margin-based loss function. [sent-21, score-0.237]
</p><p>5 However, in contrast to other methods, RFs minimize this loss greedily and –  –  implicitly via recursively reducing the uncertainty of given training samples by using independent base classifiers, i. [sent-22, score-0.273]
</p><p>6 Although these characteristics result in both, fast and parallel training capabilities, there is no control over an overall classifier loss and its proper minimization. [sent-25, score-0.289]
</p><p>7 Third, it is hard to extend the learner to special learning tasks, such as, domain adaptation, semi-supervised or multiple instance learning, as this is usually realized via regularizing a global loss function. [sent-29, score-0.24]
</p><p>8 In this paper, we propose a novel classifier termed Alternating Decision Forests (ADFs) that extends RFs by globally minimizing any given differentiable loss function, however, without losing the main characteristics and benefits of the original RF method as discussed above. [sent-30, score-0.336]
</p><p>9 We achieve this by borrowing ideas from Boosting methods, where during training a globally tracked weight distribution guides the loss minimization. [sent-31, score-0.26]
</p><p>10 To allow for incorporating arbitrary differentiable loss functions, we adopt the idea of employing gradient descent in function space [15] to calculate the weight updates. [sent-40, score-0.276]
</p><p>11 In order to let each stage of the classifier follow the adaptive weight distribution, we replace standard RFs splitting criteria with the weighted entropy, while features are still randomly subsampled. [sent-41, score-0.235]
</p><p>12 Thus, ADFs alternate between overall weight updates, and parallel randomized tree growing, where the latter one preserves the computational benefits of common Random Forests. [sent-42, score-0.218]
</p><p>13 Furthermore, we empirically demonstrate that ADFs give more compact models in terms of tree depth as the training of the decision trees is guided by the sample weights, i. [sent-44, score-0.668]
</p><p>14 Random Forests [1, 5] are ensembles of T binary deci-  sion trees Tt(x) : X → RK, where X = RM is the Msdiiomne tnrseieosn Tal f(exa)tu :re X space Rand, R whKe r=e X[0, 1=]K R describes the space osifo nclaalss fe probability d ainstdri Rbutions over the label space Y = {1, . [sent-49, score-0.212]
</p><p>15 During testing, each decision tree thus rYetu =rns { a c. [sent-53, score-0.284]
</p><p>16 (1)  During training of a RF, the decision trees are provided with a random subset of the training data (i. [sent-58, score-0.497]
</p><p>17 Training a single decision tree involves recursively splitting each node such that the training data in the newly created child nodes is pure according to class labels. [sent-61, score-0.543]
</p><p>18 Each tree is grown until some stopping criterion, e. [sent-62, score-0.209]
</p><p>19 , the maximum tree depth, is reached and the class probability distributions are estimated in the leaf nodes. [sent-64, score-0.174]
</p><p>20 Lin [23] showed that minimizing so called Fisher-consistent marginbased loss functions automatically leads to good approximations of the unknown Bayes decision rule. [sent-84, score-0.388]
</p><p>21 As reviewed above, training in RFs is performed via recursively splitting training data into child nodes, where each split locally tries to optimize Eq. [sent-91, score-0.257]
</p><p>22 ,(5)  where lmm (·) is a margin maximizing loss function. [sent-98, score-0.201]
</p><p>23 Thus, via greedily minimizing such a local score, a decision tree aims at minimizing a Fisher-consistent loss function, and is hence approximating Bayes optimal learners. [sent-100, score-0.497]
</p><p>24 However, unlike other learners like Boosting or SVM approaches, RFs do not have an explicit global loss function. [sent-101, score-0.244]
</p><p>25 In turn, the nodes operate locally and trees are grown isolated, disregarding the current state of the entire classifier. [sent-102, score-0.311]
</p><p>26 Introducing a Global Loss To allow for integrating different, global loss functions into the Random Forests training procedure, we adopt ideas from Boosting [29, 13]. [sent-107, score-0.286]
</p><p>27 A Boosting classifier F(x) consists of T weak learners ft (x) : X → RK, where each weak learner gives a prediction pt :( Xy|x) → →abo Rut the class confidences for a sample x. [sent-108, score-0.268]
</p><p>28 There exist several different Boosting variants, most of them differing by the loss function they use, which in turn determines the shape of the weight distribution [17]. [sent-123, score-0.212]
</p><p>29 1a we illustrate prominent loss functions, such as, Logit, Hinge, Exponential, Savage [25], and Tangent [24]. [sent-125, score-0.173]
</p><p>30 In more detail, given a labeled training set {xi, yi}iN=1, training a single weak learner can be written as the global loss minimization problem ? [sent-131, score-0.385]
</p><p>31 As shown in [15], this can be done by training ft (x; Θt) to have high correlation with the negative gradient of the loss, which corresponds to updating the weights wit for each training sample xi in iteration t as  wti=? [sent-140, score-0.255]
</p><p>32 This even allows for incorporating non-convex loss functions, which have been shown to be more robust to label noise [25]. [sent-154, score-0.173]
</p><p>33 Training Alternating Decision Forests In order to incorporate any differentiable, global loss function into Random Forests, we adopt the idea of the  above reviewed Gradient Boosting method, i. [sent-160, score-0.197]
</p><p>34 While we could easily train Boosted Trees that respect a global loss function, i. [sent-163, score-0.197]
</p><p>35 , Gradient Boosting having decision trees as weak learners, the goal of Alternating Decision Forests is to explicitly integrate the global loss into the tree growing scheme, thus preserving the parallel training of standard RFs. [sent-165, score-0.89]
</p><p>36 For that purpose, we need (i) a stage-wise tree growing scheme during which we update the weight distribution and (ii) splitting functions taking these weights into account. [sent-166, score-0.389]
</p><p>37 To get a stage-wise classifier, we let our trees grow in an iterative, breadth-first manner, contrary to a typical depthfirst scheme in standard RFs. [sent-167, score-0.233]
</p><p>38 , Dmax, where Dmax is the maximum tree depth of the forest. [sent-173, score-0.213]
</p><p>39 imate of sample xi returned by tree Ttd, which denotes tree? [sent-175, score-0.164]
</p><p>40 e d W bey can now use this strong tcrleaess Tifier and a given loss l(·) to update the weights oclfa asslli training samples foosrs t lh(e·) )n teoxt u iptedraatteio tnhe as ielligushttrsa twed in  ? [sent-177, score-0.289]
</p><p>41 tT=1ptd(yi|xi),  wid+1  555 010088  Figure 2: Overview of the proposed tree growing principle of Alternating Decision Forests. [sent-178, score-0.185]
</p><p>42 In the first iteration (d = 1), the weights wid are uniform, and the first split functions are trained in a breadth-first manner. [sent-179, score-0.21]
</p><p>43 This forest with depth d = 2 can give predictions on the training samples, which are used to calculate weights, based on a global loss function, for the next  iteration d = 3. [sent-180, score-0.47]
</p><p>44 This procedure is repeated until the maximum  tree  depth d = Dmax is reached. [sent-181, score-0.213]
</p><p>45 To let the splitting functions consider the sample weights, we change the standard entropy calculation from Eq. [sent-187, score-0.189]
</p><p>46 This process of alternating bqeutawle teon k training a single stage d and updating the weights for the next stage is repeated until the same stopping criteria as in standard RFs are reached. [sent-193, score-0.312]
</p><p>47 (8) 7: end for  wid+1  Please note that, if each tree would correspond to one weak learner ft (x; Θ), each being fully trained in a single iteration, we would get a version of Boosted Trees [17]. [sent-206, score-0.278]
</p><p>48 In contrast, we train all trees in parallel, by switching from a depth-first to a breadth-first tree growing scheme. [sent-207, score-0.397]
</p><p>49 Then, we run Dmax iterations, where in each iteration all trees of the forest are simultaneously grown deeper by one stage. [sent-209, score-0.364]
</p><p>50 The weighted tree growing scheme in ADFs can be interpreted as a guided training of each single decision tree, but also as an explicit collaboration between all trees in the model. [sent-210, score-0.64]
</p><p>51 All nodes in the trees concentrate on “hard” training samples and, thus, don’t waste effort on samples that are already learned relatively well by the entire model. [sent-211, score-0.362]
</p><p>52 However, unlike Boosted Trees, this global loss is now an inherent  part of Random Forests. [sent-212, score-0.197]
</p><p>53 As we show in our experimental evaluations, this property of ADFs leads to more compact models in terms of tree depth. [sent-213, score-0.149]
</p><p>54 For RFs, an upper bound on the generalization error GE can be defined as GE ≤ , where ρ denotes the mean pair-wise fcionerrde alastio GnE E b ≤etw ρeen trees and s the strength of individual trees [5]. [sent-214, score-0.424]
</p><p>55 While ADFs explicitly enforce the collaboration of all trees in the forest, the decorrelation ρ can be preserved as in standard RFs, because (i) the splitting functions s(x; Θ) are still drawn completely randomly and (ii) the set of samples falling in common nodes is different for each tree. [sent-216, score-0.454]
</p><p>56 Relation to Previous Work In [12], Freund and Mason proposed a formulation to represent AdaBoost as a single decision tree. [sent-219, score-0.154]
</p><p>57 In contrast to standard RFs, the individual trees of ADFs become interdependent or entangled during training. [sent-224, score-0.258]
</p><p>58 This is similar to the works of [27] and [20] that train decision trees breadth-first or according to a priority queue, in order to incorporate contextual features into the learning process. [sent-225, score-0.366]
</p><p>59 However, both works only consider the predictions within a single decision tree and use those predictions as additional features for the splitting functions in the next stage. [sent-226, score-0.499]
</p><p>60 , all decision trees, and uses the predictions in order to minimize a global loss function. [sent-229, score-0.395]
</p><p>61 Another work that optimizes trees according to a differentiable loss function is that of Jancsary et al. [sent-231, score-0.429]
</p><p>62 [19], where the leaves of the trees store parameters for a Gaussian Conditional Random Field with different interaction types. [sent-232, score-0.212]
</p><p>63 Each factor type is only associated with a single tree but they are connected implicitly via the random field. [sent-233, score-0.165]
</p><p>64 As mentioned before, in BTs, the decision trees are trained sequentially and the weights are updated after training each single tree. [sent-240, score-0.472]
</p><p>65 Furthermore, during training a single decision tree in BTs, the weights cannot be updated. [sent-242, score-0.37]
</p><p>66 Contrary, in ADFs, it is exactly this property that allows for learning more compact models as the growing of each tree is somehow guided by the weight updates from the previous depth in the trees. [sent-243, score-0.364]
</p><p>67 We also investigate different choices of the loss function. [sent-249, score-0.206]
</p><p>68 For ADFs and BTs we also evaluate 5 different loss functions that can be integrated in the Gradient Boosting formulation: Logit, Hinge, Exponential, Savage [25] and Tangent [24]. [sent-264, score-0.214]
</p><p>69 As can be seen, the combination of ADFs and the Tangent loss function yields the best results on all 5 data sets. [sent-267, score-0.173]
</p><p>70 ADFs with Savage loss and Exponential loss are the second best choices on 3, respectively 1data sets. [sent-268, score-0.379]
</p><p>71 Only for G50c, BTs with Savage loss gives the second best results. [sent-269, score-0.173]
</p><p>72 It is also worthwhile to investigate the influence of the different loss functions. [sent-273, score-0.192]
</p><p>73 1a), plays a crucial role for the weight updates and thus the tree growing. [sent-277, score-0.211]
</p><p>74 For our further experiments, we fix the loss function to be the Tangent loss, as we can expect the best overall performance. [sent-278, score-0.173]
</p><p>75 We investigate the number of trees T and the maximum tree depth Dmax, which we vary in the ranges [1, 100] and [10, 25], respectively. [sent-503, score-0.425]
</p><p>76 As mentioned before, we choose the Tangent loss for both ADFs and BTs due to the good overall performance in the last experiment. [sent-504, score-0.173]
</p><p>77 It is interesting that ADFs improve over RFs and BTs only for larger number of trees (see Fig. [sent-505, score-0.212]
</p><p>78 However, due to the small number of trees and the fact that the trees are not fully grown yet, these predictions are unreliable. [sent-509, score-0.525]
</p><p>79 However, as soon as the number of trees increases, also the performance of ADFs increases and outperforms both RFs and BTs. [sent-511, score-0.212]
</p><p>80 This can be explained by the fact that the weight updates can rely on more stable predictions, as a larger number of trees is available. [sent-517, score-0.293]
</p><p>81 This experiment also reflects the guidance of the tree growing mentioned in Sec. [sent-518, score-0.185]
</p><p>82 3, as both RFs and BTs need deeper trees to reach the performance of ADFs. [sent-519, score-0.212]
</p><p>83 Tre D2e0pthBRATFD25 (a) (b) Figure 3: Influence of (a) the number of trees T and (b) the maximum tree depth Dmax on the three classifiers ADFs, RFs, and BTs on the Char74k data set. [sent-525, score-0.451]
</p><p>84 Here, we compare several combinations of two different parameters again, the number of trees T and the maximum tree depth Dmax. [sent-529, score-0.425]
</p><p>85 The performance steadily increases with the number of trees and also the maximum depth of the trees. [sent-533, score-0.295]
</p><p>86 18% error on the Char74k data set with 300 trees and a maximum –  depth of 25. [sent-537, score-0.295]
</p><p>87 1572  ± ±± ±± ±± ±±  555 111311  Number of trees  10 0. [sent-571, score-0.212]
</p><p>88 13  Table 4: Different parameter choices of the number of trees T and the maximum tree depth Dmax for Alternating Decision  Forests on the Char74k data set. [sent-681, score-0.458]
</p><p>89 Given those patches Pi, a Random Forest is trained with two different splitting criteria: (i) a standard classification criterion (see Sec. [sent-684, score-0.169]
</p><p>90 We thus also incorporate the minimization of a global loss function via iterative weight updates in the classification criterion in this framework. [sent-696, score-0.3]
</p><p>91 Each patch Pi is thus assigned a weight wi, which is always updated after training a single stage of the classifier according to a given global loss function l(·), as described ianc cSoercd. [sent-697, score-0.351]
</p><p>92 For both data sets, we extract a total number of 12000 patches and train a forest with 10 trees, a maximum depth of 15 and 5000 random tests per node. [sent-712, score-0.202]
</p><p>93 We also evaluate different loss functions for ADHF and BHF. [sent-713, score-0.214]
</p><p>94 Interestingly, our evaluations revealed that different loss functions perform better on different data sets. [sent-719, score-0.214]
</p><p>95 While  for TUD-pedestrian both non-convex and noise-robust loss functions, i. [sent-720, score-0.173]
</p><p>96 For ADHF and BHF, we show the curves of the best performing loss functions as presented above. [sent-727, score-0.214]
</p><p>97 Conclusion We proposed a novel classifier, termed Alternating Decision Forest (ADF), which illustrates how to formulate the Random Forest training as a global loss minimization problem. [sent-732, score-0.275]
</p><p>98 In contrast to the local optimization in standard Random Forests, we grow the trees stage-wise and include sample weight updates to minimize the global loss, as in Boosting algorithms. [sent-733, score-0.338]
</p><p>99 Semantic texton  forests for image categorization and segmentation. [sent-921, score-0.243]
</p><p>100 New multicategory boosting algorithms based on multicategory Fisher-consistent losses. [sent-927, score-0.177]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adfs', 0.636), ('rfs', 0.361), ('forests', 0.243), ('trees', 0.212), ('bts', 0.192), ('loss', 0.173), ('decision', 0.154), ('tree', 0.13), ('savage', 0.124), ('boosting', 0.121), ('alternating', 0.115), ('dmax', 0.113), ('adhf', 0.109), ('logit', 0.096), ('hough', 0.088), ('splitting', 0.086), ('tangent', 0.069), ('boosted', 0.066), ('forest', 0.064), ('bhf', 0.062), ('hfs', 0.062), ('depth', 0.06), ('grown', 0.057), ('wid', 0.055), ('growing', 0.055), ('losses', 0.053), ('adf', 0.051), ('weak', 0.049), ('training', 0.048), ('learners', 0.047), ('freund', 0.046), ('predictions', 0.044), ('differentiable', 0.044), ('classifier', 0.044), ('usps', 0.044), ('learner', 0.043), ('updates', 0.042), ('hinge', 0.042), ('nodes', 0.042), ('functions', 0.041), ('entropy', 0.041), ('weight', 0.039), ('letter', 0.039), ('mnist', 0.039), ('weights', 0.038), ('hf', 0.037), ('ft', 0.036), ('bayes', 0.036), ('random', 0.035), ('xi', 0.034), ('pi', 0.034), ('choices', 0.033), ('node', 0.033), ('offset', 0.032), ('yi', 0.032), ('iteration', 0.031), ('adhfs', 0.031), ('bhfs', 0.031), ('mtini', 0.031), ('samples', 0.03), ('termed', 0.03), ('child', 0.028), ('lmm', 0.028), ('multicategory', 0.028), ('runs', 0.027), ('rf', 0.026), ('give', 0.026), ('exponential', 0.026), ('classifiers', 0.026), ('austrian', 0.025), ('entangled', 0.025), ('split', 0.025), ('benefits', 0.025), ('parallel', 0.024), ('austria', 0.024), ('cornelis', 0.024), ('global', 0.024), ('tt', 0.024), ('maximum', 0.023), ('stage', 0.023), ('criterion', 0.022), ('stopping', 0.022), ('collaboration', 0.022), ('criteria', 0.022), ('recursively', 0.022), ('jancsary', 0.021), ('init', 0.021), ('sets', 0.021), ('standard', 0.021), ('leaf', 0.021), ('gradient', 0.02), ('patches', 0.02), ('slower', 0.02), ('trained', 0.02), ('minimizing', 0.02), ('compact', 0.019), ('guided', 0.019), ('influence', 0.019), ('competitors', 0.019), ('machine', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="39-tfidf-1" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>2 0.18180275 <a title="39-tfidf-2" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>3 0.1621009 <a title="39-tfidf-3" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>4 0.15517384 <a title="39-tfidf-4" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>5 0.14654516 <a title="39-tfidf-5" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>6 0.10796495 <a title="39-tfidf-6" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>7 0.086225867 <a title="39-tfidf-7" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>8 0.085825197 <a title="39-tfidf-8" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>9 0.085602626 <a title="39-tfidf-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.082355328 <a title="39-tfidf-10" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>11 0.078182064 <a title="39-tfidf-11" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>12 0.075303167 <a title="39-tfidf-12" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>13 0.073868737 <a title="39-tfidf-13" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>14 0.072030783 <a title="39-tfidf-14" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>15 0.068240158 <a title="39-tfidf-15" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>16 0.067558654 <a title="39-tfidf-16" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>17 0.06035899 <a title="39-tfidf-17" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>18 0.057072047 <a title="39-tfidf-18" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>19 0.056243401 <a title="39-tfidf-19" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>20 0.056236461 <a title="39-tfidf-20" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, -0.002), (2, -0.008), (3, 0.0), (4, 0.059), (5, 0.024), (6, 0.026), (7, 0.049), (8, -0.023), (9, -0.038), (10, -0.036), (11, 0.027), (12, -0.025), (13, 0.042), (14, -0.046), (15, 0.0), (16, -0.096), (17, -0.048), (18, 0.047), (19, -0.086), (20, -0.009), (21, 0.022), (22, -0.051), (23, 0.003), (24, -0.001), (25, 0.123), (26, -0.034), (27, 0.032), (28, 0.04), (29, 0.081), (30, -0.166), (31, 0.082), (32, -0.03), (33, -0.004), (34, 0.036), (35, -0.012), (36, -0.059), (37, -0.131), (38, 0.041), (39, 0.066), (40, -0.06), (41, 0.058), (42, -0.087), (43, 0.03), (44, 0.088), (45, 0.151), (46, -0.076), (47, 0.088), (48, 0.111), (49, 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94290793 <a title="39-lsi-1" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>2 0.79118544 <a title="39-lsi-2" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>3 0.74554932 <a title="39-lsi-3" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>4 0.67253917 <a title="39-lsi-4" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>5 0.60769981 <a title="39-lsi-5" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>6 0.57675838 <a title="39-lsi-6" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>7 0.53849375 <a title="39-lsi-7" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>8 0.50645798 <a title="39-lsi-8" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>9 0.43531662 <a title="39-lsi-9" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>10 0.42955238 <a title="39-lsi-10" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>11 0.42683536 <a title="39-lsi-11" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>12 0.42199159 <a title="39-lsi-12" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>13 0.41469368 <a title="39-lsi-13" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>14 0.41149738 <a title="39-lsi-14" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>15 0.41028708 <a title="39-lsi-15" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>16 0.41023061 <a title="39-lsi-16" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>17 0.39652953 <a title="39-lsi-17" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>18 0.39327794 <a title="39-lsi-18" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>19 0.39287269 <a title="39-lsi-19" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>20 0.38818505 <a title="39-lsi-20" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.123), (16, 0.016), (26, 0.019), (33, 0.213), (67, 0.041), (69, 0.026), (87, 0.447)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91416645 <a title="39-lda-1" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>2 0.90175569 <a title="39-lda-2" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>Author: Christian Häne, Christopher Zach, Andrea Cohen, Roland Angst, Marc Pollefeys</p><p>Abstract: Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being ’too noisy’. Unfortunately, these priors generally yield overly smooth reconstructions and/or segmentations in certain regions whereas they fail in other areas to constrain the solution sufficiently. In this paper we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other’s task. As a consequence, we propose a rigorous mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. Image segmentations provide geometric cues about which surface orientations are more likely to appear at a certain location in space whereas a dense 3D reconstruction yields a suitable regularization for the segmentation problem by lifting the labeling from 2D images to 3D space. We show how appearance-based cues and 3D surface orientation priors can be learned from training data and subsequently used for class-specific regularization. Experimental results on several real data sets highlight the advantages of our joint formulation.</p><p>3 0.89703321 <a title="39-lda-3" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>Author: Eno Töppe, Claudia Nieuwenhuis, Daniel Cremers</p><p>Abstract: We introduce the concept of relative volume constraints in order to account for insufficient information in the reconstruction of 3D objects from a single image. The key idea is to formulate a variational reconstruction approach with shape priors in form of relative depth profiles or volume ratios relating object parts. Such shape priors can easily be derived either from a user sketch or from the object’s shading profile in the image. They can handle textured or shadowed object regions by propagating information. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware. In contrast to existing single view reconstruction algorithms, the proposed algorithm provides substantially more flexibility to recover shape details such as self-occlusions, dents and holes, which are not visible in the object silhouette.</p><p>4 0.87525153 <a title="39-lda-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.85574806 <a title="39-lda-5" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>Author: Yi-Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa, P. Jonathon Phillips</p><p>Abstract: We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification, where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.</p><p>6 0.85097706 <a title="39-lda-6" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>same-paper 7 0.84929204 <a title="39-lda-7" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>8 0.8400389 <a title="39-lda-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.81483984 <a title="39-lda-9" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>10 0.78185183 <a title="39-lda-10" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>11 0.77816188 <a title="39-lda-11" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>12 0.73682022 <a title="39-lda-12" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>13 0.72609234 <a title="39-lda-13" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>14 0.71724701 <a title="39-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.70926565 <a title="39-lda-15" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>16 0.70595318 <a title="39-lda-16" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>17 0.69867945 <a title="39-lda-17" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>18 0.69796658 <a title="39-lda-18" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>19 0.69792128 <a title="39-lda-19" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>20 0.697083 <a title="39-lda-20" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
