<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-365" href="#">cvpr2013-365</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</h1>
<br/><p>Source: <a title="cvpr-2013-365-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Possegger_Robust_Real-Time_Tracking_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>Reference: <a title="cvpr-2013-365-reference" href="../cvpr2013_reference/cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 at cg  Abstract Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. [sent-4, score-0.381]
</p><p>2 These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. [sent-5, score-0.181]
</p><p>3 To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. [sent-6, score-0.87]
</p><p>4 Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. [sent-7, score-0.575]
</p><p>5 Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. [sent-8, score-0.629]
</p><p>6 Introduction Motivated by numerous applications, such as visual surveillance or sports analysis, considerable research has been made in the area of tracking objects from video sequences. [sent-12, score-0.422]
</p><p>7 , [8, 10, 13, 16, 17]) assume overlapping views observing the same 3D scene by exploiting constraints like objects moving on a common ground-plane, a known number of objects, or that two objects cannot occupy the same position at the  Figure1:Homgraphy-basedaproaches(. [sent-25, score-0.175]
</p><p>8 g,acumlat-  ing projections of foreground segmentations at a common ground-plane) often cause severe artifacts and cannot handle out-of-plane motion (e. [sent-26, score-0.264]
</p><p>9 , [6, 12, 21]) are exploited in the tracking process. [sent-41, score-0.291]
</p><p>10 For that purpose, we introduce the concept of an occupancy volume, which is based on local mass densities of a coarse 3D reconstruction of the objects’ visual hull. [sent-43, score-0.807]
</p><p>11 The usage of the local mass density  reduces noise and artifacts of the visual hull. [sent-44, score-0.435]
</p><p>12 This allows to derive an occupancy map, which represents the objects’ mass center on the ground-plane for robustly estimating the objects’ (x, y) coordinates using a particle filter approach 222333999533  (a)Inputimage. [sent-45, score-0.843]
</p><p>13 Figure 2: We reconstruct the visual hull from foreground segmentations of input images (a,b), which allows for computing the occupancy volume visualized in (c), where bright colors indicate high local mass densities. [sent-49, score-1.089]
</p><p>14 The occupancy volume allows for deriving an occupancy map (d) used for robust tracking using particle filtering in combination with Voronoi partitioning. [sent-50, score-1.299]
</p><p>15 The corresponding z coordinate is then determined using the occupancy volume in a subsequent step. [sent-52, score-0.485]
</p><p>16 Therefore, in contrast to existing approaches, we are not limited to objects moving on a common ground-plane, which allows for robust tracking of complex scenes, e. [sent-53, score-0.357]
</p><p>17 Additionally, we exploit the 3D scene structure in combination with the tracking results to on-line collect samples for each individual object. [sent-56, score-0.291]
</p><p>18 To overcome this problem, in the following, we propose a novel multiple camera, multiple object tracking approach exploiting 3D geometric information, which is illustrated in Figure 2. [sent-60, score-0.374]
</p><p>19 As a first step, we generate an occupancy volume (see Figure 2c) based on the local mass densities of the 3D visual hull reconstruction (see Figure 2b), which will be introduced and discussed more detailed in Section 2. [sent-61, score-1.03]
</p><p>20 From these occupancy volumes we then estimate occupancy maps (see Figure 2d) and perform the actual tracking step, which is split into two parts to significantly reduce the computational complexity. [sent-63, score-1.142]
</p><p>21 By exploiting the 3D occupancy volume, we are able to obtain exact 3D location estimates and furthermore, are not constrained by the common ground-plane assumption. [sent-66, score-0.452]
</p><p>22 3D Occupancy Volume  Given the foreground segmentations of each camera view (e. [sent-75, score-0.179]
</p><p>23 , obtained from standard background subtraction techniques, such as [24]), we reconstruct the visual hull [20]. [sent-77, score-0.179]
</p><p>24 , no common ground-plane is assumed, and thus are perfectly suited for tracking scenarios, where the objects of interest exhibit challenging poses, as can bee seen in Figure 2a. [sent-91, score-0.389]
</p><p>25 The visual hull reconstruction is sensitive to noise, i. [sent-92, score-0.179]
</p><p>26 , missing or false positive foreground segmentations cause holes in the volume or ghost artifacts. [sent-94, score-0.319]
</p><p>27 To overcome this problem, we propose an occupancy volume which incorporates information about the voxel’s neighborhood. [sent-95, score-0.485]
</p><p>28 The 3D occu-  pancy volume can be derived from the visual hull by computing the local mass density m for every voxel vi, as  m(vi) =? [sent-97, score-0.655]
</p><p>29 Thus, for the task of tracking humans, we define the neighborhood by a cuboid as Nvi  =? [sent-102, score-0.324]
</p><p>30 Furthermore, by defining the neighborhood relationship as an axis-aligned cuboid, we can use efficient integral image representations for computing the mass densities. [sent-107, score-0.309]
</p><p>31 The mass density defines a likelihood relationship on the position of an object’s center, i. [sent-108, score-0.34]
</p><p>32 , the objects’ mass centers correspond to high local density values within the occupancy volume. [sent-110, score-0.749]
</p><p>33 ≥  Although ghost artifacts may occur during reconstruction of the visual hull, their effects are significantly reduced by computing the local mass densities (e. [sent-111, score-0.52]
</p><p>34 In general, the mass densities of ghosts vary over time, i. [sent-114, score-0.411]
</p><p>35 The lower mass densities in combination with the closed-world assumption that objects enter and leave the scene at known locations (i. [sent-117, score-0.432]
</p><p>36 , they cannot suddenly appear in the middle of the scene) allow for handling ghost artifacts robustly. [sent-119, score-0.191]
</p><p>37 Tracking using the Occupancy Volume Now, having estimated the occupancy volume, we derive a top view occupancy map M by assigning the maximum alo tcoapl mass density nvcaylu me along t bhye z asxigisn nfogr a given (imx,u ym) coordinate (see Figure 2d). [sent-124, score-1.158]
</p><p>38 The actual tracking step is then performed using a particle filtering approach [15] on M. [sent-125, score-0.405]
</p><p>39 Given the mass density observations zt of the occupancy map, the posterior probability p(xti |zt) is approximated using a finite set of weighted particles z{ˆ xti, wti}. [sent-127, score-0.749]
</p><p>40 tTehde u particle fniiltteer s eskte oftc wheedig so fdar p awrtoicrkles sw { xeˆll for single instances, however, collisions of multiple objects cannot be handled. [sent-128, score-0.214]
</p><p>41 In fact, if objects move close to each other, the respective modes at the occupancy map may coalesce into a single blob, once their visual hulls cannot be separated. [sent-129, score-0.574]
</p><p>42 However, by exploiting the assumption that multiple objects cannot occupy the same location in space at the same time, inspired by [18], we can use an efficient approach based on Voronoi partitioning of the hypotheses space (see Figure 2d). [sent-132, score-0.164]
</p><p>43 , P thNe} c, uPrri n=t s(exti, o yfi) N, we partition tshtiem occupancy map M into} a set C of pairwise-disjoint convex regions Cmi p= M {m int o∈ a sMet C| od(fm pa, Pirwi) e≤-d sdjo(imnt, Pj), v∀exj oin}s, wCher=e d {(·m) mis ∈ ∈the M Eu |c dli(dmea,nP d)ist ≤ance d mfun,cPtio)n,. [sent-136, score-0.409]
</p><p>44 Hence, the particle filter keeps the correct position and cannot drift to nearby modes on the occupancy map. [sent-146, score-0.523]
</p><p>45 Therefore, we  e=g  search for the mass center along the z axis within a local neighborhood of the corresponding xy estimate. [sent-148, score-0.343]
</p><p>46 This additionally allows for correctly tracking objects which exhibit out-of-plane motion. [sent-149, score-0.444]
</p><p>47 Resolving Geometric Ambiguities So far, the proposed algorithm operates solely on the geometric information derived from the binary foreground segmentations. [sent-152, score-0.175]
</p><p>48 First, we identify potential conflicts between the objects Qi = {j | d(Pi, Pj) < τc, ∀j i}, where a robust identity assignment (foPr objects wit,h∀ijn a ra i}d,iu wsh τc on troheb occupancy map cannot be guaranteed based on the geometric information. [sent-155, score-0.648]
</p><p>49 tBedasbeyd on the estimated posterior probability of the logistic regression classifiers we can robustly re-assign the conflicted trackers given the appearance information. [sent-185, score-0.183]
</p><p>50 This also conforms to the closed-world assumption that objects cannot suddenly appear at the middle of the scene, as applied to reduce the effect of ghosts in the visual hull reconstruction. [sent-189, score-0.327]
</p><p>51 For the automatic initialization, we observe the occupancy map at the defined entry areas by extracting maximally stable extremal regions [23]. [sent-190, score-0.443]
</p><p>52 For each candidate region, we compare whether its mass density corresponds to that of an average human. [sent-191, score-0.34]
</p><p>53 Results and Evaluations  In the following, we demonstrate our proposed multiple object tracker on several challenging real-world people tracking scenarios. [sent-196, score-0.514]
</p><p>54 The latter were recorded at our laboratory with a tracking region of approximately 7 m 4 m, using 4w isthtat aic t rAacxkisi Pg1 r3e4g7i cameras. [sent-200, score-0.291]
</p><p>55 This dataset contains various challenges like heavy occlusions, densely crowded situations as well as complex articulations, or abrupt motion changes. [sent-205, score-0.249]
</p><p>56 Further challenges are caused by the similar appearance of all players of a team, as well as strong shadows and reflections on the floor. [sent-206, score-0.189]
</p><p>57 This results in a tracking region of about 15 m 15 m. [sent-213, score-0.291]
</p><p>58 Since people move close to each other after changing their ap-  pearance, these situations impose additional challenges to color based object tracking approaches, as fixed color models cannot deal with changing appearances. [sent-222, score-0.667]
</p><p>59 These scenarios depict leapfrog games where players leap over each other’s stooped backs. [sent-224, score-0.253]
</p><p>60 Furthermore, two people may share the same xy position while performing a leapfrog which violates the closed-world assumption used for the Voronoi partitioning, as discussed in Section 2. [sent-226, score-0.222]
</p><p>61 This sequence shows 4 people playing musical chairs (also known as Going to Jerusalem) and a non-playing moderator who starts and stops the recorded music. [sent-229, score-0.274]
</p><p>62 Furthermore, sitting on the chairs is a rather unusual pose for typical surveillance scenarios and violates the commonly used constraint of standing persons. [sent-233, score-0.299]
</p><p>63 , the chairs which are removed after each round, as well as a static foreground object, i. [sent-236, score-0.159]
</p><p>64 Additionally to these poses, which again violate common tracking assumptions such as upright standing pedestrians or a common ground-plane, a changing background illumination causes further challenges w. [sent-241, score-0.512]
</p><p>65 Additional challenges are introduced by densely crowded situations and frequent occlusions. [sent-247, score-0.214]
</p><p>66 Evaluation Metrics For evaluation, we compute the standard CLEAR multiple object tracking performance metrics [4], i. [sent-250, score-0.337]
</p><p>67 We compute the distance between tracker hypotheses and annotated ground truth objects on the ground-plane to allow a comparison between different approaches. [sent-257, score-0.201]
</p><p>68 Higher MOTA values indicate a better performance, with 1 representing a perfect tracking result. [sent-262, score-0.291]
</p><p>69 Comparison to State-of-the-Art  We compare our proposed tracking algorithm to the state-of-the-art K-Shortest Paths (KSP) tracker3 [3]. [sent-268, score-0.291]
</p><p>70 This tracker operates on a discretized top view representation (grid) and uses peaked probabilistic occupancy maps, which denote the probability that an object is present at a specific grid position. [sent-269, score-0.622]
</p><p>71 Similar to the original formulation, we obtain the input probability maps using the publicly available implementation4 of the probabilistic occupancy map (POM) detector [10]. [sent-270, score-0.409]
</p><p>72 In order to ensure a fair comparison, we use the same foreground segmentations as input to both, our tracking algorithm and the POM detector. [sent-271, score-0.44]
</p><p>73 Based on the POM results, we additionally evaluated the KSP tracker  with varying input parameters, i. [sent-275, score-0.19]
</p><p>74 ch/ so ftware /pom/ 5Additional tracking results are included in the supplemental material. [sent-293, score-0.327]
</p><p>75 222333999977  Datasetτd[m]AlgorithmMOTP [m]MOTATPFPFNIDSFPSNA  accuracy metric MOTA (higher is better), as well as the total number of true positives (TP), false positives (FP), false negatives (misses, FN), and identity switches (IDS). [sent-294, score-0.276]
</p><p>76 Furthermore, we report the runtime performance in frames per second (FPS), as well as the total number of ambiguous situations NA, i. [sent-296, score-0.191]
</p><p>77 Similar to [1], we observed a large number of false positives of the POM detector if noisy foreground segmentations are used as input, e. [sent-300, score-0.219]
</p><p>78 Furthermore, in situations where people exhibit challenging poses, missed detections occur frequently. [sent-303, score-0.265]
</p><p>79 In such situations, the KSP tracker is often not able to link the true positive detections correctly or starts drifting after several frames of missed detections. [sent-304, score-0.204]
</p><p>80 These issues can be seen by the significantly lower tracking accuracy at the APIDIS, POSE, and TABLE scenarios. [sent-305, score-0.291]
</p><p>81 Considering the high number of identity switches, the KSP tracker obviously suffers from the missing color information, especially in crowded scenarios. [sent-307, score-0.251]
</p><p>82 For fair compari-  ×  son, we evaluated the proposed approach without discriminative appearance models for resolving geometrically ambiguous situations (reported as Prop. [sent-308, score-0.224]
</p><p>83 , trajectory assignment is solely based on the geometric information derived from the occupancy volume. [sent-311, score-0.449]
</p><p>84 As the local mass densities provide valuable cues for tracking, we still achieve better performances on more complex scenarios compared to the KSP approach, even without using additional color information. [sent-312, score-0.452]
</p><p>85 By additionally using a discriminative classifier to resolve these ambiguous situations, we achieve excellent tracking results, especially w. [sent-313, score-0.382]
</p><p>86 , the single identity switch at the CHAP scenario occurs after a person leaves the tracking region, changes his clothes outside, and then re-enters the scene. [sent-318, score-0.358]
</p><p>87 Since the KSP tracker is based on a discretized top view representation, it is constrained by the spatial resolution of the grid. [sent-322, score-0.168]
</p><p>88 As can be seen from the reported metrics on the APIDIS dataset, we still achieve very accurate and precise tracking results, despite the challenges caused by shadowing effects  and heavy reflections, as well as the complex and fast movement of the players. [sent-327, score-0.427]
</p><p>89 Although the on-line sample collection facilitates correctly tracking players of different teams, identity switches occur due to the similar appearance of players within a team. [sent-328, score-0.661]
</p><p>90 We achieve frame rates of up to 12 fps for standard tracking scenarios, although only the visual hull reconstruction and the occupancy volume are computed on the GPU, exploiting the inherent parallelism. [sent-336, score-1.085]
</p><p>91 The KSP tracker achieves very high frame rates due to the efficient shortest path computation. [sent-338, score-0.168]
</p><p>92 We report the runtimes for those KSP/POM configurations which achieve the best tracking performance. [sent-339, score-0.291]
</p><p>93 Thus, the reported frame rates vary for scenarios with similar input data, as the KSP runtime depends on the spatial grid density. [sent-340, score-0.164]
</p><p>94 In contrast, the POM detector exhibits a significantly  lower frame rate caused by the high resolution of the input images, as well as the required parameter configurations to handle the noisy foreground segmentations. [sent-341, score-0.155]
</p><p>95 Conclusion We proposed a real-time capable multi-object tracking approach based on local mass densities of visual hull reconstructions. [sent-346, score-0.836]
</p><p>96 In contrast to existing tracking approaches for calibrated camera networks with partially overlapping views, we are not constrained by the common ground-plane assumption and additionally reduce artifacts rising from noisy foreground masks. [sent-347, score-0.529]
</p><p>97 In particular, individual objects are tracked using the local mass density scores within a particle filter framework, constraining nearby trackers by a Voronoi partitioning. [sent-348, score-0.558]
</p><p>98 These situations show the basketball court (a), the leapfrog exercises (b), the musical chairs game (c), and violations of the common ground-plane assumption (d). [sent-474, score-0.465]
</p><p>99 Closed-world tracking of multiple interacting targets for indoor-sports applications. [sent-489, score-0.324]
</p><p>100 Multi-person tracking with overlapping cameras in complex, dynamic environments. [sent-506, score-0.336]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('occupancy', 0.409), ('ksp', 0.328), ('tracking', 0.291), ('apidis', 0.277), ('mass', 0.276), ('pom', 0.157), ('voronoi', 0.155), ('hull', 0.147), ('tracker', 0.135), ('particle', 0.114), ('situations', 0.11), ('players', 0.1), ('densities', 0.09), ('foreground', 0.09), ('people', 0.088), ('scenarios', 0.086), ('standing', 0.078), ('chap', 0.076), ('volume', 0.076), ('mota', 0.071), ('chairs', 0.069), ('switches', 0.069), ('identity', 0.067), ('conflicted', 0.067), ('leapfrog', 0.067), ('musical', 0.067), ('objects', 0.066), ('basketball', 0.065), ('density', 0.064), ('artifacts', 0.063), ('volumetric', 0.06), ('voxel', 0.06), ('segmentations', 0.059), ('ghost', 0.059), ('motp', 0.056), ('additionally', 0.055), ('challenges', 0.055), ('partitioning', 0.055), ('fps', 0.054), ('projections', 0.052), ('moderator', 0.05), ('xti', 0.05), ('crowded', 0.049), ('metrics', 0.046), ('changing', 0.046), ('cameras', 0.045), ('ghosts', 0.045), ('nvi', 0.045), ('stepping', 0.045), ('court', 0.045), ('cm', 0.045), ('runtime', 0.045), ('operates', 0.045), ('resolving', 0.044), ('robustly', 0.044), ('exploiting', 0.043), ('game', 0.042), ('upright', 0.042), ('geometric', 0.04), ('trackers', 0.038), ('furthermore', 0.037), ('horst', 0.037), ('ghosting', 0.037), ('suddenly', 0.037), ('masses', 0.037), ('eurasip', 0.037), ('ambiguous', 0.036), ('hulls', 0.036), ('ftware', 0.036), ('articulations', 0.036), ('false', 0.035), ('positives', 0.035), ('heavy', 0.035), ('missed', 0.035), ('extremal', 0.034), ('drifting', 0.034), ('cvlab', 0.034), ('collisions', 0.034), ('appearance', 0.034), ('silhouette', 0.034), ('xy', 0.034), ('leaf', 0.033), ('violates', 0.033), ('volumes', 0.033), ('surveillance', 0.033), ('frame', 0.033), ('vi', 0.033), ('neighborhood', 0.033), ('pami', 0.033), ('discretized', 0.033), ('targets', 0.033), ('multicamera', 0.032), ('berclaz', 0.032), ('visual', 0.032), ('exhibits', 0.032), ('handling', 0.032), ('exhibit', 0.032), ('move', 0.031), ('cartesian', 0.031), ('camera', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="365-tfidf-1" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>2 0.23342182 <a title="365-tfidf-2" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>Author: Jingchen Liu, Peter Carr, Robert T. Collins, Yanxi Liu</p><p>Abstract: We employ hierarchical data association to track players in team sports. Player movements are often complex and highly correlated with both nearby and distant players. A single model would require many degrees of freedom to represent the full motion diversity and could be difficult to use in practice. Instead, we introduce a set of Game Context Features extracted from noisy detections to describe the current state of the match, such as how the players are spatially distributed. Our assumption is that players react to the current situation in only a finite number of ways. As a result, we are able to select an appropriate simplified affinity model for each player and time instant using a random decisionforest based on current track and game contextfeatures. Our context-conditioned motion models implicitly incorporate complex inter-object correlations while remaining tractable. We demonstrate significant performance improvements over existing multi-target tracking algorithms on basketball and field hockey sequences several minutes in duration and containing 10 and 20 players respectively.</p><p>3 0.23035346 <a title="365-tfidf-3" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>Author: Shoou-I Yu, Yi Yang, Alexander Hauptmann</p><p>Abstract: A device just like Harry Potter’s Marauder’s Map, which pinpoints the location ofeachperson-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust lo- calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.</p><p>4 0.21812084 <a title="365-tfidf-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.21676174 <a title="365-tfidf-5" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>6 0.19879922 <a title="365-tfidf-6" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>7 0.18538584 <a title="365-tfidf-7" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>8 0.1765068 <a title="365-tfidf-8" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>9 0.16064373 <a title="365-tfidf-9" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>10 0.15971522 <a title="365-tfidf-10" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>11 0.15552871 <a title="365-tfidf-11" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>12 0.13802758 <a title="365-tfidf-12" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>13 0.13617802 <a title="365-tfidf-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.13364583 <a title="365-tfidf-14" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>15 0.12213787 <a title="365-tfidf-15" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>16 0.11715834 <a title="365-tfidf-16" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>17 0.11276514 <a title="365-tfidf-17" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>18 0.10653303 <a title="365-tfidf-18" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>19 0.097661696 <a title="365-tfidf-19" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>20 0.097315088 <a title="365-tfidf-20" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.217), (1, 0.051), (2, 0.008), (3, -0.101), (4, 0.011), (5, -0.063), (6, 0.156), (7, -0.132), (8, 0.105), (9, 0.208), (10, -0.066), (11, -0.083), (12, -0.061), (13, 0.081), (14, -0.005), (15, -0.022), (16, -0.01), (17, 0.071), (18, 0.054), (19, -0.011), (20, 0.105), (21, 0.102), (22, -0.026), (23, 0.04), (24, -0.041), (25, -0.019), (26, 0.072), (27, -0.046), (28, -0.06), (29, -0.085), (30, -0.045), (31, 0.071), (32, -0.02), (33, 0.015), (34, 0.047), (35, -0.031), (36, -0.001), (37, -0.011), (38, 0.006), (39, -0.024), (40, -0.002), (41, -0.057), (42, 0.05), (43, 0.032), (44, 0.027), (45, 0.038), (46, 0.064), (47, -0.029), (48, -0.052), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95103914 <a title="365-lsi-1" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>2 0.82899255 <a title="365-lsi-2" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>3 0.77657586 <a title="365-lsi-3" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>Author: Shoou-I Yu, Yi Yang, Alexander Hauptmann</p><p>Abstract: A device just like Harry Potter’s Marauder’s Map, which pinpoints the location ofeachperson-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust lo- calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.</p><p>4 0.75223422 <a title="365-lsi-4" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>5 0.75097901 <a title="365-lsi-5" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>6 0.74512494 <a title="365-lsi-6" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>7 0.74413413 <a title="365-lsi-7" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>8 0.73477525 <a title="365-lsi-8" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>9 0.72040474 <a title="365-lsi-9" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>10 0.70057619 <a title="365-lsi-10" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>11 0.69468427 <a title="365-lsi-11" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>12 0.68149155 <a title="365-lsi-12" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>13 0.6775887 <a title="365-lsi-13" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>14 0.65687871 <a title="365-lsi-14" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>15 0.64341909 <a title="365-lsi-15" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>16 0.63383687 <a title="365-lsi-16" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>17 0.61941075 <a title="365-lsi-17" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>18 0.56362957 <a title="365-lsi-18" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>19 0.54444474 <a title="365-lsi-19" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>20 0.52432889 <a title="365-lsi-20" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.132), (16, 0.027), (26, 0.054), (33, 0.221), (44, 0.152), (67, 0.07), (69, 0.076), (80, 0.027), (86, 0.014), (87, 0.151)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89630538 <a title="365-lda-1" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>same-paper 2 0.88914573 <a title="365-lda-2" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>3 0.85916489 <a title="365-lda-3" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>4 0.85567051 <a title="365-lda-4" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>5 0.85415781 <a title="365-lda-5" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>Author: Michael Kolomenkin, Ilan Shimshoni, Ayellet Tal</p><p>Abstract: This paper extends to surfaces the multi-scale approach of edge detection on images. The common practice for detecting curves on surfaces requires the user to first select the scale of the features, apply an appropriate smoothing, and detect the edges on the smoothed surface. This approach suffers from two drawbacks. First, it relies on a hidden assumption that all the features on the surface are of the same scale. Second, manual user intervention is required. In this paper, we propose a general framework for automatically detecting the optimal scale for each point on the surface. We smooth the surface at each point according to this optimal scale and run the curve detection algorithm on the resulting surface. Our multi-scale algorithm solves the two disadvantages of the single-scale approach mentioned above. We demonstrate how to realize our approach on two commonly-used special cases: ridges & valleys and relief edges. In each case, the optimal scale is found in accordance with the mathematical definition of the curve.</p><p>6 0.85141957 <a title="365-lda-6" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>7 0.85019028 <a title="365-lda-7" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>8 0.85007137 <a title="365-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.84731793 <a title="365-lda-9" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>10 0.84489208 <a title="365-lda-10" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>11 0.84386939 <a title="365-lda-11" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>12 0.84188378 <a title="365-lda-12" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>13 0.84180886 <a title="365-lda-13" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>14 0.83865786 <a title="365-lda-14" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>15 0.83808893 <a title="365-lda-15" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>16 0.8374083 <a title="365-lda-16" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>17 0.83688509 <a title="365-lda-17" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>18 0.83683181 <a title="365-lda-18" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>19 0.83541679 <a title="365-lda-19" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>20 0.83522022 <a title="365-lda-20" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
