<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-82" href="#">cvpr2013-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</h1>
<br/><p>Source: <a title="cvpr-2013-82-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Fenzi_Class_Generative_Models_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Michele Fenzi, Laura Leal-Taixé, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models, i.e., regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset [17] and the Pointing’04 face dataset [8]. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.</p><p>Reference: <a title="cvpr-2013-82-reference" href="../cvpr2013_reference/cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de ,  Abstract In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. [sent-3, score-0.973]
</p><p>2 , regression functions learnt from local descriptors of the same patch collected under different viewpoints. [sent-6, score-0.421]
</p><p>3 The individual generative models are then clustered in order to create class generative models which form the class representation. [sent-7, score-0.762]
</p><p>4 At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. [sent-8, score-0.993]
</p><p>5 Introduction In the last few years, the computer vision community has  seen a large increase in the interest dedicated to the problem of pose estimation for object classes. [sent-12, score-0.523]
</p><p>6 While pose estimation for individual objects has been addressed by a plethora of techniques [19, 11, 7], their extension to object classes is not straightforward. [sent-13, score-0.558]
</p><p>7 The previous works can be distinctively split into two groups: those that are based on an explicit 3D class model, either synthetic [12, 18] or reconstructed ad  Figure1. [sent-18, score-0.324]
</p><p>8 Genrativefaturemodelsarelarntfomdiferntviews  of the same patch and clustered in appearance and pose space to form class generative models. [sent-19, score-0.958]
</p><p>9 However, the price to pay for avoiding an  explicit 3D class representation is very often an estimation of the pose limited to a few discrete viewpoints. [sent-23, score-0.73]
</p><p>10 The method we propose bridges this gap, as it returns a continuous value for the pose of an unknown class instance by using only 2D data combined to weak viewpoint labelling without using an explicit 3D model. [sent-24, score-0.961]
</p><p>11 Contribution and Motivation Contribution: First, we propose a way to learn a class representation based on feature generative models derived 7 7 7 5 5 5 53 353  from training class instances. [sent-28, score-0.582]
</p><p>12 Each model is based on a regression function learnt from descriptors of the same patch, and is designed to yield an estimate of the patch descriptor given a query pose. [sent-29, score-0.742]
</p><p>13 We show that the variation is smooth in the descriptor space as function of the pose change, and we demonstrate how to use this information profitably. [sent-32, score-0.538]
</p><p>14 Finally, we propose a method to group the individual generative models by combining dynamic time warping and  spectral clustering. [sent-33, score-0.313]
</p><p>15 The clusters are thus conceived as class generative models and used to provide a continuous MAP estimation for the pose of the query instance. [sent-35, score-1.235]
</p><p>16 Motivation: Since pose estimation is ultimately a continuous problem, any method pursuing only a discrete pose estimation (i. [sent-36, score-1.137]
</p><p>17 Our motivation to use regression in order to estimate a continuous value is thus a natural choice. [sent-40, score-0.295]
</p><p>18 In this regard, the motivation for our contribution is that a class instance is best explained as a combination of individual parts appearing separately on different instances, rather than a weighted combination of whole instances. [sent-42, score-0.263]
</p><p>19 As shown in the experimental section, our method obtains more than 10% improvement in terms of pose accuracy with respect to [24]. [sent-44, score-0.442]
</p><p>20 Section 3 introduces the generative feature model, while Section 4 provides a detailed description of our MAP method for class pose estimation. [sent-46, score-0.861]
</p><p>21 Related Works Feature-based pose estimation using multiple views can be targeted towards individual objects and object classes. [sent-49, score-0.597]
</p><p>22 This is followed by a matching step between the query image and the model features, and the estimation of the object pose through the solution of the PnP problem. [sent-51, score-0.748]
</p><p>23 In the case of object classes, works can be split into two distinct groups: those that share this paradigm by employing a full 3D class model, and those relying only on 2D information or on combining 2D data with weak viewpoint annotation. [sent-52, score-0.475]
</p><p>24 Among those using an explicit 3D model, [13, 12, 18] exploit synthetic CAD models to refine rough pose hypotheses determined by the voting of a bank of global and partbased SVM classifiers trained on appearance patches. [sent-53, score-0.52]
</p><p>25 In [6], the models are fused together and voting is performed directly in the pose space. [sent-56, score-0.442]
</p><p>26 [21, 22] propose a method that, by grouping spatially close  2D features over the training images, finds and links canonical object parts among different class instances by their mutual homographies. [sent-58, score-0.286]
</p><p>27 Their approach performs pose estimation only in terms of finding the most similar canonical view within the training set. [sent-59, score-0.58]
</p><p>28 The method proposed by [17] returns only a quantized pose value, as 16 bins are used to represent the one-dimensional pose space. [sent-60, score-0.964]
</p><p>29 In their method, a SVM classifier is trained for each discrete pose with spatial pyramids of histograms based on clustered dense features. [sent-61, score-0.553]
</p><p>30 The quantized output pose is estimated by computing a joint pdf distance between test video parts and the training pdf’s of different instances of the same class. [sent-66, score-0.673]
</p><p>31 While all previous approaches return discrete pose estimates, two works which provide continuous values are [24, 9]. [sent-67, score-0.604]
</p><p>32 In [9], viewpoint classification is extended so that a continuous pose can be estimated by finding the maximum of a score function with respect to linearly deformed viewpoint templates. [sent-68, score-0.718]
</p><p>33 The variation in the component amplitude as function of the pose change shows to be smooth. [sent-75, score-0.442]
</p><p>34 The feature descriptor behaviour is modelled by means of a regression function learnt from training patches, as function of the object pose. [sent-81, score-0.534]
</p><p>35 Once learnt, a prediction of the patch appearance given a query pose can be obtained. [sent-82, score-0.797]
</p><p>36 , (fni, αni)},  (1)  where fji is the k-dimensional descriptor representing the neighbourhood of the patch in pose αji . [sent-91, score-0.678]
</p><p>37 So, it is possible to design a vector-valued function, our feature generative model, Fi : Rm → Rk, where m is the dimensionality of the pose space, as a mapping between pose and feature descriptor space. [sent-93, score-1.275]
</p><p>38 Our goal is to learn such a function for each training track ti in order to have a descriptor estimate = Fi (α) at pose α. [sent-94, score-0.853]
</p><p>39 Instead of using a plain least-square approach, a regularization term must be introduced, as the training track can be affected by camera noise, imperfect pose labelling, and outliers. [sent-102, score-0.695]
</p><p>40 Briefly, we first collect all generative models learnt from the training instances and we cluster them on the basis of their similarity both in descriptor and pose space. [sent-105, score-1.075]
</p><p>41 During run-time, the query features are set in correspondence with the clusters and the instance pose is estimated in a Bayesian fashion as the one maximizing the joint MAP probability of the observation of the query features. [sent-107, score-1.063]
</p><p>42 Since a class structure can be explained as a combination of its individual realizations, we can explain the class structure behaviour as a weighted combination of local patch behaviours, i. [sent-109, score-0.457]
</p><p>43 Class Generative Feature Models Given a set of N feature tracks T = {ti}iN=1 collected fromGi vdeifnfe are snett training itnusrtean trcaecsk, sw Ther =e e{tac}h track is defined as in (1), the first step to build a class representation is to cluster the tracks. [sent-114, score-0.682]
</p><p>44 As we want to cluster feature tracks that are similar in both the appearance and in the viewpoint space, we have first designed a similarity score for pairs of tracks based on dynamic time warping. [sent-115, score-0.649]
</p><p>45 In our case, we impose two restrictions: (a) Tracks not overlapping in the pose space are assigned a null similarity score; 7 7 7 5 5 5 75 575  (b) The alignment must be monotonic, i. [sent-117, score-0.549]
</p><p>46 We use a dynamic programming implementation for our track similarity score computation. [sent-120, score-0.27]
</p><p>47 The cost definition takes into account both the distance in the descriptor and in the pose space. [sent-128, score-0.538]
</p><p>48 Pose Estimation in a Bayesian Framework If the feature descriptor f is found in the current image, we can define a probability functionp(α, c|f) that expresses twhee claikneldiehfoionde aofp rfo being yobfsuenrcvtieodn frpo(αm, pose α atnedx pmreatscshesing to cluster c. [sent-155, score-0.675]
</p><p>49 (9)  The maximum a posteriori object pose α∗ and match c∗ are thus the ones maximizing this probability, i. [sent-157, score-0.541]
</p><p>50 (10) Since performing maximization over pose and matches can be computationally infeasible at run-time, we first match the test features against the cluster models. [sent-160, score-0.571]
</p><p>51 Matching is performed by finding the cluster center that is the nearest neighbour in the descriptor space to the query feature. [sent-161, score-0.552]
</p><p>52 The prior p(α) can be defined, for example, given a SVM pose classifier, as uniformly distributed over the classifier output bin and null elsewhere. [sent-164, score-0.576]
</p><p>53 a nW bee use the regressors of the tracks contained in each cluster to provide an estimation of the likelihood of the feature descriptor f, already matching with cluster c, being observed from pose α. [sent-167, score-1.036]
</p><p>54 Let ei = f Fi (α) be the error between the query feature and t=he descriptor estimated by the i-th regressor of cluster c. [sent-171, score-0.552]
</p><p>55 2, so that cluster tracks closer in descriptor space ? [sent-187, score-0.341]
</p><p>56 method yields the MAP pose approximation as  (14) Then, our  α∗= argαmaxp(α|F,C) ≈ argαmax? [sent-199, score-0.442]
</p><p>57 We show that our algorithm performs better than the state of the art in estimating the pose of an unknown instance given the class representation built as above. [sent-203, score-0.657]
</p><p>58 The sequences are densely sampled in the pose space, but many cars are uncommonly shaped, making the model generalization very challenging. [sent-206, score-0.557]
</p><p>59 Before showing the performance of our algorithm for class pose estimation, we also demonstrate that it can be also applied to the single exemplar case. [sent-209, score-0.665]
</p><p>60 To this end, we learn a model from a subset of training images of one instance and we evaluate the pose estimation performance on the remaining pictures. [sent-210, score-0.622]
</p><p>61 First, we extract local features from the training images and we form a set of model tracks T by tracking the feataunrdes w over rtmhe training mvioedwels. [sent-219, score-0.306]
</p><p>62 For each track, a regression function is learnt as described in  Section 3. [sent-221, score-0.278]
</p><p>63 At run-time, a set of features F is extracted from tSheec query image a-ntidm me,a atc sheetd o against eths eF s iest eofx tmraocdteedl tfrraocmk representatives defined as the features corresponding to the middle viewpoint of the track. [sent-222, score-0.487]
</p><p>64 Finally, we return the maximum a posteriori estimate of the pose by using (15). [sent-223, score-0.577]
</p><p>65 sTehte o pose prior aicsk asss Tum , aesd o ntol y be o uniformly edis istr ciobuntseidd over the entire pose space. [sent-225, score-0.92]
</p><p>66 In comparison to [24], where only the result for the first sequence is provided, we improve the pose accuracy by almost 10% (1. [sent-236, score-0.442]
</p><p>67 Pose Estimation in the Class case In the following, we provide the experimental results of our method in the class case for two datasets, the EPFL Multi-view car dataset and the Pointing’04 face dataset. [sent-242, score-0.295]
</p><p>68 [24] - 50% split 5NN Track - 50% split Regression 5NN - 50% split Our Full Method - 50% split  MAE [◦] 90% percentile  MAE [◦] 95% percentile  -  -  19. [sent-245, score-0.694]
</p><p>69 As in the single exemplar case, SIFT features  are extracted from the training images, model tracks are collected by tracking the features over the training views and a regressor function is learnt for each track. [sent-280, score-0.613]
</p><p>70 tAhet rnu gnr-otuimpee,d a pose itn otefr cvluasl tfeorrs th Ce a query image nis S feicrtstio oenst 4i-. [sent-282, score-0.667]
</p><p>71 It is an improved version of the Deformable Part Models (DPMs) introduced by [4] and it returns a discrete pose estimate over 4, 8 or 16 bins. [sent-284, score-0.51]
</p><p>72 For this experiment, we have used its 16-bin implementation, thus receiving a pose interval estimation of size 22. [sent-285, score-0.523]
</p><p>73 Then, a set of features F is extracted from the query image nan, da m seatt ochfe fde against Fth ies c elxustrtaecrt representatives dereyfin imedas the cluster centres. [sent-287, score-0.451]
</p><p>74 Eventually, the maximum a posteriori estimate of the pose is computed by using (15), where the pose prior is defined as uniformly distributed over the interval returned by the classifier and null elsewhere. [sent-288, score-1.16]
</p><p>75 We considered [17] as baseline method because they introduced the dataset, and [24] as state-of-the-art method because they also employ a regression-based pose estimation. [sent-289, score-0.442]
</p><p>76 Evaluated implementations: 5NN Track We find the 5 nearest neighbours among the model track representatives for each query feature. [sent-293, score-0.6]
</p><p>77 The returned pose is the mode calculated on the training viewpoints of the feature in each matching track which is closest to the query feature in descriptor space. [sent-294, score-1.151]
</p><p>78 This is a naive implementation providing results only in terms of the viewpoint of the five nearest neighbour tracks. [sent-295, score-0.289]
</p><p>79 (No regression and no track clustering) Regression 5NN We find again the 5 nearest neighbours among the track representatives for each query feature. [sent-297, score-0.984]
</p><p>80 The pose is estimated by using the track regression functions as described in the single exemplar case. [sent-298, score-0.95]
</p><p>81 This second variant shows the benefit of introducing regression for pose estimation. [sent-299, score-0.679]
</p><p>82 With respect to the naive 5NN variant, it shows that regression improves pose accuracy. [sent-300, score-0.69]
</p><p>83 (No track clustering) Full Method We find the nearest neighbour cluster centre for each queryfeature. [sent-301, score-0.427]
</p><p>84 Themaximum a posteriori pose is estimated as described in Section 4. [sent-302, score-0.58]
</p><p>85 It shows a further increase in the resulting pose accuracy. [sent-303, score-0.442]
</p><p>86 This can be taken as a further confirmation ofthe superiority of local approaches for pose estimation over global ones. [sent-314, score-0.523]
</p><p>87 The naive implementation performs only slightly worse than the regression variant as the training images are very finely sampled in pose. [sent-316, score-0.354]
</p><p>88 Method 5NN Track - 50% split Regression 5NN - 50% split Our Full Method - 50% split  MAE [◦] 6. [sent-320, score-0.447]
</p><p>89 2  Pointing’04 face dataset  The dataset is composed of 2790 face images whose pose is discretized in 9 and 13 angles of pitch and yaw, respectively. [sent-329, score-0.638]
</p><p>90 We choose this dataset as the pose sampling is coarser, with a minimum angular distances of 15◦ in both yaw and pitch. [sent-330, score-0.529]
</p><p>91 The slightly worse performance in the pitch estimation for all methods is due to the coarser sampling ofthe pitch poses. [sent-337, score-0.369]
</p><p>92 Unlike the previous experiment, the benefit of using feature regression comes out more evidently when the pose sampling is coarser with an increase in pose accuracy of 42%. [sent-338, score-1.214]
</p><p>93 We can think that each class generative model explains the query pose by combining the descriptors of the same person in neighbouring poses and the descriptors belonging to other persons, potentially available at the same viewpoint of the query image. [sent-340, score-1.466]
</p><p>94 Conclusion We proposed a novel method for learning a class representation in order to provide a continuous estimate for  the pose of an unknown class instance. [sent-356, score-0.847]
</p><p>95 , regression functions learnt from features computed on different views of the same patch. [sent-359, score-0.355]
</p><p>96 By clustering individual models in the appearance and pose space, we obtain a set of consistent class generative models that form the class representation. [sent-361, score-1.048]
</p><p>97 The pose of a query instance is given as a maximum a posteriori estimation by using the matching class generative models. [sent-362, score-1.23]
</p><p>98 Experiments show that our method provides a pose that is 10% more accurate than state-of-the-art methods, as a local-based approach explains a query instance better than a global one. [sent-363, score-0.709]
</p><p>99 The pose of the car in the bottom right corner is hard to guess also for a human being. [sent-408, score-0.525]
</p><p>100 The ground truth pose (green) and the estimated  one  (red)  are  given in a box. [sent-411, score-0.481]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pose', 0.442), ('mae', 0.267), ('query', 0.225), ('generative', 0.203), ('track', 0.196), ('regression', 0.188), ('tracks', 0.154), ('split', 0.149), ('class', 0.138), ('pitch', 0.118), ('epfl', 0.106), ('posteriori', 0.099), ('representatives', 0.097), ('neighbour', 0.097), ('descriptor', 0.096), ('leave', 0.093), ('cluster', 0.091), ('learnt', 0.09), ('patch', 0.089), ('viewpoint', 0.089), ('torki', 0.088), ('yaw', 0.087), ('exemplar', 0.085), ('car', 0.083), ('estimation', 0.081), ('pointing', 0.081), ('cf', 0.08), ('dp', 0.076), ('maxp', 0.07), ('cmn', 0.066), ('gourier', 0.066), ('null', 0.064), ('full', 0.064), ('ti', 0.062), ('sequences', 0.062), ('savarese', 0.06), ('naive', 0.06), ('continuous', 0.059), ('urmethod', 0.058), ('haj', 0.058), ('minj', 0.058), ('arg', 0.058), ('behaviour', 0.057), ('training', 0.057), ('regressor', 0.055), ('descriptors', 0.054), ('axp', 0.054), ('instances', 0.053), ('cars', 0.053), ('coarser', 0.052), ('clusters', 0.052), ('clustering', 0.051), ('fji', 0.051), ('liebelt', 0.051), ('tnt', 0.051), ('variant', 0.049), ('percentile', 0.049), ('labelling', 0.048), ('motivation', 0.048), ('ji', 0.047), ('surf', 0.047), ('feature', 0.046), ('clustered', 0.045), ('wji', 0.045), ('spectral', 0.044), ('quantized', 0.044), ('evidently', 0.044), ('nearest', 0.043), ('similarity', 0.043), ('returned', 0.043), ('instance', 0.042), ('appearance', 0.041), ('implementations', 0.041), ('face', 0.039), ('estimated', 0.039), ('views', 0.039), ('neighbours', 0.039), ('variants', 0.039), ('pdf', 0.038), ('features', 0.038), ('explicit', 0.037), ('accumulation', 0.037), ('uniformly', 0.036), ('return', 0.036), ('neighbouring', 0.036), ('fleuret', 0.036), ('returns', 0.036), ('individual', 0.035), ('unknown', 0.035), ('provide', 0.035), ('weak', 0.035), ('classifier', 0.034), ('fi', 0.034), ('atomic', 0.034), ('variability', 0.033), ('restrictions', 0.033), ('discrete', 0.032), ('stacked', 0.032), ('description', 0.032), ('dynamic', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="82-tfidf-1" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>Author: Michele Fenzi, Laura Leal-Taixé, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models, i.e., regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset [17] and the Pointing’04 face dataset [8]. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.</p><p>2 0.26250696 <a title="82-tfidf-2" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>3 0.25145712 <a title="82-tfidf-3" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>4 0.18126544 <a title="82-tfidf-4" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>5 0.17408447 <a title="82-tfidf-5" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>Author: Kota Hara, Rama Chellappa</p><p>Abstract: We present a hierarchical method for human pose estimation from a single still image. In our approach, a dependency graph representing relationships between reference points such as bodyjoints is constructed and thepositions of these reference points are sequentially estimated by a successive application of multidimensional output regressions along the dependency paths, starting from the root node. Each regressor takes image features computed from an image patch centered on the current node ’s position estimated by the previous regressor and is specialized for estimating its child nodes ’ positions. The use of the dependency graph allows us to decompose a complex pose estimation problem into a set of local pose estimation problems that are less complex. We design a dependency graph for two commonly used human pose estimation datasets, the Buffy Stickmen dataset and the ETHZ PASCAL Stickmen dataset, and demonstrate that our method achieves comparable accuracy to state-of-the-art results on both datasets with significantly lower computation time than existing methods. Furthermore, we propose an importance weighted boosted re- gression trees method for transductive learning settings and demonstrate the resulting improved performance for pose estimation tasks.</p><p>6 0.16589181 <a title="82-tfidf-6" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>7 0.15555541 <a title="82-tfidf-7" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>8 0.15447229 <a title="82-tfidf-8" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>9 0.15298086 <a title="82-tfidf-9" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>10 0.152421 <a title="82-tfidf-10" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>11 0.15132879 <a title="82-tfidf-11" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>12 0.14981019 <a title="82-tfidf-12" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>13 0.14947675 <a title="82-tfidf-13" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>14 0.14920372 <a title="82-tfidf-14" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>15 0.13881359 <a title="82-tfidf-15" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>16 0.13690424 <a title="82-tfidf-16" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>17 0.13520472 <a title="82-tfidf-17" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>18 0.13230455 <a title="82-tfidf-18" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>19 0.13111158 <a title="82-tfidf-19" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>20 0.12848344 <a title="82-tfidf-20" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.304), (1, -0.054), (2, -0.043), (3, -0.083), (4, 0.025), (5, 0.005), (6, 0.028), (7, -0.033), (8, 0.1), (9, -0.168), (10, -0.077), (11, 0.172), (12, -0.013), (13, 0.079), (14, -0.034), (15, -0.083), (16, 0.033), (17, -0.081), (18, -0.036), (19, -0.1), (20, 0.109), (21, 0.017), (22, -0.071), (23, -0.003), (24, -0.114), (25, -0.023), (26, -0.081), (27, -0.048), (28, 0.007), (29, -0.015), (30, 0.083), (31, -0.023), (32, -0.056), (33, -0.045), (34, 0.002), (35, 0.047), (36, 0.084), (37, -0.021), (38, 0.033), (39, -0.044), (40, -0.031), (41, -0.051), (42, 0.033), (43, -0.015), (44, -0.005), (45, 0.093), (46, 0.014), (47, -0.034), (48, 0.021), (49, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97328627 <a title="82-lsi-1" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>Author: Michele Fenzi, Laura Leal-Taixé, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models, i.e., regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset [17] and the Pointing’04 face dataset [8]. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.</p><p>2 0.79526234 <a title="82-lsi-2" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>Author: Edgar Simo-Serra, Ariadna Quattoni, Carme Torras, Francesc Moreno-Noguer</p><p>Abstract: We introduce a novel approach to automatically recover 3D human pose from a single image. Most previous work follows a pipelined approach: initially, a set of 2D features such as edges, joints or silhouettes are detected in the image, and then these observations are used to infer the 3D pose. Solving these two problems separately may lead to erroneous 3D poses when the feature detector has performed poorly. In this paper, we address this issue by jointly solving both the 2D detection and the 3D inference problems. For this purpose, we propose a Bayesian framework that integrates a generative model based on latent variables and discriminative 2D part detectors based on HOGs, and perform inference using evolutionary algorithms. Real experimentation demonstrates competitive results, and the ability of our methodology to provide accurate 2D and 3D pose estimations even when the 2D detectors are inaccurate.</p><p>3 0.7853452 <a title="82-lsi-3" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>Author: Kota Hara, Rama Chellappa</p><p>Abstract: We present a hierarchical method for human pose estimation from a single still image. In our approach, a dependency graph representing relationships between reference points such as bodyjoints is constructed and thepositions of these reference points are sequentially estimated by a successive application of multidimensional output regressions along the dependency paths, starting from the root node. Each regressor takes image features computed from an image patch centered on the current node ’s position estimated by the previous regressor and is specialized for estimating its child nodes ’ positions. The use of the dependency graph allows us to decompose a complex pose estimation problem into a set of local pose estimation problems that are less complex. We design a dependency graph for two commonly used human pose estimation datasets, the Buffy Stickmen dataset and the ETHZ PASCAL Stickmen dataset, and demonstrate that our method achieves comparable accuracy to state-of-the-art results on both datasets with significantly lower computation time than existing methods. Furthermore, we propose an importance weighted boosted re- gression trees method for transductive learning settings and demonstrate the resulting improved performance for pose estimation tasks.</p><p>4 0.75730246 <a title="82-lsi-4" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>Author: Ben Sapp, Ben Taskar</p><p>Abstract: We propose a multimodal, decomposable model for articulated human pose estimation in monocular images. A typical approach to this problem is to use a linear structured model, which struggles to capture the wide range of appearance present in realistic, unconstrained images. In this paper, we instead propose a model of human pose that explicitly captures a variety of pose modes. Unlike other multimodal models, our approach includes both global and local pose cues and uses a convex objective and joint training for mode selection and pose estimation. We also employ a cascaded mode selection step which controls the trade-off between speed and accuracy, yielding a 5x speedup in inference and learning. Our model outperforms state-of-theart approaches across the accuracy-speed trade-off curve for several pose datasets. This includes our newly-collected dataset of people in movies, FLIC, which contains an order of magnitude more labeled data for training and testing than existing datasets. The new dataset and code are avail- able online. 1</p><p>5 0.71379012 <a title="82-lsi-5" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>Author: Georgia Gkioxari, Pablo Arbeláez, Lubomir Bourdev, Jitendra Malik</p><p>Abstract: We propose a novel approach for human pose estimation in real-world cluttered scenes, and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose, we build on the notion of poselets [4] and train highly discriminative classifiers to differentiate among arm configurations, which we call armlets. We propose a rich representation which, in addition to standardHOGfeatures, integrates the information of strong contours, skin color and contextual cues in a principled manner. Unlike existing methods, we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset, where critical visual phenomena, such as occlusion, truncation, multiple instances and clutter are the norm. Our approach outperforms Yang and Ramanan [26], the state-of-the-art technique, with an improvement from 29.0% to 37.5% PCP accuracy on the arm keypoint prediction task, on this new pose estimation dataset.</p><p>6 0.7033549 <a title="82-lsi-6" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>7 0.68553638 <a title="82-lsi-7" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>8 0.68279332 <a title="82-lsi-8" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>9 0.67192847 <a title="82-lsi-9" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>10 0.66607994 <a title="82-lsi-10" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>11 0.64309925 <a title="82-lsi-11" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>12 0.63859814 <a title="82-lsi-12" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>13 0.63792139 <a title="82-lsi-13" href="./cvpr-2013-Tensor-Based_Human_Body_Modeling.html">426 cvpr-2013-Tensor-Based Human Body Modeling</a></p>
<p>14 0.62417352 <a title="82-lsi-14" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>15 0.61966652 <a title="82-lsi-15" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>16 0.61923254 <a title="82-lsi-16" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>17 0.61012101 <a title="82-lsi-17" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>18 0.60568696 <a title="82-lsi-18" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>19 0.60535359 <a title="82-lsi-19" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>20 0.60125834 <a title="82-lsi-20" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.072), (10, 0.129), (16, 0.034), (26, 0.067), (33, 0.285), (67, 0.097), (69, 0.077), (87, 0.089), (90, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96284145 <a title="82-lda-1" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>2 0.95329648 <a title="82-lda-2" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>3 0.95297235 <a title="82-lda-3" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>Author: Wongun Choi, Yu-Wei Chao, Caroline Pantofaru, Silvio Savarese</p><p>Abstract: Visual scene understanding is a difficult problem interleaving object detection, geometric reasoning and scene classification. We present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the 3D Geometric Phrase Model which captures the semantic and geometric relationships between objects whichfrequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings from a single image, while also improving individual object detections.</p><p>4 0.95266449 <a title="82-lda-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.95159638 <a title="82-lda-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.95105988 <a title="82-lda-6" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>7 0.95021999 <a title="82-lda-7" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>8 0.94968325 <a title="82-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.94941717 <a title="82-lda-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.94815695 <a title="82-lda-10" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>11 0.94804871 <a title="82-lda-11" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>12 0.94802594 <a title="82-lda-12" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>13 0.94783342 <a title="82-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.94768864 <a title="82-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.94730842 <a title="82-lda-15" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>16 0.9473021 <a title="82-lda-16" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>17 0.94723916 <a title="82-lda-17" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>18 0.94676155 <a title="82-lda-18" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>19 0.94669628 <a title="82-lda-19" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>20 0.94662958 <a title="82-lda-20" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
