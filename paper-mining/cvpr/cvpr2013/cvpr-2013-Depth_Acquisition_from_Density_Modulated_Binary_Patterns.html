<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-114" href="#">cvpr2013-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</h1>
<br/><p>Source: <a title="cvpr-2013-114-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Depth_Acquisition_from_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>Reference: <a title="cvpr-2013-114-reference" href="../cvpr2013_reference/cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn ao se Abstract This paper proposes novel density modulated binary patterns for depth acquisition. [sent-15, score-0.657]
</p><p>2 Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. [sent-16, score-0.744]
</p><p>3 Our key idea is to use the density of light spots in the patterns to carry phase information. [sent-17, score-1.32]
</p><p>4 First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. [sent-19, score-1.194]
</p><p>5 Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. [sent-20, score-1.553]
</p><p>6 We further propose a pixelbased phase matching algorithm to reduce the error. [sent-21, score-0.619]
</p><p>7 Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. [sent-22, score-1.188]
</p><p>8 The position of light spots in every small region is unique. [sent-28, score-0.544]
</p><p>9 So depth can be reconstructed by establishing the correspondences between the reference and captured images. [sent-29, score-0.467]
</p><p>10 However, depth reconstructed in this way suffers from holes and severe noise caused by the pattern to some extent. [sent-30, score-0.434]
</p><p>11 First, the position of light spots has to be identified by a block of pixels. [sent-31, score-0.631]
</p><p>12 Phase shifting, which projects a series of phase-shifted sinusoidal patterns [23], achieves better quality. [sent-39, score-0.51]
</p><p>13 Furthermore, the depth is calculated from sinusoidal phase differences. [sent-42, score-1.172]
</p><p>14 However, as shown in Figure 1(b), the phase shifting patterns are grayscale and hard to generate using infrared lasers and diffraction gratings like Kinect. [sent-44, score-1.278]
</p><p>15 In this paper, we propose a novel approach to embed phase information into binary patterns that can still be generated with infrared lasers and diffraction gratings. [sent-45, score-1.063]
</p><p>16 Similar to image dithering, our idea is to use the density of light spots to represent phase, as shown in Figure 1(c). [sent-46, score-0.549]
</p><p>17 Figure 1(d) shows the energy images averaged in a sliding window from the patterns in Figure 1(c), which have similar properties to the phase shifting patterns in Figure 1(b). [sent-47, score-1.218]
</p><p>18 The immediate advantage is that the depth quality can be improved with extracted phase information. [sent-48, score-0.853]
</p><p>19 The goal is to carry more phase information without compromising the depth reconstructed from a single captured image as with Kinect. [sent-51, score-1.054]
</p><p>20 Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. [sent-52, score-1.553]
</p><p>21 A pixel-based phase matching algorithm is further proposed to reduce the error. [sent-53, score-0.588]
</p><p>22 Finally, the depth data reconstructed by  the position of light spots in one captured image and by the phase carried in three captured images are adaptively inte222555  Figure 1. [sent-54, score-1.648]
</p><p>23 (b) Sinusoidal fringe patterns used in phase shifting. [sent-56, score-0.805]
</p><p>24 Before this paper, Zhang proposed generating phaseshifting patterns from binary patterns by projector defocusing [3 1]. [sent-60, score-0.641]
</p><p>25 In our scheme, every captured image consists of light spots and thus depth can be reconstructed just like Kinect. [sent-62, score-0.956]
</p><p>26 Meanwhile, experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. [sent-63, score-1.188]
</p><p>27 Section 2 gives an overview of structured light and phase shifting. [sent-65, score-0.792]
</p><p>28 They require multiple patterns and the scene  cannot have motion when the patterns are projected. [sent-75, score-0.396]
</p><p>29 proposed combining a set of parallel color stripes and a perpendicular set of sinusoidal intensity stripes [7]. [sent-90, score-0.404]
</p><p>30 For pattern emitting, although some patterns can be emitted by a laser, all the above schemes use a projector. [sent-93, score-0.409]
</p><p>31 Kinect features an infrared laser that can generate and emit a constant pattern with light spots [8, 14], which makes a depth camera available as a consumer-grade device. [sent-94, score-0.993]
</p><p>32 By designing new patterns with the phase information embedded, our scheme greatly improves the depth quality. [sent-96, score-1.099]
</p><p>33 Phase Shifting Phase shifting is a special SL scheme that emits a series of phase shifted sinusoidal patterns. [sent-99, score-1.144]
</p><p>34 Increasing the number of stripes in the patterns can improve the measurement  accuracy, but high frequency results in ambiguities that requires phase unwrapping. [sent-100, score-0.809]
</p><p>35 proposed embedding a period cue into the phase shifting patterns without reducing the signal-to-noise ratio [26]. [sent-104, score-1.04]
</p><p>36 As a result, each period of the sinusoidal patterns can be identified. [sent-105, score-0.617]
</p><p>37 Global illumination and defocusing are practical factors that often introduce errors into depth measurement. [sent-108, score-0.408]
</p><p>38 showed that high-frequency sinusoidal patterns can be used to separate global illumination [18]. [sent-110, score-0.541]
</p><p>39 constrained the frequencies of sinusoidal patterns to a narrow high-frequency band, which greatly reduces global illumination and defocusing [10]. [sent-116, score-0.685]
</p><p>40 For phase shifting, when a set of patterns is emitted, the scene is assumed to be static. [sent-119, score-0.745]
</p><p>41 proposed estimating lines from the projected sinusoidal patterns and 222666 calculating motion as line translation [16]. [sent-122, score-0.557]
</p><p>42 compensated for the motion  by introducing the first-order Taylor term in phase shifting [27]. [sent-127, score-0.771]
</p><p>43 used hybrid patterns of random speckle and sinusoidal fringe [33]. [sent-131, score-0.57]
</p><p>44 Our key contribution in this paper is designing the phase shifting patterns that can be emitted by infrared laser, where the phase information is approximated by the density of light spots in a local region. [sent-132, score-2.243]
</p><p>45 In our scheme, the phase ambiguity problem is solved naturally by the random position of light spots. [sent-133, score-0.781]
</p><p>46 When the scene contains moving objects, depth in the corresponding regions can be still reconstructed from a single captured image as with Kinect. [sent-134, score-0.497]
</p><p>47 Density Modulated Binary Patterns As shown in Figure 1(c), the three proposed patterns are binary and can thus be generated using infrared lasers and diffraction gratings. [sent-136, score-0.504]
</p><p>48 In this section, we discuss how to modulate the density of light spots to represent phase. [sent-137, score-0.549]
</p><p>49 In Kinect, the light spots are randomly and uniformly distributed in P(r, c). [sent-147, score-0.484]
</p><p>50 t Terhne position nof, light spots is random in a basic unit but the same for all basic units in the same row. [sent-154, score-0.512]
</p><p>51 At the same time, since the number of light spots and their positions are different in different rows, the position of light spots in every block is still unique. [sent-156, score-1.147]
</p><p>52 In the proposed algorithm, there are two parameters that must be determined, namely, the scaling factor α and the Algorithm 1 Pattern Generation Require: The number of rows in one period T, the scaling factor α, and the initial phase θ 1: for r = 1, . [sent-157, score-0.699]
</p><p>53 , M do 5: Randomly select k(r) positions from 1to N 6: Let pixels at selected positions be light spots 7: end for 8: end for  (a)(b) (c)(d)  Figure 2. [sent-164, score-0.484]
</p><p>54 (a) TFhigeu pattern wwoit hp light spots hin N every row. [sent-169, score-0.595]
</p><p>55 (c) The pattern with light spots in every other row. [sent-171, score-0.595]
</p><p>56 In (c), every other row contains the light spots while the remaining rows are black. [sent-190, score-0.573]
</p><p>57 From the captured images in (b) and (d), it is difficult to keep the position of light spots clear if using the pattern in (a), since the captured image may contain too many light spots. [sent-191, score-0.955]
</p><p>58 222777 As mentioned above, the smaller the period T is, the more accurate the depth measurement will be. [sent-193, score-0.388]
</p><p>59 Implicit Phase Next we evaluate whether the densities of generated patterns can approximate the sinusoidal fringe well. [sent-209, score-0.597]
</p><p>60 Thus the energy E in a sliding window in the proposed patterns is a sinusoidal function mathematically. [sent-217, score-0.609]
</p><p>61 Second, it can be reconstructed by phase  Figure 3. [sent-228, score-0.657]
</p><p>62 Since the position of light spots in every small region is still unique, we can use a block matching method to get the disparity of every pixel between the reference image I¯(r, c) and the captured image I(r, c). [sent-242, score-0.907]
</p><p>63 (d) Depth reconstructed from three energy images by pixel-based phase matching. [sent-277, score-0.711]
</p><p>64 Similar to original phase shifting [23], the three energy images can be represented as E1(r, c)  =  E? [sent-280, score-0.824]
</p><p>65 (r, c) is the sinusoidal amplitude, and φ(r, c) is the phase to be solved. [sent-293, score-0.883]
</p><p>66 (6)  Since the proposed patterns contain a stair-wise error for approximating sinusoidal fringes as shown in Figure 3, we cannot calculate the depth directly from φ(r, c). [sent-296, score-0.827]
</p><p>67 For every phase φ(r, c), we then search for the most matched φ¯(r − Δr, c) within one period. [sent-299, score-0.591]
</p><p>68 Here we assume the phase varies linearly at the sub-pixel level. [sent-311, score-0.559]
</p><p>69 The phase ambiguity problem still needs to be solved. [sent-313, score-0.559]
</p><p>70 It is easy to be solved for the proposed patterns because the position of light spots is unique in every period. [sent-315, score-0.73]
</p><p>71 The blue curve “a” is the reconstruction from original phase shifting. [sent-324, score-0.671]
</p><p>72 The light-blue curve “d” is the reconstruction by the proposed pixel-based phase matching algorithm. [sent-330, score-0.677]
</p><p>73 Depth Integration Although using the embedded phase to reconstruct depth produces better quality, this method requires three captured images and is not good at handling moving objects in the scene. [sent-335, score-0.984]
</p><p>74 However, the depth reconstructed by block matching only needs one captured image, which is more suitable for moving objects. [sent-336, score-0.645]
</p><p>75 The phase calculation and pixelbased phase matching are carried out on an Intel Xeon E5440 CPU with 2. [sent-343, score-1.23]
</p><p>76 The second is original phase shifting, which uses three grayscale sinusoidal patterns emitted by a DLP projector and reconstructs depth directly from phase. [sent-350, score-1.593]
</p><p>77 The third is the proposed scheme, and the binary patterns are also emitted by the DLP projector for simulation. [sent-351, score-0.438]
</p><p>78 Figure 6(b) is the depth reconstructed from original phase shifting. [sent-357, score-0.937]
</p><p>79 Figure 6(c) is the depth reconstructed by the proposed scheme but using block matching only. [sent-359, score-0.576]
</p><p>80 Figure [62(2d]) wish tihlee depth mreceroans istru ocntleyd by ×the4 proposed scheme using the embedded phase and pixel-based phase matching. [sent-362, score-1.474]
</p><p>81 It is much better than that from Kinect and also close to that from original phase shifting. [sent-363, score-0.582]
</p><p>82 (d) The proposed scheme using pixel-based phase matching. [sent-369, score-0.632]
</p><p>83 that carrying phase information in the binary patterns is a feasible way to improve the depth quality over Kinect. [sent-374, score-1.085]
</p><p>84 We further compare the point cloud from the proposed scheme and original phase shifting, as shown in Figure 7. [sent-375, score-0.655]
</p><p>85 Continuously improving the quality deserves future efforts, yet there should be an inherent compromise as binary patterns cannot represent phase as perfectly as grayscale patterns. [sent-379, score-0.866]
</p><p>86 Although our results look close to the results from original phase shifting using grayscale patterns, there are some noticeable differences. [sent-397, score-0.808]
</p><p>87 Since these scenes are static, they favor original phase shifting. [sent-398, score-0.582]
</p><p>88 (b) is the result obtained using pixel-based phase matching. [sent-406, score-0.559]
</p><p>89 The depth of the moving object is really poor because there is motion when the three patterns are emitted. [sent-407, score-0.524]
</p><p>90 Original phase shifting will suffer from the same problem. [sent-408, score-0.747]
</p><p>91 In our scheme, when the objects in the scene start to move, the depth reconstruction will degrade to block matching. [sent-417, score-0.428]
</p><p>92 For example, can we use the depth from block matching to drive the previous obtained high quality depth? [sent-419, score-0.442]
</p><p>93 Or can we use the depth from block matching to compensate for the motion? [sent-420, score-0.405]
</p><p>94 Conclusions The proposed density modulated binary patterns carve out a feasible way to improve the depth quality over Kinect. [sent-423, score-0.694]
</p><p>95 By embedding phase into the binary patterns, it provides more information for depth acquisition. [sent-424, score-0.862]
</p><p>96 We propose the pattern generation algorithm and the pixel-based phase matching algorithm for reconstruction. [sent-425, score-0.716]
</p><p>97 Experimental results show that our scheme consistently outperforms Kinect for static scenes and original phase shifting for moving objects. [sent-426, score-0.933]
</p><p>98 Accuracy and resolution of kinect depth data for indoor mapping applications. [sent-528, score-0.429]
</p><p>99 Period coded phase shifting strategy for real-time 3-d structured light illumination. [sent-610, score-1.037]
</p><p>100 Fast and accurate 3d scanning using coded phase shifting and high speed pattern projection. [sent-622, score-0.911]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phase', 0.559), ('sinusoidal', 0.324), ('spots', 0.29), ('depth', 0.257), ('light', 0.194), ('shifting', 0.188), ('patterns', 0.186), ('kinect', 0.146), ('defocusing', 0.12), ('block', 0.119), ('period', 0.107), ('modulated', 0.103), ('emitted', 0.103), ('projector', 0.103), ('infrared', 0.099), ('reconstructed', 0.098), ('diffraction', 0.093), ('captured', 0.085), ('koninckx', 0.08), ('lasers', 0.08), ('pattern', 0.079), ('scheme', 0.073), ('disparity', 0.071), ('density', 0.065), ('ncc', 0.062), ('fringe', 0.06), ('fringes', 0.06), ('moving', 0.057), ('coded', 0.057), ('energy', 0.054), ('carried', 0.052), ('reconstruction', 0.052), ('sl', 0.051), ('generation', 0.049), ('weise', 0.049), ('binary', 0.046), ('sliding', 0.045), ('laser', 0.045), ('optics', 0.043), ('schemes', 0.041), ('emitting', 0.041), ('acqusition', 0.04), ('albitar', 0.04), ('decodable', 0.04), ('dlp', 0.04), ('fong', 0.04), ('graebling', 0.04), ('grating', 0.04), ('maurice', 0.04), ('photoresist', 0.04), ('wissmann', 0.04), ('stripes', 0.04), ('structured', 0.039), ('grayscale', 0.038), ('sin', 0.037), ('curve', 0.037), ('quality', 0.037), ('xbox', 0.035), ('gratings', 0.035), ('kawasaki', 0.035), ('northeastern', 0.035), ('static', 0.033), ('looks', 0.033), ('rows', 0.033), ('multiplexed', 0.033), ('shpunt', 0.033), ('workshops', 0.032), ('every', 0.032), ('calculated', 0.032), ('salvi', 0.031), ('pixelbased', 0.031), ('illumination', 0.031), ('emit', 0.029), ('compromising', 0.029), ('matching', 0.029), ('taguchi', 0.028), ('patent', 0.028), ('position', 0.028), ('systematic', 0.028), ('scanning', 0.028), ('cos', 0.028), ('reference', 0.027), ('decided', 0.027), ('integer', 0.027), ('round', 0.027), ('densities', 0.027), ('microsoft', 0.026), ('indoor', 0.026), ('carry', 0.026), ('rounding', 0.026), ('opaque', 0.026), ('zhe', 0.026), ('embedded', 0.026), ('chen', 0.025), ('measurement', 0.024), ('greatly', 0.024), ('motion', 0.024), ('row', 0.024), ('calculating', 0.023), ('original', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="114-tfidf-1" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>2 0.26688722 <a title="114-tfidf-2" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>3 0.15932348 <a title="114-tfidf-3" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>4 0.15824772 <a title="114-tfidf-4" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>Author: Ramakrishna Kakarala, Prabhu Kaliamoorthi, Vittal Premachandran</p><p>Abstract: We show that bilateral symmetry plane estimation for three-dimensional (3-D) shapes may be carried out accurately, and efficiently, in the spherical harmonic domain. Our methods are valuable for applications where spherical harmonic expansion is already employed, such as 3-D shape registration, morphometry, and retrieval. We show that the presence of bilateral symmetry in the 3-D shape is equivalent to a linear phase structure in the corresponding spherical harmonic coefficients, and provide algorithms for estimating the orientation of the symmetry plane. The benefit of using spherical harmonic phase is that symmetry estimation reduces to matching a compact set of descriptors, without the need to solve a correspondence problem. Our methods work on point clouds as well as large-scale mesh models of 3-D shapes.</p><p>5 0.14640924 <a title="114-tfidf-5" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>6 0.13873656 <a title="114-tfidf-6" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>7 0.13628416 <a title="114-tfidf-7" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>8 0.13355289 <a title="114-tfidf-8" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>9 0.13062534 <a title="114-tfidf-9" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>10 0.12506835 <a title="114-tfidf-10" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>11 0.11657838 <a title="114-tfidf-11" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>12 0.11314696 <a title="114-tfidf-12" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>13 0.1102641 <a title="114-tfidf-13" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>14 0.10711596 <a title="114-tfidf-14" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>15 0.10591415 <a title="114-tfidf-15" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>16 0.10555644 <a title="114-tfidf-16" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>17 0.10026743 <a title="114-tfidf-17" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>18 0.098738231 <a title="114-tfidf-18" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>19 0.095700368 <a title="114-tfidf-19" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>20 0.094662867 <a title="114-tfidf-20" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.155), (1, 0.198), (2, 0.022), (3, 0.059), (4, -0.043), (5, -0.066), (6, -0.068), (7, 0.097), (8, 0.028), (9, 0.004), (10, -0.048), (11, -0.04), (12, 0.072), (13, 0.09), (14, -0.038), (15, 0.018), (16, -0.149), (17, 0.026), (18, -0.017), (19, -0.017), (20, -0.009), (21, 0.023), (22, -0.017), (23, -0.004), (24, 0.056), (25, 0.045), (26, -0.002), (27, 0.012), (28, -0.031), (29, -0.019), (30, -0.001), (31, 0.049), (32, 0.085), (33, -0.082), (34, 0.007), (35, -0.059), (36, -0.0), (37, 0.004), (38, -0.037), (39, -0.018), (40, -0.06), (41, 0.004), (42, 0.003), (43, 0.057), (44, -0.085), (45, -0.03), (46, -0.092), (47, 0.005), (48, -0.026), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97558784 <a title="114-lsi-1" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>2 0.86366355 <a title="114-lsi-2" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>3 0.8120814 <a title="114-lsi-3" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>Author: Ming-Yu Liu, Oncel Tuzel, Yuichi Taguchi</p><p>Abstract: We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. Specifically, it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. Though this is closely related to the all-pairshortest-path problem which has O(n2 log n) complexity, we develop a novel approximation algorithm whose complexity grows linearly with the image size and achieve realtime performance. We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. In addition, we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps, an important sensor fusion application.</p><p>4 0.7750777 <a title="114-lsi-4" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>5 0.7724511 <a title="114-lsi-5" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: In this paper, we propose a convex optimization framework for simultaneous estimation of super-resolved depth map and images from a single moving camera. The pixel measurement error in 3D reconstruction is directly related to the resolution of the images at hand. In turn, even a small measurement error can cause significant errors in reconstructing 3D scene structure or camera pose. Therefore, enhancing image resolution can be an effective solution for securing the accuracy as well as the resolution of 3D reconstruction. In the proposed method, depth map estimation and image super-resolution are formulated in a single energy minimization framework with a convex function and solved efficiently by a first-order primal-dual algorithm. Explicit inter-frame pixel correspondences are not required for our super-resolution procedure, thus we can avoid a huge computation time and obtain improved depth map in the accuracy and resolution as well as highresolution images with reasonable time. The superiority of our algorithm is demonstrated by presenting the improved depth map accuracy, image super-resolution results, and camera pose estimation.</p><p>6 0.75485152 <a title="114-lsi-6" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>7 0.73029703 <a title="114-lsi-7" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>8 0.72039944 <a title="114-lsi-8" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>9 0.72007817 <a title="114-lsi-9" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>10 0.71731317 <a title="114-lsi-10" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>11 0.71367878 <a title="114-lsi-11" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>12 0.66160321 <a title="114-lsi-12" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>13 0.65380722 <a title="114-lsi-13" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>14 0.64271778 <a title="114-lsi-14" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>15 0.63433814 <a title="114-lsi-15" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>16 0.61860925 <a title="114-lsi-16" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>17 0.60628706 <a title="114-lsi-17" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>18 0.59469604 <a title="114-lsi-18" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>19 0.55828768 <a title="114-lsi-19" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>20 0.54592085 <a title="114-lsi-20" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.092), (16, 0.06), (26, 0.028), (33, 0.197), (67, 0.029), (69, 0.448), (87, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84081727 <a title="114-lda-1" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>Author: Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: 3D volumetric reasoning is important for truly understanding a scene. Humans are able to both segment each object in an image, and perceive a rich 3D interpretation of the scene, e.g., the space an object occupies, which objects support other objects, and which objects would, if moved, cause other objects to fall. We propose a new approach for parsing RGB-D images using 3D block units for volumetric reasoning. The algorithm fits image segments with 3D blocks, and iteratively evaluates the scene based on block interaction properties. We produce a 3D representation of the scene based on jointly optimizing over segmentations, block fitting, supporting relations, and object stability. Our algorithm incorporates the intuition that a good 3D representation of the scene is the one that fits the data well, and is a stable, self-supporting (i.e., one that does not topple) arrangement of objects. We experiment on several datasets including controlled and real indoor scenarios. Results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation.</p><p>2 0.81041628 <a title="114-lda-2" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>same-paper 3 0.80868912 <a title="114-lda-3" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>4 0.79927772 <a title="114-lda-4" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>5 0.7812953 <a title="114-lda-5" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>6 0.76463896 <a title="114-lda-6" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>7 0.76013184 <a title="114-lda-7" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>8 0.7156319 <a title="114-lda-8" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>9 0.67060488 <a title="114-lda-9" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>10 0.63125926 <a title="114-lda-10" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>11 0.60332507 <a title="114-lda-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.60227954 <a title="114-lda-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.59765995 <a title="114-lda-13" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>14 0.59152341 <a title="114-lda-14" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>15 0.58775705 <a title="114-lda-15" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>16 0.58709246 <a title="114-lda-16" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>17 0.58696407 <a title="114-lda-17" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>18 0.58274651 <a title="114-lda-18" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>19 0.57760388 <a title="114-lda-19" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>20 0.57476979 <a title="114-lda-20" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
