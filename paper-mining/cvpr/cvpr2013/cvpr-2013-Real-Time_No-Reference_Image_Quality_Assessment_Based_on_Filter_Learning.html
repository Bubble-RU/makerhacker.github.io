<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-346" href="#">cvpr2013-346</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</h1>
<br/><p>Source: <a title="cvpr-2013-346-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ye_Real-Time_No-Reference_Image_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>Reference: <a title="cvpr-2013-346-reference" href="../cvpr2013_reference/cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. [sent-7, score-0.252]
</p><p>2 The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. [sent-8, score-0.22]
</p><p>3 We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results. [sent-11, score-0.301]
</p><p>4 In such an environment, it is critical to have good image quality assessment methods to help maintain, control and enhance the quality of the digital images. [sent-14, score-0.426]
</p><p>5 The goal of objective image quality assessment (IQA) is to build a computational model that can accurately predict the quality of digital images with respect to human perception or other measures of interest. [sent-15, score-0.48]
</p><p>6 This paper addresses the most challenging category of objective IQA methods – NR-IQA, which evaluates the quality of digital images without access to reference images [3, 11, 12, 13, 16, 19]. [sent-17, score-0.193]
</p><p>7 By extracting image quality features directly in spatial domain, recent algorithms CORNIA [22] and BRISQUE[12] have greatly accelerated this process while maintaining high prediction accuracy. [sent-26, score-0.21]
</p><p>8 Previous works on NR-IQA have focused primarily on natural scene image and image quality is defined with respect to human perception. [sent-28, score-0.168]
</p><p>9 Very limited work has been done for NR-IQA for other types ofimages, such as cameracaptured or scanned document images. [sent-29, score-0.271]
</p><p>10 Document IQA has been found to be very useful in many document image pro-  cessing applications. [sent-30, score-0.271]
</p><p>11 Document IQA may help to automatically filter pages with low predicted OCR accuracy or guide the selection of document image enhancement methods. [sent-32, score-0.424]
</p><p>12 Conventional image quality measures developed for natural scene images may not work well for document images since document images have very different characteristics than natural scene images. [sent-33, score-0.764]
</p><p>13 For example, most document images are gray-scale or binary consisting of black text and white background. [sent-34, score-0.302]
</p><p>14 normalized intensity values, wavelet co-  efficients, etc), can be modeled by some parametric distributions; second, the shape of these distributions are very different for non-distorted and distorted images. [sent-44, score-0.17]
</p><p>15 The goal is to learn local features whose distributions possess discriminative shapes for distorted and non-distorted images. [sent-53, score-0.162]
</p><p>16 Unsupervised feature learning has been explored in CORNIA [22], where the local descriptors are encoded using codeword that are learned in an unsupervised way. [sent-54, score-0.297]
</p><p>17 Our method can be considered as a supervised extension of CORNIA, where instead of using a large redundant set of filters, the proposed method learns a compact set of filters in a supervised way. [sent-57, score-0.579]
</p><p>18 The proposed supervised filter learning method is closely related to supervised dictionary learning for image classification. [sent-59, score-0.499]
</p><p>19 Unlike conventional supervised dictionary learning, which requires that the linear combination of the learned atoms in dictionary should be able to well represent image patches, we do not have this constraint in our supervised filter learning process. [sent-62, score-0.554]
</p><p>20 In fact, it will be shown later that the functionality of filter for NR-IQA and codeword for image classification are very different. [sent-63, score-0.207]
</p><p>21 Supervised filter learning has also been explored by Jain and Karu in [6] for texture classification, where feature extraction and classification tasks are performed by a neural network. [sent-64, score-0.227]
</p><p>22 The learned filters are weight vectors in the first layer of the network. [sent-65, score-0.313]
</p><p>23 Our work is along the same lines of learning a compact set of filters using a back-propagation approach but differs in final stage where we perform support vector regression (SVR) using learned filters for predicting  image quality. [sent-66, score-0.759]
</p><p>24 Our approach A typical NR-IQA system may consist of the following three components (1) a local feature extractor; (2) a global feature extractor, which summarizes the distribution oflocal features and (3) a regression model. [sent-69, score-0.137]
</p><p>25 Previous approaches usually treat local feature extraction and regression model training independently. [sent-70, score-0.19]
</p><p>26 We propose a supervised method based on back-projection, which links these two steps by jointly optimizing the prediction model and the local feature extractor. [sent-71, score-0.258]
</p><p>27 The learned compact set of filters when applied to local image patches yields more discriminative features. [sent-72, score-0.422]
</p><p>28 Feature extraction In this section, we discuss how a set of linear filters are used to obtain global features. [sent-77, score-0.325]
</p><p>29 , bK] ∈ Rd×K, where the column vector bi (||bi||l22 = 1) deno]te ∈s the i-th filter and K is the number of f(i||ltber|s|. [sent-87, score-0.184]
</p><p>30 Specifically, each local descriptor is encoded by its responses to the set of linear filters (i. [sent-91, score-0.364]
</p><p>31 x xN N⎠⎟ ⎞(1) Examples of distributions of filter responses from images with different types and levels of distortions are shown in Fig. [sent-100, score-0.273]
</p><p>32 We can see from this figure that with properly learned filters, statistics extracted from these distributions can be good indicators of image quality. [sent-102, score-0.137]
</p><p>33 The filter here is similar to the codeword in the image classification literature in that both are used for local feature encoding. [sent-103, score-0.263]
</p><p>34 For example, in the object recognition problem, whereas a codeword resembles a part of the object, and the maximal response to each codeword indicates its presence or absence in the image. [sent-105, score-0.236]
</p><p>35 In our problem, both maximal and minimal responses are informative and important for prediction task. [sent-106, score-0.237]
</p><p>36 More generally, we are interested in characterizing the entire distribution of the filter responses. [sent-107, score-0.123]
</p><p>37 Specifically, we  (a) Gaussian Blurring  (b) Fast Fading Figure 2: Examples of filter respones for different types and levels of distortions. [sent-111, score-0.123]
</p><p>38 (High DMOS indicates low quality) use the maximal and minimal value of filter responses for  describing the effect of filters on the image. [sent-112, score-0.567]
</p><p>39 Other statistics which summarize the distribution of filter responses can also be explored, such as skewness and kurtosis. [sent-114, score-0.213]
</p><p>40 Although the minimal and maximal value of filter responses may not be accurate in characterizing the shape of distribution, as is shown in Fig. [sent-116, score-0.284]
</p><p>41 Supervised filter learning In the previous section, we have introduced the use of a set of filters for obtaining global descriptors of an image. [sent-120, score-0.489]
</p><p>42 In 999998888899777  this section, we discuss how these filters are learned so that the corresponding global descriptors are good for NR-IQA  tasks. [sent-121, score-0.363]
</p><p>43 (3)  In the above formulation, the prediction model is trained with the set of filters fixed. [sent-141, score-0.359]
</p><p>44 Our supervised filter learning method jointly optimizes the prediction model and the set of filters. [sent-142, score-0.358]
</p><p>45 The initial set of filters are obtained by performing k-means clustering on a set of local features. [sent-165, score-0.31]
</p><p>46 Superscript k ·is x the index of the training image and· xl ∈ Xk means xl is a local feature vector from image Xk. [sent-178, score-0.196]
</p><p>47 We describe the SGD process for optimizing a given filter as follows: 1. [sent-185, score-0.149]
</p><p>48 Compute the gradient gk = ∇bCk (b) |b=bk , where Ck = L(yk , f(Zk , w)) + λ2aveco=rr( ∇bk) an(db )bk| is the value of the filter at the k-th iteration. [sent-188, score-0.188]
</p><p>49 Project gk on the tangent plane of the unit sphere at bk, hk = gk (gk · bk)bk and normalize it, nk = hk/ |hk |. [sent-190, score-0.13]
</p><p>50 Go back to step 2 and repeat the process until the −  +  maximal number of iterations is reached. [sent-196, score-0.159]
</p><p>51 We optimize a set of filters B by iterating the above process. [sent-197, score-0.283]
</p><p>52 At each step, we simultaneously update all filters in B and update the objective function with the new set of filters. [sent-198, score-0.283]
</p><p>53 In order to avoid over-fitting, we adopt the early stopping criteria, proposed in [15]. [sent-200, score-0.12]
</p><p>54 Specifically, we divide the training data into a training set and a validation set. [sent-201, score-0.122]
</p><p>55 The training process is terminated as soon as the error on validation set satisfies our early stopping rule. [sent-203, score-0.228]
</p><p>56 The set of filters which gives the best performance on the validation set is chosen as the output of the optimization process. [sent-204, score-0.325]
</p><p>57 The generalization loss at the k-th iteration is give  ×  by GL(k) = 100((1 − corr(k))/(1 − corropt (k)) − 1), bwyhe GreL c(ok)rr( =k) i0s0 t(h(e1 L −C Cco on kv)a)li/d(a1ti o−n soertr at t(hke) k)- −th 1i)t-, eration and corropt (k) = maxk? [sent-215, score-0.134]
</p><p>58 Two stopping rules are used: (Rule 1) GL(k) > α for some α > 0; (Rule 2) corr decreases in consecutive literations. [sent-218, score-0.149]
</p><p>59 Protocol  We refer to the supervised filter learning based method with k filters as SFk and the corresponding unsupervised method as CBk. [sent-223, score-0.609]
</p><p>60 For example CB100 refers to the method using 100 filters obtained from kmeans clustering. [sent-224, score-0.283]
</p><p>61 (5) The minimal number of iterations of the optimization process is  set to 50 and the maximal number of iterations is set to 250. [sent-234, score-0.263]
</p><p>62 After 50 iterations, if one of the two early stopping rules is true or the maximal number of iterations is reached, we terminate the filter training process. [sent-235, score-0.449]
</p><p>63 (6) Parameters in the early stopping rules are α = 10, l = 10. [sent-236, score-0.153]
</p><p>64 (1) LIVE IQA dataset: LIVE IQA dataset [17, 18] consists of images with five types of distortions - JPEG2k, JPEG, white Gaussian noise (WN), Gaussian blurring (BLUR) and fast fading channel distortion (FF) derived from 29 non-distorted images. [sent-246, score-0.205]
</p><p>65 (2) TID2008 dataset:  25 reference images and 1700 distorted images derived from them with 17 different distortions at 4 levels are included in TID2008 [14]. [sent-249, score-0.192]
</p><p>66 In this paper, we consider four of the 17 distortions which are most likely to occur in image processing systems, including Additive Gaussian noise (WN), JPEG compression (JPEG), JPEG2000 compression (JPEG2K) and Gaussian blur (BLUR). [sent-250, score-0.136]
</p><p>67 The optimization process on the entire LIVE from the first 5 1iterations averaged over all the 100-fold experiments 1Only distorted images  are  used in testing. [sent-266, score-0.124]
</p><p>68 )  Figure 3: Optimization process of the first 5 1 iterations on training set and validation set (average LCC from 100 fold experiments on LIVE). [sent-283, score-0.173]
</p><p>69 We also compare the supervised filter learning method with unsupervised filter learning CB100-BS7. [sent-287, score-0.482]
</p><p>70 When only 100 filters are used, the prediction performance can be significantly boosted by adopting the supervised approach. [sent-288, score-0.485]
</p><p>71 Supervised filter training on LIVE, test on TID2008 To show that the proposed method does not depend on any particular dataset, we train filters on the LIVE dataset and test it on the TID2008 dataset. [sent-289, score-0.48]
</p><p>72 Using the fixed set of filters trained on LIVE, in each iteration of the experiment, we split the TID2008 dataset into non-overlapping training and testing part and perform training and testing on each  CSOP BF R12NIM0 R A0 J. [sent-290, score-0.397]
</p><p>73 For a specific distortion category, the set of filters are trained on corresponding subset in LIVE. [sent-309, score-0.345]
</p><p>74 Comparing results obtained using CB100-BS5/CB200-BS5 and SF100-BS5/SF200-BS5, we can see great performance improvement by using a supervised feature learning strategy. [sent-310, score-0.188]
</p><p>75 We test the speed ofour method with 100 filters and two other recent fast NR-IQA measures. [sent-313, score-0.283]
</p><p>76 We further decompose the feature extraction process into two steps (1) extracting non-overlapping image patches and (2) extracting local and global feature based on Eq. [sent-318, score-0.191]
</p><p>77 Experiments on Document Image Instead of predicting human perceived image quality, for document IQA (Doc-IQA), we are interested in predicting the OCR accuracy with respect to a specific OCR software, which has been found useful in many document image applications. [sent-327, score-0.61]
</p><p>78 1  Dataset  To test the proposed method on document images, the following two document datasets are used. [sent-330, score-0.542]
</p><p>79 (1) SOC dataset: Sharpness-OCR-Correlation(SOC) dataset contains camera-captured document with blur. [sent-331, score-0.305]
</p><p>80 (2) Newspaper dataset: This dataset contains a total of 521 text zone images from a collection of gray scale newspaper images with machine-printed English and Greek text [2]. [sent-338, score-0.291]
</p><p>81 Blur is a relatively easy distortion and as is shown Table 2 and  Table 3, all IQA measures performs fairly well on the blur category in LIVE. [sent-347, score-0.22]
</p><p>82 However, when tested on document image, we observe a significant drop in performance for all IQA measures under comparison. [sent-348, score-0.325]
</p><p>83 Consider two document images with the same level of blur distortion, they are expected to have similar OCR accuracy. [sent-350, score-0.348]
</p><p>84 This explains why we obtained LCC around 90%, 2When the number of filters is very small, the initial set of filters are fairly unique. [sent-352, score-0.593]
</p><p>85 Figure 4: Examples of images from the newspaper dataset. [sent-354, score-0.156]
</p><p>86 while SROCC, which measures the monotonicity of the prediction model is much lower than the corresponding LCC. [sent-371, score-0.13]
</p><p>87 Only slight improvement has been observed using supervised filter based approach over unsupervised approach on  the SOC dataset. [sent-372, score-0.293]
</p><p>88 The local patch extraction on the newspaper dataset was performed only on text regions. [sent-379, score-0.29]
</p><p>89 When the initial set of filters obtained in an unsupervised manner has been able to well capture the distortion properties, supervised learning may not be very helpful. [sent-385, score-0.548]
</p><p>90 Using our approach, one can decide whether supervised extension is needed for different filter based on prediction tasks. [sent-386, score-0.325]
</p><p>91 The development of a  good Doc-IQA dataset is critical for developing any NRIQA measures in document image domain. [sent-388, score-0.359]
</p><p>92 Conclusions We have presented a supervised filter learning based algorithm for general-purpose NR-IQA. [sent-392, score-0.282]
</p><p>93 From our experiments, we conclude that a compact set of learned filters can achieve the same accuracy as by using a large number of unsupervised filters while reducing the the computation time significantly. [sent-394, score-0.684]
</p><p>94 No-reference image quality assessment based on DCT domain statistics. [sent-411, score-0.268]
</p><p>95 Data sets for OCR and document image understanding research. [sent-429, score-0.271]
</p><p>96 Building a test collection for complex document information processing. [sent-465, score-0.271]
</p><p>97 Blind image quality assessment: From natural scene statistics to perceptual quality. [sent-493, score-0.208]
</p><p>98 Tid2008 - a database for evaluation offull-reference visual quality assessment metrics. [sent-503, score-0.268]
</p><p>99 Blind image quality assessment: A natural scene statistics approach in the DCT domain. [sent-514, score-0.174]
</p><p>100 A statistical evaluation of recent full reference image quality assessment algorithms. [sent-524, score-0.303]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('iqa', 0.379), ('ocr', 0.285), ('filters', 0.283), ('document', 0.271), ('lcc', 0.251), ('cornia', 0.201), ('srocc', 0.201), ('assessment', 0.16), ('newspaper', 0.156), ('live', 0.155), ('soc', 0.144), ('supervised', 0.126), ('brisque', 0.126), ('bk', 0.124), ('filter', 0.123), ('svr', 0.113), ('quality', 0.108), ('dmos', 0.1), ('distorted', 0.098), ('sgd', 0.093), ('codeword', 0.084), ('stopping', 0.084), ('extractor', 0.083), ('zk', 0.079), ('blur', 0.077), ('prediction', 0.076), ('flk', 0.075), ('median', 0.069), ('maximal', 0.068), ('xk', 0.066), ('iterations', 0.065), ('gk', 0.065), ('distortion', 0.062), ('bi', 0.061), ('distortions', 0.059), ('dictionary', 0.058), ('responses', 0.054), ('measures', 0.054), ('regression', 0.052), ('corropt', 0.05), ('csop', 0.05), ('fading', 0.05), ('ggd', 0.05), ('communication', 0.05), ('xl', 0.05), ('digital', 0.05), ('descriptors', 0.05), ('jpeg', 0.047), ('italicized', 0.045), ('mos', 0.045), ('compact', 0.044), ('unsupervised', 0.044), ('dct', 0.043), ('validation', 0.042), ('extraction', 0.042), ('moorthy', 0.041), ('training', 0.04), ('psnr', 0.04), ('minimal', 0.039), ('bovik', 0.039), ('zone', 0.039), ('rule', 0.038), ('patches', 0.038), ('distributions', 0.037), ('opinion', 0.037), ('early', 0.036), ('statistics', 0.036), ('wavelet', 0.035), ('reference', 0.035), ('blind', 0.035), ('ssim', 0.034), ('indicators', 0.034), ('loss', 0.034), ('perceptual', 0.034), ('dataset', 0.034), ('predicting', 0.034), ('learning', 0.033), ('ye', 0.033), ('rules', 0.033), ('corr', 0.032), ('fr', 0.032), ('bf', 0.031), ('text', 0.031), ('rr', 0.031), ('kn', 0.03), ('learned', 0.03), ('natural', 0.03), ('correlation', 0.03), ('gl', 0.03), ('primarily', 0.03), ('domains', 0.03), ('pages', 0.03), ('feature', 0.029), ('sheikh', 0.029), ('local', 0.027), ('fairly', 0.027), ('rk', 0.027), ('iv', 0.027), ('process', 0.026), ('kang', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="346-tfidf-1" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>2 0.21111715 <a title="346-tfidf-2" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>Author: Roberto Rigamonti, Amos Sironi, Vincent Lepetit, Pascal Fua</p><p>Abstract: Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-theart methods on the linear structure extraction task, in terms ofboth accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.</p><p>3 0.15556975 <a title="346-tfidf-3" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>Author: Wufeng Xue, Lei Zhang, Xuanqin Mou</p><p>Abstract: General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in thefields of image processing, vision and machine learning. Stateof-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map.</p><p>4 0.13068283 <a title="346-tfidf-4" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>Author: Thomas Dean, Mark A. Ruzon, Mark Segal, Jonathon Shlens, Sudheendra Vijayanarasimhan, Jay Yagnik</p><p>Abstract: Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object’s appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times— four orders of magnitude— when compared withperforming the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.</p><p>5 0.11938003 <a title="346-tfidf-5" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>Author: Yang Yang, Guang Shu, Mubarak Shah</p><p>Abstract: We propose a novel approach to boost the performance of generic object detectors on videos by learning videospecific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-bytracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto encoders. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability; second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.</p><p>6 0.11589535 <a title="346-tfidf-6" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>7 0.10685983 <a title="346-tfidf-7" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>8 0.1044974 <a title="346-tfidf-8" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>9 0.094620183 <a title="346-tfidf-9" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>10 0.086961284 <a title="346-tfidf-10" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>11 0.082651146 <a title="346-tfidf-11" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>12 0.082607836 <a title="346-tfidf-12" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>13 0.082379036 <a title="346-tfidf-13" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>14 0.078288443 <a title="346-tfidf-14" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>15 0.077657633 <a title="346-tfidf-15" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>16 0.075611971 <a title="346-tfidf-16" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>17 0.074866027 <a title="346-tfidf-17" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>18 0.074700996 <a title="346-tfidf-18" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>19 0.073214762 <a title="346-tfidf-19" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>20 0.072111033 <a title="346-tfidf-20" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, -0.021), (2, -0.068), (3, 0.099), (4, -0.01), (5, 0.086), (6, 0.021), (7, 0.003), (8, -0.033), (9, -0.023), (10, -0.044), (11, -0.031), (12, 0.023), (13, -0.04), (14, 0.009), (15, 0.006), (16, 0.033), (17, 0.005), (18, 0.101), (19, 0.019), (20, -0.001), (21, 0.012), (22, -0.026), (23, -0.059), (24, -0.027), (25, 0.064), (26, -0.028), (27, -0.008), (28, -0.013), (29, -0.04), (30, -0.087), (31, 0.017), (32, -0.065), (33, -0.032), (34, -0.041), (35, 0.112), (36, -0.034), (37, 0.017), (38, -0.036), (39, 0.083), (40, -0.032), (41, -0.031), (42, -0.014), (43, -0.02), (44, -0.122), (45, -0.05), (46, 0.036), (47, -0.075), (48, 0.043), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92027164 <a title="346-lsi-1" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>2 0.89016587 <a title="346-lsi-2" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>Author: Roberto Rigamonti, Amos Sironi, Vincent Lepetit, Pascal Fua</p><p>Abstract: Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-theart methods on the linear structure extraction task, in terms ofboth accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.</p><p>3 0.76143777 <a title="346-lsi-3" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>Author: Hilton Bristow, Anders Eriksson, Simon Lucey</p><p>Abstract: Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence.</p><p>4 0.69887829 <a title="346-lsi-4" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>Author: Chao Liu, Geifei Yang, Jinwei Gu</p><p>Abstract: We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials, such as wood, fabric, and granite. At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.</p><p>5 0.68582344 <a title="346-lsi-5" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>Author: Wufeng Xue, Lei Zhang, Xuanqin Mou</p><p>Abstract: General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in thefields of image processing, vision and machine learning. Stateof-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map.</p><p>6 0.66840124 <a title="346-lsi-6" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>7 0.6113807 <a title="346-lsi-7" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>8 0.60748667 <a title="346-lsi-8" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>9 0.59993315 <a title="346-lsi-9" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>10 0.59740692 <a title="346-lsi-10" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>11 0.59563589 <a title="346-lsi-11" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>12 0.59326047 <a title="346-lsi-12" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>13 0.58743125 <a title="346-lsi-13" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>14 0.58729851 <a title="346-lsi-14" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>15 0.57063091 <a title="346-lsi-15" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>16 0.55247915 <a title="346-lsi-16" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>17 0.54415685 <a title="346-lsi-17" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>18 0.53960359 <a title="346-lsi-18" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>19 0.53793913 <a title="346-lsi-19" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>20 0.53482091 <a title="346-lsi-20" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.071), (26, 0.02), (33, 0.728), (67, 0.045), (69, 0.022), (87, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99975193 <a title="346-lda-1" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>Author: Petr Gronát, Guillaume Obozinski, Josef Sivic, Tomáš Pajdla</p><p>Abstract: The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as onlyfewpositive training examples are availablefor each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. 2Center for Machine Perception, Faculty of Electrical Engineering 3WILLOW project, Laboratoire d’Informatique de l’E´cole Normale Sup e´rieure, ENS/INRIA/CNRS UMR 8548. 4Universit Paris-Est, LIGM (UMR CNRS 8049), Center for Visual Computing, Ecole des Ponts - ParisTech, 77455 Marne-la-Valle, France</p><p>same-paper 2 0.99970967 <a title="346-lda-2" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>3 0.99963123 <a title="346-lda-3" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>Author: Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi</p><p>Abstract: Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. In this paper, we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i.e., analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling.</p><p>4 0.99957675 <a title="346-lda-4" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>Author: Neill D.F. Campbell, Kartic Subr, Jan Kautz</p><p>Abstract: Conditional Random Fields (CRFs) are used for diverse tasks, ranging from image denoising to object recognition. For images, they are commonly defined as a graph with nodes corresponding to individual pixels and pairwise links that connect nodes to their immediate neighbors. Recent work has shown that fully-connected CRFs, where each node is connected to every other node, can be solved efficiently under the restriction that the pairwise term is a Gaussian kernel over a Euclidean feature space. In this paper, we generalize the pairwise terms to a non-linear dissimilarity measure that is not required to be a distance metric. To this end, we propose a density estimation technique to derive conditional pairwise potentials in a nonparametric manner. We then use an efficient embedding technique to estimate an approximate Euclidean feature space for these potentials, in which the pairwise term can still be expressed as a Gaussian kernel. We demonstrate that the use of non-parametric models for the pairwise interactions, conditioned on the input data, greatly increases expressive power whilst maintaining efficient inference.</p><p>5 0.99954981 <a title="346-lda-5" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>6 0.99950814 <a title="346-lda-6" href="./cvpr-2013-Dynamic_Scene_Classification%3A_Learning_Motion_Descriptors_with_Slow_Features_Analysis.html">137 cvpr-2013-Dynamic Scene Classification: Learning Motion Descriptors with Slow Features Analysis</a></p>
<p>7 0.99928266 <a title="346-lda-7" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>8 0.99917883 <a title="346-lda-8" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>9 0.99895591 <a title="346-lda-9" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>10 0.99869663 <a title="346-lda-10" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>11 0.99856883 <a title="346-lda-11" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>12 0.99778187 <a title="346-lda-12" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>13 0.99696153 <a title="346-lda-13" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>14 0.99606538 <a title="346-lda-14" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>15 0.99105406 <a title="346-lda-15" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>16 0.99025923 <a title="346-lda-16" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>17 0.99025017 <a title="346-lda-17" href="./cvpr-2013-Scalable_Sparse_Subspace_Clustering.html">379 cvpr-2013-Scalable Sparse Subspace Clustering</a></p>
<p>18 0.98872906 <a title="346-lda-18" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>19 0.98865575 <a title="346-lda-19" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>20 0.98851216 <a title="346-lda-20" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
