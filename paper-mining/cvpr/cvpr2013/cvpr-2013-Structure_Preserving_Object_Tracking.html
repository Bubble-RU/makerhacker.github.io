<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>414 cvpr-2013-Structure Preserving Object Tracking</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-414" href="#">cvpr2013-414</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>414 cvpr-2013-Structure Preserving Object Tracking</h1>
<br/><p>Source: <a title="cvpr-2013-414-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_Structure_Preserving_Object_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>Reference: <a title="cvpr-2013-414-reference" href="../cvpr2013_reference/cvpr-2013-Structure_Preserving_Object_Tracking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nl ft  Abstract Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. [sent-6, score-0.723]
</p><p>2 Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. [sent-7, score-0.85]
</p><p>3 In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. [sent-8, score-0.497]
</p><p>4 The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. [sent-9, score-0.268]
</p><p>5 The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. [sent-10, score-0.527]
</p><p>6 We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object. [sent-11, score-0.76]
</p><p>7 Introduction Object tracking is a fundamental problem in computer vision with applications in a wide range of domains. [sent-13, score-0.194]
</p><p>8 Whereas significant progress has been made in tracking specific objects (e. [sent-14, score-0.278]
</p><p>9 , faces [22], humans [11], and rigid objects [15]), tracking generic objects remains hard. [sent-16, score-0.362]
</p><p>10 Since manually annotating sufficient examples of all objects in the world is prohibitively expensive and time-consuming, recently, approaches for model-free tracking have received increased interest [2, 12]. [sent-17, score-0.278]
</p><p>11 In model-free tracking, the object of interest is manually annotated in the first frame of a video sequence (using a rectangular bounding box). [sent-18, score-0.25]
</p><p>12 A simple approach to tracking multiple objects  is to run multiple instances of a single-object model-free tracker. [sent-24, score-0.278]
</p><p>13 For instance, flowers move in the same direction because of the wind, cars drive in the same direction on the freeway, and when the camera shakes, all objects will move in the same direction. [sent-26, score-0.312]
</p><p>14 We show that it is practical to exploit such spatial constraints between objects in model-free trackers by developing a structure-preserving object tracker (SPOT) that incorporates spatial constraints between objects via a pictorial-structures framework [8]. [sent-27, score-1.324]
</p><p>15 We train the individual object classifiers and the structural constraints jointly using an online structured SVM [3]. [sent-28, score-0.296]
</p><p>16 Our experiments show that the incorporation of structural constraints leads to substantial performance improvements in multi-object tracking: SPOT performs very well on Youtube videos with camera motion, rapidly moving objects, object appearance changes, and occlusions. [sent-29, score-0.416]
</p><p>17 In addition, we show that SPOT may also be used to significantly improve singleobject trackers by using part detectors in addition to the object detector, with spatial constraints between the parts. [sent-30, score-0.68]
</p><p>18 In summary, our main contributions are: (1) we show that incorporating spatial constraints between objects in model-free trackers to improves their performance on multiple-object tracking and (2) we show that using a partbased model improves the performance of single-object model-free trackers. [sent-31, score-0.852]
</p><p>19 Related Work Model-free trackers can be subdivided into (1) trackers that model only the appearance of the object itself [20] and (2) trackers that model the appearance of both the object and the background [2, 9, 12]. [sent-35, score-1.77]
</p><p>20 Much of the recent work in model-free tracking focuses on exploring different feature representations for the object that is being tracked: among others, previous studies have used integral histograms [1], subspace learning [20], sparse representations [16], and local binary patterns [12]. [sent-39, score-0.248]
</p><p>21 Recent work on model-free tracking also focuses on developing new learning approaches to better distinguish the target object from background. [sent-41, score-0.305]
</p><p>22 ×  Our tracker is similar to these approaches in that it updates the appearance model of the target object online. [sent-43, score-0.602]
</p><p>23 Our tracker differs from previous approaches in that it uses a learner that aims to identify configurations of objects or object parts; specifically, an online structured SVM [3]. [sent-44, score-0.685]
</p><p>24 Simultaneous tracking of multiple objects has been studied a lot as well, in particular, in the context of tracking people (e. [sent-45, score-0.496]
</p><p>25 These trackers use a model of what a human looks like, which makes tracking much easier. [sent-48, score-0.712]
</p><p>26 By contrast, we aim to develop a model-free tracker that can track generic objects based on a single annotation without any prior knowledge. [sent-49, score-0.646]
</p><p>27 Up to the best of our knowledge, there is no previous work that attempts to perform such model-free tracking of multiple objects. [sent-50, score-0.194]
</p><p>28 Structure-Preserving Object Tracker  The basis of our tracker is formed by the popular DalalTriggs detector [7], which uses HOG features to describe image patches and an SVM to predict object presence. [sent-52, score-0.519]
</p><p>29 Together, this makes HOG features more sensitive to the spatial location of the object [7], which is very important because the identified location of the object is used to update the classifiers: small localization errors may thus propagate over time, causing the tracker to drift. [sent-57, score-0.683]
</p><p>30 We represent the bounding box that indicates object i∈ V (Wweit hre pVr representing nthdein sget b oofx objects we are tracking) by Bi = {xi, wi, hi} with center location xi = (xi , yi), width wi, {axnd height h wi;i h bo ctehn wi laoncda hioin are fixed. [sent-59, score-0.505]
</p><p>31 The HOG features extracted from image I that correspond to locations inside the object bounding box are concatenated to obtain a feature vector φ(I; Bi). [sent-60, score-0.279]
</p><p>32 Subsequently, we define a graph G = (V, E) over all objects i∈ V that we want to track gwriathp edges V(i,, jE) )∈ o vEe. [sent-61, score-0.177]
</p><p>33 ,j) ∈E Herein, the parameters wi represent linear weights on the HOG features for object i, eij is a vector that represents the length and direction of the spring between object iand j, and the set of all parameters is denoted by Θ: Θ = {w1, . [sent-75, score-0.291]
</p><p>34 Like other model-free trackers [2, 9, 12], we use the previous images and tracked object configurations as positive examples to train our model. [sent-94, score-0.689]
</p><p>35 Hence, we explore two approaches to construct a tree on the objects i∈ V : (1) a star model [8] aconnd (tr2u) a am tirneiem ounm t spanning st ire ∈e mVo :d (1el) [ a24 st]a. [sent-151, score-0.273]
</p><p>36 ,r) ∈E  The minimum spanning tree model is constructed based on the object annotations in the first frame; it is obtained by searching the set of all possible co? [sent-162, score-0.24]
</p><p>37 The computational complexity grows linearly in the number of objects being tracked (i. [sent-175, score-0.201]
</p><p>38 In the first set of experiments, we evaluate the performance of the SPOT tracker on a range of multi-object tracking problems, comparing it to the performance of various state-of-the-art trackers that do not employ structural constraints between the objects. [sent-181, score-1.289]
</p><p>39 In the second set of experiments, we study the use of SPOT to improve single-object tracking by tracking parts of an object and constraining the spatial configuration of those parts. [sent-182, score-0.548]
</p><p>40 An implementation of our tracker is available from http : / /vi s i onl ab . [sent-183, score-0.441]
</p><p>41 Experiment 1: Multiple-Object Tracking We first evaluate the performance of the SPOT tracker on videos in which multiple objects need to be tracked. [sent-188, score-0.665]
</p><p>42 We used nine videos with multiple objects in this set of experiments. [sent-190, score-0.267]
</p><p>43 The videos were selected based on characteristics that are challenging 111888334088  for current model-free trackers, such as the presence of multiple, near objects with a very similar appearance. [sent-192, score-0.224]
</p><p>44 The left column of Figure 1 shows the first frame of each of the nine videos along with the corresponding ground-truth annotations of the objects, i. [sent-194, score-0.243]
</p><p>45 We experiment with three variants of the SPOT tracker: (1) a baseline tracker that does not use structural constraints (i. [sent-197, score-0.577]
</p><p>46 a SPOT tracker with λ = 0; no-SPOT), (2) a SPOT tracker that uses a star model (star-SPOT), and (3) a SPOT tracker that uses a minimum spanning tree (mst-SPOT). [sent-199, score-1.539]
</p><p>47 We compare the performance of our SPOT trackers with that of two state-of-the-art (single-object) trackers, viz. [sent-200, score-0.518]
</p><p>48 the OAB tracker [9] and the TLD tracker [12], which we use to separately track each object. [sent-201, score-0.975]
</p><p>49 The OAB and TLD trackers were run using the implementations provided by their developers. [sent-202, score-0.518]
</p><p>50 We evaluate the performance of the trackers by measuring (1) average distance error (Err. [sent-203, score-0.518]
</p><p>51 ): the average distance of the center of the identified bounding box to the center of the ground-truth bounding box and (2) precision (Prec. [sent-204, score-0.517]
</p><p>52 ): the average percentage of frames for which the overlap between the identified bounding box and the ground-truth bounding box is at least 50 percent. [sent-205, score-0.517]
</p><p>53 The performance of the five trackers on all nine videos is presented in Table 1. [sent-211, score-0.737]
</p><p>54 The performance improvements are particularly impressive for videos in which objects with a similar appearance are tracked, such as the Car Chase, Parade, and Red Flowers videos, because the structural constraints prevent the tracker from switching between objects. [sent-213, score-0.911]
</p><p>55 Structural constraints are also very helpful in videos with a lot of camera shake (such as the Air Show video), because camera shake causes all objects to move in the same direction in the image. [sent-214, score-0.418]
</p><p>56 The SPOT tracker even outperforms single-object trackers when perceptually different objects are tracked that have a relatively weak relation in terms of their location, such as in the Hunting video, because it can share information between objects to deal with, e. [sent-215, score-1.244]
</p><p>57 The mst-SPOT tracker outperforms star-SPOT in most videos, presumably, because a minimum spanning tree imposes direct (rather than indirect) constraints on the object locations. [sent-218, score-0.707]
</p><p>58 Figure 1 shows five frames of all videos with the tracks obtained by mst-SPOT (colors correspond to objects). [sent-219, score-0.232]
</p><p>59 Whereas the baseline trackers (OAB, TLD, and no-SPOT) confuse the planes several times during the course of the video, star-SPOT and mst-SPOT track the right plane throughout the entire video. [sent-224, score-0.611]
</p><p>60 Whereas this occlusion confuses the baseline trackers, the two SPOT trackers do not lose track because they can use the location of one car to estimate the location of the other. [sent-227, score-0.759]
</p><p>61 The video shows several similar flowers that are moving and changing appearance due to the wind, and that sometimes (partially) occlude each other; we track four of these flowers. [sent-229, score-0.267]
</p><p>62 The baseline trackers lose track very quickly because of the partial occlusions. [sent-230, score-0.638]
</p><p>63 By contrast, the two SPOT trackers flawlessly track all flowers during the entire length of the video (2249 frames) by using the structural constraints to distinguish the different flowers. [sent-231, score-0.89]
</p><p>64 The cheetah and gazelle in this video clip are very hard to track, because their appearance changes significantly over time and because their relative location is changing (the cheetah passes the gazelle). [sent-233, score-0.292]
</p><p>65 Nevertheless, the SPOT trackers can exploit the fact that both animals move in a similar direction, which prevents them from losing track. [sent-234, score-0.55]
</p><p>66 Experiment 2: Single Object Tracking With some minor modifications, our SPOT tracker may also be used to improve the tracking of single objects. [sent-237, score-0.659]
</p><p>67 SPOT may be used to track the parts of a single object (treating them as individual objects  in V ) with structural constrains between the parts. [sent-240, score-0.359]
</p><p>68 This makes the tracker more robust to partial occlusions. [sent-241, score-0.441]
</p><p>69 Inspired by [8], we experiment with a SPOT tracker that has a single “global” object detector and a number of “local” part detectors. [sent-242, score-0.519]
</p><p>70 We experiment with a star model in which the global detector forms the root of the star (star-SPOT), and with a model that constructs a minimum spanning tree over the global object and the local part detectors (mst-SPOT). [sent-243, score-0.406]
</p><p>71 Because a single bounding box is used to annotate the object in the first frame of the video, we need to determine what parts the model will use. [sent-245, score-0.357]
</p><p>72 As a latent SVM [8] is unlikely to work well on a single training example, we use a heuristic that assumes that relevant parts correspond to discriminative regions inside the object bounding box. [sent-246, score-0.222]
</p><p>73 Performance of five model-free trackers on multiple-object videos measured in terms of (1) average distance in pixels between centers of the predicted and the ground-truth bounding box (Err. [sent-253, score-0.944]
</p><p>74 To measure the precision, a detection is considered correct if the overlap between the identified bounding box and the ground truth bounding box is at least 50%. [sent-256, score-0.517]
</p><p>75 ))2,  (7)  where Bi and B denote the bounding box of the part and of the global object, respectively. [sent-266, score-0.225]
</p><p>76 B iW toe 4fix0% th oef w wthied hwi adnthd and height of the bounding box B, and we ensure that the selected part cannot have more than 50% overlap with the other parts. [sent-269, score-0.279]
</p><p>77 The experiments are performed on a publicly available collection of twelve videos [2]. [sent-273, score-0.24]
</p><p>78 The videos contain a wide range of objects that are subject to sudden movements and (out-of-plane) rotations, and have cluttered, dynamic backgrounds. [sent-274, score-0.224]
</p><p>79 Each video contains a single object to be tracked, which is indicated by a bounding box in the first frame of the video. [sent-276, score-0.355]
</p><p>80 ) Again, we evaluate the performance of the trackers by measuring the average distance error and the precision of the tracker, and averaging over five runs. [sent-278, score-0.583]
</p><p>81 We compare the performance of our tracker with that of three state-of-the-art trackers, viz. [sent-279, score-0.441]
</p><p>82 , the OAB tracker [9], the MILBoost tracker [2], and the TLD tracker [12]. [sent-280, score-1.323]
</p><p>83 All trackers were run on a single scale; results with multi-scale trackers are presented in the supplemental material. [sent-281, score-1.036]
</p><p>84 We could not run the implementation of the MILBoost tracker as it is outdated (the MILBoost tracker was not considered in Experiment 1 for this reason), but because we use exactly the same experi-  mental setup as in [2], we adopt the results presented there. [sent-282, score-0.882]
</p><p>85 Table 2 presents the performance of all six trackers on all twelve videos. [sent-284, score-0.618]
</p><p>86 Figure 2 shows the tracks obtained with the MIL, OAB, TLD, and mst-SPOT trackers on seven of the twelve videos. [sent-285, score-0.674]
</p><p>87 The results reveal the potential benefit of using additional part detectors when tracking a single object: mst-SPOT is the best-performing tracker on eight of the twelve videos in terms of average distance between centers, and on nine of the twelve videos in terms of precision. [sent-286, score-1.21]
</p><p>88 The results also show that mst-SPOT generally outperforms starSPOT, which suggests that for object detection in still images, pictorial-structure models with a minimum spanning tree [24] may be better than those with a star tree [8]. [sent-288, score-0.319]
</p><p>89 Conclusion and Future Work In this paper, we have developed a new model-free tracker that simultaneously tracks multiple objects by combining multiple single-object trackers via constraints on the spatial structure of the objects. [sent-290, score-1.155]
</p><p>90 Our experimental results show that the resulting SPOT tracker substantially outperforms traditional trackers in settings in which multiple objects need to be tracked. [sent-291, score-1.043]
</p><p>91 Moreover, we have showed that  the SPOT tracker can also improve the tracking of single objects by including additional detectors for object parts in the tracker. [sent-292, score-0.873]
</p><p>92 The computational costs of our tracker only grow linearly in the number of objects (or object parts) that is being tracked, which facilitates real-time tracking. [sent-293, score-0.579]
</p><p>93 Performance of six model-free trackers on single-object videos measured in terms of (1) average distance in pixels (Err) between the centers of the predicted and the ground-truth bounding box (lower is better) and (2) precision (higher is better). [sent-295, score-0.937]
</p><p>94 To measure the  precision, a detection is counted as correct if the overlap between the identified bounding box and the ground truth bounding box is at least  50%. [sent-296, score-0.517]
</p><p>95 plemented in other model-free trackers that are based on tracking-by-detection, such as the TLD tracker. [sent-299, score-0.518]
</p><p>96 It is likely that including structural constraints in such trackers will improve their performance in tracking of multiple objects, too. [sent-300, score-0.848]
</p><p>97 In future work, we aim to explore the use of different structural constraints between the tracked objects; for instance, for tracking certain deformable objects it may be better to use a structural model based on PCA (as is done in, e. [sent-301, score-0.636]
</p><p>98 Robust visual tracking and vehicle classification via sparse representation. [sent-403, score-0.194]
</p><p>99 Tracking results on seven of the twelve videos (David Indoor, Dollar, Girl, Tiger 2, Coke Can, Occl. [sent-410, score-0.24]
</p><p>100 Decentralized multiple target tracking using netted collaborative autonomous trackers. [sent-462, score-0.22]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trackers', 0.518), ('tracker', 0.441), ('spot', 0.364), ('tracking', 0.194), ('tld', 0.148), ('videos', 0.14), ('oab', 0.135), ('bounding', 0.12), ('tracked', 0.117), ('box', 0.105), ('twelve', 0.1), ('track', 0.093), ('objects', 0.084), ('spanning', 0.08), ('structural', 0.08), ('eij', 0.079), ('flowers', 0.074), ('milboost', 0.069), ('hog', 0.068), ('bi', 0.063), ('star', 0.06), ('configuration', 0.058), ('constraints', 0.056), ('tracks', 0.056), ('online', 0.055), ('appearance', 0.054), ('object', 0.054), ('gazelle', 0.052), ('iinit', 0.052), ('detectors', 0.052), ('structured', 0.051), ('hyperparameter', 0.05), ('tree', 0.049), ('parts', 0.048), ('location', 0.048), ('delft', 0.046), ('cheetah', 0.046), ('video', 0.046), ('wi', 0.043), ('hunting', 0.043), ('qij', 0.043), ('nine', 0.043), ('cars', 0.042), ('parade', 0.04), ('skating', 0.04), ('ij', 0.04), ('wind', 0.038), ('shaking', 0.038), ('springs', 0.038), ('tiger', 0.038), ('identified', 0.038), ('spring', 0.037), ('five', 0.036), ('svm', 0.035), ('pages', 0.034), ('basketball', 0.033), ('move', 0.032), ('improvements', 0.032), ('mil', 0.031), ('developing', 0.031), ('maximization', 0.03), ('annotations', 0.03), ('wit', 0.03), ('frame', 0.03), ('overlap', 0.029), ('movies', 0.029), ('shake', 0.029), ('precision', 0.029), ('annotation', 0.028), ('grabner', 0.028), ('air', 0.028), ('minimum', 0.027), ('lose', 0.027), ('david', 0.027), ('vec', 0.027), ('updates', 0.027), ('svms', 0.026), ('xi', 0.026), ('predicts', 0.026), ('target', 0.026), ('centers', 0.025), ('height', 0.025), ('deformable', 0.025), ('car', 0.025), ('lot', 0.024), ('direction', 0.024), ('minor', 0.024), ('impressive', 0.024), ('score', 0.024), ('detector', 0.024), ('flawlessly', 0.023), ('vete', 0.023), ('vande', 0.023), ('edd', 0.023), ('bej', 0.023), ('arbg', 0.023), ('pita', 0.023), ('compresses', 0.023), ('bthetew', 0.023), ('wweit', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="414-tfidf-1" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>2 0.5098958 <a title="414-tfidf-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.35359335 <a title="414-tfidf-3" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>Author: unkown-author</p><p>Abstract: We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the “right” frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.</p><p>4 0.34137565 <a title="414-tfidf-4" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>5 0.32227427 <a title="414-tfidf-5" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>6 0.3182371 <a title="414-tfidf-6" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>7 0.17903142 <a title="414-tfidf-7" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>8 0.17756276 <a title="414-tfidf-8" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>9 0.1765068 <a title="414-tfidf-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.1632573 <a title="414-tfidf-10" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>11 0.15405755 <a title="414-tfidf-11" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>12 0.14582103 <a title="414-tfidf-12" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>13 0.13362944 <a title="414-tfidf-13" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>14 0.12941273 <a title="414-tfidf-14" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>15 0.1255817 <a title="414-tfidf-15" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>16 0.12293495 <a title="414-tfidf-16" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>17 0.12292521 <a title="414-tfidf-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.1171677 <a title="414-tfidf-18" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>19 0.10349794 <a title="414-tfidf-19" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>20 0.099585444 <a title="414-tfidf-20" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.235), (1, -0.043), (2, -0.006), (3, -0.148), (4, 0.042), (5, -0.045), (6, 0.24), (7, -0.165), (8, 0.135), (9, 0.25), (10, -0.163), (11, -0.2), (12, -0.108), (13, 0.148), (14, -0.078), (15, -0.059), (16, 0.067), (17, -0.018), (18, 0.036), (19, 0.026), (20, 0.034), (21, 0.031), (22, 0.163), (23, -0.106), (24, -0.057), (25, 0.001), (26, -0.085), (27, 0.075), (28, 0.079), (29, 0.062), (30, 0.017), (31, 0.053), (32, 0.075), (33, 0.033), (34, 0.01), (35, -0.003), (36, 0.015), (37, -0.001), (38, -0.005), (39, 0.055), (40, 0.006), (41, 0.043), (42, -0.039), (43, 0.03), (44, 0.069), (45, -0.01), (46, -0.072), (47, 0.098), (48, -0.029), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94169962 <a title="414-lsi-1" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>2 0.93662173 <a title="414-lsi-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.87699401 <a title="414-lsi-3" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>4 0.85929966 <a title="414-lsi-4" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>5 0.82961559 <a title="414-lsi-5" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>Author: unkown-author</p><p>Abstract: We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the “right” frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.</p><p>6 0.75356179 <a title="414-lsi-6" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>7 0.74996531 <a title="414-lsi-7" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>8 0.73268616 <a title="414-lsi-8" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>9 0.69744468 <a title="414-lsi-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.59541523 <a title="414-lsi-10" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>11 0.5828805 <a title="414-lsi-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.56451392 <a title="414-lsi-12" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>13 0.50643134 <a title="414-lsi-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.47203556 <a title="414-lsi-14" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>15 0.45411551 <a title="414-lsi-15" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>16 0.4494848 <a title="414-lsi-16" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>17 0.44856775 <a title="414-lsi-17" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>18 0.44310698 <a title="414-lsi-18" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>19 0.43723118 <a title="414-lsi-19" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>20 0.43561253 <a title="414-lsi-20" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.087), (10, 0.211), (16, 0.019), (26, 0.072), (27, 0.016), (33, 0.222), (67, 0.114), (69, 0.044), (78, 0.031), (87, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92565513 <a title="414-lda-1" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>2 0.91878837 <a title="414-lda-2" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>3 0.91283399 <a title="414-lda-3" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>4 0.91231364 <a title="414-lda-4" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>5 0.91121298 <a title="414-lda-5" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>6 0.91020751 <a title="414-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.90780735 <a title="414-lda-7" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>8 0.90712607 <a title="414-lda-8" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>9 0.90563858 <a title="414-lda-9" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>10 0.90167719 <a title="414-lda-10" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>11 0.90147549 <a title="414-lda-11" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>12 0.89630747 <a title="414-lda-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.89614588 <a title="414-lda-13" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>14 0.89613312 <a title="414-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.8951562 <a title="414-lda-15" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>16 0.89490867 <a title="414-lda-16" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>17 0.89426732 <a title="414-lda-17" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>18 0.89189041 <a title="414-lda-18" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>19 0.89173698 <a title="414-lda-19" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>20 0.89162862 <a title="414-lda-20" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
