<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-71" href="#">cvpr2013-71</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</h1>
<br/><p>Source: <a title="cvpr-2013-71-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Karsch_Boundary_Cues_for_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>Reference: <a title="cvpr-2013-71-reference" href="../cvpr2013_reference/cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. [sent-2, score-0.446]
</p><p>2 However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. [sent-3, score-0.649]
</p><p>3 In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. [sent-4, score-0.929]
</p><p>4 In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. [sent-5, score-0.701]
</p><p>5 Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e. [sent-6, score-0.684]
</p><p>6 Early approaches to object recognition [14] considered shape reconstruction as the first step. [sent-11, score-0.203]
</p><p>7 As data-driven approaches to recognition became popular, researchers began to represent shape implicitly through weighted image gradient features, rather than explicitly through reconstruction [13]. [sent-12, score-0.203]
</p><p>8 In this paper, we focus on improving our understanding of the importance of boundary shape cues for 3D shape reconstruction and recognition. [sent-34, score-0.59]
</p><p>9 In particular, we consider boundaries due to object silhouette, self-occlusion (depth discontinuity) and folds (surface normal discontinuity). [sent-35, score-0.483]
</p><p>10 We also consider cues for whether boundaries are soft (extrema of curved surface) or sharp. [sent-36, score-0.275]
</p><p>11 On the standard dataset, reconstructions using various cues are compared via metrics of surface normal and depth accuracy. [sent-38, score-0.542]
</p><p>12 Our main contribution is to evaluate the importance of various boundary and shading cues for shape reconstruction and shape-based recognition. [sent-41, score-0.824]
</p><p>13 We extend Barron and Malik’s shape from shading and silhouette method [1] to include interior occlusions with figure/ground labels, folds, and sharp/soft boundary labels. [sent-42, score-0.789]
</p><p>14 The standard evaluation is based on depth error, surface normal, shading,  or reflectance on the MIT Intrinsic Image dataset. [sent-43, score-0.254]
</p><p>15 We also introduce perceptual and recognition-based measures of reconstruction quality for the PASCAL VOC dataset (Fig 1 shows one example of the types of reconstructions we evaluate, and the annotation required by our algorithm). [sent-44, score-0.193]
</p><p>16 Furthermore, much work has gone into shape-based representations for recognition, focusing on the cues provided by the silhouette 222111666311  Input image/labels  silh  +selfocc  +folds  Figure1. [sent-46, score-0.592]
</p><p>17 Foragiveni putimage,wehand-labelgeometric uesincluding: +occ+folds  +shading  +shading+occ+folds  smooth silhouette contour (red), sharp silhouette contour (cyan), self occlusions (green), and folds (orange). [sent-47, score-1.322]
</p><p>18 We then use various combinations of these cues (as well as appearance-based cues) to obtain different shape reconstructions  (see Sec 3). [sent-48, score-0.44]
</p><p>19 We evaluate these reconstructions  in a variety of tasks in order to find which set(s) of  cues may be most beneficial for reconstructing shapes. [sent-49, score-0.363]
</p><p>20 Our findings suggest a 3D representation that incorporates interior occlusions and folds might benefit such existing systems. [sent-54, score-0.443]
</p><p>21 Our study is a good step towards understanding shape reconstruction in the context of recognition, but we must leave several aspects of this complex problem unexplored. [sent-56, score-0.238]
</p><p>22 Eventually, we will want automatic recovery of shape cues and reconstruction algorithms that handle uncertainty. [sent-58, score-0.415]
</p><p>23 Second, cues such as ground contact points and object-level shape priors are useful but not investigated. [sent-59, score-0.373]
</p><p>24 Cues for object reconstruction We focus on reconstructing shape from geometric cues, revisiting early work on reconstructing shape from line  drawings [11, 12]. [sent-64, score-0.49]
</p><p>25 Through human labeling, we collect information about an object’s silhouette, self-occlusions, and folds in the surface. [sent-65, score-0.405]
</p><p>26 Since appearance can be a helpful factor in determining shape, we also investigate the benefit of shading cues using the shape-from-shading priors of Barron and Malik [1]. [sent-66, score-0.607]
</p><p>27 Following Barron and Malik’s notation, we write Z for the surface (represented by a height field viewed orthographically), and N : R → R3 aas h tehieg hfutn fiectlidon v itehwate dtak oertsh oag hreapighhitc afil eyld), t aon dsur Nfac :e R Rno →rma Rls (component-wise; N = (Nx, Ny, Nz)). [sent-69, score-0.189]
</p><p>28 The silhouette is rich with shape information, both perceptually and geometrically [10]. [sent-74, score-0.342]
</p><p>29 At the occluding contour of an object, the surface is tangent to all rays from the vantage point, unless however there is a discontinuity in surface normals across the visible and non-visible regions of the object (e. [sent-75, score-0.592]
</p><p>30 We treat these two cases separately, labeling parts of the silhouette as smooth if the surface normal should lie perpendicular to both the viewing direction and image silhouette, and sharp otherwise1 . [sent-78, score-0.536]
</p><p>31 In the case of a smooth silhouette contour, the z-component of the normal is 0, and the x and y components are normal to the silhouette (i. [sent-79, score-0.572]
</p><p>32 Denoting (nx , ny) as normals of the silhouette contour, and Csmooth as the set of pixels labelled as the smooth part of the silhouette, we write the silhouette constraint as:  fsfc(Z) =i∈C? [sent-82, score-0.64]
</p><p>33 (2) This is the most typical constraint used in shape-fromcontour algorithms (hence the notation fsfc), and is identical to that used by Barron and Malik, with the notable exception that we only enforce the constraint when the silhouette is not sharp. [sent-85, score-0.285]
</p><p>34 If the silhouette is labelled sharp, there is no added constraint. [sent-86, score-0.223]
</p><p>35 The boundary of a selfocclusion implies a discontinuity in depth, and thus the surface along the foreground boundary should be constrained to be tangent to the viewing direction. [sent-89, score-0.405]
</p><p>36 Besides knowing a self occlusion boundary, it is also mandatory to know which side of the contour is in front of the other (figure and ground labels). [sent-90, score-0.272]
</p><p>37 With this information, we impose additional surface normal constraints along self occlusion boundaries (Cselfocc):  fselfocc(Z) =i∈? [sent-91, score-0.356]
</p><p>38 (3) Notice that there is no explicit constraint to force the height of the foreground to be greater than that of the background; however, by constraining the foreground normals to be pointing outward and perpendicular to the viewing direction, the correct effect is achieved. [sent-94, score-0.269]
</p><p>39 A fold in the surface denotes a discontinuity in surface normals across a contour along the object, e. [sent-97, score-0.731]
</p><p>40 folds on a cube are at 90◦, but this is not always the case), and can be convex (surface normals pointing away from  ×  each other) or concave (surface normals pointing towards each other). [sent-102, score-0.708]
</p><p>41 Our labels consist of fold contours and also a flag denoting whether the given fold is convex or concave. [sent-103, score-0.488]
</p><p>42 We did not annotate exact fold orientation as this task is susceptible to human error and tedious. [sent-104, score-0.218]
</p><p>43 We incorporate fold labels by adding another term to our objective function, developed using intuition from Malik and Maydan [12]. [sent-105, score-0.218]
</p><p>44 The idea is to constrain normals at pixels that lie across a fold to have convex or concave orientation (depending on the label), and to be oriented consistently in the direction of the fold. [sent-106, score-0.335]
</p><p>45 , Nir as two corresponding normals across pixel iin the fold contour C. [sent-108, score-0.426]
</p><p>46 We use the albedo and illumination priors of Barron and Malik to incorporate shading cues into our reconstructions. [sent-126, score-0.679]
</p><p>47 For brevity, we denote priors on reflectance as g(R), and priors on illumination as h(L), where R is log-diffuse reflectance (logalbedo) and L is the 27-dimensional RGB spherical harmonic coefficient vector. [sent-129, score-0.296]
</p><p>48 Jointly estimating shape along with albedo and illumination requires an additional constraint that forces a rendering of the surface to match the input image. [sent-131, score-0.346]
</p><p>49 Assuming Lambertian reflectance and disregarding occlusions, our render-  ing function is simply reflectance multiplied by shading (or in log space, log-reflectance plus log-shading). [sent-132, score-0.511]
</p><p>50 Denoting I as the log-input image, R as log-diffuse reflectance (logalbedo), and S(Z, L) as the log-shaded surface Z under light L, we write the shape-from-shading constraint as: csfs  (Z, R, L) = R + S(Z, L) − I. [sent-133, score-0.332]
</p><p>51 Notice that shading cues are only incorporated if δsfs > 0; otherwise, our reconstructions rely purely on geometric information. [sent-139, score-0.705]
</p><p>52 com/sibl 222111666533  Input + annotations  +occ+folds View 1  View 2  +shading View 1  +shading+occ+folds View 2  View 1  View 2  contour (red), sharp silhouette contour (cyan), self occlusions (green), and folds (orange). [sent-145, score-1.065]
</p><p>53 This paper is the first that we know of to provide a rigorous analysis of shape reconstruction on typical objects in consumer photographs (e. [sent-150, score-0.203]
</p><p>54 Evaluation of shape and appearance cues In this section, we examine each of the cues used in our shape reconstruction method, and hope to find a cue or set of cues that lead to better shape estimates (qualitatively, and in  terms of recognition ability). [sent-160, score-1.114]
</p><p>55 Our objective function (Eq 1) allows us to easily produce shape reconstructions for various combinations of cues by turning “on” and “off” different cues; equivalently, setting the corresponding weights to 1 (on) or 0 (off). [sent-161, score-0.477]
</p><p>56 We use six different cue combinations to see which cue or set of cues contribute most to a better reconstruction. [sent-162, score-0.319]
</p><p>57 These six combinations are: •  •  •  •  •  •  silh: Priors on silhouette shape and surface smoothness; iP. [sent-163, score-0.499]
</p><p>58 and fold constraints  (δsfc  =  +occ+folds: Silhouette, self occlusion and fold cons+torcaicn+tsfo o(lδdsfsc: = Si δlhsoeulfoecttce =, s δeflfold osc =clu 1si). [sent-168, score-0.59]
</p><p>59 Note that silh cues are present in each algorithm (hence the ‘+’ prefix). [sent-176, score-0.369]
</p><p>60 To find which cues are most critical for recovering shape, we evaluate each algorithm on a variety of tasks that measure shape quality and shape recognition. [sent-177, score-0.45]
</p><p>61 Since we do not have ground truth shape for VOC objects, we conduct two user studies to evaluate qualitative performance: qualitative rating and shape-based recognition. [sent-181, score-0.352]
</p><p>62 Finally, we ran a quantitative comparison of depth and surface normals using the MIT depth dataset. [sent-183, score-0.316]
</p><p>63 The remainder of this section details our results for each of these tasks, split under headings concerning shape quality and shape recognition. [sent-184, score-0.238]
</p><p>64 The goal of these experiments is to find a common set of cues, or shape reconstruction algorithm(s), that consistently report the best shape. [sent-188, score-0.203]
</p><p>65 The qualitative rating portion of the user study collected subjects’ ratings for each of the six shape reconstruction algorithms. [sent-190, score-0.501]
</p><p>66 3) that displays the visualization of the six shape estimation results side by side on the screen and allows parQuality rating from user study  certain rating was assigned to it during the qualitative rating user study. [sent-192, score-0.683]
</p><p>67 For example, for the left most column, +shading was rated above silh approximately 60% of the time, below silh about 10% of the time, and rated the same otherwise. [sent-197, score-0.488]
</p><p>68 Shading seems to help when accompanied by with a silhouette cues, but when additional boundary cues are present, shading tends to produce more artifacts than improvements. [sent-198, score-0.844]
</p><p>69 We also see a strong improvement from combining fold and occlusion contours. [sent-199, score-0.249]
</p><p>70 Figure 4 shows the aver-  aged rating score grouped by algorithm; where a higher average rating indicates a better shape. [sent-206, score-0.246]
</p><p>71 In every case, as intuition suggests, adding more geometric cues leads to a more preferable shape. [sent-207, score-0.243]
</p><p>72 Here, we see geomet222111666755  ric cues (other than silh) were consistently preferred over shading cues; in one example, +occ+folds was rated higher than +shading+occ+folds about 40% of the time. [sent-209, score-0.652]
</p><p>73 Using ground truth shapes available from the MIT Intrinsic Image dataset [8], we analyze our shape reconstructions using established errors metrics. [sent-211, score-0.283]
</p><p>74 46739A0 48E273†  We observe that adding geometric cues generally increase quantitative performance. [sent-219, score-0.243]
</p><p>75 We also consider that object silhouette could be a dominating factor for recognition; to reduce this factor, we show a silhouette-masked view of each result first (Fig. [sent-234, score-0.264]
</p><p>76 For each algorithm, the left bar shows the result from the masked view; the right bar shows that result from the unmasked view. [sent-246, score-0.205]
</p><p>77 In the masked view, +occ+folds yields the lowest recognition error, consistent with qualitative rating portion of our user study. [sent-247, score-0.258]
</p><p>78 on each reconstruction as well as rgb, rgb+occ+folds, and rgb+shading+occ+folds for the kernel matching method to determine if shape and shading cues add information compared to RGB alone. [sent-258, score-0.768]
</p><p>79 The shape reconstructions increase the accuracy of the result, but this could be partially due to the mask provided by the height which is not available in the RGB only method. [sent-267, score-0.254]
</p><p>80 Conclusion We demonstrate a simple and extensible technique for reconstructing shape from images, resurrecting highly informative cues from early vision work. [sent-269, score-0.397]
</p><p>81 Our method itself is an extension of Barron and Malik’s [1] reconstruction framework, and we show how additional cues can be incorporated in this framework to create improved reconstructions. [sent-270, score-0.296]
</p><p>82 Through our experiments, we have shown the necessity of considering cues that go beyond typical shape-fromshading constraints. [sent-271, score-0.212]
</p><p>83 In almost every task we assessed, using more geometric cues gives better results. [sent-272, score-0.243]
</p><p>84 For humanbased tasks, shading cues seem to help when applied with to silhouette cues (+shading consistently outperforms silh), but adds little information once additional boundary cues are incorporated (+occ+folds performs similarly to +shad-  ing+occ+folds); see Figs 4 and 7. [sent-273, score-1.268]
</p><p>85 As one might expect, adding geometric features to the existing rgb information improves recognition accuracy, and shape tends to be more revealing than appearance alone. [sent-276, score-0.221]
</p><p>86 better than +shading+occ+folds (Fig 7; masked errors), and shading cues seem to have an adverse effect on automatic recognition algorithms (Table 1). [sent-278, score-0.621]
</p><p>87 One interesting observation from our experiments is that our shading cues tend to confound boundary cues;  e. [sent-282, score-0.621]
</p><p>88 It seems counterintuitive that incorporating shading information would degrade reconstructions, and we offer several possible causes. [sent-286, score-0.353]
</p><p>89 Foremost is the fact that we weight all terms equally, whereas learning these weights from ground truth will lead to better shading reconstructions (evidenced especially by our quantitative results on the MIT Intrinsic dataset in Sec 3. [sent-287, score-0.499]
</p><p>90 Our evaluations show that self occlusion and fold cues are undoubtedly helpful, and most importantly, point in many directions for improving existing shape reconstruction algorithms. [sent-295, score-0.787]
</p><p>91 Extracting boundary cues, such as folds and self occlusions, automatically from photographs is a logical next step. [sent-296, score-0.584]
</p><p>92 Appendix: Fold constraint implementation  Consider the (i)th point on the contour C, parametrized by position p = [px , py] and tangent vector u = [ux , uy], both on the image plane. [sent-421, score-0.228]
</p><p>93 By default, this fold is convex g—en ftol vdeecdto irn: tvhe = =di [r−euction of negative Z. [sent-424, score-0.218]
</p><p>94 = [round (px vx) , round (py vy)] = [round (px − vx) , round (py − vy)]  pr  (9) (10)  Given a normal field N we compute the normal of the surface at these “left” and “right” points:  N? [sent-427, score-0.286]
</p><p>95 Nzr)  (13)  If c = 1, then the cross product of the surface normals on both sides of the contour is exactly equal to the tangent vector, and the surface is therefore convexly folded in the direction of the contour. [sent-439, score-0.579]
</p><p>96 Intuitively, to force the surface to satisfy the fold constraint imposed by the contour, we should force c to be as close to 1 as possible. [sent-442, score-0.421]
</p><p>97 But constraining c = 1is not appropriate for our purposes, as it ignores the fact that u and therefore v lie in an image plane, while the true tangent vector of the contour may not be parallel to the image plane. [sent-444, score-0.197]
</p><p>98 -insensitive hinge loss which allows for fold contours to be oriented as much as 45◦ out of the image plane. [sent-452, score-0.246]
</p><p>99 = 1 is only satisfied by a perfect fold whose crease is parallel with the image plane. [sent-456, score-0.218]
</p><p>100 = √12 produces folds that are roughly 90◦, and which look reasonable upon inspection. [sent-458, score-0.405]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('folds', 0.405), ('occ', 0.379), ('shading', 0.353), ('silhouette', 0.223), ('fold', 0.218), ('cues', 0.212), ('barron', 0.161), ('silh', 0.157), ('surface', 0.124), ('self', 0.123), ('rating', 0.123), ('shape', 0.119), ('selfocc', 0.118), ('contour', 0.118), ('reconstructions', 0.109), ('malik', 0.109), ('unmasked', 0.099), ('normals', 0.09), ('rated', 0.087), ('reconstruction', 0.084), ('reflectance', 0.079), ('tangent', 0.079), ('niy', 0.079), ('voc', 0.072), ('rgb', 0.071), ('py', 0.071), ('nz', 0.06), ('csfs', 0.059), ('pxr', 0.059), ('pyr', 0.059), ('nix', 0.058), ('sfs', 0.058), ('discontinuity', 0.057), ('masked', 0.056), ('boundary', 0.056), ('shapes', 0.055), ('depth', 0.051), ('user', 0.048), ('uy', 0.047), ('normal', 0.046), ('participant', 0.046), ('ux', 0.044), ('folded', 0.044), ('nx', 0.044), ('mit', 0.043), ('priors', 0.042), ('orthographic', 0.042), ('albedo', 0.042), ('reconstructing', 0.042), ('view', 0.041), ('ny', 0.04), ('sharp', 0.04), ('cselfocc', 0.039), ('ffold', 0.039), ('fsfc', 0.039), ('logalbedo', 0.039), ('nzr', 0.039), ('write', 0.039), ('occlusions', 0.038), ('cube', 0.038), ('cue', 0.037), ('fig', 0.037), ('weights', 0.037), ('perpendicular', 0.036), ('study', 0.035), ('round', 0.035), ('smooth', 0.034), ('viewing', 0.033), ('six', 0.033), ('vlfeat', 0.032), ('reconsider', 0.032), ('nir', 0.032), ('boundaries', 0.032), ('curved', 0.031), ('occlusion', 0.031), ('qualitative', 0.031), ('geometric', 0.031), ('constraint', 0.031), ('illumination', 0.03), ('sec', 0.03), ('lambertian', 0.03), ('drawings', 0.029), ('pointing', 0.029), ('asked', 0.029), ('contours', 0.028), ('ratings', 0.028), ('concave', 0.027), ('px', 0.027), ('pegasos', 0.027), ('harmonics', 0.026), ('intrinsic', 0.026), ('height', 0.026), ('alone', 0.025), ('vy', 0.025), ('bar', 0.025), ('denoting', 0.024), ('spherical', 0.024), ('force', 0.024), ('early', 0.024), ('vx', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="71-tfidf-1" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>2 0.31092155 <a title="71-tfidf-2" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>3 0.23917994 <a title="71-tfidf-3" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>4 0.19888495 <a title="71-tfidf-4" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>5 0.19047734 <a title="71-tfidf-5" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>Author: Eno Töppe, Claudia Nieuwenhuis, Daniel Cremers</p><p>Abstract: We introduce the concept of relative volume constraints in order to account for insufficient information in the reconstruction of 3D objects from a single image. The key idea is to formulate a variational reconstruction approach with shape priors in form of relative depth profiles or volume ratios relating object parts. Such shape priors can easily be derived either from a user sketch or from the object’s shading profile in the image. They can handle textured or shadowed object regions by propagating information. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware. In contrast to existing single view reconstruction algorithms, the proposed algorithm provides substantially more flexibility to recover shape details such as self-occlusions, dents and holes, which are not visible in the object silhouette.</p><p>6 0.17633736 <a title="71-tfidf-6" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>7 0.13740513 <a title="71-tfidf-7" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>8 0.12161188 <a title="71-tfidf-8" href="./cvpr-2013-Whitened_Expectation_Propagation%3A_Non-Lambertian_Shape_from_Shading_and_Shadow.html">466 cvpr-2013-Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow</a></p>
<p>9 0.12011322 <a title="71-tfidf-9" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>10 0.11737731 <a title="71-tfidf-10" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>11 0.11289424 <a title="71-tfidf-11" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>12 0.11233475 <a title="71-tfidf-12" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>13 0.10818458 <a title="71-tfidf-13" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>14 0.10559191 <a title="71-tfidf-14" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>15 0.10288133 <a title="71-tfidf-15" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>16 0.10140426 <a title="71-tfidf-16" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>17 0.098795354 <a title="71-tfidf-17" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>18 0.091992073 <a title="71-tfidf-18" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>19 0.086853199 <a title="71-tfidf-19" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>20 0.080431499 <a title="71-tfidf-20" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, 0.166), (2, 0.025), (3, 0.054), (4, 0.015), (5, -0.098), (6, -0.08), (7, 0.111), (8, 0.045), (9, -0.025), (10, -0.053), (11, -0.162), (12, -0.112), (13, 0.018), (14, 0.095), (15, 0.034), (16, 0.017), (17, -0.032), (18, -0.067), (19, -0.025), (20, -0.029), (21, 0.019), (22, -0.019), (23, 0.015), (24, 0.07), (25, 0.081), (26, 0.087), (27, -0.025), (28, 0.012), (29, 0.035), (30, 0.18), (31, -0.124), (32, -0.054), (33, 0.179), (34, 0.044), (35, 0.112), (36, -0.033), (37, 0.01), (38, -0.047), (39, -0.017), (40, -0.087), (41, 0.003), (42, -0.014), (43, -0.11), (44, 0.137), (45, 0.089), (46, 0.1), (47, -0.038), (48, -0.053), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93323761 <a title="71-lsi-1" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>2 0.86876577 <a title="71-lsi-2" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>3 0.70825815 <a title="71-lsi-3" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>Author: Chen Li, Shuochen Su, Yasuyuki Matsushita, Kun Zhou, Stephen Lin</p><p>Abstract: We present a method that enhances the performance of depth-from-defocus (DFD) through the use of shading information. DFD suffers from important limitations namely coarse shape reconstruction and poor accuracy on textureless surfaces that can be overcome with the help of shading. We integrate both forms of data within a Bayesian framework that capitalizes on their relative strengths. Shading data, however, is challenging to recover accurately from surfaces that contain texture. To address this issue, we propose an iterative technique that utilizes depth information to improve shading estimation, which in turn is used to elevate depth estimation in the presence of textures. With this approach, we demonstrate improvements over existing DFD techniques, as well as effective shape reconstruction of textureless surfaces. – –</p><p>4 0.68540317 <a title="71-lsi-4" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>5 0.68077415 <a title="71-lsi-5" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>Author: Eno Töppe, Claudia Nieuwenhuis, Daniel Cremers</p><p>Abstract: We introduce the concept of relative volume constraints in order to account for insufficient information in the reconstruction of 3D objects from a single image. The key idea is to formulate a variational reconstruction approach with shape priors in form of relative depth profiles or volume ratios relating object parts. Such shape priors can easily be derived either from a user sketch or from the object’s shading profile in the image. They can handle textured or shadowed object regions by propagating information. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware. In contrast to existing single view reconstruction algorithms, the proposed algorithm provides substantially more flexibility to recover shape details such as self-occlusions, dents and holes, which are not visible in the object silhouette.</p><p>6 0.64007521 <a title="71-lsi-6" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>7 0.62847739 <a title="71-lsi-7" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>8 0.59532624 <a title="71-lsi-8" href="./cvpr-2013-Whitened_Expectation_Propagation%3A_Non-Lambertian_Shape_from_Shading_and_Shadow.html">466 cvpr-2013-Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow</a></p>
<p>9 0.58989912 <a title="71-lsi-9" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>10 0.55695814 <a title="71-lsi-10" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>11 0.5029906 <a title="71-lsi-11" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>12 0.50109428 <a title="71-lsi-12" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>13 0.47789562 <a title="71-lsi-13" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>14 0.4520683 <a title="71-lsi-14" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>15 0.45068997 <a title="71-lsi-15" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>16 0.41474593 <a title="71-lsi-16" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>17 0.40811646 <a title="71-lsi-17" href="./cvpr-2013-Monocular_Template-Based_3D_Reconstruction_of_Extensible_Surfaces_with_Local_Linear_Elasticity.html">289 cvpr-2013-Monocular Template-Based 3D Reconstruction of Extensible Surfaces with Local Linear Elasticity</a></p>
<p>18 0.39447129 <a title="71-lsi-18" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>19 0.39400727 <a title="71-lsi-19" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>20 0.39318931 <a title="71-lsi-20" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.105), (16, 0.015), (26, 0.072), (33, 0.204), (44, 0.199), (67, 0.057), (69, 0.043), (77, 0.015), (87, 0.171)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84457231 <a title="71-lda-1" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>2 0.81881613 <a title="71-lda-2" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>3 0.81116164 <a title="71-lda-3" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>4 0.81045347 <a title="71-lda-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.79900753 <a title="71-lda-5" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>6 0.79822803 <a title="71-lda-6" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>7 0.79758328 <a title="71-lda-7" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>8 0.79718584 <a title="71-lda-8" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>9 0.79393697 <a title="71-lda-9" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>10 0.79341513 <a title="71-lda-10" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>11 0.79216051 <a title="71-lda-11" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>12 0.79061842 <a title="71-lda-12" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>13 0.7775417 <a title="71-lda-13" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>14 0.76734632 <a title="71-lda-14" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>15 0.76702297 <a title="71-lda-15" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>16 0.76202041 <a title="71-lda-16" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>17 0.76175326 <a title="71-lda-17" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>18 0.76097453 <a title="71-lda-18" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>19 0.75868493 <a title="71-lda-19" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>20 0.75768638 <a title="71-lda-20" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
