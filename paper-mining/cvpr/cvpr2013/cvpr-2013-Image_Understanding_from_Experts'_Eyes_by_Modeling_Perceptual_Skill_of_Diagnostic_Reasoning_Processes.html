<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-214" href="#">cvpr2013-214</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</h1>
<br/><p>Source: <a title="cvpr-2013-214-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Image_Understanding_from_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>Reference: <a title="cvpr-2013-214-reference" href="../cvpr2013_reference/cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. [sent-4, score-1.738]
</p><p>2 Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. [sent-5, score-0.976]
</p><p>3 edu  ,  ments of the experts ’ eye movement sequences. [sent-8, score-1.09]
</p><p>4 We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. [sent-9, score-1.273]
</p><p>5 Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. [sent-10, score-1.066]
</p><p>6 The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis. [sent-12, score-0.441]
</p><p>7 However, when the cues in images are not sufficient to generate a good interpretation automatically, active learning methods are necessary in terms of incorporating human perceptual capability into this process [23, 14, 1, 15, 8]. [sent-15, score-0.268]
</p><p>8 On the other hand, image understanding in knowledgerich domains is more challenging, since complex perceptual and conceptual processing are engaged to transform image pixels into meaningful contents [12]. [sent-16, score-0.251]
</p><p>9 The extracted behavior patterns are not only more robust and consistent but also shed light on latent cognitive processing. [sent-22, score-0.27]
</p><p>10 To address this problem, We propose to combine perceptual expertise as effortless yet valuable cognitive resources into image understanding. [sent-24, score-0.37]
</p><p>11 This requires the ability of extracting and representing experts’ perceptual expertise in a form that is ready to be applied in active learning schemes. [sent-25, score-0.345]
</p><p>12 Perceptual skill is considered to be the crucial cognitive factor accounting for the advantage of highly trained experts [12]. [sent-27, score-0.31]
</p><p>13 Experts generate distinctively different perceptual representations when they view the same medical images as novices [19]. [sent-28, score-0.308]
</p><p>14 Rather than passively ”photocopying” the visual information directly from sensors into minds, visual perception actively interprets the information by altering perceptual representations of the images based on experience and goals. [sent-29, score-0.247]
</p><p>15 Without guidance of perceptual skill, medical images cannot be interpreted effectively solely based on image visual features. [sent-30, score-0.334]
</p><p>16 This motivates us to investigate how to formalize perceptual skill and reason about image contents from experts’ points of view as shown in Figure 1. [sent-31, score-0.289]
</p><p>17 In our work we focus on medical im-  ages 1 where domain knowledge and perceptual expertise are in demand. [sent-32, score-0.385]
</p><p>18 We elicit and model physicians’ perceptual skill from their diagnostic reasoning process while inspecting medical images. [sent-33, score-0.496]
</p><p>19 Physicians examine and diagnose medical images, and their eye movements are recorded. [sent-34, score-0.668]
</p><p>20 In order to summarize the stereotypical and idiosyncratic eye movement patterns shared among these physicians, we develop a hierarchical dynamic model. [sent-35, score-1.402]
</p><p>21 It allows us to discover eye movement patterns exhibited by physicians’ time-evolving eye movement sequences, and each eye movement pattern essentially characterizes a particular statistical regularity of the temporal-spatial properties inferred from multiple eye movement sequences. [sent-36, score-4.059]
</p><p>22 In particular, we specify a subset of distinctive patterns corresponding to image visual-spatial structures at pathological level. [sent-37, score-0.276]
</p><p>23 Based on the exhibitions of these patterns by physicians viewing a particular image, we are able to put the image into a category associated with a particular lesion distribution. [sent-38, score-0.563]
</p><p>24 (b)-(d) Three subjects’ eye movement sequences super-imposed onto the image. [sent-47, score-0.949]
</p><p>25 To borrow human perceptual power, active learning was proposed and benefited a broad range of computer vision applications [23, 14, 1, 15, 8]. [sent-58, score-0.268]
</p><p>26 In particular, in specific domains experts perceptual expertise is considered to be more consistent and informative than their manual markings. [sent-64, score-0.45]
</p><p>27 What’s more, we combine multiple experts’ strength by summarizing their shared eye movement patterns and decode the patterns’ semantic meanings. [sent-70, score-1.258]
</p><p>28 We utilize this combinatorial stochastic process in the following specification based on our problem scenario, so that we can treat the number of shared eye movement patterns as a random number which is learned from the observed eye movement data. [sent-78, score-2.17]
</p><p>29 Let B0 denote a fixed continuous random base measure on a space Θ which represents a library of all the potential eye movements patterns. [sent-79, score-0.54]
</p><p>30 n L fertom {G B} following the beta process which represents a random measure on the eye  movement patterns shared among multiple subjects within the group j. [sent-82, score-1.417]
</p><p>31 Pij is a binary vector of Bernoulli random variables representing whether a particular eye movement pattern exhibited in the eye movement data of subject iwithin group j. [sent-84, score-2.11]
</p><p>32 both a set of countable number of eye movement patterns ? [sent-90, score-1.166]
</p><p>33 {θjk} drawn from the eye movement pattern library aΘtt earnnds t {hθeir corresponding probability masses {gjk} given group j. [sent-91, score-1.012]
</p><p>34 hTehier ccoormrebsipnaotniodnin gof p throebsaeb btwilitoy yv mariaassbeless { cghar}acterizes how the common eye movement patterns shared among subjects within expertise-specific group j. [sent-92, score-1.359]
</p><p>35 Thus Pij is a Bernoulli process realization from the random measure Gj where pijk as a binary random variable denotes whether subject iwithin group j exhibits eye movement pattern k given probability mass gjk. [sent-93, score-1.137]
</p><p>36 Kj patterns we readily define {(θjk , gjk)} as a set of common eye mrnosv weme erenat patterns esh {ar(θed among group j and {(θjk ,pijk)} as subject i’s personal subset of eye umpo jve anmden {t( patterns given group j. [sent-97, score-1.783]
</p><p>37 The transition distribution πij = {πz(tij)} of the Hidden Markov Model (HMM) at the bottom level governs the transitions between the ith subject’s personal subset of eye movement patterns θjk of group j. [sent-98, score-1.232]
</p><p>38 It is determined by the element-wise multiplication between the eye movement subset {pijk} of subject iin group j and the gammadmisetnrtib suutbedse tr a {npdom} v oafri saubblejesc {eijk}: eijk|γj ∼ Gamma(γj, 1)  πij  ∝  Eij  ? [sent-99, score-0.999]
</p><p>39 Dynamical likelihoods We apply one autoregressive HMM as the likelihood to describe the dynamics of each subject’s eye movement sequence. [sent-107, score-0.922]
</p><p>40 Let denote the observation unit of the eye movement sequence at time step t of the ith subject in the jth group. [sent-109, score-0.963]
</p><p>41 We have  y(tij)  Let x(tij)  x(tij) y(tij) = Axt(ij) y˜t(ij)  (4)  ∼ πx(ij)  + 222111 888977  et(x(tij))  (5)  Figure 3: Six out of eleven eye movement sequences super-imposed onto one dermatological image are illustrated here. [sent-112, score-1.136]
</p><p>42 Our model respectively decomposes the eleven eye movement sequences into nine eye movement patterns (color-coded) with the transition probability matrices, so that each sequence can be represented by a certain number out of nine patterns and their corresponding transition matrix. [sent-113, score-2.423]
</p><p>43 On the right, it is the shared eye movement pattern matrix of which each row corresponds to a subject’s eye movement sequence and each column indicates one shared eye movement pattern among multiple subjects. [sent-114, score-3.074]
</p><p>44 In this case, three patterns are recognized as Signature Patterns based on their self-transition probabilities, temporal-spatial  properties and diagnostic semantics. [sent-115, score-0.324]
</p><p>45 We thus definet −θk1 = (Ak , Σk) as one eye movement pattern. [sent-124, score-0.922]
</p><p>46 Although our model is capable of profiling eye movement patterns shared among multiple expertisespecific groups, we focus on expert group’s performance and its potential contribution to image understanding. [sent-127, score-1.282]
</p><p>47 A SMI (Senso-Motoric Instruments) eye tracking apparatus was applied to display the stimuli at a resolution of 1680x1050 pixels for the collection of eye movement data and recording of verbal descriptions. [sent-130, score-1.445]
</p><p>48 The eye tracker was running at 50 Hz sampling rate and has reported accuracy of 0. [sent-131, score-0.491]
</p><p>49 The experiment was conducted in an eye tracking laboratory with ambient light. [sent-134, score-0.491]
</p><p>50 The subjects were instructed not only to view the medical images and make a diagnosis, but also to describe what they see as well as their thought processes leading them to the diagnosis. [sent-139, score-0.254]
</p><p>51 Both eye movements and verbal descriptions were recorded for the viewing durations controlled by each subject. [sent-140, score-0.633]
</p><p>52 Image Analysis through Signature Patterns We generate 387 eye movement patterns based on eleven  subjects examining and diagnosing fifty dermatological images. [sent-143, score-1.516]
</p><p>53 Eye movement pattern estimation In Figure 3, we illustrate one set of observed eye movement sequences and estimating processes from our model of the eleven dermatologists diagnosing a case of a skin manifestation of endocarditis. [sent-147, score-1.793]
</p><p>54 In the medical image, there 222111889088  are multiple skin lesions spreading over the thumb nail and tip, the two parts of index finger and the middle finger. [sent-148, score-0.288]
</p><p>55 The eye movement sequences in Figure 2 indicate that dermatologists examine the image in a highly patterned manner by fixating on the primary abnormality heavily and switching their visual attention actively between and within the primary and secondary abnormalities. [sent-150, score-1.656]
</p><p>56 Our model decomposes each eye movement sequence into several subsets of its segments. [sent-151, score-0.922]
</p><p>57 The way that the patterns are shared among the subjects is also indicated by their matrix in Figure 3. [sent-153, score-0.401]
</p><p>58 For example the first subject’s eye movements evolve over time with the first eight out of nine patterns, and the eleventh subject has seven patterns  except pattern 5 and pattern 9. [sent-154, score-0.901]
</p><p>59 Transition probability matrices indicated these patterns are persistent with high selftransition probabilities. [sent-155, score-0.212]
</p><p>60 Although such analysis estimates varied image-specific patterns, we discover several basic yet distinctive types of patterns shared across multiple images called Signature Patterns. [sent-156, score-0.334]
</p><p>61 Signature pattern recognition We define a type of signature patterns by three criteria: first, its self-transition probability, which is indicated by the transition matrix, is no less than 0. [sent-159, score-0.455]
</p><p>62 65; second, it manifests clear diagnostic regions; third, the temporal-spatial properties of signature pattern exemplars within each type are similar but distinctive from other types, which is depicted in Figure 4. [sent-160, score-0.374]
</p><p>63 In the illustrated case in Figure 3, there are three instantiations of the signature patterns recognized. [sent-161, score-0.367]
</p><p>64 Pattern 2 and Pattern 5 is characterized by fixations switching back and forth between the primary and the different secondary abnormalities with long saccade amplitudes and relatively short fixation durations. [sent-162, score-0.62]
</p><p>65 These patterns suggest that subjects compare and associate the two types of abnormalities. [sent-163, score-0.289]
</p><p>66 Pattern 7 is characterized by a series of long-duration fixations only on the primary abnormality with extremely short saccades. [sent-164, score-0.344]
</p><p>67 This pattern suggest that subjects fixate on primary abnormality to make diagnosis. [sent-165, score-0.312]
</p><p>68 Based on the eye movement patterns generated from our model over fifty images, we are able to specify three types of signature patterns. [sent-166, score-1.321]
</p><p>69 5  Figure 4: Distinctive temporal-spatial properties of 217 eye movement units from 12 exemplars forms the three types of signature patterns. [sent-171, score-1.177]
</p><p>70 Each blue dot represents one eye movement unit from a signature pattern exemplar. [sent-172, score-1.131]
</p><p>71 Both eye movement units and their corresponding exemplars are projected from a four-dimension space (including x-y coordinate, fixation duration and saccade amplitude) onto this space. [sent-174, score-1.144]
</p><p>72 The signature patterns are characterized by a three-component Gaussian mixture. [sent-175, score-0.418]
</p><p>73 To quantify the temporal-spatial properties of the three types of signature patterns, we illustrate some of their exemplars in Figure 4. [sent-179, score-0.212]
</p><p>74 The estimation of the signature patterns based on their exemplar features can be solved using different classification techniques. [sent-180, score-0.367]
</p><p>75 We first adopt quadratic discrimination analysis (QDA) by assuming a simple parametric model for the densities of the temporal-spatial properties of the eye movement units and a training set includes 217 eye movement units of 12 exemplar patterns from 10 images. [sent-181, score-2.142]
</p><p>76 Perceptual category specification  Three additional experienced board-certified dermatologists as our consultants suggests four broad perceptual categories in terms of lesion distribution and configuration. [sent-186, score-0.539]
</p><p>77 We further determine the associations between the combinations of the exhibitions of these three types of signature 222111889199  Figure 5: ROC  curves  summarizing categorization performance for the four perceptual categories. [sent-187, score-0.486]
</p><p>78 patterns and the four specified categories: •  •  •  If the set of eye movement patterns exhibited on an image solely fin ecyleud meosv Concentrating e Pxahttiebrintse,d t ohen image is categorized as Solitary which means that the image contains a solitary lesion as primary abnormality. [sent-190, score-1.784]
</p><p>79 If the set of eye movement patterns exhibited on an image solely i enycelu mdeosv Switching t Pearnttser enxsh, itbhiete image ains categorized as Symmetry which means that the lesions in the image are symmetrically distributed. [sent-191, score-1.461]
</p><p>80 •  If the set of eye movement patterns exhibited on an image einc selut odefs e Cyelu mttoevre e Pmaettnetr pnsa,t ttehren image i tse categorized as High-Density Lesions which means that the image contains multiple lesions distributed in either scattered or clustered manner. [sent-193, score-1.471]
</p><p>81 According to the signature patterns recognized on the images, we can put them into the four categories as shown in Figure 6a, 6b, 6c, and 6d. [sent-194, score-0.405]
</p><p>82 Results and Discussion To measure the performance of our image categorization approach, we conduct an experiment following the same procedure by recruiting another ten dermatologists and using a different set of forty dermatological images as stimuli. [sent-196, score-0.392]
</p><p>83 Our three consulting dermatologists achieve consensus to categorize the forty images into the four perceptual categories. [sent-198, score-0.463]
</p><p>84 We use 232 estimated eye movement patterns on these images and the ones from the previous experiment as a testing set. [sent-199, score-1.134]
</p><p>85 We assume each eye movement sequence exhibits the same set of patterns in order to implement the canonical HMMs. [sent-201, score-1.134]
</p><p>86 The difference between Multiple Morphologies images and Symmetry images is that the eye movement patterns exhibited on the latter do not contain Concentrating Pattern. [sent-205, score-1.239]
</p><p>87 This is because the symmetrical visual-spatial structures imply that lesions are equivalent important without single primary one for the subjects to concentrate their focus on as shown in Figure 6b. [sent-206, score-0.323]
</p><p>88 Since the specifications of signature patterns are heuristic, we may be able to improve the categorization performance by identifying extra meaningful and distinctive eye movement patterns, and these extra patterns may also lead to image categorization at a finer detailed level. [sent-207, score-1.607]
</p><p>89 Since the dermatological images are collected for future diagnosis, and training purposes, the dermatologists took them in a particular way. [sent-208, score-0.326]
</p><p>90 They tend to center primary abnormalities and preserve as much related contextual information as possible, such as patients’ demographic information, body parts, lesion size and so on. [sent-209, score-0.286]
</p><p>91 For instance, the solitary lesions  have large scales in some images, this leads to cluttered eye movement patterns rather than concentrating ones as shown in Figure 6d. [sent-213, score-1.426]
</p><p>92 ) as in Figure 6b and distributions of multiple abnormalities (primary abnormality versus secondary abnormality, e. [sent-217, score-0.212]
</p><p>93 We obtain certain aspects of experts’ domain-specific knowledge by summarizing their perceptual skills from their eye movements while diagnosing images. [sent-220, score-0.832]
</p><p>94 This will benefit the traditional pixel-based statistical methods for image understanding by evaluating perceptual significance and relations of the image features which spatially correspond to the eye movement patterns. [sent-222, score-1.199]
</p><p>95 This combination of expert knowledge and image features allows us to generalize our  approach to images on which there is no experts’ eye movements recorded. [sent-223, score-0.54]
</p><p>96 The different viewing times of dermatologists yield length-varying eye movement sequences. [sent-224, score-1.182]
</p><p>97 Since each sequence is modeled with one HMM separately, the emission distributions of which group multiple fixation-saccadic units into one pattern exhibited repeatedly. [sent-225, score-0.28]
</p><p>98 Conclusions This paper presents a hierarchical probabilistic dynamic framework to summarize eye movement patterns shared among dermatologists while they are examining medical images. [sent-228, score-1.637]
</p><p>99 This novel approach allows us to elicit perceptual skill as additional human capabilities to achieve image understanding at the pathological level. [sent-229, score-0.395]
</p><p>100 We also demonstrate one instantiation of the signature patterns recognized from the set of subjects’ eye movement patterns which is estimated by our model. [sent-413, score-1.539]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eye', 0.491), ('movement', 0.431), ('patterns', 0.212), ('perceptual', 0.205), ('dermatologists', 0.199), ('experts', 0.168), ('signature', 0.155), ('lesions', 0.149), ('dermatological', 0.127), ('physicians', 0.127), ('fixations', 0.112), ('lesion', 0.109), ('exhibited', 0.105), ('medical', 0.103), ('primary', 0.097), ('tij', 0.093), ('concentrating', 0.089), ('gj', 0.088), ('shared', 0.088), ('skill', 0.084), ('abnormality', 0.084), ('switching', 0.083), ('abnormalities', 0.08), ('subjects', 0.077), ('expertise', 0.077), ('saccade', 0.074), ('diagnostic', 0.074), ('pij', 0.071), ('active', 0.063), ('viewing', 0.061), ('eleven', 0.06), ('beta', 0.058), ('cognitive', 0.058), ('exemplars', 0.057), ('morphologies', 0.056), ('exhibitions', 0.054), ('gjk', 0.054), ('idiosyncratic', 0.054), ('pijk', 0.054), ('solitary', 0.054), ('pattern', 0.054), ('jk', 0.053), ('diagnosing', 0.051), ('characterized', 0.051), ('movements', 0.049), ('processes', 0.049), ('saccades', 0.048), ('stereotypical', 0.048), ('secondary', 0.048), ('fixation', 0.048), ('categorized', 0.047), ('understanding', 0.046), ('diagnosis', 0.045), ('ij', 0.044), ('units', 0.043), ('actively', 0.042), ('emission', 0.042), ('hmm', 0.041), ('subject', 0.041), ('recognized', 0.038), ('rit', 0.037), ('expertisespecific', 0.036), ('odefs', 0.036), ('permission', 0.036), ('thumb', 0.036), ('categorization', 0.036), ('group', 0.036), ('summarizing', 0.036), ('examining', 0.035), ('transition', 0.034), ('distinctive', 0.034), ('bernoulli', 0.033), ('anne', 0.032), ('countable', 0.032), ('fixating', 0.032), ('qda', 0.032), ('eijk', 0.032), ('fifty', 0.032), ('rochester', 0.032), ('thibaux', 0.032), ('verbal', 0.032), ('behaviors', 0.03), ('elicit', 0.03), ('iwithin', 0.03), ('effortless', 0.03), ('forty', 0.03), ('pathological', 0.03), ('categorize', 0.029), ('personal', 0.028), ('ineffective', 0.028), ('dynamic', 0.028), ('amplitudes', 0.027), ('sequences', 0.027), ('solely', 0.026), ('hierarchical', 0.026), ('specification', 0.026), ('significance', 0.026), ('examine', 0.025), ('instructed', 0.025), ('among', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="214-tfidf-1" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>2 0.18038489 <a title="214-tfidf-2" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>3 0.11323808 <a title="214-tfidf-3" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>Author: Qiang Zhang, Baoxin Li</p><p>Abstract: This paper is concerned with a novel problem: learning temporal models using only relative information. Such a problem arises naturally in many applications involving motion or video data. Our focus in this paper is on videobased surgical training, in which a key task is to rate the performance of a trainee based on a video capturing his motion. Compared with the conventional method of relying on ratings from senior surgeons, an automatic approach to this problem is desirable for its potential lower cost, better objectiveness, and real-time availability. To this end, we propose a novel formulation termed Relative Hidden Markov Model and develop an algorithm for obtaining a solution under this model. The proposed method utilizes only a relative ranking (based on an attribute of interest) between pairs of the inputs, which is easier to obtain and often more consistent, especially for the chosen application domain. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration is linked to the likelihood of the inputs under the learned model. Hence the model can be used to compare new sequences. Synthetic data is first used to systematically evaluate the model and the algorithm, and then we experiment with real data from a surgical training system. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video.</p><p>4 0.10669147 <a title="214-tfidf-4" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>5 0.098880634 <a title="214-tfidf-5" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>Author: Dmitry Rudoy, Dan B. Goldman, Eli Shechtman, Lihi Zelnik-Manor</p><p>Abstract: During recent years remarkable progress has been made in visual saliency modeling. Our interest is in video saliency. Since videos are fundamentally different from still images, they are viewed differently by human observers. For example, the time each video frame is observed is a fraction of a second, while a still image can be viewed leisurely. Therefore, video saliency estimation methods should differ substantially from image saliency methods. In this paper we propose a novel methodfor video saliency estimation, which is inspired by the way people watch videos. We explicitly model the continuity of the video by predicting the saliency map of a given frame, conditioned on the map from the previousframe. Furthermore, accuracy and computation speed are improved by restricting the salient locations to a carefully selected candidate set. We validate our method using two gaze-tracked video datasets and show we outperform the state-of-the-art.</p><p>6 0.096408159 <a title="214-tfidf-6" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>7 0.090099886 <a title="214-tfidf-7" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>8 0.084799744 <a title="214-tfidf-8" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>9 0.079998642 <a title="214-tfidf-9" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>10 0.073175102 <a title="214-tfidf-10" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>11 0.072754994 <a title="214-tfidf-11" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>12 0.070933148 <a title="214-tfidf-12" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>13 0.065354921 <a title="214-tfidf-13" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>14 0.059559509 <a title="214-tfidf-14" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>15 0.059376031 <a title="214-tfidf-15" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>16 0.057865091 <a title="214-tfidf-16" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>17 0.054518718 <a title="214-tfidf-17" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>18 0.054397333 <a title="214-tfidf-18" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>19 0.053943239 <a title="214-tfidf-19" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>20 0.053919088 <a title="214-tfidf-20" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, -0.022), (2, 0.025), (3, -0.008), (4, -0.014), (5, -0.01), (6, -0.023), (7, -0.006), (8, 0.028), (9, 0.019), (10, 0.037), (11, -0.015), (12, -0.009), (13, -0.023), (14, 0.007), (15, 0.027), (16, 0.023), (17, 0.063), (18, 0.013), (19, 0.013), (20, -0.028), (21, -0.006), (22, 0.02), (23, -0.025), (24, 0.008), (25, 0.029), (26, 0.088), (27, -0.02), (28, -0.021), (29, 0.019), (30, -0.03), (31, -0.003), (32, -0.033), (33, 0.031), (34, 0.009), (35, -0.031), (36, 0.011), (37, 0.109), (38, -0.132), (39, 0.013), (40, -0.077), (41, 0.023), (42, -0.066), (43, 0.123), (44, -0.031), (45, 0.057), (46, -0.046), (47, -0.119), (48, -0.108), (49, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95131153 <a title="214-lsi-1" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>2 0.6219424 <a title="214-lsi-2" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>3 0.56749922 <a title="214-lsi-3" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>4 0.55684328 <a title="214-lsi-4" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>Author: Guha Balakrishnan, Fredo Durand, John Guttag</p><p>Abstract: We extract heart rate and beat lengths from videos by measuring subtle head motion caused by the Newtonian reaction to the influx of blood at each beat. Our method tracks features on the head and performs principal component analysis (PCA) to decompose their trajectories into a set of component motions. It then chooses the component that best corresponds to heartbeats based on its temporal frequency spectrum. Finally, we analyze the motion projected to this component and identify peaks of the trajectories, which correspond to heartbeats. When evaluated on 18 subjects, our approach reported heart rates nearly identical to an electrocardiogram device. Additionally we were able to capture clinically relevant information about heart rate variability.</p><p>5 0.52519661 <a title="214-lsi-5" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>6 0.51270747 <a title="214-lsi-6" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>7 0.49631467 <a title="214-lsi-7" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>8 0.48162055 <a title="214-lsi-8" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>9 0.47994125 <a title="214-lsi-9" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>10 0.462672 <a title="214-lsi-10" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>11 0.46146253 <a title="214-lsi-11" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>12 0.45601586 <a title="214-lsi-12" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>13 0.4501532 <a title="214-lsi-13" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>14 0.44992408 <a title="214-lsi-14" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>15 0.44226855 <a title="214-lsi-15" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>16 0.43533295 <a title="214-lsi-16" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>17 0.4344252 <a title="214-lsi-17" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>18 0.43113661 <a title="214-lsi-18" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>19 0.40612668 <a title="214-lsi-19" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>20 0.40348193 <a title="214-lsi-20" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.079), (16, 0.04), (21, 0.272), (26, 0.068), (28, 0.011), (33, 0.221), (65, 0.018), (67, 0.052), (69, 0.055), (77, 0.014), (87, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80659628 <a title="214-lda-1" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>same-paper 2 0.79204154 <a title="214-lda-2" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>3 0.74201465 <a title="214-lda-3" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>Author: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar</p><p>Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In thispaper wepresent an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.</p><p>4 0.73573422 <a title="214-lda-4" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>5 0.7301634 <a title="214-lda-5" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>6 0.69536293 <a title="214-lda-6" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>7 0.68522376 <a title="214-lda-7" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>8 0.68336612 <a title="214-lda-8" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>9 0.68167818 <a title="214-lda-9" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>10 0.68091136 <a title="214-lda-10" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>11 0.68071103 <a title="214-lda-11" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>12 0.68007058 <a title="214-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.68005759 <a title="214-lda-13" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>14 0.67976642 <a title="214-lda-14" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>15 0.67964697 <a title="214-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.67949289 <a title="214-lda-16" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>17 0.67885238 <a title="214-lda-17" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>18 0.67870849 <a title="214-lda-18" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>19 0.67841744 <a title="214-lda-19" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>20 0.67834628 <a title="214-lda-20" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
