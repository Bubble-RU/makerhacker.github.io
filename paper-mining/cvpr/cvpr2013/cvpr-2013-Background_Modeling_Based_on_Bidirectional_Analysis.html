<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-55" href="#">cvpr2013-55</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</h1>
<br/><p>Source: <a title="cvpr-2013-55-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shimada_Background_Modeling_Based_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi</p><p>Abstract: Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. In this paper, we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i.e., analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling.</p><p>Reference: <a title="cvpr-2013-55-reference" href="../cvpr2013_reference/cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Most traditional studies use information observed in past frames to create and update a background model. [sent-2, score-0.518]
</p><p>2 To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. [sent-3, score-0.428]
</p><p>3 Our proposed approach realizes a low-cost and highly accurate background model. [sent-5, score-0.376]
</p><p>4 The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i. [sent-6, score-1.353]
</p><p>5 Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. [sent-9, score-0.538]
</p><p>6 Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling. [sent-10, score-0.534]
</p><p>7 Introduction Background modeling and subtraction is an essential task in video surveillance applications, as it provides foreground segmentation with no prior information about the foreground. [sent-12, score-0.499]
</p><p>8 Pixel-level background modeling is a typical approach in which a Gaussian mixture model (GMM) or kernel density estimation is often used to represent the frequency of pixel values in an observed image sequence[14, 4]. [sent-13, score-0.676]
</p><p>9 Other effective solutions to enhance the performance of background subtraction are the use of temporal information[13, 18] and hybrid modeling[16, 15]. [sent-17, score-0.635]
</p><p>10 Generally, future information is not often used in time-series analysis that requires real-time processing since there is a delay in the availability of a result. [sent-27, score-0.383]
</p><p>11 Our approach defines an acceptable delay as 33 milliseconds (the duration of just one video frame). [sent-31, score-0.383]
</p><p>12 The background model is improved in terms of its ability to han-  dle background changes and accurately subtract the background compared with a typical approach that does not use future information. [sent-32, score-1.281]
</p><p>13 Moreover, our approach obtains the background model using the same amount of memory as used in the typical approach even though it uses additional information obtained from future image frames. [sent-33, score-0.571]
</p><p>14 Acceptable Delay The background model is allowed to output a result N frames after the current frame. [sent-38, score-0.412]
</p><p>15 In 111999777977  other words, information observed in the period extending to N frames after the current frame is used to determine the background subtraction. [sent-39, score-0.641]
</p><p>16 The proposed method improves the accuracy of background subtraction at the expense of a delay in the output. [sent-40, score-0.932]
</p><p>17 However, the proposed method requires a delay of just one frame, which can be ignored in most visual surveillance applications. [sent-41, score-0.392]
</p><p>18 In contrast, the proposed method includes backward analysis using N future frames, in addition  to forward analysis. [sent-45, score-0.748]
</p><p>19 The backward analysis is performed from the future to present. [sent-46, score-0.594]
</p><p>20 Figure 2 shows a typical example of the advantage realized by including backward analysis. [sent-47, score-0.545]
</p><p>21 These changes are the same from the viewpoint of the change in the pixel value, and background modeling based on forward analysis cannot distinguish the reason for the changes at the time of the current frame. [sent-50, score-0.93]
</p><p>22 In contrast, backward analysis is able to investigate the change using the pixel values observed in the future period (the right side of Figure 2). [sent-51, score-0.937]
</p><p>23 If the change is due to a moving object, both forward and backward analyses will observe a change in the pixel value. [sent-52, score-0.943]
</p><p>24 The proposed bidirectional backgroundmodel uses pixel values observed in the future period to improve the accuracy of background subtraction at the expense of some delay in the output. [sent-55, score-1.498]
</p><p>25 We are able to acquire a result for background subtraction with a reasonable delay. [sent-57, score-0.663]
</p><p>26 In tthhea case osfe nfotsrw anar odb analysis, a background model M}. [sent-67, score-0.399]
</p><p>27 t− In1 is estimated from this sequence, and whether an observed pixel value Xt is part of the background is determined by P(Xt |Mt−1). [sent-68, score-0.562]
</p><p>28 (In fact, a GMM-based background model is ofte|nM used for the calculation of background probability. [sent-69, score-0.752]
</p><p>29 ) Meanwhile, backward analysis provides a background model Mt+1 using {Xt+N, . [sent-70, score-0.909]
</p><p>30 In the remainder of this paper, we refer to the background model Mt+1 as the “backward background model”. [sent-76, score-0.752]
</p><p>31 The proposed bidirectional background modeling can be said to calculate the background probability of Xt as P(Xt|Mt−1, Mt+1), where Mt−1 and Mt+1 are acquired by for|wMard analysis and backward analysis respectively. [sent-77, score-1.565]
</p><p>32 j Nstsote th teh caot nifwe set α to zero, the model is a typical background model based on forward analysis alone. [sent-79, score-0.618]
</p><p>33 Backward Background Model Ideally, the acceptable delay N should be a large value to acquire a good backward background model. [sent-82, score-1.236]
</p><p>34 To solve this trade-off, a new concept of “piecewise time-reversal symmetry” is introduced, where we can set N to a small value yet realize a reasonable backward background model. [sent-84, score-0.916]
</p><p>35 Piecewise time-reversal symmetry is an assumption that background change has a symmetric property in a short pe-  riod if the order of observation from past to present is inversed to observation from future to present. [sent-85, score-0.751]
</p><p>36 For instance, phenomena of “a pixel getting darker” and “a pixel getting brighter” are symmetric. [sent-86, score-0.372]
</p><p>37 A phenomenon of “a pixel getting brighter then getting darker repeatedly” also has a symmetric property if we consider a short time period of the repetition. [sent-87, score-0.525]
</p><p>38 If we inverse the piecewise change within period B, the inversed sequence appears similar to the sequence 111999778088  Present  Frame  Figure 4. [sent-89, score-0.46]
</p><p>39 If we can assume this kind of time-reversal symmetry, an observed sequence of pixel values in the past period might include a time-reversal pattern that will be observed in the future period. [sent-93, score-0.481]
</p><p>40 A background model created for the piecewise past period could then be used on behalf of a background model that is estimated by pixel values in the future period. [sent-94, score-1.253]
</p><p>41 In other words, we do not have to explicitly create a backward background model since we can substitute an “inversed forward” background model for a backward background model. [sent-95, score-2.088]
</p><p>42 Implementation This section explains the detailed implementation strategy to apply the proposed bidirectional background modeling using a GMM-based statistical background model. [sent-102, score-1.008]
</p><p>43 Step 1: Background model retrieval A background model Mq or Mr that satisfies a search query q for forward analysis or r for backward analysis is retrieved from the background database (which is described in detail in section 3. [sent-110, score-1.651]
</p><p>44 Each query q or r is constructed from a pixel feature in the past period or future period respectively. [sent-112, score-0.557]
</p><p>45 Step 2: Background subtraction If a background model is retrieved in Step 1 (i. [sent-115, score-0.71]
</p><p>46 According to GMMbPa(sXed| M bac)kg anrodu/nodr Pm(oXde|Mling, if the pixel value X is within a predefined standard deviation s of the distribution, the background label is tentatively given to the pixel. [sent-118, score-0.596]
</p><p>47 Step 3: Adding a new example and exception processing If a background model is not retrieved, the process is a little different between the foreground analysis and 111999778199  backward analysis. [sent-120, score-0.977]
</p><p>48 In the case of forward analysis in Step 1, a new GMM-based background model is added to the database with initial mean value X and predefined variance and weight. [sent-121, score-0.639]
</p><p>49 In this case, the foreground label is tentatively given to the pixel since there is no example that guarantees the pixel to be background. [sent-122, score-0.373]
</p><p>50 In the case of backward analysis, we only give a tentative label of foreground to the pixel, and do not add any background model to the database. [sent-123, score-1.034]
</p><p>51 Step 5: Update of background models The parameters of the background models are updated. [sent-127, score-0.752]
</p><p>52 Note that when a background model is used by more than one pixel, one of the pixels is randomly selected for the update. [sent-131, score-0.4]
</p><p>53 “Case-based background model retrieval” is a framework with which to realize “case-by-case model sharing”. [sent-135, score-0.405]
</p><p>54 Unlike the clustering-based approach or traditional pixelbased approaches, the same background model is not continuously used for an individual pixel. [sent-136, score-0.376]
</p><p>55 , the location of the pixel or the trend of the value), an appropriate background model is selected from the database for an individual pixel frame by frame, meaning that a given background model is not always selected for the same pixel. [sent-139, score-1.072]
</p><p>56 Moreover, a background model is sometimes shared by several pixels. [sent-140, score-0.376]
</p><p>57 The important point is that we do not create separate background databases for forward analysis and backward analysis. [sent-141, score-1.063]
</p><p>58 Forward and backward analyses share the same background database through the use of piecewise timereversal symmetry. [sent-142, score-1.134]
</p><p>59 In practice, the query to retrieve a background model is set as follows. [sent-143, score-0.435]
</p><p>60 In forward analysis, a background model Mq is retrieved where a similar pixel change was observed around (u, v) in the past period. [sent-145, score-0.888]
</p><p>61 On the other hand, the query of the backward analysis changes the time ordering of pixel values. [sent-148, score-0.765]
</p><p>62 Therefore, a background model that corresponds to piecewise time-reversal symmetric change will be retrieved as Mr from the database. [sent-150, score-0.679]
</p><p>63 Foreground/Background Label Assignment Each pixel has two tentative labels from the forward analysis and backward analysis. [sent-155, score-0.883]
</p><p>64 Preparation The evaluation items in our experiments are the accuracy of background subtraction, memory cost and computational time. [sent-185, score-0.499]
</p><p>65 The artificial datasets (see Figure 6(b)) separately include the following background changes. [sent-194, score-0.425]
</p><p>66 Bootstrap If initialization data free from foreground objects are not available, the background model is initialized using a bootstrapping strategy. [sent-201, score-0.52]
</p><p>67 Darkening It is desirable that the background model adapts to gradual changes in the appearance of the environment. [sent-202, score-0.433]
</p><p>68 Light Switch Sudden one-off changes are not covered by the background model. [sent-204, score-0.433]
</p><p>69 They occur, for example, with a sudden switch of light, and they strongly affect the appearance of the background and result in false positive detections. [sent-205, score-0.521]
</p><p>70 Background subtraction approaches for video surveillance have to cope with such degraded signals affected by different types of noise, such as sensor noise and compression artifacts. [sent-207, score-0.375]
</p><p>71 Background subtraction accuracy for outdoor scenes  With regards to parameter settings, we set the contribution parameter α to 0. [sent-213, score-0.378]
</p><p>72 , the parameter α = 0) and the proposed method, were compared in terms of background subtraction accuracy. [sent-222, score-0.635]
</p><p>73 The result of the GMM-based method[14] is a typ-  ical baseline with much lower precision and higher recall because of the method’s low flexibility to background changes. [sent-225, score-0.47]
</p><p>74 The case-based method (which did not employ backward analysis) provides better results than the GMMbased method. [sent-226, score-0.48]
</p><p>75 Therefore, the backward analysis hypothesis contributed to gain the accuracy. [sent-234, score-0.579]
</p><p>76 The ratios of the proposed method are almost the same as those of the case-based method because the same background database was used even though the proposed method employed analyses in two directions. [sent-253, score-0.472]
</p><p>77 The backward background model was completely estimated using all the frames from the end of the image sequence to the initial frame. [sent-258, score-0.924]
</p><p>78 The result was almost the same with the full backward analysis. [sent-271, score-0.48]
</p><p>79 We suppose that the background model update is strongly affected by the successive frames from the current frame even using all of the future sequences. [sent-272, score-0.536]
</p><p>80 Considering each scene in turn, all methods achieved high scores for the scenes “Basic” and “Dynamic Background” since these scenes did not include severe background changes. [sent-284, score-0.581]
</p><p>81 The reason for this is that the scene included the background getting darker only. [sent-287, score-0.54]
</p><p>82 The inverse change of the background getting brighter was not included, and therefore, the assumption ofpiecewise time-reversal symmetry did not work well. [sent-288, score-0.63]
</p><p>83 This is a limitation of the proposed method; however, considering the practical use, background subtraction is usually applied to scenes that not only become darker but also become brighter. [sent-289, score-0.797]
</p><p>84 Indeed, the proposed method performed well for the real scene (Scene 1, captured outdoors), which included the background becoming darker. [sent-290, score-0.417]
</p><p>85 Meanwhile, the case-based sharing strategy used in the proposed method can tackle such an initialization problem by creating a new background model immediately. [sent-294, score-0.438]
</p><p>86 In the cases of “Light Switch” and “Noisy Night”, the proposed backward analysis contributed considerably to an improvement in accuracy. [sent-295, score-0.579]
</p><p>87 Firstly, the proposed method achieved better results than most other methods including state-of-the-art methods in terms of the accuracy of background subtraction in various scenes. [sent-302, score-0.635]
</p><p>88 One is the case-bycase model sharing strategy, which allows pixels to share  a background model according to the pixel property. [sent-306, score-0.549]
</p><p>89 The other is the idea that piecewise time-reversal symmetry allows forward analysis and backward analysis to share the same background database. [sent-307, score-1.317]
</p><p>90 The 33-millisecond delay can be ignored in most visual surveillance applications, but it improves the accuracy of background subtraction remarkably. [sent-311, score-1.027]
</p><p>91 Conclusion This paper discussed background modeling based on bidirectional analysis. [sent-313, score-0.603]
</p><p>92 The introduction of backward analysis and its combined use with forward analysis provide a good solution to improve background subtraction accuracy. [sent-314, score-1.375]
</p><p>93 The proposed method still has some limitation that the backward analysis does not always work well in some scenes where the pixel values constantly increase/decrease,  where the occlusion lasts for a long time, including the situations of a human/car stops, near-field object detection. [sent-319, score-0.731]
</p><p>94 In future work, further scenes need to be used in evaluating the proposed method, and application of the bidirectional background modeling framework to other background models will be studied. [sent-321, score-1.122]
</p><p>95 We believe that the casebased background modeling framework has great potential. [sent-322, score-0.432]
</p><p>96 Vibe: A powerful random technique to estimate the background in video sequences. [sent-327, score-0.407]
</p><p>97 A self-organizing approach to background subtraction for visual surveillance applications. [sent-369, score-0.72]
</p><p>98 Towards robust object detection: integrated background modeling based on spatio-temporal features. [sent-443, score-0.432]
</p><p>99 Dynamic background modeling and subtraction using spatio-temporal local binary patterns. [sent-463, score-0.691]
</p><p>100 Efficient adaptive density estimation per image pixel for the task of background subtraction. [sent-468, score-0.516]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('backward', 0.48), ('background', 0.376), ('delay', 0.269), ('subtraction', 0.259), ('xt', 0.204), ('bidirectional', 0.171), ('sabs', 0.156), ('forward', 0.154), ('mt', 0.137), ('piecewise', 0.13), ('period', 0.127), ('pixel', 0.116), ('mq', 0.11), ('shimada', 0.107), ('memory', 0.099), ('switch', 0.093), ('surveillance', 0.085), ('scenes', 0.082), ('tentative', 0.08), ('inversed', 0.078), ('bootstrapping', 0.076), ('retrieved', 0.075), ('symmetry', 0.071), ('analyses', 0.071), ('getting', 0.07), ('darkening', 0.069), ('foreground', 0.068), ('past', 0.067), ('night', 0.065), ('mr', 0.064), ('frame', 0.063), ('recall', 0.061), ('future', 0.061), ('change', 0.061), ('query', 0.059), ('changes', 0.057), ('modeling', 0.056), ('light', 0.055), ('analysis', 0.053), ('darker', 0.053), ('gmm', 0.052), ('backgroundmodel', 0.052), ('ichiro', 0.052), ('timereversal', 0.052), ('acceptable', 0.052), ('brighter', 0.052), ('sudden', 0.052), ('artificial', 0.049), ('contributed', 0.046), ('taniguchi', 0.046), ('tpt', 0.046), ('meanwhile', 0.044), ('nagahara', 0.043), ('tentatively', 0.043), ('tk', 0.042), ('scene', 0.041), ('observed', 0.039), ('conference', 0.039), ('ignored', 0.038), ('symmetric', 0.037), ('outdoor', 0.037), ('frames', 0.036), ('maximal', 0.035), ('typical', 0.035), ('illumination', 0.034), ('precision', 0.033), ('harwood', 0.033), ('waving', 0.033), ('sharing', 0.033), ('regard', 0.033), ('sequence', 0.032), ('value', 0.031), ('milliseconds', 0.031), ('video', 0.031), ('realized', 0.03), ('mixture', 0.03), ('label', 0.03), ('strategy', 0.029), ('regarded', 0.029), ('realize', 0.029), ('acquire', 0.028), ('expense', 0.028), ('practical', 0.027), ('signal', 0.026), ('indoor', 0.026), ('international', 0.025), ('database', 0.025), ('dynamic', 0.025), ('density', 0.024), ('cost', 0.024), ('pixels', 0.024), ('gives', 0.024), ('usage', 0.023), ('eofr', 0.023), ('rids', 0.023), ('tanaka', 0.023), ('tthhea', 0.023), ('aba', 0.023), ('atsushi', 0.023), ('brumitt', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="55-tfidf-1" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>Author: Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi</p><p>Abstract: Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. In this paper, we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i.e., analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling.</p><p>2 0.18255933 <a title="55-tfidf-2" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>Author: Xiaobo Ren, Tony X. Han, Zhihai He</p><p>Abstract: We consider video object cut as an ensemble of framelevel background-foreground object classifiers which fuses information across frames and refine their segmentation results in a collaborative and iterative manner. Our approach addresses the challenging issues of modeling of background with dynamic textures and segmentation of foreground objects from cluttered scenes. We construct patch-level bagof-words background models to effectively capture the background motion and texture dynamics. We propose a foreground salience graph (FSG) to characterize the similarity of an image patch to the bag-of-words background models in the temporal domain and to neighboring image patches in the spatial domain. We incorporate this similarity information into a graph-cut energy minimization framework for foreground object segmentation. The background-foreground classification results at neighboring frames are fused together to construct a foreground probability map to update the graph weights. The resulting object shapes at neighboring frames are also used as constraints to guide the energy minimization process during graph cut. Our extensive experimental results and performance comparisons over a diverse set of challenging videos with dynamic scenes, including the new Change Detection Challenge Dataset, demonstrate that the proposed ensemble video object cut method outperforms various state-ofthe-art algorithms.</p><p>3 0.16112451 <a title="55-tfidf-3" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>Author: Luc Oth, Paul Furgale, Laurent Kneip, Roland Siegwart</p><p>Abstract: Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devices—from smart-phones to high-end cameras. It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.</p><p>4 0.10408522 <a title="55-tfidf-4" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><p>5 0.10060527 <a title="55-tfidf-5" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>Author: Adarsh Kowdle, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: In this work, we consider images of a scene with a moving object captured by a static camera. As the object (human or otherwise) moves about the scene, it reveals pairwise depth-ordering or occlusion cues. The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. We cast the problem of depth-layer segmentation as a discrete labeling problem on a spatiotemporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently, and using a na¨ ıve combination of the features.</p><p>6 0.10057009 <a title="55-tfidf-6" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>7 0.09940064 <a title="55-tfidf-7" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>8 0.093535036 <a title="55-tfidf-8" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>9 0.088125169 <a title="55-tfidf-9" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>10 0.08126197 <a title="55-tfidf-10" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>11 0.073834509 <a title="55-tfidf-11" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>12 0.072822444 <a title="55-tfidf-12" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>13 0.072372869 <a title="55-tfidf-13" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>14 0.069996096 <a title="55-tfidf-14" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>15 0.068459645 <a title="55-tfidf-15" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>16 0.068382636 <a title="55-tfidf-16" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>17 0.067096986 <a title="55-tfidf-17" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>18 0.066806026 <a title="55-tfidf-18" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>19 0.065497428 <a title="55-tfidf-19" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>20 0.06423831 <a title="55-tfidf-20" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, 0.034), (2, 0.02), (3, -0.011), (4, 0.005), (5, -0.029), (6, 0.022), (7, -0.036), (8, 0.001), (9, 0.045), (10, 0.01), (11, -0.022), (12, 0.037), (13, 0.018), (14, 0.003), (15, -0.016), (16, 0.01), (17, -0.006), (18, 0.033), (19, -0.031), (20, 0.059), (21, 0.087), (22, -0.05), (23, -0.093), (24, -0.015), (25, -0.078), (26, 0.062), (27, 0.105), (28, -0.049), (29, 0.04), (30, 0.03), (31, 0.032), (32, -0.064), (33, 0.01), (34, -0.068), (35, -0.043), (36, 0.022), (37, -0.038), (38, -0.103), (39, -0.036), (40, -0.021), (41, 0.035), (42, -0.019), (43, -0.002), (44, -0.141), (45, -0.014), (46, -0.019), (47, 0.002), (48, -0.021), (49, 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96514875 <a title="55-lsi-1" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>Author: Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi</p><p>Abstract: Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. In this paper, we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i.e., analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling.</p><p>2 0.70516771 <a title="55-lsi-2" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>Author: Róisín Rowley-Brooke, François Pitié, Anil Kokaram</p><p>Abstract: This paper presents recent work on a new framework for non-blind document bleed-through removal. The framework includes image preprocessing to remove local intensity variations, pixel region classification based on a segmentation of the joint recto-verso intensity histogram and connected component analysis on the subsequent image labelling. Finally restoration of the degraded regions is performed using exemplar-based image inpainting. The proposed method is evaluated visually and numerically on a freely available database of 25 scanned manuscript image pairs with ground truth, and is shown to outperform recent non-blind bleed-through removal techniques.</p><p>3 0.68594295 <a title="55-lsi-3" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>Author: Xiaobo Ren, Tony X. Han, Zhihai He</p><p>Abstract: We consider video object cut as an ensemble of framelevel background-foreground object classifiers which fuses information across frames and refine their segmentation results in a collaborative and iterative manner. Our approach addresses the challenging issues of modeling of background with dynamic textures and segmentation of foreground objects from cluttered scenes. We construct patch-level bagof-words background models to effectively capture the background motion and texture dynamics. We propose a foreground salience graph (FSG) to characterize the similarity of an image patch to the bag-of-words background models in the temporal domain and to neighboring image patches in the spatial domain. We incorporate this similarity information into a graph-cut energy minimization framework for foreground object segmentation. The background-foreground classification results at neighboring frames are fused together to construct a foreground probability map to update the graph weights. The resulting object shapes at neighboring frames are also used as constraints to guide the energy minimization process during graph cut. Our extensive experimental results and performance comparisons over a diverse set of challenging videos with dynamic scenes, including the new Change Detection Challenge Dataset, demonstrate that the proposed ensemble video object cut method outperforms various state-ofthe-art algorithms.</p><p>4 0.63946092 <a title="55-lsi-4" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>5 0.62371522 <a title="55-lsi-5" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>Author: Mehrsan Javan Roshtkhari, Martin D. Levine</p><p>Abstract: We present a novel approach for video parsing and simultaneous online learning of dominant and anomalous behaviors in surveillance videos. Dominant behaviors are those occurring frequently in videos and hence, usually do not attract much attention. They can be characterized by different complexities in space and time, ranging from a scene background to human activities. In contrast, an anomalous behavior is defined as having a low likelihood of occurrence. We do not employ any models of the entities in the scene in order to detect these two kinds of behaviors. In this paper, video events are learnt at each pixel without supervision using densely constructed spatio-temporal video volumes. Furthermore, the volumes are organized into large contextual graphs. These compositions are employed to construct a hierarchical codebook model for the dominant behaviors. By decomposing spatio-temporal contextual information into unique spatial and temporal contexts, the proposed framework learns the models of the dominant spatial and temporal events. Thus, it is ultimately capable of simultaneously modeling high-level behaviors as well as low-level spatial, temporal and spatio-temporal pixel level changes.</p><p>6 0.61435819 <a title="55-lsi-6" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>7 0.61312521 <a title="55-lsi-7" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>8 0.58392388 <a title="55-lsi-8" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>9 0.55078411 <a title="55-lsi-9" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>10 0.54135692 <a title="55-lsi-10" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>11 0.52914667 <a title="55-lsi-11" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>12 0.52888173 <a title="55-lsi-12" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>13 0.52047402 <a title="55-lsi-13" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>14 0.51707357 <a title="55-lsi-14" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>15 0.50523198 <a title="55-lsi-15" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>16 0.49908447 <a title="55-lsi-16" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>17 0.49419117 <a title="55-lsi-17" href="./cvpr-2013-Image_Matting_with_Local_and_Nonlocal_Smooth_Priors.html">211 cvpr-2013-Image Matting with Local and Nonlocal Smooth Priors</a></p>
<p>18 0.49191856 <a title="55-lsi-18" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>19 0.48840034 <a title="55-lsi-19" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>20 0.48665065 <a title="55-lsi-20" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.057), (16, 0.015), (26, 0.023), (33, 0.751), (67, 0.033), (69, 0.015), (87, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99960333 <a title="55-lda-1" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>2 0.99934965 <a title="55-lda-2" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>Author: Neill D.F. Campbell, Kartic Subr, Jan Kautz</p><p>Abstract: Conditional Random Fields (CRFs) are used for diverse tasks, ranging from image denoising to object recognition. For images, they are commonly defined as a graph with nodes corresponding to individual pixels and pairwise links that connect nodes to their immediate neighbors. Recent work has shown that fully-connected CRFs, where each node is connected to every other node, can be solved efficiently under the restriction that the pairwise term is a Gaussian kernel over a Euclidean feature space. In this paper, we generalize the pairwise terms to a non-linear dissimilarity measure that is not required to be a distance metric. To this end, we propose a density estimation technique to derive conditional pairwise potentials in a nonparametric manner. We then use an efficient embedding technique to estimate an approximate Euclidean feature space for these potentials, in which the pairwise term can still be expressed as a Gaussian kernel. We demonstrate that the use of non-parametric models for the pairwise interactions, conditioned on the input data, greatly increases expressive power whilst maintaining efficient inference.</p><p>3 0.99931806 <a title="55-lda-3" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>Author: Adarsh Kowdle, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: In this work, we consider images of a scene with a moving object captured by a static camera. As the object (human or otherwise) moves about the scene, it reveals pairwise depth-ordering or occlusion cues. The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. We cast the problem of depth-layer segmentation as a discrete labeling problem on a spatiotemporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently, and using a na¨ ıve combination of the features.</p><p>4 0.9991954 <a title="55-lda-4" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>Author: Petr Gronát, Guillaume Obozinski, Josef Sivic, Tomáš Pajdla</p><p>Abstract: The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as onlyfewpositive training examples are availablefor each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. 2Center for Machine Perception, Faculty of Electrical Engineering 3WILLOW project, Laboratoire d’Informatique de l’E´cole Normale Sup e´rieure, ENS/INRIA/CNRS UMR 8548. 4Universit Paris-Est, LIGM (UMR CNRS 8049), Center for Visual Computing, Ecole des Ponts - ParisTech, 77455 Marne-la-Valle, France</p><p>5 0.99919331 <a title="55-lda-5" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>Author: Zhen Li, Shiyu Chang, Feng Liang, Thomas S. Huang, Liangliang Cao, John R. Smith</p><p>Abstract: This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community.</p><p>6 0.99917203 <a title="55-lda-6" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>same-paper 7 0.9991287 <a title="55-lda-7" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>8 0.99870986 <a title="55-lda-8" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>9 0.99861568 <a title="55-lda-9" href="./cvpr-2013-Dynamic_Scene_Classification%3A_Learning_Motion_Descriptors_with_Slow_Features_Analysis.html">137 cvpr-2013-Dynamic Scene Classification: Learning Motion Descriptors with Slow Features Analysis</a></p>
<p>10 0.99790347 <a title="55-lda-10" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>11 0.99738312 <a title="55-lda-11" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>12 0.99723744 <a title="55-lda-12" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>13 0.9946332 <a title="55-lda-13" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>14 0.99441278 <a title="55-lda-14" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>15 0.9885962 <a title="55-lda-15" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>16 0.98818237 <a title="55-lda-16" href="./cvpr-2013-Scalable_Sparse_Subspace_Clustering.html">379 cvpr-2013-Scalable Sparse Subspace Clustering</a></p>
<p>17 0.98736203 <a title="55-lda-17" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>18 0.98678613 <a title="55-lda-18" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>19 0.98559117 <a title="55-lda-19" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>20 0.98534149 <a title="55-lda-20" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
