<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-326" href="#">cvpr2013-326</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</h1>
<br/><p>Source: <a title="cvpr-2013-326-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lu_Patch_Match_Filter_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jiangbo Lu, Hongsheng Yang, Dongbo Min, Minh N. Do</p><p>Abstract: Though many tasks in computer vision can be formulated elegantly as pixel-labeling problems, a typical challenge discouraging such a discrete formulation is often due to computational efficiency. Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. This paper presents a generic and fast computational framework for general multi-labeling problems called PatchMatch Filter (PMF). For the very first time, we explore effective and efficient strategies to weave together these two fundamental techniques developed in isolation, i.e., PatchMatch-based randomized search and efficient edge-aware image filtering. By decompositing an image into compact superpixels, we also propose superpixelbased novel search strategies that generalize and improve the original PatchMatch method. Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases.</p><p>Reference: <a title="cvpr-2013-326-reference" href="../cvpr2013_reference/cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. [sent-2, score-0.437]
</p><p>2 However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. [sent-3, score-0.225]
</p><p>3 When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. [sent-4, score-0.576]
</p><p>4 Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. [sent-5, score-0.311]
</p><p>5 But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. [sent-6, score-0.407]
</p><p>6 Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. [sent-12, score-0.369]
</p><p>7 Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases. [sent-13, score-0.346]
</p><p>8 Introduction Many computer vision tasks such as stereo, optical flow and dense image alignment [13] can be formulated ele∗This study is supported by the HSSP research grant at the ADSC from Singapores Agency for Science, Technology and Research (A*STAR). [sent-15, score-0.237]
</p><p>9 In general, the common goal is to find a labeling solution that is both spatially smooth and discontinuity-preserving, while matching the observed data/label cost at the same time. [sent-19, score-0.195]
</p><p>10 However, a serious challenge posed to this discrete optimization framework is computational complexity, as global energy minimization algorithms such as graph cut or belief propagation become very slow when the image resolution is high or the label space is large. [sent-21, score-0.288]
</p><p>11 Recently, edge-aware filtering (EAF) of the cost volume [17, 14] has emerged as a competitive and fast alternative to energy-based global approaches. [sent-22, score-0.257]
</p><p>12 Though simple, this kind ofcost volume filtering techniques can achieve high-quality labeling results efficiently. [sent-23, score-0.229]
</p><p>13 However, despite their runtime being independent of the filter kernel size, EAF-based methods do not scale well to large label spaces. [sent-24, score-0.263]
</p><p>14 , PatchMatch-based randomized search and EAF, can be seamlessly woven together to address the curse of large label spaces very efficiently, while still maintaining or even improving the solution quality. [sent-34, score-0.271]
</p><p>15 In fact, the random and fragmented data access strategy within the cost volume effected by PatchMatch is drastically opposed to the highly regular and deterministic computing style of EAF methods. [sent-41, score-0.219]
</p><p>16 We take compact superpixels and subimages parsimoniously containing them as the atomic data units, and perform random search, label propagation and efficient cost aggregation collaboratively for them. [sent-43, score-0.641]
</p><p>17 PMF’s  run-time complexity is independent of the aggregation kernel size and only proportional to the logarithm of the search range [4]. [sent-45, score-0.268]
</p><p>18 Though not limited to the correspondence field estimation, PMF’s applications in stereo matching and optical flow estimation are instantiated and evaluated in this paper. [sent-47, score-0.533]
</p><p>19 , two-dimensional motion search space, displacement in subpixel accuracy, or over-parameterized surface or motion modeling [7]. [sent-50, score-0.257]
</p><p>20 They often achieve labeling results as good as those obtained by global energy-based approaches but at much faster speed, with the complexity typically independent ofthe filter kernel size. [sent-59, score-0.208]
</p><p>21 However, filtering each cost  slice individually, albeit allowing straightforward application of various efficient EAF techniques, makes the runtime scale linearly with the label space size. [sent-60, score-0.444]
</p><p>22 Motivated by the coherent natural structure in images, the PatchMatch method [4, 5] devised a very efficient randomized search and nearest-neighbor propagation approach, achieving substantial improvements in speed and memory efficiency over the prior arts. [sent-90, score-0.254]
</p><p>23 They showed that this method can deal with slanted surfaces much better than previous methods and achieved leading subpixel disparity accuracy. [sent-96, score-0.314]
</p><p>24 To handle disparity discontinuities, adaptive-weight cost aggregation [21] in 35 35 windows is used in [7]. [sent-98, score-0.331]
</p><p>25 Though PatchMatch can significantly dreodwucse i sth ues complexity dependency on tahtceh hla cbaenl space size, such a brute-force adaptive-weight summation has a linear complexity dependent on the window size and it slows down the overall runtime noticeably. [sent-99, score-0.265]
</p><p>26 In addition, more general and challenging dense correspondence problems such as optical flow are not addressed in these methods [7, 6]. [sent-100, score-0.317]
</p><p>27 But this reduction is not as aggressive as in PatchMatch, and also efficient local cost aggregation was not supported. [sent-104, score-0.197]
</p><p>28 Problem Formulation and Challenges We briefly present a general framework and notations of cost volume filtering-based methods for discrete labeling problems, and focus particularly on visual correspondence field estimation here. [sent-106, score-0.31]
</p><p>29 , the goal is to assign each pixel p = (xp, yp) a label l from the label set L = {0, 1, . [sent-108, score-0.283]
</p><p>30 For the stereo and optical flow problems considered here, l = (u, v), where u and v correspond to the displacement in x and y directions. [sent-114, score-0.428]
</p><p>31 Stereo matching degenerates to assigning a disparity d(u = d) to pixel p, where v = 0. [sent-115, score-0.208]
</p><p>32 Unlike global optimization-based discrete methods [18], local window-based methods stress reliable cost aggregation from the neighborhood and evaluate exhaustively every single hypothetical label l ∈ L. [sent-116, score-0.42]
</p><p>33 The final label lp for each pixel p hisy pdoetchiedteidca lw litahb a W l i∈nn Ler. [sent-117, score-0.208]
</p><p>34 To achieve spatially smooth yet discontinuity-preserving labeling results, edge-aware smoothing filters have been adopted in the local cost aggregation step of several leading  local methods [17, 14]. [sent-119, score-0.285]
</p><p>35 Given the raw cost slice C(l) computed for a label l, we denote its edge-aware filtered output as C˜(l). [sent-120, score-0.282]
</p><p>36 (1)  (r) Wp(r) is the local aggregation window centered at pixel p with a filter kernel radius r. [sent-123, score-0.217]
</p><p>37 Though EAF is very efficient, the linear complexity dependency on the label space size L requires repeated filtering of C(l) as in (1), and C(l) is of the same size of I. [sent-126, score-0.357]
</p><p>38 However, it can be discerned that PatchMatch’s randomized label space visit pattern for each individual pixel p is very incompatible with the regular image-wise cost filtering routine that is essential to the efficiency ofEAF-based methods. [sent-129, score-0.46]
</p><p>39 PatchMatch Filter Based on Superpixels This section proposes a superpixel-based computational framework for fast correspondence field estimation by exploiting PatchMatch-like random search and EAF-based cost aggregation synergistically. [sent-132, score-0.442]
</p><p>40 (b) Bounding-box B(ck) containing the superpixel S(k) centered at pixel ck and r-pixel extended subimage R(ck) . [sent-137, score-0.367]
</p><p>41 truth labeling solutions actually advocates a collaborative label search and propagation strategy for similar pixels covered in the same compact superpixel, without necessarily going to the pixel-wise fine granularity in PatchMatch [4]. [sent-138, score-0.462]
</p><p>42 Another key motivation from a computing perspective is that the efficiency of EAF essentially comes from the high computational redundancy or the vast opportunity for shared computation reuse among neighboring pixels when filtering an image or cost slice. [sent-139, score-0.259]
</p><p>43 However, PatchMatch processes each pixel with its random set of label candidates  individually in raster scan order. [sent-140, score-0.214]
</p><p>44 This renders EAF techniques not applicable and the cost aggregation runtime to grow linearly with the filter kernel size m = (2r + 1)2 [7]. [sent-141, score-0.365]
</p><p>45 Based on the above analysis, we propose to partition the input image into non-overlapping superpixels, and use them as the basic units for performing random search, propagation and subimage-based efficient cost aggregation collaboratively. [sent-142, score-0.355]
</p><p>46 As a spatially regularized labeling solution is favored, such a superpixel-based strategy, adapting to the underlying image structures, is more consistent with the goal of correspondence field estimation than its pixel-based counterpart. [sent-143, score-0.199]
</p><p>47 Compared to the propagation from the immediate causal pixels [4], taking superpixels as the basic primitive also effectively extends the propagation range and ameliorates the issue of being trapped in local optimum. [sent-144, score-0.333]
</p><p>48 Another important advantage is that SLIC superpixels are compact and of more regular shapes and sizes (M/K on average), giving a low overhead when their bounding-boxes are sought as discussed later. [sent-161, score-0.217]
</p><p>49 2(b), for a given segment S(k), B(ck) represents its minimum bounding-box centered at pixel ck and B(ck) ∈ I. [sent-167, score-0.256]
</p><p>50 PatchMatch Filter Algorithm Now we present the PatchMatch filter (PMF) a general computational framework to efficiently address discrete labeling problems, which exploits both superpixel-based PatchMatch search and efficient edge-aware cost filtering. [sent-172, score-0.309]
</p><p>51 Gareive pnro tphaisset of propagated labels Lt, EAF-based cost aggregation in (se1)t oisf tphroenp performed fso Lr the subimage R(ck) defined for S(k), but the filtering result is used only for the pixels in B(ck). [sent-188, score-0.439]
</p><p>52 ′(Ikj)  any pixel p ∈ B(ck), its current best label lp is updated instantly by a new label l ∈ Lt whenever C˜(p, l) < C˜(p, lp). [sent-193, score-0.327]
</p><p>53 Antlfyter b yth ae n preceding propagation step, a center-biased random search as in PatchMatch [4] is performed for the current segment S(k). [sent-194, score-0.264]
</p><p>54 Here we randomly pick a reference pixel s ∈ S(k) to promote the label propagation within a segment. [sent-198, score-0.266]
</p><p>55 The function f is then applied again to filter those cost subimages specified by Lr by substituting for Lt in (2). [sent-200, score-0.193]
</p><p>56 Therefore, no subimage filtering will be needed if a candidate label has been visited before. [sent-202, score-0.374]
</p><p>57 2(b) that compact superpixels S(k) are favored in our  PMF algorithm, as the filtering overhead incurred by the stretched sizes of R(ck) and B(ck) will be kept low. [sent-204, score-0.327]
</p><p>58 Note that prior stereo or optical flow methods [22, 12] often take segments as the matching units and infer a single displacement for each segment. [sent-205, score-0.523]
</p><p>59 It directly estimates and decides the optimal label for each pixel independently, while leveraging their shared spatial neighbors and plausible label candidates for fast computation. [sent-208, score-0.39]
</p><p>60 2 based on a baseline search and propagation strategy conceptually close to the original PatchMatch principle [4]. [sent-213, score-0.208]
</p><p>61 h Tbhoriss enrichment scheme allows effective and fast= propagation oifs plausible label candidates from similar segments. [sent-239, score-0.274]
</p><p>62 As image representation in superpixels greatly reduces the graph complexity, this motivates us to design a better label initialization strategy than the random initialization [4]. [sent-241, score-0.324]
</p><p>63 The basic idea is to assign a potentially good candidate label rather than a random label to each segment S(k). [sent-242, score-0.315]
</p><p>64 Given the maximum label search range W, we select for segment S(k) in image I closest segment S? [sent-243, score-0.306]
</p><p>65 Complexity Given an image of size M, the label space size L and the superpixel number K, w? [sent-252, score-0.194]
</p><p>66 O(M) is used to hold the filtered cost associated with the current best label at each pixel. [sent-266, score-0.227]
</p><p>67 In our implementation, we pre-organize all the subimages {R(ck)} of the input image pIr ein-otorg an array lo tfh compact g2eDs {bRuf(fecrs),} }w ohfitc hhe f iancpiluitta itmescost computation and filtering in the label search process. [sent-268, score-0.427]
</p><p>68 Applications We present two applications of the proposed PMF framework: stereo matching and optical flow estimation. [sent-273, score-0.422]
</p><p>69 Stereo Matching We present two different PMF-based stereo methods that model the scene disparity and parameterize the corresponding label space differently. [sent-278, score-0.409]
</p><p>70 Like most stereo methods [17, 14], the first approach makes an assumption of fronto-parallel local support windows, whereby pixels inside are matched to pixels in another view at a constant (integer) disparity. [sent-279, score-0.21]
</p><p>71 Similar to [7], the second approach attempts to estimate a 3D plane Qp at each pixel p, so pixels lying on the same slanted surfaces can then be used for reliable cost aggregation with high subpixel precision. [sent-281, score-0.494]
</p><p>72 Both methods can benefit from the PMF technique, as the disparity search range can be quite large due to high-resolution stereo images or an infinite number of possible 3D planes. [sent-283, score-0.375]
</p><p>73 For each pixel p, we search for a 3D plane Qp defined by a three-parameter vector lp = (ap, bp, cp). [sent-286, score-0.219]
</p><p>74 For both PMF-C and PMF-S, we compute the raw matching cost between a pair of hypothetical matching pixels q and q? [sent-297, score-0.219]
</p><p>75 2 to perform superpixel-based collaborative random search, propagation and cost subimage filtering. [sent-330, score-0.319]
</p><p>76 The implementation of cost aggregation for PMF-C is straightforward, whereas more care needs to be taken for the random plane initialization and iterative random search steps in PMF-S1. [sent-331, score-0.407]
</p><p>77 To this end, we adopt the approach presented in [7], and use a random unit normal vector (nx, ny, nz) plus a random disparity value sampled from the allowed continuous range as proxy for the plane representation. [sent-332, score-0.231]
</p><p>78 After deciding an initial disparity map using a WTA strategy, we detect unreliable disparity estimates by conducting a left-right cross-checking. [sent-335, score-0.268]
</p><p>79 Optical Flow We now present a PMF-based optical flow method named PMF-OF. [sent-340, score-0.237]
</p><p>80 Its main work flow closely resembles that of PMF-C, but a label l represents a displacement vector (u, v) in x and y directions. [sent-341, score-0.289]
</p><p>81 The label space for optical flow is therefore often much larger than typical label spaces tackled in stereo matching. [sent-342, score-0.631]
</p><p>82 Based on a discrete labeling formulation, PMF-OF solves for subpixel accurate flow vectors by upscaling the label dimension to allow fractional displacements along both x and y directions. [sent-343, score-0.458]
</p><p>83 Given a candidate label l, a pixel q in image I matched is to the pixel q? [sent-345, score-0.209]
</p><p>84 The PMF-based label search and cost filtering algorithm is then applied, including those improved strategies presented in Sect. [sent-349, score-0.474]
</p><p>85 The following same parameter settings are used across all stereo and optical flow datasets: {r, σr, β, γ1 } = {9, 0. [sent-355, score-0.393]
</p><p>86 It can be observed that for a reasonable range of K settings, optical flow or stereo results have almost always converged after  8-10 iterations. [sent-378, score-0.393]
</p><p>87 For the same iteration number, choosing a larger K (namely a smaller superpixel size) gives a better gain in accuracy on optical flow estimation than stereo, due to intrinsically more complex 2D motions. [sent-384, score-0.312]
</p><p>88 However, this is at a price of a longer runtime per iteration caused by the increased adjacency graph size and increased subimage processing overhead. [sent-385, score-0.207]
</p><p>89 Stereo Matching Results We evaluate our PMF-C and PMF-S stereo methods combined with CLMF-0 and GF filtering techniques, using the Middlebury stereo benchmark [2] in Table 2. [sent-389, score-0.44]
</p><p>90 They also achieve disparity accuracy comparable to or even better than the top-performing local stereo methods – PatchMatch stereo [7] and CostFilter [17]. [sent-392, score-0.446]
</p><p>91 5 shows the disparity maps estimated by our PMF-S methods,  ×  which preserve depth discontinuities while generating spatially smooth disparities with high subpixel accuracy. [sent-421, score-0.318]
</p><p>92 Our PMF-C methods have a runtime comparable with CostFilter [17] on the Middlebury dataset of small disparity ranges, but run over 3-7 times faster than [17] for highresolution stereo images (e. [sent-425, score-0.423]
</p><p>93 Without reducing the iteration number nor turning off plane refinement, our PMF-S methods achieve a few times speedup over PM stereo [7] (e. [sent-428, score-0.254]
</p><p>94 1), even though image-wise cost filtering has been exhaustively per-  formed for every single label in [17]. [sent-445, score-0.39]
</p><p>95 Tested on the same CPU, PMF-OF runs even over 30-times faster than CostFilter [17] on the Urban sequence, thanks to slashing the complexity dependency on the huge label space size. [sent-456, score-0.299]
</p><p>96 particularly  demonstrated  its effectiveness  in estimating  smoothly varying yet discontinuity-preserving stereo and optical flow maps. [sent-469, score-0.393]
</p><p>97 Similarly, optical flow can also be overparameterized with an affine motion model by taking advantage of the power of randomized search in the highdimensional parameter space. [sent-473, score-0.414]
</p><p>98 Interestingly, dense patch matching [4] has been employed in state-of-the-art optical flow method [20] to find multiple extended displacements. [sent-474, score-0.287]
</p><p>99 PMBP:  [7]  [8] [9] [10] [11] [12]  [13]  [14] [15]  [16]  [17]  [18]  Patchmatch belief propagation for correspondence field estimation. [sent-516, score-0.213]
</p><p>100 A revisit to cost aggregation in stereo matching: How far can we reduce its computational redundancy? [sent-580, score-0.353]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pmf', 0.516), ('patchmatch', 0.446), ('annf', 0.244), ('eaf', 0.185), ('ck', 0.16), ('stereo', 0.156), ('flow', 0.135), ('disparity', 0.134), ('costfilter', 0.131), ('filtering', 0.128), ('gf', 0.123), ('aggregation', 0.119), ('label', 0.119), ('optical', 0.102), ('propagation', 0.102), ('superpixels', 0.102), ('slanted', 0.093), ('runtime', 0.091), ('subimage', 0.087), ('subpixel', 0.087), ('search', 0.085), ('correspondence', 0.08), ('middlebury', 0.078), ('cost', 0.078), ('logl', 0.077), ('superpixel', 0.075), ('yq', 0.069), ('xq', 0.067), ('randomized', 0.067), ('complexity', 0.064), ('strategies', 0.064), ('subimages', 0.062), ('filter', 0.053), ('speedup', 0.053), ('segment', 0.051), ('labeling', 0.049), ('dependency', 0.046), ('plane', 0.045), ('pixel', 0.045), ('discrete', 0.044), ('lp', 0.044), ('slic', 0.043), ('faster', 0.042), ('cq', 0.042), ('endpoint', 0.04), ('visited', 0.04), ('spatially', 0.039), ('aee', 0.038), ('overhead', 0.037), ('bleyer', 0.037), ('segments', 0.036), ('displacement', 0.035), ('discontinuities', 0.035), ('lt', 0.035), ('though', 0.034), ('multipoint', 0.033), ('compact', 0.033), ('field', 0.031), ('neighbors', 0.031), ('exhaustively', 0.031), ('pmbp', 0.031), ('units', 0.03), ('competing', 0.03), ('filtered', 0.03), ('plausible', 0.029), ('adjacency', 0.029), ('cp', 0.029), ('matching', 0.029), ('hypothetical', 0.029), ('slice', 0.028), ('volume', 0.028), ('wta', 0.028), ('huge', 0.028), ('initialization', 0.028), ('pixels', 0.027), ('lr', 0.027), ('raw', 0.027), ('favored', 0.027), ('teddy', 0.027), ('random', 0.026), ('collaborative', 0.026), ('cones', 0.026), ('reuse', 0.026), ('rhemann', 0.025), ('disc', 0.025), ('motion', 0.025), ('techniques', 0.024), ('fractional', 0.024), ('candidates', 0.024), ('barnes', 0.023), ('regular', 0.023), ('disparities', 0.023), ('fast', 0.023), ('slow', 0.023), ('fragmented', 0.022), ('giving', 0.022), ('access', 0.021), ('strategy', 0.021), ('patch', 0.021), ('wp', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="326-tfidf-1" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>Author: Jiangbo Lu, Hongsheng Yang, Dongbo Min, Minh N. Do</p><p>Abstract: Though many tasks in computer vision can be formulated elegantly as pixel-labeling problems, a typical challenge discouraging such a discrete formulation is often due to computational efficiency. Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. This paper presents a generic and fast computational framework for general multi-labeling problems called PatchMatch Filter (PMF). For the very first time, we explore effective and efficient strategies to weave together these two fundamental techniques developed in isolation, i.e., PatchMatch-based randomized search and efficient edge-aware image filtering. By decompositing an image into compact superpixels, we also propose superpixelbased novel search strategies that generalize and improve the original PatchMatch method. Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases.</p><p>2 0.21558003 <a title="326-tfidf-2" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>Author: Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang</p><p>Abstract: This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants, a new tree structure, ”Segment-Tree ”, is proposed for non-local matching cost aggregation. Conceptually, the segment-tree is constructed in a three-step process: first, the pixels are grouped into a set of segments with the reference color or intensity image; second, a tree graph is created for each segment; and in the final step, these independent segment graphs are linked to form the segment-tree structure. In practice, this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights, our method introduces some ’non-local’ decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities, and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree within two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable to previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore, the tree structure can be refined with the estimated disparities, which leads to consistent scene segmentation and significantly better aggregation results.</p><p>3 0.20259061 <a title="326-tfidf-3" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>Author: Zhuoyuan Chen, Hailin Jin, Zhe Lin, Scott Cohen, Ying Wu</p><p>Abstract: We present an optical flow algorithm for large displacement motions. Most existing optical flow methods use the standard coarse-to-fine framework to deal with large displacement motions which has intrinsic limitations. Instead, we formulate the motion estimation problem as a motion segmentation problem. We use approximate nearest neighbor fields to compute an initial motion field and use a robust algorithm to compute a set of similarity transformations as the motion candidates for segmentation. To account for deviations from similarity transformations, we add local deformations in the segmentation process. We also observe that small objects can be better recovered using translations as the motion candidates. We fuse the motion results obtained under similarity transformations and under translations together before a final refinement. Experimental validation shows that our method can successfully handle large displacement motions. Although we particularly focus on large displacement motions in this work, we make no sac- rifice in terms of overall performance. In particular, our method ranks at the top of the Middlebury benchmark.</p><p>4 0.20254515 <a title="326-tfidf-4" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>5 0.17218617 <a title="326-tfidf-5" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>6 0.16790426 <a title="326-tfidf-6" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>7 0.15377094 <a title="326-tfidf-7" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>8 0.11973462 <a title="326-tfidf-8" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>9 0.11923105 <a title="326-tfidf-9" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>10 0.11226916 <a title="326-tfidf-10" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>11 0.1109553 <a title="326-tfidf-11" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>12 0.10624233 <a title="326-tfidf-12" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>13 0.10499782 <a title="326-tfidf-13" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>14 0.10326421 <a title="326-tfidf-14" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>15 0.10155788 <a title="326-tfidf-15" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>16 0.099261582 <a title="326-tfidf-16" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>17 0.086559951 <a title="326-tfidf-17" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>18 0.085651092 <a title="326-tfidf-18" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>19 0.084262438 <a title="326-tfidf-19" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>20 0.08335764 <a title="326-tfidf-20" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, 0.103), (2, 0.048), (3, 0.017), (4, 0.045), (5, -0.021), (6, 0.022), (7, -0.001), (8, -0.083), (9, 0.054), (10, 0.123), (11, 0.081), (12, 0.15), (13, 0.061), (14, 0.022), (15, 0.059), (16, -0.078), (17, -0.224), (18, 0.066), (19, 0.013), (20, -0.048), (21, 0.0), (22, 0.125), (23, 0.036), (24, -0.04), (25, -0.067), (26, -0.064), (27, -0.058), (28, 0.032), (29, 0.043), (30, -0.014), (31, -0.052), (32, 0.067), (33, 0.003), (34, -0.016), (35, -0.037), (36, 0.018), (37, 0.011), (38, 0.023), (39, -0.034), (40, 0.032), (41, -0.035), (42, -0.009), (43, 0.05), (44, 0.047), (45, 0.021), (46, 0.075), (47, -0.042), (48, -0.053), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95277172 <a title="326-lsi-1" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>Author: Jiangbo Lu, Hongsheng Yang, Dongbo Min, Minh N. Do</p><p>Abstract: Though many tasks in computer vision can be formulated elegantly as pixel-labeling problems, a typical challenge discouraging such a discrete formulation is often due to computational efficiency. Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. This paper presents a generic and fast computational framework for general multi-labeling problems called PatchMatch Filter (PMF). For the very first time, we explore effective and efficient strategies to weave together these two fundamental techniques developed in isolation, i.e., PatchMatch-based randomized search and efficient edge-aware image filtering. By decompositing an image into compact superpixels, we also propose superpixelbased novel search strategies that generalize and improve the original PatchMatch method. Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases.</p><p>2 0.83474112 <a title="326-lsi-2" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>3 0.72601438 <a title="326-lsi-3" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>4 0.70139295 <a title="326-lsi-4" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>Author: Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang</p><p>Abstract: This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants, a new tree structure, ”Segment-Tree ”, is proposed for non-local matching cost aggregation. Conceptually, the segment-tree is constructed in a three-step process: first, the pixels are grouped into a set of segments with the reference color or intensity image; second, a tree graph is created for each segment; and in the final step, these independent segment graphs are linked to form the segment-tree structure. In practice, this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights, our method introduces some ’non-local’ decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities, and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree within two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable to previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore, the tree structure can be refined with the estimated disparities, which leads to consistent scene segmentation and significantly better aggregation results.</p><p>5 0.65268362 <a title="326-lsi-5" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>Author: David Pfeiffer, Stefan Gehrig, Nicolai Schneider</p><p>Abstract: Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste- riori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level.</p><p>6 0.61674309 <a title="326-lsi-6" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>7 0.61377954 <a title="326-lsi-7" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>8 0.60686976 <a title="326-lsi-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.58249825 <a title="326-lsi-9" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>10 0.56705308 <a title="326-lsi-10" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>11 0.52888328 <a title="326-lsi-11" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>12 0.52723426 <a title="326-lsi-12" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>13 0.52115643 <a title="326-lsi-13" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>14 0.47486597 <a title="326-lsi-14" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>15 0.47477069 <a title="326-lsi-15" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>16 0.471136 <a title="326-lsi-16" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>17 0.46306133 <a title="326-lsi-17" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>18 0.45407659 <a title="326-lsi-18" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>19 0.44370553 <a title="326-lsi-19" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>20 0.44321543 <a title="326-lsi-20" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.099), (16, 0.326), (26, 0.039), (33, 0.283), (67, 0.041), (69, 0.038), (87, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98631513 <a title="326-lda-1" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>Author: Hyeongwoo Kim, Hailin Jin, Sunil Hadap, Inso Kweon</p><p>Abstract: We present a novel method to separate specular reflection from a single image. Separating an image into diffuse and specular components is an ill-posed problem due to lack of observations. Existing methods rely on a specularfree image to detect and estimate specularity, which however may confuse diffuse pixels with the same hue but a different saturation value as specular pixels. Our method is based on a novel observation that for most natural images the dark channel can provide an approximate specular-free image. We also propose a maximum a posteriori formulation which robustly recovers the specular reflection and chromaticity despite of the hue-saturation ambiguity. We demonstrate the effectiveness of the proposed algorithm on real and synthetic examples. Experimental results show that our method significantly outperforms the state-of-theart methods in separating specular reflection.</p><p>2 0.92247498 <a title="326-lda-2" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>Author: Guha Balakrishnan, Fredo Durand, John Guttag</p><p>Abstract: We extract heart rate and beat lengths from videos by measuring subtle head motion caused by the Newtonian reaction to the influx of blood at each beat. Our method tracks features on the head and performs principal component analysis (PCA) to decompose their trajectories into a set of component motions. It then chooses the component that best corresponds to heartbeats based on its temporal frequency spectrum. Finally, we analyze the motion projected to this component and identify peaks of the trajectories, which correspond to heartbeats. When evaluated on 18 subjects, our approach reported heart rates nearly identical to an electrocardiogram device. Additionally we were able to capture clinically relevant information about heart rate variability.</p><p>3 0.90997458 <a title="326-lda-3" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>4 0.89526224 <a title="326-lda-4" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>Author: Ahmed T. Kamal, Jay A. Farrell, Amit K. Roy-Chowdhury</p><p>Abstract: Due to their high fault-tolerance, ease of installation and scalability to large networks, distributed algorithms have recently gained immense popularity in the sensor networks community, especially in computer vision. Multitarget tracking in a camera network is one of the fundamental problems in this domain. Distributed estimation algorithms work by exchanging information between sensors that are communication neighbors. Since most cameras are directional sensors, it is often the case that neighboring sensors may not be sensing the same target. Such sensors that do not have information about a target are termed as “naive ” with respect to that target. In this paper, we propose consensus-based distributed multi-target tracking algorithms in a camera network that are designed to address this issue of naivety. The estimation errors in tracking and data association, as well as the effect of naivety, are jointly addressed leading to the development of an informationweighted consensus algorithm, which we term as the Multitarget Information Consensus (MTIC) algorithm. The incorporation of the probabilistic data association mecha- nism makes the MTIC algorithm very robust to false measurements/clutter. Experimental analysis is provided to support the theoretical results.</p><p>5 0.88271254 <a title="326-lda-5" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>Author: Wei Li, Xiaogang Wang</p><p>Abstract: In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person reidentification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsityinducing norm and information theoretical regularization. . cuhk . edu .hk (a) Camera view A (b) Camera view B This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.</p><p>6 0.87551087 <a title="326-lda-6" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>7 0.86590368 <a title="326-lda-7" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>8 0.85451829 <a title="326-lda-8" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>same-paper 9 0.84045893 <a title="326-lda-9" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>10 0.79908878 <a title="326-lda-10" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>11 0.79016948 <a title="326-lda-11" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>12 0.78927028 <a title="326-lda-12" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>13 0.78488278 <a title="326-lda-13" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>14 0.78075272 <a title="326-lda-14" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>15 0.77605933 <a title="326-lda-15" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>16 0.77571553 <a title="326-lda-16" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>17 0.77386028 <a title="326-lda-17" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>18 0.77238619 <a title="326-lda-18" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>19 0.76350421 <a title="326-lda-19" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>20 0.76119018 <a title="326-lda-20" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
