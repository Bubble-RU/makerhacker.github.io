<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-457" href="#">cvpr2013-457</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</h1>
<br/><p>Source: <a title="cvpr-2013-457-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/He_Visual_Tracking_via_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>Reference: <a title="cvpr-2013-457-reference" href="../cvpr2013_reference/cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 lau  Abstract This paper presents a novel locality sensitive histogram algorithm for visual tracking. [sent-5, score-0.68]
</p><p>2 The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. [sent-7, score-0.494]
</p><p>3 An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. [sent-8, score-0.604]
</p><p>4 A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. [sent-9, score-1.99]
</p><p>5 Extensive experiments demonstrate that the proposed tracking framework outper-  ,  forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically. [sent-10, score-0.624]
</p><p>6 Introduction Visual tracking is one of the most active research areas in computer vision, with numerous applications including augmented reality, surveillance, and object identification. [sent-12, score-0.437]
</p><p>7 The chief issue for robust visual tracking is to handle the appearance change of the target object. [sent-13, score-0.685]
</p><p>8 Based on the appearance models used, tracking algorithms can be divided into generative tracking [4, 17, 19, 14, 3] and discriminative tracking [7, 2, 11, 8]. [sent-14, score-1.144]
</p><p>9 Generative tracking represents the target object in a particular feature space, and then searches for the best matching score within the image region. [sent-15, score-0.605]
</p><p>10 Discriminative tracking treats visual tracking as a binary classification problem to define the boundary between a target image patch and the ∗Correspondence author. [sent-17, score-0.937]
</p><p>11 While numerous algorithms of two categories have been proposed with success, it remains a challenging task to develop a tracking algorithm that is  both accurate and efficient. [sent-23, score-0.403]
</p><p>12 The appearance of an object changes drastically when illumination varies significantly. [sent-25, score-0.373]
</p><p>13 Early works on visual tracking represent objects with contours [9] with success when the brightness constancy assumption holds. [sent-28, score-0.429]
</p><p>14 The eigentracking approach [4] operates on the subspace constancy assumption to account for the appearance change caused by illumination variation based on a set of training images. [sent-29, score-0.363]
</p><p>15 Most recently, Harr-like features and online subspace models have been used in object tracking with demonstrated success in dealing with large lighting variation [13, 2, 3, 22]. [sent-30, score-0.463]
</p><p>16 However, spatial information of object appearance is missing in this holistic representation, which makes it sensitive to noise as well as occlusion in tracking applications. [sent-33, score-0.653]
</p><p>17 The fragment-based tracker [1] divides the target object into several regions and  represents them with multiple local histograms. [sent-35, score-0.605]
</p><p>18 A vote map is used to combine the votes from all the regions in the target frame. [sent-36, score-0.349]
</p><p>19 However, the computation of multiple local histograms and the vote map can be time consuming even with the use of integral histograms [15]. [sent-37, score-0.358]
</p><p>20 As a trade-off between accuracy and speed, the fragment-based method [1] uses up to 40 regions to represent the target object and thus causes jitter effects. [sent-38, score-0.374]
</p><p>21 In [20], each target object is modeled by a small number of rectangular blocks, which positions within the tracking window are adaptively determined. [sent-40, score-0.68]
</p><p>22 In this paper, we propose a novel locality sensitive histogram algorithm, which takes into account contributions from every pixel in an image instead of from pixels only inside local neighborhoods as the local histogram algorithm. [sent-43, score-0.979]
</p><p>23 However, instead of counting the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a floating-point value is added to the corresponding bin for each occurrence of an intensity value. [sent-45, score-0.333]
</p><p>24 The floating-point value declines exponentially with respect to the distance to the pixel location where the locality sensitive histogram is computed. [sent-46, score-0.861]
</p><p>25 This tracking framework effectively deals with drastic illumination changes by extracting dense illumination invariant features using the proposed locality sensitive histogram. [sent-50, score-1.394]
</p><p>26 It also handles significant pose and scale variation, occlusion and visual drifts, out of plane rotation and abrupt motion, and background clutters with the use of a novel multi-region tracking algorithm. [sent-51, score-0.519]
</p><p>27 This unique property facilitates robust multi-region tracking, and the efficiency of the locality sensitive histogram enables the proposed tracker to run in real time. [sent-53, score-0.977]
</p><p>28 1, the computational complexity of the histogram is linear in the number of bins at each pixel location: O(B). [sent-68, score-0.34]
</p><p>29 They are computed at each pixel location, and have been proved to be very useful for many tasks like the bilateral filtering [25, 16], median filtering [25, 12] and tracking [15, 1]. [sent-72, score-0.547]
</p><p>30 Nevertheless, this dependency can be removed using integral histogram [15], which reduces the computational complexity to O(B) at each pixel location. [sent-74, score-0.359]
</p><p>31 Let HpI denote the integral histogram computed at pixel p. [sent-75, score-0.393]
</p><p>32 It can be computed based on the previous integral histogram computed at pixel p − 1 ionn a way esivmioiluasr ntot ethgrea integral image m[6p, u2te3]d: HpI(b) = Q(Ip, b) + HpI−1(b),  b = 1, . [sent-76, score-0.518]
</p><p>33 HpI contains the contributions of all the pixels to the left of pixel p, and the local histogram between pixel p and another pixel q on the left of p is computed as HpI(b) − HqI(b) for b = 1, . [sent-81, score-0.53]
</p><p>34 We propose a novel locality sensitive histogram algorithm to address this problem. [sent-88, score-0.635]
</p><p>35 Let HpE denote the locality sensitive histogram computed at pixel p. [sent-89, score-0.763]
</p><p>36 Thus, the computational complexity of the locality sensitive histogram is reduced to O(B) per pixel. [sent-105, score-0.635]
</p><p>37 4-6, np can also be computed in the same way: np  =  nlpeft + nrpight  − 1,  (8)  nlpeft  =  1+ α  nrpight  =  1+ α  · nlpe−ft1, · npr+igh1t. [sent-127, score-0.362]
</p><p>38 The red dash curve shows the runtime of computing the local histograms from the integral histogram for a 1MB image w. [sent-130, score-0.374]
</p><p>39 The blue solid curve shows the runtime of computing the proposed locality sensitive histograms. [sent-134, score-0.461]
</p><p>40 For instance, when B = 16, the locality sensitive histograms can be computed at 30 FPS. [sent-136, score-0.604]
</p><p>41 Figure 1 shows the speed of the proposed algorithm for computing the locality sensitive histogram for a 1MB 2D image with respect to the logarithm of the number of bins B. [sent-143, score-0.707]
</p><p>42 Under the assumption of affine illumination changes, we can synthesize images of the scene presented in Figure 2 (a) captured under different illumination conditions as shown in Figure 2 (b) and (c). [sent-147, score-0.464]
</p><p>43 It is basically an image transform to convert the original image into a new image, where the new pixel values do not change when the illumination changes. [sent-150, score-0.333]
</p><p>44 p denote the intensity values of pixel p before and after an affine illumination change. [sent-152, score-0.467]
</p><p>45 Lmet A HpS denote the histogram computed from a window Sp centered at the pixel p, and bp denote the bin corresponding to the intensity value Ip. [sent-155, score-0.627]
</p><p>46 If rp scales linear with the illumination so that  rp? [sent-163, score-0.355]
</p><p>47 With an additionaaln assumption etha ntu tmheaffine illumination changes are locally smooth so that the affine transform is the same for all pixels inside window Sp, we have: rp? [sent-175, score-0.386]
</p><p>48 12 with the proposed locality sensitive histogram HpE, which adaptively takes into account the contribution from all image pixels. [sent-192, score-0.716]
</p><p>49 Unlike the intensity values, they remain the same even under dramatic illumination changes, and are used as the input to the tracking algorithm proposed in Section 4. [sent-204, score-0.681]
</p><p>50 Multi-Region Tracking The proposed multi-region tracking algorithm aims at capturing the spatial information of a target object, which is missing in single region tracking, to account for appearance change caused by factors such as illumination and occlusion. [sent-206, score-0.98]
</p><p>51 We exploit the proposed locality sensitive histograms for multi-region tracking, since illumination invariant features and region matching scores can be computed efficiently. [sent-208, score-0.89]
</p><p>52 Tracking via Locality Sensitive Histograms The proposed tracking algorithm represents a target object with multiple overlapping regions, each of which de222444223088  scribes some local configuration. [sent-212, score-0.605]
</p><p>53 The spatial relationship of these regions remains fixed and is used for region-to-region matching between the template and the potential target object of the current frame. [sent-213, score-0.404]
</p><p>54 The spacing between regions depends on the size of the target object and the user defined number of regions. [sent-214, score-0.334]
</p><p>55 Representing the object appearance by hundreds of regions allows the proposed tracker to better handle occlusion and large appearance change. [sent-215, score-0.547]
</p><p>56 The locality sensitive histograms also allow us to process a large number of regions in real-time. [sent-216, score-0.665]
</p><p>57 The fragmentbased tracking method approximates the kernel function with weights by computing histograms from three rectangular regions of different sizes. [sent-219, score-0.57]
</p><p>58 Since object movements are typically non-ballistic, object tracking entails only searches within the nearby area of the current target location. [sent-222, score-0.639]
</p><p>59 Figure 3 shows an example of the proposed multi-region tracking algorithm, where the blue circles indicate the regions. [sent-223, score-0.366]
</p><p>60 Based on the location of the target object center in the previous frame, we aim to locate the new center location  within the search region. [sent-226, score-0.333]
</p><p>61 Similar to recent tracking-bydetection methods, exhaustive search within the search region is performed in this work, where every pixel is considered as a candidate target center. [sent-227, score-0.367]
</p><p>62 The proposed locality sensitive histogram is normalized as presented in Section 2. [sent-232, score-0.635]
</p><p>63 Similar to the fragment-based tracking method, we use a least-mediansquares estimator to accumulate all the votes. [sent-243, score-0.366]
</p><p>64 The final tracking result is the candidate object with the lowest joint score (as the vote map measures the dissimilarity between regions). [sent-246, score-0.514]
</p><p>65 Online Template Update Visual tracking with a fixed template is not effective over a long period of time as the appearance may have changed significantly. [sent-249, score-0.482]
</p><p>66 It is also likely to cause jitter and drift as observed in the fragment-based tracking method [1]. [sent-250, score-0.484]
</p><p>67 Taking advantage of using multiple regions, updating a fraction of them in each frame allows the template to adapt to the appearance change and alleviate the tracking drift problem. [sent-252, score-0.596]
</p><p>68 Once the new target location is determined, the local histograms are updated as follows: HpE1(·) = HpE2(·)  if F1 · M < d(S1, S2) < F2 · M,  (18) where M is the median distance of all the regions at the new position. [sent-253, score-0.456]
</p><p>69 Experiments This section evaluates the effectiveness and efficiency of the proposed tracking method. [sent-263, score-0.366]
</p><p>70 We have compared the proposed tracker with 12 state-of-the-art trackers (the implementations provided by the authors were used for fair comparisons). [sent-267, score-0.393]
</p><p>71 Three of them are multi-region based methods, including the fragment-based tracking method (Frag) [1], the articulating block and histogram tracker (BHT) [20] and the local-global tracker (LGT) [22]. [sent-268, score-1.127]
</p><p>72 field tracker (DFT) [19] and the multi-task sparse learning tracker (MTT) [28]. [sent-270, score-0.542]
</p><p>73 Quantitative Evaluation Two evaluation criteria are used in our experiments: center location error and tracking success rate, both computed against manually labeled ground truth. [sent-274, score-0.51]
</p><p>74 5, the tracking result of the current frame is considered as a success. [sent-277, score-0.366]
</p><p>75 Figure 4 shows the tracking performance of our method with respect to different numbers of regions. [sent-278, score-0.366]
</p><p>76 We observe that the tracking performance of our method reaches its peak when the number of regions reaches 400, thus we use 400 regions in the following experiments. [sent-280, score-0.556]
</p><p>77 Group 1 contains 6 sequences with illumination change and group 2 includes all remaining 14 sequences with other challenging factors. [sent-282, score-0.479]
</p><p>78 We then test the tracker on the two groups of video sequences to evaluate the performance of using our feature and using intensity. [sent-283, score-0.391]
</p><p>79 Figure 4 shows that the proposed feature outperforms intensity not only on sequences with illumination change, but also on sequences without illumination change. [sent-284, score-0.758]
</p><p>80 Table 1 and Table 2 show the tracking performance and the speed (in frame rate) of our method with the 12 other methods. [sent-286, score-0.366]
</p><p>81 We note that the TLD tracker does not report tracking result (or bounding box) when the drift problem occurs and the target object is re-detected. [sent-287, score-0.954]
</p><p>82 Thus we only report the center location errors for the sequences that the TLD method does not lose track of target objects. [sent-288, score-0.405]
</p><p>83 The proposed tracker performs favorably against the state-of-the-art algorithms as it achieve the best or the second best performance in most of sequences using both evaluation criteria. [sent-289, score-0.391]
</p><p>84 Figure 5 shows some tracking results of different trackers. [sent-292, score-0.366]
</p><p>85 We qualitatively evaluate the tracking results of these 20 sequences in four different ways as follows. [sent-299, score-0.486]
</p><p>86 The David Indoor and Trellis sequences contain gradual illumination changes and pose variation. [sent-304, score-0.409]
</p><p>87 We use the full sequences for better assessment of all tracking algorithms. [sent-308, score-0.486]
</p><p>88 Likewise, most of the other trackers do not perform well in the Shaking sequence since the object appearance changes drastically due to the stage light and sudden pose change. [sent-311, score-0.393]
</p><p>89 In addition, the proposed tracker performs well in the Basketball and Bolt sequences where the target objects undergo large pose variation. [sent-312, score-0.676]
</p><p>90 For the Woman sequence, the target object enclose the whole body instead of only upper body used in the fragment-based tracking method [1]. [sent-315, score-0.605]
</p><p>91 Most tracking methods do not perform well when the objects are heavily occluded. [sent-316, score-0.366]
</p><p>92 Few trackers recover from tracking drift since these methods focus on learning the appearance change. [sent-319, score-0.612]
</p><p>93 As only some regions are updated at any time instance by the proposed method, the tracking drift problem can be better handled where heavy occlusion occurs. [sent-333, score-0.632]
</p><p>94 The target objects in the Biker and Surfer2 sequences undergo large out of plane rotation with abrupt movement. [sent-335, score-0.441]
</p><p>95 The proposed algorithm is able  to track the baby well despite all the abrupt movement and 2Since we do not consider object scale here, only part of the sequences are used. [sent-338, score-0.337]
</p><p>96 The MIL tracker and the proposed algorithm perform well whereas the others fail to locate the target objects. [sent-345, score-0.476]
</p><p>97 Conclusion In this paper, we propose a novel locality sensitive histogram method and a simple yet effective tracking framework. [sent-347, score-1.001]
</p><p>98 Experimental results show that the proposed multi222444333311  Figure 5: Screenshots of the visual tracking results. [sent-348, score-0.366]
</p><p>99 region tracking algorithm performs  favorably  merous state-of-the-art  The proposed locality  algorithms. [sent-351, score-0.713]
</p><p>100 Eigentracking: Robust matching and tracking of articulated objects using a view-based representation. [sent-382, score-0.366]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tracking', 0.366), ('locality', 0.309), ('tracker', 0.271), ('target', 0.205), ('illumination', 0.203), ('histogram', 0.174), ('ip', 0.164), ('struck', 0.163), ('tld', 0.155), ('rp', 0.152), ('sensitive', 0.152), ('hpe', 0.147), ('trackers', 0.122), ('hpi', 0.121), ('sequences', 0.12), ('intensity', 0.112), ('histograms', 0.109), ('bp', 0.109), ('mtt', 0.101), ('mil', 0.098), ('dft', 0.097), ('regions', 0.095), ('pixel', 0.094), ('integral', 0.091), ('bht', 0.082), ('hps', 0.082), ('lgt', 0.082), ('drift', 0.078), ('bin', 0.074), ('bins', 0.072), ('template', 0.07), ('sp', 0.068), ('spt', 0.067), ('abrupt', 0.067), ('frag', 0.064), ('vtd', 0.064), ('success', 0.063), ('drastic', 0.061), ('mover', 0.061), ('emd', 0.061), ('affine', 0.058), ('occlusion', 0.055), ('changes', 0.055), ('forgetting', 0.055), ('nlpeft', 0.055), ('nrpight', 0.055), ('shengfeng', 0.055), ('np', 0.054), ('bilateral', 0.053), ('earth', 0.051), ('baby', 0.051), ('factors', 0.05), ('undergo', 0.049), ('vote', 0.049), ('cityu', 0.049), ('declines', 0.049), ('location', 0.047), ('iq', 0.047), ('normalization', 0.046), ('appearance', 0.046), ('invariant', 0.045), ('adaptively', 0.045), ('lau', 0.045), ('articulating', 0.045), ('ct', 0.044), ('eigentracking', 0.042), ('jitter', 0.04), ('pixels', 0.04), ('facilitates', 0.039), ('trellis', 0.039), ('pages', 0.038), ('heavy', 0.038), ('region', 0.038), ('resides', 0.037), ('numerous', 0.037), ('exponentially', 0.036), ('sudden', 0.036), ('change', 0.036), ('account', 0.036), ('occurrence', 0.035), ('woman', 0.035), ('kwon', 0.035), ('dissimilarity', 0.035), ('drastically', 0.035), ('invariants', 0.034), ('sequence', 0.034), ('computed', 0.034), ('object', 0.034), ('summation', 0.033), ('track', 0.033), ('ap', 0.032), ('interval', 0.032), ('operation', 0.032), ('robust', 0.032), ('rectangle', 0.032), ('movement', 0.032), ('pose', 0.031), ('conventional', 0.031), ('candidate', 0.03), ('window', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="457-tfidf-1" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>2 0.44316724 <a title="457-tfidf-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.34137565 <a title="457-tfidf-3" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>4 0.32006297 <a title="457-tfidf-4" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>5 0.28654435 <a title="457-tfidf-5" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>Author: unkown-author</p><p>Abstract: We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the “right” frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.</p><p>6 0.23995966 <a title="457-tfidf-6" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>7 0.22821175 <a title="457-tfidf-7" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>8 0.21471965 <a title="457-tfidf-8" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>9 0.19879922 <a title="457-tfidf-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.19443318 <a title="457-tfidf-10" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>11 0.16518365 <a title="457-tfidf-11" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>12 0.16338937 <a title="457-tfidf-12" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>13 0.1483667 <a title="457-tfidf-13" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>14 0.14291735 <a title="457-tfidf-14" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>15 0.14238791 <a title="457-tfidf-15" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>16 0.13636073 <a title="457-tfidf-16" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>17 0.11964057 <a title="457-tfidf-17" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>18 0.10220528 <a title="457-tfidf-18" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>19 0.099596038 <a title="457-tfidf-19" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>20 0.097463213 <a title="457-tfidf-20" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, 0.024), (2, -0.018), (3, -0.087), (4, 0.008), (5, -0.088), (6, 0.206), (7, -0.207), (8, 0.144), (9, 0.245), (10, -0.157), (11, -0.177), (12, -0.144), (13, 0.165), (14, -0.046), (15, -0.054), (16, 0.021), (17, -0.08), (18, 0.102), (19, 0.041), (20, 0.117), (21, 0.026), (22, 0.051), (23, -0.094), (24, -0.073), (25, -0.035), (26, -0.035), (27, 0.001), (28, 0.049), (29, 0.066), (30, 0.033), (31, 0.011), (32, 0.05), (33, 0.034), (34, -0.075), (35, -0.062), (36, -0.021), (37, 0.087), (38, 0.005), (39, 0.06), (40, -0.061), (41, 0.046), (42, 0.017), (43, 0.006), (44, 0.054), (45, -0.053), (46, -0.026), (47, 0.042), (48, -0.047), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97209501 <a title="457-lsi-1" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>2 0.94660312 <a title="457-lsi-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.85455716 <a title="457-lsi-3" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><p>4 0.83367872 <a title="457-lsi-4" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>Author: Dong Wang, Huchuan Lu, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods, the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution, which can be solved efficiently. Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.</p><p>5 0.8143453 <a title="457-lsi-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.79612237 <a title="457-lsi-6" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>7 0.77200139 <a title="457-lsi-7" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>8 0.75516391 <a title="457-lsi-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.6940524 <a title="457-lsi-9" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>10 0.65358317 <a title="457-lsi-10" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>11 0.62831014 <a title="457-lsi-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.59182823 <a title="457-lsi-12" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>13 0.58895332 <a title="457-lsi-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.55967516 <a title="457-lsi-14" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>15 0.51551104 <a title="457-lsi-15" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>16 0.49645534 <a title="457-lsi-16" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>17 0.49297172 <a title="457-lsi-17" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>18 0.49273774 <a title="457-lsi-18" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>19 0.4711864 <a title="457-lsi-19" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>20 0.41295382 <a title="457-lsi-20" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.176), (16, 0.037), (26, 0.047), (27, 0.033), (28, 0.013), (33, 0.306), (42, 0.12), (67, 0.09), (69, 0.035), (87, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95347625 <a title="457-lda-1" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>Author: Yu Ji, Jinwei Ye, Jingyi Yu</p><p>Abstract: Transparent gas flows are difficult to reconstruct: the refractive index field (RIF) within the gas volume is uneven and rapidly evolving, and correspondence matching under distortions is challenging. We present a novel computational imaging solution by exploiting the light field probe (LFProbe). A LF-probe resembles a view-dependent pattern where each pixel on the pattern maps to a unique ray. By . ude l. edu observing the LF-probe through the gas flow, we acquire a dense set of ray-ray correspondences and then reconstruct their light paths. To recover the RIF, we use Fermat’s Principle to correlate each light path with the RIF via a Partial Differential Equation (PDE). We then develop an iterative optimization scheme to solve for all light-path PDEs in conjunction. Specifically, we initialize the light paths by fitting Hermite splines to ray-ray correspondences, discretize their PDEs onto voxels, and solve a large, over-determined PDE system for the RIF. The RIF can then be used to refine the light paths. Finally, we alternate the RIF and light-path estimations to improve the reconstruction. Experiments on synthetic and real data show that our approach can reliably reconstruct small to medium scale gas flows. In particular, when the flow is acquired by a small number of cameras, the use of ray-ray correspondences can greatly improve the reconstruction.</p><p>2 0.94464892 <a title="457-lda-2" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>3 0.94090551 <a title="457-lda-3" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>4 0.93882024 <a title="457-lda-4" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>5 0.93874198 <a title="457-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.93817353 <a title="457-lda-6" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>7 0.9378159 <a title="457-lda-7" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>8 0.9368853 <a title="457-lda-8" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>same-paper 9 0.93608493 <a title="457-lda-9" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>10 0.93535805 <a title="457-lda-10" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>11 0.93517756 <a title="457-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.93120956 <a title="457-lda-12" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>13 0.9311536 <a title="457-lda-13" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>14 0.92954201 <a title="457-lda-14" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>15 0.92900145 <a title="457-lda-15" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>16 0.92881596 <a title="457-lda-16" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>17 0.92848235 <a title="457-lda-17" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>18 0.92813176 <a title="457-lda-18" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>19 0.92806935 <a title="457-lda-19" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>20 0.92796838 <a title="457-lda-20" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
