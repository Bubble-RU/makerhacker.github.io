<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>368 cvpr-2013-Rolling Shutter Camera Calibration</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-368" href="#">cvpr2013-368</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>368 cvpr-2013-Rolling Shutter Camera Calibration</h1>
<br/><p>Source: <a title="cvpr-2013-368-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oth_Rolling_Shutter_Camera_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Luc Oth, Paul Furgale, Laurent Kneip, Roland Siegwart</p><p>Abstract: Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devices—from smart-phones to high-end cameras. It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.</p><p>Reference: <a title="cvpr-2013-368-reference" href="../cvpr2013_reference/cvpr-2013-Rolling_Shutter_Camera_Calibration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. [sent-9, score-0.338]
</p><p>2 The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. [sent-10, score-1.012]
</p><p>3 The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. [sent-11, score-0.366]
</p><p>4 We present a new method that only requires video of a known calibration pattern. [sent-12, score-0.249]
</p><p>5 Introduction Most consumer electronic devices—such as smartphones—leave no room for a mechanical global shutter (GS). [sent-15, score-0.331]
</p><p>6 The electronic shutter of CCD sensors exposes every row of the image during the same timespan, similar to a mechanical shutter. [sent-17, score-0.331]
</p><p>7 CMOS sensors often only have a rolling shutter (RS). [sent-18, score-0.509]
</p><p>8 The image rows are exposed and read sequentially, which introduces significant image distortions if either the camera or the scene are in motion. [sent-19, score-0.228]
</p><p>9 Using this assumption, the timestamp of each line is uniquely defined by the line delay and the frame time. [sent-22, score-0.89]
</p><p>10 The RS camera may be interpreted as a high frequency sensor returning sparse spatial information with a dense temporal coverage encoded by distortions [2]. [sent-24, score-0.232]
</p><p>11 Conventional structure from motion (SfM) methods estimate a single camera pose per frame. [sent-25, score-0.333]
</p><p>12 The reprojected chessboard corners after a continuoustime camera pose estimation. [sent-27, score-0.281]
</p><p>13 The red dots show the corner positions for our continuous-time rolling shutter model. [sent-28, score-0.509]
</p><p>14 The green dots show the results with a discrete-time global shutter model. [sent-29, score-0.305]
</p><p>15 A common solution is to only estimate one pose per frame and perform a linear interpolation between consecutive camera poses to approximate the intermediate states. [sent-32, score-0.34]
</p><p>16 However, to perform SfM using a rolling shutter camera, the line delay must be known accurately. [sent-36, score-1.221]
</p><p>17 Hence, the line delay must be discovered through a calibration process. [sent-38, score-0.897]
</p><p>18 This paper presents an offline calibration procedure to determine the rolling shutter line delay for use in high accuracy SfM, such as the work of [9]. [sent-39, score-1.406]
</p><p>19 [9] shows that SfM on wobble-corrected images  performs worse (in terms ofpose estimation accuracy) when compared to a continuous motion parametrisation operating on raw images with a calibrated RS model. [sent-43, score-0.225]
</p><p>20 In light of this, high-accuracy rolling shutter calibration that does not need specialised hardware (as in [14, 15]) is highly desirable. [sent-44, score-0.829]
</p><p>21 The current state of the art for line delay calibration 1 1 13 3 35 56 80 8  was proposed by Geyer et al. [sent-45, score-0.897]
</p><p>22 The resulting images include light and dark lines whose spatial frequency is linked to the line delay and the known LED frequency. [sent-48, score-0.712]
</p><p>23 We propose to use standard GS calibration means to also calibrate the RS line delay. [sent-50, score-0.378]
</p><p>24 First, the intrinsics and distortion coefficients of a RS camera are determined with GS methods using still images and the help of a known pattern. [sent-51, score-0.306]
</p><p>25 Next, RS distortion generating camera motion is produced in front of the known pattern. [sent-52, score-0.314]
</p><p>26 The resulting video sequence is used to simultaneously estimate the camera pose in continuous-time and the line delay. [sent-53, score-0.406]
</p><p>27 This method removes the dependency on expensive hardware and we are able to show that the resulting line delay estimates are more accurate than those produced by the method of Geyer et al. [sent-54, score-0.722]
</p><p>28 we propose to use a continuous-time trajectory model combined with a rolling shutter camera model;  2. [sent-57, score-0.689]
</p><p>29 we propose a new method for RS calibration that requires no additional hardware except for a known pattern e. [sent-58, score-0.291]
</p><p>30 we parametrise the pose of the camera as a fourthorder B-spline and propose the first scheme for adaptively choosing the number of B-spline basis functions needed to represent different parts of the trajectory. [sent-62, score-0.24]
</p><p>31 Additionally, they propose the current state-of-the-art method for line delay calibration. [sent-66, score-0.68]
</p><p>32 The precise knowledge of the LED frequency is essential for a successful calibration and the authors suggest to remove the lens for best sensor illumination. [sent-68, score-0.256]
</p><p>33 Besides the line  delay calibration proposed by Geyer et al. [sent-70, score-0.897]
</p><p>34 [1] propose a method to estimate the full calibration of a rigidly attached camera and gyroscope, including the camera’s line delay. [sent-72, score-0.553]
</p><p>35 [2] stop considering the RS artefacts as drawback and exploit them to simultaneously extract the pose and velocity of an object relative to the camera frame from a single view. [sent-74, score-0.302]
</p><p>36 They first estimate a constant camera velocity between two consecutive frames which is then used to undistort the RS keypoints. [sent-80, score-0.317]
</p><p>37 The camera trajectory is described by linearly interpolating between the camera poses at the beginning of  each frame. [sent-84, score-0.325]
</p><p>38 For solving the perspective pose problem, they suggest to use a multi-frame PnP solver and simultaneously estimate the pose of the camera in multiple frames. [sent-96, score-0.361]
</p><p>39 The pose parametrisations proposed in earlier publications are however not optimal as they assume a constant velocity between two consecutive discrete camera poses. [sent-98, score-0.38]
</p><p>40 The camera motion is thus assumed to be linear, which stands in conflict with a general rapid motion. [sent-99, score-0.233]
</p><p>41 We propose a new calibration approach involving only a known pattern as also used for the intrinsics and distortion coefficient calibration. [sent-106, score-0.352]
</p><p>42 Next, we propose a continuous-time perspective projection model for RS cameras derived from the general projection equations, and give the related perspective localisation theory. [sent-109, score-0.296]
</p><p>43 We derive the equations for line delay estimation and combine them with the perspective pose equations to simultaneously estimate the camera pose and the line delay based on a set of known landmarks. [sent-110, score-1.843]
</p><p>44 Beginning the integration at the first line, and assuming that the line delay, d, remains constant, the time of exposure of the vth line becomes: t = t¯ + vd. [sent-146, score-0.367]
</p><p>45 Reprojection Error Modelling In the following, we derive the reprojection error terms for the RS case and build the perspective localisation problem. [sent-153, score-0.304]
</p><p>46 Perspective localisation is widely known and aims at minimising the squared reprojection error of a set of known landmarks (k) in every frame (i), weighted by the inverse covariance matrix ofthe error, R¯k,i. [sent-154, score-0.482]
</p><p>47 Th∼e Nm(i0ni,mRum of (7) is found using a Gauss-Newton based optimisation which requires the linearisation of the error terms with respect to the estimated quantity, c: ek,i = yik π ? [sent-159, score-0.28]
</p><p>48 (11) We define the nominal error ek,i = yik − π ? [sent-163, score-0.336]
</p><p>49 Shutter Calibration To estimate the line delay, we linearise the error term (9) with respect to small changes? [sent-175, score-0.261]
</p><p>50 (13) To simplify the expressions equation (13) is merged with (11) to obtain a single equation for simultaneous line delay and pose estimation:  ek,i≈¯ ek,i−Jπ? [sent-184, score-0.75]
</p><p>51 Error Term Standardisation The standardisation of the error terms aims at scaling every term by its inverse covariance matrix such that all terms end up with unit variance [17]. [sent-191, score-0.223]
</p><p>52 We approximate the covariance of the error terms by linearisation of the measurement equations (8). [sent-199, score-0.286]
</p><p>53 The expected value of the error terms is zero, E [ek,i] = 0, and the second condition—Gaussian error term variance—is given, as the Gaussian noise, nk,i, is linearly mapped onto the error terms. [sent-221, score-0.237]
</p><p>54 The influence of the off-diagonal terms becomes significant for most fast motion patterns: small changes in the feature row may, under rapid motion, induce a large change in the camera position and propagate back to a large change in the feature position in the image plane. [sent-229, score-0.372]
</p><p>55 (20) (21)  Please note that the covariance is not constant in time and needs to be re-evaluated each time the line delay changes during estimation. [sent-238, score-0.77]
</p><p>56 To visualise the effect of motion on the error covariance ellipse we plot several examples in Figure 2. [sent-239, score-0.248]
</p><p>57 In this example, the camera x-axis is aligned with the image u coordinates, parallel to the sensor rows, the camera y-axis is aligned with the v coordinates, aligned with the line scanning direction, and the camera z-axis points towards the landmarks in the scene. [sent-240, score-0.72]
</p><p>58 If the camera moves along the x-axis, the position of the camera also changes. [sent-243, score-0.347]
</p><p>59 Figure 2 (b) shows the covariance ellipses if the cam1 1 13 3 36 6 613 1  era motion is parallel to the line scanning direction. [sent-245, score-0.379]
</p><p>60 If the camera motion has the same direction as the line scanning, the timespan during which a landmark is projected onto a specific line is enlarged. [sent-246, score-0.682]
</p><p>61 The covariance matrix may, however, become degenerate for fast motion along the line scanning direction. [sent-248, score-0.379]
</p><p>62 Moving the camera in the opposite direction reduces the timespan during which a feature is projected onto a same line. [sent-250, score-0.223]
</p><p>63 We show that a weak motion model is important in continuous-time estimation and discuss the problems related to a prior in the context of line delay calibration. [sent-279, score-0.768]
</p><p>64 Finally, we propose our adaptive knot placement algorithm which enables precise calibration results despite a motion prior. [sent-280, score-0.406]
</p><p>65 Furthermore, we assume a zero frame-delay to initialise the line delay as d0 = fp1s N1R, where fps is the average number of Frames Per Second and NR the number of rows of the sensor. [sent-288, score-0.68]
</p><p>66 The cost is constantly reduced by overfitting the reprojection errors while the estimated line delay starts to deviate from the nominal value. [sent-341, score-0.928]
</p><p>67 shows how a motion prior improves the stability of the estimated camera position in a general setup. [sent-344, score-0.29]
</p><p>68 Figure 4 shows the line delay estimates for an increasing number ofuniformly spaced knots. [sent-351, score-0.68]
</p><p>69 As the line delay is a physical parameter, we expect its value to be independent of the motion parametrisation as long as enough representational power is available to accurately describe the motion. [sent-352, score-0.905]
</p><p>70 However, in Figure 4 we see that the line delay estimate stabilises between 600 and 1000 knots but then begins to diverge as more knots allow the curve to over fit. [sent-353, score-0.954]
</p><p>71 We conclude that the selection of an appropriate number of knots becomes important for line delay calibration. [sent-355, score-0.802]
</p><p>72 We propose an adaptive knot placement method that avoids over-fitting by adaptively adjusting the number of knots until the residuals agree with their theoretical expected value. [sent-356, score-0.223]
</p><p>73 When J > E [J] for some segment of the trajectory, this implies that the spline does not have enough knots to be able to represent the motion in that segment. [sent-374, score-0.252]
</p><p>74 This adaptive knot picking scheme was crucial for us to be able to estimate the line delay over a wide range of datasets. [sent-376, score-0.786]
</p><p>75 Calibration Algorithm Finally, we can outline our complete algorithm for line delay calibration: 1. [sent-379, score-0.68]
</p><p>76 Collect a video sequence of a known calibration pattern and extract keypoint measurements. [sent-380, score-0.249]
</p><p>77 For all three types we collected a series of datasets by moving the camera in front of a calibration pattern for 30– 60s. [sent-391, score-0.362]
</p><p>78 The camera is attached to a large rig equipped with Vicon motion capture system markers for ground truth. [sent-393, score-0.257]
</p><p>79 The calibration process as well as the 1 1 13 3 36 6 635 3  perspective localisation datasets used chessboard corners, with known relative positions as landmarks, extracted using OpenCV. [sent-394, score-0.447]
</p><p>80 The camera-rig transformation and the time synchronisation are only required for validation against the ground truth and have no influence on the calibration or localisation tasks themselves. [sent-397, score-0.328]
</p><p>81 For the calibration of the intrinsics and the distortion coefficients of the RS camera the OpenCV tools were used. [sent-398, score-0.491]
</p><p>82 Continuous-Time Localisation Our first experiment is designed to show that our algorithm is capable of estimating the motion of a rolling shutter camera; simultaneous calibration ofthe line delay is not performed. [sent-402, score-1.494]
</p><p>83 The line delay is first calibrated using the approach proposed by Geyer et al. [sent-403, score-0.68]
</p><p>84 We chose a low framerate (≈ 7fps) for which the line delay lies around d = 137μs. [sent-405, score-0.68]
</p><p>85 A significant increase of the position error is however unpreventable as no information on the camera pose is available. [sent-413, score-0.342]
</p><p>86 As this publication only presents a calibration method a thorough analysis of the required motion patterns for a successful calibration is left for future research. [sent-418, score-0.522]
</p><p>87 The nominal values were obtained using the relation between the pixel clock (P [Hz]) and the time required for reading a single line containing NP pixels given in the data sheet: 3to quantify the errors  an  Euler Roll-Pitch-Yaw parametrisation is used Position SSE Error  m]m[1200000 d]ra[000. [sent-421, score-0.49]
</p><p>88 Deviation of the calibration results with our approach and the Geyer approach from the nominal values based on 40 real datasets at different values of pixel-clock. [sent-433, score-0.363]
</p><p>89 The numerical values in Table 1 and the error histogram in Figure 6 both confirm that our approach delivers line delay estimates close to the nominal values. [sent-436, score-0.896]
</p><p>90 We expect a linear relation between the line delay and the pixel timing (μs) passing through the origin. [sent-438, score-0.68]
</p><p>91 The behaviour of the line delay as a function of the pixel clock is more accurately described with our results. [sent-440, score-0.726]
</p><p>92 t7o2y81i0m3e2nrpai-  son between our approach, the Geyer calibration and the nominal values. [sent-454, score-0.363]
</p><p>93 The line delay d and the standard deviation σ are given in μs. [sent-468, score-0.68]
</p><p>94 Conclusion and Future Work In this paper, we derived a new method of estimating the line delay of a rolling shutter camera, using only video images of a chessboard or other calibration pattern with known geometry. [sent-475, score-1.504]
</p><p>95 The line delay calibration method outperforms the current state-of-the-art technique [14] without requiring any specialised hardware. [sent-477, score-0.958]
</p><p>96 The complexity of the simultaneous calibration of the camera intrinsics and distortion parameters remains an open problem which should be addressed by further research. [sent-478, score-0.465]
</p><p>97 Besides that, a series of RS cameras influence the line delay by tuning the exposure while recording a video. [sent-479, score-0.794]
</p><p>98 We believe that our approach could be modified to an online line delay estimation to take full advantage of the sensors. [sent-480, score-0.68]
</p><p>99 Digital video stabilization and rolling shutter correction using gyroscopes. [sent-488, score-0.509]
</p><p>100 Simultaneous object pose and velocity computation using a single view from a rolling shutter camera. [sent-495, score-0.641]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('delay', 0.519), ('shutter', 0.305), ('calibration', 0.217), ('rs', 0.213), ('rolling', 0.204), ('gs', 0.167), ('line', 0.161), ('tik', 0.155), ('vkid', 0.154), ('geyer', 0.152), ('nominal', 0.146), ('camera', 0.145), ('parametrisation', 0.137), ('knots', 0.122), ('pnp', 0.12), ('yik', 0.12), ('reprojection', 0.102), ('covariance', 0.09), ('pk', 0.089), ('motion', 0.088), ('localisation', 0.086), ('hedborg', 0.086), ('knot', 0.076), ('sfm', 0.073), ('pose', 0.07), ('error', 0.07), ('furgale', 0.069), ('ringaby', 0.069), ('chessboard', 0.066), ('velocity', 0.062), ('forss', 0.061), ('specialised', 0.061), ('position', 0.057), ('sse', 0.057), ('aik', 0.056), ('optimisation', 0.056), ('intrinsics', 0.054), ('timespan', 0.051), ('vicon', 0.051), ('batch', 0.049), ('distortion', 0.049), ('landmark', 0.049), ('distortions', 0.048), ('measurement', 0.047), ('perspective', 0.046), ('karpenko', 0.046), ('clock', 0.046), ('undistort', 0.046), ('exposure', 0.045), ('landmarks', 0.045), ('equations', 0.045), ('cameras', 0.044), ('bundle', 0.043), ('hardware', 0.042), ('spline', 0.042), ('scanning', 0.04), ('grey', 0.04), ('sensor', 0.039), ('measurements', 0.037), ('projection', 0.037), ('interpolation', 0.036), ('modelling', 0.035), ('tj', 0.035), ('exposed', 0.035), ('publications', 0.035), ('trajectory', 0.035), ('cholmod', 0.034), ('flashing', 0.034), ('linearisation', 0.034), ('parametrisations', 0.034), ('slerp', 0.034), ('standardisation', 0.034), ('consecutive', 0.034), ('adjustment', 0.033), ('offered', 0.033), ('known', 0.032), ('keypoints', 0.032), ('lourakis', 0.03), ('estimate', 0.03), ('variance', 0.029), ('klein', 0.029), ('forty', 0.028), ('undistortion', 0.028), ('vki', 0.028), ('onto', 0.027), ('kno', 0.027), ('electronic', 0.026), ('coefficients', 0.026), ('ch', 0.026), ('kid', 0.025), ('vik', 0.025), ('cholesky', 0.025), ('basis', 0.025), ('influence', 0.025), ('frame', 0.025), ('placement', 0.025), ('initialised', 0.024), ('sheet', 0.024), ('timestamp', 0.024), ('rig', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="368-tfidf-1" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>Author: Luc Oth, Paul Furgale, Laurent Kneip, Roland Siegwart</p><p>Abstract: Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devices—from smart-phones to high-end cameras. It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.</p><p>2 0.17145176 <a title="368-tfidf-2" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>3 0.16112451 <a title="368-tfidf-3" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>Author: Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi</p><p>Abstract: Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. In this paper, we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i.e., analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling.</p><p>4 0.1199027 <a title="368-tfidf-4" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>5 0.10994425 <a title="368-tfidf-5" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>Author: Jonathan Balzer, Stefano Soatto</p><p>Abstract: We describe a method to efficiently generate a model (map) of small-scale objects from video. The map encodes sparse geometry as well as coarse photometry, and could be used to initialize dense reconstruction schemes as well as to support recognition and localization of three-dimensional objects. Self-occlusions and the predominance of outliers present a challenge to existing online Structure From Motion and Simultaneous Localization and Mapping systems. We propose a unified inference criterion that encompasses map building and localization (object detection) relative to the map in a coupled fashion. We establish correspondence in a computationally efficient way without resorting to combinatorial matching or random-sampling techniques. Instead, we use a simpler M-estimator that exploits putative correspondence from tracking after photometric and topological validation. We have collected a new dataset to benchmark model building in the small scale, which we test our algorithm on in comparison to others. Although our system is significantly leaner than previous ones, it compares favorably to the state of the art in terms of accuracy and robustness.</p><p>6 0.10991271 <a title="368-tfidf-6" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>7 0.10847858 <a title="368-tfidf-7" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>8 0.10752964 <a title="368-tfidf-8" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>9 0.10622587 <a title="368-tfidf-9" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>10 0.098958999 <a title="368-tfidf-10" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>11 0.094568074 <a title="368-tfidf-11" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>12 0.094533525 <a title="368-tfidf-12" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>13 0.0911415 <a title="368-tfidf-13" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>14 0.089275122 <a title="368-tfidf-14" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>15 0.087516174 <a title="368-tfidf-15" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>16 0.086804144 <a title="368-tfidf-16" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>17 0.085888192 <a title="368-tfidf-17" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>18 0.084489182 <a title="368-tfidf-18" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>19 0.081583068 <a title="368-tfidf-19" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>20 0.080877066 <a title="368-tfidf-20" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.158), (1, 0.127), (2, -0.015), (3, -0.004), (4, -0.03), (5, -0.037), (6, 0.005), (7, -0.066), (8, 0.043), (9, 0.034), (10, -0.021), (11, 0.124), (12, 0.071), (13, -0.06), (14, -0.069), (15, -0.026), (16, 0.053), (17, 0.085), (18, -0.078), (19, 0.018), (20, 0.073), (21, 0.008), (22, -0.078), (23, -0.054), (24, -0.006), (25, 0.019), (26, -0.026), (27, 0.049), (28, -0.009), (29, 0.053), (30, -0.026), (31, 0.008), (32, -0.094), (33, 0.076), (34, -0.089), (35, 0.044), (36, 0.007), (37, -0.073), (38, -0.017), (39, 0.069), (40, 0.027), (41, 0.023), (42, -0.056), (43, -0.035), (44, -0.048), (45, -0.076), (46, 0.022), (47, -0.028), (48, -0.028), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9508763 <a title="368-lsi-1" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>Author: Luc Oth, Paul Furgale, Laurent Kneip, Roland Siegwart</p><p>Abstract: Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devices—from smart-phones to high-end cameras. It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.</p><p>2 0.78066969 <a title="368-lsi-2" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>3 0.77854121 <a title="368-lsi-3" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>4 0.76239151 <a title="368-lsi-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.75725847 <a title="368-lsi-5" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>6 0.74518996 <a title="368-lsi-6" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>7 0.7424109 <a title="368-lsi-7" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>8 0.72201794 <a title="368-lsi-8" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>9 0.7045905 <a title="368-lsi-9" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>10 0.70119649 <a title="368-lsi-10" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>11 0.68420923 <a title="368-lsi-11" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>12 0.67590243 <a title="368-lsi-12" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>13 0.59235537 <a title="368-lsi-13" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>14 0.56382406 <a title="368-lsi-14" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>15 0.56256509 <a title="368-lsi-15" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>16 0.55329579 <a title="368-lsi-16" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>17 0.54363561 <a title="368-lsi-17" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>18 0.54175472 <a title="368-lsi-18" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>19 0.54148519 <a title="368-lsi-19" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>20 0.53638071 <a title="368-lsi-20" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.105), (16, 0.022), (26, 0.058), (33, 0.258), (43, 0.254), (67, 0.052), (69, 0.044), (87, 0.079), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8834703 <a title="368-lda-1" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>Author: Hang Chang, Alexander Borowsky, Paul Spellman, Bahram Parvin</p><p>Abstract: Image-based classification oftissue histology, in terms of different components (e.g., normal signature, categories of aberrant signatures), provides a series of indices for tumor composition. Subsequently, aggregation of these indices in each whole slide image (WSI) from a large cohort can provide predictive models of clinical outcome. However, the performance of the existing techniques is hindered as a result of large technical and biological variations that are always present in a large cohort. In this paper, we propose two algorithms for classification of tissue histology based on robust representations of morphometric context, which are built upon nuclear level morphometric features at various locations and scales within the spatial pyramid matching (SPM) framework. These methods have been evaluated on two distinct datasets of different tumor types collected from The Cancer Genome Atlas (TCGA), and the experimental results indicate that our methods are (i) extensible to different tumor types; (ii) robust in the presence of wide technical and biological variations; (iii) invariant to different nuclear segmentation strategies; and (iv) scalable with varying training sample size. In addition, our experiments suggest that enforcing sparsity, during the construction of morphometric context, further improves the performance of the system.</p><p>same-paper 2 0.83242887 <a title="368-lda-2" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>Author: Luc Oth, Paul Furgale, Laurent Kneip, Roland Siegwart</p><p>Abstract: Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devices—from smart-phones to high-end cameras. It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.</p><p>3 0.79085612 <a title="368-lda-3" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>Author: Wangmeng Zuo, Lei Zhang, Chunwei Song, David Zhang</p><p>Abstract: Image denoising is a classical yet fundamental problem in low level vision, as well as an ideal test bed to evaluate various statistical image modeling methods. One of the most challenging problems in image denoising is how to preserve the fine scale texture structures while removing noise. Various natural image priors, such as gradient based prior, nonlocal self-similarity prior, and sparsity prior, have been extensively exploited for noise removal. The denoising algorithms based on these priors, however, tend to smooth the detailed image textures, degrading the image visual quality. To address this problem, in this paper we propose a texture enhanced image denoising (TEID) method by enforcing the gradient distribution of the denoised image to be close to the estimated gradient distribution of the original image. A novel gradient histogram preservation (GHP) algorithm is developed to enhance the texture structures while removing noise. Our experimental results demonstrate that theproposed GHP based TEID can well preserve the texture features of the denoised images, making them look more natural.</p><p>4 0.76981181 <a title="368-lda-4" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>5 0.75907731 <a title="368-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.75794947 <a title="368-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.75689143 <a title="368-lda-7" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>8 0.75678205 <a title="368-lda-8" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>9 0.75596702 <a title="368-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.75448334 <a title="368-lda-10" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>11 0.75418633 <a title="368-lda-11" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>12 0.75373805 <a title="368-lda-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.75362647 <a title="368-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.75336552 <a title="368-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.75322092 <a title="368-lda-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.75290656 <a title="368-lda-16" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>17 0.7528559 <a title="368-lda-17" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>18 0.75277364 <a title="368-lda-18" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>19 0.7525866 <a title="368-lda-19" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>20 0.75239611 <a title="368-lda-20" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
