<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-64" href="#">cvpr2013-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</h1>
<br/><p>Source: <a title="cvpr-2013-64-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Blessing_of_Dimensionality_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Dong Chen, Xudong Cao, Fang Wen, Jian Sun</p><p>Abstract: Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. This prevents further exploration of the use of a highdimensional feature. In this paper, we study the performance of a highdimensional feature. We first empirically show that high dimensionality is critical to high performance. A 100K-dim feature, based on a single-type Local Binary Pattern (LBP) descriptor, can achieve significant improvements over both its low-dimensional version and the state-of-the-art. We also make the high-dimensional feature practical. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.</p><p>Reference: <a title="cvpr-2013-64-reference" href="../cvpr2013_reference/cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. [sent-6, score-0.396]
</p><p>2 With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality. [sent-12, score-0.835]
</p><p>3 Introduction Modern face verification pipelines mainly consist of two stages: extracting low-level features, and building classification models. [sent-14, score-0.264]
</p><p>4 The second stage usually exploits supervised information to learn a classification model [10, 26, 30], discriminative subspace [3, 26, 36], or mid-level representation [4, 24, 34, 38]. [sent-16, score-0.386]
</p><p>5 In short, we densely sample multi-scale descriptors centered at dense facial landmarks and concatenate them. [sent-29, score-0.488]
</p><p>6 To make high-dimensional feature really useful, we propose a simple two-step scheme for obtaining a sparse linear projection. [sent-38, score-0.301]
</p><p>7 In the first step, any conventional subspace learning methods can be applied to get the compressed, lowdimensional feature. [sent-39, score-0.28]
</p><p>8 In the second step, we adopt 푙1 regres-  sion to learn a sparse project matrix which maps the feature from the original high dimension to low dimension. [sent-40, score-0.484]
</p><p>9 , Euclidean and Cosine) are invariant to a rotation transformation, we further introduce an additional freedom of rotation in the mapping. [sent-43, score-0.211]
</p><p>10 Our method, called Rotated Sparse Regression, can reduce the cost of linear projection and its storage 1Under unrestricted protocol;  no  outside training data in recognition  system. [sent-44, score-0.455]
</p><p>11 Related Works Since the topics covered in face recognition literature are numerous, we focus on two most-related aspects. [sent-49, score-0.235]
</p><p>12 In unsupervised feature learning, densely sampling overlapped image patches [5, 12] consistently improve performance. [sent-51, score-0.315]
</p><p>13 Examples include multi-scale LBP [9] and multi-scale SIFT [18, 19] for face recognition, Gist descriptor for image retrieval [14], and scene classification [32, 35]. [sent-56, score-0.237]
</p><p>14 Two common approaches for compressing features are feature selection and the subspace method. [sent-58, score-0.36]
</p><p>15 It is usually formulated in a greedy way such as boosting [15], or in a more principled way by enforcing 푙1 penalty [20] or structure sparsity [28]. [sent-60, score-0.324]
</p><p>16 It can be implemented as an unsupervised [21, 36] or supervised subspace methods [3, 10, 26]. [sent-62, score-0.348]
</p><p>17 For linear subspace methods, the high-dimensional feature is projected into a lowdimensional subspace with a linear projection. [sent-63, score-0.533]
</p><p>18 developed a sparse version of PCA [41] and LDA [11] by adding a sparse penalty and formulating them as elastic net problems [40]. [sent-65, score-0.49]
</p><p>19 However, the  additional sparse penalty often makes the original optimization method inapplicable. [sent-66, score-0.273]
</p><p>20 This drawback could become an insurmountable obstacle when trying to enforce sparsity to other more sophisticated subspace learning methods. [sent-67, score-0.342]
</p><p>21 High-dimensional  Feature  is Necessary  In this section, we describe our construction of the highdimensional feature in detail and study its accuracy though  (a)  (b) Figure 1. [sent-69, score-0.263]
</p><p>22 The small scale describes the detailed appearance around the fiducial points and the large scale captures the shape of face in relative large range. [sent-72, score-0.341]
</p><p>23 Constructing high-dimensional feature We construct the feature simply by extracting multi-scale patches centered at dense facial landmarks. [sent-76, score-0.418]
</p><p>24 We first locate dense facial landmarks with a recent face alignment method [7] and rectify similarity transformation based on five landmarks (eyes, nose, and mouth corners). [sent-77, score-1.054]
</p><p>25 Our feature is based on accurate and dense facial landmarks. [sent-83, score-0.248]
</p><p>26 This is only possible with recent great progress made in face alignment (i. [sent-84, score-0.265]
</p><p>27 Using sampling or regression techniques, today’s face alignment methods can output both accurate and dense landmarks on faces in the wild. [sent-87, score-0.885]
</p><p>28 We select landmarks of the inner face due to their relatively high accuracy and reliability. [sent-89, score-0.527]
</p><p>29 Figure 1 (a) (from sparse to dense) shows the landmarks we used for feature extraction, which are salient points on the eye brows, eyes, nose and mouth. [sent-90, score-0.631]
</p><p>30 High dimensionality leads to high performance In this section, we investigate the effect of the dimensionality of our feature on face verification accuracy. [sent-103, score-0.693]
</p><p>31 We use the LFW benchmark, following its unrestricted protocol [23]. [sent-104, score-0.321]
</p><p>32 In this experiment, the feature dimension is increased by varying landmark numbers from 5 to 27 and sampling scales from 1to 5. [sent-108, score-0.321]
</p><p>33 To effectively apply a supervised learning method in the second stage, the dimension of these features is reduced to 400 by PCA2. [sent-109, score-0.235]
</p><p>34 We believe the results of the high performance of highdimensional feature are due to a few reasons. [sent-113, score-0.295]
</p><p>35 First, the landmarks based sampling make the feature invariant to variations like poses and expressions. [sent-114, score-0.493]
</p><p>36 Second, dense landmarks functions similar to the dense sampling in BOV framework [5, 12], which includes more information by the over-  completed representation. [sent-115, score-0.463]
</p><p>37 Given sufficient supervised data, the high-dimensional feature is more preferable. [sent-124, score-0.238]
</p><p>38 Pooling in spatial [25] and feature spaces [6] also lead to higher dimensionality and better performance. [sent-131, score-0.235]
</p><p>39 In this section, we propose a novel method for learning a sparse linear projection which maps the high-dimensional feature to a discriminative subspace with a much lower computational/storage cost. [sent-134, score-0.627]
</p><p>40 Then the supervised subspace learning methods such as LDA [3] or Joint Bayesian [10] are applied to extract discriminative information for face recognition and (potentially) further reduce the dimension. [sent-137, score-0.587]
</p><p>41 In the second step, we learn a sparse linear projection which directly maps high-dimensional feature set 푋 to lowdimensional feature set 푌 learned in the first step. [sent-138, score-0.59]
</p><p>42 Specifically, we adopt an 푙1-based regression to learn a sparse matrix 퐵 with additional freedom in rotation which can further promote the resulting sparsity. [sent-139, score-0.624]
</p><p>43 , 푦푁] be the corresponding low-dimensional feature set obtained from any conventional subspace learning methods. [sent-148, score-0.327]
</p><p>44 Our objective is to find a sparse linear projection 퐵 which maps 푋 to 푌 with low error:  m퐵in  ∥푌  − 퐵푇푋∥22 + 휆∥퐵∥1,  (1)  333000222755  Figure 3. [sent-150, score-0.265]
</p><p>45 This figure illustrates our method for sparse subspace learning. [sent-151, score-0.343]
</p><p>46 In the training phase, low-dimensional features 푌 are first obtained by PCA and supervised subspace learning. [sent-152, score-0.322]
</p><p>47 Then we learn the sparse projection matrix 퐵 which maps 푋 to 푌 by the rotated sparse regression. [sent-153, score-0.735]
</p><p>48 In the testing phase, we compute the low-dimensional feature by directly projecting high-dimensional feature using sparse matrix 퐵. [sent-154, score-0.459]
</p><p>49 , Euclidean and Cosine) are invariant to rotation transformation, we can introduce additional freedom in rotation to promote sparsity without sacrificing accuracy. [sent-159, score-0.468]
</p><p>50 (2)  Since the above formulation is a linear regression with sparse penalty and additional freedom in rotation, we term it as Rotated Sparse Regression. [sent-163, score-0.517]
</p><p>51 When matrix 퐵 is fixed, the sparse penalty term is constant. [sent-174, score-0.307]
</p><p>52 By iteratively optimizing two sub-problems, we can efficiently learn a rotated sparse regression. [sent-180, score-0.436]
</p><p>53 With the learned linear projection matrix 퐵, the lowdimensional feature is simply computed by 퐵푇푋. [sent-181, score-0.323]
</p><p>54 Due to the sparse penalty, the number of non-zero elements of matrix 퐵 is reduced by orders of magnitude (see our experiments in Section 5. [sent-182, score-0.245]
</p><p>55 Discussion An alternative approach to sparse subspace learning is  directly adding an 푙1 penalty term into the original objective function [41, 11]. [sent-187, score-0.476]
</p><p>56 In contrast, our method directly exploits the original subspace method to compute the low-dimensional feature and avoid difficulties in developing new optimization methods. [sent-189, score-0.362]
</p><p>57 Moreover, since only the low-dimensional feature is required in the second step, it is not necessary for the original subspace learning method to be linear. [sent-190, score-0.327]
</p><p>58 Feature selection is also a common approach to dealing with high-dimension problems such as boosting [15] and multi-task feature selection [28]. [sent-192, score-0.3]
</p><p>59 Compared with feature selection methods, our method exploits the information in all dimensions rather than a subset of them. [sent-194, score-0.264]
</p><p>60 Experimental Results In this section, we present more experimental results of our high-dimensional feature and rotated sparse regres333000222866  sion method. [sent-198, score-0.592]
</p><p>61 We evaluate the high-dimensional feature un-  der three settings: unsupervised learning, supervised learning with limited and unlimited training data. [sent-199, score-0.439]
</p><p>62 We adopt the Joint Bayesian method[10]3 for supervised subspace learning. [sent-200, score-0.315]
</p><p>63 The baseline method first normalize the image to 100* 100 pixels by an affine transformation calculated based on 5 landmarks (two eyes, noise and two mouth tips). [sent-214, score-0.423]
</p><p>64 The High-dimensional feature is better In the first experiment, we evaluate the performance of the high-dimensional feature with supervised learning. [sent-220, score-0.362]
</p><p>65 We  ×  extract image patches at 27 landmarks in 5 scales4. [sent-221, score-0.338]
</p><p>66 To better understand our high-dimensional feature, we separately investigate three factors: sampling at landmarks, landmark number, and scale number. [sent-233, score-0.23]
</p><p>67 To investigate this factor, we extract image patches in a single scale at 9 landmarks and compare it with the baseline feature. [sent-235, score-0.471]
</p><p>68 Their dimensionality 3We have tried several supervised learning methods such as LDA [3], PLDA [26] and Joint Bayesian [10]. [sent-236, score-0.262]
</p><p>69 a32760ta8 l% a r%engdumlargksid(Baeand the baseline feature under LFW unrestricted protocol. [sent-247, score-0.418]
</p><p>70 As shown in Table 2, sampling at the landmarks leads to comparatively better performance, which indicates sampling at the landmarks effectively reduce the intra-personal geometric variations due to pose and expressions. [sent-250, score-0.738]
</p><p>71 In this experiment, we increase the landmarks number from 5 to 27 to investigate performance as a function of the number of landmarks. [sent-252, score-0.343]
</p><p>72 Figure 4 shows  the accuracies of all descriptors improve monotonically, when the number of landmarks increases from 5 to 22. [sent-253, score-0.332]
</p><p>73 Large scale dataset favors high dimensionality To investigate the performance of the high-dimensional feature on a large scale dataset, we use the recent Wide and Deep Reference (WDRef) [10] database for training. [sent-262, score-0.422]
</p><p>74 Since we have more training data now, the feature dimension is reduced to 2,000 by PCA for supervised learning. [sent-263, score-0.364]
</p><p>75 High-dimensional feature with unsupervised learning In this experiment, we study the impact of high dimensionality under the unsupervised setting. [sent-278, score-0.44]
</p><p>76 We first reduce the dimension of the feature to 400 by PCA and then compute the cosine similarity of a pair of faces. [sent-282, score-0.222]
</p><p>77 As shown in the Table 4, in both databases, the highdimensional features are 3% ∼ 4% higher than the baseline dmiemtheondsi,o wnahlic fhea proves rteh e3 %eff ∼ec 4ti%ven heisgsh eorf high dhiem beansesiloinneality in the unsupervised setting. [sent-283, score-0.289]
</p><p>78 Compression by rotated sparse regression In this experiment, we evaluate the proposed rotated sparse regression method by comparing it with a sparse regression based on Equation 1. [sent-286, score-1.55]
</p><p>79 By varying the value of 휆, we compare the sparse regression and the rotated sparse regression under different sparsity. [sent-287, score-0.947]
</p><p>80 We follow the LFW unrestricted protocol and report the average sparsity (the proportion of zeros elements) over 10 rounds. [sent-288, score-0.46]
</p><p>81 9l1h02f83de% aimtur  and the baseline feature on LFW and Multi-PIE database under unsupervised setting. [sent-296, score-0.282]
</p><p>82 98, the proposed rotated sparse regression can still retain fairly good accuracy, but sparse regression suffers from a significant accuracy drop. [sent-307, score-0.947]
</p><p>83 It makes the projection matrix more sparse given the same reconstruction error. [sent-309, score-0.299]
</p><p>84 99, with the aid of rotated sparse regression, we reduce the cost oflinear projection by 100 times with less than 0. [sent-311, score-0.524]
</p><p>85 This figure compares the rotated sparse regression and two feature selection methods 5. [sent-314, score-0.797]
</p><p>86 Comparison with Feature Selection In this experiment, we compare the rotated sparse regression and two feature selection methods: backward greedy [39] and structure sparsity [28]. [sent-316, score-1.031]
</p><p>87 We use the highdimensional LBP feature as input in all methods. [sent-317, score-0.263]
</p><p>88 For structure sparsity, we follow the method in [28] which uses 푙2,1-norm to enforce structure sparsity for feature selection. [sent-320, score-0.263]
</p><p>89 As shown in Figure 6, feature selection methods suffer  from a significant accuracy drop when sparsity is larger than 60%. [sent-321, score-0.333]
</p><p>90 When sparsity is around 80%, the rotated sparse regression is slightly better than no sparse compression, as sparsity may promote generalization. [sent-322, score-1.125]
</p><p>91 When sparsity is higher than 90%, our method outperforms the feature selection method by 6%, which verifies the effectiveness of the proposed method. [sent-323, score-0.333]
</p><p>92 It also indicates that the majority of dimensions in our high-dimensional feature are informative and complementary. [sent-324, score-0.218]
</p><p>93 07% [26]) under the LFW unrestricted protocol (know identity information). [sent-331, score-0.321]
</p><p>94 Conclusion In this paper, we have studied the performance of face feature as a function of dimensionality. [sent-337, score-0.327]
</p><p>95 through experimentation that high dimensionality is critical to achieving high performance. [sent-344, score-0.244]
</p><p>96 We also made the highdimensional feature practical enough to be introduced into a rotated sparse regression technique. [sent-345, score-0.866]
</p><p>97 Large scale strongly supervised ensemble metric learning, with applications to face verification and retrieval. [sent-500, score-0.41]
</p><p>98 Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition. [sent-539, score-0.327]
</p><p>99 Leveraging billions of faces to overcome performance barriers in unconstrained face recognition. [sent-571, score-0.24]
</p><p>100 Linear spatial pyramid matching using sparse coding for image classification. [sent-599, score-0.214]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lfw', 0.332), ('landmarks', 0.292), ('rotated', 0.259), ('unrestricted', 0.244), ('wdref', 0.209), ('face', 0.203), ('sparse', 0.177), ('regression', 0.167), ('subspace', 0.166), ('highdimensional', 0.139), ('sparsity', 0.139), ('feature', 0.124), ('lbp', 0.122), ('supervised', 0.114), ('dimensionality', 0.111), ('penalty', 0.096), ('projection', 0.088), ('compression', 0.087), ('protocol', 0.077), ('freedom', 0.077), ('facial', 0.077), ('hastie', 0.077), ('lowdimensional', 0.077), ('sampling', 0.077), ('trans', 0.074), ('fiducial', 0.074), ('landmark', 0.07), ('selection', 0.07), ('alfetur', 0.07), ('plda', 0.07), ('pca', 0.07), ('experimentation', 0.069), ('bayesian', 0.069), ('unsupervised', 0.068), ('pages', 0.068), ('lda', 0.067), ('promote', 0.067), ('rotation', 0.067), ('conference', 0.067), ('cao', 0.066), ('gabor', 0.066), ('patch', 0.063), ('alignment', 0.062), ('verification', 0.061), ('informative', 0.059), ('taigman', 0.057), ('unlimited', 0.054), ('greedy', 0.053), ('wen', 0.052), ('sacrificing', 0.051), ('investigate', 0.051), ('baseline', 0.05), ('dimension', 0.05), ('storage', 0.049), ('cosine', 0.048), ('dense', 0.047), ('eyes', 0.047), ('pattern', 0.046), ('patches', 0.046), ('transformation', 0.043), ('boureau', 0.043), ('training', 0.042), ('belhumeur', 0.042), ('zou', 0.042), ('backward', 0.042), ('guillaumin', 0.041), ('descriptors', 0.04), ('elastic', 0.04), ('database', 0.04), ('mouth', 0.038), ('nose', 0.038), ('difficulties', 0.037), ('pyramid', 0.037), ('faces', 0.037), ('verbeek', 0.037), ('learning', 0.037), ('factors', 0.037), ('stage', 0.036), ('cells', 0.036), ('boosting', 0.036), ('discriminative', 0.035), ('berg', 0.035), ('exploits', 0.035), ('adopt', 0.035), ('joint', 0.035), ('dimensions', 0.035), ('reduced', 0.034), ('descriptor', 0.034), ('matrix', 0.034), ('significance', 0.033), ('scale', 0.032), ('sion', 0.032), ('yin', 0.032), ('concatenate', 0.032), ('recognition', 0.032), ('experiment', 0.032), ('gist', 0.032), ('british', 0.032), ('high', 0.032), ('roc', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="64-tfidf-1" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>Author: Dong Chen, Xudong Cao, Fang Wen, Jian Sun</p><p>Abstract: Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. This prevents further exploration of the use of a highdimensional feature. In this paper, we study the performance of a highdimensional feature. We first empirically show that high dimensionality is critical to high performance. A 100K-dim feature, based on a single-type Local Binary Pattern (LBP) descriptor, can achieve significant improvements over both its low-dimensional version and the state-of-the-art. We also make the high-dimensional feature practical. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.</p><p>2 0.27853984 <a title="64-tfidf-2" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>3 0.26211655 <a title="64-tfidf-3" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>4 0.24638732 <a title="64-tfidf-4" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>Author: Dong Yi, Zhen Lei, Stan Z. Li</p><p>Abstract: Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper, we propose a novel method for pose robust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-ofthe-art methods significantly, meanwhile, the method works well on LFW.</p><p>5 0.19706945 <a title="64-tfidf-5" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>6 0.18977971 <a title="64-tfidf-6" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>7 0.15139182 <a title="64-tfidf-7" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>8 0.14871472 <a title="64-tfidf-8" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>9 0.14475954 <a title="64-tfidf-9" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>10 0.13938576 <a title="64-tfidf-10" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>11 0.13873062 <a title="64-tfidf-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.1327744 <a title="64-tfidf-12" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>13 0.13242193 <a title="64-tfidf-13" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>14 0.12793395 <a title="64-tfidf-14" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>15 0.12286606 <a title="64-tfidf-15" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>16 0.12185876 <a title="64-tfidf-16" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>17 0.11977829 <a title="64-tfidf-17" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>18 0.11860321 <a title="64-tfidf-18" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>19 0.11715969 <a title="64-tfidf-19" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>20 0.11285095 <a title="64-tfidf-20" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.255), (1, -0.09), (2, -0.108), (3, 0.088), (4, 0.04), (5, -0.008), (6, -0.062), (7, -0.142), (8, 0.189), (9, -0.242), (10, 0.056), (11, -0.021), (12, 0.087), (13, 0.039), (14, 0.038), (15, 0.017), (16, -0.021), (17, -0.05), (18, 0.027), (19, 0.043), (20, 0.062), (21, 0.035), (22, 0.028), (23, -0.056), (24, 0.002), (25, 0.076), (26, -0.052), (27, 0.001), (28, 0.01), (29, -0.081), (30, -0.077), (31, 0.086), (32, 0.029), (33, -0.048), (34, 0.007), (35, 0.079), (36, -0.063), (37, -0.021), (38, 0.008), (39, 0.045), (40, -0.007), (41, 0.057), (42, -0.003), (43, -0.101), (44, -0.041), (45, -0.013), (46, 0.008), (47, -0.019), (48, -0.085), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95882207 <a title="64-lsi-1" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>Author: Dong Chen, Xudong Cao, Fang Wen, Jian Sun</p><p>Abstract: Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. This prevents further exploration of the use of a highdimensional feature. In this paper, we study the performance of a highdimensional feature. We first empirically show that high dimensionality is critical to high performance. A 100K-dim feature, based on a single-type Local Binary Pattern (LBP) descriptor, can achieve significant improvements over both its low-dimensional version and the state-of-the-art. We also make the high-dimensional feature practical. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.</p><p>2 0.85717702 <a title="64-lsi-2" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>3 0.81495482 <a title="64-lsi-3" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>4 0.79257637 <a title="64-lsi-4" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>Author: Dong Yi, Zhen Lei, Stan Z. Li</p><p>Abstract: Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper, we propose a novel method for pose robust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-ofthe-art methods significantly, meanwhile, the method works well on LFW.</p><p>5 0.74871385 <a title="64-lsi-5" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>Author: Xuehan Xiong, Fernando De_la_Torre</p><p>Abstract: Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization ofa general smoothfunction. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, thispaperproposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-ofthe-art performance in the problem of facial feature detec- tion. The code is available at www. .human sen sin g. . cs . cmu . edu/in t ra fa ce.</p><p>6 0.74577695 <a title="64-lsi-6" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>7 0.74357378 <a title="64-lsi-7" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>8 0.73289597 <a title="64-lsi-8" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>9 0.73073554 <a title="64-lsi-9" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>10 0.69799471 <a title="64-lsi-10" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>11 0.6936574 <a title="64-lsi-11" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>12 0.65850705 <a title="64-lsi-12" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>13 0.6577636 <a title="64-lsi-13" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>14 0.65596867 <a title="64-lsi-14" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>15 0.64744365 <a title="64-lsi-15" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>16 0.64414865 <a title="64-lsi-16" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>17 0.63819063 <a title="64-lsi-17" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>18 0.6352331 <a title="64-lsi-18" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>19 0.597978 <a title="64-lsi-19" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>20 0.58651543 <a title="64-lsi-20" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.136), (16, 0.024), (19, 0.187), (26, 0.067), (33, 0.3), (67, 0.115), (69, 0.052), (87, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96174419 <a title="64-lda-1" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>2 0.94154441 <a title="64-lda-2" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>Author: Patrick Lucey, Alina Bialkowski, Peter Carr, Stuart Morgan, Iain Matthews, Yaser Sheikh</p><p>Abstract: In this paper, we describe a method to represent and discover adversarial group behavior in a continuous domain. In comparison to other types of behavior, adversarial behavior is heavily structured as the location of a player (or agent) is dependent both on their teammates and adversaries, in addition to the tactics or strategies of the team. We present a method which can exploit this relationship through the use of a spatiotemporal basis model. As players constantly change roles during a match, we show that employing a “role-based” representation instead of one based on player “identity” can best exploit the playing structure. As vision-based systems currently do not provide perfect detection/tracking (e.g. missed or false detections), we show that our compact representation can effectively “denoise ” erroneous detections as well as enabling temporal analysis, which was previously prohibitive due to the dimensionality of the signal. To evaluate our approach, we used a fully instrumented field-hockey pitch with 8 fixed highdefinition (HD) cameras and evaluated our approach on approximately 200,000 frames of data from a state-of-the- art real-time player detector and compare it to manually labelled data.</p><p>3 0.92217565 <a title="64-lda-3" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>Author: Yun Jiang, Hema Koppula, Ashutosh Saxena</p><p>Abstract: For scene understanding, one popular approach has been to model the object-object relationships. In this paper, we hypothesize that such relationships are only an artifact of certain hidden factors, such as humans. For example, the objects, monitor and keyboard, are strongly spatially correlated only because a human types on the keyboard while watching the monitor. Our goal is to learn this hidden human context (i.e., the human-object relationships), and also use it as a cue for labeling the scenes. We present Infinite Factored Topic Model (IFTM), where we consider a scene as being generated from two types of topics: human configurations and human-object relationships. This enables our algorithm to hallucinate the possible configurations of the humans in the scene parsimoniously. Given only a dataset of scenes containing objects but not humans, we show that our algorithm can recover the human object relationships. We then test our algorithm on the task ofattribute and object labeling in 3D scenes and show consistent improvements over the state-of-the-art.</p><p>4 0.89417887 <a title="64-lda-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>same-paper 5 0.89164644 <a title="64-lda-5" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>Author: Dong Chen, Xudong Cao, Fang Wen, Jian Sun</p><p>Abstract: Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. This prevents further exploration of the use of a highdimensional feature. In this paper, we study the performance of a highdimensional feature. We first empirically show that high dimensionality is critical to high performance. A 100K-dim feature, based on a single-type Local Binary Pattern (LBP) descriptor, can achieve significant improvements over both its low-dimensional version and the state-of-the-art. We also make the high-dimensional feature practical. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.</p><p>6 0.88758779 <a title="64-lda-6" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>7 0.88496208 <a title="64-lda-7" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>8 0.87674892 <a title="64-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.87578714 <a title="64-lda-9" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>10 0.87550104 <a title="64-lda-10" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>11 0.87544435 <a title="64-lda-11" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>12 0.87488925 <a title="64-lda-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.87478554 <a title="64-lda-13" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>14 0.87418109 <a title="64-lda-14" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>15 0.87310404 <a title="64-lda-15" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>16 0.87153274 <a title="64-lda-16" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>17 0.87082285 <a title="64-lda-17" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>18 0.87079805 <a title="64-lda-18" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>19 0.87079614 <a title="64-lda-19" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>20 0.87054449 <a title="64-lda-20" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
