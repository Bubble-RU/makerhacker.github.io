<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-345" href="#">cvpr2013-345</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</h1>
<br/><p>Source: <a title="cvpr-2013-345-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Pauwels_Real-Time_Model-Based_Rigid_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Karl Pauwels, Leonardo Rubio, Javier Díaz, Eduardo Ros</p><p>Abstract: We propose a novel model-based method for estimating and tracking the six-degrees-of-freedom (6DOF) pose of rigid objects of arbitrary shapes in real-time. By combining dense motion and stereo cues with sparse keypoint correspondences, and by feeding back information from the model to the cue extraction level, the method is both highly accurate and robust to noise and occlusions. A tight integration of the graphical and computational capability of Graphics Processing Units (GPUs) results in pose updates at framerates exceeding 60 Hz. Since a benchmark dataset that enables the evaluation of stereo-vision-based pose estimators in complex scenarios is currently missing in the literature, we have introduced a novel synthetic benchmark dataset with varying objects, background motion, noise and occlusions. Using this dataset and a novel evaluation methodology, we show that the proposed method greatly outperforms state-of-the-art methods. Finally, we demonstrate excellent performance on challenging real-world sequences involving object manipulation.</p><p>Reference: <a title="cvpr-2013-345-reference" href="../cvpr2013_reference/cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 By combining dense motion and stereo cues with sparse keypoint correspondences, and by feeding back information from the model to the cue extraction level, the method is both highly accurate and robust to noise and occlusions. [sent-2, score-0.934]
</p><p>2 A tight integration of the graphical and computational capability of Graphics Processing Units (GPUs) results in pose updates at framerates exceeding 60 Hz. [sent-3, score-0.33]
</p><p>3 Since a benchmark dataset that enables the evaluation of stereo-vision-based pose estimators in complex scenarios is currently missing in the literature, we have introduced a novel synthetic benchmark dataset with varying objects, background motion, noise and occlusions. [sent-4, score-0.309]
</p><p>4 Introduction Estimating and tracking the 6DOF (three translation and three rotation) pose of rigid objects is critical for robotic applications involving object grasping and manipulation, and also for activity interpretation. [sent-8, score-0.643]
</p><p>5 The most popular tracking methods match expected to observed edges by projecting a 3D wireframe model in the image [6]. [sent-13, score-0.211]
</p><p>6 Many extensions have been proposed that exploit also texture information and particle filtering in order to reduce sensitivity to background clutter and noise [5, 13]. [sent-14, score-0.226]
</p><p>7 e s  ,  Tracking-by-detection approaches on the other hand can recover the pose without requiring an initial estimate by using sparse keypoints and descriptors with wide-baseline matching [9, 13]. [sent-16, score-0.372]
</p><p>8 These methods can include additional cues such as optical flow and sparse keypoints,  but at a large computational cost [4]. [sent-19, score-0.532]
</p><p>9 Recently, due to the prevalence of cheap depth sensors, depth information is being applied with great success in various related problems, such as on-line scene modeling [15], articulated body pose estimation [18], and texture-less object detection [7]. [sent-21, score-0.383]
</p><p>10 First of all, we introduce a novel model-based 6DOF pose estimation and tracking method that combines dense motion and stereo measurements with sparse keypoint features. [sent-24, score-1.033]
</p><p>11 Secondly, the method has been designed specifically for high-performance operation (> 60 Hz) and this is achieved through the exploitation and tight integration of the graphics and computation pipelines of modern GPUs in both the low-level cue extraction and pose estimation stages. [sent-26, score-0.292]
</p><p>12 Different visual cues are extracted from a stereo video 222333444755  ? [sent-31, score-0.39]
</p><p>13 The cues are combined to estimate the object pose and model information is fed back to facilitate the cue extraction. [sent-67, score-0.628]
</p><p>14 Visual cues The dense motion and stereo cues are obtained using modified coarse-to-fine GPU-accelerated phase-based algorithms [16], and the SIFT features are extracted and matched using a GPU library [20]. [sent-78, score-0.773]
</p><p>15 1  Model-based dense stereo  Coarse-to-fine stereo algorithms, although highly efficient, support only a limited disparity range and have difficulties detecting fine structures. [sent-81, score-0.654]
</p><p>16 To overcome these problems we feed the object pose estimate obtained in the previous frame back as a prior in the stereo algorithm. [sent-82, score-0.659]
</p><p>17 Figure 2A,B show an example real-world stereo image pair of a box being manipulated. [sent-83, score-0.258]
</p><p>18 Using the box’s previous frame pose estimate, stereo priors are generated for the current frame (Fig. [sent-84, score-0.629]
</p><p>19 2C,D) and introduced at the lowest scale used by the stereo algorithm. [sent-85, score-0.258]
</p><p>20 n  (C) left and (D) right stereo priors generated using the previous frame pose estimate of the manipulated box. [sent-152, score-0.584]
</p><p>21 (E) Stereo obtained using four scales and initializing with the prior, and (F) stereo obtained using six scales and without using the prior. [sent-153, score-0.258]
</p><p>22 2  Dense motion cues  The optical flow algorithm integrates the temporal phase gradient across different orientations and also uses a coarseto-fine scheme to increase the dynamic range. [sent-156, score-0.581]
</p><p>23 A prior is not required since displacements in the motion scenario are much smaller than in the stereo scenario. [sent-159, score-0.437]
</p><p>24 Figure 3C contains the (subsampled and scaled) optical flow vectors from Fig. [sent-160, score-0.336]
</p><p>25 We also extract a second type ofmotion which we refer to as Augmented Reality (AR) flow that includes feedback from the model. [sent-162, score-0.267]
</p><p>26 The model’s texture is rendered at the current pose estimate and overlaid on It, resulting in an ‘augmented image’ Iˆt. [sent-163, score-0.342]
</p><p>27 Because of the erroneous pose estimate (deliberately large in this case to better illustrate the concept) this motion is quite different from the optical flow. [sent-168, score-0.513]
</p><p>28 It allows the tracker to recover from such errors by effectively countering the drift that results from tracking based on optical flow alone. [sent-169, score-0.659]
</p><p>29 (C) Real optical flow from real image in A to the next real image. [sent-172, score-0.336]
</p><p>30 Both flow fields are subsampled (15 ) aBn tdo s cthaele nde (5 ). [sent-174, score-0.242]
</p><p>31 Pose Estimation and Tracking  The proposed method incorporates the differential rigid motion constraint into a fast variant of the iterative closest point (ICP) algorithm [1] to allow all the dense cues to simultaneously and robustly minimize the pose error. [sent-177, score-0.766]
</p><p>32 2) is used to re-initialize the  tracker with an estimate obtained from a sparse trackingby-detection approach, if required. [sent-180, score-0.261]
</p><p>33 1  Dense Pose Tracking  Our aim is to recover the rigid rotation and translation that best explains the dense visual cues and transforms each model point m = [mx , my, mz]T at time t into point m? [sent-183, score-0.447]
</p><p>34 ⎦⎤  ,(3)  with x = [x, y]T the pixel coordinates (with nodal point as origin), f the focal length, and b the baseline of the stereo rig. [sent-198, score-0.258]
</p><p>35 We deliberately avoid using the motion cues to facilitate this correspondence finding, as done in many ICP extensions, since this requires both valid stereo and valid motion measurements at the same pixel. [sent-201, score-0.785]
</p><p>36 Instead we use the efficient projective data association algorithm [2] that corresponds stereo measurements to model points that project to the same pixel. [sent-202, score-0.304]
</p><p>37 (5)  (6)  Note that this looks very similar to the differential motion equation from classical kinematics that expresses the 3D motion of a point, m˙ , in terms of 3D translational and rotational velocity: m˙ = t ω m. [sent-219, score-0.298]
</p><p>38 This can now be used with the optical flow to provide additional constraints on the rigid motion. [sent-221, score-0.435]
</p><p>39 Unlike in the stereo case (3), m˙ cannot be reconstructed from the optical flow and instead (7) needs to be enforced in the image domain. [sent-222, score-0.594]
</p><p>40 We obtain this depth mz by rendering the model at the current pose estimate (8). [sent-229, score-0.467]
</p><p>41 i  with o = [ox, oy]T and a = [ax, ay]T the observed optical and AR flow respectively. [sent-238, score-0.336]
</p><p>42 Therefore, the minimization of (14) needs to be iterated a number of times, at each iteration updating the pose, the shape correspondences, and the unexplained part of the optical and AR flow measurements. [sent-242, score-0.336]
</p><p>43 At iteration k, an incremental pose update is obtained by minimizing E(Δtk, Δωk), and accumulated into the pose estimate at the previous iteration k − 1: Rk  =  ΔRk Rk−1 ,  (15)  tk where ΔRk =  =  ΔRk tk−1 + Δtk ,  e[Δωk]× . [sent-243, score-0.554]
</p><p>44 The model is updated  (16) as:  mk = Rk m + tk ,  (17)  and used to obtain the new (projective) shape correspondences and the part of the optical and AR flow explained thus far, Δxk = [Δxk, Δyk]T:  Δxk  =  f(mxk/mzk − mx/mz) ,  Δyk  =  f(myk/mzk − my/mz) . [sent-244, score-0.505]
</p><p>45 (19)  (18)  This explained flow is subtracted from the observed optical and AR flow: ok  =  o − Δxk ,  (20)  ak  =  a − Δxk . [sent-245, score-0.38]
</p><p>46 (21)  The next iteration incremental pose updates are then obtained by minimizing E(Δtk+1 , Δωk+1), which operates on ok, ak, mk, and s? [sent-246, score-0.266]
</p><p>47 2  Combined Sparse and Dense Pose Estimation  A RANSAC-based monocular perspective-n-point pose estimator is used to robustly extract the 6DOF object pose on the basis of correspondences between image (2D) and model codebook (3D) SIFT keypoint descriptors [3, 9]. [sent-252, score-0.628]
</p><p>48 Exhaustive matching is used and therefore, unlike the dense component of the proposed method, this sparse estimator provides a pose estimate that does not depend on the previous frame’s estimate. [sent-253, score-0.466]
</p><p>49 Due to the nonlinear and multimodal nature of the sparse and dense components, directly merging them using for instance a Kalman filtering framework is not suitable here. [sent-254, score-0.27]
</p><p>50 Instead we select either the sparse or dense estimate based on the effectiveness of its feedback on cue extraction, as measured by the proportion of valid AR flow vectors in the projected object region. [sent-255, score-0.757]
</p><p>51 A flow vector is considered valid if it passes a simple forward/backward consistency check (2. [sent-256, score-0.236]
</p><p>52 This measure is used since, unlike dense opticalflow-based or stereo-based measures, AR flow is affected by object occlusions. [sent-259, score-0.381]
</p><p>53 We will show in Section 4 that this simple measure is adequate both for selecting and determining the reliability of the pose estimate. [sent-260, score-0.212]
</p><p>54 v Tdehde iAnR T aflbolwe 1is computed twice, based on the previous frame’s sparse and dense estimate. [sent-264, score-0.202]
</p><p>55 Note that the low-level component computes dense stereo and three times optical flow altogether at ±95 Hz. [sent-268, score-0.732]
</p><p>56 The tracking module requires 16 ms in total and thus exceeds 60 Hz. [sent-277, score-0.354]
</p><p>57 Processing times dense tracking (in ms) dense cues (640×480 image size) uGesab (6o4r pyramid m(4ag×e) – optical pflyorwam AR flow (2×) AmoRd felol-wba (s2e×d )stereo – rendering dense pose (50,000 samples) residual flow (18–21) – rendering –  –  –  –  10. [sent-280, score-1.638]
</p><p>58 8  gathering/compaction – pose estimate error evaluation  1. [sent-289, score-0.264]
</p><p>59 Synthetic Benchmark Dataset We have constructed an extensive synthetic benchmark dataset to evaluate pose trackers under a variety of realistic conditions. [sent-299, score-0.294]
</p><p>60 This, possibly erroneous, motion trace was then used to generate synthetic sequences and so it is, by definition, the ground-truth object motion. [sent-454, score-0.326]
</p><p>61 The realism and complexity of the sequences is further increased by blending the rendered objects into a real-world stereo sequence recorded with a moving camera in a cluttered office environment. [sent-457, score-0.427]
</p><p>62 Noise and Occluder To further explore the limitations of pose estimation methods, different sequences are created corrupted either by noise or an occluding object. [sent-462, score-0.339]
</p><p>63 1 intensity) is added separately to each color channel, frame, and stereo image (Fig. [sent-464, score-0.258]
</p><p>64 To obtain realistic occlusion (with meaningful motion and stereo cues), we added a randomly bouncing 3D teddy bear object to the sequence (Fig. [sent-466, score-0.469]
</p><p>65 The occlusion proportion of the cube object over  the sequence is shown in Fig. [sent-468, score-0.257]
</p><p>66 our dense stereo cue is affected by either left or right occlusions). [sent-472, score-0.476]
</p><p>67 Performance Evaluation Pose trackers are usually evaluated by comparing the estimated to the ground-truth or approximate ground-truth pose across an entire sequence [4, 5, 17]. [sent-475, score-0.299]
</p><p>68 For example, if tracking is lost early in the scene, the average error will typically be very large, but 222333445 919  A  soda - orig - left 339  B  clown - noisy - left 1  Ccube - oc l - left 540Dcube - oc l - right 540  Figure7. [sent-477, score-0.92]
</p><p>69 Indicatvesamplesfromthedif rentsy heticsquencs  illustrating (A) large distance range, (B) added noise, and (C,D) different occlusion proportions in the left and right sequences (orig = noise free ; occl = occluded ; number = frame index). [sent-478, score-0.551]
</p><p>70 Since we use synthetic sequences, we can continuously monitor tracking accuracy and automatically reset when tracking is lost. [sent-481, score-0.534]
</p><p>71 Instead we use the largest distance between corresponding vertices, vj, of the object transformed according to the ground-truth (R, t) and estimated pose (Rˆ, tˆ) : eP  = mjax? [sent-484, score-0.255]
</p><p>72 10 mm), the  tracker is reset to the ground-truth. [sent-489, score-0.273]
</p><p>73 As a consequence all resets are triggered by the object motion only and the error provides an indication ofthe sequence complexity. [sent-493, score-0.211]
</p><p>74 Stereo and Optical Flow Synergy Table 2 shows results on the least textured sequences soda and edge. [sent-498, score-0.238]
</p><p>75 Tracking success rate (in %) – stereo and optical flow soda noisy  edge noisy  s ot pea tr ie co al+flo w 15790 73 4895713645 6273 57 9082958 130 453 02 orig  occl  orig  occl  both which means that, without tracking, a reset is required approximately every other frame. [sent-500, score-2.329]
</p><p>76 Due to the low texture, shape-symmetry (soda) and shape-planarity (edge), stereoonly performance is quite bad in these sequences and even  below static in the noisy and occluded scenarios. [sent-501, score-0.232]
</p><p>77 The AR flow and sparse cues further improve the results, but these are discussed in the next section (and shown in Table 3). [sent-504, score-0.396]
</p><p>78 State-of-the-art Methods The Blocks World Robotic Vision Toolbox (BLORT) [13] provides a number of object learning, recognition, and tracking components that can be combined to robustly track object pose in real-time. [sent-507, score-0.552]
</p><p>79 We only evaluate the particle-filterbased tracking module here since the recognition module is very similar to our sparse-only method. [sent-508, score-0.324]
</p><p>80 Each particle represents a pose estimate and is used to render an edge model (combining edges from geometry and texture) that is matched against edges extracted from the current image. [sent-509, score-0.439]
</p><p>81 We evaluate BLORT’s tracking module with the default (real-time) setting with 200 particles and a high precision variant with 10,000 particles. [sent-510, score-0.327]
</p><p>82 Due to an inefficient rendering procedure, the current tracker implementation of BLORT can not handle models with a high vertex count. [sent-511, score-0.263]
</p><p>83 The PWP3D method [17] uses a 3D geometry model to maximize the discrimination between  statistical foreground and background appearance models, by directly operating on the 3D pose parameters. [sent-514, score-0.212]
</p><p>84 Table 3 summarizes all the results obtained with a track222333555200  = noise free; occl = occluded sodasoupclowncandycubeedge orig noisy occl orig noisy occl orig noisy occl orig noisy occl orig noisy occl orig noisy occl Table 3. [sent-519, score-4.639]
</p><p>85 Tracking success rate (in %) – orig  spdretagrntios. [sent-520, score-0.312]
</p><p>86 The proposed denseonly tracking method obtains an almost perfect performance regardless of model shape, texture (see edge), or sequence noise. [sent-523, score-0.276]
</p><p>87 In the occluded scenario however, it is frequently outperformed by the sparse-only and high quality particle filtering methods. [sent-524, score-0.213]
</p><p>88 The region-based tracker performs consistently well on the noise-free and noisy sequences, but fails dramatically in the presence of an occluder. [sent-533, score-0.219]
</p><p>89 Figure 8D finally shows some tracking failures that are detected correctly by the reliability measure (proportion AR flow < 0. [sent-544, score-0.43]
</p><p>90 The sparse keypoint-based method is highly robust to occlusions and provides excellent synergy with the dense methods proposed here. [sent-551, score-0.379]
</p><p>91 Note that the proposed method also supports depth cues other than stereo (e. [sent-555, score-0.431]
</p><p>92 from a Kinect sensor), and conversely, enables for the incorporation of motion cues in current depth-only applications [15]. [sent-557, score-0.28]
</p><p>93 Current Kinect versions however do not provide the high shape detail close to the camera, nor the high framerates achieved by our modelbased stereo algorithm[19]. [sent-558, score-0.322]
</p><p>94 Finally, the articulated case represents a relatively straightforward extension of the proposed methods, and the high-level of detail provided by the dense cues should be especially powerful in this, and also the deformable, scenario. [sent-559, score-0.316]
</p><p>95 Conclusion We have presented a novel multi-cue approach for the  6DOF model-based pose tracking of rigid objects of arbitrary shapes that exploits dense motion and stereo cues, sparse keypoint features, and feedback from the model to the cue extraction. [sent-561, score-1.279]
</p><p>96 222333555311  A  dense pose wins  B  dense pose wins  C  sparse pose wins  D  detected failure  Figure8. [sent-566, score-1.231]
</p><p>97 Indicatver al-worldpose stimationresult ,showinghowthed nseposei sel ctedwhen(A)theobjectisfar nd/or(B)  motion-blurred, how the sparse pose is selected in case of (C) strong occlusions, and how (D) failures can be detected correctly. [sent-567, score-0.328]
</p><p>98 Combined region and motion-based 3D tracking of rigid and articulated objects. [sent-588, score-0.323]
</p><p>99 Robust 3D visual tracking using particle filtering on the special euclidean group: A combined approach of keypoint and edge features. [sent-594, score-0.466]
</p><p>100 Real-time human pose recognition in parts from single depth images. [sent-704, score-0.253]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('occl', 0.319), ('orig', 0.312), ('stereo', 0.258), ('pose', 0.212), ('flow', 0.2), ('tracking', 0.178), ('ar', 0.16), ('soda', 0.158), ('tracker', 0.145), ('dense', 0.138), ('optical', 0.136), ('cues', 0.132), ('blort', 0.128), ('reset', 0.128), ('motion', 0.113), ('particle', 0.101), ('rigid', 0.099), ('clown', 0.096), ('synergy', 0.096), ('gpu', 0.086), ('wins', 0.085), ('rendering', 0.083), ('cube', 0.082), ('sequences', 0.08), ('cue', 0.08), ('tk', 0.078), ('proportion', 0.077), ('particles', 0.076), ('robotic', 0.074), ('noisy', 0.074), ('module', 0.073), ('rk', 0.07), ('keypoint', 0.07), ('ms', 0.069), ('feedback', 0.067), ('sparse', 0.064), ('framerates', 0.064), ('pauwels', 0.064), ('deliberately', 0.063), ('frame', 0.062), ('xk', 0.06), ('correspondences', 0.058), ('linearized', 0.057), ('xfy', 0.057), ('candy', 0.057), ('triangle', 0.056), ('sequence', 0.055), ('updates', 0.054), ('rubio', 0.052), ('soup', 0.052), ('estimate', 0.052), ('failures', 0.052), ('icp', 0.05), ('synthetic', 0.05), ('ros', 0.049), ('noise', 0.047), ('kit', 0.047), ('supplemental', 0.046), ('projective', 0.046), ('articulated', 0.046), ('az', 0.045), ('tz', 0.045), ('gpus', 0.045), ('keypoints', 0.044), ('occlusions', 0.044), ('ok', 0.044), ('mz', 0.044), ('combined', 0.043), ('occluded', 0.043), ('texture', 0.043), ('object', 0.043), ('subsampled', 0.042), ('kinect', 0.041), ('depth', 0.041), ('rotation', 0.041), ('trace', 0.04), ('differential', 0.039), ('edge', 0.039), ('lost', 0.038), ('excellent', 0.037), ('translation', 0.037), ('valid', 0.036), ('methodology', 0.035), ('static', 0.035), ('current', 0.035), ('filtering', 0.035), ('scenario', 0.034), ('mm', 0.034), ('facilitate', 0.034), ('exceeds', 0.034), ('increased', 0.034), ('robustly', 0.033), ('mk', 0.033), ('expresses', 0.033), ('projecting', 0.033), ('multimodal', 0.033), ('oc', 0.032), ('displacements', 0.032), ('back', 0.032), ('trackers', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="345-tfidf-1" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>Author: Karl Pauwels, Leonardo Rubio, Javier Díaz, Eduardo Ros</p><p>Abstract: We propose a novel model-based method for estimating and tracking the six-degrees-of-freedom (6DOF) pose of rigid objects of arbitrary shapes in real-time. By combining dense motion and stereo cues with sparse keypoint correspondences, and by feeding back information from the model to the cue extraction level, the method is both highly accurate and robust to noise and occlusions. A tight integration of the graphical and computational capability of Graphics Processing Units (GPUs) results in pose updates at framerates exceeding 60 Hz. Since a benchmark dataset that enables the evaluation of stereo-vision-based pose estimators in complex scenarios is currently missing in the literature, we have introduced a novel synthetic benchmark dataset with varying objects, background motion, noise and occlusions. Using this dataset and a novel evaluation methodology, we show that the proposed method greatly outperforms state-of-the-art methods. Finally, we demonstrate excellent performance on challenging real-world sequences involving object manipulation.</p><p>2 0.23281769 <a title="345-tfidf-2" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>Author: Katerina Fragkiadaki, Han Hu, Jianbo Shi</p><p>Abstract: Human pose detectors, although successful in localising faces and torsos of people, often fail with lower arms. Motion estimation is often inaccurate under fast movements of body parts. We build a segmentation-detection algorithm that mediates the information between body parts recognition, and multi-frame motion grouping to improve both pose detection and tracking. Motion of body parts, though not accurate, is often sufficient to segment them from their backgrounds. Such segmentations are crucialfor extracting hard to detect body parts out of their interior body clutter. By matching these segments to exemplars we obtain pose labeled body segments. The pose labeled segments and corresponding articulated joints are used to improve the motion flow fields by proposing kinematically constrained affine displacements on body parts. The pose-based articulated motion model is shown to handle large limb rotations and displacements. Our algorithm can detect people under rare poses, frequently missed by pose detectors, showing the benefits of jointly reasoning about pose, segmentation and motion in videos.</p><p>3 0.20515338 <a title="345-tfidf-3" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>Author: Zhuoyuan Chen, Hailin Jin, Zhe Lin, Scott Cohen, Ying Wu</p><p>Abstract: We present an optical flow algorithm for large displacement motions. Most existing optical flow methods use the standard coarse-to-fine framework to deal with large displacement motions which has intrinsic limitations. Instead, we formulate the motion estimation problem as a motion segmentation problem. We use approximate nearest neighbor fields to compute an initial motion field and use a robust algorithm to compute a set of similarity transformations as the motion candidates for segmentation. To account for deviations from similarity transformations, we add local deformations in the segmentation process. We also observe that small objects can be better recovered using translations as the motion candidates. We fuse the motion results obtained under similarity transformations and under translations together before a final refinement. Experimental validation shows that our method can successfully handle large displacement motions. Although we particularly focus on large displacement motions in this work, we make no sac- rifice in terms of overall performance. In particular, our method ranks at the top of the Middlebury benchmark.</p><p>4 0.18614325 <a title="345-tfidf-4" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>5 0.18566667 <a title="345-tfidf-5" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>6 0.18083267 <a title="345-tfidf-6" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>7 0.17064567 <a title="345-tfidf-7" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>8 0.16518365 <a title="345-tfidf-8" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>9 0.16049603 <a title="345-tfidf-9" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>10 0.15941773 <a title="345-tfidf-10" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>11 0.1553071 <a title="345-tfidf-11" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>12 0.15511294 <a title="345-tfidf-12" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>13 0.15405755 <a title="345-tfidf-13" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>14 0.15180412 <a title="345-tfidf-14" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>15 0.14981019 <a title="345-tfidf-15" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>16 0.14395306 <a title="345-tfidf-16" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>17 0.14384417 <a title="345-tfidf-17" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>18 0.14350784 <a title="345-tfidf-18" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>19 0.14020123 <a title="345-tfidf-19" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>20 0.13994588 <a title="345-tfidf-20" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.289), (1, 0.177), (2, 0.013), (3, -0.084), (4, -0.069), (5, -0.084), (6, 0.141), (7, -0.104), (8, 0.085), (9, 0.132), (10, -0.034), (11, 0.143), (12, 0.013), (13, 0.101), (14, 0.112), (15, 0.018), (16, -0.046), (17, -0.145), (18, -0.012), (19, 0.029), (20, -0.008), (21, -0.039), (22, 0.111), (23, -0.025), (24, -0.085), (25, -0.052), (26, -0.049), (27, 0.001), (28, 0.042), (29, 0.042), (30, 0.021), (31, 0.001), (32, -0.041), (33, 0.019), (34, -0.025), (35, 0.055), (36, 0.058), (37, 0.057), (38, 0.042), (39, -0.002), (40, -0.005), (41, 0.013), (42, 0.051), (43, 0.005), (44, 0.02), (45, 0.065), (46, 0.025), (47, -0.029), (48, -0.058), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97054726 <a title="345-lsi-1" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>Author: Karl Pauwels, Leonardo Rubio, Javier Díaz, Eduardo Ros</p><p>Abstract: We propose a novel model-based method for estimating and tracking the six-degrees-of-freedom (6DOF) pose of rigid objects of arbitrary shapes in real-time. By combining dense motion and stereo cues with sparse keypoint correspondences, and by feeding back information from the model to the cue extraction level, the method is both highly accurate and robust to noise and occlusions. A tight integration of the graphical and computational capability of Graphics Processing Units (GPUs) results in pose updates at framerates exceeding 60 Hz. Since a benchmark dataset that enables the evaluation of stereo-vision-based pose estimators in complex scenarios is currently missing in the literature, we have introduced a novel synthetic benchmark dataset with varying objects, background motion, noise and occlusions. Using this dataset and a novel evaluation methodology, we show that the proposed method greatly outperforms state-of-the-art methods. Finally, we demonstrate excellent performance on challenging real-world sequences involving object manipulation.</p><p>2 0.71538001 <a title="345-lsi-2" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>Author: Katerina Fragkiadaki, Han Hu, Jianbo Shi</p><p>Abstract: Human pose detectors, although successful in localising faces and torsos of people, often fail with lower arms. Motion estimation is often inaccurate under fast movements of body parts. We build a segmentation-detection algorithm that mediates the information between body parts recognition, and multi-frame motion grouping to improve both pose detection and tracking. Motion of body parts, though not accurate, is often sufficient to segment them from their backgrounds. Such segmentations are crucialfor extracting hard to detect body parts out of their interior body clutter. By matching these segments to exemplars we obtain pose labeled body segments. The pose labeled segments and corresponding articulated joints are used to improve the motion flow fields by proposing kinematically constrained affine displacements on body parts. The pose-based articulated motion model is shown to handle large limb rotations and displacements. Our algorithm can detect people under rare poses, frequently missed by pose detectors, showing the benefits of jointly reasoning about pose, segmentation and motion in videos.</p><p>3 0.70580745 <a title="345-lsi-3" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>4 0.68586171 <a title="345-lsi-4" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>5 0.65338588 <a title="345-lsi-5" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>Author: Jonathan Balzer, Stefano Soatto</p><p>Abstract: We describe a method to efficiently generate a model (map) of small-scale objects from video. The map encodes sparse geometry as well as coarse photometry, and could be used to initialize dense reconstruction schemes as well as to support recognition and localization of three-dimensional objects. Self-occlusions and the predominance of outliers present a challenge to existing online Structure From Motion and Simultaneous Localization and Mapping systems. We propose a unified inference criterion that encompasses map building and localization (object detection) relative to the map in a coupled fashion. We establish correspondence in a computationally efficient way without resorting to combinatorial matching or random-sampling techniques. Instead, we use a simpler M-estimator that exploits putative correspondence from tracking after photometric and topological validation. We have collected a new dataset to benchmark model building in the small scale, which we test our algorithm on in comparison to others. Although our system is significantly leaner than previous ones, it compares favorably to the state of the art in terms of accuracy and robustness.</p><p>6 0.63857669 <a title="345-lsi-6" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>7 0.63738072 <a title="345-lsi-7" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>8 0.6354171 <a title="345-lsi-8" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>9 0.63304937 <a title="345-lsi-9" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>10 0.62807429 <a title="345-lsi-10" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>11 0.62375188 <a title="345-lsi-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.6148147 <a title="345-lsi-12" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>13 0.60674727 <a title="345-lsi-13" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>14 0.60395604 <a title="345-lsi-14" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>15 0.58913481 <a title="345-lsi-15" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>16 0.5823406 <a title="345-lsi-16" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>17 0.57972723 <a title="345-lsi-17" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>18 0.57784855 <a title="345-lsi-18" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>19 0.57089794 <a title="345-lsi-19" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>20 0.56966013 <a title="345-lsi-20" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.101), (16, 0.025), (26, 0.037), (33, 0.264), (67, 0.413), (69, 0.038), (87, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96417642 <a title="345-lda-1" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>Author: Pramod Sharma, Ram Nevatia</p><p>Abstract: In this work, we present a novel and efficient detector adaptation method which improves the performance of an offline trained classifier (baseline classifier) by adapting it to new test datasets. We address two critical aspects of adaptation methods: generalizability and computational efficiency. We propose an adaptation method, which can be applied to various baseline classifiers and is computationally efficient also. For a given test video, we collect online samples in an unsupervised manner and train a randomfern adaptive classifier . The adaptive classifier improves precision of the baseline classifier by validating the obtained detection responses from baseline classifier as correct detections or false alarms. Experiments demonstrate generalizability, computational efficiency and effectiveness of our method, as we compare our method with state of the art approaches for the problem of human detection and show good performance with high computational efficiency on two different baseline classifiers.</p><p>2 0.96152997 <a title="345-lda-2" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>3 0.93904001 <a title="345-lda-3" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>4 0.8984201 <a title="345-lda-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.89412344 <a title="345-lda-5" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>Author: Georgia Gkioxari, Pablo Arbeláez, Lubomir Bourdev, Jitendra Malik</p><p>Abstract: We propose a novel approach for human pose estimation in real-world cluttered scenes, and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose, we build on the notion of poselets [4] and train highly discriminative classifiers to differentiate among arm configurations, which we call armlets. We propose a rich representation which, in addition to standardHOGfeatures, integrates the information of strong contours, skin color and contextual cues in a principled manner. Unlike existing methods, we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset, where critical visual phenomena, such as occlusion, truncation, multiple instances and clutter are the norm. Our approach outperforms Yang and Ramanan [26], the state-of-the-art technique, with an improvement from 29.0% to 37.5% PCP accuracy on the arm keypoint prediction task, on this new pose estimation dataset.</p><p>6 0.88643312 <a title="345-lda-6" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>7 0.88168436 <a title="345-lda-7" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>8 0.873528 <a title="345-lda-8" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>9 0.86391199 <a title="345-lda-9" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>same-paper 10 0.8506633 <a title="345-lda-10" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>11 0.82693863 <a title="345-lda-11" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>12 0.81800008 <a title="345-lda-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.81589991 <a title="345-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.80053473 <a title="345-lda-14" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>15 0.79934388 <a title="345-lda-15" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>16 0.79187101 <a title="345-lda-16" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>17 0.76929045 <a title="345-lda-17" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>18 0.76799333 <a title="345-lda-18" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>19 0.76480377 <a title="345-lda-19" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>20 0.76405811 <a title="345-lda-20" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
