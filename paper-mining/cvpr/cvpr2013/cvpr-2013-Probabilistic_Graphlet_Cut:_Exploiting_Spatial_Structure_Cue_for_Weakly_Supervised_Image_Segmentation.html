<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-339" href="#">cvpr2013-339</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</h1>
<br/><p>Source: <a title="cvpr-2013-339-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_Probabilistic_Graphlet_Cut_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen</p><p>Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.</p><p>Reference: <a title="cvpr-2013-339-reference" href="../cvpr2013_reference/cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. [sent-5, score-0.369]
</p><p>2 Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. [sent-6, score-1.436]
</p><p>3 Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. [sent-7, score-0.812]
</p><p>4 Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. [sent-9, score-1.493]
</p><p>5 Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models. [sent-10, score-0.375]
</p><p>6 The ignorance of spatial structure in measuring the homogeneity nocfe superpixels: beyond tnh me appearance f heoa-tures, the spatial structure of superpixels is also important for measuring their homogeneity. [sent-26, score-0.424]
</p><p>7 However, the triangularly arranged superpixels are distinctive for the pyramid, thus they should be assigned with strong homogeneity and are further encouraged to merge. [sent-29, score-0.414]
</p><p>8 111999000866  To address the above two problems, we propose to learn the  distribution of graphlets from image-level labels and use the learned distribution to guide the image segmentation. [sent-30, score-0.679]
</p><p>9 To capture the spatial structure of superpixels, we extract graphlets by connecting spatially neighboring superpixels, wherein the graphlets are small-sized graphs effectively capturing the neighboring structures of superpixels. [sent-31, score-1.301]
</p><p>10 Because different-sized graphlets are incomparable in Euclidean space, we project graphlets onto the Grassmann manifold and subsequently develop a manifold embedding algorithm which incorporates image-level labels into graphlets. [sent-32, score-1.565]
</p><p>11 Through the embedding, different-sized graphlets are transformed into equal length feature vectors, thus making it possible to model the distribution of graphlets. [sent-33, score-0.624]
</p><p>12 Since the learned graphlet distribution reflects the spatial structure of superpixels, we propose a new segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution. [sent-34, score-1.892]
</p><p>13 Related Work Recently, several weakly supervised image segmentation methods have been proposed, which focus on developing statistical models to transfer image-level labels into superpixels unary or pairwise potentials. [sent-36, score-0.479]
</p><p>14 [7] proposed a graphical model, termed multi-image model (MIM), to integrate image appearance features, image-level labels and superpixel labels into one network. [sent-43, score-0.235]
</p><p>15 [8] designed an active learning scheme to select a few semantically most uncertain superpixels within an image. [sent-45, score-0.206]
</p><p>16 The selected superpixels are accurately labeled by querying an oracle database, and they guide the label inference for the remaining superpixels. [sent-46, score-0.231]
</p><p>17 [9] developed a parametric family of structured models, where multi-channel visual features are employed to form the pairwise potential, and the weights of each channel is computed by minimizing the discrepancy between superpixels labeled by segmentation models trained by different image sets. [sent-48, score-0.337]
</p><p>18 One weakness of these weakly supervised segmentation methods is the low descriptive unary/pairwise potentials, resulting in many ambiguous segment boundaries. [sent-49, score-0.238]
</p><p>19 To alleviate this problem, high-order potentials among superpixels are exploited to refine image segmentation. [sent-50, score-0.232]
</p><p>20 An overview As shown in Figure 2, the proposed approach learns the distribution of graphlets [23] and then facilitates image segmentation based on the learned graphlet distribution. [sent-64, score-1.313]
</p><p>21 We first extract graphlets from each image, which capture the spatial structure of the superpixels. [sent-65, score-0.652]
</p><p>22 Then, the extracted graphlets are projected onto the Grassmann manifold and a manifold embedding algorithm is proposed to integrate image-level labels, global spatial layout, and rough geometric context, into graphlets. [sent-66, score-0.996]
</p><p>23 After embedding, graphlets are transformed into equal length feature vectors, and we use GMM to learn their distribution. [sent-67, score-0.624]
</p><p>24 Finally, we propose a graphlet cut algorithm based on the homo-  geneity measure for image segmentation. [sent-69, score-0.655]
</p><p>25 To avoid the high computational cost caused by such gigantic amount of pixels in image segmentation, these pixels are clustered into superpixels and further associated with their spatial structure. [sent-73, score-0.234]
</p><p>26 And since graph is a natural and powerful tool to describe the relationships between objects, usually  the region adjacency graph (RAG) is adopted to model the superpixels and their spatial structures, i. [sent-74, score-0.234]
</p><p>27 First, the appearance and spatial structure of the superpixels collaboratively contribute to their homogeneity. [sent-79, score-0.234]
</p><p>28 As shown in Figure 1, the superpixel set in the sky region and the superpixel set in the sand region have similar spatial structure but different superpixel appearance, thus they should be assigned with different homogeneities. [sent-81, score-0.411]
</p><p>29 Compared with the stripe-distributed yellow superpixels, the strip distributed blue superpixels appear more common in semantic objects, such as lake and river, which indicates they are low correlated with any particular semantic object, thus should be assigned with a weaker homogeneity. [sent-82, score-0.293]
</p><p>30 On the other hand, the pyramid-covered and the sand-covered superpixel sets have similar superpixel appearance but different spatial structure, thus they should also be assigned with different homogeneities. [sent-83, score-0.259]
</p><p>31 Compared with the stripe distributed yellow superpixels, the triangularly distributed  yellow superpixels are unique for the Egyptian pyramid, thus they should be assigned with a stronger homogeneity. [sent-84, score-0.252]
</p><p>32 We propose to use graphlets to capture the appearance and spatial structure of superpixels. [sent-85, score-0.652]
</p><p>33 The graphlets are obtained by extracting connected subgraphs from an RAG. [sent-86, score-0.624]
</p><p>34 The size of a graphlet is defined as the number of its constituent superpixels. [sent-87, score-0.578]
</p><p>35 In this work, only small-sized graphlets are adopted because: 1) the number of all the possible graphlets is exponentially increasing with its size; 2) the graphlet embedding implicitly extends the homogeneity beyond single small-sized graphlets. [sent-88, score-2.102]
</p><p>36 3); 3) empirical results show that the segmentation accuracy stops increasing when the graphlet size increases from 5 to 10, thus small-sized graphlets are descriptive enough. [sent-91, score-1.333]
</p><p>37 Let T denote the maximum graphlet size, we extract graphlets of all sizes ranging from 2 to T. [sent-92, score-1.202]
</p><p>38 The graphlet extraction is based on depth-first search, which is computationally efficient. [sent-93, score-0.578]
</p><p>39 Given 50 superpixels in an image, and assuming the average superpixel degree is 5 and the maximum graphlet size is also 5, there are 50 ∗ 55/5! [sent-95, score-0.89]
</p><p>40 Note that graphlets extend the non-structural homogeneity of superpixels [1, 2]. [sent-100, score-0.992]
</p><p>41 As shown in Figure 3, both the pairwise and high-order potentials represent the homogeneity of orderless superpixels, whereas the  ×  graphlet represents the homogeneity of spatially structured superpixels. [sent-101, score-0.973]
</p><p>42 If we ignore graphlet topology, the proposed graphlet-based homogeneity reduces to the high-order potential homogeneity. [sent-102, score-0.74]
</p><p>43 A quantitative description of graphlets is necessary  Figure3. [sent-103, score-0.624]
</p><p>44 Given a t-sized graphlet, we characterize the appearance of its superpixels as a matrix Mr. [sent-125, score-0.206]
</p><p>45 0θ(Ri,Rojt)heriwf Riseiand Rjare spatially adjacent  (2) where θ(Ri, Rj) is the angle between the positive horizontal direction and the vector from the center of superpixel Ri to the center of superpixel Rj . [sent-130, score-0.237]
</p><p>46 Based on Mr and Ms, a t-  sized graphlet can be represented by a t is. [sent-131, score-0.578]
</p><p>47 To measure the distance between graphlets on the Grassmann manifold, their Golub-Werman distance is defined as: dGW(M, M? [sent-134, score-0.624]
</p><p>48 2, the appearance and spatial structures of semantically-consistent superpixels reflect strong homogeneity. [sent-143, score-0.234]
</p><p>49 Thus, it is necessary to integrate category information into graphlets in measuring the homogeneity of superpixels. [sent-144, score-0.805]
</p><p>50 To this end, a manifold embedding algorithm is proposed to encode image-level labels into graphlets. [sent-145, score-0.243]
</p><p>51 This is helpful  to expand the homogeneity of superpixels across individual graphlets. [sent-149, score-0.368]
</p><p>52 As shown in the left of Figure 4, preserving the relative distances in the embedding process encodes global spatial layout into graphlets, which implicitly extends the homogeneity beyond the individual small-sized graphlets. [sent-150, score-0.329]
</p><p>53 This motivates us to integrate geometric context information into the embedding process. [sent-154, score-0.213]
</p><p>54 Intuitively, a graphlet with consistent geometric context should reflect stronger homogeneity. [sent-155, score-0.658]
</p><p>55 As shown in the right of Figure 4, graphlet G1 has more consistent geometric context than graphlet G2, thus superpixels within G1 should be assigned with stronger homogeneity than those within G2. [sent-156, score-1.623]
</p><p>56 ;BRight:  adding rough geometric context into graphlets, ground(green), sky(blue), different oriented vertical regions(red), non-planar solid(‘x’)  labels, global spatial layout, and geometric context, we propose a manifold embedding algorithm with the objective function defined as: argmYin? [sent-159, score-0.359]
</p><p>57 That is, graphlets with more consistent geometric context are assigned with smaller weights. [sent-190, score-0.723]
</p><p>58 Let b(G) denote the C-dimensional row vector containing the  class label of the image corresponding to graphlets G. [sent-203, score-0.649]
</p><p>59 the i-th graphlet, which is implemented as the i-th graphlet entropy, i. [sent-212, score-0.578]
</p><p>60 metric context obtained from the i-th graphlet in the h-th training image. [sent-218, score-0.62]
</p><p>61 Denoting DGhW = [dGW (Mih, Mjh)] as the matrix whose entry dGW (Mih, Mjh) is the Golub-Werman distance between the i-th and j-th identical-sized graphlets extracted from the h-th image. [sent-219, score-0.624]
</p><p>62 Assuming the maximum graphlet size is T, the embedding is repeated T times. [sent-246, score-0.692]
</p><p>63 Probabilistic graphlet cut After the embedding process, graphlets of different sizes are transformed into d-dimensional feature vectors. [sent-257, score-1.393]
</p><p>64 To employ these post-embedding graphlets for image segmentation, we train a standard GMM to model their distribution. [sent-258, score-0.624]
</p><p>65 Given an post-embedding graphlet f(Gtest) from the test image, the homogeneity of its superpixels is computed by: p(f(Gtest)|θ)  =? [sent-259, score-0.946]
</p><p>66 Next, we apply the graphlet-based homogeneity measure for image segmentation in the normalized cut framework. [sent-261, score-0.35]
</p><p>67 First, the conventional normalized  cut measures the similarity between superpixels using the distance between their appearance feature vectors, whereas our approach measures their similarity by taking into consideration their spatial structures. [sent-263, score-0.33]
</p><p>68 Second, conventional normalized cut fails to incorporate supervision, while our approach integrates image-level labels, global spatial layout, and rough geometric context to refine the segmentation process. [sent-264, score-0.34]
</p><p>69 The term G ⊇ (u, v) contains all the parent graphlets of superpixel pair ⊇(u, ( vu,),v a)n cdo n1t/a|iGns| faulln tchteio pnasr as a rnaoprmhleatliszoa ftsiuopne frapictxoelr. [sent-275, score-0.73]
</p><p>70 The two denominators in (11) respectively accumulate connections from superpixels in set V1 and V2 to the entire superpixels, i. [sent-276, score-0.206]
</p><p>71 (u,v)p(G|θ) Similar to many segmentation methods such as [6], which assign semantics to segmented regions, our approach can also label semantics for each superpixel. [sent-286, score-0.21]
</p><p>72 Particularly, we first learn a multi-label SVM based on the d-dimensional post-embedding graphlets and the category labels of the images from which the graphlets are extracted. [sent-287, score-1.303]
</p><p>73 Given a test graphlet Gtest, based on the probabilistic output of SVM [17], we obtain its probability of belonging to semantic class c : p(Gtest → c), and the semantic label of segmented region R is computed by tmhaex siemmuamnt majority voting of all its spatially overlapping graphlets:  argmcax? [sent-288, score-0.724]
</p><p>74 lementation procedure of the proposed probabilistic graphlet cut is given in Table 1. [sent-291, score-0.655]
</p><p>75 Construct RAG for each image and extracted graphelts from these RAGs; then use manifold graphelt embedding to transformeach graphlet into d-dimensional feature vector according to (5); 2. [sent-294, score-0.833]
</p><p>76 Learn a multi-label SVM based on the post-embedding graphlets and the image-level labels; //test stage: input: a test image Itest and its image-level label ctest ; the number of segmented regions L; output: a segmentation mask of Itest; 1. [sent-296, score-0.788]
</p><p>77 Construct RAG for Itest and extracted its graphelts; Using the trained manifold embedding model to represent each graphelt by a d-dimensional feature vector; 2. [sent-297, score-0.215]
</p><p>78 To compare our approach with the existing weakly supervised segmentation methods, we carry out experiments on SIFT-flow, because the objects are of diversified structures and the image-level labels are off-the-shelf. [sent-307, score-0.273]
</p><p>79 Additionally, it is important to compare our approach with fully supervised segmentation methods, because the comparative results show how effectively the image-level labels facilitate image segmentation. [sent-308, score-0.212]
</p><p>80 , graphlet extraction, manifold graphlet embedding, and the proba-  bilistic segmentation model. [sent-330, score-1.341]
</p><p>81 To justify the effectiveness of graphlets for weaklysupervised segmentation, two experimental settings are adopted to weaken the description power of the graphlets. [sent-331, score-0.647]
</p><p>82 First, we reduce graphlets to superpixels, that is, 1-sized graphlet which captures no spatial structure of superpixels. [sent-332, score-1.23]
</p><p>83 We can see that segmentation using superpixels or non-structural graphlets results in many ambiguous segment boundaries. [sent-335, score-0.941]
</p><p>84 To justify the effectiveness of manifold graphlet embedding, three experimental settings are used. [sent-336, score-0.675]
</p><p>85 Furthermore, very poor segmentation results are observed when kernel PCA is adopted because both geometric context and image-level label supervision are neglected. [sent-366, score-0.238]
</p><p>86 To justify the effectiveness of the probabilistic segmentation model, we restrict the graphlet size to two and thus only binary relationships of superpixels are exploited in the normalized cut based segmentation. [sent-367, score-0.995]
</p><p>87 As shown in Figure 5, segmentation with 2-sized graphlets results in numerous over-segmented patches, because of the limited superpixel label smoothing capability of 2-sized graphlets. [sent-368, score-0.866]
</p><p>88 Effects of maximum graphlet size The maximum graphlet size T influences significantly the segmentation results. [sent-383, score-1.267]
</p><p>89 Segmentation accuracy and time consumption per image under different maximum graphlet size T  T45213S79e830. [sent-388, score-0.597]
</p><p>90 This implies that 6-sized graphlet is adequately descriptive to capture the homogeneity of superpixels. [sent-395, score-0.76]
</p><p>91 Second, segmentation time increases exponentially as the graphlet size goes up. [sent-396, score-0.689]
</p><p>92 Such semantic relationships are well captured by the proposed graphlets; 2) The objects in these groups are sufficiently large relative to the superpixel size, thus not many superpixels span multiple objects; 3) photos in SIFT-flow are accurately assigned with multiple image-level labels. [sent-406, score-0.39]
</p><p>93 The proposed graphlet embedding method effectively leverages the image-level labels to refine the segmentation process. [sent-407, score-0.877]
</p><p>94 Conclusions and future work In this paper, we have presentd a weakly supervised image segmentation method by learning the distribution of s111999111422  #$  ? [sent-414, score-0.218]
</p><p>95 Example segmentation results on SIFT-flow (OP: original photo, GT: ground truth, PM: proposed method) patially structured superpixel sets. [sent-464, score-0.237]
</p><p>96 We introduced the notion of graphlet that captures the spatial structures of superpixels. [sent-465, score-0.606]
</p><p>97 To integrate image-level labels, a manifold embedding technique is proposed to transform different-sized graphlets into equal length feature vectors. [sent-466, score-0.831]
</p><p>98 The embedding allows us to use GMM to learn the distribution of the embedded graphlets, which is used to measure the homogeneity of superpixels for image segmentation. [sent-467, score-0.482]
</p><p>99 An important property of such homogeneity measure is that it takes the spatial structure of the superpixels into consideration. [sent-468, score-0.396]
</p><p>100 In the future, we will investigate an active-learning [8]based graphlet selection scheme to accelerate image segmentation, and a new semi-supervised [22] segmentation framework that simultaneously decomposes an image into regions and determines their semantics. [sent-470, score-0.689]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('graphlets', 0.624), ('graphlet', 0.578), ('superpixels', 0.206), ('homogeneity', 0.162), ('dgw', 0.133), ('embedding', 0.114), ('segmentation', 0.111), ('superpixel', 0.106), ('gtest', 0.093), ('cut', 0.077), ('manifold', 0.074), ('weakly', 0.061), ('itest', 0.059), ('vezhnevets', 0.059), ('ih', 0.058), ('labels', 0.055), ('jh', 0.054), ('buhmann', 0.053), ('dghw', 0.053), ('mih', 0.053), ('mjh', 0.053), ('proc', 0.051), ('supervised', 0.046), ('sky', 0.046), ('assoc', 0.044), ('argmaxy', 0.044), ('grassmann', 0.043), ('context', 0.042), ('ij', 0.041), ('gmm', 0.04), ('graphelts', 0.04), ('rag', 0.04), ('verzhnevetz', 0.04), ('geometric', 0.038), ('vittorio', 0.035), ('joachim', 0.035), ('chun', 0.035), ('luming', 0.035), ('mingli', 0.035), ('semantic', 0.034), ('yj', 0.033), ('jiajun', 0.033), ('mim', 0.033), ('geo', 0.029), ('segmented', 0.028), ('spatial', 0.028), ('acsusto', 0.027), ('argmyaxy', 0.027), ('cbn', 0.027), ('enh', 0.027), ('gcj', 0.027), ('gmim', 0.027), ('graphelt', 0.027), ('rital', 0.027), ('rnh', 0.027), ('triangularly', 0.027), ('yih', 0.027), ('yjh', 0.027), ('mo', 0.026), ('voc', 0.026), ('pushmeet', 0.026), ('potentials', 0.026), ('hypergraph', 0.026), ('song', 0.026), ('label', 0.025), ('photos', 0.025), ('alexander', 0.025), ('rough', 0.025), ('layout', 0.025), ('spatially', 0.025), ('hcrf', 0.024), ('feiping', 0.024), ('lubor', 0.024), ('dacheng', 0.024), ('semantics', 0.023), ('justify', 0.023), ('yt', 0.022), ('supervision', 0.022), ('bu', 0.022), ('argminy', 0.022), ('textonboost', 0.021), ('yh', 0.021), ('inh', 0.021), ('ferrari', 0.02), ('structured', 0.02), ('descriptive', 0.02), ('gi', 0.02), ('zicheng', 0.02), ('assigned', 0.019), ('integrate', 0.019), ('ms', 0.019), ('ls', 0.019), ('consumption', 0.019), ('leverages', 0.019), ('dominated', 0.019), ('storage', 0.019), ('conventional', 0.019), ('road', 0.018), ('philip', 0.018), ('cv', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="339-tfidf-1" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>Author: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen</p><p>Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.</p><p>2 0.12863249 <a title="339-tfidf-2" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>3 0.12733814 <a title="339-tfidf-3" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>Author: Guang Shu, Afshin Dehghan, Mubarak Shah</p><p>Abstract: We propose an approach to improve the detection performance of a generic detector when it is applied to a particular video. The performance of offline-trained objects detectors are usually degraded in unconstrained video environments due to variant illuminations, backgrounds and camera viewpoints. Moreover, most object detectors are trained using Haar-like features or gradient features but ignore video specificfeatures like consistent colorpatterns. In our approach, we apply a Superpixel-based Bag-of-Words (BoW) model to iteratively refine the output of a generic detector. Compared to other related work, our method builds a video-specific detector using superpixels, hence it can handle the problem of appearance variation. Most importantly, using Conditional Random Field (CRF) along with our super pixel-based BoW model, we develop and algorithm to segment the object from the background . Therefore our method generates an output of the exact object regions instead of the bounding boxes generated by most detectors. In general, our method takes detection bounding boxes of a generic detector as input and generates the detection output with higher average precision and precise object regions. The experiments on four recent datasets demonstrate the effectiveness of our approach and significantly improves the state-of-art detector by 5-16% in average precision.</p><p>4 0.12106155 <a title="339-tfidf-4" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>Author: Gautam Singh, Jana Kosecka</p><p>Abstract: This paper presents a nonparametric approach to semantic parsing using small patches and simple gradient, color and location features. We learn the relevance of individual feature channels at test time using a locally adaptive distance metric. To further improve the accuracy of the nonparametric approach, we examine the importance of the retrieval set used to compute the nearest neighbours using a novel semantic descriptor to retrieve better candidates. The approach is validated by experiments on several datasets used for semantic parsing demonstrating the superiority of the method compared to the state of art approaches.</p><p>5 0.1196192 <a title="339-tfidf-5" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>Author: Yan Wang, Rongrong Ji, Shih-Fu Chang</p><p>Abstract: Recent years have witnessed a growing interest in understanding the semantics of point clouds in a wide variety of applications. However, point cloud labeling remains an open problem, due to the difficulty in acquiring sufficient 3D point labels towards training effective classifiers. In this paper, we overcome this challenge by utilizing the existing massive 2D semantic labeled datasets from decadelong community efforts, such as ImageNet and LabelMe, and a novel “cross-domain ” label propagation approach. Our proposed method consists of two major novel components, Exemplar SVM based label propagation, which effectively addresses the cross-domain issue, and a graphical model based contextual refinement incorporating 3D constraints. Most importantly, the entire process does not require any training data from the target scenes, also with good scalability towards large scale applications. We evaluate our approach on the well-known Cornell Point Cloud Dataset, achieving much greater efficiency and comparable accuracy even without any 3D training data. Our approach shows further major gains in accuracy when the training data from the target scenes is used, outperforming state-ofthe-art approaches with far better efficiency.</p><p>6 0.11561123 <a title="339-tfidf-6" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>7 0.10634094 <a title="339-tfidf-7" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>8 0.10086939 <a title="339-tfidf-8" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>9 0.096610822 <a title="339-tfidf-9" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>10 0.091760918 <a title="339-tfidf-10" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>11 0.090015225 <a title="339-tfidf-11" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>12 0.086610414 <a title="339-tfidf-12" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>13 0.07699763 <a title="339-tfidf-13" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>14 0.074087583 <a title="339-tfidf-14" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>15 0.072326489 <a title="339-tfidf-15" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>16 0.07022249 <a title="339-tfidf-16" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>17 0.064561896 <a title="339-tfidf-17" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>18 0.060197502 <a title="339-tfidf-18" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>19 0.060048763 <a title="339-tfidf-19" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>20 0.056853335 <a title="339-tfidf-20" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.012), (2, 0.027), (3, 0.0), (4, 0.09), (5, 0.016), (6, 0.02), (7, 0.042), (8, -0.107), (9, -0.006), (10, 0.141), (11, -0.065), (12, -0.014), (13, 0.043), (14, -0.034), (15, 0.01), (16, 0.02), (17, -0.045), (18, -0.121), (19, 0.073), (20, 0.066), (21, 0.039), (22, -0.051), (23, 0.021), (24, -0.097), (25, 0.024), (26, -0.088), (27, 0.032), (28, 0.009), (29, 0.026), (30, 0.008), (31, -0.038), (32, -0.028), (33, -0.015), (34, -0.004), (35, -0.052), (36, -0.035), (37, 0.006), (38, 0.048), (39, -0.028), (40, 0.011), (41, 0.006), (42, -0.014), (43, -0.007), (44, -0.019), (45, -0.009), (46, -0.033), (47, 0.021), (48, -0.004), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91985893 <a title="339-lsi-1" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>Author: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen</p><p>Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.</p><p>2 0.79360092 <a title="339-lsi-2" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>Author: Jason Chang, Donglai Wei, John W. Fisher_III</p><p>Abstract: We develop a generative probabilistic model for temporally consistent superpixels in video sequences. In contrast to supervoxel methods, object parts in different frames are tracked by the same temporal superpixel. We explicitly model flow between frames with a bilateral Gaussian process and use this information to propagate superpixels in an online fashion. We consider four novel metrics to quantify performance of a temporal superpixel representation and demonstrate superior performance when compared to supervoxel methods.</p><p>3 0.78143942 <a title="339-lsi-3" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>4 0.74854892 <a title="339-lsi-4" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>Author: Yan Wang, Rongrong Ji, Shih-Fu Chang</p><p>Abstract: Recent years have witnessed a growing interest in understanding the semantics of point clouds in a wide variety of applications. However, point cloud labeling remains an open problem, due to the difficulty in acquiring sufficient 3D point labels towards training effective classifiers. In this paper, we overcome this challenge by utilizing the existing massive 2D semantic labeled datasets from decadelong community efforts, such as ImageNet and LabelMe, and a novel “cross-domain ” label propagation approach. Our proposed method consists of two major novel components, Exemplar SVM based label propagation, which effectively addresses the cross-domain issue, and a graphical model based contextual refinement incorporating 3D constraints. Most importantly, the entire process does not require any training data from the target scenes, also with good scalability towards large scale applications. We evaluate our approach on the well-known Cornell Point Cloud Dataset, achieving much greater efficiency and comparable accuracy even without any 3D training data. Our approach shows further major gains in accuracy when the training data from the target scenes is used, outperforming state-ofthe-art approaches with far better efficiency.</p><p>5 0.73867482 <a title="339-lsi-5" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>Author: Xiaobai Liu, Liang Lin, Alan L. Yuille</p><p>Abstract: In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.</p><p>6 0.73712939 <a title="339-lsi-6" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>7 0.72650522 <a title="339-lsi-7" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>8 0.72009403 <a title="339-lsi-8" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>9 0.68745899 <a title="339-lsi-9" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>10 0.66662115 <a title="339-lsi-10" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>11 0.61813086 <a title="339-lsi-11" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>12 0.59939212 <a title="339-lsi-12" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>13 0.59587348 <a title="339-lsi-13" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>14 0.56497562 <a title="339-lsi-14" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>15 0.54809791 <a title="339-lsi-15" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>16 0.5347085 <a title="339-lsi-16" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>17 0.52031916 <a title="339-lsi-17" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>18 0.49560383 <a title="339-lsi-18" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>19 0.49159169 <a title="339-lsi-19" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>20 0.49063697 <a title="339-lsi-20" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.106), (16, 0.016), (26, 0.03), (33, 0.214), (67, 0.43), (69, 0.028), (87, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93369734 <a title="339-lda-1" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>Author: Pramod Sharma, Ram Nevatia</p><p>Abstract: In this work, we present a novel and efficient detector adaptation method which improves the performance of an offline trained classifier (baseline classifier) by adapting it to new test datasets. We address two critical aspects of adaptation methods: generalizability and computational efficiency. We propose an adaptation method, which can be applied to various baseline classifiers and is computationally efficient also. For a given test video, we collect online samples in an unsupervised manner and train a randomfern adaptive classifier . The adaptive classifier improves precision of the baseline classifier by validating the obtained detection responses from baseline classifier as correct detections or false alarms. Experiments demonstrate generalizability, computational efficiency and effectiveness of our method, as we compare our method with state of the art approaches for the problem of human detection and show good performance with high computational efficiency on two different baseline classifiers.</p><p>2 0.92747796 <a title="339-lda-2" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>3 0.89963621 <a title="339-lda-3" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>4 0.84961176 <a title="339-lda-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.84457541 <a title="339-lda-5" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>Author: Georgia Gkioxari, Pablo Arbeláez, Lubomir Bourdev, Jitendra Malik</p><p>Abstract: We propose a novel approach for human pose estimation in real-world cluttered scenes, and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose, we build on the notion of poselets [4] and train highly discriminative classifiers to differentiate among arm configurations, which we call armlets. We propose a rich representation which, in addition to standardHOGfeatures, integrates the information of strong contours, skin color and contextual cues in a principled manner. Unlike existing methods, we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset, where critical visual phenomena, such as occlusion, truncation, multiple instances and clutter are the norm. Our approach outperforms Yang and Ramanan [26], the state-of-the-art technique, with an improvement from 29.0% to 37.5% PCP accuracy on the arm keypoint prediction task, on this new pose estimation dataset.</p><p>6 0.8344599 <a title="339-lda-6" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>7 0.83262545 <a title="339-lda-7" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>8 0.81953984 <a title="339-lda-8" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>9 0.8089143 <a title="339-lda-9" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>10 0.79343975 <a title="339-lda-10" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>same-paper 11 0.76945645 <a title="339-lda-11" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>12 0.76331335 <a title="339-lda-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.7538271 <a title="339-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.73893124 <a title="339-lda-14" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>15 0.73626351 <a title="339-lda-15" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>16 0.72911066 <a title="339-lda-16" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>17 0.70359814 <a title="339-lda-17" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>18 0.70073259 <a title="339-lda-18" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>19 0.69616544 <a title="339-lda-19" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>20 0.69583833 <a title="339-lda-20" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
