<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-209" href="#">cvpr2013-209</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</h1>
<br/><p>Source: <a title="cvpr-2013-209-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hofmann_Hypergraphs_for_Joint_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>Reference: <a title="cvpr-2013-209-reference" href="../cvpr2013_reference/cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de f@ l  Abstract We generalize the network flow formulation for multiobject tracking to multi-camera setups. [sent-7, score-0.622]
</p><p>2 In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. [sent-9, score-0.272]
</p><p>3 The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. [sent-11, score-0.451]
</p><p>4 Introduction In this work, we consider the problem of tracking a variable number of objects in setups with multiple overlapping views. [sent-15, score-0.375]
</p><p>5 Thus, given a set of object detections in each frame and from each  camera, the problem of tracking becomes a data association problem. [sent-17, score-0.6]
</p><p>6 Challenges of this association problem include false positives and the fact that detections may be missing due to false negatives of the detector or due to occlusions. [sent-18, score-0.532]
</p><p>7 The main contribution in this work is an extension of the global tracking framework to setups with multiple overlapping views. [sent-24, score-0.375]
</p><p>8 Besides temporal data association, as used in the single camera case, the additional problem of data association between cameras arises in the multi-camera approach. [sent-25, score-0.347]
</p><p>9 In their work, one tracking graph is constructed for each view and the multi-camera couplings are incorporated by an additional tracking graph for each possible camera pair. [sent-28, score-0.978]
</p><p>10 The tracking task is first modeled as a global maximum a  posteriori problem (similar to [21]), however, the formulation is generalized to take the multi-camera couplings into account. [sent-32, score-0.463]
</p><p>11 Experimental results show that the method can successfully leverage input from multiple cameras and results outperform related methods on low, medium and dense tracking scenarios of the PETS 2009 database [8]. [sent-34, score-0.674]
</p><p>12 Local tracking approaches (where each target is tracked independently) using for example the Kalman Filter [19] have high precision and localization accuracy, but fail in multi-object scenarios where association of detections and trajectories becomes a major issue. [sent-37, score-0.718]
</p><p>13 Another class of recent and very successful approaches define tracking as a global optimization over the complete sequence [21, 16, 10]. [sent-40, score-0.334]
</p><p>14 Figure 1: Example showing detections (in worldcoordinates) of three cameras in three frames. [sent-107, score-0.248]
</p><p>15 Arrows indicate temporal tracking edges (here, only final tracking edges are shown). [sent-110, score-0.715]
</p><p>16 Thus far, reconstruction and tracking have been handled as separate stages (see [20] for a comparison). [sent-115, score-0.497]
</p><p>17 An attempt to jointly solve these two problems for multi-camera multi-target tracking has recently been presented in [14]. [sent-116, score-0.334]
</p><p>18 In this work, a separate tracking graph is constructed for each view. [sent-117, score-0.393]
</p><p>19 In addition, for each pair of cameras, an additional tracking graph is constructed, providing the coupling constraints for the involved views. [sent-118, score-0.492]
</p><p>20 This leads to a huge tracking graph, which relies on a specialized optimization technique. [sent-121, score-0.334]
</p><p>21 In contrast, in our work, we present a solution which only requires a single tracking graph. [sent-122, score-0.334]
</p><p>22 Multicamera coupling constraints are incorporated into the reconstruction nodes within the tracking graph. [sent-123, score-0.625]
</p><p>23 Tracking The input to the tracking stage are object detections from each frame and from each camera. [sent-126, score-0.507]
</p><p>24 Figure 2: Example showing the network flow graph corresponding to the hypergraph in Figure 1. [sent-316, score-0.279]
</p><p>25 Furthermore, because of false positive detections as well as missing detections and occlusions, reconstruction becomes challenging. [sent-325, score-0.559]
</p><p>26 eT sheetre Rfor weo, utlhde bseet hofu reconstructions is reduced to reconstructions Rk which have a prior rioencosn isstru recdtiuocne probability tPrurcecti(oRnks) R R> 0 as defined in Section 5. [sent-331, score-0.263]
</p><p>27 333666445 919  The complete association hypothesis T is then defined as  a hseet oofm trajectory hypotheses, tthheuss Ts T T= i s{ tThuen}. [sent-339, score-0.257]
</p><p>28 The detection likelihood takes the probability of each underlying 2D detection being a true detection or a false positive, as well as the likelihood of missed detections into account:  Pdet(Rk|T ) =? [sent-351, score-0.499]
</p><p>29 β(1|R −k| β·) (|R1 k−|· γ γ)nn((RRkk))−−||RRkk||, R elkse∈ T(7) where β and γ are the false positive and false negative rates of the detector and n(Rk) is the number of cameras that oshfo thueld generate a dde nte(cRtion for the 3D reconstruction Rk. [sent-352, score-0.735]
</p><p>30 The quality of the reconstruction is measured by the reconstruction likelihood:  Prec(Rk|T ) =? [sent-353, score-0.326]
</p><p>31 As it is typically done in tracking approaches, the a priori likelihood of a single trajectory hypothesis P(Tu) is given by a iMhoaorkdo ovf c ah asiinng:  P(Tu)= = P Pe(n{(RRuu00,)RPluin1,k. [sent-357, score-0.563]
</p><p>32 Pex(Runu)  (9)  Pen (Rui ) and Pex (Rui ) define the probability of a trajectory tRo start and e(nRd at reconstruction Rui , respectively. [sent-362, score-0.337]
</p><p>33 Mapping to a constrained  Min-Cost  Flow  Graph The final MAP formulation ofEquation (6), which corresponds to a hypergraph like the one in Figure 1, can be efficiently reformulated into a constrained min-cost flow graph (as the one in Figure 2). [sent-368, score-0.29]
</p><p>34 Using this min-cost flow formulation, the tracking problem can be efficiently solved using binary linear programming algorithms. [sent-369, score-0.475]
</p><p>35 The min-cost flow graph is built of the hyperedges Rk (corresponding tto f o3Dw rg ercaponhst irsuc btuiioltnos )f, as w heylpl as dtghee ste Rmporal edges Ek,l = {Rk , Rl }, which connect 3D reconpstorruaclti eondsg e bse tEween= =fra {mRes. [sent-371, score-0.28]
</p><p>36 Each reconstruction hyperedge, each temporal edge as well as each source and sink edge can carry a certain amount of flow f generating a cost per flow unit c. [sent-374, score-0.606]
</p><p>37 Thus, to be precise, the reconstruction hyperedges Rk have flow fk with associated cost Ck. [sent-375, score-0.42]
</p><p>38 Each temporal edge Ek,l has a flow of fk,l with a cost of Ck,l and analogously, teh Ee source and sink edges have flow of fen,k and fex,k, with a cost of Cen,k and Cex,k, respectively. [sent-376, score-0.414]
</p><p>39 Each flow path through the graph corresponds to an object trajectory and the total amount of flow from S to T represents the number of tracked trajectories. [sent-377, score-0.5]
</p><p>40 As we impose the constraint that each trajectory can only belong to one object and vice versa, the flow f through an edge can be either 0 or 1. [sent-378, score-0.321]
</p><p>41 A flow of f = 1implies that the corresponding edge is part of the trajectory, a flow of f = 0 means that the edge (or hyperedge) is not used. [sent-379, score-0.34]
</p><p>42 l  Recall that in the MAP formulation the coupling constraint of Equation (3) was introduced to ensure that every 2D detection xi can only be used for one 3D reconstruction Rk. [sent-384, score-0.419]
</p><p>43 This coupling constraint is translated to the flow graph representation in the following way: For all 3D reconstructions Rk and Rl, which have at least one 2D detection in common (ai. [sent-385, score-0.478]
</p><p>44 The last term models the reconstruction cost, which is negative (thus a bonus) for small reconstruction errors (Prec(Rk) > 0. [sent-413, score-0.383]
</p><p>45 With Equation (12), our tracking and reconstruction problem is completely defined as a binary integer programming problem (BIP). [sent-416, score-0.497]
</p><p>46 Modeling of Probabilities The presented tracking model requires a definition of the reconstruction probabilities Prec, the transition probabilities Plink, as well as the enter and exit probabilities Pen and Pex. [sent-420, score-0.842]
</p><p>47 Reconstruction Probability Prec The prior reconstruction probability Prec(Rk) measures the a priori quality of a 3D reconstruction R(kR based on the tmheutu aa plr iaonrdi aq busaollituyte o positions coofn nthster u2cDti odnet Rections xi ∈ Rk contained within a 3D reconstruction set. [sent-424, score-0.615]
</p><p>48 Ideally, a∈ll R2D detections within a 3D reconstruction set should map to the same position in world coordinates. [sent-425, score-0.327]
</p><p>49 However, the quality of a 3D reconstruction also largely depends on the localization error εdet of the object detector, as well as on the camera calibration error εcal. [sent-427, score-0.381]
</p><p>50 First, the reconstruction error εk is defined as the root mean square deviation from the mean position of the detections within the set Rk :  εk= ε(Rk) =? [sent-428, score-0.32]
</p><p>51 ∈RkΦc(x)  (18)  Here, Φc(x) is the transformation function from image to world coordinates for camera c and χ(Rk) is the reconwstrourcldted c average 3sD f position aof c t ahen coupled 2D detections in Rk. [sent-431, score-0.273]
</p><p>52 RThe prior reconstruction probability Prec(Rk) maps the reconstruction error εk to a probability. [sent-432, score-0.422]
</p><p>53 5 (ε(Rk),0,εmax(Rk))  |eRlske| > 1  (20)  333666555311  For 3D reconstructions which originate from a single 2D detection no reconstruction error exists. [sent-437, score-0.347]
</p><p>54 As can be seen, the reconstruction probability decreases as the 3D reconstruction error increases. [sent-440, score-0.422]
</p><p>55 As stated above, the reconstruction probability also largely depends on the detector inaccuracies (error in the 2D image plane) as well as the error of the camera calibration (error in the 3D world coordinate space). [sent-443, score-0.537]
</p><p>56 Because of the perspective distortion of the camera calibration, the 2D detection error εdet has different influence on the world coordinates depending on the object location and on the camera position. [sent-452, score-0.313]
</p><p>57 (23)  In Equation (21), the constant detection error εdet is weighted by the sum of the absolute sensitivity functions Θc(x) of all cameras that contribute to Rk. [sent-460, score-0.258]
</p><p>58 Entrance and Exit Probabilities Pen and Pex The entrance and exit probabilities of an observation define the probability of a trajectory to start and end at this observation, respectively. [sent-463, score-0.394]
</p><p>59 Pen and Pex are modeled to account for the following considerations: (1) Observations which are close to the boundary of the tracking area are likely to enter or exit the scene. [sent-464, score-0.475]
</p><p>60 (3) All observations in the first frame have a high entrance probability and all observations in the last frame have a high exit probability. [sent-466, score-0.401]
</p><p>61 0γn(Δτ−1)  ,e1lse ≤ Δτ ≤ Δτmax  (26)  where Δτmax is the maximal allowed frame difference between two observations, γ is the false negative rate of the detector and n is the (average) number of cameras that should have seen the object in the frame gap. [sent-478, score-0.539]
</p><p>62 Tracking Post-Processing After the BIP optimization, trajectories are defined by tracking back along the active edges (f = 1). [sent-481, score-0.395]
</p><p>63 Evaluation We evaluate our multi-camera multi-person tracking system on the publicly available PETS 2009 dataset [8]. [sent-491, score-0.334]
</p><p>64 PETS 2009 is a very challenging dataset for multiple person tracking as there are many inter-object occlusions, especially in S2. [sent-496, score-0.334]
</p><p>65 The frame rate of the videos is only 7 frames per second, so persons can move quite far between two consecutive frames making precise tracking even more challenging. [sent-500, score-0.44]
</p><p>66 While many previously presented approaches are merely evaluated on the low and medium density scenarios, we are also able to show results for the high density scenes. [sent-501, score-0.287]
</p><p>67 Assignment of tracking output to ground truth is done using the Hungarian algorithm with an assignment cut-off at 1meter. [sent-508, score-0.334]
</p><p>68 We found MOTA, FM and IDS to be the most meaningful of the listed measures as they best measure the quality of a stable tracking of identities over a whole scenario. [sent-510, score-0.334]
</p><p>69 Experimental Settings The tracking algorithm can in principle be used with arbitrarily many views. [sent-514, score-0.334]
</p><p>70 We show results for using one, two and three cameras for each of the three tracking scenarios. [sent-515, score-0.464]
</p><p>71 Default settings The maximal walking speed of a person  is limited to vmax = 5 m/s, such that tracking a running person is possible. [sent-518, score-0.377]
</p><p>72 The false positive rate β, and the false negative rate γ are estimated by evaluating the detector output against the available ground truth for each scenario. [sent-528, score-0.51]
</p><p>73 Using this evaluation, we found for all scenarios an almost constant false positive rate of β = 0. [sent-529, score-0.281]
</p><p>74 These false negative rates are reasonable since in more crowded scenes, the used detector will by far not be able to find enough detections due to mutual occlusions. [sent-539, score-0.436]
</p><p>75 Influence of parameters Most of the used parameters (such as maximum walking speed, distance to tracking boundary, etc. [sent-542, score-0.334]
</p><p>76 ) can be set by intuition and have little impact on tracking performance. [sent-543, score-0.334]
</p><p>77 The parameters which show the most significant influence are the false positive rate Pfp = β and the false negative rate Pfn = γ, which highly depend on the detector quality. [sent-544, score-0.51]
</p><p>78 The tracking results for low, medium 333666555533  SequenceMethodCamera IDsMOTA [%]MOTP [%]MT [%]PT [%]ML [%]FMIDS  PPEETSTS2. [sent-548, score-0.441]
</p><p>79 L3 (high density)  Figure  4: Sensitivity  of MOTA to the assumed false positive rate Pfp  assumed false negative rate Pfn  =  = β and the  γ for low, medium and high density scenes. [sent-579, score-0.646]
</p><p>80 For the actual (true) false positive and false negative rates ofthe detector (obtained using ground truth), best results are in fact achieved. [sent-580, score-0.442]
</p><p>81 and high density scenarios with these “assumed” false positive and false negative rates are shown in Figure 4. [sent-587, score-0.541]
</p><p>82 Thus, ifthe false positive and false negative rates can be estimated correctly on a given dataset, the tracker can get the most out of the erroneous detections. [sent-591, score-0.421]
</p><p>83 It can be seen that, with the presented multi-camera tracker, the low density tracking scene S2. [sent-601, score-0.424]
</p><p>84 0% in the three camera version indicates a very reliable tracking precision. [sent-607, score-0.411]
</p><p>85 Using more cameras leads to slightly higher tracking precision (MOTP) since localization information from multiple sources can be used. [sent-608, score-0.464]
</p><p>86 Regarding the more challenging scenarios, and using two 333666555644  cameras, we still see very good tracking accuracy with a MOTA of 87. [sent-609, score-0.334]
</p><p>87 L1, using more cameras slightly increases tracking precision (MOTP) due to more available localization data. [sent-617, score-0.464]
</p><p>88 Here, the high false negative rate of the detector stage (about 30%-40%) cannot be fully recovered in the tracking stage. [sent-621, score-0.633]
</p><p>89 Conclusion and Outlook In this paper we have contributed to the field of tracking by global data association. [sent-635, score-0.334]
</p><p>90 We extend the well established global tracking method to multi-camera setups, which so far has only been attempted in a few works. [sent-636, score-0.334]
</p><p>91 In contrast to these works, our formulation only requires a single tracking graph and no prior knowledge about the number of tracking targets is needed. [sent-637, score-0.801]
</p><p>92 Currently, in our experimental implementation, only three cameras are used and future work can leverage more cameras for better performance. [sent-641, score-0.293]
</p><p>93 Furthermore, performance gains can be expected when appearance and motion information is incorporated into the tracking formulation. [sent-642, score-0.363]
</p><p>94 Sonar tracking of multiple targets using joint probabilistic data association. [sent-703, score-0.365]
</p><p>95 Globally optimal solution to multi-object tracking with merged measurements. [sent-710, score-0.334]
</p><p>96 Fusion of multi-modal sensors in a voxel occupancy grid for tracking and behaviour analysis. [sent-724, score-0.334]
</p><p>97 Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol. [sent-736, score-0.376]
</p><p>98 Globally-optimal greedy algorithms for tracking a variable number of objects. [sent-756, score-0.334]
</p><p>99 comparison of two multiple hypothesis tracking approaches to interpret 3d object motion from several camera views. [sent-782, score-0.458]
</p><p>100 Global data association for multiobject tracking using network flows. [sent-788, score-0.531]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rk', 0.502), ('tracking', 0.334), ('prec', 0.236), ('reconstruction', 0.163), ('rl', 0.16), ('flow', 0.141), ('cameras', 0.13), ('false', 0.13), ('motp', 0.125), ('plink', 0.121), ('detections', 0.118), ('trajectory', 0.117), ('medium', 0.107), ('reconstructions', 0.103), ('mota', 0.103), ('coupling', 0.099), ('exit', 0.096), ('association', 0.093), ('pets', 0.091), ('density', 0.09), ('pen', 0.089), ('couplings', 0.086), ('pex', 0.086), ('ruj', 0.086), ('tu', 0.084), ('hyperedges', 0.08), ('camera', 0.077), ('multiobject', 0.072), ('hyperedge', 0.072), ('det', 0.071), ('scenarios', 0.07), ('probabilities', 0.068), ('calibration', 0.063), ('multicamera', 0.062), ('detector', 0.061), ('trajectories', 0.061), ('bonus', 0.06), ('graph', 0.059), ('negative', 0.057), ('probability', 0.057), ('sink', 0.056), ('entrance', 0.056), ('rui', 0.055), ('frame', 0.055), ('cplex', 0.051), ('rate', 0.051), ('switches', 0.05), ('argtmaxr', 0.048), ('argtminrk', 0.048), ('bip', 0.048), ('crtion', 0.048), ('pdet', 0.048), ('pfn', 0.048), ('rrk', 0.048), ('rrkk', 0.048), ('runu', 0.048), ('sensitivity', 0.047), ('temporal', 0.047), ('hypergraph', 0.047), ('hypothesis', 0.047), ('world', 0.046), ('enter', 0.045), ('vmax', 0.043), ('formulation', 0.043), ('tracked', 0.042), ('detection', 0.042), ('observations', 0.041), ('setups', 0.041), ('tracker', 0.04), ('pfp', 0.04), ('technische', 0.04), ('error', 0.039), ('equation', 0.039), ('xi', 0.038), ('fragments', 0.038), ('fk', 0.036), ('tum', 0.036), ('andriyenko', 0.036), ('crowded', 0.036), ('likelihood', 0.034), ('constraint', 0.034), ('cal', 0.034), ('logp', 0.034), ('rates', 0.034), ('id', 0.033), ('leverage', 0.033), ('coordinates', 0.032), ('network', 0.032), ('max', 0.031), ('targets', 0.031), ('inaccuracies', 0.031), ('berclaz', 0.031), ('priori', 0.031), ('ck', 0.031), ('emerge', 0.03), ('positive', 0.03), ('universit', 0.03), ('incorporated', 0.029), ('edge', 0.029), ('box', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="209-tfidf-1" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>2 0.23555174 <a title="209-tfidf-2" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>Author: Shoou-I Yu, Yi Yang, Alexander Hauptmann</p><p>Abstract: A device just like Harry Potter’s Marauder’s Map, which pinpoints the location ofeachperson-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust lo- calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.</p><p>3 0.23395626 <a title="209-tfidf-3" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>Author: Asad A. Butt, Robert T. Collins</p><p>Abstract: We propose a method for global multi-target tracking that can incorporate higher-order track smoothness constraints such as constant velocity. Our problem formulation readily lends itself to path estimation in a trellis graph, but unlike previous methods, each node in our network represents a candidate pair of matching observations between consecutive frames. Extra constraints on binary flow variables in the graph result in a problem that can no longer be solved by min-cost network flow. We therefore propose an iterative solution method that relaxes these extra constraints using Lagrangian relaxation, resulting in a series of problems that ARE solvable by min-cost flow, and that progressively improve towards a high-quality solution to our original optimization problem. We present experimental results showing that our method outperforms the standard network-flow formulation as well as other recent algorithms that attempt to incorporate higher-order smoothness constraints.</p><p>4 0.21812084 <a title="209-tfidf-4" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>5 0.21424411 <a title="209-tfidf-5" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>6 0.19379491 <a title="209-tfidf-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.18272386 <a title="209-tfidf-7" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>8 0.17064567 <a title="209-tfidf-8" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>9 0.16449039 <a title="209-tfidf-9" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>10 0.16338937 <a title="209-tfidf-10" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>11 0.16026846 <a title="209-tfidf-11" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>12 0.15509251 <a title="209-tfidf-12" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>13 0.14849906 <a title="209-tfidf-13" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>14 0.13918746 <a title="209-tfidf-14" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>15 0.13582678 <a title="209-tfidf-15" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>16 0.13362944 <a title="209-tfidf-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.11553168 <a title="209-tfidf-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.11520536 <a title="209-tfidf-18" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>19 0.11496641 <a title="209-tfidf-19" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>20 0.11467082 <a title="209-tfidf-20" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, 0.086), (2, -0.002), (3, -0.096), (4, 0.008), (5, -0.076), (6, 0.178), (7, -0.17), (8, 0.091), (9, 0.237), (10, -0.027), (11, 0.002), (12, -0.029), (13, 0.037), (14, 0.017), (15, 0.019), (16, -0.008), (17, 0.045), (18, 0.04), (19, -0.012), (20, 0.086), (21, -0.009), (22, -0.059), (23, 0.096), (24, 0.04), (25, 0.047), (26, 0.048), (27, -0.032), (28, -0.048), (29, -0.033), (30, -0.044), (31, -0.028), (32, 0.033), (33, 0.025), (34, 0.034), (35, 0.055), (36, 0.025), (37, -0.029), (38, -0.026), (39, -0.041), (40, 0.016), (41, -0.054), (42, 0.042), (43, -0.062), (44, 0.021), (45, 0.013), (46, 0.027), (47, -0.078), (48, -0.003), (49, -0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97306705 <a title="209-lsi-1" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>2 0.85753 <a title="209-lsi-2" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>3 0.83175045 <a title="209-lsi-3" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>4 0.83116013 <a title="209-lsi-4" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>Author: Xinchu Shi, Haibin Ling, Junling Xing, Weiming Hu</p><p>Abstract: In this paper we formulate multi-target tracking (MTT) as a rank-1 tensor approximation problem and propose an ?1 norm tensor power iteration solution. In particular, a high order tensor is constructed based on trajectories in the time window, with each tensor element as the affinity of the corresponding trajectory candidate. The local assignment variables are the ?1 normalized vectors, which are used to approximate the rank-1 tensor. Our approach provides a flexible and effective formulation where both pairwise and high-order association energies can be used expediently. We also show the close relation between our formulation and the multi-dimensional assignment (MDA) model. To solve the optimization in the rank-1 tensor approximation, we propose an algorithm that iteratively powers the intermediate solution followed by an ?1 normalization. Aside from effectively capturing high-order motion information, the proposed solver runs efficiently with proved convergence. The experimental validations are conducted on two challenging datasets and our method demonstrates promising performances on both.</p><p>5 0.78084105 <a title="209-lsi-5" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>Author: Jingchen Liu, Peter Carr, Robert T. Collins, Yanxi Liu</p><p>Abstract: We employ hierarchical data association to track players in team sports. Player movements are often complex and highly correlated with both nearby and distant players. A single model would require many degrees of freedom to represent the full motion diversity and could be difficult to use in practice. Instead, we introduce a set of Game Context Features extracted from noisy detections to describe the current state of the match, such as how the players are spatially distributed. Our assumption is that players react to the current situation in only a finite number of ways. As a result, we are able to select an appropriate simplified affinity model for each player and time instant using a random decisionforest based on current track and game contextfeatures. Our context-conditioned motion models implicitly incorporate complex inter-object correlations while remaining tractable. We demonstrate significant performance improvements over existing multi-target tracking algorithms on basketball and field hockey sequences several minutes in duration and containing 10 and 20 players respectively.</p><p>6 0.75012869 <a title="209-lsi-6" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>7 0.7499724 <a title="209-lsi-7" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>8 0.74456191 <a title="209-lsi-8" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>9 0.73558384 <a title="209-lsi-9" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>10 0.71788436 <a title="209-lsi-10" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>11 0.7066896 <a title="209-lsi-11" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>12 0.66531366 <a title="209-lsi-12" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>13 0.65521973 <a title="209-lsi-13" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>14 0.62974453 <a title="209-lsi-14" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>15 0.61345983 <a title="209-lsi-15" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>16 0.59055209 <a title="209-lsi-16" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>17 0.58600843 <a title="209-lsi-17" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>18 0.57442576 <a title="209-lsi-18" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>19 0.57386017 <a title="209-lsi-19" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>20 0.56691432 <a title="209-lsi-20" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.105), (16, 0.018), (26, 0.04), (33, 0.225), (67, 0.047), (69, 0.026), (87, 0.475)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91226828 <a title="209-lda-1" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>2 0.90116167 <a title="209-lda-2" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>Author: Christian Häne, Christopher Zach, Andrea Cohen, Roland Angst, Marc Pollefeys</p><p>Abstract: Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being ’too noisy’. Unfortunately, these priors generally yield overly smooth reconstructions and/or segmentations in certain regions whereas they fail in other areas to constrain the solution sufficiently. In this paper we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other’s task. As a consequence, we propose a rigorous mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. Image segmentations provide geometric cues about which surface orientations are more likely to appear at a certain location in space whereas a dense 3D reconstruction yields a suitable regularization for the segmentation problem by lifting the labeling from 2D images to 3D space. We show how appearance-based cues and 3D surface orientation priors can be learned from training data and subsequently used for class-specific regularization. Experimental results on several real data sets highlight the advantages of our joint formulation.</p><p>3 0.89596516 <a title="209-lda-3" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>Author: Eno Töppe, Claudia Nieuwenhuis, Daniel Cremers</p><p>Abstract: We introduce the concept of relative volume constraints in order to account for insufficient information in the reconstruction of 3D objects from a single image. The key idea is to formulate a variational reconstruction approach with shape priors in form of relative depth profiles or volume ratios relating object parts. Such shape priors can easily be derived either from a user sketch or from the object’s shading profile in the image. They can handle textured or shadowed object regions by propagating information. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware. In contrast to existing single view reconstruction algorithms, the proposed algorithm provides substantially more flexibility to recover shape details such as self-occlusions, dents and holes, which are not visible in the object silhouette.</p><p>same-paper 4 0.87134647 <a title="209-lda-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.85571373 <a title="209-lda-5" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>Author: Yi-Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa, P. Jonathon Phillips</p><p>Abstract: We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification, where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.</p><p>6 0.8473562 <a title="209-lda-6" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>7 0.84395885 <a title="209-lda-7" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>8 0.83687782 <a title="209-lda-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.81071758 <a title="209-lda-9" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>10 0.78054059 <a title="209-lda-10" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>11 0.77198243 <a title="209-lda-11" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>12 0.73282731 <a title="209-lda-12" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>13 0.72115743 <a title="209-lda-13" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>14 0.71056187 <a title="209-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.7008028 <a title="209-lda-15" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>16 0.70067173 <a title="209-lda-16" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>17 0.69368494 <a title="209-lda-17" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>18 0.69344801 <a title="209-lda-18" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>19 0.69318223 <a title="209-lda-19" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>20 0.69312072 <a title="209-lda-20" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
