<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-237" href="#">cvpr2013-237</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</h1>
<br/><p>Source: <a title="cvpr-2013-237-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Vemulapalli_Kernel_Learning_for_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Raviteja Vemulapalli, Jaishanker K. Pillai, Rama Chellappa</p><p>Abstract: In computer vision applications, features often lie on Riemannian manifolds with known geometry. Popular learning algorithms such as discriminant analysis, partial least squares, support vector machines, etc., are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. However, for kernel based approaches, poor choice of kernel often results in reduced performance. In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.</p><p>Reference: <a title="cvpr-2013-237-reference" href="../cvpr2013_reference/cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Pillai and Rama Chellappa Department of Electrical and Computer Engineering Center for Automation Research, UMIACS, University of Maryland, College Park, MD 20742  Abstract In computer vision applications, features often lie on Riemannian manifolds with known geometry. [sent-2, score-0.326]
</p><p>2 Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. [sent-5, score-0.509]
</p><p>3 However, for kernel based approaches, poor choice of kernel often results in reduced performance. [sent-6, score-0.394]
</p><p>4 In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. [sent-7, score-0.597]
</p><p>5 We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. [sent-8, score-0.356]
</p><p>6 Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. [sent-9, score-0.354]
</p><p>7 Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over  existing methods for classification of manifold features. [sent-10, score-0.582]
</p><p>8 For instance, popular features and models in computer vision like shapes [10], histograms, covariance features [22] , linear dynamical systems (LDS) [6], etc. [sent-14, score-0.269]
</p><p>9 In such cases, one needs good classification techniques that make use of the underlying manifold structure. [sent-16, score-0.427]
</p><p>10 Hence, classification is often performed in an extrinsic manner by first mapping the manifold to an Euclidean space, and then learning classifiers in the new space. [sent-19, score-0.587]
</p><p>11 However, tangent spaces preserves only the local structure of the manifold and can often lead to sub-optimal performance. [sent-21, score-0.445]
</p><p>12 An alternative approach is to map the manifold to a reproducing kernel Hilbert space (RKHS) [8, 9, 5] by using kernels. [sent-22, score-0.514]
</p><p>13 Though kernel-based methods have been successfully used in many computer vi-  sion applications, poor choice of kernel can often result in reduced classification performance. [sent-23, score-0.27]
</p><p>14 This gives rise to an important question: How to find good kernels for Riemannian manifolds ? [sent-25, score-0.456]
</p><p>15 In this paper, we answer this question using the kernel learning approach [14, 17], in which appropriate kernels are learned directly from the data. [sent-27, score-0.429]
</p><p>16 Since we are interested in learning good kernels for the purpose of classification, we learn the kernel and the classifier jointly by solving a single optimization problem. [sent-28, score-0.526]
</p><p>17 In order to preserve the manifold structure, we constrain the distances in the mapped space to be close to the manifold distances. [sent-35, score-0.77]
</p><p>18 standard solvers such as SeDuMi [19], it is transductive in nature: both training and test data need to be present while learning the kernel matrix. [sent-38, score-0.26]
</p><p>19 To solve both these issues, we follow the multiple kernel learning (MKL) approach [14, 17] and parametrize the kernel as a linear combination of known base kernels. [sent-40, score-0.549]
</p><p>20 We performed experiments using two different manifold features: linear subspaces and covariance features, and three different applications: face recognition using image  sets, object recognition using image sets and human activity recognition. [sent-42, score-0.903]
</p><p>21 The superior performance of the proposed approach clearly shows that it can be successfully used in classification applications that use manifold features. [sent-43, score-0.39]
</p><p>22 Contributions: 1) We introduce a general framework for developing extrinsic classifiers for features that lie on Riemannian manifolds using the kernel learning approach. [sent-44, score-0.643]
</p><p>23 To the best of our knowledge, the proposed approach is the first one to use kernel learning techniques for classification of features that lie on Riemannian manifolds with known geometry. [sent-45, score-0.597]
</p><p>24 2) We propose to use a geodesic distance-based regularizer for learning appropriate kernels directly from the data. [sent-46, score-0.358]
</p><p>25 Section 4 briefly discusses the Riemannian geometry of two popularly used features, namely linear subspaces and covariance features. [sent-49, score-0.438]
</p><p>26 Previous Work Existing classification methods for Riemannian manifolds (with known geometry) can be broadly grouped into three main categories: nearest-neighbor methods, Bayesian methods, and Euclidean-mapping methods. [sent-52, score-0.298]
</p><p>27 In [27, 13, 12], image sets were modeled using linear subspaces and then compared using the largest canonical correlation in [27], the direct sum of canonical correlations in [13] and a weighted sum canonical correlations in [12]. [sent-56, score-0.264]
</p><p>28 In [21] parametric pdfs like Gaussian were defined on the tangent space and then wrapped back on to the manifold to define intrinsic pdfs for the Grassmann manifold. [sent-58, score-0.542]
</p><p>29 Both these approaches along with Bayes classifier were used for human activity recognition and video-based face recognition. [sent-60, score-0.3]
</p><p>30 , can be extended to manifolds by mapping the manifolds to Euclidean spaces. [sent-63, score-0.546]
</p><p>31 In [22], a LogitBoost classifier was developed using weak classifiers learned on tangent spaces, and then used for pedestrian detection with covariance features. [sent-65, score-0.324]
</p><p>32 Tangent spaces only preserves the local structure of the manifold and can often lead to sub-optimal performance. [sent-66, score-0.389]
</p><p>33 Alternatively, one can map manifolds to Euclidean spaces by defining Mercer kernels on them. [sent-67, score-0.509]
</p><p>34 In [24], a kernel defined for the manifold of symmetric positive definite matrices was used with PLS for image setbased recognition tasks. [sent-69, score-0.69]
</p><p>35 In [5], the Binet-Cauchy kernels defined on non-linear dynamical systems were used for human activity recognition. [sent-70, score-0.402]
</p><p>36 Hence, in this paper we address the issue of kernel-selection for the classification of manifold features. [sent-72, score-0.39]
</p><p>37 The idea of using manifold structure as a regularizer was previously explored in the context of data manifolds [4, 18], where the given high dimensional data samples were simply assumed to lie on a lower dimensional manifold. [sent-73, score-0.719]
</p><p>38 Since the structure of the underlying manifold was unknown, a graph Laplacian-based empirical estimate of the data distribution was used in [4, 18]. [sent-74, score-0.373]
</p><p>39 Contrary to this, in this paper, we are interested in analytical manifolds, like Grassmann manifold and manifold of symmetric positive definite matrices, whose underlying geometry is known. [sent-75, score-0.79]
</p><p>40 Let M denote the Riemannian manifold on which the featLuertes M Mlie d. [sent-94, score-0.336]
</p><p>41 (ii) Structure preservation: Since the features lie on a Riemannian manifold with a well defined structure, the mapping should be structure-preserving. [sent-117, score-0.476]
</p><p>42 SVM classifier in the mapped space: The SVM classifier in the mapped space is given by f(x) = w? [sent-125, score-0.39]
</p><p>43 Preserving the manifold structure: To preserve the manifold structure, we constrain the distances in the mapped space to be close to the manifold distances. [sent-167, score-1.106]
</p><p>44 The squared Euclidean distance between two points xi and xj in the mapped space can be expressed in terms of kernel values as ? [sent-168, score-0.363]
</p><p>45 at in (5) we are learning the entire kernel matrix K directly in a non-parametric finasghi tohen, e anntidr eth kee rcnlealss mifaietrri txer Km dhiraesc only Ktr,tr. [sent-217, score-0.217]
</p><p>46 Once the kernel matrix K is obtained, the SVM classifier in the mapped space can bKe sob otabitnaiende by solving tchlea SsiVfieMr ndu tahle ( m4)a. [sent-224, score-0.373]
</p><p>47 pNedote s tahcaet ctahen above formulation is transductive in nature: both training and test data need to be present while learning the kernel matrix. [sent-225, score-0.26]
</p><p>48 Extrinsic SVM Using MKL Framework Instead of learning a non-parametric kernel matrix K,  following [ o1f4, l 1a7rn],i we parametrize trihce keernrneell as a l Kin-, ear combination of fixed base kernels K1, K2 , . [sent-230, score-0.583]
</p><p>49 for both training and test data, the weights μ can be learned using only the training data, and the kernel values for test data can be computed using the known base kernels and the learned weights. [sent-242, score-0.466]
</p><p>50 Let pimj denote the squared distance between samples xi and xj induced by the base kernel Km, i. [sent-282, score-0.391]
</p><p>51 2, (7) Let Φm be the mapping corresponding to the kernel Km and ? [sent-311, score-0.236]
</p><p>52 ) is also convex and differentiable if all the base kernel matrices Km are strictly positive tdieafbilnei itfe. [sent-351, score-0.398]
</p><p>53 Once the optimal μ∗ is computed, the classifier in the mapped spa? [sent-373, score-0.195]
</p><p>54 Riemannian Manifolds in Computer Vision In this section we briefly discuss the Riemannian geometry of two popularly used features, namely linear subspaces and covariance features, and show how these features are  used in various computer vision applications. [sent-381, score-0.438]
</p><p>55 yT nhe × geodesic dnoisrtamnacle mbae-tween two subspaces S1 and S2 on the Grassmann manifold is given by where = [θ1, . [sent-386, score-0.533]
</p><p>56 O ∈th e[0r, popularly used distances for the Grassmann manifold are the Procrustes metric given by (θi/2))1/2, and the Projection metric given by sin2θi) 1/2. [sent-392, score-0.456]
</p><p>57 rassmann kernels: Grassmann manifold can be mapped to Euclidean spaces by using Mercer kernels [8]. [sent-396, score-0.699]
</p><p>58 One popularly used kernel [8, 9, 24] is the Projection kernel given by  ? [sent-397, score-0.476]
</p><p>59 Various kernels can be generated from KP and ΦP using  KKrPPpbofl(yY(Y1,1,YY2)2) = = e (xγpK? [sent-412, score-0.212]
</p><p>60 , (Y)  We refer to the family of kernels  (12)  as projection-RBF  ×  kernels and the family of kernels polynomial kernels. [sent-417, score-0.76]
</p><p>61 ,d non-singular ciocv aporisainticvee m deaftirnicitees, can ) be m aftorri-mulated as a Riemannian manifold [16], and the resulting affine-invariant geodesic distance (AID) is given by (C1, C2)) 1/2 , where λi (C1, C2) are the gener? [sent-422, score-0.386]
</p><p>62 Kernels for SPD matrices: Similar to the Grassmann manifold, we can define kernels for the set of SPD matrices. [sent-435, score-0.212]
</p><p>63 We refer to the family of kernels Klrobgf as LED-RBF  ,  (13) ker-  nels and the family of kernels kernels. [sent-446, score-0.535]
</p><p>64 Since covariance matrices are positive semi-definite in general, a small ridge may be added to their diagonals to make them positive definite. [sent-461, score-0.262]
</p><p>65 Activity recognition using dynamical models: The autoregressive and moving average (ARMA) model is a dynamical model widely used in computer vision for modeling various kinds of time-series data [6, 3] and has been successfully used for activity recognition [21, 23, 5]. [sent-462, score-0.38]
</p><p>66 Experimental Evaluation In this section, we evaluate the proposed approach using three applications where manifold features are used: (i)  ×  Face recognition using image sets, (ii) Object recognition using image sets and (iii) Human activity recognition from videos. [sent-484, score-0.566]
</p><p>67 We use two different manifold features, namely linear subspaces and covariance features. [sent-485, score-0.654]
</p><p>68 For both of these datasets, we performed experiments with two different manifold features: covariance matrices and linear subspaces. [sent-499, score-0.556]
</p><p>69 3, to avoid matrix singularity, we added a small ridge δI to each covariance matrix C, where δ = 10−3 trace(C) and I the is identity matrix. [sent-501, score-0.213]
</p><p>70 ed as a convex combination of fixed base kernels (K = ? [sent-515, score-0.386]
</p><p>71 iii) Statistical modeling (SM) [21]: This approach uses parametric (SM-P) and non-parametric (SM-NP) probability density estimation on the manifold followed by Bayes  b? [sent-518, score-0.409]
</p><p>72 (iv) Grassmann discriminant analysis (GDA) [8]: Performs discriminant analysis followed by NN classification for the Grassmann manifold using the Projection kernel. [sent-523, score-0.554]
</p><p>73 (v) PLS with the Projection kernel (Proj+PLS) [24]: Uses PLS combined with the Projection kernel for the Grassmann manifold. [sent-524, score-0.356]
</p><p>74 (vi) Covariance discriminative learning (CDL) [24]: Uses discriminant analysis and PLS for covariance features using a kernel derived from the LED metric. [sent-525, score-0.47]
</p><p>75 For the experiments with linear subspaces, we used multiple projection-RBF and projectionpolynomial kernels defined in (12). [sent-537, score-0.212]
</p><p>76 Specifically, for the INRIA IXMAS dataset, we used 6 projection-polynomial kernels and 13 projection-RBF kernels. [sent-539, score-0.212]
</p><p>77 For the YouTube dataset, we used 10 projection-polynomial kernels and 15 projection-RBF kernels. [sent-540, score-0.212]
</p><p>78 For the ETH80 dataset, we used 10 projection-polynomial kernels and 13 projection-RBF kernels. [sent-541, score-0.212]
</p><p>79 Polynomial kernels were generated by taking γ = n1 and varying the degree from 1to 6 for the INRIA IXMAS dataset, and from 1to 10 for the other two datasets. [sent-557, score-0.212]
</p><p>80 For the experiments with covariance features, we used multiple LED-RBF and LED-polynomial kernels defined  in (13), whose parameters were chosen based on their individual crossvalidation performance. [sent-558, score-0.383]
</p><p>81 Specifically, for the YouTube dataset, we used 10 LED-polynomial kernels and 15 LED-RBF kernels. [sent-559, score-0.212]
</p><p>82 For the ETH80 dataset, we used 10 LED-polynomial kernels and 20 LED-RBF kernels. [sent-560, score-0.212]
</p><p>83 For both linear subspaces and covariance features, geodesic distances were used in the distance preserving constraints. [sent-573, score-0.368]
</p><p>84 Results Table 1 shows the recognition rates for human activity recognition using dynamical models. [sent-577, score-0.338]
</p><p>85 Tables 2 and 3 show the recognition rates for image set-based object and face recognition tasks using linear subspaces and covariance features respectively. [sent-578, score-0.531]
</p><p>86 The horizontal axis corresponds to the kernel index Table 1: Recognition rates for human activity recognition on the INRIA IXMAS dataset using dynamical models  IdNXatRMasIAe tS8N0 . [sent-588, score-0.47]
</p><p>87 o08rpoascehd  Table 2: Recognition rates for image set-based face and object recognition tasks using linear subspaces  Table 3: Recognition rates for image set-based face and object recognition tasks using covariance features  dEYaTotuaHsT8eu0tbe94N20. [sent-598, score-0.652]
</p><p>88 In the case of other datasets, the S-MKL approach mostly picked few base kernels (usually RBF kernels with high γ value or polynomial kernels of high degree d), whereas the weights for the proposed approach were distributed over many kernels. [sent-606, score-0.766]
</p><p>89 In the case of SM-NP, the poor performance could be due to the sub-optimal choice of the kernel width used in [21]. [sent-610, score-0.216]
</p><p>90 In general, non-parametric density estimation methods are sensitive to the choice of kernel  width and a sub-optimal choice often results in poor performance [21]. [sent-611, score-0.251]
</p><p>91 The relatively lower performance of the other kernel-based methods suggests that, it is effective to jointly learn the kernel and the classifier directly from the data using the proposed framework. [sent-612, score-0.275]
</p><p>92 Recently, covariance feature combined with PLS has been shown [24] to perform better than various other recent methods for image set-based recognition tasks. [sent-613, score-0.217]
</p><p>93 Our 111777888866  results show that the classification performance can be further improved by combining the covariance feature with the proposed approach. [sent-614, score-0.225]
</p><p>94 Conclusion and Future Work In this paper, we introduced a general framework for developing extrinsic classifiers for features that lie on Riemannian manifolds using the kernel learning approach. [sent-616, score-0.643]
</p><p>95 We proposed two criteria for learning a good kernel-classifier combination for manifold features. [sent-617, score-0.456]
</p><p>96 In the case of SVM classifier, based on the proposed criteria, we showed that the problem of learning a good kernel-classifier combination can be formulated as a convex optimization problem and efficiently solved following the multiple kernel learning approach. [sent-618, score-0.354]
</p><p>97 We performed experiments using two popularly used manifold features and obtained superior performance compared to other relevant approaches. [sent-619, score-0.456]
</p><p>98 In this paper, the manifold structure has been used as a regularizer using simple distance-preserving constraints. [sent-621, score-0.393]
</p><p>99 Another possible direction of future work is to explore more sophisticated regularizers that can make use of the underlying manifold structure. [sent-622, score-0.373]
</p><p>100 Riemannian geometry of Grassmann manifolds with a view on algorithmic computation. [sent-629, score-0.244]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('manifold', 0.336), ('grassmann', 0.322), ('riemannian', 0.262), ('manifolds', 0.244), ('kernels', 0.212), ('ixmas', 0.2), ('kernel', 0.178), ('covariance', 0.171), ('subspaces', 0.147), ('pls', 0.145), ('popularly', 0.12), ('kij', 0.116), ('spd', 0.111), ('extrinsic', 0.1), ('dynamical', 0.098), ('mapped', 0.098), ('classifier', 0.097), ('km', 0.094), ('activity', 0.092), ('inria', 0.091), ('mkl', 0.088), ('lie', 0.082), ('discriminant', 0.082), ('svm', 0.077), ('base', 0.076), ('arma', 0.075), ('kjj', 0.075), ('klog', 0.075), ('sdps', 0.075), ('youtube', 0.07), ('kii', 0.067), ('rntr', 0.067), ('face', 0.065), ('sedumi', 0.062), ('convex', 0.059), ('mapping', 0.058), ('log', 0.058), ('regularizer', 0.057), ('tangent', 0.056), ('rates', 0.056), ('pdfs', 0.056), ('om', 0.054), ('rbf', 0.054), ('classification', 0.054), ('polynomial', 0.054), ('stiefel', 0.053), ('spaces', 0.053), ('euclidean', 0.052), ('lds', 0.051), ('bke', 0.05), ('gda', 0.05), ('kimi', 0.05), ('kji', 0.05), ('kjmj', 0.05), ('pimj', 0.05), ('geodesic', 0.05), ('matrices', 0.049), ('ite', 0.048), ('veeraraghavan', 0.048), ('proof', 0.047), ('semidefinite', 0.047), ('xj', 0.047), ('recognition', 0.046), ('theorem', 0.046), ('ij', 0.046), ('bea', 0.045), ('cdl', 0.045), ('action', 0.044), ('transductive', 0.043), ('symmetric', 0.043), ('ridge', 0.042), ('criteria', 0.042), ('kthe', 0.041), ('nels', 0.041), ('proj', 0.041), ('fillard', 0.041), ('chiuso', 0.041), ('ke', 0.041), ('subspace', 0.04), ('xi', 0.04), ('canonical', 0.039), ('learning', 0.039), ('combination', 0.039), ('parametrize', 0.039), ('parametric', 0.038), ('definite', 0.038), ('poor', 0.038), ('niyogi', 0.037), ('pennec', 0.037), ('observability', 0.037), ('underlying', 0.037), ('mm', 0.036), ('differentiable', 0.036), ('risk', 0.036), ('density', 0.035), ('family', 0.035), ('turaga', 0.034), ('ntr', 0.034), ('projection', 0.034), ('dual', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="237-tfidf-1" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>Author: Raviteja Vemulapalli, Jaishanker K. Pillai, Rama Chellappa</p><p>Abstract: In computer vision applications, features often lie on Riemannian manifolds with known geometry. Popular learning algorithms such as discriminant analysis, partial least squares, support vector machines, etc., are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. However, for kernel based approaches, poor choice of kernel often results in reduced performance. In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.</p><p>2 0.38713229 <a title="237-tfidf-2" href="./cvpr-2013-Kernel_Methods_on_the_Riemannian_Manifold_of_Symmetric_Positive_Definite_Matrices.html">238 cvpr-2013-Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices</a></p>
<p>Author: Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi</p><p>Abstract: Symmetric Positive Definite (SPD) matrices have become popular to encode image information. Accounting for the geometry of the Riemannian manifold of SPD matrices has proven key to the success of many algorithms. However, most existing methods only approximate the true shape of the manifold locally by its tangent plane. In this paper, inspired by kernel methods, we propose to map SPD matrices to a high dimensional Hilbert space where Euclidean geometry applies. To encode the geometry of the manifold in the mapping, we introduce a family of provably positive definite kernels on the Riemannian manifold of SPD matrices. These kernels are derived from the Gaussian kernel, but exploit different metrics on the manifold. This lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of SPD matrices. We demonstrate the benefits of our approach on the problems of pedestrian detection, object categorization, texture analysis, 2D motion segmentation and Diffusion Tensor Imaging (DTI) segmentation.</p><p>3 0.37722385 <a title="237-tfidf-3" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>Author: Rui Caseiro, Pedro Martins, João F. Henriques, Fátima Silva Leite, Jorge Batista</p><p>Abstract: In the past few years there has been a growing interest on geometric frameworks to learn supervised classification models on Riemannian manifolds [31, 27]. A popular framework, valid over any Riemannian manifold, was proposed in [31] for binary classification. Once moving from binary to multi-class classification thisparadigm is not valid anymore, due to the spread of multiple positive classes on the manifold [27]. It is then natural to ask whether the multi-class paradigm could be extended to operate on a large class of Riemannian manifolds. We propose a mathematically well-founded classification paradigm that allows to extend the work in [31] to multi-class models, taking into account the structure of the space. The idea is to project all the data from the manifold onto an affine tangent space at a particular point. To mitigate the distortion induced by local diffeomorphisms, we introduce for the first time in the computer vision community a well-founded mathematical concept, so-called Rolling map [21, 16]. The novelty in this alternate school of thought is that the manifold will be firstly rolled (without slipping or twisting) as a rigid body, then the given data is unwrapped onto the affine tangent space, where the classification is performed.</p><p>4 0.31298789 <a title="237-tfidf-4" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>Author: Shaokang Chen, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell</p><p>Abstract: Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once, with fixed clusters used for matching with other image sets. However, this may result in the two closest clusters to represent different characteristics of an object, due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem, we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace, we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error, we approach the nearest point on a Grassmann manifold. Experiments on Honda, ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques, such as Affine Hull based Image Set Distance (AHISD), Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).</p><p>5 0.22052228 <a title="237-tfidf-5" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied, we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers, we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components, across other videos in the system, to perform subsequent video analysis. We demonstrate the efficacy of our approach for activity classification and clustering by reporting competitive results on standard datasets such as, HMDB, UCF-50, Olympic Sports and KTH.</p><p>6 0.18351945 <a title="237-tfidf-6" href="./cvpr-2013-MKPLS%3A_Manifold_Kernel_Partial_Least_Squares_for_Lipreading_and_Speaker_Identification.html">276 cvpr-2013-MKPLS: Manifold Kernel Partial Least Squares for Lipreading and Speaker Identification</a></p>
<p>7 0.18094279 <a title="237-tfidf-7" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>8 0.18005885 <a title="237-tfidf-8" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>9 0.17710516 <a title="237-tfidf-9" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>10 0.17640871 <a title="237-tfidf-10" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>11 0.16922268 <a title="237-tfidf-11" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>12 0.14370702 <a title="237-tfidf-12" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>13 0.12340665 <a title="237-tfidf-13" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>14 0.12170223 <a title="237-tfidf-14" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>15 0.11244728 <a title="237-tfidf-15" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>16 0.11096758 <a title="237-tfidf-16" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>17 0.11087102 <a title="237-tfidf-17" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>18 0.1038595 <a title="237-tfidf-18" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>19 0.10239583 <a title="237-tfidf-19" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>20 0.10149407 <a title="237-tfidf-20" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.224), (1, -0.024), (2, -0.103), (3, 0.03), (4, -0.035), (5, 0.04), (6, -0.088), (7, -0.212), (8, -0.064), (9, -0.113), (10, 0.062), (11, -0.072), (12, -0.185), (13, -0.199), (14, -0.138), (15, 0.026), (16, -0.23), (17, 0.02), (18, -0.239), (19, -0.009), (20, 0.044), (21, 0.109), (22, 0.134), (23, 0.086), (24, -0.098), (25, 0.027), (26, -0.042), (27, 0.161), (28, -0.052), (29, 0.062), (30, -0.063), (31, -0.113), (32, 0.029), (33, 0.111), (34, -0.015), (35, -0.128), (36, 0.123), (37, 0.013), (38, 0.066), (39, 0.093), (40, 0.049), (41, -0.011), (42, -0.024), (43, -0.065), (44, 0.001), (45, 0.082), (46, 0.058), (47, -0.092), (48, -0.008), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94650847 <a title="237-lsi-1" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>Author: Rui Caseiro, Pedro Martins, João F. Henriques, Fátima Silva Leite, Jorge Batista</p><p>Abstract: In the past few years there has been a growing interest on geometric frameworks to learn supervised classification models on Riemannian manifolds [31, 27]. A popular framework, valid over any Riemannian manifold, was proposed in [31] for binary classification. Once moving from binary to multi-class classification thisparadigm is not valid anymore, due to the spread of multiple positive classes on the manifold [27]. It is then natural to ask whether the multi-class paradigm could be extended to operate on a large class of Riemannian manifolds. We propose a mathematically well-founded classification paradigm that allows to extend the work in [31] to multi-class models, taking into account the structure of the space. The idea is to project all the data from the manifold onto an affine tangent space at a particular point. To mitigate the distortion induced by local diffeomorphisms, we introduce for the first time in the computer vision community a well-founded mathematical concept, so-called Rolling map [21, 16]. The novelty in this alternate school of thought is that the manifold will be firstly rolled (without slipping or twisting) as a rigid body, then the given data is unwrapped onto the affine tangent space, where the classification is performed.</p><p>same-paper 2 0.94533694 <a title="237-lsi-2" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>Author: Raviteja Vemulapalli, Jaishanker K. Pillai, Rama Chellappa</p><p>Abstract: In computer vision applications, features often lie on Riemannian manifolds with known geometry. Popular learning algorithms such as discriminant analysis, partial least squares, support vector machines, etc., are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. However, for kernel based approaches, poor choice of kernel often results in reduced performance. In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.</p><p>3 0.92807746 <a title="237-lsi-3" href="./cvpr-2013-Kernel_Methods_on_the_Riemannian_Manifold_of_Symmetric_Positive_Definite_Matrices.html">238 cvpr-2013-Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices</a></p>
<p>Author: Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi</p><p>Abstract: Symmetric Positive Definite (SPD) matrices have become popular to encode image information. Accounting for the geometry of the Riemannian manifold of SPD matrices has proven key to the success of many algorithms. However, most existing methods only approximate the true shape of the manifold locally by its tangent plane. In this paper, inspired by kernel methods, we propose to map SPD matrices to a high dimensional Hilbert space where Euclidean geometry applies. To encode the geometry of the manifold in the mapping, we introduce a family of provably positive definite kernels on the Riemannian manifold of SPD matrices. These kernels are derived from the Gaussian kernel, but exploit different metrics on the manifold. This lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of SPD matrices. We demonstrate the benefits of our approach on the problems of pedestrian detection, object categorization, texture analysis, 2D motion segmentation and Diffusion Tensor Imaging (DTI) segmentation.</p><p>4 0.86428922 <a title="237-lsi-4" href="./cvpr-2013-MKPLS%3A_Manifold_Kernel_Partial_Least_Squares_for_Lipreading_and_Speaker_Identification.html">276 cvpr-2013-MKPLS: Manifold Kernel Partial Least Squares for Lipreading and Speaker Identification</a></p>
<p>Author: Amr Bakry, Ahmed Elgammal</p><p>Abstract: Visual speech recognition is a challenging problem, due to confusion between visual speech features. The speaker identification problem is usually coupled with speech recognition. Moreover, speaker identification is important to several applications, such as automatic access control, biometrics, authentication, and personal privacy issues. In this paper, we propose a novel approach for lipreading and speaker identification. Wepropose a new approachfor manifold parameterization in a low-dimensional latent space, where each manifold is represented as a point in that space. We initially parameterize each instance manifold using a nonlinear mapping from a unified manifold representation. We then factorize the parameter space using Kernel Partial Least Squares (KPLS) to achieve a low-dimension manifold latent space. We use two-way projections to achieve two manifold latent spaces, one for the speech content and one for the speaker. We apply our approach on two public databases: AVLetters and OuluVS. We show the results for three different settings of lipreading: speaker independent, speaker dependent, and speaker semi-dependent. Our approach outperforms for the speaker semi-dependent setting by at least 15% of the baseline, and competes in the other two settings.</p><p>5 0.8116737 <a title="237-lsi-5" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>Author: Nikolaos Pitelis, Chris Russell, Lourdes Agapito</p><p>Abstract: In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image.</p><p>6 0.63513935 <a title="237-lsi-6" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>7 0.63507521 <a title="237-lsi-7" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>8 0.60025692 <a title="237-lsi-8" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>9 0.57540888 <a title="237-lsi-9" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>10 0.49560398 <a title="237-lsi-10" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>11 0.4870061 <a title="237-lsi-11" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>12 0.47567457 <a title="237-lsi-12" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>13 0.45842165 <a title="237-lsi-13" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>14 0.45142362 <a title="237-lsi-14" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>15 0.44577697 <a title="237-lsi-15" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>16 0.44343737 <a title="237-lsi-16" href="./cvpr-2013-Graph-Laplacian_PCA%3A_Closed-Form_Solution_and_Robustness.html">191 cvpr-2013-Graph-Laplacian PCA: Closed-Form Solution and Robustness</a></p>
<p>17 0.43984696 <a title="237-lsi-17" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>18 0.42478973 <a title="237-lsi-18" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>19 0.4145745 <a title="237-lsi-19" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>20 0.40466067 <a title="237-lsi-20" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.018), (3, 0.013), (10, 0.145), (16, 0.018), (26, 0.063), (33, 0.263), (52, 0.198), (67, 0.064), (69, 0.047), (76, 0.015), (87, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89130348 <a title="237-lda-1" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>2 0.89116973 <a title="237-lda-2" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>3 0.87419444 <a title="237-lda-3" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan</p><p>Abstract: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the andnodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC2007and UIUCPeople, and it achieves state-of-the-arts performance.</p><p>4 0.8572095 <a title="237-lda-4" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>same-paper 5 0.84876341 <a title="237-lda-5" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>Author: Raviteja Vemulapalli, Jaishanker K. Pillai, Rama Chellappa</p><p>Abstract: In computer vision applications, features often lie on Riemannian manifolds with known geometry. Popular learning algorithms such as discriminant analysis, partial least squares, support vector machines, etc., are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. However, for kernel based approaches, poor choice of kernel often results in reduced performance. In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.</p><p>6 0.84335494 <a title="237-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.84125942 <a title="237-lda-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.83831131 <a title="237-lda-8" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>9 0.83712173 <a title="237-lda-9" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>10 0.83670831 <a title="237-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.83582741 <a title="237-lda-11" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>12 0.83470666 <a title="237-lda-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.83439744 <a title="237-lda-13" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>14 0.83432639 <a title="237-lda-14" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>15 0.83242261 <a title="237-lda-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.83224088 <a title="237-lda-16" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>17 0.83197582 <a title="237-lda-17" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>18 0.83166224 <a title="237-lda-18" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>19 0.83125389 <a title="237-lda-19" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>20 0.83094114 <a title="237-lda-20" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
