<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-395" href="#">cvpr2013-395</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</h1>
<br/><p>Source: <a title="cvpr-2013-395-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Tabb_Shape_from_Silhouette_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>Reference: <a title="cvpr-2013-395-reference" href="../cvpr2013_reference/cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Shape from Silhouette Probability Maps: reconstruction of thin objects in the presence of silhouette extraction and calibration error Amy Tabb USDA-ARS-AFRS and Purdue University Kearneysville, West Virginia and West Lafayette, Indiana  amy . [sent-1, score-0.841]
</p><p>2 Abstract This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. [sent-3, score-0.721]
</p><p>3 Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. [sent-4, score-0.606]
</p><p>4 We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. [sent-5, score-0.63]
</p><p>5 Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. [sent-7, score-0.214]
</p><p>6 The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54. [sent-8, score-0.422]
</p><p>7 gov  (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where  intersection-based approaches currently fail. [sent-13, score-0.221]
</p><p>8 Introduction The reconstruction of thin, texture-less objects such as leafless trees is a necessary step for agricultural applications such as robotic pruning. [sent-15, score-0.233]
</p><p>9 ] The goal of this paper is to reconstruct the shape of leafless trees in the presence of silhouette extraction and camera calibration error. [sent-22, score-0.748]
</p><p>10 The visual hull (VH) is a type of SfS reconstruction generated by intersecting the backprojected silhouette view2A concavity in the 3D sense is an egg-shaped depression in the surface of the object. [sent-30, score-0.575]
</p><p>11 The VH reconstruction can be implemented in fast algorithms for either voxelbased or polyhedral-based representations ([5],[7],[9],[17], [19],[20]), and allows the use of silhouettes when other features are not available or reliable. [sent-32, score-0.263]
</p><p>12 In real-world applications, silhouette or camera calibration error is often present, and the VH approach is illequipped to deal with this error, particularly false negatives (false negatives are pixels that are marked background but represent the target object). [sent-33, score-0.775]
</p><p>13 Reconstructions of thin objects are particularly sensitive to the effects of silhouette errors, whether from silhouette extraction errors, noise, or camera calibration error, because the two-dimensional projections of thin objects may be only a few pixels wide. [sent-34, score-1.236]
</p><p>14 As a result, small errors in silhouette extraction can have large effects on the accuracy of a VH reconstruction. [sent-35, score-0.39]
</p><p>15 In addition, voxel and pixel resolution settings, even when silhouettes are accurate, can produce VH reconstructions that fail to reconstruct many portions of thin objects. [sent-36, score-0.603]
</p><p>16 Given these preliminaries, a statement of our problem is: assuming silhouette and camera calibration error is present, generate representative reconstructions of thin, texture-less objects from silhouette images. [sent-38, score-1.086]
</p><p>17 The set of input silhouettes may take one of two different forms. [sent-40, score-0.161]
</p><p>18 3 The second is Shape from Silhouette Probability Maps (SfSPM); instead of binary silhouettes, silhouette probability maps (SPM) are given as input, where the probalities that the pixels represent the object are continuously-valued. [sent-43, score-0.456]
</p><p>19 Our approach to SfSPM is to penalize false positive and false negative SPM error equally. [sent-46, score-0.385]
</p><p>20 This error function penalizes false positive and false negative error equally, unlike the VH approach. [sent-49, score-0.464]
</p><p>21 4 Consequently, we focus on local minimum  search methods to find representative reconstructions from SPMs, and describe a local minimum search algorithm that uses heuristics developed for SfSPM. [sent-52, score-0.307]
</p><p>22 n Tt over rtehseu existing ithntaetr souecrti soolnu-tbiaosned is approaches, as eitreconstructs the complicated branching structure of trees, even under silhouette extraction and camera calibration error. [sent-55, score-0.644]
</p><p>23 The formulation ofthe SfSPM (and by extension, SfIS) problem as a pseudo-Boolean optimization problem where false negative and false positive error is equally weighted. [sent-57, score-0.385]
</p><p>24 Introduce local minimum search algorithms of pseudoBoolean optimization to the SfSPM problem and show how heuristics developed for SfSPM allow for lower values of the error function to be found. [sent-59, score-0.191]
</p><p>25 A reconstruction method that produces representative reconstructions of thin, texture-less objects in the presence of silhouette extraction and camera calibration error. [sent-61, score-0.759]
</p><p>26 Related work Recent works on SfIS and SfSPM have sought to compensate for the problems of VH approaches by delaying de-  cisions about a voxel’s label until more information about the voxel can be gained. [sent-64, score-0.204]
</p><p>27 They can be divided into three main categories: sensor fusion, probabilistic, and minimization of silhouette inconsistency approaches. [sent-66, score-0.466]
</p><p>28 In the sensor fusion approach, an observation is represented using a sensor model, and then the model information is fused to determine voxel occupancy probabilities. [sent-67, score-0.324]
</p><p>29 Franco and Boyer [8] used a forward sensor model for each pixel in order to jointly infer voxel occupancy probabilities from all pixels. [sent-68, score-0.314]
</p><p>30 Also, in [15], the authors propose a method called the unbiased hull that compensates for silhouette error by classifying voxels that backproject to a minimum  number of silhouettes T? [sent-76, score-1.123]
</p><p>31 is found by minimizing an error function that describes the probability of voxel misclassification. [sent-79, score-0.322]
</p><p>32 Their approach is to minimize an approximation of the silhouette inconsistency error, where the function used depends on whether the SfIS or SfSPM problem is being considered. [sent-81, score-0.401]
</p><p>33 They consider voxels as continuouslyvalued variables in the range of 0 and 1, and seek a minimum by gradient descent. [sent-82, score-0.291]
</p><p>34 In comparison, our approach uses a closed-form, exact silhouette inconsistency error function which is identical for SfIS and SfSPM and considers voxel labels as Boolean during the minimization process, and is not dependent on the setting of parameters or thresholds. [sent-84, score-0.709]
</p><p>35 The pseudo-Boolean error function is formulated over a set of voxels x = {x0, x1, . [sent-87, score-0.283]
</p><p>36 A voxel is empty if istest l oafbe vol xise l1s, xan =d o {cxcupied if its lab}el. [sent-92, score-0.244]
</p><p>37 For  instance, if we had labeled empty voxels 0 and occupied voxels 1, all of the variables (xi) would have been negated ( x¯i). [sent-95, score-0.51]
</p><p>38 Since the labels of occupied and empty are reversed for voxels, we also reverse the usual labeling for pixels. [sent-99, score-0.169]
</p><p>39 Consequently, the value of pi represents the probability that pi is viewing background. [sent-100, score-0.309]
</p><p>40 For example, if pi = 0, then pi is viewing the object with one hundrend precent probability P(pi = object) = 1, and if ri = 0, then ri back-projects to the reconstruction where voxels are labeled occupied. [sent-101, score-0.701]
</p><p>41 5 Given voxel labeling x, the value of reconstruction pixels can be found. [sent-102, score-0.338]
</p><p>42 Let Spi be the set of voxels that are intersected by a viewing ray from pixel pi. [sent-103, score-0.264]
</p><p>43 ∈Spi  and  (1)  In other words, ri = 1, representing background, only if all voxels viewed by pi are empty. [sent-108, score-0.383]
</p><p>44 The Silhouette Inconsistency Error (SIE) function rep-  resents the differences between reconstruction images and silhouette probability maps. [sent-109, score-0.472]
</p><p>45 First we split SIE(I, x) into two parts: the false positive error (FP) and the false negative error (FN). [sent-130, score-0.464]
</p><p>46 If a pixel pi is part of the silhouette (0) and the reconstruction image pixel ri is 1, then pi is a false positive. [sent-131, score-0.908]
</p><p>47 Then the false positive and false negative error is 111666333  FP(I,x) =  ? [sent-133, score-0.385]
</p><p>48 I,pi  =1  SIE(I, x) =FP(I, x) + FN(I, x)  (9)  In the VH approach, the false negative error is zero: all nonsilhouette pixels project to empty voxels. [sent-137, score-0.304]
</p><p>49 We can conclude that the VH approach minimizes false negative error, setting it to zero, while ignoring false positive error. [sent-138, score-0.306]
</p><p>50 SIEvh(I, x) = FP(I, x) + M · FN(I, x)  (10)  The global minimum of SIEvh is the VH reconstruction, where false positive and false negative errors are unequally weighted. [sent-140, score-0.393]
</p><p>51 As a result, false negative pixels have a disproportionally large impact on the VH reconstruction as compared to false positive pixels. [sent-141, score-0.397]
</p><p>52 Even after reduction to a quadratic pseudo-Boolean function, graph cut methods such as QPBO [12] are unable to label any voxel for SIE in Eq. [sent-147, score-0.204]
</p><p>53 For these reasons, in the next section we describe an approximation solution to minx SIE(x) that finds a local minimum given an initial voxel labeling. [sent-149, score-0.291]
</p><p>54 For a labeling of voxels x, there is a neighborhood N of other labelings y, where y tish equal tao n x except otohadt Nthe o lfa obetlh eorf one vinogxsel y d,i wffehresr e be ytween the two labelings. [sent-154, score-0.3]
</p><p>55 A particular labeling x is called a local minimum if there are no other labelings y in the neighborhood of x that have a lower value of f than x does. [sent-155, score-0.183]
</p><p>56 Then, x is a local minimum if and only if for each voxel xi of x the following is satisfied:  ∂∂xfi  xi=? [sent-160, score-0.291]
</p><p>57 01 i f ∂ ∂ x f i ( x ) ≥≤ 0  (11)  In order to find a local minimum given an initial labeling x(0) , the labels of individual voxels are changed until Eq 11 is satisfied for all voxels. [sent-161, score-0.334]
</p><p>58 In the VH theory, there are many labelings of voxels that are silhouette consistent. [sent-168, score-0.62]
</p><p>59 In our alteration of the local minima search, we also specify that any local minima xmin where SIE(xmin) = c have the greatest volume labeling out of all labelings y where 111 666444  SIE(y) = c. [sent-170, score-0.279]
</p><p>60 We require that the local minimum labeling xmin has the greatest volume property because of the following situation, as shown in Fig 2a. [sent-172, score-0.249]
</p><p>61 In that figure, Camera 0 has no silhouette error, as silhouette pixels from Camera 0 project to at least one of the three occupied voxels. [sent-173, score-0.809]
</p><p>62 Camera 1 has some false negative error from voxel v. [sent-174, score-0.447]
</p><p>63 If voxel v is removed as in Fig 2b, the false negative error for Camera 1 will decrease by e1, but the false positive error for Camera 0 will increase by e0, e0 > e1. [sent-175, score-0.697]
</p><p>64 Because of this type of deadlock, voxel v will never be removed during algorithm LOCAL-MIN-SEARCH. [sent-176, score-0.233]
</p><p>65 Since the local minimum search is done iteratively, thin protrusions and isolated voxels in the local minimum, like those in Fig. [sent-177, score-0.436]
</p><p>66 However, by altering the local minimum search to require that local minima have the greatest volume property, it is possible to avoid getting trapped in minima with high values of SIE. [sent-179, score-0.224]
</p><p>67 Here, the voxel labeling has the same value of SIE, c, as in Fig. [sent-182, score-0.247]
</p><p>68 Then when we test whether or not to change voxel v’s label to empty, we can see in Fig. [sent-185, score-0.204]
</p><p>69 2d that the value of SIE decreases by e1, since the false negative error is removed for Camera 1and Camera 0 has no error. [sent-186, score-0.272]
</p><p>70 To alter LOCAL-MIN-SEARCH for SfSPM, we simply change the conditions on line 6 so that voxels with ∂∂xfi(x(k)) ≥ 0 and = 1 will have their labels changed. [sent-187, score-0.204]
</p><p>71 Algorithm  LOCAL-MIN-SEARCH-SFSPM will also return different results depending on the order in which voxels are tested (Alg. [sent-196, score-0.204]
</p><p>72 1, line 4 has the voxel indices tested in increasing order). [sent-197, score-0.204]
</p><p>73 We found that randomizing the voxel indices for testing on every iteration (at the while loop on line 3 of Alg. [sent-198, score-0.204]
</p><p>74 The regions of both cameras that represent silhouette regions are denoted with an S, and the viewing rays on the boundary of S are black lines. [sent-224, score-0.448]
</p><p>75 Camera 0’s entire image consists of silhouette pixels, while Camera 1 has two non-silhouette regions, one either side of a central silhouette region. [sent-225, score-0.726]
</p><p>76 Experiments on trees and thin metal object Our tests were conducted on three different leafless trees with different branching characteristics. [sent-229, score-0.462]
</p><p>77 All of these experiments used the same camera configuration, which consisted of 10 low-cost webcameras and 20 industrial cameras, both with image size 640x480 pixels, mounted on one wall and ceiling and pointed toward the area where the trees and metal object would be placed. [sent-232, score-0.312]
</p><p>78 The webcams had a larger field of view than the industrial cameras, and inspecting the camera calibration matrices for both types of cameras showed that the focal length in terms of pixel dimensions for the webcameras was half that of the industrial cameras. [sent-233, score-0.395]
</p><p>79 We acquired silhouettes and silhouette probability maps  through background subtraction (reviews and evaluations are given in [1] and [18]). [sent-235, score-0.668]
</p><p>80 For the silhouette probability maps, we discretized the continuous range [0, 1] into 256 values for ease in displaying the silhouette probability maps as unsigned 8-bit integer images. [sent-238, score-0.837]
</p><p>81 In this manner, we were able to compare the results of our algorithm for the SfIS problem by using the silhouettes and the SfSPM problem by using the silhouette probability maps, when the silhouettes and silhouette probability maps are derived from the same background model. [sent-241, score-1.181]
</p><p>82 Since we used a background subtraction method to extract silhouettes, silhouette error resulted from the following: the target object matched the background color, shadow effects, thin object regions relative to pixel size, and image sensor noise. [sent-242, score-0.708]
</p><p>83 Camera calibration was performed using a custom-made calibration rig with seven different calibration patterns so that all cameras could be calibrated to the same world coordinate frame. [sent-243, score-0.392]
</p><p>84 However, the calibration error was dependent on the camera’s location with respect to the calibration rig and the images used for calibration,  and some cameras did have higher calibration error than others (average error for external parameters ranged from 0. [sent-244, score-0.653]
</p><p>85 2-10 pixels, and the webcameras had greater calibration error than the industrial cameras, as was expected). [sent-245, score-0.278]
</p><p>86 While this error was small, since the objects to be tested were quite thin, the camera calibration error did have a large effect on the quality of a visual hull reconstruction. [sent-246, score-0.498]
</p><p>87 The reconstructions using our approach for both the SfIS and SfSPM problems offer a great improvement over the traditional visual hull approach. [sent-258, score-0.203]
</p><p>88 Our approach reconstructs very complicated branching structure in the presence of small camera calibration error and silhouette extraction error. [sent-259, score-0.761]
</p><p>89 We hypothesize that  6Our results are shown without postprocessing other than to smooth the voxels into a surface. [sent-262, score-0.204]
</p><p>90 the small differences between the SfIS and SfSPM reconstructions are an artifact of the random order for voxel testing (as mentioned §4. [sent-263, score-0.265]
</p><p>91 While SPOT works well for noisy silhouettes of thicker objects such as humans, SPOT does not reconstruct branching structure. [sent-267, score-0.268]
</p><p>92 The unbiased hull approach was able to reconstruct more of the large-scale features of the tree than SPOT, but it introduced artifacts near the boundaries of image viewing regions and misses smallscale features of branches. [sent-268, score-0.254]
</p><p>93 In the absence of occlusion by the visual hull, which is typical in our datasets, the unbiased hull approach classifies voxels that backproject to a minimum number of silhouettes T? [sent-269, score-0.681]
</p><p>94 Larger-scale details exert a greater influence over the value of the cost function than fine details in the unbiased hull approach, so larger branches are reconstructed and smaller branches are not. [sent-273, score-0.235]
</p><p>95 Closing  remarks  We presented an algorithm for reconstructing thin, textureless objects from silhouette probability maps or silhouettes by formulating the difference between input and reconstruction images as a pseudo-Boolean minimization problem. [sent-277, score-0.753]
</p><p>96 We were able to reconstruct the objects with greater fidelity than the tradiional visual hull technique. [sent-278, score-0.193]
</p><p>97 A real time system for robust 3d voxel reconstruction of hu-  man motions. [sent-308, score-0.274]
</p><p>98 Shape from incomplete silhouettes based on the reprojection error. [sent-373, score-0.161]
</p><p>99 (a) SPOT reconstruction  [16]  [17]  [18]  [19]  [20] [21]  t(ibo)n Unbiased hull reconstruc- (c) SfIS using our method (d) SfSPM using our method Figure 6. [sent-390, score-0.212]
</p><p>100 Generating octree models of 3d objects from their silhouettes in a sequence of images. [sent-414, score-0.214]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sie', 0.491), ('silhouette', 0.363), ('sfspm', 0.339), ('sfis', 0.25), ('vh', 0.234), ('voxels', 0.204), ('voxel', 0.204), ('silhouettes', 0.161), ('hull', 0.142), ('pi', 0.12), ('thin', 0.12), ('false', 0.116), ('calibration', 0.104), ('xa', 0.092), ('leafless', 0.089), ('spms', 0.089), ('minimum', 0.087), ('xfi', 0.079), ('error', 0.079), ('metal', 0.079), ('landabaso', 0.071), ('xmin', 0.071), ('reconstruction', 0.07), ('camera', 0.07), ('spi', 0.069), ('spot', 0.062), ('occupied', 0.062), ('reconstructions', 0.061), ('ri', 0.059), ('franco', 0.059), ('branching', 0.056), ('unbiased', 0.055), ('cameras', 0.055), ('webcameras', 0.054), ('labelings', 0.053), ('trees', 0.05), ('greatest', 0.048), ('negative', 0.048), ('pard', 0.047), ('nonsubmodular', 0.044), ('labeling', 0.043), ('industrial', 0.041), ('sensor', 0.04), ('occupancy', 0.04), ('empty', 0.04), ('qpbo', 0.039), ('probability', 0.039), ('inconsistency', 0.038), ('amy', 0.036), ('haro', 0.036), ('sievh', 0.036), ('usda', 0.036), ('maps', 0.033), ('subtraction', 0.032), ('minima', 0.032), ('copper', 0.032), ('voxelbased', 0.032), ('backproject', 0.032), ('fp', 0.031), ('viewing', 0.03), ('fn', 0.03), ('pixel', 0.03), ('spm', 0.029), ('octree', 0.029), ('methodological', 0.029), ('west', 0.029), ('removed', 0.029), ('al', 0.027), ('reconstruct', 0.027), ('extraction', 0.027), ('sfs', 0.026), ('positive', 0.026), ('search', 0.025), ('altered', 0.025), ('rig', 0.025), ('guan', 0.025), ('minimization', 0.025), ('pole', 0.024), ('cheung', 0.024), ('complicated', 0.024), ('eq', 0.024), ('objects', 0.024), ('reverse', 0.024), ('ranged', 0.024), ('inconsistent', 0.023), ('representative', 0.022), ('background', 0.022), ('kolmogorov', 0.021), ('pixels', 0.021), ('royal', 0.02), ('reconstructs', 0.02), ('conference', 0.019), ('branches', 0.019), ('meters', 0.019), ('reconstructing', 0.019), ('textureless', 0.019), ('tests', 0.018), ('acquired', 0.018), ('presence', 0.018), ('mounted', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="395-tfidf-1" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>2 0.11289424 <a title="395-tfidf-2" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>3 0.11007656 <a title="395-tfidf-3" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>Author: Jeremie Papon, Alexey Abramov, Markus Schoeler, Florentin Wörgötter</p><p>Abstract: Unsupervised over-segmentation of an image into regions of perceptually similar pixels, known as superpixels, is a widely used preprocessing step in segmentation algorithms. Superpixel methods reduce the number of regions that must be considered later by more computationally expensive algorithms, with a minimal loss of information. Nevertheless, as some information is inevitably lost, it is vital that superpixels not cross object boundaries, as such errors will propagate through later steps. Existing methods make use of projected color or depth information, but do not consider three dimensional geometric relationships between observed data points which can be used to prevent superpixels from crossing regions of empty space. We propose a novel over-segmentation algorithm which uses voxel relationships to produce over-segmentations which are fully consistent with the spatial geometry of the scene in three dimensional, rather than projective, space. Enforcing the constraint that segmented regions must have spatial connectivity prevents label flow across semantic object boundaries which might otherwise be violated. Additionally, as the algorithm works directly in 3D space, observations from several calibrated RGB+D cameras can be segmented jointly. Experiments on a large data set of human annotated RGB+D images demonstrate a significant reduction in occurrence of clusters crossing object boundaries, while maintaining speeds comparable to state-of-the-art 2D methods.</p><p>4 0.10033117 <a title="395-tfidf-4" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>Author: Sagi Katz, Ayellet Tal</p><p>Abstract: Point sets are the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way.</p><p>5 0.097315088 <a title="395-tfidf-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.090860568 <a title="395-tfidf-6" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>7 0.085573658 <a title="395-tfidf-7" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>8 0.078933381 <a title="395-tfidf-8" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>9 0.076177552 <a title="395-tfidf-9" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>10 0.067561522 <a title="395-tfidf-10" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>11 0.066273309 <a title="395-tfidf-11" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>12 0.06610883 <a title="395-tfidf-12" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>13 0.064477079 <a title="395-tfidf-13" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>14 0.063996382 <a title="395-tfidf-14" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>15 0.059191376 <a title="395-tfidf-15" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>16 0.056149889 <a title="395-tfidf-16" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>17 0.053645838 <a title="395-tfidf-17" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>18 0.053570069 <a title="395-tfidf-18" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<p>19 0.052587688 <a title="395-tfidf-19" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>20 0.050744683 <a title="395-tfidf-20" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.113), (1, 0.074), (2, 0.007), (3, 0.009), (4, 0.028), (5, -0.033), (6, -0.003), (7, 0.008), (8, 0.005), (9, 0.023), (10, -0.01), (11, 0.014), (12, -0.004), (13, -0.012), (14, -0.051), (15, -0.023), (16, 0.014), (17, 0.069), (18, 0.004), (19, 0.021), (20, 0.018), (21, 0.013), (22, -0.077), (23, 0.014), (24, 0.011), (25, 0.022), (26, 0.031), (27, -0.027), (28, 0.01), (29, 0.012), (30, 0.008), (31, 0.045), (32, -0.009), (33, 0.039), (34, 0.014), (35, 0.019), (36, 0.049), (37, -0.034), (38, -0.062), (39, -0.071), (40, 0.02), (41, -0.109), (42, 0.028), (43, -0.003), (44, 0.063), (45, 0.021), (46, 0.027), (47, -0.002), (48, -0.1), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89519143 <a title="395-lsi-1" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>2 0.59794712 <a title="395-lsi-2" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>3 0.57091963 <a title="395-lsi-3" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>4 0.57044029 <a title="395-lsi-4" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>Author: Amit Agrawal, Srikumar Ramalingam</p><p>Abstract: Imaging systems consisting of a camera looking at multiple spherical mirrors (reflection) or multiple refractive spheres (refraction) have been used for wide-angle imaging applications. We describe such setups as multi-axial imaging systems, since a single sphere results in an axial system. Assuming an internally calibrated camera, calibration of such multi-axial systems involves estimating the sphere radii and locations in the camera coordinate system. However, previous calibration approaches require manual intervention or constrained setups. We present a fully automatic approach using a single photo of a 2D calibration grid. The pose of the calibration grid is assumed to be unknown and is also recovered. Our approach can handle unconstrained setups, where the mirrors/refractive balls can be arranged in any fashion, not necessarily on a grid. The axial nature of rays allows us to compute the axis of each sphere separately. We then show that by choosing rays from two or more spheres, the unknown pose of the calibration grid can be obtained linearly and independently of sphere radii and locations. Knowing the pose, we derive analytical solutions for obtaining the sphere radius and location. This leads to an interesting result that 6-DOF pose estimation of a multi-axial camera can be done without the knowledge of full calibration. Simulations and real experiments demonstrate the applicability of our algorithm.</p><p>5 0.56728095 <a title="395-lsi-5" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>Author: Kilho Son, Eduardo B. Almeida, David B. Cooper</p><p>Abstract: Thispaper introduces a novel approachfor reassembling pot sherds found at archaeological excavation sites, for the purpose ofreconstructing claypots that had been made on a wheel. These pots and the sherds into which they have broken are axially symmetric. The reassembly process can be viewed as 3D puzzle solving or generalized cylinder learning from broken fragments. The estimation exploits both local and semi-global geometric structure, thus making it a fundamental problem of geometry estimation from noisy fragments in computer vision and pattern recognition. The data used are densely digitized 3D laser scans of each fragment’s outer surface. The proposed reassembly system is automatic and functions when the pile of available fragments is from one or multiple pots, and even when pieces are missing from any pot. The geometric structure used are curves on the pot along which the surface had broken and the silhouette of a pot with respect to an axis, called axisprofile curve (APC). For reassembling multiple pots with or without missing pieces, our algorithm estimates the APC from each fragment, then reassembles into configurations the ones having distinctive APC. Further growth of configurations is based on adding remaining fragments such that their APC and break curves are consistent with those of a configuration. The method is novel, more robust and handles the largest numbers of fragments to date.</p><p>6 0.55209726 <a title="395-lsi-6" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>7 0.54688114 <a title="395-lsi-7" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>8 0.51879495 <a title="395-lsi-8" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>9 0.51326007 <a title="395-lsi-9" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>10 0.50307626 <a title="395-lsi-10" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>11 0.50286341 <a title="395-lsi-11" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<p>12 0.50265855 <a title="395-lsi-12" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>13 0.49536878 <a title="395-lsi-13" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>14 0.49175093 <a title="395-lsi-14" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>15 0.48670051 <a title="395-lsi-15" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>16 0.47915226 <a title="395-lsi-16" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>17 0.47908732 <a title="395-lsi-17" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>18 0.47597939 <a title="395-lsi-18" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>19 0.47559685 <a title="395-lsi-19" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>20 0.4744747 <a title="395-lsi-20" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.121), (16, 0.034), (26, 0.035), (28, 0.032), (33, 0.214), (67, 0.033), (69, 0.046), (87, 0.107), (94, 0.28)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79773152 <a title="395-lda-1" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>Author: Hu Ding, Branislav Stojkovic, Ronald Berezney, Jinhui Xu</p><p>Abstract: Computing accurate and robust organizational patterns of chromosome territories inside the cell nucleus is critical for understanding several fundamental genomic processes, such as co-regulation of gene activation, gene silencing, X chromosome inactivation, and abnormal chromosome rearrangement in cancer cells. The usage of advanced fluorescence labeling and image processing techniques has enabled researchers to investigate interactions of chromosome territories at large spatial resolution. The resulting high volume of generated data demands for high-throughput and automated image analysis methods. In this paper, we introduce a novel algorithmic tool for investigating association patterns of chromosome territories in a population of cells. Our method takes as input a set of graphs, one for each cell, containing information about spatial interaction of chromosome territories, and yields a single graph that contains essential information for the whole population and stands as its structural representative. We formulate this combinato- rial problem as a semi-definite programming and present novel techniques to efficiently solve it. We validate our approach on both artificial and real biological data; the experimental results suggest that our approach yields a nearoptimal solution, and can handle large-size datasets, which are significant improvements over existing techniques.</p><p>same-paper 2 0.77323037 <a title="395-lda-2" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>3 0.68751484 <a title="395-lda-3" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>4 0.68688995 <a title="395-lda-4" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>5 0.68354046 <a title="395-lda-5" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>6 0.6832853 <a title="395-lda-6" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>7 0.68326551 <a title="395-lda-7" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>8 0.6820541 <a title="395-lda-8" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>9 0.6809715 <a title="395-lda-9" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>10 0.68042886 <a title="395-lda-10" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>11 0.68022382 <a title="395-lda-11" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>12 0.68012446 <a title="395-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.68007517 <a title="395-lda-13" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>14 0.67998707 <a title="395-lda-14" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>15 0.67963892 <a title="395-lda-15" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>16 0.67950243 <a title="395-lda-16" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>17 0.67802429 <a title="395-lda-17" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>18 0.67798108 <a title="395-lda-18" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>19 0.67796856 <a title="395-lda-19" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>20 0.67785674 <a title="395-lda-20" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
