<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 cvpr-2013-An Approach to Pose-Based Action Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-40" href="#">cvpr2013-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 cvpr-2013-An Approach to Pose-Based Action Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-40-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_An_Approach_to_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>Reference: <a title="cvpr-2013-40-reference" href="../cvpr2013_reference/cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An approach to pose-based action recognition Chunyu Wang1, Yizhou Wang1, and Alan L. [sent-1, score-0.488]
</p><p>2 edu le  Abstract We address action recognition in videos by modeling the spatial-temporal structures of human poses. [sent-8, score-0.672]
</p><p>3 Then we group the estimated joints into five body parts (e. [sent-11, score-0.64]
</p><p>4 This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. [sent-14, score-0.452]
</p><p>5 Experimental results first show that our approach is able to localize body joints more accurately than existing methods. [sent-16, score-0.502]
</p><p>6 Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets. [sent-17, score-0.575]
</p><p>7 Recent action recognition systems rely on low-level and mid-level features such as local space-time interest points (e. [sent-22, score-0.488]
</p><p>8 (a)A pose is composed of 14 joints at the bottom layer, which are grouped into five body parts in the layer above; (b)shows two spatial-part-sets which combine frequently co-occurring configurations of body parts in an action class. [sent-29, score-1.728]
</p><p>9 An alternative line of work represent actions by sequences of poses in time (e. [sent-47, score-0.415]
</p><p>10 [4][26]), where poses refer to spatial configurations of body joints. [sent-49, score-0.536]
</p><p>11 However, pose-based action recognition can be very hard because of the difficulty to estimate high quality poses from action videos, except in special cases (e. [sent-52, score-1.18]
</p><p>12 In this paper we present a novel pose-based action recognition approach which is effective on some challenging videos. [sent-55, score-0.488]
</p><p>13 We first extend a state of the art method [27] to estimate human poses from action videos. [sent-56, score-0.823]
</p><p>14 Given a video, we first obtain best-K pose estimations for each frame using the method of [27], then we infer the best poses by incorporating segmentation and temporal constraints for all frames in the video. [sent-57, score-0.655]
</p><p>15 We experimentally show that this extension localizes body joints more accurately. [sent-58, score-0.502]
</p><p>16 To represent human actions, we first group the estimated joints into five body parts (e. [sent-59, score-0.698]
</p><p>17 We then apply data mining techniques in the spatial domain to obtain sets of distinctive co-occurring spatial configurations(poses) of body parts, which we call spatial-part-sets. [sent-63, score-0.458]
</p><p>18 Similarly, in the temporal domain, we obtain sets of distinctive co-occurring pose sequences of body parts, which we call temporal-part-sets (e. [sent-64, score-0.617]
</p><p>19 For test videos, we first detect these partsets from the estimated poses then represent the videos by histograms of the detected part-sets. [sent-68, score-0.363]
</p><p>20 (i) It is interpretable, because we decompose poses into parts, guided by human body anatomy, and represent actions by the temporal movements of these parts. [sent-71, score-0.792]
</p><p>21 bag of lowlevel features), because it helps prevent overfitting when training action classifiers. [sent-77, score-0.454]
</p><p>22 This boosts action recognition performance compared with holistic pose features. [sent-79, score-0.791]
</p><p>23 We demonstrate these advantages by showing that our proposed method outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets. [sent-80, score-0.575]
</p><p>24 Section 3, 4 introduces pose estimation and action representation, respectively. [sent-83, score-0.718]
</p><p>25 Related Work We briefly review the pose-based action recognition methods in literature. [sent-87, score-0.488]
</p><p>26 Xu et al[25] propose to automatically estimate joint locations from videos, and use joint locations coupled with motion features for action recognition. [sent-90, score-0.795]
</p><p>27 Modest joint estimation can degrade the action recognition performance as shown in experiments. [sent-91, score-0.622]
</p><p>28 However, implicit pose representations are difficult to relate to body parts, and so are it is hard to model meaningful body part movements in actions. [sent-97, score-0.796]
</p><p>29 Instead, we use body parts as building blocks as they are more meaningful and compact. [sent-106, score-0.32]
</p><p>30 Secondly, we model spatial pose structures as well as temporal pose evolutions, which are neglected in [21]. [sent-107, score-0.551]
</p><p>31 Pose Estimation in Videos We now extend a state of the art image-based pose estimation method [27] to video sequences. [sent-109, score-0.388]
</p><p>32 Our extension can localize joints more accurately, which is important for achieving good action recognition performance. [sent-110, score-0.738]
</p><p>33 Initial Frame-based Pose Estimation A pose P is represented by 14 joints Ji: head, neck, (left/right)-hand/elbow/shoulder/hip/knee/foot. [sent-116, score-0.473]
</p><p>34 Firstly, the learnt kinematic constraints tend to bias estimations to dominating poses in −  training data, which decreases estimation accuracy for rare poses. [sent-134, score-0.353]
</p><p>35 However, looking at 15-best poses returned by the model for each frame, we observe a high probability that the “correct” pose is among them. [sent-136, score-0.461]
</p><p>36 1  Where φ(Pjii , Ii) is a unary term that measures the likelihood of the pose and ψ(Pjii,Pjii++11,Ii,Ii+1) is a pairwise term that measures the appearance, and location consistency  of the joints in consecutive frames. [sent-169, score-0.473]
</p><p>37 In particular, we group the 14 joints of pose P into five body parts(head, left/right arm, left/right leg) by human anatomy, i. [sent-176, score-0.853]
</p><p>38 (a)we start by estimating poses for videos of the two action classes, i. [sent-215, score-0.783]
</p><p>39 (b)then we cluster poses of each body part in training data and construct a part pose dictionary as described in Section 4. [sent-218, score-0.848]
</p><p>40 (c)we extract temporal-part-sets(1-2) and spatial-part-sets(3) for the two action classes as described in Section 4. [sent-222, score-0.454]
</p><p>41 Action Representation We next extract representative spatial/temporal pose structures from body poses for representing actions. [sent-254, score-0.748]
</p><p>42 For spatial pose structures, we pursue sets of frequently cooccurring spatial configurations of body parts in a single frame, which we call the spatial-part-set, spi = {pj1, . [sent-255, score-0.81]
</p><p>43 For temporal pose structures, we pursue s{epts of frequently co-occurring body part sequences ali = (pj1 , . [sent-259, score-0.756]
</p><p>44 Note that body part sequence ali captures t{hael temporal pose eovtoelu thtaiotn b oofd a single body part (e. [sent-266, score-0.927]
</p><p>45 See Figure 3 for the overall framework of the action representation. [sent-270, score-0.454]
</p><p>46 Body Part A body part pi is composed of  zi  joint locations  pi  =  (xi1, y1i , . [sent-273, score-0.558]
</p><p>47 We learn a dictionary of pose templates Vi = {v1i , vi2 , . [sent-280, score-0.357]
</p><p>48 , }, for each body part by clustering the poses {ofv training da}ta,. [sent-283, score-0.522]
</p><p>49 r gE tahceh toesme-s plate pose represents a certain spatial configuration of body parts(See Figure 3. [sent-285, score-0.475]
</p><p>50 We quantize all body part poses pi  vkii  999991111188666  Figure 4. [sent-287, score-0.588]
</p><p>51 (a)shows estimated poses for videos of turn-left and stop-left actions. [sent-289, score-0.329]
</p><p>52 Each row in(1) is a transaction composed by five indexes of quantized body parts. [sent-292, score-0.536]
</p><p>53 Quantized poses are then represented by the five indexes of the templates in the dictionaries. [sent-304, score-0.429]
</p><p>54 Spatial-part-sets We propose spatial-part-sets to capture spatial configurations of multiple body parts: spi = {pj1, . [sent-307, score-0.34]
</p><p>55 The ideal spatial-part-sets are those which occur frequently in one action class but rarely in other classes (and hence have both representative and discriminative power). [sent-315, score-0.499]
</p><p>56 We obtain sets of spatial-part-sets for each action class using Contrast Mining techniques[6]. [sent-316, score-0.454]
</p><p>57 We now relate the notations in contrast mining to our problem of mining spatial-part-sets. [sent-334, score-0.348]
</p><p>58 Recall that the poses are quantized and represented by the five indexes of pose templates. [sent-335, score-0.634]
</p><p>59 A pose P represented by five pose templates is∪ a tr. [sent-341, score-0.579]
</p><p>60 All poses in the training data constitute the transaction database D(See Figure 4. [sent-345, score-0.391]
</p><p>61 We pursue sets of spatial-part-sets for each pair of action classes y1 and y2. [sent-351, score-0.511]
</p><p>62 By increasing the support rate, we guarantee  if TSD+→D−  the representative power of the spatial-part-sets for the positive action class. [sent-359, score-0.481]
</p><p>63 Temporal-part-sets We propose temporal-part-sets to capture joint pose evolution of multiple body parts. [sent-364, score-0.568]
</p><p>64 We denote pose sequences of body parts as ali = (pj1 , . [sent-365, score-0.62]
</p><p>65 We mine a set of frequently cooccurring pose sequences, which we call temporal-part-sets, 999991111199777  Figure 5. [sent-369, score-0.43]
</p><p>66 In implementation, for each of the five pose sequences (pi1, . [sent-377, score-0.333]
</p><p>67 sub-sequences bofthe video compose a transaction, and the transactions of all videos compose the transaction database. [sent-394, score-0.412]
</p><p>68 We mine a set of co-occurring sub-sequences for each pair of action classes as spatial-part-sets mining. [sent-395, score-0.539]
</p><p>69 Classification of Actions We use the bag-of-words model to leverage spatial-partsets and temporal-part-sets for action recognition. [sent-399, score-0.454]
</p><p>70 In the off-line mode, we pursue a set of part-sets for each pair of action classes. [sent-400, score-0.511]
</p><p>71 For the UCF sport and Keck Gesture datasets, we estimate poses from videos by our proposed approach. [sent-409, score-0.487]
</p><p>72 We report performance for both pose estimation and action recognition. [sent-410, score-0.718]
</p><p>73 pose estimation and use the provided 3D poses (because the video frames are not provided) to recognize actions. [sent-413, score-0.596]
</p><p>74 We also evaluate our approach’s robustness to ambiguous poses by perturbing the joint locations in MSR-Action3D. [sent-414, score-0.42]
</p><p>75 The Keck gesture dataset [11] contains 14 different gesture classes. [sent-419, score-0.324]
</p><p>76 The MSR-Action3D dataset [15] contains 20 actions, with each action performed three times by ten subjects. [sent-422, score-0.454]
</p><p>77 Comparison to two baselines We compare our proposed representation with two baselines: holistic pose features and local body part based features. [sent-425, score-0.587]
</p><p>78 A holistic pose feature is a concatenated vector of 14 joint locations. [sent-426, score-0.396]
</p><p>79 We cluster holistic pose features using the k-means algorithm and obtain a prototype dictionary of size 600. [sent-427, score-0.404]
</p><p>80 For local body part based features, we compute a separate pose dictionary for each body part, extract “bag-ofbody part” features, and concatenate them into a high dimensional vector. [sent-429, score-0.83]
</p><p>81 We set dictionary sizes to (8, 25, 25, 25, 25) for the five body parts by cross validation. [sent-431, score-0.461]
</p><p>82 On the UCF sport dataset, the holistic pose features and the local body part based features get 69. [sent-446, score-0.745]
</p><p>83 Note that go back and come near actions have very similar poses but in reverse temporal order. [sent-460, score-0.472]
</p><p>84 Sadanand’s action bank [18] achieves the highest recognition rate. [sent-471, score-0.534]
</p><p>85 But their  action bank is constructed by manually selecting frames from training data, so it is not appropriate to compare our fully automatic method to their’s. [sent-472, score-0.543]
</p><p>86 Comparison of action recognition using poses estimated by [27] and by our method. [sent-477, score-0.726]
</p><p>87 We also evaluated the pose estimation method in the context of action recognition. [sent-501, score-0.718]
</p><p>88 Table 3 compares the action recognition accuracy using poses obtained by different pose estimation methods. [sent-502, score-0.99]
</p><p>89 The tables shows that using the poses obtained by our method (which are more accurate) does improve the action recognition performance compared to using the poses obtained by[27] 5. [sent-503, score-0.964]
</p><p>90 The performance of holistic pose features drop dramatically as the perturbation gets severe, which is expected since the accuracy of joint locations has large impact on holistic pose features. [sent-509, score-0.748]
</p><p>91 Conclusion  We proposed a novel action representation based on human poses. [sent-512, score-0.512]
</p><p>92 The poses were obtained by extending an existing state-of-the-art pose estimation algorithm. [sent-513, score-0.502]
</p><p>93 −  data mining techniques to mine spatial-temporal pose structures for action representation. [sent-516, score-0.971]
</p><p>94 Recognition of human body motion using phase space constraints. [sent-544, score-0.367]
</p><p>95 Scale invariant action recognition using compound features mined from dense spatio-temporal corners. [sent-584, score-0.538]
</p><p>96 Human action recognition using distribution of oriented rectangular patches. [sent-589, score-0.527]
</p><p>97 Learning a hierarchy of discriminative space-time neighborhood feature for human action recognition. [sent-604, score-0.512]
</p><p>98 Action mach a spatio-temporal maximum average correlation height filter for action recognition. [sent-629, score-0.454]
</p><p>99 Mining actionlet ensemble for action recognition with depth cameras. [sent-657, score-0.53]
</p><p>100 Combining skeletal pose with local motion for human activity recognition. [sent-689, score-0.338]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('action', 0.454), ('keck', 0.298), ('body', 0.252), ('joints', 0.25), ('poses', 0.238), ('pose', 0.223), ('mining', 0.174), ('ucf', 0.165), ('gesture', 0.162), ('sport', 0.158), ('actions', 0.137), ('transaction', 0.111), ('joint', 0.093), ('videos', 0.091), ('mine', 0.085), ('itemset', 0.081), ('holistic', 0.08), ('ji', 0.078), ('arm', 0.074), ('growth', 0.073), ('dictionary', 0.071), ('temporal', 0.07), ('five', 0.07), ('parts', 0.068), ('kf', 0.068), ('pi', 0.066), ('compose', 0.063), ('templates', 0.063), ('pages', 0.061), ('indexes', 0.058), ('human', 0.058), ('motion', 0.057), ('pursue', 0.057), ('interpretable', 0.056), ('iji', 0.054), ('pjni', 0.054), ('ulthoff', 0.054), ('kb', 0.052), ('video', 0.051), ('sadanand', 0.051), ('compound', 0.05), ('locations', 0.049), ('interpretability', 0.048), ('itemsets', 0.048), ('recognizers', 0.048), ('tsd', 0.048), ('estimations', 0.046), ('bank', 0.046), ('configurations', 0.046), ('quantized', 0.045), ('frequently', 0.045), ('cooccurring', 0.045), ('fulfill', 0.045), ('frames', 0.043), ('pj', 0.042), ('actionlet', 0.042), ('spi', 0.042), ('constitute', 0.042), ('confusion', 0.041), ('art', 0.041), ('estimation', 0.041), ('perturbing', 0.04), ('sequences', 0.04), ('rectangular', 0.039), ('ib', 0.039), ('head', 0.037), ('anatomy', 0.037), ('pursued', 0.037), ('movements', 0.037), ('ali', 0.037), ('svms', 0.037), ('frame', 0.035), ('explaining', 0.035), ('structures', 0.035), ('coherence', 0.035), ('histograms', 0.034), ('going', 0.034), ('blank', 0.034), ('bobick', 0.034), ('recognition', 0.034), ('ieee', 0.033), ('neck', 0.033), ('transactions', 0.033), ('part', 0.032), ('item', 0.032), ('evolving', 0.032), ('state', 0.032), ('call', 0.032), ('ep', 0.032), ('jj', 0.031), ('rate', 0.031), ('prototype', 0.03), ('sd', 0.03), ('intersection', 0.029), ('articulated', 0.029), ('distributions', 0.029), ('captures', 0.029), ('learnt', 0.028), ('support', 0.027), ('go', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="40-tfidf-1" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>2 0.46717244 <a title="40-tfidf-2" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>3 0.35060954 <a title="40-tfidf-3" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>4 0.34457445 <a title="40-tfidf-4" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>Author: Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</p><p>Abstract: How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What defines these spatiotemporal patches is their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore, these patches can be used as a discriminative vocabulary for action classification where they demonstrate stateof-the-art performance on UCF50 and Olympics datasets.</p><p>5 0.3224071 <a title="40-tfidf-5" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>Author: Chao-Yeh Chen, Kristen Grauman</p><p>Abstract: We propose an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process. Using unlabeled video containing various human activities, the system first learns how body pose tends to change locally in time. Then, given a small number of labeled static images, it uses that model to extrapolate beyond the given exemplars and generate “synthetic ” training examples—poses that could link the observed images and/or immediately precede or follow them in time. In this way, we expand the training set without requiring additional manually labeled examples. We explore both example-based and manifold-based methods to implement our idea. Applying our approach to recognize actions in both images and video, we show it enhances a state-of-the-art technique when very few labeled training examples are available.</p><p>6 0.28192478 <a title="40-tfidf-6" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>7 0.27951238 <a title="40-tfidf-7" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>8 0.26525012 <a title="40-tfidf-8" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>9 0.26147515 <a title="40-tfidf-9" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>10 0.25726771 <a title="40-tfidf-10" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>11 0.25272188 <a title="40-tfidf-11" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>12 0.24870418 <a title="40-tfidf-12" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>13 0.24431767 <a title="40-tfidf-13" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>14 0.23828387 <a title="40-tfidf-14" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>15 0.22636454 <a title="40-tfidf-15" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>16 0.22283721 <a title="40-tfidf-16" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>17 0.2090296 <a title="40-tfidf-17" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>18 0.19824548 <a title="40-tfidf-18" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>19 0.19492823 <a title="40-tfidf-19" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>20 0.19403787 <a title="40-tfidf-20" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.309), (1, -0.124), (2, -0.055), (3, -0.275), (4, -0.403), (5, -0.051), (6, 0.023), (7, 0.136), (8, -0.02), (9, -0.164), (10, -0.073), (11, 0.182), (12, -0.112), (13, 0.064), (14, -0.072), (15, 0.032), (16, 0.036), (17, -0.1), (18, -0.009), (19, 0.089), (20, 0.009), (21, -0.006), (22, -0.058), (23, 0.036), (24, -0.025), (25, -0.047), (26, -0.008), (27, 0.005), (28, 0.023), (29, -0.009), (30, 0.073), (31, -0.017), (32, -0.035), (33, 0.031), (34, 0.017), (35, -0.007), (36, -0.007), (37, 0.045), (38, 0.011), (39, -0.004), (40, -0.007), (41, -0.064), (42, -0.008), (43, 0.029), (44, -0.009), (45, 0.012), (46, -0.01), (47, 0.012), (48, -0.023), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97134799 <a title="40-lsi-1" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>2 0.91951972 <a title="40-lsi-2" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>3 0.88451606 <a title="40-lsi-3" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>Author: Michalis Raptis, Leonid Sigal</p><p>Abstract: In this paper, we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework, where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and minefor hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive with the state of the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.</p><p>4 0.79283404 <a title="40-lsi-4" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>5 0.78841853 <a title="40-lsi-5" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>Author: Chao-Yeh Chen, Kristen Grauman</p><p>Abstract: We propose an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process. Using unlabeled video containing various human activities, the system first learns how body pose tends to change locally in time. Then, given a small number of labeled static images, it uses that model to extrapolate beyond the given exemplars and generate “synthetic ” training examples—poses that could link the observed images and/or immediately precede or follow them in time. In this way, we expand the training set without requiring additional manually labeled examples. We explore both example-based and manifold-based methods to implement our idea. Applying our approach to recognize actions in both images and video, we show it enhances a state-of-the-art technique when very few labeled training examples are available.</p><p>6 0.78809482 <a title="40-lsi-6" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>7 0.74005795 <a title="40-lsi-7" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>8 0.72396481 <a title="40-lsi-8" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>9 0.67973822 <a title="40-lsi-9" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>10 0.66833866 <a title="40-lsi-10" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>11 0.6473797 <a title="40-lsi-11" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>12 0.64436442 <a title="40-lsi-12" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>13 0.63431805 <a title="40-lsi-13" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>14 0.62780493 <a title="40-lsi-14" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>15 0.62196976 <a title="40-lsi-15" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>16 0.61763895 <a title="40-lsi-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.61613703 <a title="40-lsi-17" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>18 0.61271346 <a title="40-lsi-18" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>19 0.60519642 <a title="40-lsi-19" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>20 0.58611363 <a title="40-lsi-20" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.14), (16, 0.02), (26, 0.073), (28, 0.011), (33, 0.302), (63, 0.186), (67, 0.074), (69, 0.033), (80, 0.021), (87, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94218606 <a title="40-lda-1" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>2 0.94181454 <a title="40-lda-2" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>Author: Kilho Son, Eduardo B. Almeida, David B. Cooper</p><p>Abstract: Thispaper introduces a novel approachfor reassembling pot sherds found at archaeological excavation sites, for the purpose ofreconstructing claypots that had been made on a wheel. These pots and the sherds into which they have broken are axially symmetric. The reassembly process can be viewed as 3D puzzle solving or generalized cylinder learning from broken fragments. The estimation exploits both local and semi-global geometric structure, thus making it a fundamental problem of geometry estimation from noisy fragments in computer vision and pattern recognition. The data used are densely digitized 3D laser scans of each fragment’s outer surface. The proposed reassembly system is automatic and functions when the pile of available fragments is from one or multiple pots, and even when pieces are missing from any pot. The geometric structure used are curves on the pot along which the surface had broken and the silhouette of a pot with respect to an axis, called axisprofile curve (APC). For reassembling multiple pots with or without missing pieces, our algorithm estimates the APC from each fragment, then reassembles into configurations the ones having distinctive APC. Further growth of configurations is based on adding remaining fragments such that their APC and break curves are consistent with those of a configuration. The method is novel, more robust and handles the largest numbers of fragments to date.</p><p>3 0.92971164 <a title="40-lda-3" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>Author: Jonathan Balzer, Stefano Soatto</p><p>Abstract: We describe a method to efficiently generate a model (map) of small-scale objects from video. The map encodes sparse geometry as well as coarse photometry, and could be used to initialize dense reconstruction schemes as well as to support recognition and localization of three-dimensional objects. Self-occlusions and the predominance of outliers present a challenge to existing online Structure From Motion and Simultaneous Localization and Mapping systems. We propose a unified inference criterion that encompasses map building and localization (object detection) relative to the map in a coupled fashion. We establish correspondence in a computationally efficient way without resorting to combinatorial matching or random-sampling techniques. Instead, we use a simpler M-estimator that exploits putative correspondence from tracking after photometric and topological validation. We have collected a new dataset to benchmark model building in the small scale, which we test our algorithm on in comparison to others. Although our system is significantly leaner than previous ones, it compares favorably to the state of the art in terms of accuracy and robustness.</p><p>4 0.92038375 <a title="40-lda-4" href="./cvpr-2013-Nonlinearly_Constrained_MRFs%3A_Exploring_the_Intrinsic_Dimensions_of_Higher-Order_Cliques.html">308 cvpr-2013-Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques</a></p>
<p>Author: Yun Zeng, Chaohui Wang, Stefano Soatto, Shing-Tung Yau</p><p>Abstract: This paper introduces an efficient approach to integrating non-local statistics into the higher-order Markov Random Fields (MRFs) framework. Motivated by the observation that many non-local statistics (e.g., shape priors, color distributions) can usually be represented by a small number of parameters, we reformulate the higher-order MRF model by introducing additional latent variables to represent the intrinsic dimensions of the higher-order cliques. The resulting new model, called NC-MRF, not only provides the flexibility in representing the configurations of higher-order cliques, but also automatically decomposes the energy function into less coupled terms, allowing us to design an efficient algorithmic framework for maximum a posteriori (MAP) inference. Based on this novel modeling/inference framework, we achieve state-of-the-art solutions to the challenging problems of class-specific image segmentation and template-based 3D facial expression tracking, which demonstrate the potential of our approach.</p><p>same-paper 5 0.90677679 <a title="40-lda-5" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>6 0.88336903 <a title="40-lda-6" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>7 0.87839615 <a title="40-lda-7" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>8 0.87702537 <a title="40-lda-8" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>9 0.87608504 <a title="40-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.87594175 <a title="40-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.87576276 <a title="40-lda-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.87567472 <a title="40-lda-12" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>13 0.87362194 <a title="40-lda-13" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>14 0.87337047 <a title="40-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.87205559 <a title="40-lda-15" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>16 0.87201053 <a title="40-lda-16" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>17 0.87179184 <a title="40-lda-17" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>18 0.87108552 <a title="40-lda-18" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>19 0.87051922 <a title="40-lda-19" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>20 0.87023824 <a title="40-lda-20" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
