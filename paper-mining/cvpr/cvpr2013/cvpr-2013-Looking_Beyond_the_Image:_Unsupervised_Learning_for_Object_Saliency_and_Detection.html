<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-273" href="#">cvpr2013-273</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</h1>
<br/><p>Source: <a title="cvpr-2013-273-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Siva_Looking_Beyond_the_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>Reference: <a title="cvpr-2013-273-reference" href="../cvpr2013_reference/cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We propose a principled probabilistic formulation of object saliency as a sampling problem. [sent-5, score-0.849]
</p><p>2 This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. [sent-6, score-0.373]
</p><p>3 We then sample the object saliency map to propose object locations. [sent-7, score-0.863]
</p><p>4 We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. [sent-8, score-0.413]
</p><p>5 Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. [sent-9, score-0.798]
</p><p>6 Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches. [sent-10, score-0.739]
</p><p>7 find 1000 boxes covering every object in the image [3, 25]) annotation (i) can be easily validated with a simple “Yes/No” from a human, and can be directly used to learn an object detector (see section 5. [sent-18, score-0.5]
</p><p>8 Human saliency was first formulated as a predictor of  human fixation in images [16]. [sent-23, score-0.726]
</p><p>9 Recent applications in computer vision have led to an increased interest in object saliency formulations [3, 6, 13, 15, 31] that propose salient bounding boxes in images as potential object locations. [sent-24, score-1.281]
</p><p>10 These boxes can be used to speed up object detection [3, 3 1] or weakly supervised object annotation for training a detector [8, 29]. [sent-25, score-0.848]
</p><p>11 Most existing approaches for object saliency can be characterised as extensions of expert-driven human saliency methods or supervised learning methods. [sent-26, score-1.586]
</p><p>12 Object saliency methods that build on expert-driven human saliency approaches [6, 13, 15] tend to use cognitive psychological knowledge of the human visual system and finds image patches on edges and junctions as salient using local contrast or global unique frequencies. [sent-27, score-1.735]
</p><p>13 Recently, object saliency approaches based on supervised learning have emerged [3, 20, 25]. [sent-28, score-0.865]
</p><p>14 These annotations can then be used to train a saliency model (based on global and local image features) to predict patches of interest in unseen images. [sent-30, score-0.839]
</p><p>15 We propose an unsupervised approach to object saliency (fig. [sent-31, score-0.793]
</p><p>16 To understand why our saliency measure exhibits the same bias towards edges and junctions, consider an image composed of a single, approximately homogeneous object with a smooth boundary, and a homogeneous background. [sent-42, score-0.783]
</p><p>17 Sampling Bounding Boxes It remains an open problem as to how bounding boxes (boxes that propose the location of objects) should be sampled from a per pixel object  saliency map. [sent-48, score-1.108]
</p><p>18 However, the presence of an object in one box can cause neighbouring boxes to appear salient, leading to the selection of boxes which highly overlap each other and only partially overlap the actual object. [sent-50, score-0.495]
</p><p>19 In particular, if a selected box narrowly misses an object it may block the future selection of a box that overlaps this object. [sent-54, score-0.418]
</p><p>20 To avoid these near misses, we propose a novel sampling method which encourages the selection of a box that “explains away” possible bounding boxes in the area blocked by non-maximum suppression. [sent-55, score-0.48]
</p><p>21 Furthermore, our object proposals, based on sampling our object saliency map, correctly locate objects in many images on the first proposal. [sent-58, score-0.963]
</p><p>22 This is an ideal behaviour for using our object proposal as an unsupervised approach to annotating objects of interest in weakly labelled data. [sent-59, score-0.731]
</p><p>23 Prior work  Early works on human saliency were developed from biological models of the human visual system, and estimated fixation points where a human viewer would initially focus. [sent-61, score-0.772]
</p><p>24 Our interest, motivated by applications in object detection, lies in object saliency approaches that can detect salient regions 333222333977  as potential object locations. [sent-63, score-1.132]
</p><p>25 Object saliency methods have made use of global frequency-based features [1, 15], which finds regions characterised by rare frequencies in the Fourier domain as salient. [sent-64, score-0.73]
</p><p>26 MSRA [1]) with a single salient object per image, they do not provide means of proposing multiple object locations in an image, and they do not consider the use of other similar images. [sent-71, score-0.396]
</p><p>27 [12] develops an unsupervised approach that integrates both saliency computation and object location proposal. [sent-73, score-0.85]
</p><p>28 [3] starts by sampling rectangular regions based on the global frequency saliency map of [15] then adds additional cues such as colour contrast and super-pixel straddling. [sent-75, score-0.909]
</p><p>29 In particular we define a patch as salient if it is uncommon not only in the current image, but also in other similar images drawn from a large corpus of unlabelled images. [sent-80, score-0.473]
</p><p>30 In [32] the current image is registered to similar images and the difference between the registered image and the similar images are used as the saliency map. [sent-84, score-0.662]
</p><p>31 Most object location proposal methods [3, 12, 25, 3 1] which report on the challenging PASCAL VOC dataset attempt to achieve a high recall rate given a large number of object location proposals. [sent-87, score-0.51]
</p><p>32 In this paper we are interested in the weakly supervised object annotation task [32], which requires high precision of a few object proposals. [sent-88, score-0.774]
</p><p>33 In weakly supervised object annotation, an algorithm attempts to place a tight bounding box around objects of interest, after taking as input two sets of images: one of images not containing the objects, and the other set of images containing them. [sent-89, score-0.652]
</p><p>34 However, the simplest method to annotate the object of interest in an image is to assume that the most object like region in the image is the object of interest, i. [sent-91, score-0.321]
</p><p>35 [28]1 showed a relatively high accuracy for the weakly supervised annotation task can be achieved by this simple approach. [sent-96, score-0.531]
</p><p>36 In this paper we show that our saliency based object location proposal achieves higher weakly supervised annotation accuracy than other methods that propose object locations, or even those weakly supervised learning methods that make additional use of annotated weak labels. [sent-97, score-1.959]
</p><p>37 Sampling-based Saliency Given an image I a large corpus of unlabelled imand ages D, we wish to find a saliency map SI for image I. [sent-105, score-0.919]
</p><p>38 a Hineerde Dfrom the corpus of unlabelled images D and patches are n n regions around each image pixel. [sent-108, score-0.356]
</p><p>39 the  probability  of  sampl  1 The main method of [28] used the weak annotation but they show results for annotating object location using the most object like instance proposed by [3]. [sent-110, score-0.532]
</p><p>40 Finally, the saliency map is smoothed based on image segmentation (SI). [sent-144, score-0.699]
</p><p>41 of px indicates that the patch x is common in the image corpus, and the saliency of patch x is obtained as: Sx = 1 − px (8) where px, over all patches x in the image I, was normalised to the range [0, 1] . [sent-145, score-1.107]
</p><p>42 To account for scale changes in salient objects, we compute saliency Sx (8) at four different image scales [1, . [sent-146, score-0.832]
</p><p>43 First, as in [13], immediate context information is included by weighting the saliency value of each pixel by their distance from the high salient pixel locations. [sent-151, score-0.832]
</p><p>44 Second, a segmentation based smoothing is applied to the saliency map to recover image boundary information. [sent-152, score-0.699]
</p><p>45 To encode immediate context information, high salient pixel locations F = S¯x > T are found and the saliency  ×  pvaixlueel alotc aaltli pixel Flo =cation iis weighted by their distance to F. [sent-153, score-0.862]
</p><p>46 2, the resulting saliency map Sc is blurred due to the use of overlapping patches and image boundaries (edges between objects and background) are not preserved. [sent-159, score-0.835]
</p><p>47 For each segment region, the average saliency from Sc is obtained and used as the final saliency value for that segment, producing our saliency map SI. [sent-161, score-2.023]
</p><p>48 Sampling from the saliency map without non-maximum suppression (NMS) results in an over sampling of high saliency regions. [sent-169, score-1.525]
</p><p>49 While this allows exact alignment to the true object to be found in the first 3 salient boxes, objects in a lower salient region are missed. [sent-170, score-0.48]
</p><p>50 Sampling with NMS means that the lower saliency region will still be sampled from. [sent-171, score-0.727]
</p><p>51 However, the selection of a box that narrowly misses the object may cause the later rejection of the most salient box containing the object. [sent-172, score-0.588]
</p><p>52 Bounding Box Sampling  Bounding boxes that should contain an object can be selected by sampling from a per-pixel saliency map. [sent-180, score-1.002]
</p><p>53 In the  past several options have been explored [19], such as thresholding the saliency map followed by connected region detection [15] or selecting a bounding box containing 95% of the image saliency [19]. [sent-181, score-1.601]
</p><p>54 For proposing multiple bounding boxes per image the saliency map may be randomly sampled from [3], or sampled from the highest score to the lowest score with non-maximum suppression (NMS) [12]. [sent-183, score-1.128]
</p><p>55 Random sampling based on saliency map density results in over-sampling regions of high saliency. [sent-184, score-0.86]
</p><p>56 However, in this case, low saliency regions containing objects will be missed. [sent-186, score-0.726]
</p><p>57 While non-maximum suppression ensures that even low salient regions are sampled from, it does not allow for the repeated sampling of high salient regions. [sent-187, score-0.575]
</p><p>58 As with NMS, we select the box with the highest saliency score (b0) that is not near the other T locations. [sent-194, score-0.817]
</p><p>59 Unlike standard NMS sampling we do not automatically add b0 (the box with the highest saliency score) to the top T proposed boxes. [sent-195, score-0.898]
</p><p>60 Instead we consider B, the set of all boxes that would be blocked by b0,  including Bit,s tehlef, s aent do fse aellk b ob∗x e∈s Btha, tth we o buolxd tbheat b bloecskt explains the saliency of all bounding ∈ bo Bxe,s t hine bBo. [sent-196, score-0.93]
</p><p>61 region from which the boxes in B are drawn using a saliency weighted average tBhoeW bo SxIeFsT i histogram:  μSIFT=? [sent-198, score-0.841]
</p><p>62 ense SIFT BoW histogram representation of bi and di is the saliency score of box bi. [sent-201, score-0.897]
</p><p>63 Then to maximise the overlap with the salient boxes in B that will mbea suppressed, vb∗e rilsa cph wosiethn as th sael i beonxt bwoixthe sth ien c Blo stheastt hwiisl-l togram to μSIFT. [sent-202, score-0.323]
</p><p>64 2  (11)  The saliency score di for box bi is defined as:  di=|bi1|rp? [sent-205, score-0.897]
</p><p>65 Experiments  All results are reported on the PASCAL VOC 2007 [9] Train and Validation set, the standard dataset used for the weakly supervised annotation task [8, 24, 28, 29]. [sent-214, score-0.531]
</p><p>66 Object Proposals Performance Metric: The precision recall curve (PRC) is used to evaluate the performance of the object location proposals as it captures the behaviour of both precision and recall as the number of proposed boxes increases. [sent-220, score-0.66]
</p><p>67 Alternatively, the recall rate as a function of the number of object 333222444200  location proposals is used by [3]. [sent-221, score-0.319]
</p><p>68 Note that recall rate vs object proposals is good for comparing the recall rate at high number of proposed locations but not for evaluating the precision when only one object location is proposed. [sent-222, score-0.653]
</p><p>69 We are also interested in detecting only one object from each image because this is important for the weakly supervised annotation task (see section 5. [sent-228, score-0.635]
</p><p>70 Let Di,j ∈ {0, 1} be a vector indicating if the jth box proposed by t∈he { saliency algorithm correctly d iefte thcets j an object in the ith image, then: Rone(j)  =  ? [sent-231, score-0.851]
</p><p>71 ,Di,j)  (13)  (14)  where N is the number of images in the dataset, j is the number of boxes proposed per image, Rone and Pone are the recall and precision assuming one object per image. [sent-239, score-0.429]
</p><p>72 Particularly, our first object proposal per image correctly locates an object in 42% of the images nearly 10% higher than our closest competitor (see table 1). [sent-251, score-0.332]
</p><p>73 Best bounding boxes taken from the top 10 proposed object locations by our coherent sampling method (Our), MSR [12], Alexe et al. [sent-260, score-0.502]
</p><p>74 Comparison of different object proposal based on the annotation of weakly labelled data. [sent-288, score-0.682]
</p><p>75 2 that our object proposals are particularly suitable for the task of annotating of weakly labelled data which requires maximal precision at the first proposed object location. [sent-292, score-0.664]
</p><p>76 However, for the initial object proposal there is a 3% boost in precision which is beneficial for the weakly supervised annotation task (see section 5. [sent-297, score-0.806]
</p><p>77 Weakly Supervised Object Annotation In weakly supervised object annotation, a set of images with the object of interest and a set of images without the object of interest is given and the goal is to locate the object of interest in all images that contain it. [sent-301, score-0.823]
</p><p>78 As discussed in section 2, we select the first object location proposal in each image as the annotation of the object of interest. [sent-302, score-0.564]
</p><p>79 We test the weakly supervised annotation accuracy on the 20 classes of PASCAL VOC 2007 (VOC07) as defined in [29] and 6 classes (aeroplane, bicycle, boat, bus, horse, and motorbike) with Left and Right pose separately (VOC07-6x2) as defined in [8]. [sent-303, score-0.531]
</p><p>80 Precision recall curve and recall vs number of object location proposal on the PASCAL 2007 TrainVal dataset. [sent-305, score-0.49]
</p><p>81 Average annotation results for PASCAL datasets using different weakly supervised learning methods. [sent-315, score-0.531]
</p><p>82 Comparison with Other Saliency Methods: We compare our object proposal based annotation (Our) results against other object proposal methods in table 3. [sent-316, score-0.619]
</p><p>83 For all methods, except Alexe MN, we select the first object location proposal per image as the object of interest (annotation for weakly labelled data). [sent-317, score-0.743]
</p><p>84 For Alexe MN, we select the top 100 proposed bounding boxes and from these we select the bounding box with the highest objectness score, with objectness scores taken from [3]. [sent-318, score-0.594]
</p><p>85 Our object proposal has a relative improvement from all other object proposal methods of at least 21% on PASCAL07 and 25% on the PASCAL07-6x2 datasets. [sent-319, score-0.436]
</p><p>86 Note for the iterative refinement we make use of weak annotation, while the initialisation is unsupervised, the final iterative annotation result is weakly supervised. [sent-322, score-0.437]
</p><p>87 Overall our object proposal based annotation obtains high initial annotation accuracy and high iteratively refined annotation accuracy; outperforming almost all existing approaches using a much simpler approach. [sent-324, score-0.767]
</p><p>88 Evaluation of Saliency Maps As in [13] we evaluate the saliency map’s ability to predict foreground pixels by reporting the precision recall curve (PRC) and average precision (AP) as a function of the saliency map threshold. [sent-332, score-1.572]
</p><p>89 We evaluate on the PASCAL dataset as it is a more challenging dataset and the common dataset used for the task of annotating weakly labelled object data. [sent-334, score-0.441]
</p><p>90 For completeness we also report PRCs for the MSRA saliency dataset [1]. [sent-335, score-0.685]
</p><p>91 Figure 6 shows the PRCs of our method and some other existing saliency approaches; some examples can be seen in fig. [sent-336, score-0.662]
</p><p>92 In MSRA, unique colors often indicate salient objects but on VOC, unique color is often indicative of a small patch of sky (see fig. [sent-344, score-0.329]
</p><p>93 In section 2, we defined as salient patches with a low probability being sampled from a set of similar images DI. [sent-347, score-0.336]
</p><p>94 7ili, we analyse tphleed c foronmtrib aut sieotn o fo sfi using itmhea gceusrr Dent image I addition to other similar images when computin  ing the saliency map. [sent-349, score-0.662]
</p><p>95 We plot the precision-recall curve of the saliency map computed using similar images without current image DI \ {I} (across image saliency A), using just tnhte i mcaugrreen Dt image I (a c(wroisthsin im image saliency , W us)-, and both combined DI (our combined A+W). [sent-350, score-2.023]
</p><p>96 Note that although across-image saliency and within-image saliency have similar performance, combining them provides a boost in performance, particularly in the region of high-precision we are most concerned with. [sent-351, score-1.35]
</p><p>97 Conclusion We have presented a novel unsupervised approach to the problems of saliency and bounding box annotation5, and shown how it substantially outperforms all other saliency based approaches to bounding box annotation on real world data. [sent-353, score-1.936]
</p><p>98 A framework for visual saliency detection with applications to image thumbnailing. [sent-492, score-0.662]
</p><p>99 Scene recognition and weakly supervised object localization with deformable part-based model. [sent-515, score-0.43]
</p><p>100 In defence of negative mining for annotating weakly labelled data. [sent-547, score-0.359]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saliency', 0.662), ('weakly', 0.227), ('annotation', 0.183), ('nms', 0.175), ('salient', 0.17), ('boxes', 0.153), ('alexe', 0.144), ('proposal', 0.136), ('unlabelled', 0.121), ('supervised', 0.121), ('box', 0.107), ('sampling', 0.105), ('patches', 0.104), ('stuff', 0.1), ('corpus', 0.099), ('di', 0.092), ('proposals', 0.084), ('patch', 0.083), ('bounding', 0.083), ('narrowly', 0.082), ('object', 0.082), ('pascal', 0.079), ('annotating', 0.078), ('msra', 0.076), ('px', 0.075), ('prc', 0.073), ('recall', 0.073), ('junctions', 0.069), ('vs', 0.069), ('siva', 0.068), ('msr', 0.064), ('voc', 0.061), ('suppression', 0.059), ('things', 0.057), ('location', 0.057), ('precision', 0.057), ('rahtu', 0.054), ('labelled', 0.054), ('colour', 0.051), ('unsupervised', 0.049), ('interest', 0.049), ('coherent', 0.049), ('objectness', 0.048), ('fsift', 0.041), ('jpr', 0.041), ('pone', 0.041), ('prcs', 0.041), ('rone', 0.041), ('fixation', 0.041), ('dj', 0.04), ('misses', 0.04), ('sampled', 0.039), ('bias', 0.039), ('hc', 0.038), ('map', 0.037), ('neighbours', 0.037), ('characterised', 0.036), ('reoccurring', 0.036), ('bi', 0.036), ('rc', 0.035), ('annotated', 0.034), ('per', 0.032), ('regions', 0.032), ('flann', 0.032), ('blocked', 0.032), ('objects', 0.032), ('nonmaximum', 0.03), ('locations', 0.03), ('dy', 0.029), ('trainval', 0.028), ('sc', 0.028), ('pandey', 0.027), ('abnormality', 0.027), ('weak', 0.027), ('mn', 0.027), ('region', 0.026), ('abnormal', 0.025), ('junction', 0.025), ('normalised', 0.025), ('predict', 0.024), ('behaviour', 0.024), ('highest', 0.024), ('density', 0.024), ('selecting', 0.024), ('select', 0.024), ('sx', 0.023), ('hit', 0.023), ('residual', 0.023), ('rate', 0.023), ('sr', 0.023), ('deselaers', 0.023), ('human', 0.023), ('probability', 0.023), ('russell', 0.023), ('completeness', 0.023), ('rectangular', 0.022), ('lies', 0.022), ('unique', 0.022), ('interested', 0.022), ('restrictive', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="273-tfidf-1" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>2 0.59421742 <a title="273-tfidf-2" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>3 0.56633204 <a title="273-tfidf-3" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>4 0.56502622 <a title="273-tfidf-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.55028826 <a title="273-tfidf-5" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>6 0.53020585 <a title="273-tfidf-6" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>7 0.41249815 <a title="273-tfidf-7" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>8 0.29540467 <a title="273-tfidf-8" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>9 0.27659735 <a title="273-tfidf-9" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>10 0.27578643 <a title="273-tfidf-10" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>11 0.2738378 <a title="273-tfidf-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.23448415 <a title="273-tfidf-12" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>13 0.20234877 <a title="273-tfidf-13" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>14 0.17801537 <a title="273-tfidf-14" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>15 0.16448572 <a title="273-tfidf-15" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>16 0.14658551 <a title="273-tfidf-16" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>17 0.13890502 <a title="273-tfidf-17" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>18 0.13537143 <a title="273-tfidf-18" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>19 0.12867685 <a title="273-tfidf-19" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>20 0.12615505 <a title="273-tfidf-20" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.247), (1, -0.263), (2, 0.597), (3, 0.269), (4, -0.106), (5, -0.014), (6, 0.031), (7, -0.022), (8, 0.058), (9, 0.021), (10, -0.056), (11, -0.014), (12, -0.008), (13, -0.033), (14, -0.021), (15, -0.003), (16, 0.033), (17, 0.021), (18, -0.041), (19, 0.007), (20, -0.017), (21, -0.006), (22, 0.072), (23, -0.025), (24, 0.025), (25, -0.004), (26, -0.057), (27, -0.029), (28, 0.004), (29, -0.042), (30, -0.032), (31, -0.023), (32, 0.018), (33, 0.016), (34, -0.002), (35, 0.022), (36, 0.028), (37, -0.06), (38, -0.023), (39, -0.022), (40, 0.024), (41, 0.012), (42, 0.027), (43, 0.001), (44, -0.017), (45, 0.012), (46, 0.001), (47, -0.009), (48, 0.012), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94335163 <a title="273-lsi-1" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>2 0.93903446 <a title="273-lsi-2" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>Author: Long Mai, Yuzhen Niu, Feng Liu</p><p>Abstract: A variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First, different methods perform differently in saliency analysis. Second, the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neighboringpixels. To account for the dependence of aggregation on an individual image, our approach selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods.</p><p>3 0.9360199 <a title="273-lsi-3" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>same-paper 4 0.92641586 <a title="273-lsi-4" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>5 0.90646315 <a title="273-lsi-5" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>6 0.90578908 <a title="273-lsi-6" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>7 0.88692224 <a title="273-lsi-7" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>8 0.80523556 <a title="273-lsi-8" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>9 0.79401344 <a title="273-lsi-9" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>10 0.62128049 <a title="273-lsi-10" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>11 0.61281329 <a title="273-lsi-11" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>12 0.59270811 <a title="273-lsi-12" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>13 0.49286669 <a title="273-lsi-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.48770526 <a title="273-lsi-14" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>15 0.38951299 <a title="273-lsi-15" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>16 0.385019 <a title="273-lsi-16" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>17 0.37790874 <a title="273-lsi-17" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>18 0.35766315 <a title="273-lsi-18" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>19 0.33860776 <a title="273-lsi-19" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>20 0.32861361 <a title="273-lsi-20" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.115), (26, 0.049), (33, 0.247), (65, 0.01), (67, 0.15), (69, 0.065), (77, 0.011), (80, 0.19), (87, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89752054 <a title="273-lda-1" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>2 0.88161689 <a title="273-lda-2" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: In this paper we consider the challenging problem of articulated human pose estimation in still images. We observe that despite high variability of the body articulations, human motions and activities often simultaneously constrain the positions of multiple body parts. Modelling such higher order part dependencies seemingly comes at a cost of more expensive inference, which resulted in their limited use in state-of-the-art methods. In this paper we propose a model that incorporates higher order part dependencies while remaining efficient. We achieve this by defining a conditional model in which all body parts are connected a-priori, but which becomes a tractable tree-structured pictorial structures model once the image observations are available. In order to derive a set of conditioning variables we rely on the poselet-based features that have been shown to be effective for people detection but have so far found limited application for articulated human pose estimation. We demon- strate the effectiveness of our approach on three publicly available pose estimation benchmarks improving or being on-par with state of the art in each case.</p><p>3 0.88000405 <a title="273-lda-3" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>same-paper 4 0.86512506 <a title="273-lda-4" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>5 0.86423528 <a title="273-lda-5" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>Author: Georgia Gkioxari, Pablo Arbeláez, Lubomir Bourdev, Jitendra Malik</p><p>Abstract: We propose a novel approach for human pose estimation in real-world cluttered scenes, and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose, we build on the notion of poselets [4] and train highly discriminative classifiers to differentiate among arm configurations, which we call armlets. We propose a rich representation which, in addition to standardHOGfeatures, integrates the information of strong contours, skin color and contextual cues in a principled manner. Unlike existing methods, we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset, where critical visual phenomena, such as occlusion, truncation, multiple instances and clutter are the norm. Our approach outperforms Yang and Ramanan [26], the state-of-the-art technique, with an improvement from 29.0% to 37.5% PCP accuracy on the arm keypoint prediction task, on this new pose estimation dataset.</p><p>6 0.85677439 <a title="273-lda-6" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>7 0.85460889 <a title="273-lda-7" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>8 0.84223658 <a title="273-lda-8" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>9 0.8415553 <a title="273-lda-9" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>10 0.84021902 <a title="273-lda-10" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>11 0.83997327 <a title="273-lda-11" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>12 0.83813614 <a title="273-lda-12" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>13 0.83801258 <a title="273-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.83770877 <a title="273-lda-14" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>15 0.83753246 <a title="273-lda-15" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>16 0.83740765 <a title="273-lda-16" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>17 0.83348835 <a title="273-lda-17" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>18 0.83312029 <a title="273-lda-18" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>19 0.83290607 <a title="273-lda-19" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>20 0.83250684 <a title="273-lda-20" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
