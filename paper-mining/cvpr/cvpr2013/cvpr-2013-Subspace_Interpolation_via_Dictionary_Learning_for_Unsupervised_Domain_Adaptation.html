<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-419" href="#">cvpr2013-419</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</h1>
<br/><p>Source: <a title="cvpr-2013-419-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ni_Subspace_Interpolation_via_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>Reference: <a title="cvpr-2013-419-reference" href="../cvpr2013_reference/cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. [sent-4, score-0.976]
</p><p>2 This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. [sent-5, score-0.683]
</p><p>3 We propose to interpolate subspaces through dictionary learning to link the source and target domains. [sent-6, score-0.797]
</p><p>4 These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. [sent-7, score-0.995]
</p><p>5 Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. [sent-8, score-0.752]
</p><p>6 Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. [sent-16, score-0.486]
</p><p>7 Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art. [sent-19, score-0.432]
</p><p>8 This is often known as the domain adaptation problem which has recently drawn much attention in the computer vision community [28, 14, 13, 17]. [sent-26, score-0.451]
</p><p>9 Domain Adaptation (DA) aims to utilize a source domain with plenty of labeled data to learn a classifier for a target domain which is collected from a different distribution. [sent-27, score-1.257]
</p><p>10 Semi-supervised DA leverages the few labels in the target data or correspon-  dence between the source and target data to reduce the divergence between two domains. [sent-29, score-0.757]
</p><p>11 Given labeled data in the source domain and unlabeled data in the target domain, our DA procedure learns a set of intermediate  {Dk}kK=−11) and the target domain (represented by dictionary DK) {ΔDk }kK=−01 characterize the gradual transition between these subspaces. [sent-35, score-2.078]
</p><p>12 domains (represented by dictionaries shift between two domains. [sent-36, score-0.446]
</p><p>13 to capture the intrinsic domain  As it is very costly to collect labels for target data under various acquisition conditions ‘in the wild’, it is more desirable that the recognition system be able to adapt in an unsupervised fashion. [sent-37, score-0.832]
</p><p>14 In this paper, we use subspace representations to model the source and target domains. [sent-39, score-0.571]
</p><p>15 In this work, we use a dictionary to represent one domain, as dictionary learning based methods [1, 24] have recently become very popular for subspace modeling. [sent-42, score-0.497]
</p><p>16 Specifically, the presence of domain shifts violates the assumption that test data lie in the linear span of training data. [sent-48, score-0.394]
</p><p>17 As the dictionary atoms learned from one domain are not optimal to fit a different domain, and only a small subset of the atoms are allowed for representation, it will incur large reconstruction errors for the target data. [sent-49, score-1.137]
</p><p>18 Further, signals of the same class in the target domain will not have similar sparse codes as those from the source domain. [sent-50, score-0.942]
</p><p>19 Therefore, effectively leverage unlabeled target data  to adapt the dictionary from one domain to another while maintaining certain invariant representation becomes crucial for successful DA. [sent-52, score-0.91]
</p><p>20 We hypothesize existence of a virtual path which smoothly connects the source and target domains. [sent-55, score-0.578]
</p><p>21 Imagine the source domain consists of face images in the frontal view while the target domain contains those in the profile view. [sent-56, score-1.393]
</p><p>22 Intuitively, face images which gradually transform from the frontal to profile view will form a smooth transition path. [sent-57, score-0.391]
</p><p>23 Recovering intermediate representations along the transition path allows us to more likely capture the underlying domain shift, as well as to build meaningful feature representations which are preserved across different domains. [sent-58, score-0.894]
</p><p>24 Specifically, we sample several intermediate domains along a virtual path between the source and target domains, and represent each intermediate domain using a dictionary. [sent-60, score-1.527]
</p><p>25 We then utilize the good reconstruction property of dictionaries, and learn the set of intermediate domain dictionaries which incrementally reduce the reconstruction residue of the target data. [sent-61, score-1.254]
</p><p>26 In the mean time, we constrain the magnitude of changes between dictionaries for adjacent intermediate domains to ensure the smoothness ofthe transition path ( refer to Figure 2 for an illustration). [sent-62, score-0.729]
</p><p>27 (2) We then apply invariant sparse codes across the source, intermediate and target domains to render inter-  mediate representations, which convey a smooth transition in the data signal space. [sent-63, score-0.972]
</p><p>28 It also provides a shared feature representation where the sample differences caused by distribution shifts are reduced, and we utilize this new feature representation for cross domain recognition. [sent-64, score-0.486]
</p><p>29 (3) We provide a quantification of domain shift by measuring the similarity between the source and target domain dictionaries which are learned using our DA approach. [sent-65, score-1.567]
</p><p>30 Presented with multiple domains, this quantitative measure can be exploited to select the optimal domain to adapt to. [sent-66, score-0.41]
</p><p>31 (4) We demonstrate the wide applicability of our approach for face recognition across pose, illumination and blur variations, cross dataset object recognition, and report the improved performance of our approach over existing DA methods. [sent-67, score-0.432]
</p><p>32 In Section 3, we present our general unsupervised DA approach supported by a quantitative measure of domain shift. [sent-69, score-0.435]
</p><p>33 Semi-supervised DA methods rely on labeled target data to perform cross domain classification. [sent-75, score-0.723]
</p><p>34 Metric learning approaches [28, 18] were also proposed to learn a cross domain transformation to link two domains. [sent-79, score-0.419]
</p><p>35 [17] utilized low-rank reconstructions to learn a transformation so that the transformed source samples can be linearly reconstructed by the target samples. [sent-81, score-0.528]
</p><p>36 Given no labels in the target domain to learn the similarity measure between data instances across domains, unsupervised DA is more difficult to tackle. [sent-82, score-0.763]
</p><p>37 Therefore it usually enforces certain prior assumptions to relate source and target data. [sent-83, score-0.486]
</p><p>38 The techniques in [25, 26] reduce the distance across two domains by learning a latent feature space where domain similarity is measured through maximum mean discrepancy. [sent-86, score-0.59]
</p><p>39 Shi and Sha [29] define an information-theoretic measure which balances between maximizing domain similarity and minimizing expected classification error on the  target domain. [sent-87, score-0.65]
</p><p>40 Two recent approaches [14], [13] in the computer vision community are more relevant to our methodology, where the source and target domains are linked by sampling finite or infinite number of intermediate subspaces on the Grassmannian manifold. [sent-88, score-0.932]
</p><p>41 These intermediate subspaces appear to be able to capture the intrinsic domain shift. [sent-89, score-0.627]
</p><p>42 Compared to their abstract manifold walking strategies, our approach emphasizes on synthesizing intermediate subspaces in a manner which gradually reduces the reconstruction residue of the target data. [sent-90, score-0.802]
</p><p>43 Domain invariant sparse codes are designed for cross domain recognition, alignment and synthesis. [sent-92, score-0.492]
</p><p>44 Let Ys ∈ Rn∗Ns, Yt ∈ Rn∗Nt be the data instances from the source and target d∈om Rain respectively, where n is the dimension of the data instance, Ns and Nt denote the number of samples in the source and target domains. [sent-97, score-0.972]
</p><p>45 Let D0 ∈ Rn∗m be the dictionary learned from Ys using standard∈ dictionary learning methods, e. [sent-98, score-0.484]
</p><p>46 As introduced in Section 1, our approach samples several intermediate domains from a smooth transition path between the source and target domains. [sent-100, score-1.095]
</p><p>47 We associate each intermediate domain with a dictionary Dk , k ∈ [1, K], where K is the number of intermediate domai,nks w∈h [i1c,hK Kw],il w bhee dreet Kerm isin theed in our DA approach. [sent-101, score-1.002]
</p><p>48 Learning Intermediate Domain Dictionaries Starting from the source domain dictionary D0, we sequentially learn the intermediate domain dictionaries {Dk}kK=1 to gradually adapt to the target data. [sent-104, score-1.871]
</p><p>49 The final dictionary DK which best represents the target data in terms of reconstruction error is taken as the target domain dictionary. [sent-106, score-1.18]
</p><p>50 Given the k-th domain dictionary Dk , k ∈ [0, K − 1], we learn the next domain dictionary Dk+1 kb ∈ase [d0 on i t−s c1o],he wreence with Dk and the remaining residue of the target data. [sent-107, score-1.614]
</p><p>51 Specifically, we decompose the target data Yt with Dk and get the reconstruction residue Jk: Γk= argΓmin? [sent-108, score-0.494]
</p><p>52 The next intermediate dois main dictionary Dk+1 is then obtained as: Dk+1 = Dk + ΔDk  (5)  Note that when λ = 0, the Method of Optimal Direction (MOD) [12] becomes a special case of equation (3), where no regularization is enforced. [sent-130, score-0.437]
</p><p>53 Starting from the source domain dictionary D0, we apply the above adaptation framework iteratively, and stop the procedure when the magnitude of ? [sent-131, score-0.895]
</p><p>54 two domains is absorbed into the learned intermediate domain dictionaries. [sent-135, score-0.767]
</p><p>55 This stopping criteria also automatically gives the number of intermediate domains to sample from the transition path. [sent-136, score-0.571]
</p><p>56 t the current intermediate domain dictionary and the encoding coefficients. [sent-140, score-0.794]
</p><p>57 Algorithm 1 Algorithm to interpolate intermediate subspaces between source and target domains. [sent-150, score-0.776]
</p><p>58 1:Input: Dictionary D0trained from the source data, target data Yt, sparsity level T, stopping threshold δ, parameter λ, k = 0. [sent-151, score-0.536]
</p><p>59 2: Output: Dictionaries {Dk}kK=−11 for the intermediate Odoumtpauint:s, dictionary sD {KD Dfor} the target domain. [sent-152, score-0.708]
</p><p>60 Recognition Under Domain Shift Up to now, we have learned a transition path which is encoded with the underlying domain shift. [sent-159, score-0.585]
</p><p>61 This provides us with rich information to obtain new representations to associate source and target data. [sent-160, score-0.532]
</p><p>62 Here, we simply apply invariant sparse codes across the source, intermediate, target domain dictionaries {Dk}kK=0. [sent-161, score-0.879]
</p><p>63 , (DKα)T]T where α ∈ Rm is the sparse code of a source data signal decomposed Rwith D0, or a target data signal decomposed with DK. [sent-165, score-0.679]
</p><p>64 This new representation incorporates the smooth domain transition recovered in the intermediate dictionaries into the signal space. [sent-166, score-0.894]
</p><p>65 It brings the source and target data into a shared feature space where the data distribution shift is mitigated. [sent-167, score-0.643]
</p><p>66 Given the new feature vectors, we apply PCA for dimension reduction1 , and then  employ a SVM classifier for cross domain recognition. [sent-169, score-0.419]
</p><p>67 For instance, we may be faced with more than one source domains in some scenarios. [sent-173, score-0.391]
</p><p>68 QDS will allow us to select the optimal source domain which has the least domain shift w. [sent-174, score-1.056]
</p><p>69 We propose to obtain QDS by measuring the similarity between the source domain dictionary D0 and the target domain dictionary DK which is learned using Algorithm 1. [sent-177, score-1.706]
</p><p>70 This similarity characterizes the amount of domain shift encoded along the transition path. [sent-178, score-0.643]
</p><p>71 en D0 and DK, and less domain shift along the learned transition path. [sent-183, score-0.647]
</p><p>72 Similarly, by reversing the role of source and target domain to learn the transition path, we can obtain Qt,s which is the amount of shift from target to source domain. [sent-184, score-1.593]
</p><p>73 We selected the frontal face images as the source domain, with a total of 1428 images. [sent-193, score-0.408]
</p><p>74 83468 target domain contains images at different poses, which are denoted as c05 and c29 (yawning about ±22. [sent-202, score-0.628]
</p><p>75 5hose the farnodnt c-1il1lum (yianwatneindg source images to be the labeled data in the source domain. [sent-205, score-0.463]
</p><p>76 The task is to determine the identity of the images in the target domain with the same illumination condition. [sent-206, score-0.693]
</p><p>77 1) Baseline K-SVD [1], where target data is directly decomposed with the dictionary learned from the source domain, and the resulting sparse codes are compared using a nearest neighbor classifier. [sent-209, score-0.87]
</p><p>78 As our DA approach gradually updates the dictionary learned from frontal face images using non-frontal images, these transformed representations thus convey the transition process in this scenario. [sent-216, score-0.734]
</p><p>79 The remaining images with the other 10 illumination conditions  were convolved with a blur kernel to form the target domain. [sent-225, score-0.486]
</p><p>80 Synthesized intermediate representations between frontal face images and face images at pose c11. [sent-227, score-0.601]
</p><p>81 The first row shows the transformed images from a source image (in red box) to the target domain. [sent-228, score-0.528]
</p><p>82 The second row shows the transformed images from a target image (in green box) to the source domain. [sent-229, score-0.528]
</p><p>83 Since the domain shift in this experiment consists of both illumination and blur variations, traditional methods which are only illumination insensitive or robust to blur are not able to fully handle both variations. [sent-240, score-0.893]
</p><p>84 We also show transformed intermediate representations along the transition path of our approach in Figure 4, which clearly captures the transition from clear to blur images and vice versa. [sent-242, score-0.764]
</p><p>85 Synthesized intermediate representations from face recognition across blur and illumination variations (motion blur with length of 9). [sent-279, score-0.792]
</p><p>86 The first row shows the transformed images from a source image (in red box) to the target domain. [sent-280, score-0.528]
</p><p>87 The second row shows the transformed images from a target image (in green box) to the source domain. [sent-281, score-0.528]
</p><p>88 We report performance on eight different pairs of source and target combinations. [sent-295, score-0.486]
</p><p>89 We ran 20 different trials  corresponding to different selections of labeled data from the source and target domains. [sent-299, score-0.519]
</p><p>90 It is seen that baseline K-SVD has the lowest recognition rate except for one pair of source and target combination in the semi-supervised setting. [sent-301, score-0.518]
</p><p>91 Average  reconstruction  (b) error of the target domain decomposed  (c) with the source and intermediate  domains. [sent-307, score-1.159]
</p><p>92 The combinations  of source and target domains are (a) frontal face images v. [sent-308, score-0.855]
</p><p>93 l279tAe586ch  Decrease of reconstruction residue along the transition path: Figure 6 shows the average reconstruction residue of target data decomposed with the source, and intermediate domain dictionaries {Dk}kK=0 along the transittieornm path ew dhoicmha were lteioarnnaerdie using Algorithm 1. [sent-320, score-1.683]
</p><p>94 We provide results on three pairs of source and target combinations: frontal face images v. [sent-321, score-0.679]
</p><p>95 These quantitative values of domain shift are in line with our experimental performance, i. [sent-331, score-0.484]
</p><p>96 , higher QDS values indicate less domain shift, and a higher recognition rate between the corresponding two domains. [sent-333, score-0.389]
</p><p>97 Conclusions We presented a fully unsupervised DA method by incrementally learning intermediate domain dictionaries to capture the underlying domain shift. [sent-335, score-1.143]
</p><p>98 This allows us to transform original data instances from different modalities into a shared feature representation, which serves as a robust sig-  nature for cross domain classification. [sent-336, score-0.449]
</p><p>99 Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach. [sent-384, score-0.451]
</p><p>100 Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. [sent-571, score-0.435]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dk', 0.428), ('domain', 0.357), ('target', 0.271), ('dictionary', 0.229), ('da', 0.221), ('source', 0.215), ('intermediate', 0.208), ('domains', 0.176), ('residue', 0.171), ('qds', 0.169), ('jk', 0.166), ('dictionaries', 0.143), ('transition', 0.137), ('blur', 0.129), ('shift', 0.127), ('kt', 0.117), ('face', 0.109), ('atoms', 0.101), ('adaptation', 0.094), ('frontal', 0.084), ('unsupervised', 0.078), ('gfk', 0.069), ('webcam', 0.068), ('path', 0.065), ('illumination', 0.065), ('subspaces', 0.062), ('cross', 0.062), ('dslr', 0.06), ('decomposed', 0.056), ('tut', 0.056), ('yt', 0.053), ('adapt', 0.053), ('sgf', 0.052), ('reconstruction', 0.052), ('caltech', 0.051), ('amazon', 0.051), ('stopping', 0.05), ('quantification', 0.049), ('pages', 0.048), ('kk', 0.047), ('representations', 0.046), ('pose', 0.045), ('codes', 0.044), ('qiu', 0.043), ('angel', 0.042), ('kjktjk', 0.042), ('yawning', 0.042), ('transformed', 0.042), ('subspace', 0.039), ('variations', 0.039), ('life', 0.039), ('gradually', 0.038), ('shifts', 0.037), ('synthesized', 0.037), ('across', 0.035), ('albedo', 0.034), ('jhuo', 0.033), ('kwok', 0.033), ('labeled', 0.033), ('recognition', 0.032), ('rama', 0.031), ('tr', 0.031), ('proposition', 0.031), ('shared', 0.03), ('mairal', 0.03), ('sparse', 0.029), ('biswas', 0.029), ('diag', 0.028), ('aggarwal', 0.028), ('virtual', 0.027), ('signal', 0.026), ('tsang', 0.026), ('ijcai', 0.026), ('signals', 0.026), ('learned', 0.026), ('nt', 0.026), ('correspondence', 0.025), ('saenko', 0.025), ('chellappa', 0.024), ('kulis', 0.024), ('sha', 0.024), ('collected', 0.024), ('proof', 0.024), ('bach', 0.023), ('convey', 0.023), ('substitute', 0.023), ('ys', 0.023), ('smooth', 0.023), ('elad', 0.023), ('pan', 0.023), ('deconvolution', 0.023), ('similarity', 0.022), ('insensitive', 0.021), ('conditions', 0.021), ('shi', 0.021), ('costly', 0.02), ('adjustment', 0.02), ('interpolate', 0.02), ('june', 0.02), ('surf', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="419-tfidf-1" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>2 0.44378743 <a title="419-tfidf-2" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>3 0.39600596 <a title="419-tfidf-3" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>4 0.3699089 <a title="419-tfidf-4" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>Author: Lin Chen, Lixin Duan, Dong Xu</p><p>Abstract: In this work, we propose to leverage a large number of loosely labeled web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which the samples from different source domains can be represented by different types of features with different dimensions (e.g., the SIFTfeaturesfrom web images and space-time (ST) features from web videos) while the target domain samples have all types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier, in which we simultaneously seek the optimal weights for different source domains with different types of features as well as infer the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of MDA-HS for event recognition in consumer videos.</p><p>5 0.23690537 <a title="419-tfidf-5" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>6 0.22956061 <a title="419-tfidf-6" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>7 0.22691381 <a title="419-tfidf-7" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>8 0.20804578 <a title="419-tfidf-8" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>9 0.18943323 <a title="419-tfidf-9" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>10 0.17979573 <a title="419-tfidf-10" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>11 0.17182995 <a title="419-tfidf-11" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>12 0.16291051 <a title="419-tfidf-12" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>13 0.15388286 <a title="419-tfidf-13" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>14 0.15034616 <a title="419-tfidf-14" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>15 0.14210974 <a title="419-tfidf-15" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>16 0.13394845 <a title="419-tfidf-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.13266714 <a title="419-tfidf-17" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>18 0.11740869 <a title="419-tfidf-18" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>19 0.11426979 <a title="419-tfidf-19" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>20 0.11326716 <a title="419-tfidf-20" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.22), (1, -0.107), (2, -0.251), (3, 0.241), (4, -0.117), (5, 0.002), (6, 0.067), (7, 0.002), (8, 0.087), (9, 0.065), (10, 0.002), (11, -0.048), (12, -0.023), (13, -0.025), (14, -0.151), (15, -0.122), (16, -0.021), (17, -0.125), (18, -0.114), (19, -0.064), (20, -0.156), (21, -0.219), (22, -0.101), (23, -0.061), (24, -0.02), (25, -0.037), (26, 0.126), (27, -0.191), (28, -0.092), (29, -0.019), (30, -0.014), (31, -0.061), (32, 0.05), (33, 0.048), (34, -0.022), (35, -0.09), (36, 0.037), (37, 0.091), (38, -0.018), (39, 0.11), (40, 0.015), (41, 0.1), (42, 0.027), (43, -0.016), (44, -0.066), (45, -0.049), (46, 0.087), (47, 0.022), (48, -0.006), (49, -0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98007351 <a title="419-lsi-1" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>2 0.8507635 <a title="419-lsi-2" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>3 0.81818515 <a title="419-lsi-3" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>Author: Lin Chen, Lixin Duan, Dong Xu</p><p>Abstract: In this work, we propose to leverage a large number of loosely labeled web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which the samples from different source domains can be represented by different types of features with different dimensions (e.g., the SIFTfeaturesfrom web images and space-time (ST) features from web videos) while the target domain samples have all types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier, in which we simultaneously seek the optimal weights for different source domains with different types of features as well as infer the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of MDA-HS for event recognition in consumer videos.</p><p>4 0.7654146 <a title="419-lsi-4" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>5 0.68932772 <a title="419-lsi-5" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>6 0.53118259 <a title="419-lsi-6" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>7 0.52500445 <a title="419-lsi-7" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>8 0.51812583 <a title="419-lsi-8" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>9 0.50004995 <a title="419-lsi-9" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>10 0.49840289 <a title="419-lsi-10" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>11 0.48839122 <a title="419-lsi-11" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>12 0.48026916 <a title="419-lsi-12" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>13 0.46982533 <a title="419-lsi-13" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>14 0.43946558 <a title="419-lsi-14" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>15 0.41835126 <a title="419-lsi-15" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>16 0.39431685 <a title="419-lsi-16" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>17 0.3909235 <a title="419-lsi-17" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>18 0.38602793 <a title="419-lsi-18" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>19 0.37527606 <a title="419-lsi-19" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>20 0.35259485 <a title="419-lsi-20" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.17), (16, 0.033), (19, 0.014), (26, 0.028), (33, 0.26), (39, 0.012), (44, 0.123), (67, 0.084), (69, 0.07), (87, 0.087), (91, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93101335 <a title="419-lda-1" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>2 0.9249602 <a title="419-lda-2" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>3 0.92102766 <a title="419-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>same-paper 4 0.91876793 <a title="419-lda-4" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>5 0.91415143 <a title="419-lda-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.91069824 <a title="419-lda-6" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>7 0.90989763 <a title="419-lda-7" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>8 0.9096778 <a title="419-lda-8" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>9 0.90842909 <a title="419-lda-9" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>10 0.90770578 <a title="419-lda-10" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>11 0.90765071 <a title="419-lda-11" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>12 0.90625465 <a title="419-lda-12" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>13 0.90378755 <a title="419-lda-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.90346038 <a title="419-lda-14" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>15 0.90315419 <a title="419-lda-15" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>16 0.90140879 <a title="419-lda-16" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>17 0.9012174 <a title="419-lda-17" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>18 0.90105331 <a title="419-lda-18" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>19 0.9005571 <a title="419-lda-19" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>20 0.90022278 <a title="419-lda-20" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
