<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-176" href="#">cvpr2013-176</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</h1>
<br/><p>Source: <a title="cvpr-2013-176-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Herout_Five_Shades_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Adam Herout, István Szentandrási, Michal Zachariáš, Markéta Dubská, Rudolf Kajan</p><p>Abstract: We introduce here an improved design of the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution.</p><p>Reference: <a title="cvpr-2013-176-reference" href="../cvpr2013_reference/cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. [sent-7, score-1.122]
</p><p>2 Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. [sent-8, score-2.2]
</p><p>3 The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. [sent-9, score-1.272]
</p><p>4 The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. [sent-10, score-1.126]
</p><p>5 At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile  and low-power devices. [sent-12, score-1.002]
</p><p>6 We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution. [sent-13, score-1.024]
</p><p>7 Introduction For augmented reality applications and other similar problems in computer vision, camera localization within a captured scene is crucial. [sent-15, score-0.17]
</p><p>8 Camera localization can be done either by using fiduciary markers [6] or without them (by using PTAM [10], keypoint template tracking [16], homography-based [11], etc. [sent-16, score-0.313]
</p><p>9 We are dealing with applications and scenes where fiduciary markers are acceptable (see Sec. [sent-18, score-0.238]
</p><p>10 5) allow for perfectly planar markers placed on a tabletop, a wall, computer screen, etc. [sent-30, score-0.279]
</p><p>11 The challenge is that the marker must cover a large planar area and, at the same time, it must be reliably detected even from a small visible portion of the marker. [sent-31, score-0.962]
</p><p>12 What marker design and corresponding detection algorithm can meet these requirements and, at the same time, be aesthetically appealing? [sent-33, score-0.905]
</p><p>13 Their Uniform Marker Fields are planar checkerboard fields oflargely overlapping markers, shaped as a 4-orientable n2-window array [3]. [sent-36, score-0.161]
</p><p>14 This overlapping property allows the marker –  fields to outperform arrays of conventional disjoint markers such as the ARtag [5], ALVAR [1], or CALTag [2]. [sent-37, score-1.112]
</p><p>15 Markerbased solutions such as ARtag and ALVAR (a number of other similar solutions exists) are using square black-andwhite markers with their identity digitally encoded. [sent-38, score-0.209]
</p><p>16 One 1 1 13 3 38 8 824 2  part of the marker’s design is used for the marker’s localization (typically the outer black/white rim) and another part is used for distinguishing between individual markers (typically inner content of the square). [sent-39, score-0.278]
</p><p>17 An array of such individual markers is used to cover a larger planar area. [sent-40, score-0.298]
</p><p>18 CALtag [2] alters black-and-white with white-and-black (inverse) markers and attaches the markers one to another. [sent-41, score-0.418]
</p><p>19 Another approach to overlapping individual detectable windows within a large marker are the Random Dot Markers [17] by Uchiyama et al. [sent-42, score-0.857]
</p><p>20 The field of random dots can be also used as a deformable marker [15]. [sent-44, score-0.905]
</p><p>21 In their short work, they used binary De Bruijn tori [3] in a checkerboard as the marker field. [sent-47, score-0.85]
</p><p>22 In this paper, we propose to use a greyscale grid of squares (instead of a binary one): it offers more edges in the marker field to be detected and, at the same time, a smaller window of the De Bruijn torus is necessary for identifying a unique location. [sent-48, score-1.222]
</p><p>23 We present here an algorithm for the detection of  ×  the greyscale grid of squares. [sent-49, score-0.21]
</p><p>24 Our algorithm detects the planar projected grid as a single compound object, instead of detecting straight lines and then forming a grid from them. [sent-50, score-0.207]
</p><p>25 A unique location in the marker field is identified by the edges between the marker field modules. [sent-51, score-1.876]
</p><p>26 We highlight three target applications of this marker field design and detection algorithm (Sec. [sent-60, score-0.958]
</p><p>27 All these applications (and others as well) can readily use our marker in the scene and they can ensure that the marker is planar. [sent-62, score-1.646]
</p><p>28 4) that our marker field design and detection algorithm outperforms the existing solutions for this class of camera pose estimation problems. [sent-64, score-1.073]
</p><p>29 [14] interpret such an array as a black-and-white checkerboard and propose to use it as a marker field for augmented reality. [sent-69, score-0.973]
</p><p>30 Thanks to this overlap, only a small fraction of the marker field must be visible in order to be detected and recognized. [sent-71, score-0.975]
</p><p>31 We work with grayscale or color k-ary marker fields (aij ∈ {0, . [sent-80, score-0.868]
</p><p>32 bs Holouwtee greyscale or rcoisloonr values of the grid modules cannot be reliably discerned under varying lighting and camera conditions. [sent-89, score-0.436]
</p><p>33 That is why we −  use the edge gradients between the modules for localization within the marker field. [sent-90, score-1.032]
</p><p>34 , e(↓r+n−2,c+n−1))  (13)) Synthesis of the marker field is done in a manner similar to the genetic algorithm sketched out by Szentandr a´si et al. [sent-101, score-0.925]
</p><p>35 In our case, the fitness function must also reflect the quality of edges between the modules edges with higher absolute value |eij | are preferred. [sent-102, score-0.25]
</p><p>36 Small Footprint Detection & Recognition of Planar Greyscale Grids of Squares This section describes the algorithm for detection of the greyscale checkerboard-like marker field. [sent-104, score-0.977]
</p><p>37 This algorithm supposes that the grid of squares is planar and projected by a perspective projection. [sent-105, score-0.202]
</p><p>38 OneDachscanli e, dgesared tec d (Red) and extended to edgels (Green) by iteratively  finding further edge pixels in the direction perpendicular to the gradient. [sent-111, score-0.162]
</p><p>39 B: The  edgels are grouped into two dominant groups using RANSAC; two vanishing points are computed by hyperplane fitting. [sent-112, score-0.247]
</p><p>40 C: Based on the vanishing points, the optimal grid is fitted to the set of the edgels (orange dots denote the estimated centers of grid modules). [sent-113, score-0.337]
</p><p>41 Greyscale Grid Detection Conventional marker detectors typically rely on first detecting the bounding borders [9, 5] of the markers by finding the contours in a thresholded image and choosing shapes consisting of four straight-line contours. [sent-124, score-1.032]
</p><p>42 Our uniform marker field does not distinguish between marker design features intended for general marker detection and features for marker identification. [sent-125, score-3.427]
</p><p>43 The motivation for this approach is to better use the marker field’s surface: the localization features are much denser in the field, while still preserving the identification capabilities. [sent-127, score-0.866]
</p><p>44 Extraction of edgels (edge element or edge pixel;  term borrowed from Martin Hirzer [8]) typically, the algorithm extracts around one hundred straight edge fragments in the whole image. [sent-130, score-0.21]
</p><p>45 When a video input is being processed, the detected edgels are filtered based on the previous detected position of the marker field. [sent-133, score-1.039]
</p><p>46 In the tests we used a simple rectangular mask to filter out the edges outside the area corresponding to the previously detected marker field. [sent-134, score-0.951]
</p><p>47 Determining two dominant vanishing points among the edgels (Fig. [sent-136, score-0.225]
</p><p>48 Using homogeneous coordinates for the vanishing point v and the pencil of lines li, all the lines are supposed to be coincident with the vanishing point, i. [sent-138, score-0.297]
</p><p>49 Points of the real projective plane correspond to hyperplanes passing through the origin, so the vanishing point can be found by fitting a hyperplane through all the lines (extended edgels) observed in the pencil. [sent-142, score-0.158]
</p><p>50 Finding the grid ofmarker field edges as two groups (pencils) of regularly repeated lines coincident with each vanishing point. [sent-154, score-0.365]
</p><p>51 Marker edges of one direction can be computed using the horizon as ( xˆ denotes normalized vector) li = + (ki + q) (6)  ˆlbase  hˆ,  where lbase is an arbitrarily chosen base line through the vanishing point, different from the horizon [13]. [sent-156, score-0.276]
</p><p>52 For simplicity, the algorithm description supposes that a significant portion of the input image is covered by the marker field. [sent-165, score-0.852]
</p><p>53 However, steps 2 and 3 of the algorithm are conditionally applied on rectangular parts of the image (quarters, ninths); in high-resolution images, the marker  field is thus found even if it covers an arbitrary fraction of the camera input. [sent-166, score-1.011]
</p><p>54 Edge Classification When only a small fraction of the marker field is visible, it is crucial that the edge gradients (Eq. [sent-169, score-0.981]
</p><p>55 In order to correctly classify an edge, given the locations of the neighboring marker field modules, our algorithm samples pixels from the edge’s vicinity. [sent-178, score-0.905]
</p><p>56 )I,f t an edge cioannn isot m baed ceo,n ofthiremrewdis, eth me olroecation between the modules is treated as a place without an  edge: ei∗j = 0. [sent-181, score-0.166]
</p><p>57 Values in the table represent locations in the marker field (two discrete coordinates in the terms of grid modules; enumerated orientation 0◦/ 90◦/ 180◦/ 270◦). [sent-187, score-0.961]
</p><p>58 An absent record in the hash table means a wrongly recognized fragment of the marker field. [sent-188, score-0.887]
</p><p>59 When a compact piece of the marker field is detected in an input image, the edges are classified and used for traversing the tree. [sent-191, score-1.033]
</p><p>60 A central edge in the detected cluster of edges decides the root node, and surrounding edges follow in a predefined order (Fig. [sent-192, score-0.246]
</p><p>61 Any cluster of neighboring edges is recognized by the tree the leaf node would either define the cluster’s location and orientation within the marker field or reject the cluster of edges as invalid (due to misdetection). [sent-194, score-1.115]
</p><p>62 right: Decision tree the leaves are either invalid or contain a location + orientation to the marker field. [sent-202, score-0.823]
</p><p>63 Corner Search and Iterative Refinement For a precise camera pose estimation we find all possible corners in the marker field (with a sub-pixel precision). [sent-205, score-1.048]
</p><p>64 The corners of the grid of squares are projected from the detected overall position and iteratively searched for in the neighborhood. [sent-206, score-0.17]
</p><p>65 Based on the marker field layout, the algorithm knows each corner’s appearance including its rotation and searches for such a particular pattern. [sent-207, score-0.96]
</p><p>66 This helps mostly in cases when the image is motion blurred, the marker is not perfectly planar, or noise in the edgel data cause the grid not to fit the edges precisely. [sent-208, score-0.996]
</p><p>67 Another way of improving the precision of the pose estimation accuracy is to iteratively  search for correct corners in the marker field in the image space using back-projection. [sent-209, score-0.988]
</p><p>68 The other baseline is the Random Dot Markers (RDM) [17] as an alternative “marker field” solution, where individual localization markers overlap in the field. [sent-213, score-0.252]
</p><p>69 For comparing our solution with the alternatives, we shot videos of side-by-side markers (Fig. [sent-215, score-0.231]
</p><p>70 The marker fields have comparable (as much the same as possible) dimensions and resolution of the individual markers (n2-windows vs. [sent-217, score-1.077]
</p><p>71 Videos were recorded in 1080p, capturing different classes of movement:  zig-zag move-  ment, upright rotation, rotation with severe perspective distortion, near/far movement, variable lighting conditions with fixed camera and general movement with occlusion. [sent-238, score-0.193]
</p><p>72 precision of our algorithm we used the local variance (in time domain) of the estimated camera pose (position and rotation) see Tab. [sent-239, score-0.158]
</p><p>73 Our method gave smoother results thanks to the good spatial distribution of matched points between 2D and 3D and ALVAR’s inability to find the corners of the individual markers precisely in blurred images and for partially occluded corners. [sent-244, score-0.271]
</p><p>74 The number of detected corners used for the camera pose estimation is shown in Fig. [sent-245, score-0.185]
</p><p>75 Random dot markers were the least successful, mostly due to their high sensitivity to the motion blur. [sent-248, score-0.244]
</p><p>76 More points naturally mean a more stable and precise camera pose estimation and better tolerance to wrongly detected points (caused by a motion blur, partial occlusion, etc. [sent-264, score-0.157]
</p><p>77 Figure 6 shows the clear advantage of our continuous marker field over ALVAR. [sent-269, score-0.905]
</p><p>78 ALVAR only detected two completely visible markers and one which had one edge slightly occluded. [sent-270, score-0.299]
</p><p>79 Success rate is the fraction of video frames where at least one of the markers was correctly detected in all the video frames. [sent-278, score-0.279]
</p><p>80 Pixel Footprint and Computation Complexity Table 3 shows the speed of the three tested algorithms and the breakdown of speed of our marker detection algorithm. [sent-281, score-0.87]
</p><p>81 1); grid: reconstructing the grid using RANSAC and vanishing point detection  (Sec. [sent-300, score-0.194]
</p><p>82 While camera localization in natural scenes (SLAM/PTAM) is already achieving very good results and some applications do not require markers anymore, these sample applications deal with scenes where presence of reliable natural keypoints is impossible or undesirable. [sent-311, score-0.399]
</p><p>83 We are experimenting with possibilities of mixing the marker field in an unobtrusive way into any – static or dynamic on-screen situation. [sent-318, score-0.956]
</p><p>84 Once the mobile device knows the location, the marker is displayed only in the vicinity of the mobile’s view frustum so that it is as unobtrusive as possible. [sent-321, score-0.932]
</p><p>85 If enough distinct and stable keypoints are present, the marker is completely hidden and camera the pose is tracked. [sent-322, score-0.971]
</p><p>86 camera pose can be determined by using sensors mounted to the camera (e. [sent-331, score-0.193]
</p><p>87 Camera movement recovery is a technique which estimates the movement using markers or keypoints placed and detected on the mating plate4. [sent-336, score-0.35]
</p><p>88 The marker field presented in this work can be used for the camera pose estimation without any human effort involved in the tracking (Fig. [sent-338, score-1.052]
</p><p>89 [10]), but the presence of a visually unobtrusive and cheaply detectable marker field can provide a reliable starting point for the tracking and offload some of the expensive computation (Fig. [sent-354, score-1.058]
</p><p>90 Conclusions We presented a new design of marker fields whose square modules are greyscale and the location within the field is determined by the edges’ gradients. [sent-360, score-1.221]
</p><p>91 Then, we proposed an efficient and reliable algorithm for detection of the marker field. [sent-361, score-0.886]
</p><p>92 We discussed three representative target appli-  ×  cations where planar marker fields are desirable. [sent-362, score-0.938]
</p><p>93 The results confirm that marker fields based on edges between the greyscale modules outperform the existing comparable solutions: arrays of black-and-white markers and random dot fields. [sent-363, score-1.458]
</p><p>94 The detection algorithm is efficient and reliable because the grid of squares is being detected as a whole and the edgels thus can be detected roughly and sparsely. [sent-364, score-0.343]
</p><p>95 We are working on an altered algorithm that will not require the marker to be planar on the contrary, the marker could be strongly deformed as on a cloth or wrinkled paper. [sent-370, score-1.716]
</p><p>96 The omnipresence of the detectable edges in the marker field will allow for real-time and precise detection of a deformed marker field. [sent-371, score-1.855]
</p><p>97 We will further experiment with the color marker fields where shades of grey are replaced by different tones of color. [sent-372, score-0.925]
</p><p>98 The abundance of localization information will allow for introducing further constraints into the marker field design – namely similarity to a given raster image. [sent-373, score-0.974]
</p><p>99 We expect these markers to be found even more aesthetically pleasing to the user. [sent-374, score-0.238]
</p><p>100 Planar grouping for automatic detection of vanishing lines and points. [sent-460, score-0.163]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('marker', 0.823), ('alvar', 0.229), ('markers', 0.209), ('greyscale', 0.127), ('modules', 0.118), ('edgels', 0.114), ('vanishing', 0.111), ('szentandr', 0.086), ('field', 0.082), ('camera', 0.078), ('caltag', 0.072), ('umf', 0.072), ('planar', 0.07), ('edges', 0.066), ('rdm', 0.064), ('artag', 0.057), ('uchiyama', 0.057), ('ultramobile', 0.057), ('grid', 0.056), ('ismar', 0.052), ('unobtrusive', 0.051), ('edgel', 0.051), ('edge', 0.048), ('fields', 0.045), ('footprint', 0.045), ('localization', 0.043), ('bruijn', 0.043), ('lbase', 0.043), ('wald', 0.043), ('detected', 0.042), ('pose', 0.037), ('reliable', 0.036), ('tabletop', 0.035), ('dot', 0.035), ('arrays', 0.035), ('detectable', 0.034), ('hash', 0.034), ('mobile', 0.034), ('gave', 0.034), ('keypoints', 0.033), ('movement', 0.033), ('screen', 0.032), ('tracking', 0.032), ('shades', 0.032), ('rotation', 0.031), ('fiducial', 0.03), ('targeted', 0.03), ('recognized', 0.03), ('lighting', 0.03), ('aesthetically', 0.029), ('brno', 0.029), ('cezmsmt', 0.029), ('dubsk', 0.029), ('fiduciary', 0.029), ('gimbal', 0.029), ('herout', 0.029), ('oieatrn', 0.029), ('scanlines', 0.029), ('supposes', 0.029), ('xzy', 0.029), ('zachari', 0.029), ('zyx', 0.029), ('horizon', 0.028), ('fraction', 0.028), ('corners', 0.028), ('reliably', 0.027), ('checkerboard', 0.027), ('reality', 0.027), ('detection', 0.027), ('squares', 0.026), ('design', 0.026), ('coincident', 0.025), ('grey', 0.025), ('lines', 0.025), ('variance', 0.025), ('production', 0.025), ('cluster', 0.024), ('rigs', 0.024), ('knows', 0.024), ('aij', 0.023), ('corner', 0.023), ('videos', 0.022), ('augmented', 0.022), ('hyperplane', 0.022), ('blur', 0.022), ('conflict', 0.021), ('perspective', 0.021), ('sequential', 0.021), ('classified', 0.02), ('sketched', 0.02), ('visited', 0.02), ('tests', 0.02), ('devices', 0.02), ('breakdown', 0.02), ('handbook', 0.019), ('array', 0.019), ('precision', 0.018), ('position', 0.018), ('ei', 0.018), ('xz', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="176-tfidf-1" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>Author: Adam Herout, István Szentandrási, Michal Zachariáš, Markéta Dubská, Rudolf Kajan</p><p>Abstract: We introduce here an improved design of the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution.</p><p>2 0.11498455 <a title="176-tfidf-2" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>3 0.055356465 <a title="176-tfidf-3" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>4 0.04905441 <a title="176-tfidf-4" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>5 0.047214378 <a title="176-tfidf-5" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>Author: Karl Pauwels, Leonardo Rubio, Javier Díaz, Eduardo Ros</p><p>Abstract: We propose a novel model-based method for estimating and tracking the six-degrees-of-freedom (6DOF) pose of rigid objects of arbitrary shapes in real-time. By combining dense motion and stereo cues with sparse keypoint correspondences, and by feeding back information from the model to the cue extraction level, the method is both highly accurate and robust to noise and occlusions. A tight integration of the graphical and computational capability of Graphics Processing Units (GPUs) results in pose updates at framerates exceeding 60 Hz. Since a benchmark dataset that enables the evaluation of stereo-vision-based pose estimators in complex scenarios is currently missing in the literature, we have introduced a novel synthetic benchmark dataset with varying objects, background motion, noise and occlusions. Using this dataset and a novel evaluation methodology, we show that the proposed method greatly outperforms state-of-the-art methods. Finally, we demonstrate excellent performance on challenging real-world sequences involving object manipulation.</p><p>6 0.045515575 <a title="176-tfidf-6" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>7 0.045260187 <a title="176-tfidf-7" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>8 0.043918006 <a title="176-tfidf-8" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>9 0.042318508 <a title="176-tfidf-9" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>10 0.041842185 <a title="176-tfidf-10" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>11 0.041816477 <a title="176-tfidf-11" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>12 0.041659474 <a title="176-tfidf-12" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>13 0.041254297 <a title="176-tfidf-13" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>14 0.040753547 <a title="176-tfidf-14" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>15 0.039626203 <a title="176-tfidf-15" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>16 0.039622553 <a title="176-tfidf-16" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>17 0.038598623 <a title="176-tfidf-17" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>18 0.038306229 <a title="176-tfidf-18" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>19 0.037774067 <a title="176-tfidf-19" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>20 0.036997169 <a title="176-tfidf-20" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.101), (1, 0.052), (2, 0.004), (3, -0.013), (4, -0.004), (5, 0.003), (6, 0.018), (7, -0.02), (8, 0.005), (9, 0.019), (10, -0.022), (11, 0.037), (12, 0.032), (13, -0.01), (14, -0.011), (15, -0.003), (16, 0.028), (17, 0.044), (18, -0.019), (19, 0.011), (20, 0.011), (21, 0.033), (22, -0.021), (23, -0.025), (24, 0.038), (25, -0.009), (26, 0.008), (27, -0.029), (28, 0.011), (29, 0.07), (30, -0.01), (31, 0.033), (32, -0.024), (33, 0.051), (34, -0.03), (35, 0.089), (36, 0.004), (37, 0.02), (38, 0.014), (39, 0.03), (40, 0.027), (41, -0.004), (42, -0.056), (43, -0.001), (44, 0.005), (45, -0.043), (46, -0.002), (47, -0.038), (48, -0.005), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90341187 <a title="176-lsi-1" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>Author: Adam Herout, István Szentandrási, Michal Zachariáš, Markéta Dubská, Rudolf Kajan</p><p>Abstract: We introduce here an improved design of the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution.</p><p>2 0.76718754 <a title="176-lsi-2" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>3 0.74721009 <a title="176-lsi-3" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>Author: Luc Oth, Paul Furgale, Laurent Kneip, Roland Siegwart</p><p>Abstract: Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devices—from smart-phones to high-end cameras. It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.</p><p>4 0.72016454 <a title="176-lsi-4" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>5 0.68816167 <a title="176-lsi-5" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>6 0.66908717 <a title="176-lsi-6" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>7 0.65779513 <a title="176-lsi-7" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>8 0.59284538 <a title="176-lsi-8" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>9 0.58116376 <a title="176-lsi-9" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>10 0.56698996 <a title="176-lsi-10" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>11 0.56435859 <a title="176-lsi-11" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>12 0.55576646 <a title="176-lsi-12" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>13 0.55196381 <a title="176-lsi-13" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>14 0.55157781 <a title="176-lsi-14" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>15 0.54982728 <a title="176-lsi-15" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>16 0.5427916 <a title="176-lsi-16" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>17 0.53629881 <a title="176-lsi-17" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>18 0.53295535 <a title="176-lsi-18" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>19 0.52187985 <a title="176-lsi-19" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>20 0.52108419 <a title="176-lsi-20" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.08), (16, 0.024), (26, 0.053), (28, 0.012), (33, 0.203), (59, 0.012), (65, 0.332), (67, 0.05), (69, 0.052), (87, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80690587 <a title="176-lda-1" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>Author: Ajay Kumar, Cyril Kwong</p><p>Abstract: In order to avail the benefits of higher user convenience, hygiene, and improved accuracy, contactless 3D fingerprint recognition techniques have recently been introduced. One of the key limitations of these emerging 3D fingerprint technologies to replace the conventional 2D fingerprint system is their bulk and high cost, which mainly results from the use of multiple imaging cameras or structured lighting employed in these systems. This paper details the development of a contactless 3D fingerprint identification system that uses only single camera. We develop a new representation of 3D finger surface features using Finger Surface Codes and illustrate its effectiveness in matching 3D fingerprints. Conventional minutiae representation is extended in 3D space to accurately match the recovered 3D minutiae. Multiple 2D fingerprint images (with varying illumination profile) acquired to build 3D fingerprints can themselves be used recover 2D features for further improving 3D fingerprint identification and has been illustrated in this paper. The experimental results are shown on a database of 240 client fingerprints and confirm the advantages of the single camera based 3D fingerprint identification.</p><p>2 0.75038141 <a title="176-lda-2" href="./cvpr-2013-Efficient_3D_Endfiring_TRUS_Prostate_Segmentation_with_Globally_Optimized_Rotational_Symmetry.html">139 cvpr-2013-Efficient 3D Endfiring TRUS Prostate Segmentation with Globally Optimized Rotational Symmetry</a></p>
<p>Author: Jing Yuan, Wu Qiu, Eranga Ukwatta, Martin Rajchl, Xue-Cheng Tai, Aaron Fenster</p><p>Abstract: Segmenting 3D endfiring transrectal ultrasound (TRUS) prostate images efficiently and accurately is of utmost importance for the planning and guiding 3D TRUS guided prostate biopsy. Poor image quality and imaging artifacts of 3D TRUS images often introduce a challenging task in computation to directly extract the 3D prostate surface. In this work, we propose a novel global optimization approach to delineate 3D prostate boundaries using its rotational resliced images around a specified axis, which properly enforces the inherent rotational symmetry of prostate shapes to jointly adjust a series of 2D slicewise segmentations in the global 3D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard, we propose a novel coupled continuous max-flow model, which not only provides a powerful mathematical tool to analyze the proposed optimization problem but also amounts to a new and efficient duality-basedalgorithm. Ex- tensive experiments demonstrate that the proposed method significantly outperforms the state-of-art methods in terms ofefficiency, accuracy, reliability and less user-interactions, and reduces the execution time by a factor of 100.</p><p>same-paper 3 0.74332511 <a title="176-lda-3" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>Author: Adam Herout, István Szentandrási, Michal Zachariáš, Markéta Dubská, Rudolf Kajan</p><p>Abstract: We introduce here an improved design of the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution.</p><p>4 0.72306329 <a title="176-lda-4" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>5 0.71144104 <a title="176-lda-5" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>6 0.67491329 <a title="176-lda-6" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>7 0.67345065 <a title="176-lda-7" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>8 0.66534001 <a title="176-lda-8" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>9 0.59153444 <a title="176-lda-9" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>10 0.58975971 <a title="176-lda-10" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>11 0.5878756 <a title="176-lda-11" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>12 0.58720708 <a title="176-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.58648896 <a title="176-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.5857318 <a title="176-lda-14" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>15 0.58497357 <a title="176-lda-15" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>16 0.58402091 <a title="176-lda-16" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>17 0.58377641 <a title="176-lda-17" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>18 0.58340317 <a title="176-lda-18" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>19 0.58338809 <a title="176-lda-19" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>20 0.58269233 <a title="176-lda-20" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
