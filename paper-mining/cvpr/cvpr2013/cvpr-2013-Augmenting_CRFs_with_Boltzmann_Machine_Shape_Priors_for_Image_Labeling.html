<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-50" href="#">cvpr2013-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</h1>
<br/><p>Source: <a title="cvpr-2013-50-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kae_Augmenting_CRFs_with_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>Reference: <a title="cvpr-2013-50-reference" href="../cvpr2013_reference/cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. [sent-12, score-0.253]
</p><p>2 In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been  learned without any attribute-level supervision. [sent-14, score-0.555]
</p><p>3 Introduction Segmentation and region labeling are core techniques for the critical mid-level vision tasks of grouping and organizing image regions into coherent parts. [sent-16, score-0.177]
</p><p>4 Segmentation refers to the grouping of image pixels into parts without applying labels to those parts, and region labeling assigns specific category names to those parts. [sent-17, score-0.207]
</p><p>5 While many segmentation and region labeling algorithms have been used in general object recognition and scene analysis, they have played a surprisingly small role in the challenging problems of face recognition. [sent-18, score-0.319]
</p><p>6 [13] identified the potential role of region labeling in face recognition, noting that a variety of high-level features, such as pose, hair length, and gender can often be inferred (by people) from the labeling of a face image into hair, skin and background regions. [sent-20, score-1.174]
</p><p>7 The center image shows the superpixel version of the image  which is used as a basis for the labeling. [sent-23, score-0.159]
</p><p>8 In this work, we address the problem of labeling face regions with hair, skin, and background labels as an intermediate step in modeling face structure. [sent-27, score-0.413]
</p><p>9 In region labeling applications, the conditional random field (CRF) [16] is effective at modeling region boundaries. [sent-28, score-0.284]
</p><p>10 For example, the CRF can make a correct transition between the hair and background labels when there is a clear difference between those regions. [sent-29, score-0.514]
</p><p>11 However, when a person’s hair color is similar to that of the background, the CRF may have difficulty deciding where to draw the boundary between the regions. [sent-30, score-0.46]
</p><p>12 It has been shown that restricted Boltzmann machines (RBMs) [28] and their extension to deeper architectures such as deep Boltzmann machines (DBMs) [25], can be used to build effective generative models of object shape. [sent-32, score-0.168]
</p><p>13 Motivated by these examples, we propose the GLOC (GLObal and LOCal) model, a strong model for image  labeling problems, that combines the best properties of the CRF (that enforces local consistency between adjacent nodes) and the RBM (that models global shape prior of the 222000111977  object). [sent-34, score-0.309]
</p><p>14 • The complete image labeling should be consistent with shape priors dee ifminaegde by tehlien segmentation training d waitath. [sent-37, score-0.223]
</p><p>15 For each new image, our model uses mean-field inference to find a good balance between the CRF and RBM potentials in setting the image labels and hidden node values. [sent-39, score-0.244]
</p><p>16 We evaluate our proposed model on a face labeling task using the Labeled Faces in the Wild (LFW) data set. [sent-40, score-0.28]
</p><p>17 As shown in Section 4, our model brings significant improvements in labeling accuracy over the baseline methods, such as the CRF and the conditional RBM. [sent-41, score-0.251]
</p><p>18 In addition, we show in Section 5 that the hidden units in the GLOC model can be interpreted as face attributes, such as whether an individual has long hair or a  beard, or faces left or right. [sent-43, score-0.835]
</p><p>19 These attributes can be useful in retrieving face images with similar structure and properties. [sent-44, score-0.168]
</p><p>20 We summarize our main contributions as follows: • We propose the GLOC model, a strong model for face labeling otasseks th, eth GatL cOoCmb minoedse tlh, ea CstrRonF gan mdo tdheel R foBrM fa ctoe achieve both local and global consistency. [sent-45, score-0.308]
</p><p>21 f • We achieve significant improvements over the state-oftWhee- aarcth iine vfaec seig labeling accuracy on stus bosveetrs tohef th stae tLe-FoWfdata set. [sent-47, score-0.147]
</p><p>22 Our model also produces qualitatively better labeling than the baseline CRF models. [sent-48, score-0.174]
</p><p>23 • We demonstrate that our model learns face attributes automatically wteith thoautt o oautrtri mbuotdee l alb eealsrn. [sent-49, score-0.195]
</p><p>24 Because of the variety of hair styles, configurations, and amount of hair, the shape of a hair segmentation can be extremely variable. [sent-55, score-0.996]
</p><p>25 In our work, we treat facial hair as part of “hair” in general, hoping to develop hidden units corresponding to beards, sideburns, mustaches, and other hair parts, which further increases the complexity of the hair segments. [sent-56, score-1.569]
</p><p>26 Furthermore, we include skin of the neck as part of the “skin”  segmentation when it is visible, which is different from other labeling regimes. [sent-57, score-0.337]
</p><p>27 [29] limit the skin region to the face and include regions covered by beards, hats, and glasses as being skin, which simplifies their labeling problem. [sent-59, score-0.437]
</p><p>28 [32] build a hair color model and then adopt a region growing algorithm to modify the hair region. [sent-61, score-0.977]
</p><p>29 This method has difficulty when the hair color changes significantly from one region to another, especially for dark hair, and the work was targeted at images with controlled backgrounds. [sent-62, score-0.49]
</p><p>30 [19] used a mixture model to learn six distinct hair styles, and other mixture models to learn color distributions for hair, skin, and background. [sent-64, score-0.487]
</p><p>31 [29] used a compositional exemplar-based model, focusing mostly on the problem of hair segmentation. [sent-73, score-0.46]
</p><p>32 To our knowledge, this  is the best-performing algorithm for hair, skin, and background labeling to date. [sent-78, score-0.171]
</p><p>33 Specifically, they used multiple RBMs at different scales to model the regional or global label fields (layers) separately, and combined those conditional distributions multiplicatively. [sent-85, score-0.167]
</p><p>34 [5] introduced the Shape Boltzmann machine (ShapeBM), a two-layer DBM with local connectivity in the first layer for local consistency and generalization (by weight sharing), and full connectivity in the second layer for modeling global shapes, as a strong model for object shapes. [sent-87, score-0.179]
</p><p>35 Our model is similar at a high-level to these models in that we use RBMs for object shape modeling to solve image labeling problems. [sent-89, score-0.214]
</p><p>36 First, 222000112088  our model has an edge potential that enforces local consistency between adjacent superpixel labels. [sent-91, score-0.278]
</p><p>37 Second, we de-  fine our model on the superpixel graph using a virtual pooling technique, which is computationally much more efficient. [sent-92, score-0.289]
</p><p>38 Finally, we propose a model combined with an RBM to act as a shape prior, which makes the training much easier while showing significant improvement over the baseline models in face labeling tasks. [sent-94, score-0.32]
</p><p>39 We denote V(I) = {1, · · · , S(I)} as a set of superpixel nodes, annotde eE V(I) as a s {e1t ,o·f· edges connecting adjacent superpixels. [sent-102, score-0.194]
</p><p>40 2  Restricted Boltzmann Machines  The restricted Boltzmann machine [28] is a bipartite, undirected graphical model composed of visible and hidden layers. [sent-147, score-0.195]
</p><p>41 In our context, we assume R2 multinomial visible units yr ∈ {0, 1}L and K binary hidden units hk ∈ {0, 1}. [sent-148, score-0.353]
</p><p>42 1  (4)  RR2  where W ∈ ×L×Kis a 3D tensor specifying the conwnehcetiroen W weights between visible and hidden units, bk is the hidden bias, and crl is the visible bias. [sent-175, score-0.472]
</p><p>43 The GLOC Model  To build a strong model for image labeling, both local consistency (adjacent nodes are likely to have similar labels) and global consistency (the overall shape of the object should look realistic) are desirable. [sent-185, score-0.245]
</p><p>44 On the other hand, the RBM is good at capturing global shape structure through the hidden units. [sent-187, score-0.177]
</p><p>45 Specifically, we describe the conditional likelihood of labels set Y given wthee superpixel fee caotnurdeisti oXn as fiokellloiwhoso: Pgloc(Y|X)  ∝  ? [sent-189, score-0.266]
</p><p>46 In other words, we cannot simply connect label (visible) nodes defined over superpixels to hidden nodes as in Equation (4) because 1) the RBM is defined on a fixed number of visible nodes and 2) the number of superpixels  ×  and their underlying graph structure can vary across images. [sent-198, score-0.617]
</p><p>47 1 Virtual Pooling Layer To resolve this issue, we introduce a virtual, fixed-sized pooling layer between the label and the hidden layers, where we map each superpixel label node into the virtual visible nodes of the R R square grid. [sent-201, score-0.675]
</p><p>48 Specifically, we define the energy function between the label nodes and the hidden nodes for an image I follows: as  Erbm(Y,h)  ? [sent-205, score-0.316]
</p><p>49 sS=1  Here, the virtual visible nodes ¯y rl = prsysl are deterministically mapped from the superp? [sent-234, score-0.247]
</p><p>50 ixel label nodes using the projection matrix {prs} that deter? [sent-235, score-0.149]
</p><p>51 nTehse t projection matrix is defined as follows:2  prs=Area(RAergeiao(nR(esg)i ∩on R(erg)i)on(r)), where Region(s) and Region(r) denote sets of pixels cor-  responding to superpixel s and grid r, respectively. [sent-237, score-0.229]
</p><p>52 Due to the deterministic connection, the pooling layer is actually a virtual layer that only exists to map between the superpixel nodes and the hidden nodes. [sent-238, score-0.549]
</p><p>53 We can also view our GLOC model as having a set of grid-structured nodes that performs average pooling over the adjacent superpixel nodes. [sent-239, score-0.348]
</p><p>54 For example, knowing that hair rests on the shoulders makes it less likely to be gray. [sent-246, score-0.46]
</p><p>55 To define the RBM over a fixed-size we use an image-specific “projection matrix” (top-down amndag bottom-up) ipnrofojercmtaiotinon m baettrwixe”e {np the virtual grid of the RBM’s visible layer. [sent-251, score-0.163]
</p><p>56 See  y¯r and the hidden visible node grid, } that transfers the }la tbhealt layer faenrds text for details. [sent-252, score-0.257]
</p><p>57 1  (8)  RN2×D×L  where Γ ∈ is a 3D tensor specifying the conwnehcetiroen Γ weights between the superpixel node features and labels at each spatial location. [sent-274, score-0.293]
</p><p>58 In this energy function, we define a different projection matrix {psn} which specifies tdheef mapping frreonmt pthroej Nect i×o nN m vatirrtiuxal { grid t ow superpixel liea-s btheel n moadpeps. [sent-275, score-0.229]
</p><p>59 3 Inference and Learning Inference Since the joint inference of superpixel labels and the hidden nodes is intractable, we resort to the meanfield approximation. [sent-278, score-0.451]
</p><p>60 mInu practice, lhyo wtoe mvearx, i mt i sz e be tnheef cicoinal3Note that the projection matrices used in the RBM and spatial CRF are different in that {prs } used in the RBM describes a projection from superpixel to grid ( ? [sent-291, score-0.257]
</p><p>61 First, we adapted the pretraining method of deep Boltzmann machines (DBM) [25] to train the conditional RBM (CRBM). [sent-347, score-0.155]
</p><p>62 4 Specifically, we pretrain the model parameters {W, b, C} of the CRBM as if it is a top layer of the DBM {toW Wav,obid, double-counting w ashe ifn cito imsb ai tnoepd awyiethr tohfe t edge potential in the GLOC model. [sent-348, score-0.163]
</p><p>63 Second, the CRBM and the GLOC models can be trained to either maximize the conditional log-likelihood using contrastive divergence (CD) or minimize generalized perceptron loss [18] using CDPercLoss [22]. [sent-349, score-0.154]
</p><p>64 The RBM can generate novel, realistic examples by combining hair, beard and mustache shapes along with diverse face shapes. [sent-364, score-0.172]
</p><p>65 In principle, however, we can also use such deep architectures in our GLOC model as a rich global shape prior without much modification to inference and learning. [sent-377, score-0.169]
</p><p>66 Experiments We evaluated our proposed model on a task to label face images from the LFW data set [14] as hair, skin, and background. [sent-379, score-0.168]
</p><p>67 6 We provide ground truth for a set of 2927  ×  LFW images by labeling each superpixel as either hair, skin, or background [2]. [sent-392, score-0.33]
</p><p>68 While some superpixels may contain pixels from more than one region, most superpixels are generally “pure” hair, skin, or background. [sent-393, score-0.156]
</p><p>69 There are several reasons why we used superpixel labeling instead of pixel labeling for this problem. [sent-394, score-0.453]
</p><p>70 First, the superpixel representation is computationally much more efficient. [sent-395, score-0.159]
</p><p>71 The number of nodes would be too large for pixel labeling since the LFW images are of size 250 250. [sent-396, score-0.233]
</p><p>72 For example, if the superpixel is mostly black but contains a few blue pixels, the blue pixels will be smoothed out from the feature vector, which can simplify inference. [sent-399, score-0.159]
</p><p>73 For each superpixel we used the following node features: •  •  •  Color: Normalized histogram over 64 bins generated by running Km-amliezaends h over pixels vienr rL 6A4B b space. [sent-402, score-0.202]
</p><p>74 Position: Normalized histogram of the proportion of a superpixel trmhaat ifzaeldls hwisitthoignr emach of o thf eth per o8p o×r i8o grid eale smupeenrtpsi on lt thhea itm faalgles. [sent-404, score-0.226]
</p><p>75 We report the superpixel-wise labeling accuracy in the second column, and the error reduction over the CRF in the third column. [sent-415, score-0.147]
</p><p>76 We evaluated the labeling performance of four different models: a standard CRF, the spatial CRF, the CRBM, and our GLOC model. [sent-419, score-0.147]
</p><p>77 As shown in Table 1, the GLOC model substantially improves the superpixel labeling accuracy over the baseline CRF model as well as the spatial CRF and CRBM models. [sent-428, score-0.36]
</p><p>78 As we can see, the global shape prior of the GLOC model helps “clean up” the guess made by the spatial CRF in many cases, resulting in a more confident prediction. [sent-434, score-0.162]
</p><p>79 In many cases, the RBM prior encourages a more realistic segmentation by either “filling in” or removing parts of the hair or face shape. [sent-435, score-0.626]
</p><p>80 For example, the woman in the second row on the left set recovers the left side of her hair and gets a more recognizable hair shape under our model. [sent-436, score-1.011]
</p><p>81 Also, the man in the first row on the right set gets a more realistic looking hair shape by removing the small (incor-  rect) hair shape on top of his head. [sent-437, score-1.053]
</p><p>82 In addition, there were cases (such as the woman in the fifth row of the left set) where an additional face in close proximity to the centered face may confuse the model. [sent-439, score-0.235]
</p><p>83 As we can see, the model made significant errors in their hair regions. [sent-443, score-0.487]
</p><p>84 Specifically, in the first row, the hair of a nearby face is similar in color to the hair of the foreground face as well as the background, and our model incorrectly guesses more hair by emphasizing the hair shape prior, perhaps too strongly. [sent-444, score-2.147]
</p><p>85 We first generated the superpixels and features for each image, then ran our GLOC model to get label guesses for each superpixel, and finally mapped back to pixels for eval-  cally learned by the GLOC model. [sent-455, score-0.168]
</p><p>86 The attributes from left to right can be interpreted as “no hair showing”, “looking left”, “looking right”, “beard/occluded chin”, “big hair”. [sent-457, score-0.546]
</p><p>87 We noticed that even with a perfect superpixel labeling, this mapping already incurs approximately 3% labeling error. [sent-460, score-0.306]
</p><p>88 The ground truth for a superpixel is a normalized histogram of the pixel labels in the superpixel. [sent-466, score-0.189]
</p><p>89 Attributes and Retrieval While the labeling accuracy (as shown in Section 4) is a direct way of measuring progress, we have an additional goal in our work: to build models that capture the natural statistical structure in faces. [sent-468, score-0.147]
</p><p>90 It is not an accident that human languages have words for beards, baldness, and other salient high-level attributes of human face appearance. [sent-469, score-0.168]
</p><p>91 In many cases, the retrieved results for the hidden units form meaningful clusters. [sent-479, score-0.212]
</p><p>92 Thus, the learned hidden units may be useful as attribute representations for faces. [sent-481, score-0.189]
</p><p>93 Conclusion Face segmentation and labeling is challenging due to the diversity of hair styles, head poses, clothing, occlusions, and other phenomena that are difficult to model, especially in a database like LFW. [sent-483, score-0.643]
</p><p>94 Our GLOC model combines the CRF and the RBM to model both local and global structure in face segmentations. [sent-484, score-0.188]
</p><p>95 Our model has consistently reduced the error in face labeling over previous models which lack global shape priors. [sent-485, score-0.348]
</p><p>96 In addition, we have shown that the hidden units in our model can be interpreted as face attributes, which were learned without any attribute-level supervision. [sent-486, score-0.346]
</p><p>97 Learning hierarchical representations for face verification with convolutional deep belief networks. [sent-558, score-0.168]
</p><p>98 Conditional random fields: Probabilistic models for segmenting and labeling sequence data. [sent-597, score-0.147]
</p><p>99 Markov random field models for hair and face segmentation. [sent-633, score-0.566]
</p><p>100 Joint adaptive colour modelling and skin, hair and clothing segmentation using coherent probabilistic index maps. [sent-683, score-0.525]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gloc', 0.493), ('hair', 0.46), ('rbm', 0.302), ('crf', 0.223), ('superpixel', 0.159), ('skin', 0.154), ('labeling', 0.147), ('lfw', 0.12), ('boltzmann', 0.116), ('crbm', 0.113), ('prs', 0.113), ('rbms', 0.113), ('hidden', 0.109), ('face', 0.106), ('erbm', 0.091), ('nodes', 0.086), ('units', 0.08), ('superpixels', 0.078), ('conditional', 0.077), ('eslami', 0.075), ('enode', 0.073), ('shapebm', 0.073), ('psn', 0.071), ('pretrain', 0.065), ('attributes', 0.062), ('virtual', 0.062), ('dbm', 0.06), ('visible', 0.059), ('beards', 0.055), ('ecrf', 0.055), ('wrlk', 0.055), ('ysl', 0.055), ('layer', 0.046), ('xv', 0.046), ('crl', 0.045), ('node', 0.043), ('beard', 0.042), ('grid', 0.042), ('pooling', 0.041), ('guess', 0.041), ('contrastive', 0.04), ('rl', 0.04), ('ys', 0.04), ('shape', 0.04), ('deep', 0.039), ('machines', 0.039), ('maximize', 0.037), ('bkhk', 0.036), ('cdpercloss', 0.036), ('conwnehcetiroen', 0.036), ('eedge', 0.036), ('egloc', 0.036), ('fnsolde', 0.036), ('fsrblm', 0.036), ('scheffler', 0.036), ('sprs', 0.036), ('stl', 0.036), ('huang', 0.036), ('segmentation', 0.036), ('inference', 0.035), ('adjacent', 0.035), ('label', 0.035), ('styles', 0.034), ('crfs', 0.033), ('dbms', 0.032), ('meanfield', 0.032), ('dn', 0.032), ('consistency', 0.032), ('labels', 0.03), ('bk', 0.03), ('yacoob', 0.03), ('sigmoid', 0.03), ('region', 0.03), ('faces', 0.029), ('looking', 0.029), ('clothing', 0.029), ('projection', 0.028), ('guesses', 0.028), ('recognizable', 0.028), ('global', 0.028), ('model', 0.027), ('mnih', 0.027), ('zemel', 0.027), ('generative', 0.027), ('confident', 0.026), ('amherst', 0.026), ('tensor', 0.025), ('hk', 0.025), ('thf', 0.025), ('chin', 0.025), ('edge', 0.025), ('wild', 0.024), ('background', 0.024), ('vis', 0.024), ('realistic', 0.024), ('interpreted', 0.024), ('deeper', 0.024), ('woman', 0.023), ('belief', 0.023), ('retrieved', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000015 <a title="50-tfidf-1" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>2 0.312565 <a title="50-tfidf-2" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>3 0.29518956 <a title="50-tfidf-3" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>Author: Yujia Li, Daniel Tarlow, Richard Zemel</p><p>Abstract: When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs); we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss;and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.</p><p>4 0.28839377 <a title="50-tfidf-4" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>Author: Roni Mittelman, Honglak Lee, Benjamin Kuipers, Silvio Savarese</p><p>Abstract: The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. Attributes provide an intermediate feature representation in between low-level features and the class categories, leading to improved learning on novel categories from few examples. However, a major caveat is that learning semantic attributes is a laborious task, requiring a significant amount of time and human intervention to provide labels. In order to address this issue, we propose a weakly supervised approach to learn mid-level features, where only class-level supervision is provided during training. We develop a novel extension of the restricted Boltzmann machine (RBM) by incorporating a Beta-Bernoulli process factor potential for hidden units. Unlike the standard RBM, our model uses the class labels to promote category-dependent sharing of learned features, which tends to improve the generalization performance. By using semantic attributes for which annotations are available, we show that we can find correspondences between the learned mid-level features and the labeled attributes. Therefore, the mid-level features have distinct semantic characterization which is similar to that given by the semantic attributes, even though their labeling was not provided during training. Our experimental results on object recognition tasks show significant performance gains, outperforming existing methods which rely on manually labeled semantic attributes.</p><p>5 0.2703653 <a title="50-tfidf-5" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>Author: Siddhartha Chandra, Shailesh Kumar, C.V. Jawahar</p><p>Abstract: Understanding the nature of data is the key to building good representations. In domains such as natural images, the data comes from very complex distributions which are hard to capture. Feature learning intends to discover or best approximate these underlying distributions and use their knowledge to weed out irrelevant information, preserving most of the relevant information. Feature learning can thus be seen as a form of dimensionality reduction. In this paper, we describe a feature learning scheme for natural images. We hypothesize that image patches do not all come from the same distribution, they lie in multiple nonlinear subspaces. We propose a framework that uses K Restricted Boltzmann Machines (K-RBMS) to learn multiple non-linear subspaces in the raw image space. Projections of the image patches into these subspaces gives us features, which we use to build image representations. Our algorithm solves the coupled problem of finding the right non-linear subspaces in the input space and associating image patches with those subspaces in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that representations based on our framework outperform the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks.</p><p>6 0.2235458 <a title="50-tfidf-6" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>7 0.1403586 <a title="50-tfidf-7" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>8 0.13907923 <a title="50-tfidf-8" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>9 0.13631859 <a title="50-tfidf-9" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>10 0.11417969 <a title="50-tfidf-10" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>11 0.109005 <a title="50-tfidf-11" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>12 0.10477659 <a title="50-tfidf-12" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>13 0.10304252 <a title="50-tfidf-13" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>14 0.10017315 <a title="50-tfidf-14" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>15 0.095993772 <a title="50-tfidf-15" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>16 0.094014317 <a title="50-tfidf-16" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>17 0.093708456 <a title="50-tfidf-17" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>18 0.091704942 <a title="50-tfidf-18" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>19 0.091215573 <a title="50-tfidf-19" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>20 0.090984941 <a title="50-tfidf-20" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, -0.045), (2, 0.001), (3, -0.001), (4, 0.144), (5, 0.039), (6, -0.027), (7, 0.053), (8, 0.082), (9, -0.072), (10, 0.178), (11, -0.039), (12, -0.016), (13, 0.104), (14, -0.01), (15, 0.256), (16, -0.043), (17, 0.059), (18, 0.063), (19, 0.109), (20, 0.118), (21, -0.248), (22, -0.011), (23, -0.077), (24, -0.109), (25, -0.106), (26, 0.083), (27, -0.032), (28, 0.021), (29, 0.053), (30, 0.041), (31, 0.057), (32, -0.015), (33, 0.092), (34, 0.192), (35, -0.053), (36, -0.021), (37, -0.065), (38, 0.0), (39, -0.006), (40, 0.159), (41, -0.018), (42, 0.03), (43, -0.117), (44, 0.056), (45, -0.111), (46, 0.04), (47, 0.005), (48, -0.071), (49, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90854412 <a title="50-lsi-1" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>2 0.6944229 <a title="50-lsi-2" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>Author: Siddhartha Chandra, Shailesh Kumar, C.V. Jawahar</p><p>Abstract: Understanding the nature of data is the key to building good representations. In domains such as natural images, the data comes from very complex distributions which are hard to capture. Feature learning intends to discover or best approximate these underlying distributions and use their knowledge to weed out irrelevant information, preserving most of the relevant information. Feature learning can thus be seen as a form of dimensionality reduction. In this paper, we describe a feature learning scheme for natural images. We hypothesize that image patches do not all come from the same distribution, they lie in multiple nonlinear subspaces. We propose a framework that uses K Restricted Boltzmann Machines (K-RBMS) to learn multiple non-linear subspaces in the raw image space. Projections of the image patches into these subspaces gives us features, which we use to build image representations. Our algorithm solves the coupled problem of finding the right non-linear subspaces in the input space and associating image patches with those subspaces in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that representations based on our framework outperform the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks.</p><p>3 0.68427759 <a title="50-lsi-3" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>Author: Yujia Li, Daniel Tarlow, Richard Zemel</p><p>Abstract: When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs); we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss;and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.</p><p>4 0.68219513 <a title="50-lsi-4" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>Author: Roni Mittelman, Honglak Lee, Benjamin Kuipers, Silvio Savarese</p><p>Abstract: The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. Attributes provide an intermediate feature representation in between low-level features and the class categories, leading to improved learning on novel categories from few examples. However, a major caveat is that learning semantic attributes is a laborious task, requiring a significant amount of time and human intervention to provide labels. In order to address this issue, we propose a weakly supervised approach to learn mid-level features, where only class-level supervision is provided during training. We develop a novel extension of the restricted Boltzmann machine (RBM) by incorporating a Beta-Bernoulli process factor potential for hidden units. Unlike the standard RBM, our model uses the class labels to promote category-dependent sharing of learned features, which tends to improve the generalization performance. By using semantic attributes for which annotations are available, we show that we can find correspondences between the learned mid-level features and the labeled attributes. Therefore, the mid-level features have distinct semantic characterization which is similar to that given by the semantic attributes, even though their labeling was not provided during training. Our experimental results on object recognition tasks show significant performance gains, outperforming existing methods which rely on manually labeled semantic attributes.</p><p>5 0.61813629 <a title="50-lsi-5" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>6 0.51900661 <a title="50-lsi-6" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>7 0.45612627 <a title="50-lsi-7" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>8 0.38642871 <a title="50-lsi-8" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>9 0.38423502 <a title="50-lsi-9" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>10 0.36610514 <a title="50-lsi-10" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>11 0.36580533 <a title="50-lsi-11" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>12 0.35531008 <a title="50-lsi-12" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>13 0.3428196 <a title="50-lsi-13" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>14 0.33774 <a title="50-lsi-14" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>15 0.32700464 <a title="50-lsi-15" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>16 0.32296804 <a title="50-lsi-16" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>17 0.3211019 <a title="50-lsi-17" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>18 0.31868064 <a title="50-lsi-18" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>19 0.31455797 <a title="50-lsi-19" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>20 0.3143447 <a title="50-lsi-20" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.148), (16, 0.015), (19, 0.014), (26, 0.046), (33, 0.263), (39, 0.012), (67, 0.058), (69, 0.033), (80, 0.013), (86, 0.214), (87, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87891299 <a title="50-lda-1" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>Author: Jingchen Liu, Peter Carr, Robert T. Collins, Yanxi Liu</p><p>Abstract: We employ hierarchical data association to track players in team sports. Player movements are often complex and highly correlated with both nearby and distant players. A single model would require many degrees of freedom to represent the full motion diversity and could be difficult to use in practice. Instead, we introduce a set of Game Context Features extracted from noisy detections to describe the current state of the match, such as how the players are spatially distributed. Our assumption is that players react to the current situation in only a finite number of ways. As a result, we are able to select an appropriate simplified affinity model for each player and time instant using a random decisionforest based on current track and game contextfeatures. Our context-conditioned motion models implicitly incorporate complex inter-object correlations while remaining tractable. We demonstrate significant performance improvements over existing multi-target tracking algorithms on basketball and field hockey sequences several minutes in duration and containing 10 and 20 players respectively.</p><p>same-paper 2 0.86129564 <a title="50-lda-2" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>3 0.85770488 <a title="50-lda-3" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>Author: David Pfeiffer, Stefan Gehrig, Nicolai Schneider</p><p>Abstract: Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste- riori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level.</p><p>4 0.85548377 <a title="50-lda-4" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>5 0.83773839 <a title="50-lda-5" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>Author: Guang Chen, Yuanyuan Ding, Jing Xiao, Tony X. Han</p><p>Abstract: Context has been playing an increasingly important role to improve the object detection performance. In this paper we propose an effective representation, Multi-Order Contextual co-Occurrence (MOCO), to implicitly model the high level context using solely detection responses from a baseline object detector. The so-called (1st-order) context feature is computed as a set of randomized binary comparisons on the response map of the baseline object detector. The statistics of the 1st-order binary context features are further calculated to construct a high order co-occurrence descriptor. Combining the MOCO feature with the original image feature, we can evolve the baseline object detector to a stronger context aware detector. With the updated detector, we can continue the evolution till the contextual improvements saturate. Using the successful deformable-partmodel detector [13] as the baseline detector, we test the proposed MOCO evolution framework on the PASCAL VOC 2007 dataset [8] and Caltech pedestrian dataset [7]: The proposed MOCO detector outperforms all known state-ofthe-art approaches, contextually boosting deformable part models (ver.5) [13] by 3.3% in mean average precision on the PASCAL 2007 dataset. For the Caltech pedestrian dataset, our method further reduces the log-average miss rate from 48% to 46% and the miss rate at 1 FPPI from 25% to 23%, compared with the best prior art [6].</p><p>6 0.82493526 <a title="50-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.82482153 <a title="50-lda-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.8226456 <a title="50-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.82259381 <a title="50-lda-9" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>10 0.82220286 <a title="50-lda-10" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>11 0.82077825 <a title="50-lda-11" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>12 0.82048935 <a title="50-lda-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.81807727 <a title="50-lda-13" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>14 0.81770879 <a title="50-lda-14" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>15 0.81754565 <a title="50-lda-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.81717581 <a title="50-lda-16" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>17 0.81706858 <a title="50-lda-17" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>18 0.81704432 <a title="50-lda-18" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>19 0.81667703 <a title="50-lda-19" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>20 0.81643188 <a title="50-lda-20" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
