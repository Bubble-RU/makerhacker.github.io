<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-305" href="#">cvpr2013-305</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</h1>
<br/><p>Source: <a title="cvpr-2013-305-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Liao_Non-parametric_Filtering_for_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>Reference: <a title="cvpr-2013-305-reference" href="../cvpr2013_reference/cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We decompose an image (a) into three components: (b) albedo, (c) coarse-scale shading, and (d) shading detail. [sent-5, score-0.703]
</p><p>2 The albedo and coarse-scale shading represent surface color and directional lighting effect. [sent-6, score-1.257]
</p><p>3 The shading detail image captures fine-scale surface geometry,  or material property. [sent-7, score-1.205]
</p><p>4 Abstract Geometric detail is a universal phenomenon in real world objects. [sent-8, score-0.327]
</p><p>5 In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. [sent-10, score-0.506]
</p><p>6 We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. [sent-11, score-0.539]
</p><p>7 n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification. [sent-13, score-0.778]
</p><p>8 Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer. [sent-14, score-0.748]
</p><p>9 Introduction It is usual to decompose the brightness I(x, y) of an image pixel into its albedo (the percentage of that light reflected to the camera, A(x, y)), and its irradiance (the amount of light collected by the surface viewed by the pixel, S(x, y), sometimes called shading). [sent-16, score-0.637]
</p><p>10 However, this decomposition fails to model geometric detail properly. [sent-19, score-0.468]
</p><p>11 Geometric detail refers to fast changing patterns of surface geometry. [sent-20, score-0.336]
</p><p>12 Geometric detail occurs nearly universally on real world objects, and is an important component of object modelling. [sent-23, score-0.327]
</p><p>13 Geometric detail does not fit naturally into an albedo-irradiance decomposition, because it can generate large, localized gradients (like albedo) but is essentially a shading effect. [sent-24, score-1.004]
</p><p>14 Because geometric detail has a drastically different local spatial pattern than albedo or shading, we use a non-parametric patch-based model learned from albedo to remove geometric detail error in albedo; similarly for separating geometric detail in shading. [sent-25, score-2.264]
</p><p>15 Based on the patch-based filtering, we decompose irradiance into coarse-scale shading and shading detail. [sent-28, score-1.449]
</p><p>16 Coarsescale shading is a relatively smooth signal which represents the average irradiance over a patch of surface. [sent-29, score-0.786]
</p><p>17 Shading detail is a fine scale detail, emphasizing effects caused by local surface geometry like bumps and grooves. [sent-30, score-0.434]
</p><p>18 Figure 1 shows our decomposition; notice that the shading detail represents geometric effects like the creases and fibers in the paper, 999996666633111  whereas the shading represents the irradiance at a longer  spatial scale. [sent-31, score-1.855]
</p><p>19 Dropping pixel coordinates, we have I A(Sc + Sd) =  (1)  where A is albedo, Sc is coarse-scale shading, and Sd is shading detail. [sent-32, score-0.672]
</p><p>20 Contributions: (1) We propose an effective method of separating geometric detail from image. [sent-34, score-0.428]
</p><p>21 Our decomposition results in improved albedo estimation because it does not force the albedo signal to account for all fast local effects (section 3. [sent-39, score-1.074]
</p><p>22 The shading detail image permits attractive image editing applications, where the apparent material of an object in an image can be smoothed, enhanced or transferred (section 3. [sent-41, score-1.279]
</p><p>23 Finally, the shading detail signal is a powerful cue to material identity, as we demonstrate in classification experiments (section 3. [sent-43, score-1.181]
</p><p>24 Related work Intrinsic image decomposition: Land’s influential Retinex model assumes effective albedo displays sharp, localized changes (which result in large image gradients), and that shading has small gradients [19]; important variants include [16, 3, 6]. [sent-47, score-1.2]
</p><p>25 Sharp shading changes occur at shadow  boundaries or normal discontinuities, but using chromaticity [13] or multiple images under different lighting conditions [30] yields improved estimates. [sent-48, score-0.694]
</p><p>26 Discriminative methods to classify edges into albedo or shading help [27]; chromaticity cues can contribute [10], as can user input [5]. [sent-49, score-1.202]
</p><p>27 Image-based editing: White and Forsyth [3 1] show that, for screenprinted surfaces, one can separate shading and albedo sufficiently accurately to composite the shading together with a new albedo map. [sent-56, score-2.36]
</p><p>28 In our work, the separation of shading into coarse-scale shading and shading detail provides well-behaved editing primitives: The coarsescale shading captures gross shading; and the detail shading  carries only fine-scale material details. [sent-59, score-4.34]
</p><p>29 We exploit the image analogy technique [15] to synthesize and transfer shading details from one object to another. [sent-60, score-0.737]
</p><p>30 Transferring detail from one image to another produces visually realistic results, even though local shading inconsistencies are inevitable; but as [24] shows, human observers are unable to spot inconsistency in fine-scale shading, as long as gross shading is correct. [sent-61, score-1.692]
</p><p>31 It allows us to remove geometric detail from albedo, shading or im-  age, and extract the shading detail image. [sent-76, score-2.066]
</p><p>32 The operations on shading and albedo lead to our new image decomposition. [sent-77, score-1.18]
</p><p>33 We correct using our patch-based filter (section 2); the result is the albedo field A. [sent-82, score-0.553]
</p><p>34 Furthermore, we compute the shading field Sc from the irradiance field using a similar filter learned from shading (again, section 2). [sent-83, score-1.487]
</p><p>35 Finally, we compute the shading detail field by Sd = − Sc, (Fig. [sent-84, score-1.008]
</p><p>36 Note there is missing shading details in theS al −be Sdo correction step. [sent-86, score-0.696]
</p><p>37 Recovering it for Sd requires division by A (as Equation 1 suggests), which causes artifact around albedo edges. [sent-87, score-0.508]
</p><p>38 So we extract an approximated Sd only from the shading field instead. [sent-88, score-0.696]
</p><p>39 Patch-based filtering  Aˆ  Assume we have obtained a preliminary albedo field from existing methods. [sent-91, score-0.588]
</p><p>40 Typical errors in albedo fields from modern methods look like the shading on local geometric detail; this appears in albedo maps  because shading priors strongly discourage large gradients. [sent-93, score-2.474]
</p><p>41 We expect that the exact albedo fields are properly described using a patch dictionary. [sent-95, score-0.548]
</p><p>42 Typical albedo patches will have large constant domains, while geometric detail has a wildly different spatial pattern (see an illustration in Fig. [sent-96, score-0.951]
</p><p>43 So the error of geometric detail in albedo estimates will be poorly encoded by an albedo dictionary. [sent-98, score-1.426]
</p><p>44 We apply the same method to learn the albedo and shading dictionaries using the (clean) ground truth albedo and shading images of the MIT intrinsic image dataset [14], respectively. [sent-120, score-2.478]
</p><p>45 Figure 3 also shows exemplar entries of a learned geometric detail dictionary, for which we use a set of typical bump images collected online. [sent-121, score-0.494]
</p><p>46 DecomposingintialshadingSˆ(Left)intocoarse-scale shading Sc (Middle) and shading detail image Sd (Right). [sent-123, score-1.656]
</p><p>47 Visualization of patches learned from albedo (left), shading (center) and geometric bumps maps. [sent-125, score-1.364]
</p><p>48 Notice how they differ from each other: albedo patches have large pieces of constant value with sharp boundaries; shading patches have smooth intensity change; and the geometric detail patches have more complex fast changing patterns. [sent-126, score-1.705]
</p><p>49 Applications We show four applications of our decomposition: geometric detail removal, albedo estimation, material editing, and material recognition. [sent-128, score-1.312]
</p><p>50 Geometric detail removal  Geometric detail in images can operate as a form ofnoise that impedes matching and tracking. [sent-131, score-0.663]
</p><p>51 For example, geometric detail on clothing can create real difficulty scoring the match between image regions in a tracker (e. [sent-132, score-0.41]
</p><p>52 It is beneficial to be able to remove geometric detail from images. [sent-137, score-0.41]
</p><p>53 The output image is a geometry-detail free image, while albedo and shading are well preserved. [sent-140, score-1.18]
</p><p>54 Figure 4 shows a few examples of our geometric detail removal operation. [sent-141, score-0.449]
</p><p>55 Top row: images with geometric detail; Bottom row: detail removal results. [sent-147, score-0.449]
</p><p>56 Notice how effectively the various types of geometric detail are removed,  while the shading and albedo edges are preserved. [sent-151, score-1.59]
</p><p>57 Albedo estimation Geometric detail causes problems for albedo estimation algorithms. [sent-155, score-0.82]
</p><p>58 Barron and Malik show strong albedo reconstruction results on the MIT intrinsic image dataset [1] for two illumination conditions: laboratory illumination, and “natural” illumination. [sent-156, score-0.695]
</p><p>59 Table 1 of [1] shows an improvement of near an order of magnitude in albedo recovery MSE for “natural” illumination over laboratory illumination. [sent-159, score-0.614]
</p><p>60 Notice how the albedo field contains high frequency shading terms caused  by geometric detail. [sent-162, score-1.317]
</p><p>61 We evaluate our refined albedo estimates on the MIT intrinsic image dataset quantitatively and show significant improvement. [sent-164, score-0.604]
</p><p>62 We compare our result (obtained by refining the Barron and Malik albedo estimate with our patch-based filtering) with two baseline methods: color Retinex [14] and  suppressing vestiges of albedo detail. [sent-165, score-1.039]
</p><p>63 (b): Our refined albedo by our patch-based filtering; Notice the geometric detail is  suppressed. [sent-167, score-0.918]
</p><p>64 We split the MIT intrinsic image dataset into two sets, learn the dictionary using the groundtruth albedo of one set (10 images) for our filter, and use the other set for evaluation. [sent-169, score-0.667]
</p><p>65 Color channel correction is always applied to the reconstruction outputs to preserve albedo edges. [sent-171, score-0.532]
</p><p>66 Our albedo estimates have slight improvements over Barron and Malik in MSE, but MSE is not an accurate metric for albedo error. [sent-173, score-1.016]
</p><p>67 ferred albedo may be insignificant in practice, but produce large MSE values. [sent-189, score-0.508]
</p><p>68 Material editing Human eyes are insensitive to fine-scale shading inconsistencies as long as long-scale directional shading is correct, according to [24]. [sent-196, score-1.492]
</p><p>69 This allows us to apply image-based editing on the shading detail map (Equation 4) without creating noticeable visual artifacts. [sent-197, score-1.082]
</p><p>70 To perform image-based detail editing we do not need to explicitly compute the surface geometry and material; nor do we need to re-render the object using a physically based renderer which is difficult and computationally expensive. [sent-198, score-0.449]
</p><p>71 The shading detail map mostly represents the material property; by editing on it we modify object material appearance while preserving the original color and directional shading. [sent-199, score-1.529]
</p><p>72 Detail smooth/enhancement is achieved simply by adding a coefficient to the shading detail map Sd in the decomposition and adjusting it: I? [sent-200, score-1.042]
</p><p>73 When k = 0, we get a geometric detail free image. [sent-203, score-0.41]
</p><p>74 To demonstrate its convenience, we developed an easy-to-use interface which loads the three components of an image, and allows users to adjust the scale of shading detail magnitude interactively with a slider bar. [sent-209, score-1.02]
</p><p>75 Material synthesis and transfer We can also apply the shading detail image ofone object to another by texture synthesis. [sent-210, score-1.056]
</p><p>76 [15] to synthesize a detail image from a source detail. [sent-214, score-0.348]
</p><p>77 In our shading detail synthesis, we use the shading and shading detail image of the source object as the exemplar and exemplar filtered image pair. [sent-217, score-2.779]
</p><p>78 The synthesized image will inherit patterns from the source shading detail image, yet adapt to the local appearance of the target object’s shading. [sent-219, score-1.031]
</p><p>79 After we obtained the synthesized shading detail, we use Equation 4 and adjust the weight k by our interface to reach a desired material transfer effect. [sent-220, score-0.952]
</p><p>80 Material recognition The shading detail map is decoupled from image albedo  and directional shading, amplifying the fine structure of object material. [sent-223, score-1.56]
</p><p>81 But it includes object scale material examples (each class has 50/100 images at object scale) which incorporate object context but lose fine-scale material detail. [sent-227, score-0.394]
</p><p>82 Noticehowef ctivelywechangethep rception fsurfaceg ometry hrough  image-based editing using the shading detail. [sent-233, score-0.77]
</p><p>83 Original  image  Material transfer  Synthesized shading detail  appearance. [sent-234, score-1.003]
</p><p>84 Right:  Synthesized  shading detail maps for composition (with detail source image in the corner). [sent-235, score-1.315]
</p><p>85 The detail images are  translated and normalized to 0-1 for display. [sent-236, score-0.312]
</p><p>86 We use a variant of Barron and Malik [1] in shading estimation for the shading detail extraction. [sent-240, score-1.656]
</p><p>87 Barron and Malik [1] has a strong shading smoothness prior (parametrized in shape and illumination) that pushes geometric shading detail into albedo, which is lost in shading detail extraction. [sent-241, score-2.738]
</p><p>88 Instead, we use a weaker shading prior:  =  fs(S) ? [sent-242, score-0.672]
</p><p>89 9 for examples of the shading detail maps we get at the end of the pipeline. [sent-248, score-0.984]
</p><p>90 Our shading detail, when combined with image, outperforms the other two detail images. [sent-254, score-0.984]
</p><p>91 The image is more discriminative than detail images alone, confirming that albedo is correlated to material. [sent-258, score-0.836]
</p><p>92 Supplemental detail images help in material classification, because they capture complementary material information. [sent-260, score-0.706]
</p><p>93 Our shading detail performs better than other detail images in combination with the color image. [sent-262, score-1.319]
</p><p>94 This suggests our shading detail best captures decoupled material features from the image. [sent-263, score-1.204]
</p><p>95 Although the micro-texture (Bilateral filtered detail map) from [21] has higher accuracy on its own, it provides little gain in accuracy when combined with image features,  meaning that it carries mostly redundant material information. [sent-264, score-0.566]
</p><p>96 Classification confusion matrix on our 18-class material dataset by image feature + our shading detail feature. [sent-266, score-1.181]
</p><p>97 Micro-texture is the micro-texture image from[21], “sd bl” is and sd is our shading detail. [sent-273, score-0.73]
</p><p>98 Conclusions  and Future work  In this paper, we separate geometric detail from intrinsic image components by a non-parametric patch-based filter, and propose a new image decomposition. [sent-280, score-0.506]
</p><p>99 Our method leads to quantitative improvement in albedo recovery and material discrimination. [sent-281, score-0.72]
</p><p>100 999996666699777  We believe separating geometric detail from intrinsic image components has high application potential, especially in the direction of material representation. [sent-285, score-0.721]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shading', 0.672), ('albedo', 0.508), ('detail', 0.312), ('material', 0.197), ('geometric', 0.098), ('editing', 0.098), ('intrinsic', 0.096), ('barron', 0.084), ('irradiance', 0.074), ('mse', 0.072), ('dictionary', 0.063), ('retinex', 0.059), ('sd', 0.058), ('decomposition', 0.058), ('illumination', 0.056), ('malik', 0.055), ('bumps', 0.053), ('fmd', 0.048), ('varma', 0.048), ('bump', 0.043), ('exemplar', 0.041), ('patch', 0.04), ('removal', 0.039), ('filtered', 0.038), ('sc', 0.038), ('lightness', 0.037), ('laboratory', 0.035), ('filtering', 0.035), ('tappen', 0.034), ('patches', 0.033), ('curet', 0.032), ('decompose', 0.031), ('directional', 0.03), ('mit', 0.03), ('analogy', 0.029), ('sharan', 0.028), ('synthesized', 0.028), ('notice', 0.027), ('phow', 0.026), ('coarsescale', 0.026), ('enhancement', 0.026), ('field', 0.024), ('surface', 0.024), ('correction', 0.024), ('recovering', 0.023), ('rk', 0.023), ('bilateral', 0.023), ('color', 0.023), ('decoupled', 0.023), ('zisserman', 0.023), ('fig', 0.022), ('chromaticity', 0.022), ('dictionaries', 0.022), ('preliminary', 0.021), ('filter', 0.021), ('pages', 0.021), ('synthesis', 0.02), ('illinois', 0.02), ('gradients', 0.02), ('adjust', 0.02), ('inconsistencies', 0.02), ('signals', 0.019), ('carries', 0.019), ('transfer', 0.019), ('siggraph', 0.019), ('texture', 0.019), ('source', 0.019), ('adelson', 0.019), ('categorical', 0.019), ('separating', 0.018), ('channels', 0.018), ('demanding', 0.018), ('forsyth', 0.018), ('land', 0.018), ('graphics', 0.017), ('conference', 0.017), ('synthesize', 0.017), ('bs', 0.017), ('khan', 0.016), ('gross', 0.016), ('discriminative', 0.016), ('strongly', 0.016), ('sharp', 0.016), ('interface', 0.016), ('transferring', 0.016), ('hertzmann', 0.016), ('award', 0.015), ('world', 0.015), ('mairal', 0.015), ('geometry', 0.015), ('volume', 0.015), ('recovery', 0.015), ('fine', 0.015), ('caused', 0.015), ('tends', 0.014), ('liu', 0.014), ('hearv', 0.014), ('mixes', 0.014), ('elm', 0.014), ('ofone', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="305-tfidf-1" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>2 0.31362039 <a title="305-tfidf-2" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>Author: Chen Li, Shuochen Su, Yasuyuki Matsushita, Kun Zhou, Stephen Lin</p><p>Abstract: We present a method that enhances the performance of depth-from-defocus (DFD) through the use of shading information. DFD suffers from important limitations namely coarse shape reconstruction and poor accuracy on textureless surfaces that can be overcome with the help of shading. We integrate both forms of data within a Bayesian framework that capitalizes on their relative strengths. Shading data, however, is challenging to recover accurately from surfaces that contain texture. To address this issue, we propose an iterative technique that utilizes depth information to improve shading estimation, which in turn is used to elevate depth estimation in the presence of textures. With this approach, we demonstrate improvements over existing DFD techniques, as well as effective shape reconstruction of textureless surfaces. – –</p><p>3 0.31092155 <a title="305-tfidf-3" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>4 0.26435864 <a title="305-tfidf-4" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>5 0.24128385 <a title="305-tfidf-5" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>6 0.23361938 <a title="305-tfidf-6" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>7 0.16138205 <a title="305-tfidf-7" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>8 0.12018382 <a title="305-tfidf-8" href="./cvpr-2013-Whitened_Expectation_Propagation%3A_Non-Lambertian_Shape_from_Shading_and_Shadow.html">466 cvpr-2013-Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow</a></p>
<p>9 0.11724962 <a title="305-tfidf-9" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>10 0.077010952 <a title="305-tfidf-10" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>11 0.076504722 <a title="305-tfidf-11" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>12 0.071317255 <a title="305-tfidf-12" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>13 0.063262329 <a title="305-tfidf-13" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>14 0.062348135 <a title="305-tfidf-14" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>15 0.061431684 <a title="305-tfidf-15" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>16 0.060594626 <a title="305-tfidf-16" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>17 0.059772372 <a title="305-tfidf-17" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>18 0.059498474 <a title="305-tfidf-18" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>19 0.052720267 <a title="305-tfidf-19" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>20 0.052227937 <a title="305-tfidf-20" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.099), (2, -0.033), (3, 0.104), (4, -0.022), (5, -0.085), (6, -0.064), (7, 0.107), (8, 0.032), (9, -0.011), (10, -0.067), (11, -0.155), (12, -0.062), (13, 0.033), (14, 0.096), (15, 0.04), (16, 0.018), (17, -0.092), (18, -0.02), (19, -0.084), (20, 0.021), (21, 0.032), (22, -0.014), (23, 0.003), (24, 0.095), (25, 0.019), (26, 0.06), (27, -0.081), (28, -0.026), (29, 0.064), (30, 0.165), (31, -0.203), (32, -0.011), (33, 0.219), (34, -0.024), (35, 0.116), (36, -0.105), (37, -0.044), (38, -0.082), (39, 0.012), (40, -0.201), (41, -0.022), (42, -0.011), (43, -0.192), (44, 0.189), (45, 0.039), (46, 0.101), (47, -0.026), (48, -0.024), (49, -0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96694601 <a title="305-lsi-1" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>2 0.71816838 <a title="305-lsi-2" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>3 0.6349321 <a title="305-lsi-3" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>Author: Chen Li, Shuochen Su, Yasuyuki Matsushita, Kun Zhou, Stephen Lin</p><p>Abstract: We present a method that enhances the performance of depth-from-defocus (DFD) through the use of shading information. DFD suffers from important limitations namely coarse shape reconstruction and poor accuracy on textureless surfaces that can be overcome with the help of shading. We integrate both forms of data within a Bayesian framework that capitalizes on their relative strengths. Shading data, however, is challenging to recover accurately from surfaces that contain texture. To address this issue, we propose an iterative technique that utilizes depth information to improve shading estimation, which in turn is used to elevate depth estimation in the presence of textures. With this approach, we demonstrate improvements over existing DFD techniques, as well as effective shape reconstruction of textureless surfaces. – –</p><p>4 0.61467874 <a title="305-lsi-4" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>5 0.54714423 <a title="305-lsi-5" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>6 0.53519517 <a title="305-lsi-6" href="./cvpr-2013-Whitened_Expectation_Propagation%3A_Non-Lambertian_Shape_from_Shading_and_Shadow.html">466 cvpr-2013-Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow</a></p>
<p>7 0.53016001 <a title="305-lsi-7" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>8 0.46950883 <a title="305-lsi-8" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>9 0.43147486 <a title="305-lsi-9" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>10 0.4155356 <a title="305-lsi-10" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>11 0.40168595 <a title="305-lsi-11" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>12 0.3208316 <a title="305-lsi-12" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>13 0.30435801 <a title="305-lsi-13" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>14 0.29312509 <a title="305-lsi-14" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>15 0.28589588 <a title="305-lsi-15" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>16 0.27938801 <a title="305-lsi-16" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>17 0.25208586 <a title="305-lsi-17" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>18 0.24147463 <a title="305-lsi-18" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>19 0.23880969 <a title="305-lsi-19" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>20 0.23557749 <a title="305-lsi-20" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.074), (16, 0.046), (18, 0.157), (26, 0.048), (33, 0.256), (65, 0.016), (66, 0.031), (67, 0.084), (69, 0.047), (80, 0.014), (87, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92276287 <a title="305-lda-1" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>Author: Laurent Sifre, Stéphane Mallat</p><p>Abstract: An affine invariant representation is constructed with a cascade of invariants, which preserves information for classification. A joint translation and rotation invariant representation of image patches is calculated with a scattering transform. It is implemented with a deep convolution network, which computes successive wavelet transforms and modulus non-linearities. Invariants to scaling, shearing and small deformations are calculated with linear operators in the scattering domain. State-of-the-art classification results are obtained over texture databases with uncontrolled viewing conditions.</p><p>2 0.90050846 <a title="305-lda-2" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>Author: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman</p><p>Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependentfeatures computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.</p><p>same-paper 3 0.88937235 <a title="305-lda-3" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>4 0.86890495 <a title="305-lda-4" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>5 0.86721629 <a title="305-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.86402583 <a title="305-lda-6" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>7 0.86285412 <a title="305-lda-7" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>8 0.86143804 <a title="305-lda-8" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>9 0.8611837 <a title="305-lda-9" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>10 0.86071503 <a title="305-lda-10" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>11 0.86032796 <a title="305-lda-11" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>12 0.86031848 <a title="305-lda-12" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>13 0.86021692 <a title="305-lda-13" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>14 0.86010706 <a title="305-lda-14" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>15 0.85975271 <a title="305-lda-15" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>16 0.85877657 <a title="305-lda-16" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>17 0.85862637 <a title="305-lda-17" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>18 0.85822868 <a title="305-lda-18" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>19 0.85809392 <a title="305-lda-19" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>20 0.85771978 <a title="305-lda-20" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
