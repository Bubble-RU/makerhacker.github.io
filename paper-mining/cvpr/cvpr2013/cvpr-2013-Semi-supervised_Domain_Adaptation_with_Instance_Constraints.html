<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-387" href="#">cvpr2013-387</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</h1>
<br/><p>Source: <a title="cvpr-2013-387-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Donahue_Semi-supervised_Domain_Adaptation_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>Reference: <a title="cvpr-2013-387-reference" href="../cvpr2013_reference/cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. [sent-5, score-1.252]
</p><p>2 Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. [sent-6, score-0.976]
</p><p>3 We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video. [sent-11, score-0.959]
</p><p>4 Introduction Domain adaptation methods are necessary for many realworld applications where test examples differ significantly from the examples used for learning. [sent-13, score-0.597]
</p><p>5 Prior methods have shown that explicitly modeling and compensating for the domain shift from the source domain to the target (test) domain can significantly boost performance on the target domain. [sent-14, score-1.983]
</p><p>6 Supervised approaches do this by utilizing a few labeled examples in the target domain [16, 1, 7, 9, 18, 24, 28], while semi-supervised methods also take into account the (typically much more abundant) unlabeled target samples [13, 14]. [sent-15, score-1.506]
</p><p>7 In many problems, additional instance constraints are available over the unlabeled target data, encoding the knowledge that certain samples belong to the same object instance, and thus should be classified in a similar way. [sent-16, score-0.88]
</p><p>8 edu  ploit unlabeled instance constraints in addition to labeled examples. [sent-22, score-0.659]
</p><p>9 In this case, the source domain is static images, the target is surveillance video, and the instance constraints come from tracking an object between video frames. [sent-23, score-1.172]
</p><p>10 The second scenario (Figure 2) is adapting multi-category classifiers to a domain where only a subset of categories have (limited) labels, but the same object instances are observed from multiple views/cameras. [sent-27, score-0.532]
</p><p>11 In both of the above scenar-  ios, unlabeled instance constraints can provide additional information to the classifier about the structure of the target domain, yet such information has not to our knowledge been used for domain adaptation. [sent-28, score-1.236]
</p><p>12 In this paper, we present a unified domain adaptation framework that incorporates both traditional labels and unlabeled instance constraints. [sent-29, score-1.221]
</p><p>13 Our approach is broadly applicable in a range of adaptation settings, including heterogeneous features, detection, classification, and transfer of learned domain shift to unlabeled categories. [sent-30, score-1.267]
</p><p>14 Our main contribution 6 6 6 6 6 6 86 686  tion: a target classifier is learned using labeled images from a subset of categories, plus unlabeled images with instance constraints, which in this case come from images ofthe same object taken from different views. [sent-32, score-0.912]
</p><p>15 In both cases, our algorithm provides a significant improvement over algorithms with no adaptation and those using adaptation without instance constraints. [sent-35, score-1.05]
</p><p>16 Outline The paper is structured as follows: We first review related work in the area of domain adaptation for object recognition and detection in videos (Sect. [sent-36, score-0.969]
</p><p>17 Our domain adaptation method and the integration of instance constraints is presented in Sect. [sent-38, score-1.063]
</p><p>18 Instance similarity constraints have been used in multiview learning [4, 11, 21], canonical-correlation analysis [15], and as constraints between domains when available [24]. [sent-54, score-0.49]
</p><p>19 As far as we know, our approach is the first to utilize such constraints in the target domain. [sent-55, score-0.459]
</p><p>20 We build on the ideas of Laplacian SVM [22], which requires that the target function vary smoothly on the unlabeled examples. [sent-56, score-0.617]
</p><p>21 However, the authors focus on the automatic spatio-temporal segmentation of objects in videos to obtain labeled examples and use a very simple adaptation scheme (weighted combination of source and target). [sent-62, score-0.95]
</p><p>22 Domain adaptation with auxiliary similarity constraints A popular and effective class of domain adaptation algorithms jointly learns a hyperplane classifier of the source and target domains. [sent-65, score-2.308]
</p><p>23 We build on this general approach by  additionally incorporating constraints obtained from a given similarity graph defined on unlabeled target instances. [sent-66, score-0.916]
</p><p>24 We assume we are given labeled source data D = {(xi,yi)}in=s1 and labeled target = where ntL << ns. [sent-67, score-0.819]
</p><p>25 Additionally we are given unlabeled  dataD˜L  target data  D˜U  =  ? [sent-68, score-0.617]
</p><p>26 With the unlabeled target data, and the edge weight matrix, B, we then construct a graph G= B). [sent-72, score-0.617]
</p><p>27 , defines the similarity between two unlabeled target examples and is incorporated to integrate domain the unlabeled examples into domain adaptation. [sent-74, score-1.861]
</p><p>28 We first describe our approach and then we demonstrate its generality by integrating it with two specific domain adaptation algorithms. [sent-75, score-0.877]
</p><p>29 Learning framework Our goal is to learn classifier functions, f(x) = θTx for the source and f˜( x˜) = θ˜T x˜, for the target domain. [sent-80, score-0.56]
</p><p>30 Many max-margin based domain adaptation optimization techniques can be described generally in terms of the hyperplane parameters, θ, θ˜, an optional transformation parameter, A, and loss functions of these parameters and the data. [sent-81, score-1.069]
</p><p>31 Formally, this can be denoted as follows: min  R(θ, θ˜, A) + C · L(D, θ) + C˜ ·L˜(D˜L, θ˜)  (1)  θ,θ˜,A  where R is a regularizer over all parameters and L, L˜ repwrehseenret tRhe i slo ass r etegurmlasr on rt ohev source aarandm target dnadta L, respectively. [sent-82, score-0.623]
</p><p>32 C and C˜ are scalar parameters to be set to trade-off the impact of the source and target data. [sent-83, score-0.513]
</p><p>33 For our algorithm we will modify this general formulation to include additional constraints available from the similarity graph, G, available on the unlabeled target data. [sent-84, score-0.925]
</p><p>34 Manifold regularization To integrate unlabeled data with similarity constraints into a learning objective function, we use manifold regularization in the form of a Laplacian regularizer, which has been shown effective for semisupervised learning [22]. [sent-85, score-0.737]
</p><p>35 i Finally, tahgefollowing function expresses the regularization term that incorporates the similarity constraints over the unlabeled target data. [sent-89, score-0.908]
</p><p>36 X˜U  where denotes the matrix containing the unlabeled target training examples as columns and = θ˜TX˜U. [sent-100, score-0.707]
</p><p>37 (1) to produce a unified optimization framework, which can utilize both labeled examples from the target and unlabeled examples that have auxiliary similarity information. [sent-102, score-1.049]
</p><p>38 This can be seen as a generalization and extension of the semi-supervised approach of [22] to the domain adaptation setting. [sent-103, score-0.855]
</p><p>39 Domain adaptation models  f˜  For concreteness we next present our full optimization framework applied to two separate semi-supervised domain adaptation algorithms. [sent-106, score-1.368]
</p><p>40 Projective model transfer SVM (PMT-SVM) The PMT-SVM method of [1] assumes that the source hyperplane θ is given and was learned on the source dataset D pwlaithne examples x ainn dth we same nfeeadtu oren space as examples D˜x  D˜. [sent-107, score-0.671]
</p><p>41 adaptation ies ktehye adaptation regularizer that couples the target and the given f? [sent-109, score-1.411]
</p><p>42 h eTnh teh see source earnmd target hyperplane parameters are similar in terms of angular distance, which directly models the main assumption of domain adaptation that both domains share common properties and relevant features. [sent-128, score-1.527]
</p><p>43 θTθ˜  L˜,  Max-margin domain transforms (MMDT) The idea of MMDT, a transform-based domain adaptation approach proposed by [16], is to find a transformation A between the target and source domains allowing for joint learning of the classifiers in both domains. [sent-130, score-1.877]
</p><p>44 Therefore, we implicitly define = ATθ such that the final latent function  θ˜  6 6 6 76 67608 808  f˜( x˜)  of the target domain is modeled by = θ˜T x˜ = θTA x˜ and the optimization of Eq. [sent-133, score-0.747]
</p><p>45 2 regularizer for the source hyperplane and a Frobenius norm regularizer for the transformation leading to an over-all regularizer function as follows:  Rtrans(θ,A) =21? [sent-136, score-0.688]
</p><p>46 (4)  Note that the transformation is regularized with respect to the identity matrix so that in the case of large values of γ a classifier using source and target data will be learned. [sent-141, score-0.613]
</p><p>47 Optimization details We incorporate similarity constraints into the PMTSVM and transform-based domain adaptation (Sect. [sent-153, score-1.107]
</p><p>48 Multi-category adaptation Next we consider the setting where we have labeled examples for multiple categories in the source domain, and very few or no examples of some categories in the target domain. [sent-162, score-1.392]
</p><p>49 Additionally, for the unlabeled data in the target domain, we assume instance constraints are available. [sent-163, score-0.825]
</p><p>50 Since these constraints are not connected to any labeled examples, we need to use our semi-supervised framework to create a multi-category target classifier. [sent-164, score-0.612]
</p><p>51 Video domain adaptation We consider a setting in which we have an object detector (source detector) trained on a source domain, and we would like to perform detection on a target dataset consisting of videos. [sent-174, score-1.598]
</p><p>52 In particular, we adapt a filter-based object detector such as the deformable parts model (DPM) [12] to a video corpus in which we exploit the signal in the temporal structure of the video data by imposing similarity constraints on the adapted detector. [sent-175, score-0.523]
</p><p>53 Our method assumes that a small subset ofthe target video dataset has labeled bounding boxes, and another subset has unlabeled bounding boxes with tracks, for example from background subtraction or from another automatic approach, such as [23]. [sent-176, score-1.05]
</p><p>54 Experiments In the following, we evaluate our approach in two different scenarios where similarity constraints can easily be exploited: multi-category classification with instance-level constraints and video domain adaptation of a pedestrian detector. [sent-206, score-1.335]
</p><p>55 Multi-category classification Dataset We evaluate our algorithm in a multi-class classification setting using the Office benchmark domain adaptation dataset of [18]. [sent-209, score-0.961]
</p><p>56 The webcam domain is a collection of objects in an office environment taken with a webcam. [sent-211, score-0.463]
</p><p>57 We explore the setting where most of the available training data is from a source domain (webcam) that is misaligned with the test data that is drawn from the target domain (ds l r). [sent-214, score-1.333]
</p><p>58 Following [13] we first apply PCA to the source and target data and then use the lower dimensional data as input to our method and all baselines. [sent-216, score-0.513]
</p><p>59 Experimental setup and baselines We assume that there is very little labeled data available from the target domain. [sent-217, score-0.523]
</p><p>60 This means that there are a total of only 16 labeled examples available in the target domain. [sent-219, score-0.553]
</p><p>61 Only one labeled example is available from only half of the categories in the target domain. [sent-233, score-0.551]
</p><p>62 The rest of the target data is  assumed to have similarity constraints which can be used by the full similarity constraint algorithm. [sent-234, score-0.683]
</p><p>63 svmS  A standard Support Vector Machine classifier trained using both the source and labeled target data. [sent-236, score-0.768]
</p><p>64 svmS∪T  da only Uses source and labeled target data to train a semisupervised domain adaptation model using the MMDT optimization from Sect. [sent-237, score-1.722]
</p><p>65 Additionally, we show results for our proposed extension of MMDT, denoted as da + lap-sim, for domain adaptation with Laplacian regularization. [sent-242, score-0.999]
</p><p>66 We found that in this setting, with only a small amount of target training data, the transform-based domain adaptation method was able to learn to successfully adapt to the target. [sent-246, score-1.234]
</p><p>67 Additionally, we found that adding the similarity constraints from the unlabeled target data resulted in a signif-  icant performance increase. [sent-247, score-0.891]
</p><p>68 The Laplacian regularization explicitly optimizes the classifier scores of the same instances to be similar, which added further constraints that aided in adapting the final target classifier. [sent-248, score-0.635]
</p><p>69 This experiment validates our claim that adding the unlabeled instance constraints from the target can boost performance of a semisupervised domain adaptation method. [sent-249, score-1.791]
</p><p>70 To further evaluate the effect of adding the similarity constraints we also report the multi-class accuracy for only the categories that have no labeled target training data in Table 3. [sent-250, score-0.834]
</p><p>71 It is interesting to note that domain adaptation alone does not dramatically improve the results on the novel target categories. [sent-251, score-1.205]
</p><p>72 However, with the similarity constraints added, our full model dramatically improves on the novel target categories. [sent-253, score-0.571]
</p><p>73 This again validates our argument that the auxiliary similarity 6 6 6 7 7 7 20 020  svmS  svmS∪T  da only  da + lap-sim  S∪T  ±  ±  ±  ±  35. [sent-254, score-0.466]
</p><p>74 Classification results for only the target test data from categories with no labeled target training data. [sent-263, score-0.879]
</p><p>75 PASCAL to VisInt dataset description constraints can be used in conjunction with a domain adaptation algorithm to learn a more generalizable target model. [sent-266, score-1.337]
</p><p>76 Object detection in video We now present results showing that similarity constraints can be used to capture useful information about relationships among examples from the same track, which can be leveraged to significantly improve the performance of a domain adapted detector in video. [sent-269, score-0.858]
</p><p>77 The source domain has images from the PASCAL VOC 2007 dataset [10], and the target domain consists of frames of the videos from the VisInt dataset [25]. [sent-271, score-1.38]
</p><p>78 Experimental setup and baselines In training both the source and target models, we mainly follow the training protocol of the deformable parts model (DPM) [12], which we briefly summarize here. [sent-272, score-0.637]
</p><p>79 The source domain detector is trained as in [12], with the exceptions that we use only a single component (with leftand right-facing versions) and do not use any parts. [sent-278, score-0.683]
</p><p>80 Our target domain training protocol, on the other hand, differs somewhat more significantly from that of the base DPM. [sent-279, score-0.72]
</p><p>81 Rather than initializing an “empty” model with θ = 0, the target model is instead initialized to the source model, allowing us to skip to the latent positive and hard negative phases of training immediately. [sent-280, score-0.618]
</p><p>82 Both labeled and unlabeled bounding boxes are used in the latent positive stage of training. [sent-281, score-0.643]
</p><p>83 For unlabeled bounding boxes, the latent positives x˜j , x˜j? [sent-283, score-0.455]
</p><p>84 The source detector used in all domain adaptation detection experiments is trained on the train+val portion of the PASCAL 2007 dataset. [sent-291, score-1.202]
</p><p>85 In each experiment we choose a total of Nf labeled frames and 5Nf unlabeled frames. [sent-292, score-0.485]
</p><p>86 To evaluate our domain adaptation algorithm with similarity constraints, we compare against the following baselines. [sent-295, score-0.967]
</p><p>87 dpmS∪T DPM trained using both the source and labeled target data. [sent-300, score-0.721]
</p><p>88 da only DPM with domain adaptation trained using source and labeled target data model using the PMT-based optimization from Sect. [sent-301, score-1.742]
</p><p>89 With a single video (Nf = 10) of labeled training data, our results show that our method of integrating similarity constraints with domain adaptation (da + lap-sim) gives a 3. [sent-307, score-1.379]
</p><p>90 With two videos (Nf = 20) of labeled training data (and 10 videos of unlabeled training data used in similarity constraints), our method (da + lapsim) shows a 10. [sent-309, score-0.755]
</p><p>91 Our results show that using domain adaptation techniques and similarity constraints significantly improves over  using either one alone or neither. [sent-323, score-1.138]
</p><p>92 However, the results for lower Nf clearly demonstrate that similarity constraints can be highly informative in a video detection setting when labeled data is relatively scarce. [sent-328, score-0.52]
</p><p>93 Conclusion We have proposed a new approach for semi-supervised domain adaptation, which explicitly makes use of similarity constraints in the target domain to improve adaptation performance and to enrich learning with unlabeled training examples. [sent-331, score-2.147]
</p><p>94 Our method is based on manifold regularization and we showed how to extend two different supervised domain adaptation methods, the PMT-SVM from [1], and  MMDT, a transform-based approach proposed by [16]. [sent-332, score-0.924]
</p><p>95 Our experimental results show that using similarity constraints to incorporate knowledge about unlabeled examples da only da + lap-sim  Figure4. [sent-337, score-0.891]
</p><p>96 In general, our algorithm contributes a new formulation to seamlessly incorporate instance constraints into a wide class of semi-supervised domain adaptation algorithms. [sent-340, score-1.085]
</p><p>97 For future work, we plan to perform adaptation on the part level of the detector, which includes the appearance of each part as well as the constellation between them. [sent-341, score-0.491]
</p><p>98 Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach. [sent-354, score-0.882]
</p><p>99 A literature survey on domain adaptation of statistical classifiers. [sent-461, score-0.855]
</p><p>100 What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. [sent-470, score-0.491]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adaptation', 0.491), ('domain', 0.364), ('target', 0.319), ('unlabeled', 0.298), ('source', 0.194), ('mmdt', 0.169), ('labeled', 0.153), ('visint', 0.145), ('da', 0.144), ('constraints', 0.14), ('ju', 0.128), ('similarity', 0.112), ('hyperplane', 0.111), ('regularizer', 0.11), ('nf', 0.096), ('laplacian', 0.095), ('svms', 0.073), ('bounding', 0.073), ('ntl', 0.072), ('dpm', 0.071), ('detector', 0.07), ('instance', 0.068), ('adapting', 0.068), ('perturbation', 0.063), ('webcam', 0.062), ('video', 0.06), ('videos', 0.059), ('dpms', 0.057), ('trained', 0.055), ('examples', 0.053), ('pascal', 0.053), ('transformation', 0.053), ('boxes', 0.051), ('categories', 0.051), ('track', 0.05), ('rtrans', 0.048), ('domains', 0.048), ('additionally', 0.047), ('classifier', 0.047), ('transfer', 0.043), ('donahue', 0.043), ('latent', 0.042), ('saenko', 0.042), ('positives', 0.042), ('negatives', 0.042), ('regularization', 0.039), ('heterogeneous', 0.039), ('tx', 0.039), ('auxiliary', 0.039), ('eecs', 0.038), ('office', 0.037), ('training', 0.037), ('svm', 0.037), ('duan', 0.037), ('ds', 0.036), ('semisupervised', 0.035), ('frames', 0.034), ('shift', 0.032), ('icdm', 0.032), ('adapted', 0.031), ('category', 0.031), ('alone', 0.031), ('manifold', 0.03), ('tsang', 0.029), ('berkeley', 0.029), ('loss', 0.028), ('detection', 0.028), ('hyperparameters', 0.028), ('classification', 0.028), ('available', 0.028), ('kulis', 0.028), ('setting', 0.027), ('object', 0.027), ('validates', 0.027), ('protocol', 0.027), ('boost', 0.027), ('detectors', 0.027), ('positive', 0.026), ('unsupervised', 0.026), ('weights', 0.026), ('tj', 0.025), ('box', 0.024), ('voc', 0.024), ('projective', 0.023), ('baselines', 0.023), ('adapt', 0.023), ('dataset', 0.023), ('optimization', 0.022), ('integrating', 0.022), ('classifiers', 0.022), ('learning', 0.022), ('adding', 0.022), ('adaptive', 0.022), ('contributes', 0.022), ('hinge', 0.022), ('generic', 0.022), ('added', 0.022), ('multicategory', 0.021), ('christoudias', 0.021), ('traces', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="387-tfidf-1" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>2 0.51855516 <a title="387-tfidf-2" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>Author: Lin Chen, Lixin Duan, Dong Xu</p><p>Abstract: In this work, we propose to leverage a large number of loosely labeled web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which the samples from different source domains can be represented by different types of features with different dimensions (e.g., the SIFTfeaturesfrom web images and space-time (ST) features from web videos) while the target domain samples have all types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier, in which we simultaneously seek the optimal weights for different source domains with different types of features as well as infer the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of MDA-HS for event recognition in consumer videos.</p><p>3 0.39600596 <a title="387-tfidf-3" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>4 0.24314462 <a title="387-tfidf-4" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>5 0.2401984 <a title="387-tfidf-5" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>Author: Chao-Yeh Chen, Kristen Grauman</p><p>Abstract: We propose an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process. Using unlabeled video containing various human activities, the system first learns how body pose tends to change locally in time. Then, given a small number of labeled static images, it uses that model to extrapolate beyond the given exemplars and generate “synthetic ” training examples—poses that could link the observed images and/or immediately precede or follow them in time. In this way, we expand the training set without requiring additional manually labeled examples. We explore both example-based and manifold-based methods to implement our idea. Applying our approach to recognize actions in both images and video, we show it enhances a state-of-the-art technique when very few labeled training examples are available.</p><p>6 0.22034512 <a title="387-tfidf-6" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>7 0.21251012 <a title="387-tfidf-7" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>8 0.20305352 <a title="387-tfidf-8" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>9 0.19596173 <a title="387-tfidf-9" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>10 0.18038639 <a title="387-tfidf-10" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>11 0.16655618 <a title="387-tfidf-11" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>12 0.15792283 <a title="387-tfidf-12" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>13 0.13861643 <a title="387-tfidf-13" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>14 0.13419317 <a title="387-tfidf-14" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>15 0.11339588 <a title="387-tfidf-15" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>16 0.11228045 <a title="387-tfidf-16" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>17 0.10771754 <a title="387-tfidf-17" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>18 0.10608115 <a title="387-tfidf-18" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>19 0.10447829 <a title="387-tfidf-19" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>20 0.1012544 <a title="387-tfidf-20" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.238), (1, -0.108), (2, -0.087), (3, -0.015), (4, 0.027), (5, 0.028), (6, 0.014), (7, -0.038), (8, 0.048), (9, 0.09), (10, -0.022), (11, -0.144), (12, -0.007), (13, -0.124), (14, -0.246), (15, -0.199), (16, -0.067), (17, -0.233), (18, -0.122), (19, -0.082), (20, -0.268), (21, -0.318), (22, -0.133), (23, -0.132), (24, 0.017), (25, 0.052), (26, 0.129), (27, -0.152), (28, -0.061), (29, -0.079), (30, -0.081), (31, -0.064), (32, 0.007), (33, 0.08), (34, -0.013), (35, -0.04), (36, 0.029), (37, 0.04), (38, 0.041), (39, -0.022), (40, -0.028), (41, 0.09), (42, 0.074), (43, 0.013), (44, -0.073), (45, -0.07), (46, 0.036), (47, 0.022), (48, 0.043), (49, -0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98568279 <a title="387-lsi-1" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>2 0.93580806 <a title="387-lsi-2" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>Author: Lin Chen, Lixin Duan, Dong Xu</p><p>Abstract: In this work, we propose to leverage a large number of loosely labeled web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which the samples from different source domains can be represented by different types of features with different dimensions (e.g., the SIFTfeaturesfrom web images and space-time (ST) features from web videos) while the target domain samples have all types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier, in which we simultaneously seek the optimal weights for different source domains with different types of features as well as infer the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of MDA-HS for event recognition in consumer videos.</p><p>3 0.83099437 <a title="387-lsi-3" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>4 0.82067603 <a title="387-lsi-4" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>5 0.63608003 <a title="387-lsi-5" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>Author: Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu, Cunzhao Shi</p><p>Abstract: In this paper, we propose a novel method for cross-view action recognition via a continuous virtual path which connects the source view and the target view. Each point on this virtual path is a virtual view which is obtained by a linear transformation of the action descriptor. All the virtual views are concatenated into an infinite-dimensional feature to characterize continuous changes from the source to the target view. However, these infinite-dimensional features cannot be used directly. Thus, we propose a virtual view kernel to compute the value of similarity between two infinite-dimensional features, which can be readily used to construct any kernelized classifiers. In addition, there are a lot of unlabeled samples from the target view, which can be utilized to improve the performance of classifiers. Thus, we present a constraint strategy to explore the information contained in the unlabeled samples. The rationality behind the constraint is that any action video belongs to only one class. Our method is verified on the IXMAS dataset, and the experimental results demonstrate that our method achieves better performance than the state-of-the-art methods.</p><p>6 0.59837377 <a title="387-lsi-6" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>7 0.53992617 <a title="387-lsi-7" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>8 0.53208798 <a title="387-lsi-8" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>9 0.49464667 <a title="387-lsi-9" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>10 0.47648889 <a title="387-lsi-10" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>11 0.45773333 <a title="387-lsi-11" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>12 0.45409724 <a title="387-lsi-12" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>13 0.44430086 <a title="387-lsi-13" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>14 0.4197666 <a title="387-lsi-14" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>15 0.41788298 <a title="387-lsi-15" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>16 0.41119227 <a title="387-lsi-16" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>17 0.3912338 <a title="387-lsi-17" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>18 0.38797662 <a title="387-lsi-18" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>19 0.37646013 <a title="387-lsi-19" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>20 0.3760986 <a title="387-lsi-20" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.179), (16, 0.041), (26, 0.031), (33, 0.29), (39, 0.01), (48, 0.099), (67, 0.138), (69, 0.045), (80, 0.01), (87, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94452548 <a title="387-lda-1" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>2 0.94291937 <a title="387-lda-2" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>Author: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen</p><p>Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.</p><p>3 0.94269568 <a title="387-lda-3" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>4 0.94239676 <a title="387-lda-4" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xingyu Zeng, Xiaogang Wang</p><p>Abstract: Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. The difficulty is added when several pedestrians overlap in images and occlude each other. We observe, however, that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the CaltechTrain dataset, the Caltech-Test dataset and the ETHdataset. Including mutual visibility leads to 4% −8% improvements on mluudlitnipglem ubteunaclh vmiasibrki ditayta lesaedtss.</p><p>5 0.94233173 <a title="387-lda-5" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>6 0.93976414 <a title="387-lda-6" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>7 0.93965375 <a title="387-lda-7" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>8 0.93954527 <a title="387-lda-8" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>9 0.93853903 <a title="387-lda-9" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>10 0.93833309 <a title="387-lda-10" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>11 0.93675208 <a title="387-lda-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>same-paper 12 0.93595445 <a title="387-lda-12" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>13 0.93361104 <a title="387-lda-13" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>14 0.93118548 <a title="387-lda-14" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>15 0.93111819 <a title="387-lda-15" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>16 0.93093342 <a title="387-lda-16" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>17 0.9294191 <a title="387-lda-17" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>18 0.92920238 <a title="387-lda-18" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>19 0.92911559 <a title="387-lda-19" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>20 0.92839539 <a title="387-lda-20" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
