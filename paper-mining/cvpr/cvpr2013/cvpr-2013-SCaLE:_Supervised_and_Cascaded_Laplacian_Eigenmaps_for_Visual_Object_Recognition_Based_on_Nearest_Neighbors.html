<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-371" href="#">cvpr2013-371</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</h1>
<br/><p>Source: <a title="cvpr-2013-371-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wu_SCaLE_Supervised_and_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ruobing Wu, Yizhou Yu, Wenping Wang</p><p>Abstract: Recognizing the category of a visual object remains a challenging computer vision problem. In this paper we develop a novel deep learning method thatfacilitates examplebased visual object category recognition. Our deep learning architecture consists of multiple stacked layers and computes an intermediate representation that can be fed to a nearest-neighbor classifier. This intermediate representation is discriminative and structure-preserving. It is also capable of extracting essential characteristics shared by objects in the same category while filtering out nonessential differences among them. Each layer in our model is a nonlinear mapping, whose parameters are learned through two sequential steps that are designed to achieve the aforementioned properties. The first step computes a discrete mapping called supervised Laplacian Eigenmap. The second step computes a continuous mapping from the discrete version through nonlinear regression. We have extensively tested our method and it achieves state-of-the-art recognition rates on a number of benchmark datasets.</p><p>Reference: <a title="cvpr-2013-371-reference" href="../cvpr2013_reference/cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hk  Abstract Recognizing the category of a visual object remains a challenging computer vision problem. [sent-3, score-0.23]
</p><p>2 In this paper we develop a novel deep learning method thatfacilitates examplebased visual object category recognition. [sent-4, score-0.46]
</p><p>3 Our deep learning architecture consists of multiple stacked layers and computes an intermediate representation that can be fed to a nearest-neighbor classifier. [sent-5, score-0.705]
</p><p>4 It is also capable of extracting essential characteristics shared by objects in the same category while filtering out nonessential differences among them. [sent-7, score-0.322]
</p><p>5 Each layer in our model is a nonlinear mapping, whose parameters are learned through two sequential steps that are designed to achieve the aforementioned properties. [sent-8, score-0.357]
</p><p>6 The first step computes a discrete mapping called supervised Laplacian Eigenmap. [sent-9, score-0.371]
</p><p>7 The second step computes a continuous mapping from the discrete version through nonlinear regression. [sent-10, score-0.492]
</p><p>8 Introduction Recognizing the category of a visual object is an important and challenging aspect of automatic object recognition. [sent-13, score-0.23]
</p><p>9 Visual objects from the same category may exhibit a wide range of shape and appearance variations due to many reasons, including inherent within-class shape and appearance diversity as well as the presence of deformation and illumination changes. [sent-14, score-0.196]
</p><p>10 Therefore, correctly rec-  ognizing visual object categories with large within-class diversity requires the brain to form an intermediate representation that grasps the essence of those characteristics shared by objects in the same category while filtering out nonessential differences. [sent-24, score-0.535]
</p><p>11 Our goal is to develop a deep learning method that facilitates example-based reasoning for visual object category recognition. [sent-26, score-0.46]
</p><p>12 Inspired by the important role played by examples in human visual recognition, we would like to determine the category of an object by computing its similarities with a set of examples with known category labels. [sent-27, score-0.466]
</p><p>13 As discussed above, such similarities need to be computed with the assistance of a proper intermediate representation, which is crucial in achieving high recognition rates. [sent-28, score-0.219]
</p><p>14 Since this intermediate representation needs to grasp the essence of an object’s attributes, it is likely to be the result of a complicated transformation applied to the features extracted from original input images. [sent-29, score-0.226]
</p><p>15 Since multilayer deep learning architectures have a reputation for learning such transformations, we devise a novel deep learning architecture for computing a desired intermediate representation. [sent-30, score-0.798]
</p><p>16 The intermediate representation of a visual object is in fact a point in a new multidimensional feature space, and the dissimilarity between two objects can be measured by the  Euclidean distance between their corresponding points. [sent-33, score-0.249]
</p><p>17 In 888886666677555  this context, being discriminative means points with different category labels should be further away from each other than those with same category labels. [sent-34, score-0.392]
</p><p>18 Second, nonessential differences between objects with the same category label should be suppressed, which has been much discussed earlier. [sent-35, score-0.285]
</p><p>19 It is important for supervised learning to capture intrinsic connections between the input (image features) and the output of the trained system. [sent-37, score-0.179]
</p><p>20 In summary, we propose a supervised layered visual object category classification framework inspired by recent deep learning methods. [sent-39, score-0.637]
</p><p>21 In the second stage, a layered model is applied to the extracted features to obtain a desired intermediate representation. [sent-43, score-0.23]
</p><p>22 In the last stage, the intermediate representation is fed to a classifier based on nearest neighbors. [sent-44, score-0.26]
</p><p>23 At each layer, the output representation from the previous layer is taken as the input and is nonlinearly mapped. [sent-47, score-0.333]
</p><p>24 The result of this mapping defines the output representation at the cur-  rent layer. [sent-48, score-0.226]
</p><p>25 This nonlinear continuous mapping approximates (through regression) a discrete mapping called supervised Laplacian Eigenmap, which is the optimal solution of an energy function designed to address the aforementioned three requirements. [sent-49, score-0.795]
</p><p>26 Related Work Many image classification and object category recognition approaches [26, 3 1, 17, 28] have been developed to process medium-scale benchmark datasets such as Caltech 101 [14], Caltech 256 [18] and PASCAL VOC [13]. [sent-54, score-0.294]
</p><p>27 Thus, deep learning simulates the subconscious process undertaken by humans when they make classification decisions. [sent-58, score-0.262]
</p><p>28 Deep belief networks (DBN) are a type of deep learning models where lower-level features are progressively combined into more compact high-level representations. [sent-59, score-0.268]
</p><p>29 Stacked denoising autoencoders [27] represent another deep learning architecture where each layer serves as a compact encoder of its previous layer. [sent-60, score-0.562]
</p><p>30 In recent years, deep  convolutional neural networks (DNN) [16], have demonstrated their capability of automatically extracting spatial and spatial-temporal features from raw images while being resistant to object appearance variations in the images. [sent-61, score-0.358]
</p><p>31 tailed illustration of one layer, where an input feature vector xi is mapped to an output feature vector xi+1 through Fi, which itself has multiple components. [sent-70, score-0.245]
</p><p>32 Architecture The architecture of our learning system, as demonstrated in Figure 1, is a layered model which aims at constructing a discriminative and structure-preserving image feature space. [sent-73, score-0.297]
</p><p>33 Images from the same category have inherent connections and similarities even though they may differ visually in geometric deformation, color, texture, illumination, etc. [sent-74, score-0.236]
</p><p>34 Our model is designed to dig out hidden similarities and force together images with the same category label in a new feature space. [sent-75, score-0.314]
</p><p>35 That means feature vectors with different category labels should be further away from each other than those with same category labels, and feature distribution in the new space should be regularized. [sent-77, score-0.58]
</p><p>36 Since it would be hard to meet these goals at once for challenging datasets, in the spirit of deep learning, we envision compositing multiple layers of nonlinear transformations together would be a more feasible solution. [sent-78, score-0.397]
</p><p>37 Each layer performs a (nonlinear) mapping between an input feature 8 8 86 6 68 6 6  space and an output feature space. [sent-80, score-0.667]
</p><p>38 Let us first focus on a sin-  gle layer Li (1 ≤ i≤ d). [sent-81, score-0.244]
</p><p>39 xji (e∈ t aRinsii )n gis d a training sample, lj aiss i {ts( category label, which is( independent ofthe layer index, N is the number of training samples, and si denotes the dimensionality of the training samples. [sent-83, score-1.03]
</p><p>40 For any of the subsequent layers, they are actually transformed features produced by the previous layer Li−1 (i > 1). [sent-86, score-0.244]
</p><p>41 In this context, we define the mapping at Li as F1i : Rsi → Rsi+1 . [sent-87, score-0.178]
</p><p>42 Let Si (j) be the set of n nearest neighbors of xji, Rand Wjik (≥ 0) be a similarity measure between xji and xik. [sent-88, score-0.303]
</p><p>43 =Slik(j),  (2)  is a pulling energy that attempts to pull closer a training sample and its neighbors with the same category label in the output feature space Rsi+1 . [sent-96, score-0.653]
</p><p>44 =lq  where djik = | |Fi (xij ) − Fi (xik) | |, is a pushing energy that attempts =to | push neighbors wi)t|h|, ,d i sffe ar epnuts category glaybels at least a unit margin further away in the output feature space than neighbors with same labels. [sent-106, score-0.72]
</p><p>45 Si (j)  (4)  is a standard structure-preserving energy that keeps training samples close in the output feature space if they have high similarity in the input feature space. [sent-112, score-0.469]
</p><p>46 This last energy term is very similar to the energy used for deriving Laplacian Eigenmap [1] except that we seek a continuous mapping Fi instead of a discrete embedding and there is no normalization constraint on the magnitude of the mapped training samples. [sent-114, score-0.709]
</p><p>47 These are the reasons we call the mapping we learn at each layer a supervised Laplacian Eigenmap. [sent-116, score-0.474]
</p><p>48 For example, without a normalization constraint, minimizing Einner in (4) alone would make all input samples collapse to a single point in the output feature space. [sent-118, score-0.246]
</p><p>49 On the other hand, minimizing Epull in (2) alone would also have the potential of collapsing input samples with the same category label to a single point in the output feature space. [sent-120, score-0.442]
</p><p>50 Results in Figure 2 show that minimizing the sum of all three energy terms with proper weights can yield much better (7% to 9% in accuracy) recognition performance than minimizing Epush Epull only or Einner only. [sent-123, score-0.233]
</p><p>51 +  Our complete model consists of multiple layers of continuous mappings stacked on top of each other, i. [sent-124, score-0.259]
</p><p>52 Each layer is constructed and learned in the same way while using potentially different parameters, which include the dimensionality si of input features, and the weights λi and μi for different energy terms. [sent-127, score-0.537]
</p><p>53 It neither extracts features from a raw input image nor performs final classification to determine  the category label of the input image. [sent-129, score-0.353]
</p><p>54 It only performs nonlinear transformations that eventually define a final feature space through multiple intermediate feature spaces defined at intermediate layers. [sent-130, score-0.511]
</p><p>55 Given an input feature vector, x1 (∈ Rs1), to the first layer, the output feature vector, Fd(Fd−(∈1(. [sent-131, score-0.245]
</p><p>56 )) (∈ Rsd), of the last layer is fed to a final multi-category . [sent-137, score-0.295]
</p><p>57 In the first subproblem, we seek a discrete mapping between an input feature space and an output feature space for the training samples only. [sent-140, score-0.688]
</p><p>58 This discrete mapping minimizes the same energy as defined in (1). [sent-141, score-0.392]
</p><p>59 In the second subproblem, we seek a continuous mapping by performing regression on the discrete results obtained in the first subproblem. [sent-142, score-0.484]
</p><p>60 Such feature vectors are used as the input to the first layer of our learned model. [sent-149, score-0.395]
</p><p>61 The first of these two steps is to compute a discrete mapping that minimizes (1). [sent-172, score-0.287]
</p><p>62 Instead of taking the continu)(ojus = mapping NFi) as th Re unknown in the minimization, we take {Yji}jN=1 as a set of mutually independent unknowns and replace all Fi (xij)’s in (2)-(4) with Yji. [sent-177, score-0.178]
</p><p>63 Note that the input feature vectors of the discrete mapping are known since we solve the mapping for one layer at a time. [sent-178, score-0.86]
</p><p>64 Nevertheless, converting from a continuous mapping to a discrete mapping makes it easier to compute a good initial solution, which is often crucial in obtaining a high-quality final solution for nonconvex problems. [sent-180, score-0.592]
</p><p>65 Because of the similarity between the energy term in (4) and the energy for Laplacian Eigenmap [1], we decided to take the Laplacian Eigenmap as the initial solution to our discrete mapping. [sent-181, score-0.319]
</p><p>66 With the set of input vectors {Yji−1}jN=1 at layer Li in mind, we t saekte tfhe in following steps. [sent-183, score-0.317]
</p><p>67 Th}e adjacency graph among these input vectors is defined by the set of nearest neighbors, Si (j), of Yji−1 . [sent-184, score-0.176]
</p><p>68 Although Laplacian Eigenmap generates a starting point for our optimization, it does not take category label information into consideration. [sent-192, score-0.196]
</p><p>69 We further minimize the complete energy function defined in (1) from the initial solution provided by Laplacian Eigenmap using gradient descent, which typically result in a high-quality solution to the discrete mapping we seek in our experiments. [sent-193, score-0.429]
</p><p>70 Note that energy terms similar to Epull and Epush in a discrete setting have been previously exploited in metric learning literature, including the Large Margin Nearest Neighbor (LMNN) approach in [29], which learns a Mahanalobis distance metric for k-NN classification. [sent-194, score-0.252]
</p><p>71 Compared with LMNN, we directly optimize the coordinates of transformed feature vectors in a new si+1-dimensional space instead of just training a linear transformation for the original feature space. [sent-195, score-0.271]
</p><p>72 Furthermore, the new feature vectors learned from our problem formulation is typically the result of a more powerful nonlinear transformation. [sent-197, score-0.191]
</p><p>73 Continuous Mapping So far we have obtained a discrete mapping which redistributes training samples in a new feature space according to the three requirements discussed in the Introduction. [sent-200, score-0.484]
</p><p>74 A continuous mapping is then learned from the discrete mapping through nonlinear regression, which essentially solves another optimization problem that seeks a continuous mapping that can approximately satisfy the constraints defined by the discrete mapping, i. [sent-201, score-1.017]
</p><p>75 We have experimented with multiple nonlinear regression techniques from the literature, including kernel SVM regression, random forests [5], and radial basis functions (RBFs) [6]. [sent-204, score-0.191]
</p><p>76 Since the output feature space of layer Li is si+1-dimensional, we actually perform RBF regression si+1 times once for each dimension. [sent-206, score-0.438]
</p><p>77 Classification The energy terms Epush and Epull in our optimization  make it naturally suitable (slightly better than using linear and kernel SVMs) to apply k-NN classification (voting among k nearest neighbors) on the final feature vector Rd(Rd−1 (. [sent-220, score-0.287]
</p><p>78 The final feature vectors of all training samples are stored in a k-d tree for fast nearest neighbor lookup. [sent-227, score-0.301]
</p><p>79 7 layers were used, at the first 4 of which the dimensionality was reduced by half with λ = μ = 0. [sent-247, score-0.198]
</p><p>80 At each layer the dimen8 8 876 671 9 9  (a) Comparison results on Caltech 256  (b) Comparison results on Caltech 101 Figure 3. [sent-274, score-0.244]
</p><p>81 3 for the first 4 layers, and at the last layer λ = 0. [sent-279, score-0.244]
</p><p>82 Like other deep learning methods, our method requires a sufficient amount of training data to perform well, and when compared to other approaches, yields better performance once the training set becomes sufficiently large. [sent-293, score-0.429]
</p><p>83 At each layer the dimensionality was reduced by a third with λ = μ = 0. [sent-300, score-0.318]
</p><p>84 The number of nearest neighbors used for training was 26 for all layers. [sent-302, score-0.239]
</p><p>85 Note that since we are confronted with multiple labels here, a training sample is considered to belong to a category as long as they share a certain label. [sent-303, score-0.279]
</p><p>86 At each layer the dimensionality is reduced by a quarter with λ = μ = 0. [sent-317, score-0.318]
</p><p>87 Results using 2 folds (58320) of training data and comparisons with state-of-the-art techniques can be found in Table 2. [sent-321, score-0.178]
</p><p>88 67% accuracy which outperforms another recent deep learning architecture [10] (97. [sent-323, score-0.318]
</p><p>89 With large weights assigned to Epush and Epull, our method is capable ofwell separating images from different categories especially when each category has a relatively large number of training images. [sent-325, score-0.347]
</p><p>90 Discussions In this section, we explore the effects of model depth d, the number of nearest neighbors n during training and the number of RBF kernels Nrbf on final classification performance. [sent-425, score-0.354]
</p><p>91 When the number of layers is too small, we may lose key feature dimensions from the beginning. [sent-433, score-0.202]
</p><p>92 However, adopting too many layers does not necessarily increase the recognition rate after a certain value (5 in this case) since regression introduces errors which gradually decrease the quality of the mapped feature vectors. [sent-434, score-0.27]
</p><p>93 On the other hand, too many neighbors would make an entire category almost shrink to a single point in the new feature space. [sent-438, score-0.358]
</p><p>94 Conclusions and Future Work We have presented a supervised deep learning method for visual object category recognition based on nearest neighbors. [sent-445, score-0.584]
</p><p>95 In a flexible layered architecture, discriminative and structure-preserving intermediate data representations are obtained for modeling complex nonlinear feature spaces. [sent-446, score-0.389]
</p><p>96 Such performance demonstrates that classification based on nearest neighbors can become at least as powerful as other classification approaches if a suitable feature space is learned. [sent-448, score-0.298]
</p><p>97 In the extreme case, if we could learn a continuous mapping from the raw input data to a feature space identical to the label space where objects in the same category share the same value, a nearest-neighbor classifier would be able to achieve perfect accuracy. [sent-449, score-0.628]
</p><p>98 In future, we aim at further boosting recognition rates by exploring more effective feature descriptors used as the initial feature fed to the first layer in our model. [sent-451, score-0.509]
</p><p>99 Domain transformation techniques other than the Laplacian Eigenmap could be explored to remove the restriction that the dimensionality of intermediate feature vectors cannot be increased from layer to layer. [sent-453, score-0.565]
</p><p>100 Neural dynamics of category learning and recognition: Attention, memory consolidation, and amnesia. [sent-493, score-0.234]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('layer', 0.244), ('eigenmap', 0.237), ('caltech', 0.215), ('epull', 0.208), ('category', 0.196), ('yji', 0.195), ('deep', 0.192), ('phog', 0.184), ('mapping', 0.178), ('einner', 0.178), ('epush', 0.178), ('norb', 0.148), ('xji', 0.147), ('laplacian', 0.138), ('intermediate', 0.137), ('xij', 0.128), ('rbf', 0.128), ('layers', 0.124), ('centrist', 0.122), ('discrete', 0.109), ('fi', 0.108), ('energy', 0.105), ('layered', 0.093), ('continuous', 0.092), ('rsi', 0.092), ('nonessential', 0.089), ('architecture', 0.088), ('neighbors', 0.084), ('kernels', 0.083), ('training', 0.083), ('nonlinear', 0.081), ('feature', 0.078), ('dimensionality', 0.074), ('si', 0.073), ('nearest', 0.072), ('regression', 0.068), ('saliency', 0.061), ('folds', 0.061), ('wjik', 0.059), ('voc', 0.059), ('pull', 0.059), ('rates', 0.058), ('jn', 0.057), ('chatfield', 0.056), ('xik', 0.054), ('rbfs', 0.053), ('dnn', 0.053), ('supervised', 0.052), ('fed', 0.051), ('pascal', 0.049), ('essence', 0.048), ('output', 0.048), ('lj', 0.047), ('push', 0.046), ('neural', 0.044), ('multilayer', 0.044), ('imagenet', 0.044), ('stacked', 0.043), ('raw', 0.043), ('minimizing', 0.043), ('fki', 0.042), ('margin', 0.042), ('radial', 0.042), ('proper', 0.042), ('sift', 0.041), ('convolutional', 0.041), ('curve', 0.041), ('input', 0.041), ('similarities', 0.04), ('pyramid', 0.039), ('bins', 0.039), ('li', 0.039), ('bosch', 0.038), ('lmnn', 0.038), ('networks', 0.038), ('learning', 0.038), ('mnist', 0.037), ('pushing', 0.037), ('capable', 0.037), ('nevertheless', 0.037), ('seek', 0.037), ('samples', 0.036), ('nonconvex', 0.035), ('eigenmaps', 0.035), ('visual', 0.034), ('benchmark', 0.034), ('subproblem', 0.034), ('comparisons', 0.034), ('codebook', 0.033), ('sufficiently', 0.033), ('sequential', 0.032), ('vectors', 0.032), ('classification', 0.032), ('datasets', 0.032), ('computes', 0.032), ('architectures', 0.031), ('categories', 0.031), ('coding', 0.031), ('lecun', 0.031), ('adjacency', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="371-tfidf-1" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>Author: Ruobing Wu, Yizhou Yu, Wenping Wang</p><p>Abstract: Recognizing the category of a visual object remains a challenging computer vision problem. In this paper we develop a novel deep learning method thatfacilitates examplebased visual object category recognition. Our deep learning architecture consists of multiple stacked layers and computes an intermediate representation that can be fed to a nearest-neighbor classifier. This intermediate representation is discriminative and structure-preserving. It is also capable of extracting essential characteristics shared by objects in the same category while filtering out nonessential differences among them. Each layer in our model is a nonlinear mapping, whose parameters are learned through two sequential steps that are designed to achieve the aforementioned properties. The first step computes a discrete mapping called supervised Laplacian Eigenmap. The second step computes a continuous mapping from the discrete version through nonlinear regression. We have extensively tested our method and it achieves state-of-the-art recognition rates on a number of benchmark datasets.</p><p>2 0.16659339 <a title="371-tfidf-2" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>3 0.1539212 <a title="371-tfidf-3" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>4 0.14378123 <a title="371-tfidf-4" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>Author: Yang Yang, Guang Shu, Mubarak Shah</p><p>Abstract: We propose a novel approach to boost the performance of generic object detectors on videos by learning videospecific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-bytracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto encoders. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability; second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.</p><p>5 0.14186119 <a title="371-tfidf-5" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>6 0.13104057 <a title="371-tfidf-6" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>7 0.12629555 <a title="371-tfidf-7" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>8 0.12354483 <a title="371-tfidf-8" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>9 0.11808095 <a title="371-tfidf-9" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>10 0.11625566 <a title="371-tfidf-10" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>11 0.11200304 <a title="371-tfidf-11" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>12 0.10725451 <a title="371-tfidf-12" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>13 0.10344396 <a title="371-tfidf-13" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>14 0.10251661 <a title="371-tfidf-14" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>15 0.10238817 <a title="371-tfidf-15" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>16 0.099166945 <a title="371-tfidf-16" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>17 0.098605417 <a title="371-tfidf-17" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>18 0.092188157 <a title="371-tfidf-18" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>19 0.091647923 <a title="371-tfidf-19" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>20 0.091249488 <a title="371-tfidf-20" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.255), (1, -0.062), (2, 0.024), (3, 0.08), (4, 0.05), (5, 0.041), (6, -0.005), (7, 0.007), (8, -0.033), (9, -0.045), (10, -0.033), (11, -0.03), (12, 0.004), (13, -0.086), (14, 0.044), (15, 0.039), (16, -0.12), (17, 0.036), (18, 0.08), (19, 0.031), (20, 0.003), (21, -0.106), (22, 0.05), (23, -0.083), (24, -0.032), (25, 0.069), (26, 0.009), (27, 0.05), (28, 0.001), (29, 0.009), (30, -0.001), (31, -0.007), (32, -0.001), (33, 0.01), (34, -0.004), (35, -0.017), (36, 0.016), (37, 0.032), (38, 0.139), (39, -0.04), (40, -0.101), (41, -0.022), (42, 0.058), (43, 0.036), (44, 0.046), (45, 0.035), (46, 0.036), (47, 0.024), (48, -0.025), (49, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93030572 <a title="371-lsi-1" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>Author: Ruobing Wu, Yizhou Yu, Wenping Wang</p><p>Abstract: Recognizing the category of a visual object remains a challenging computer vision problem. In this paper we develop a novel deep learning method thatfacilitates examplebased visual object category recognition. Our deep learning architecture consists of multiple stacked layers and computes an intermediate representation that can be fed to a nearest-neighbor classifier. This intermediate representation is discriminative and structure-preserving. It is also capable of extracting essential characteristics shared by objects in the same category while filtering out nonessential differences among them. Each layer in our model is a nonlinear mapping, whose parameters are learned through two sequential steps that are designed to achieve the aforementioned properties. The first step computes a discrete mapping called supervised Laplacian Eigenmap. The second step computes a continuous mapping from the discrete version through nonlinear regression. We have extensively tested our method and it achieves state-of-the-art recognition rates on a number of benchmark datasets.</p><p>2 0.79190296 <a title="371-lsi-2" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Complex real-world signals, such as images, contain discriminative structures that differ in many aspects including scale, invariance, and data channel. While progress in deep learning shows the importance of learning features through multiple layers, it is equally important to learn features through multiple paths. We propose Multipath Hierarchical Matching Pursuit (M-HMP), a novel feature learning architecture that combines a collection of hierarchical sparse features for image classification to capture multiple aspects of discriminative structures. Our building blocks are MI-KSVD, a codebook learning algorithm that balances the reconstruction error and the mutual incoherence of the codebook, and batch orthogonal matching pursuit (OMP); we apply them recursively at varying layers and scales. The result is a highly discriminative image representation that leads to large improvements to the state-of-the-art on many standard benchmarks, e.g., Caltech-101, Caltech-256, MITScenes, Oxford-IIIT Pet and Caltech-UCSD Bird-200.</p><p>3 0.75928551 <a title="371-lsi-3" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>4 0.74697256 <a title="371-lsi-4" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Peng Wang, Jingdong Wang, Gang Zeng, Weiwei Xu, Hongbin Zha, Shipeng Li</p><p>Abstract: In visual recognition tasks, the design of low level image feature representation is fundamental. The advent of local patch features from pixel attributes such as SIFT and LBP, has precipitated dramatic progresses. Recently, a kernel view of these features, called kernel descriptors (KDES) [1], generalizes the feature design in an unsupervised fashion and yields impressive results. In this paper, we present a supervised framework to embed the image level label information into the design of patch level kernel descriptors, which we call supervised kernel descriptors (SKDES). Specifically, we adopt the broadly applied bag-of-words (BOW) image classification pipeline and a large margin criterion to learn the lowlevel patch representation, which makes the patch features much more compact and achieve better discriminative ability than KDES. With this method, we achieve competitive results over several public datasets comparing with stateof-the-art methods.</p><p>5 0.70134145 <a title="371-lsi-5" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>Author: Bin Zhao, Eric P. Xing</p><p>Abstract: Many vision tasks require a multi-class classifier to discriminate multiple categories, on the order of hundreds or thousands. In this paper, we propose sparse output coding, a principled way for large-scale multi-class classification, by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem. Specifically, sparse output coding is composed of two steps: efficient coding matrix learning with scalability to thousands of classes, and probabilistic decoding. Empirical results on object recognition and scene classification demonstrate the effectiveness ofour proposed approach.</p><p>6 0.68640739 <a title="371-lsi-6" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>7 0.68534678 <a title="371-lsi-7" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>8 0.67710441 <a title="371-lsi-8" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>9 0.66806102 <a title="371-lsi-9" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>10 0.66778582 <a title="371-lsi-10" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>11 0.66727102 <a title="371-lsi-11" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>12 0.6646592 <a title="371-lsi-12" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>13 0.66398543 <a title="371-lsi-13" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>14 0.65288562 <a title="371-lsi-14" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>15 0.65197396 <a title="371-lsi-15" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>16 0.64726233 <a title="371-lsi-16" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>17 0.64397466 <a title="371-lsi-17" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>18 0.6425041 <a title="371-lsi-18" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>19 0.63900805 <a title="371-lsi-19" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>20 0.62165248 <a title="371-lsi-20" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.104), (16, 0.018), (26, 0.033), (28, 0.011), (33, 0.27), (67, 0.054), (69, 0.391), (80, 0.011), (87, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92541122 <a title="371-lda-1" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>Author: Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: 3D volumetric reasoning is important for truly understanding a scene. Humans are able to both segment each object in an image, and perceive a rich 3D interpretation of the scene, e.g., the space an object occupies, which objects support other objects, and which objects would, if moved, cause other objects to fall. We propose a new approach for parsing RGB-D images using 3D block units for volumetric reasoning. The algorithm fits image segments with 3D blocks, and iteratively evaluates the scene based on block interaction properties. We produce a 3D representation of the scene based on jointly optimizing over segmentations, block fitting, supporting relations, and object stability. Our algorithm incorporates the intuition that a good 3D representation of the scene is the one that fits the data well, and is a stable, self-supporting (i.e., one that does not topple) arrangement of objects. We experiment on several datasets including controlled and real indoor scenarios. Results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation.</p><p>2 0.90761423 <a title="371-lda-2" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>3 0.9006561 <a title="371-lda-3" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>4 0.89720064 <a title="371-lda-4" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>5 0.88405097 <a title="371-lda-5" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>6 0.87215537 <a title="371-lda-6" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>7 0.86855888 <a title="371-lda-7" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>same-paper 8 0.83528745 <a title="371-lda-8" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>9 0.79380155 <a title="371-lda-9" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>10 0.75650394 <a title="371-lda-10" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>11 0.74365193 <a title="371-lda-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.73567623 <a title="371-lda-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.73274541 <a title="371-lda-13" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>14 0.72502744 <a title="371-lda-14" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>15 0.72442693 <a title="371-lda-15" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>16 0.72394627 <a title="371-lda-16" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>17 0.72315103 <a title="371-lda-17" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>18 0.7189092 <a title="371-lda-18" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>19 0.71881759 <a title="371-lda-19" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>20 0.71743613 <a title="371-lda-20" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
