<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-416" href="#">cvpr2013-416</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</h1>
<br/><p>Source: <a title="cvpr-2013-416-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yun_Studying_Relationships_between_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>Reference: <a title="cvpr-2013-416-reference" href="../cvpr2013_reference/cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ins  Abstract Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. [sent-7, score-0.29]
</p><p>2 In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. [sent-8, score-0.692]
</p><p>3 This creates the unprecedented opportunity to harness these devices and use information about eye, head, and body movements to inform intelligent systems about the content that we find interesting and the tasks that we are trying to perform. [sent-15, score-0.286]
</p><p>4 This is particularly true in the case of gaze behavior, which provides direct insight into a person’s interests and intent. [sent-16, score-0.583]
</p><p>5 We envision a day when reliable eye tracking can be performed using standard front facing cameras, making it possible for visual imagery to be tagged with individualized interpretations of content, each a unique “story” simply through the act of a person viewing their favorite images and videos. [sent-17, score-0.333]
</p><p>6 Bottom: objects described by people and detected objects from each method (green - correct, blue - incorrect). [sent-20, score-0.189]
</p><p>7 symbiotic relationship might be exploited to better analyze and index content that people find important. [sent-21, score-0.294]
</p><p>8 Recent advances have started to look at problems of recognition at a human scale, classifying or localizing thousands of object categories with reasonable accuracy [19, 24, 5, 6, 18]. [sent-26, score-0.263]
</p><p>9 Information from Gaze It has long been known that eye movements are not directly determined by an image, but are also influenced by task [33]. [sent-34, score-0.297]
</p><p>10 The clearest examples of this come from the extensive literature on eye movements during visual search [8, 21, 34, 35]; specifying different targets yields different patterns of eye movements even for the same image. [sent-35, score-0.654]
</p><p>11 However, clear relationships also exist between the properties of an image and the eye movements that people make during free viewing. [sent-36, score-0.469]
</p><p>12 For example, when presented with a complex scene, people overwhelmingly choose to direct their initial fixations toward the center of the image [27], probably in an attempt to maximize extraction of information from the scene [27]. [sent-37, score-0.464]
</p><p>13 Figure/ground relationships play a role as well; people prefer to look at objects even when the background is made more relevant to the task [22]. [sent-38, score-0.31]
</p><p>14 All things being equal, eye movements also tend to be directed to corners and regions of high feature density [20, 29], sudden onsets [30, 3 1], object motion [14, 15], and regions of brightness, texture, and color contrast [16, 17, 23]. [sent-39, score-0.417]
</p><p>15 The focus of our experiments is on less well explored semantic factors how categories of objects or events might influence gaze [9] and how we can use gaze to predict semantic categories. [sent-41, score-1.322]
</p><p>16 Second, the patterns of saccades and fixations made during image viewing might be used as a direct indication of content information. [sent-44, score-0.593]
</p><p>17 To the extent that gaze is drawn to oddities and inconsistencies in a scene [28], fixations might also serve to predict unusual events [1]. [sent-45, score-0.961]
</p><p>18 There are many recognition tasks that could benefit from gaze information. [sent-49, score-0.583]
</p><p>19 Rather than applying object detectors at every location in an image arbitrarily, they could be more intelligently applied only at important locations as indicated by gaze fixations. [sent-52, score-0.665]
</p><p>20 Humans can provide: • Passive indications of content through gaze patterns. [sent-56, score-0.731]
</p><p>21 In this paper we describe several combined behavioralcomputational experiments aimed at exploring the relationships between the pixels in an image, the eye movements that people make while viewing that image, and the words that they produce when asked to describe it. [sent-66, score-0.589]
</p><p>22 For these experiments we have collected gaze fixations and some descriptions for images from two commonly used computer vision datasets. [sent-68, score-0.999]
</p><p>23 Dataset & Experimental Settings We investigate the relationships between eye movements, description, image content, and computational recognition algorithms using images from two standard computer vision datasets, the Pascal VOC dataset [10] and the SUN 2009 dataset [3]. [sent-74, score-0.25]
</p><p>24 These descriptions generally describe the main image content (objects), relationships, and sometimes the overall scene. [sent-80, score-0.226]
</p><p>25 We train 22 deformable part model object detectors [12] using images with associated bounding boxes from ImageNet [7]. [sent-83, score-0.23]
</p><p>26 These categories were selected to cover, as much as possible, the main object content of our selected scene images. [sent-84, score-0.283]
</p><p>27 Eye movements were recorded during this time using a remote eye tracker (EL1000) sampling at 1000 Hz. [sent-88, score-0.32]
</p><p>28 Image descriptions were not collected from observers during the experiment, as we wanted to examine the general relationships between gaze and description that hold across different people. [sent-89, score-0.811]
</p><p>29 Figure 2 shows an example gaze pattern and description. [sent-96, score-0.583]
</p><p>30 2), and 3) What is the relationship between what people look at and what they describe? [sent-103, score-0.259]
</p><p>31 categories, and 22 classes from SUN09) represent the interesting content of these images, we first need to validate to what degree people actually look at these objects. [sent-119, score-0.321]
</p><p>32 Hence, we first compute how many fixations fall into the image regions corresponding to selected object categories. [sent-122, score-0.439]
</p><p>33 57% of fixations fall into selected object category bounding boxes for the PASCAL and Sun09 datasets respectively. [sent-125, score-0.638]
</p><p>34 Therefore, while these objects do reasonably cover human fixation locations they do not represent all of the fixated image content. [sent-126, score-0.711]
</p><p>35 Gaze vs Object Type: Here we explore which objects tend to attract the most human attention by computing the rate of fixation for each category. [sent-127, score-0.402]
</p><p>36 NF(I, b) denotes the normalized percentage of fixations of bounding box b in image I. [sent-129, score-0.54]
</p><p>37 In the SUN dataset, people are more likely to look at content elements like televisions (if they are on), people, and ovens than objects like rugs or cabinets. [sent-134, score-0.359]
</p><p>38 We also study the overall fixation rate for each category (results are shown in Figure 4). [sent-135, score-0.312]
</p><p>39 We evaluate this in two ways, 1) by computing the average percentage of fixated instances for each category (blue bars), and 2) by computing the percentage of images where at least one instance of a category was fixated when present (red bars). [sent-136, score-0.974]
</p><p>40 While viewers will probably not take the time to look at every single sheep in the image, if sheep are important then they are likely to look at at least one sheep in the image. [sent-140, score-0.371]
</p><p>41 We find that while only 45% of all sheep in images are fixated, at least one sheep is fixated in 97% of images containing sheep. [sent-141, score-0.488]
</p><p>42 We also find that object categories like person, cat, or dog are nearly always fixated on while more common scene elements like curtains or potted plants are fixated on much less frequently. [sent-142, score-0.964]
</p><p>43 Gaze vs Location on Objects:  Here we explore the  gaze patterns people produce for different object categories, examining how the patterns vary across categories, and whether bounding boxes are a reasonable representation for object localization (as indicated by gaze patterns on objects). [sent-143, score-1.647]
</p><p>44 To analyze location information from fixations, we first transform fixations into a density map. [sent-144, score-0.401]
</p><p>45 For a given image, a two-dimensional Gaussian distribution that models the human visual system with appropriately chosen parameters is centered at each fixation point. [sent-145, score-0.326]
</p><p>46 Then, a fixation density map is calculated by summing the Gaussians over the entire image. [sent-148, score-0.311]
</p><p>47 For each category, we average the fixation density maps  (a) PASCAL  (b) SUN09) Figure 4: Blue bars show the average percentage of fixated  instance per category. [sent-149, score-0.823]
</p><p>48 Red bars show the percentage of images where a category was fixated when present (at least one fixated instance in an image). [sent-150, score-0.937]
</p><p>49 (a) person(b) horse(c) tvmonitor (d) bicycle(e) chair(f) diningtable Figure 5: Examples of average fixation density maps. [sent-151, score-0.311]
</p><p>50 over the ground truth bounding boxes to create an “average” fixation density map for that category. [sent-153, score-0.459]
</p><p>51 Figure 5 shows how gaze patterns differ for example object categories. [sent-154, score-0.65]
</p><p>52 We find that when people look at an animal such as a person or horse (5a, 5b), they tend to look near the animal’s head. [sent-155, score-0.463]
</p><p>53 For some categories such as bicycle or chair (5d, 5e), which tend to have people sitting on them, we find that fixations are pulled toward the top/middle of the bounding box. [sent-156, score-0.74]
</p><p>54 For other categories like tv monitor (5c), people tend to look at the center of the object. [sent-158, score-0.34]
</p><p>55 This observation suggest that designing or training different gaze models for different categories could potentially be useful for recognizing what someone is looking at. [sent-159, score-0.726]
</p><p>56 We compute the percentage of fixations that fall into the true 7 7 7 4 4 4 20 020  AllPersonChairPainting  % of area68. [sent-161, score-0.442]
</p><p>57 We measure what percentage of the bounding box is part of the segmented object, and what percentage of the human fixations in that bounding box fall in the segmented object. [sent-170, score-0.796]
</p><p>58 We compare the extracted nouns to our selected object categories using WordNet distance [32] and keep nouns with small WordNet distance. [sent-178, score-0.204]
</p><p>59 Previous work has shown that object categories are described preferentially [2]. [sent-184, score-0.184]
</p><p>60 We examine the relationship between gaze and description by studying: 1) whether subjects look at the objects they describe, and 2) whether subjects describe the objects they look at. [sent-189, score-1.093]
</p><p>61 We find that there is a strong relationship between gaze and description in both datasets. [sent-194, score-0.686]
</p><p>62 In the PASCAL dataset, for categories aeroplane, bus, cat, cow, horse, motorbike, person, sofa, people tends to look much more in the detection boxes with high scores. [sent-202, score-0.427]
</p><p>63 For other categories, people tend to fixate evenly at detection boxes. [sent-203, score-0.23]
</p><p>64 Gaze-Enabled Computer Vision In this section, we discuss the implications of human gaze as a potential signal for two computer vision tasks object detection and image annotation. [sent-206, score-0.729]
</p><p>65 Analysis of human gaze with object detectors We first examine correlations between the confidence of visual detection systems and fixation. [sent-209, score-0.78]
</p><p>66 Positive or negative correlations give us insight into whether fixations have the potential to improve detection performance. [sent-210, score-0.449]
</p><p>67 In this experiment, we compute detection score versus fixation rate (Equation 3). [sent-211, score-0.311]
</p><p>68 in general, we find that observers look at bounding boxes with high confidence scores more often, but that detections with lower confidence scores are also sometimes fixated. [sent-213, score-0.365]
</p><p>69 As indicated by our previous studies, in general some categories are fixated more often than others, suggesting that we might focus on integrating gaze and computer vision predictions in a category specific manner. [sent-214, score-1.126]
</p><p>70 Given these observations, we also measure for what percentage of cases fixations could provide useful or detrimental evidence for object detection. [sent-215, score-0.447]
</p><p>71 In this experiment, we select the bounding boxes output by the detectors at their  selected default thresholds. [sent-216, score-0.252]
</p><p>72 For these cases, gaze cannot possibly help to improve the result, 2) There are both true positive (TP) and false pos7 7 7 4 4 4 31 131  ? [sent-218, score-0.611]
</p><p>73 Figure 7: Analysis of where gaze could potentially decrease (yellow), increase (pink), or not affect (green & blue) performance of detection. [sent-315, score-0.583]
</p><p>74 In some of these cases there will be more fixations falling into a FP box than into a TP. [sent-317, score-0.403]
</p><p>75 In these cases it is likely that adding gaze information could hurt object detection performance (yellow bars). [sent-318, score-0.704]
</p><p>76 3) In other cases, where we have more fixations in a TP box than in any other FP box, gaze has the potential to improve object detection (pink bars). [sent-319, score-1.094]
</p><p>77 4) Green bars show detections where the object detector already provides  the correct answer and no FP boxes overlap with the ground truth (therefore adding gaze will neither hurt nor help these cases). [sent-320, score-0.843]
</p><p>78 We first consider the simplest possible algorithm filter out all detected bounding boxes that do not contain any fixations (or conversely run object detectors only on parts of the image containing fixations). [sent-324, score-0.581]
</p><p>79 At the same time, it also removes a lot of true positive boxes for objects that are less likely fixated such as bottle and plant, resulting in improvements for some categories, but overall decreased detection performance (Table 3 shows detection performance on the 20 PASCAL categories). [sent-326, score-0.585]
</p><p>80 For gaze features, we first create a fixation density map for each image (as described in Section 3. [sent-329, score-0.894]
</p><p>81 To remove outliers, fixation density maps are weighted by fixation duration [ 13]. [sent-331, score-0.572]
</p><p>82 Then, we compute the average fixation density map per image across –  viewers. [sent-332, score-0.311]
</p><p>83 To compute gaze features of each detection box, we calculate the average and the maximum of the fixation density map inside of the detection box. [sent-333, score-0.994]
</p><p>84 Then, the final gaze feature of each box is a three dimensional feature vector (eg. [sent-334, score-0.635]
</p><p>85 detection score, and the average and maximum of the fixation density map). [sent-335, score-0.361]
</p><p>86 However, for training, we also consider bounding boxes with detection scores somewhat lower than the default threshold for training our gaze classifier and consider a more generous criterion (ie. [sent-339, score-0.812]
</p><p>87 We generally find gaze helps improve object detection on categories that are usually fixated while it can hurt those that are not fixated (e. [sent-350, score-1.543]
</p><p>88 Since people often look at planes, gaze-enabled classifiers could increase this confusion. [sent-355, score-0.213]
</p><p>89 Annotation Prediction We evaluate applicability of gaze to another end-user application, image annotation – outputing a set of object tags for an image. [sent-360, score-0.654]
</p><p>90 Here, we consider a successful annotation to be one that matches the set of objects a person describes when viewing the image. [sent-361, score-0.203]
</p><p>91 2), we find gaze to be a useful cue for annotation. [sent-364, score-0.583]
</p><p>92 Overall, both simple filtering and classification improve average annotation performance (Table 4), and are especially helpful for those categories that tend to draw fixations and description, e. [sent-365, score-0.515]
</p><p>93 Conclusion and Future work In this paper through a series of behavioral studies and experimental evaluations, we explored the information con-  7 7 7 4 4 4 42 242  tained in eye movements and description and analyzed their relationship with image content. [sent-372, score-0.4]
</p><p>94 We also examined the complex relationships between human gaze and outputs of current visual detection methods. [sent-373, score-0.757]
</p><p>95 Modelling search for people in 900 scenes: A combined source model of eye guidance. [sent-455, score-0.258]
</p><p>96 Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes. [sent-495, score-0.335]
</p><p>97 Gaze-enabled detection improves over the baseline for objects that people often fixate on (e. [sent-510, score-0.232]
</p><p>98 The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions. [sent-605, score-0.335]
</p><p>99 The long and the short of it: Spatial statistics at fixation vary with saccade amplitude and task. [sent-614, score-0.261]
</p><p>100 A theory of eye movements during target  acquisition. [sent-640, score-0.297]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gaze', 0.583), ('fixated', 0.374), ('fixations', 0.351), ('fixation', 0.261), ('movements', 0.152), ('eye', 0.145), ('pascal', 0.119), ('people', 0.113), ('content', 0.108), ('look', 0.1), ('categories', 0.091), ('bars', 0.076), ('bounding', 0.075), ('viewing', 0.074), ('boxes', 0.073), ('descriptions', 0.065), ('percentage', 0.062), ('relationships', 0.059), ('preferentially', 0.059), ('sheep', 0.057), ('description', 0.057), ('rashtchian', 0.056), ('person', 0.054), ('potted', 0.052), ('someone', 0.052), ('cat', 0.052), ('box', 0.052), ('category', 0.051), ('density', 0.05), ('detection', 0.05), ('detectors', 0.048), ('observers', 0.047), ('relationship', 0.046), ('chair', 0.043), ('wordnet', 0.04), ('indications', 0.04), ('sec', 0.04), ('plant', 0.04), ('detections', 0.04), ('gazeenabled', 0.039), ('inanimate', 0.039), ('iox', 0.039), ('jenc', 0.039), ('neider', 0.039), ('timeframe', 0.039), ('cognition', 0.039), ('dog', 0.039), ('objects', 0.038), ('human', 0.038), ('annotation', 0.037), ('hurt', 0.037), ('tend', 0.036), ('humans', 0.036), ('object', 0.034), ('fp', 0.034), ('itti', 0.034), ('horse', 0.033), ('patterns', 0.033), ('imagery', 0.033), ('brook', 0.032), ('onybrook', 0.032), ('stony', 0.032), ('psychology', 0.031), ('bicycle', 0.031), ('default', 0.031), ('fixate', 0.031), ('overt', 0.031), ('sometimes', 0.03), ('subjects', 0.03), ('cow', 0.03), ('nf', 0.029), ('deng', 0.029), ('fall', 0.029), ('imagenet', 0.029), ('explore', 0.029), ('amazon', 0.029), ('false', 0.028), ('positives', 0.027), ('collaborative', 0.027), ('might', 0.027), ('visual', 0.027), ('animal', 0.027), ('pink', 0.027), ('dining', 0.027), ('nouns', 0.027), ('dte', 0.027), ('attentional', 0.027), ('berg', 0.027), ('voc', 0.026), ('language', 0.026), ('inform', 0.026), ('selected', 0.025), ('motorbike', 0.025), ('potential', 0.024), ('whether', 0.024), ('amt', 0.023), ('remote', 0.023), ('surprise', 0.023), ('describe', 0.023), ('dataset', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="416-tfidf-1" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>2 0.43014577 <a title="416-tfidf-2" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>Author: Dmitry Rudoy, Dan B. Goldman, Eli Shechtman, Lihi Zelnik-Manor</p><p>Abstract: During recent years remarkable progress has been made in visual saliency modeling. Our interest is in video saliency. Since videos are fundamentally different from still images, they are viewed differently by human observers. For example, the time each video frame is observed is a fraction of a second, while a still image can be viewed leisurely. Therefore, video saliency estimation methods should differ substantially from image saliency methods. In this paper we propose a novel methodfor video saliency estimation, which is inspired by the way people watch videos. We explicitly model the continuity of the video by predicting the saliency map of a given frame, conditioned on the map from the previousframe. Furthermore, accuracy and computation speed are improved by restricting the salient locations to a carefully selected candidate set. We validate our method using two gaze-tracked video datasets and show we outperform the state-of-the-art.</p><p>3 0.18038489 <a title="416-tfidf-3" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>4 0.11444117 <a title="416-tfidf-4" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>Author: Xin Guo, Dong Liu, Brendan Jou, Mojun Zhu, Anni Cai, Shih-Fu Chang</p><p>Abstract: Object co-detection aims at simultaneous detection of objects of the same category from a pool of related images by exploiting consistent visual patterns present in candidate objects in the images. The related image set may contain a mixture of annotated objects and candidate objects generated by automatic detectors. Co-detection differs from the conventional object detection paradigm in which detection over each test image is determined one-by-one independently without taking advantage of common patterns in the data pool. In this paper, we propose a novel, robust approach to dramatically enhance co-detection by extracting a shared low-rank representation of the object instances in multiple feature spaces. The idea is analogous to that of the well-known Robust PCA [28], but has not been explored in object co-detection so far. The representation is based on a linear reconstruction over the entire data set and the low-rank approach enables effective removal of noisy and outlier samples. The extracted low-rank representation can be used to detect the target objects by spectral clustering. Extensive experiments over diverse benchmark datasets demonstrate consistent and significant performance gains of the proposed method over the state-of-the-art object codetection method and the generic object detection methods without co-detection formulations.</p><p>5 0.11144113 <a title="416-tfidf-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.1097272 <a title="416-tfidf-6" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>7 0.10881326 <a title="416-tfidf-7" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>8 0.10237738 <a title="416-tfidf-8" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>9 0.094276249 <a title="416-tfidf-9" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>10 0.085029103 <a title="416-tfidf-10" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>11 0.082420766 <a title="416-tfidf-11" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>12 0.081347018 <a title="416-tfidf-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.078516252 <a title="416-tfidf-13" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>14 0.078412697 <a title="416-tfidf-14" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>15 0.076524615 <a title="416-tfidf-15" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>16 0.072908834 <a title="416-tfidf-16" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>17 0.072489619 <a title="416-tfidf-17" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>18 0.072377615 <a title="416-tfidf-18" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>19 0.072033308 <a title="416-tfidf-19" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>20 0.07183437 <a title="416-tfidf-20" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, -0.084), (2, 0.097), (3, -0.021), (4, 0.028), (5, 0.018), (6, 0.012), (7, 0.044), (8, 0.048), (9, 0.041), (10, -0.043), (11, -0.049), (12, 0.037), (13, -0.059), (14, -0.013), (15, 0.011), (16, 0.052), (17, 0.067), (18, -0.049), (19, -0.014), (20, -0.059), (21, -0.001), (22, 0.076), (23, -0.001), (24, 0.023), (25, -0.028), (26, 0.071), (27, 0.012), (28, 0.018), (29, -0.093), (30, -0.057), (31, -0.026), (32, -0.035), (33, 0.014), (34, 0.027), (35, 0.034), (36, -0.004), (37, 0.16), (38, -0.123), (39, 0.04), (40, -0.108), (41, -0.002), (42, -0.079), (43, 0.138), (44, 0.019), (45, 0.022), (46, 0.049), (47, -0.158), (48, -0.019), (49, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89974123 <a title="416-lsi-1" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>2 0.75251764 <a title="416-lsi-2" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>3 0.68545842 <a title="416-lsi-3" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>4 0.59945744 <a title="416-lsi-4" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>5 0.58137935 <a title="416-lsi-5" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>Author: Yun Jiang, Hema Koppula, Ashutosh Saxena</p><p>Abstract: For scene understanding, one popular approach has been to model the object-object relationships. In this paper, we hypothesize that such relationships are only an artifact of certain hidden factors, such as humans. For example, the objects, monitor and keyboard, are strongly spatially correlated only because a human types on the keyboard while watching the monitor. Our goal is to learn this hidden human context (i.e., the human-object relationships), and also use it as a cue for labeling the scenes. We present Infinite Factored Topic Model (IFTM), where we consider a scene as being generated from two types of topics: human configurations and human-object relationships. This enables our algorithm to hallucinate the possible configurations of the humans in the scene parsimoniously. Given only a dataset of scenes containing objects but not humans, we show that our algorithm can recover the human object relationships. We then test our algorithm on the task ofattribute and object labeling in 3D scenes and show consistent improvements over the state-of-the-art.</p><p>6 0.56866044 <a title="416-lsi-6" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>7 0.5646385 <a title="416-lsi-7" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>8 0.51959854 <a title="416-lsi-8" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>9 0.51414675 <a title="416-lsi-9" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>10 0.51246601 <a title="416-lsi-10" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>11 0.50186372 <a title="416-lsi-11" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>12 0.4995465 <a title="416-lsi-12" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>13 0.48976383 <a title="416-lsi-13" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>14 0.48958939 <a title="416-lsi-14" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>15 0.47788081 <a title="416-lsi-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.47284564 <a title="416-lsi-16" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>17 0.4713819 <a title="416-lsi-17" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>18 0.46429068 <a title="416-lsi-18" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>19 0.46082091 <a title="416-lsi-19" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>20 0.45993444 <a title="416-lsi-20" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.097), (16, 0.033), (26, 0.053), (28, 0.018), (33, 0.221), (60, 0.211), (67, 0.089), (69, 0.066), (72, 0.015), (77, 0.013), (87, 0.073), (99, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87930459 <a title="416-lda-1" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>same-paper 2 0.82723373 <a title="416-lda-2" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>3 0.77660346 <a title="416-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.77268809 <a title="416-lda-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.7694025 <a title="416-lda-5" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>Author: Jianguo Li, Yimin Zhang</p><p>Abstract: This paper presents a novel learning framework for training boosting cascade based object detector from large scale dataset. The framework is derived from the wellknown Viola-Jones (VJ) framework but distinguished by three key differences. First, the proposed framework adopts multi-dimensional SURF features instead of single dimensional Haar features to describe local patches. In this way, the number of used local patches can be reduced from hundreds of thousands to several hundreds. Second, it adopts logistic regression as weak classifier for each local patch instead of decision trees in the VJ framework. Third, we adopt AUC as a single criterion for the convergence test during cascade training rather than the two trade-off criteria (false-positive-rate and hit-rate) in the VJ framework. The benefit is that the false-positive-rate can be adaptive among different cascade stages, and thus yields much faster convergence speed of SURF cascade. Combining these points together, the proposed approach has three good properties. First, the boosting cascade can be trained very efficiently. Experiments show that the proposed approach can train object detectors from billions of negative samples within one hour even on personal computers. Second, the built detector is comparable to the stateof-the-art algorithm not only on the accuracy but also on the processing speed. Third, the built detector is small in model-size due to short cascade stages.</p><p>6 0.7693187 <a title="416-lda-6" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>7 0.76914191 <a title="416-lda-7" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>8 0.76809752 <a title="416-lda-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.76776636 <a title="416-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.76637673 <a title="416-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.76631337 <a title="416-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.76602232 <a title="416-lda-12" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>13 0.76579028 <a title="416-lda-13" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>14 0.7657215 <a title="416-lda-14" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>15 0.76551914 <a title="416-lda-15" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>16 0.76545602 <a title="416-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.76512277 <a title="416-lda-17" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>18 0.76495254 <a title="416-lda-18" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>19 0.76388872 <a title="416-lda-19" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>20 0.76372367 <a title="416-lda-20" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
