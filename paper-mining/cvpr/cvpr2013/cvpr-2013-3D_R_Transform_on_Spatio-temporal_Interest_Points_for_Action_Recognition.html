<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-3" href="#">cvpr2013-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-3-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yuan_3D_R_Transform_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chunfeng Yuan, Xi Li, Weiming Hu, Haibin Ling, Stephen Maybank</p><p>Abstract: Spatio-temporal interest points serve as an elementary building block in many modern action recognition algorithms, and most of them exploit the local spatio-temporal volume features using a Bag of Visual Words (BOVW) representation. Such representation, however, ignorespotentially valuable information about the global spatio-temporal distribution of interest points. In this paper, we propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the ℛ transform which is defined as an extended 3D discrete Rℛa tdroann transform, followed by applying a tewdo 3-dDir decitsicorneatel two-dimensional principal component analysis. Such ℛ feature captures the geometrical information of the Sinuctehre ℛst points and keeps invariant to geometry transformation and robust to noise. In addition, we propose a new fusion strategy to combine the ℛ feature with the BOVW representation for further improving recognition accuracy. Wpree suetnilitzaea context-aware fusion method to capture both the pairwise similarities and higher-order contextual interactions of the videos. Experimental results on several publicly available datasets demonstrate the effectiveness of the proposed approach for action recognition.</p><p>Reference: <a title="cvpr-2013-3-reference" href="../cvpr2013_reference/cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Spatio-temporal interest points serve as an elementary building block in many modern action recognition algorithms, and most of them exploit the local spatio-temporal volume features using a Bag of Visual Words (BOVW) representation. [sent-9, score-0.508]
</p><p>2 Such representation, however, ignorespotentially valuable information about the global spatio-temporal distribution of interest points. [sent-10, score-0.141]
</p><p>3 In this paper, we propose a new global feature to capture the detailed geometrical distribution of interest points. [sent-11, score-0.333]
</p><p>4 It is calculated by using the ℛ transform which is defined as an extended 3D discrete Rℛa tdroann transform, followed by applying a tewdo 3-dDir decitsicorneatel two-dimensional principal component analysis. [sent-12, score-0.227]
</p><p>5 Such ℛ feature captures the geometrical information of the Sinuctehre ℛst points and keeps invariant to geometry transformation and robust to noise. [sent-13, score-0.289]
</p><p>6 In addition, we propose a new fusion strategy to combine the ℛ feature with the BOVW representation for further improving recognition accuracy. [sent-14, score-0.523]
</p><p>7 Wpree suetnilitzaea context-aware fusion method to capture both the pairwise similarities and higher-order contextual interactions of the  videos. [sent-15, score-0.463]
</p><p>8 Experimental results on several publicly available datasets demonstrate the effectiveness of the proposed approach for action recognition. [sent-16, score-0.27]
</p><p>9 Introduction Many modern action recognition approaches are based on the combination of the spatio-temporal interest point feature and the Bag of Visual Words (BOVW) model [1, 2] due to its simplicity and efficiency. [sent-18, score-0.561]
</p><p>10 , HOG, HOF [8]) encoded in spatio-temporal volumes (cuboid), however, most BOVW based representations ignores the location information of the interest points. [sent-21, score-0.178]
</p><p>11 In this paper, we propose a novel method to extract a global feature from the location information of interest points extracted from a video. [sent-22, score-0.283]
</p><p>12 We focus on the geometrical distribution of points in the 3D space and characterize the interest points from the perspective of geometry. [sent-23, score-0.371]
</p><p>13 a sWede on tuhcee e3 tDhe ed fisocrrmet ean Rda pdroonp etrrtainessf oorfm th, ea nℛd apply the 3D ℛ transform to the problem of action recognition for spatio-temporal rinmter toes tth points representation. [sent-25, score-0.594]
</p><p>14 Given an input video, the 3D ℛ transform produces a result in the faonr min pouft a 2idDe ofe,a thtuer 3eD Dm ℛatri txra. [sent-28, score-0.253]
</p><p>15 mTehde aℛs fheea tℛure fe captures the global geometrical distributionT hinef ℛorm feataitounr,e w cahpitlue rtehse BheO VgloWb representation denisctroidbeu-s the discriminability of local features. [sent-30, score-0.201]
</p><p>16 To take this benefit, we propose a new context-aware fusion strategy to combine the two features. [sent-32, score-0.376]
</p><p>17 Specifically, we first use one feature to decide the context for a video. [sent-33, score-0.146]
</p><p>18 Then, using the other feature, we design a context-aware kernel to measure the context-aware similarity between two videos. [sent-34, score-0.151]
</p><p>19 This new context-aware kernel is more robust to noise and outliers than the traditional context-free kernels, which consider only the pairwise relationships between samples. [sent-35, score-0.165]
</p><p>20 To summarize, the proposed fusion method combines two different features and captures the underlying contextual information from videos. [sent-36, score-0.442]
</p><p>21 Section 2 gives a review of the improved BOVW approaches and fusion approaches for action recognition. [sent-38, score-0.679]
</p><p>22 Section 4 discusses tihnter ovdiduecoe representations b ℛas efeda on spatio-temporal interest  points and describes the proposed fusion method. [sent-40, score-0.611]
</p><p>23 Section 5 reports experimental results on two human action datasets. [sent-41, score-0.27]
</p><p>24 The grid structure captures some simple location information, but richer geometrical distribution information is yet discarded. [sent-47, score-0.147]
</p><p>25 [10] propose to treat the interest points inside a spatiotemporal window as a point “cloud”. [sent-49, score-0.244]
</p><p>26 Therefore, a number of approaches, which propose feature fusion for improving action recognition in video sequences, have recently appeared in the literature [11, 12, 13, 14]. [sent-53, score-0.82]
</p><p>27 All the feature vectors produced by different approaches are concatenated to form a larger feature vector. [sent-55, score-0.195]
</p><p>28 [15] employ the kernel-level fusion approach and utilize a multi-kernel classifier for combining different features. [sent-59, score-0.38]
</p><p>29 The above fusion approaches rely only on the pairwise similarities of videos without considering the highorder correlations among videos. [sent-60, score-0.509]
</p><p>30 Context-aware kernel methods [16, 17] have been proposed since they take advantage of the higher-order contextual information from samples. [sent-62, score-0.159]
</p><p>31 Nevertheless, the context-aware kernel has not been explored for action recognition. [sent-64, score-0.371]
</p><p>32 3D ℛ Transform The definition of the 3D ℛ transform is based on the 3D RadTohne t draenfinsfiotriomn. [sent-69, score-0.227]
</p><p>33 The 3D discrete Radon transform is defined by summing the interpolated samples of a discrete 3D array lying on planes which satisfy certain constraints [3]. [sent-74, score-0.227]
</p><p>34 sc Greitvee nR {adxon} transform of the 3D model 푓(x) is defined by [5]: ∑퐽  푇푓(휼,휌) =  ∑푓(x푗)훿(x푗푇휼 − 휌)  (1)  ∑푗=1  where 휼 is a unit vector in 3D space, 휌 is a real number, and 훿(⋅) is the Dirac delta function. [sent-76, score-0.227]
</p><p>35 (1) is rewritten as:  휃,  휃, 푇푓(휌,휃,휙) = ∑푓(푥푗,푦푗,푡푗) 훿(푥푗 cos 휙 sin 휃 + sin 휙 sin 휃 + 푡푗 cos 휃 ∑퐽  ⋅  ∑푗=1  휌) . [sent-80, score-0.532]
</p><p>36 Therefore, we define the 3D transform as efo plalorawms:e 푦푗  −  ℛ  ℛ  ℛ푓(휃,휙) =∫∞푇푓2(휌,휃,휙)푑휌 . [sent-84, score-0.227]
</p><p>37 The 3D ℛ transform of six videos belonging to differeFnitg uarcetio 1n. [sent-93, score-0.3]
</p><p>38 vFidroemos le beftl oton right: oin dtiefrfeers-t points, ℛ transform with (휃, 휙) = [1 : 10 : 180] , and ℛ transform pwoiithn (휃, 휙) = [1 : m2 : 180] . [sent-95, score-0.454]
</p><p>39 (6)  From Equations (4)-(6), we can see: first, ℛ transform is invFarrioamnt Etoq utraatniosnlsati (o4n)-;( s6)ec,o wned c, scaling ilresatd,s ℛ t tor amplitude scaling; and third, rotation results in phase shift. [sent-97, score-0.258]
</p><p>40 To achieve  the robustness to rotation, we normalize the ℛ transform to get troheb scaling tionv roartaiatniocne, by et nheo following equation:  ℛ′푓(휃,휙) =max휃ℛ,휙푓{(ℛ휃,푓휙(휃),휙)}. [sent-98, score-0.258]
</p><p>41 (7)  These properties make the ℛ transform useful for representing tshee p rdiospterribtiuetsio mna okfe th thee in ℛte trreasnts points fsoefru alc ftoiornre recognition. [sent-99, score-0.287]
</p><p>42 3D ℛ Transform on Spatio-Temporal Interest Poℛints We propose to apply ℛ transform to 3D video sequence to dWeesc prirboep tohsee dt oist apripbulytiℛo n srtarnuscftourrme o tof t 3hDe spatio-temporal interest points extracted from a video. [sent-102, score-0.538]
</p><p>43 The minimal spatiotemporal window containing all the interest points extracted from a video is regarded as a 3D model. [sent-103, score-0.328]
</p><p>44 Denote {(푥푗, 푦푗, 푡푗)}푗퐽=1 be the positions of spatio-temporal inetneoretest { points dete)c}ted in a video, where 퐽 is the number of interest points. [sent-105, score-0.201]
</p><p>45 훿(푥푗 cos 휙 sin + 푦푗 sin 휙 sin + 푡푗 cos 휌)] (9) Observed from Eq. [sent-108, score-0.532]
</p><p>46 (9), each interest point i]s first projected into all planes with parameters (휌, 휙) and then the ℛ feature is obtained by the integral of the square of projectℛio fnesa over 휌. [sent-109, score-0.223]
</p><p>47 bTthaeirneefdor bey, tthhee i3nDte gℛra ltr oanf tshfoerm sq efficiently dece-tsicornibse osv vtehre geometrical ,d itshteri 3buDtio ℛn torafn instfoerrmest points. [sent-110, score-0.162]
</p><p>48 w The ℛ transform uses a two dimensional variable ℛ푓 T(h휃,e e휙) ℛ ℛto represent th ues edsistr aib tuwtioon doimf tehnes iiontnearelst v points. [sent-114, score-0.227]
</p><p>49 p Figure 1r ss h휃ow ansd t 휙he, 3ℛD ℛ transform of tsoix b veid aeo 2sD belonging tigou drieff 1er sehnotw wacst itohne c3Dlass ℛes t riann tshfeo rKmT oHf dataset. [sent-116, score-0.286]
</p><p>50 In the first column, all the interest points detected in a video are superposed on a single frame. [sent-117, score-0.311]
</p><p>51 It can be seen that the geometrical distribution of interest points varies according to the different action classes and is very helpful for improving the action recognition accuracy. [sent-118, score-0.888]
</p><p>52 Video Representation based temporal Interest Points  on  Spatio-  We represent each video sequence by two types of features of spatio-temporal interest points: the global ℛ feature taunrde sthoef BspOaVtioW-t representation tofp othinet slo:c thael cgulobobiadl ℛfeafetuarteusr. [sent-129, score-0.394]
</p><p>53 e We first perform the spatio-temporal interest point detection for a given video using the Harris3D detector [1]. [sent-130, score-0.225]
</p><p>54 Afterward, we employ the HOG/HOF feature [2] to describe the cuboid extracted at each interest point. [sent-131, score-0.32]
</p><p>55 So, a video 푉 is denoted as (x푖, 휶푖), 1 ≤ 푖 ≤ 푁, where x푖 is the spatiotemporal position ve)c,to 1r o≤f 푖th ≤e 푁푖푡ℎ, dwehteecreted x interest point, 휶푖 is the HOG/HOF feature, and 푁 is the total number of interest points detected in the video. [sent-132, score-0.469]
</p><p>56 The BOVW based representation only utilizes the HOG/HOF feature 휶푖 of each interest point, while the global ℛ feature utilizes spatio-temporal position hfeilaetu trhee x푖. [sent-134, score-0.333]
</p><p>57 For the detected interest points, we use the new ℛ transformFo tro t hceh aderatectcetreidze in tthereiers spatio-temporal hdeis ntreiwbu tℛion tr aannsdthen refine it by (2퐷)2PCA, as described in Section 3. [sent-140, score-0.169]
</p><p>58 The BOVW based representations rely on the discriminative power of individual local cuboid features, whilst the ℛ featpuorwese exploit tvhied global spatio-temporal ,d wishtriilbsutt tihoen ℛof f tehaeinterest points. [sent-142, score-0.145]
</p><p>59 e, lhaeb eℛls f oeaft tuhree test videos from the labeled training videos according to the between-video similarity. [sent-149, score-0.146]
</p><p>60 A crucial step is to compute the  similarity between videos, and, since kernel-based classifier is used in our study, to build a kernel matrix from the similarity measure. [sent-150, score-0.201]
</p><p>61 Traditionally, a kernel matrix is computed based on the pairwise comparison between videos, but such a kernel matrix can be sensitive to noise, outliers, etc. [sent-151, score-0.23]
</p><p>62 Addressing this issue, we propose a context-aware feature fusion method which not only combines the two feature representations but also captures the underlying contextual information from videos. [sent-152, score-0.643]
</p><p>63 The proposed context-aware feature fusion method includes two steps: context selection and context-aware kernel construction. [sent-153, score-0.594]
</p><p>64 In order to efficiently combine the obtained two features of a video, we use one feature to compute the context of each video and use the other feature to calculate the context-aware kernel for action recognition. [sent-154, score-0.712]
</p><p>65 The context of each video is computed using the 푘 nearest neighbor method. [sent-155, score-0.148]
</p><p>66 We obtain the 푟 nearest neighboring videos of each video in one feature space as its context. [sent-156, score-0.239]
</p><p>67 Let 푉푖1, 푉푗1, 푉푖2 and 푉푗2 denote two feature representations of two video sequences 푖 and Let 풩푗 denote the 푟 nearest neighboring vqiudeenocse osf 푖 푖vi adnedo 푗 LTheten 풩 풩푗 is computed by the following equation:  푗. [sent-157, score-0.203]
</p><p>68 Secondly, the context-aware kernel is composed of three  푗  parts, the similarity between the two videos, the similarity between the first video and the context of the second video and vice versa. [sent-160, score-0.433]
</p><p>69 The similarity between one video and the context of the other video is defined as:  푘푁(푉푖2,푉풩2푗) =1푟푚∑∈풩푗푘(푉푖2,푉푚2). [sent-162, score-0.282]
</p><p>70 Experiments We test our approach on three human action datasets: KTH [20], UCF sports dataset [21], and UCF films [21]. [sent-176, score-0.343]
</p><p>71 Parameter Evaluation of the ℛ Feature There are two parameters 휃 and 휙 in ℛ transform during theT computation o pfa trahem proposed dℛ 휙 f einat ℛure t. [sent-180, score-0.227]
</p><p>72 The blue curve is the obtained recognition accuracy using the ℛ transform feature without (re2c퐷o)gn2PitCioAn, acncdu tahcey r uesdi one ies ℛ recognition accuracy using the (2퐷)2PCA to refine the ℛ transform feature. [sent-187, score-0.638]
</p><p>73 67% is obtained under (휃, 휙) = [1 : 10 : 180] ; ii) the features obtained by (2퐷)2PCA gain a higher recognition accuracy in most cases than the ℛ transform freecatougrneist on tahceciru own. [sent-189, score-0.264]
</p><p>74 It demonstrates that ℛ transform feature is an effective descriptor arnatde sth teh a(t2 퐷ℛ) t2rPaCnsAfo rfmurth feera improves tehfefe dcitsivcerim dei-native of the ℛ transform feature. [sent-193, score-0.536]
</p><p>75 1 I : 1al0l : t1h8e0r] eaxnpde employ (2퐷)2PCA on ℛ transform as the final ℛ feature. [sent-195, score-0.26]
</p><p>76 Parameter Evaluation Feature Fusion  of the Context-aware  We evaluate the parameter 푟, which is the number of neighbors for context construction in fusion method, on the KTH dataset. [sent-199, score-0.411]
</p><p>77 (1 T 1h)e ea nℛd tehaet uBreOV caWn feea utusered for kernel calculation according to Eq. [sent-204, score-0.2]
</p><p>78 t Namely, we employ the same type offeature for both context calculation and kernel calculation, referred to as ‘BOVW+BOVW’ or ’ℛ+ℛ’ respectively. [sent-210, score-0.271]
</p><p>79 In each experiment, we use the first feature of the legend to calculate the context by Eq. [sent-212, score-0.146]
</p><p>80 Namely, ‘BOVW+ℛ’ and ‘ℛ+BOVW’ are our proposed fusion approaches; ℛw’h ailned ‘B ‘ℛO+VBWO+VBWOV’ aWre’ oaunrd p ‘rℛop+oℛse’ are tihoen one fperaotaucrhee ebs;as wedh ialend ‘ BcoOnVteWxt aware approaches. [sent-215, score-0.443]
</p><p>81 Experiments on the KTH Database To evaluate the two proposed fusion methods, we compared our fusion approaches with two other feature fusion approaches: the feature-level fusion approach [11] [12] and the similarity kernel-level fusion approach. [sent-222, score-1.898]
</p><p>82 All of these fusion approaches combine the proposed two features. [sent-223, score-0.407]
</p><p>83 Specifically, the feature-level fusion approach concatenates the two normalized feature vectors to form a larger feature vector as input to the SVM classifier. [sent-224, score-0.511]
</p><p>84 (15)  ∑푚  Table 1 lists the recognition accuracies of eight approaches on KTH dataset, including one feature based approaches (e. [sent-226, score-0.22]
</p><p>85 , ‘BOVW’, ‘ℛ’), One feature based and contperxota aware approaches (e. [sent-228, score-0.165]
</p><p>86 , O‘fVeaWtu’r,e- ‘lℛev+elℛ f’u)-, sion’, ‘kernel-level fusion’), and our proposed fusion approaches (e. [sent-232, score-0.378]
</p><p>87 Namely, the proposed context aware kernel method obtains higher performance than the traditional context-free kernel method. [sent-246, score-0.348]
</p><p>88 Recognition accuracies obtained by the ℛ transform featFuigreuraend 2t. [sent-248, score-0.266]
</p><p>89 tu Irte s are complementary taunrde are dfe tahseib BleO VtoW W becombined for action recognition. [sent-263, score-0.329]
</p><p>90 Our fusion approach (‘ℛ+BOVW’) obtains higher accuracy ntha anp trhoea hoth (e‘rℛ +twBoO common fiunssio hni approaches, which demonstrates the effectiveness of our proposed fusion strategy. [sent-264, score-0.776]
</p><p>91 Moreover, our fusion approach with the ℛ feature for context calculation apnroda cthhe wBitOhVtW he f ℛeatu freea fuorre k foerrne clo cnatelcxtul cataiolcnu (e. [sent-265, score-0.566]
</p><p>92 Experiments on the UCF Sports Dataset The UCF sports database is tested in a leave-one-out manner, cycling each example in as a test video one at a time, following [21] [22] [23]. [sent-270, score-0.183]
</p><p>93 The similar results as in KTH are obtained in the Table 2, which demonstrate the effectiveness of our proposed cloud feature and fusion method on the realistic and complicated dataset. [sent-274, score-0.477]
</p><p>94 Conclusion In this paper we presented a new action recognition framework based on spatio-temporal interest points. [sent-300, score-0.448]
</p><p>95 We first proposed a new holistic video representation, the 3D ℛ transform on spatio-temporal interest points, to capture tℛhe t irnafnosfromramtio onn o spf tahtieo global geometrical doiinsttrsi,b utotio cna. [sent-301, score-0.588]
</p><p>96 p uWree then proposed a new fusion strategy to combine the local cuboid feature and the global ℛ feature for action recognitciuobno. [sent-302, score-0.874]
</p><p>97 Experimental results on several datasets have demonstrated the effectiveness of our proposed ℛ feature and fusion method. [sent-304, score-0.429]
</p><p>98 Fusing appearance and distribution information of interest points for action recognition. [sent-419, score-0.471]
</p><p>99 Action MACH: a spatiotemporal maximum average correlation height filter for action recognition. [sent-465, score-0.313]
</p><p>100 Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. [sent-486, score-0.307]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bovw', 0.597), ('fusion', 0.347), ('action', 0.27), ('transform', 0.227), ('ucf', 0.163), ('interest', 0.141), ('radon', 0.138), ('sin', 0.122), ('bregonzio', 0.119), ('geometrical', 0.11), ('kth', 0.109), ('kernel', 0.101), ('video', 0.084), ('cos', 0.083), ('feature', 0.082), ('videos', 0.073), ('sports', 0.073), ('context', 0.064), ('cuboid', 0.064), ('points', 0.06), ('taunrde', 0.059), ('tshfeo', 0.059), ('contextual', 0.058), ('tthhee', 0.052), ('aware', 0.052), ('similarity', 0.05), ('cloud', 0.048), ('laptev', 0.048), ('calculation', 0.047), ('codebook', 0.044), ('tihoen', 0.044), ('spatiotemporal', 0.043), ('accuracies', 0.039), ('captures', 0.037), ('representations', 0.037), ('recognition', 0.037), ('outliers', 0.036), ('ure', 0.035), ('svm', 0.035), ('aser', 0.035), ('pca', 0.034), ('employ', 0.033), ('approaches', 0.031), ('scaling', 0.031), ('kernels', 0.03), ('similarities', 0.03), ('definite', 0.03), ('grant', 0.03), ('obtains', 0.03), ('combine', 0.029), ('ree', 0.029), ('namely', 0.028), ('fusing', 0.028), ('refine', 0.028), ('representation', 0.028), ('pairwise', 0.028), ('aenlyd', 0.026), ('aint', 0.026), ('andg', 0.026), ('aosr', 0.026), ('atri', 0.026), ('awndith', 0.026), ('awre', 0.026), ('birkbeck', 0.026), ('breelo', 0.026), ('cfyuan', 0.026), ('chunfeng', 0.026), ('cycling', 0.026), ('ebdo', 0.026), ('eke', 0.026), ('eofr', 0.026), ('faonr', 0.026), ('feea', 0.026), ('frangi', 0.026), ('fuorre', 0.026), ('haibin', 0.026), ('hbling', 0.026), ('hni', 0.026), ('hwei', 0.026), ('jeon', 0.026), ('mga', 0.026), ('ntha', 0.026), ('offeature', 0.026), ('pcm', 0.026), ('rism', 0.026), ('rna', 0.026), ('rtehse', 0.026), ('rvi', 0.026), ('sfoetr', 0.026), ('spf', 0.026), ('superposed', 0.026), ('tchha', 0.026), ('tdra', 0.026), ('tehaet', 0.026), ('tihnter', 0.026), ('tihvee', 0.026), ('tiorann', 0.026), ('tohsee', 0.026), ('trhaien', 0.026), ('troet', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000019 <a title="3-tfidf-1" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>Author: Chunfeng Yuan, Xi Li, Weiming Hu, Haibin Ling, Stephen Maybank</p><p>Abstract: Spatio-temporal interest points serve as an elementary building block in many modern action recognition algorithms, and most of them exploit the local spatio-temporal volume features using a Bag of Visual Words (BOVW) representation. Such representation, however, ignorespotentially valuable information about the global spatio-temporal distribution of interest points. In this paper, we propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the ℛ transform which is defined as an extended 3D discrete Rℛa tdroann transform, followed by applying a tewdo 3-dDir decitsicorneatel two-dimensional principal component analysis. Such ℛ feature captures the geometrical information of the Sinuctehre ℛst points and keeps invariant to geometry transformation and robust to noise. In addition, we propose a new fusion strategy to combine the ℛ feature with the BOVW representation for further improving recognition accuracy. Wpree suetnilitzaea context-aware fusion method to capture both the pairwise similarities and higher-order contextual interactions of the videos. Experimental results on several publicly available datasets demonstrate the effectiveness of the proposed approach for action recognition.</p><p>2 0.25281852 <a title="3-tfidf-2" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>Author: Dong Liu, Kuan-Ting Lai, Guangnan Ye, Ming-Syan Chen, Shih-Fu Chang</p><p>Abstract: Late fusion addresses the problem of combining the prediction scores of multiple classifiers, in which each score is predicted by a classifier trained with a specific feature. However, the existing methods generally use a fixed fusion weight for all the scores of a classifier, and thus fail to optimally determine the fusion weight for the individual samples. In this paper, we propose a sample-specific late fusion method to address this issue. Specifically, we cast the problem into an information propagation process which propagates the fusion weights learned on the labeled samples to individual unlabeled samples, while enforcing that positive samples have higher fusion scores than negative samples. In this process, we identify the optimal fusion weights for each sample and push positive samples to top positions in the fusion score rank list. We formulate our problem as a L∞ norm constrained optimization problem and apply the Alternating Direction Method of Multipliers for the optimization. Extensive experiment results on various visual categorization tasks show that the proposed method consis- tently and significantly beats the state-of-the-art late fusion methods. To the best knowledge, this is the first method supporting sample-specific fusion weight learning.</p><p>3 0.21497399 <a title="3-tfidf-3" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>Author: Chunfeng Yuan, Weiming Hu, Guodong Tian, Shuang Yang, Haoran Wang</p><p>Abstract: In this paper, we formulate human action recognition as a novel Multi-Task Sparse Learning(MTSL) framework which aims to construct a test sample with multiple features from as few bases as possible. Learning the sparse representation under each feature modality is considered as a single task in MTSL. Since the tasks are generated from multiple features associated with the same visual input, they are not independent but inter-related. We introduce a Beta process(BP) prior to the hierarchical MTSL model, which efficiently learns a compact dictionary and infers the sparse structure shared across all the tasks. The MTSL model enforces the robustness in coefficient estimation compared with performing each task independently. Besides, the sparseness is achieved via the Beta process formulation rather than the computationally expensive l1 norm penalty. In terms of non-informative gamma hyper-priors, the sparsity level is totally decided by the data. Finally, the learning problem is solved by Gibbs sampling inference which estimates the full posterior on the model parameters. Experimental results on the KTH and UCF sports datasets demonstrate the effectiveness of the proposed MTSL approach for action recognition.</p><p>4 0.21129525 <a title="3-tfidf-4" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>Author: Feng Shi, Emil Petriu, Robert Laganière</p><p>Abstract: Local spatio-temporal features and bag-of-features representations have become popular for action recognition. A recent trend is to use dense sampling for better performance. While many methods claimed to use dense feature sets, most of them are just denser than approaches based on sparse interest point detectors. In this paper, we explore sampling with high density on action recognition. We also investigate the impact of random sampling over dense grid for computational efficiency. We present a real-time action recognition system which integrates fast random sampling method with local spatio-temporal features extracted from a Local Part Model. A new method based on histogram intersection kernel is proposed to combine multiple channels of different descriptors. Our technique shows high accuracy on the simple KTH dataset, and achieves state-of-the-art on two very challenging real-world datasets, namely, 93% on KTH, 83.3% on UCF50 and 47.6% on HMDB51.</p><p>5 0.1973924 <a title="3-tfidf-5" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>Author: Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</p><p>Abstract: How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What defines these spatiotemporal patches is their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore, these patches can be used as a discriminative vocabulary for action classification where they demonstrate stateof-the-art performance on UCF50 and Olympics datasets.</p><p>6 0.19403787 <a title="3-tfidf-6" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>7 0.19186334 <a title="3-tfidf-7" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>8 0.18585868 <a title="3-tfidf-8" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>9 0.18470289 <a title="3-tfidf-9" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>10 0.18426664 <a title="3-tfidf-10" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>11 0.14773943 <a title="3-tfidf-11" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>12 0.13612586 <a title="3-tfidf-12" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>13 0.13383223 <a title="3-tfidf-13" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>14 0.13332312 <a title="3-tfidf-14" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>15 0.12586761 <a title="3-tfidf-15" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>16 0.1174986 <a title="3-tfidf-16" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>17 0.11565858 <a title="3-tfidf-17" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>18 0.11515068 <a title="3-tfidf-18" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>19 0.11307342 <a title="3-tfidf-19" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>20 0.10973936 <a title="3-tfidf-20" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.068), (2, -0.032), (3, -0.109), (4, -0.237), (5, 0.007), (6, -0.089), (7, -0.013), (8, -0.097), (9, -0.067), (10, 0.003), (11, -0.065), (12, -0.017), (13, -0.009), (14, -0.075), (15, -0.053), (16, -0.024), (17, -0.022), (18, 0.072), (19, 0.11), (20, 0.001), (21, -0.006), (22, -0.0), (23, 0.068), (24, 0.053), (25, -0.023), (26, -0.027), (27, 0.043), (28, 0.002), (29, -0.042), (30, -0.04), (31, 0.024), (32, 0.028), (33, -0.009), (34, -0.035), (35, 0.006), (36, 0.026), (37, -0.012), (38, 0.072), (39, -0.039), (40, 0.051), (41, -0.081), (42, -0.028), (43, -0.01), (44, 0.046), (45, 0.047), (46, 0.031), (47, 0.15), (48, -0.118), (49, 0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93984145 <a title="3-lsi-1" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>Author: Chunfeng Yuan, Xi Li, Weiming Hu, Haibin Ling, Stephen Maybank</p><p>Abstract: Spatio-temporal interest points serve as an elementary building block in many modern action recognition algorithms, and most of them exploit the local spatio-temporal volume features using a Bag of Visual Words (BOVW) representation. Such representation, however, ignorespotentially valuable information about the global spatio-temporal distribution of interest points. In this paper, we propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the ℛ transform which is defined as an extended 3D discrete Rℛa tdroann transform, followed by applying a tewdo 3-dDir decitsicorneatel two-dimensional principal component analysis. Such ℛ feature captures the geometrical information of the Sinuctehre ℛst points and keeps invariant to geometry transformation and robust to noise. In addition, we propose a new fusion strategy to combine the ℛ feature with the BOVW representation for further improving recognition accuracy. Wpree suetnilitzaea context-aware fusion method to capture both the pairwise similarities and higher-order contextual interactions of the videos. Experimental results on several publicly available datasets demonstrate the effectiveness of the proposed approach for action recognition.</p><p>2 0.73506755 <a title="3-lsi-2" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>3 0.72527105 <a title="3-lsi-3" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>Author: Feng Shi, Emil Petriu, Robert Laganière</p><p>Abstract: Local spatio-temporal features and bag-of-features representations have become popular for action recognition. A recent trend is to use dense sampling for better performance. While many methods claimed to use dense feature sets, most of them are just denser than approaches based on sparse interest point detectors. In this paper, we explore sampling with high density on action recognition. We also investigate the impact of random sampling over dense grid for computational efficiency. We present a real-time action recognition system which integrates fast random sampling method with local spatio-temporal features extracted from a Local Part Model. A new method based on histogram intersection kernel is proposed to combine multiple channels of different descriptors. Our technique shows high accuracy on the simple KTH dataset, and achieves state-of-the-art on two very challenging real-world datasets, namely, 93% on KTH, 83.3% on UCF50 and 47.6% on HMDB51.</p><p>4 0.71355605 <a title="3-lsi-4" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>Author: LiMin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motionlet, a mid-level and spatiotemporal part, for human motion recognition. Motionlet can be seen as a tight cluster in motion and appearance space, corresponding to the moving process of different body parts. We postulate three key properties of motionlet for action recognition: high motion saliency, multiple scale representation, and representative-discriminative ability. Towards this goal, we develop a data-driven approach to learn motionlets from training videos. First, we extract 3D regions with high motion saliency. Then we cluster these regions and preserve the centers as candidate templates for motionlet. Finally, we examine the representative and discriminative power of the candidates, and introduce a greedy method to select effective candidates. With motionlets, we present a mid-level representation for video, called motionlet activation vector. We conduct experiments on three datasets, KTH, HMDB51, and UCF50. The results show that the proposed methods significantly outperform state-of-the-art methods.</p><p>5 0.70255846 <a title="3-lsi-5" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>6 0.69019294 <a title="3-lsi-6" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>7 0.68343091 <a title="3-lsi-7" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>8 0.66536194 <a title="3-lsi-8" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>9 0.66533989 <a title="3-lsi-9" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>10 0.64876944 <a title="3-lsi-10" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>11 0.62586707 <a title="3-lsi-11" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>12 0.61958444 <a title="3-lsi-12" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>13 0.5762372 <a title="3-lsi-13" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>14 0.56741083 <a title="3-lsi-14" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>15 0.55973744 <a title="3-lsi-15" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>16 0.55510163 <a title="3-lsi-16" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>17 0.51321334 <a title="3-lsi-17" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>18 0.50210387 <a title="3-lsi-18" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>19 0.49970907 <a title="3-lsi-19" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>20 0.48877266 <a title="3-lsi-20" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.592), (16, 0.011), (26, 0.024), (33, 0.219), (67, 0.04), (69, 0.024), (87, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92838579 <a title="3-lda-1" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>2 0.9144603 <a title="3-lda-2" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>3 0.90776867 <a title="3-lda-3" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>4 0.90206611 <a title="3-lda-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.89795452 <a title="3-lda-5" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>6 0.88358021 <a title="3-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>same-paper 7 0.86689299 <a title="3-lda-7" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>8 0.86247092 <a title="3-lda-8" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>9 0.82581431 <a title="3-lda-9" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>10 0.82463402 <a title="3-lda-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.80272526 <a title="3-lda-11" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>12 0.76609933 <a title="3-lda-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.76170135 <a title="3-lda-13" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>14 0.7457552 <a title="3-lda-14" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>15 0.74355412 <a title="3-lda-15" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>16 0.72507572 <a title="3-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.7220757 <a title="3-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.71303713 <a title="3-lda-18" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>19 0.70891833 <a title="3-lda-19" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>20 0.70822859 <a title="3-lda-20" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
