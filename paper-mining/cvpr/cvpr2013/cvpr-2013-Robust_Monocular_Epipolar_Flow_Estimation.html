<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-362" href="#">cvpr2013-362</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</h1>
<br/><p>Source: <a title="cvpr-2013-362-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yamaguchi_Robust_Monocular_Epipolar_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>Reference: <a title="cvpr-2013-362-reference" href="../cvpr2013_reference/cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu e c  Abstract We consider the problem of computing optical flow in monocular video taken from a moving vehicle. [sent-4, score-0.563]
</p><p>2 In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. [sent-5, score-0.501]
</p><p>3 We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. [sent-6, score-1.042]
</p><p>4 Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. [sent-8, score-0.46]
</p><p>5 We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm. [sent-9, score-2.034]
</p><p>6 Introduction Optical flow is an important classical problem in computer vision, as it can be used in support of 3D reconstruction, perceptual grouping and object recognition. [sent-11, score-0.46]
</p><p>7 In  this setting, most of the flow can be explained by the vehicle’s ego-motion. [sent-13, score-0.46]
</p><p>8 As a consequence, once the ego-motion is computed, one can treat flow as a matching problem along epipolar lines. [sent-14, score-1.018]
</p><p>9 The main difference with stereo vision resides in the fact that the epipolar lines radiate from a single epipole, called the focus of expansion (FOE). [sent-15, score-0.691]
</p><p>10 A few attempts to utilize these constraints have been proposed [22], mainly in the context of scene flow (i. [sent-16, score-0.46]
</p><p>11 However, so far, we have not witnessed big performance gains by employing the epipolar constraints. [sent-19, score-0.546]
</p><p>12 In contrast, we take advantage of recent developments in stereo vision to construct robust solutions to the epipolar flow problem. [sent-20, score-1.122]
</p><p>13 Our first contribution is to adapt slanted plane stereo models [39, 2] to the problem of monocular epipolar flow estimation. [sent-22, score-1.351]
</p><p>14 This allow us to exploit global energy minimization methods in order to alleviate problems in texture-less regions and produce dense flow fields. [sent-23, score-0.509]
</p><p>15 In particular, we represent the problem as one of inference in a hybrid Markov random field (MRF), where a slanted plane represents the epipolar flow for each segment and discrete random variables represent the boundary relations between each pair of neighboring segments (i. [sent-24, score-1.372]
</p><p>16 The introduction of these boundary variables allows the model to reason about ownerships of the boundary as well as to enforce physical validity of the boundary types at junctions. [sent-27, score-0.194]
</p><p>17 In order to produce accurate results, slanted plane MRF  models require a good over-segmentation of the image, where the planar assumption for each superpixel is approximately satisfied. [sent-28, score-0.303]
</p><p>18 Towards this goal, our second contribution is an efficient flow-aware segmentation algorithm in the spirit of SLIC [1], but where the segmentation energy involves both image and flow terms. [sent-29, score-0.509]
</p><p>19 This encourages the segmentation to respect both image and flow discontinuities. [sent-30, score-0.46]
</p><p>20 Our last contribution is a local flow matching algorithm, inspired by the very successful stereo algorithm semi-global block matching [20], which computes very accurate semi-dense flow fields. [sent-32, score-1.177]
</p><p>21 We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best competing epipolar flow algorithm. [sent-33, score-2.079]
</p><p>22 In the remainder of the paper, we first review related work and present our local epipolar flow algorithm. [sent-34, score-0.981]
</p><p>23 We then discuss our unsupervised segmentation algorithm which preserves epipolar flow discontinuities, and present our slanted plane MRF formulation. [sent-35, score-1.186]
</p><p>24 Related Work Over the past few decades we have witnessed a great improvement in performance of flow algorithms. [sent-38, score-0.485]
</p><p>25 While existing many works use a variational approach for continuous flow optimization [21, 5, 6, 41], a number of recent approaches have proposed discrete MRF formulations [26, 35, 14, 25]. [sent-44, score-0.579]
</p><p>26 The problem is more severe than in stereo, as instead of 1D disparities, a 2D flow field has to be discretized. [sent-46, score-0.46]
</p><p>27 [14, 25] use a coarse-to-fine approach and sampling, while [26, 35] create a set of candidate flow estimates by standard continuous optical flow algorithms. [sent-47, score-1.049]
</p><p>28 When dealing with mostly static scenes, optical flow can be expressed as a 3D rigid motion due to the camera motion. [sent-48, score-0.573]
</p><p>29 The knowledge of this epipolar geometry has been in-  troduced as a soft constraint in the energy function [36, 37] or as a hard constraint [33, 22]. [sent-49, score-0.57]
</p><p>30 In the latter, first the fundamental matrix is calculated and the flow estimation is formulated as a 1D search by restricting a corresponding point to lie on the epipolar line. [sent-50, score-1.046]
</p><p>31 While a soft constraint can yield less errors in independently moving objects, hard constraints can reduce computational complexity and achieve robust estimation of flow in stationary objects if the fundamental matrix is accurately estimated. [sent-51, score-0.525]
</p><p>32 In this paper we take the latter approach and adapt the highly successful slantedplane MRF approach to stereo vision for the problem of epipolar flow estimation. [sent-52, score-1.122]
</p><p>33 Semi-global Block Matching for Flow In this section we extend the popular stereo algorithm, semi-global block matching [20] to tackle the epipolar flow problem. [sent-55, score-1.201]
</p><p>34 In particular, we first convert the estimation from a 2D matching problem to a 1D search along the epipolar lines, which are defined by the vehicle’s ego-motion. [sent-56, score-0.593]
</p><p>35 We then define parameterizations and cost functions which are appropriate for epipolar flow. [sent-57, score-0.552]
</p><p>36 Epipolar Flow as a 1D Search Problem The first step of our algorithm consists on estimating the fundamental matrix that defines the set of epipolar lines. [sent-60, score-0.551]
</p><p>37 We then estimate the parameters of the flow that is due to camera rotation, and pose the flow problem as a 1D search along the translational flow component. [sent-62, score-1.469]
</p><p>38 Assuming that the camera rotation between two images is small, uw (p) can be expressed as follows [27],  uw(p) =  ? [sent-65, score-0.23]
</p><p>39 xTh =us, x we can yw =rite y uw c(p) as a 5-parameter model. [sent-68, score-0.172]
</p><p>40 An additional constraint that we can exploit to estimate the rotational component of the flow is given by the fact that uv (p, Zp) is parallel to the epipolar line passing though that point at time t+1. [sent-71, score-1.123]
</p><p>41 Thus, as uv (p, Zp) being parallel to the epipolar line ? [sent-75, score-0.631]
</p><p>42 ( p˜ + u˜w(p)) = 0 (2) with u˜w (p) representing uw (p) in homogeneous coordinates. [sent-82, score-0.197]
</p><p>43 Once this is done, we only need to estimate the flow in the direction of the epipolar lines. [sent-88, score-1.013]
</p><p>44 Semi-global Block Matching for Flow We now discuss how we can adapt the semi-global block matching stereo algorithm (SGM) [20] to estimate the trans-  lational component of flow. [sent-92, score-0.252]
</p><p>45 We need to define a good parameterization and a good cost function for epipolar flow. [sent-96, score-0.596]
</p><p>46 In the case of flow, using this parameterization leads to the interaction between the epipolar geometry and the scene depth, as the disparity at each point is a complex non-linear function of depth Zp. [sent-98, score-0.645]
</p><p>47 (p) a unit vector in the direction of the epipolar line ? [sent-102, score-0.549]
</p><p>48 (p) and d(p, Zp) the disparity along the epipolar line. [sent-104, score-0.601]
</p><p>49 Fig 1 (left) shows the epipolar geometry of two images, where C and C? [sent-105, score-0.521]
</p><p>50 Adding the rotation flow vector uw (p) to each pixel p means that the image plane at time t is rotated so that its camera direction is the same as the one at time t + 1, as shown in Fig. [sent-109, score-0.803]
</p><p>51 As a result, the epipole and the epipolar line in the rotated image at time t are exactly the same as those in the image at time t + 1. [sent-111, score-0.601]
</p><p>52 2 shows the geometric configuration on the epipolar plane, where r and r? [sent-113, score-0.521]
</p><p>53 We can then compute the disparity as vz vz  d(p,Zp) = r? [sent-123, score-0.27]
</p><p>54 Since the z-component vz of the camera translation is constant for all pixels, the ratio Zvpz, denoted VZ-ratio, depends only on the distance Zp. [sent-127, score-0.155]
</p><p>55 (3))  represents the scene independent of the epipolar geometry. [sent-129, score-0.521]
</p><p>56 Next, we need to define a cost function adequate for estimating the epipolar flow. [sent-135, score-0.552]
</p><p>57 (q, ωq) = q + uw (q) + uv (q, Zq; ωq) is the corresponding pixel in the second image whose VZ-index is ωq, is a constant, W(p) is a window centered at pixel p and G(·) ias ctohen dtairnetc,t Wion(apl) )d iesri av watiinvde oinw th ceen image itn p tihxee ld pire acntdion G (o·)f the epipolar line. [sent-145, score-0.845]
</p><p>58 Using lower penalties for small changes permits an adaptation to slanted or curved surfaces. [sent-153, score-0.155]
</p><p>59 The flow can then be estimated by solving for the disparities {ωp} by minimizing the energy in Eq. [sent-154, score-0.577]
</p><p>60 This provides the sets Ft and Ft+1 of the pixels, whose flow has been estsiemtsat Fed, aanndd FVZ-indices ωˆt (p) and ωˆt+1 (p? [sent-162, score-0.46]
</p><p>61 111888666422  Algorithm 1 MotionSLIC  Init superpixels by sampling pixels in a regular grid for i= 1to #iterations do for all pixel p do sp = argminiE(p, i, θi , μi , ci)  end for for all super? [sent-165, score-0.306]
</p><p>62 Joint Segmentation and Flow Estimation Given an estimate of the flow in a subset of the pixels, we are interested in computing an over-segmentation of the image that respects both flow and image boundaries. [sent-171, score-0.979]
</p><p>63 This over-segmentation will be used in the next section by our slanted-plane MRF model in order to produce more accurate dense flow estimations. [sent-172, score-0.46]
</p><p>64 Towards this goal, we represent the VZ-index of each superpixel with a slanted plane, ω(p, θsp)  = αspx + βspy + γsp,  (4)  defined with parameters θsp = (αsp , βsp , γsp ), where sp indexes the superpixel that pixel p belongs to. [sent-173, score-0.553]
</p><p>65 We frame joint unsupervised segmentation and flow estimation as an energy minimization problem, and define the energy of each pixel as the sum of energies encoding shape, appearance and flow, taking special care into modeling occlusions. [sent-176, score-0.628]
</p><p>66 The input to our algorithm is the two images as well as our initial (possibly sparse) flow estimate ˆω (see sec-  ×  tion 3). [sent-177, score-0.492]
</p><p>67 Flow: This potential enforces that the plane parameters should agree with the input flow ωˆt (p) as follows  Edtisp(p,θsp) =? [sent-215, score-0.571]
</p><p>68 2  ioft δhneorw(pis,eθsp)  We can define the total energy of a pixel as E = Ectol(p, csp) + Ect+ol1 (p, csp , θsp) + λposEpos(p, μsp) + λdisp  λpos  ? [sent-222, score-0.212]
</p><p>69 blem of joint unsupervised segmentation and flow estimation becomes  Θm,S,inμ,c? [sent-227, score-0.495]
</p><p>70 We derive an iterative scheme that works in three steps: first we minimize the energy with respect to the assignments, we update the parameters μsp , csp by simply computing their means, and then compute the plane parameters by using a robust estimator. [sent-231, score-0.255]
</p><p>71 This algorithm can be extended to stereo vision by simply replacing the two consecutive frames with the left and right images of the stereo pair, and the VZ-ratio with disparity. [sent-233, score-0.282]
</p><p>72 Our experimental evaluation will demonstrate the effectiveness of our method in both epipolar flow and stereo estimation problems. [sent-234, score-1.157]
</p><p>73 Recently, [39] proposed a slanted-plane MRF model for stereo vision that reasons about segments as well as occlusion boundaries. [sent-237, score-0.176]
</p><p>74 Here we follow a similar idea, and represent the epipolar flow estimation problem as inference in a mixed continuous-discrete  random field. [sent-238, score-1.016]
</p><p>75 The continuous variables represent 3D planes encoding the VZ-ratio, while the discrete variables encode the type of boundaries between pairs of superpixels. [sent-239, score-0.217]
</p><p>76 Our approach takes as input epipolar flow as well as an oversegmentation of the image. [sent-240, score-0.981]
</p><p>77 In particular, we employ the epipolar flow fields and segmentations estimated by MotionSLIC (see section 4). [sent-241, score-1.043]
</p><p>78 Let oi,j ∈ {co, hi, lo, ro} be a discrete random variable representing {wchoe,thhei,r tow,or neighboring planes are coplanar, f roerpmre a hinge or an occlusion boundary. [sent-246, score-0.202]
</p><p>79 Here, lo implies that plane ioccludes plane j, and ro the opposite. [sent-247, score-0.156]
</p><p>80 We define our hybrid conditional random field in terms of all slanted-planes and boundary variables and encode potentials over sets of continuous, discrete or mixture of both types of variables. [sent-248, score-0.175]
</p><p>81 VZ-ratio: We define truncated quadratic potentials for each segment encoding that the plane should agree with the  epipolar flow estimated using the algorithm from section 3. [sent-250, score-1.19]
</p><p>82 Boundary: We employ 3-way potentials linking our discrete and continuous variables expressing the fact that when 111888666644  O u rs MSPGCotBiMPo-nFSlLoIwCN76o. [sent-251, score-0.191]
</p><p>83 two neighboring planes are hinge or coplanar they should agree on the boundary, and when a segment occludes another, the boundary should be explained by the occluder. [sent-276, score-0.314]
</p><p>84 Color similarity: This potential encodes the fact that we expect segments which are coplanar to have similar color statistics, while the entropy is higher when the planes form an occlusion boundary or a hinge. [sent-284, score-0.222]
</p><p>85 As shown in Table 1, our approach significantly outperforms all approaches, yielding approximately half the error of the best general flow algorithm, and a third of the error of the best epipolar flow algorithm, i. [sent-312, score-1.529]
</p><p>86 The error of the best oracle match along the epipolar line when employing our estimated FOE is  also very small. [sent-355, score-0.733]
</p><p>87 In “Oracle GT”, ground truth flow vectors are converted into VZ-index values using the epipolar lines estimated from ground truth, and VZ-index planes are fitted to the superpixel segments, which are generated by motionSLIC. [sent-356, score-1.181]
</p><p>88 In “Oracle estimated”,  flow  vectors of ground truth are converted to VZ-index values using our estimated epipolar lines. [sent-357, score-1.008]
</p><p>89 Our first assumption is that most of the flow is due to the ego-motion. [sent-362, score-0.485]
</p><p>90 When utilizing our estimated FOE (via SIFT matching and 8-point algorithm with RANSAC), the error of the best oracle match along the epipolar line is also very small. [sent-364, score-0.77]
</p><p>91 Note that given the ground truth epipolar lines (“Oracle GT”), the piece-wise planar assumption is fairly accurate. [sent-367, score-0.575]
</p><p>92 When the epipolar lines are estimated by our ego-motion estimation (“Oracle estimated”), the piecewise planar assumption becomes worse, but is still a good fit. [sent-368, score-0.637]
</p><p>93 Stereo: Our MotionSLIC algorithm can be utilized for stereo vision in order to compute disparities and segmentations that respect depth boundaries. [sent-373, score-0.182]
</p><p>94 Conclusion  and Future Work  We have presented a slanted-plane MRF model for the problem of epipolar flow estimation which utilizes a robust data term as well as an over-segmentation of the image that respects flow boundaries. [sent-380, score-1.503]
</p><p>95 We have demonstrated the effectiveness of our approach in the challenging KITTI flow benchmark, achieving half the error of the best competing general flow algorithm and one third of the error of the best competing epipolar flow algorithm. [sent-381, score-2.079]
</p><p>96 High accuracy optical flow estimation based on a theory for warping. [sent-417, score-0.574]
</p><p>97 Large displacement optical flow: Descriptor matching in variational motion estimation. [sent-422, score-0.155]
</p><p>98 Hierarchical scan line dynamic programming for optical flow using semi-global matching. [sent-488, score-0.567]
</p><p>99 Optical flow estimation on coarse-to-fine region-trees using discrete optimization. [sent-524, score-0.525]
</p><p>100 Optic flow goes stereo: a variational method for estimating discontinuitypreserving dense disparity maps. [sent-579, score-0.579]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('epipolar', 0.521), ('flow', 0.46), ('sp', 0.245), ('motionslic', 0.223), ('uw', 0.172), ('zp', 0.147), ('stereo', 0.141), ('csp', 0.128), ('slanted', 0.127), ('mrf', 0.127), ('oracle', 0.125), ('kitti', 0.1), ('foe', 0.099), ('vz', 0.095), ('uv', 0.082), ('disparity', 0.08), ('optical', 0.079), ('pcbp', 0.079), ('plane', 0.078), ('coplanar', 0.074), ('superpixel', 0.073), ('planes', 0.071), ('hazan', 0.07), ('sgm', 0.069), ('ciu', 0.067), ('disp', 0.067), ('zvpz', 0.067), ('epipole', 0.052), ('continuous', 0.05), ('energy', 0.049), ('bruhn', 0.047), ('pock', 0.047), ('lj', 0.047), ('cen', 0.046), ('competing', 0.045), ('ectol', 0.045), ('hermann', 0.045), ('hneorw', 0.045), ('rpz', 0.045), ('parameterization', 0.044), ('potentials', 0.043), ('boundary', 0.042), ('block', 0.042), ('vehicle', 0.041), ('disparities', 0.041), ('hinge', 0.04), ('ft', 0.039), ('variational', 0.039), ('slic', 0.038), ('matching', 0.037), ('cech', 0.037), ('autonomous', 0.036), ('estimation', 0.035), ('depicts', 0.035), ('gt', 0.035), ('occlusion', 0.035), ('dagm', 0.035), ('validity', 0.035), ('pixel', 0.035), ('employ', 0.035), ('edt', 0.035), ('camera', 0.034), ('variables', 0.033), ('census', 0.033), ('agree', 0.033), ('estimate', 0.032), ('error', 0.032), ('driving', 0.031), ('cost', 0.031), ('discrete', 0.03), ('fundamental', 0.03), ('bleyer', 0.03), ('cremers', 0.03), ('lines', 0.029), ('discretized', 0.029), ('segment', 0.028), ('penalties', 0.028), ('pos', 0.028), ('line', 0.028), ('optic', 0.027), ('yamaguchi', 0.027), ('hybrid', 0.027), ('estimated', 0.027), ('respects', 0.027), ('particles', 0.027), ('neighboring', 0.026), ('superpixels', 0.026), ('translation', 0.026), ('geiger', 0.025), ('homogeneous', 0.025), ('witnessed', 0.025), ('assumption', 0.025), ('importance', 0.024), ('cy', 0.024), ('half', 0.024), ('rotation', 0.024), ('monocular', 0.024), ('spurious', 0.024), ('translational', 0.023), ('discretization', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="362-tfidf-1" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>2 0.23101841 <a title="362-tfidf-2" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>3 0.22602364 <a title="362-tfidf-3" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>Author: Zhuoyuan Chen, Hailin Jin, Zhe Lin, Scott Cohen, Ying Wu</p><p>Abstract: We present an optical flow algorithm for large displacement motions. Most existing optical flow methods use the standard coarse-to-fine framework to deal with large displacement motions which has intrinsic limitations. Instead, we formulate the motion estimation problem as a motion segmentation problem. We use approximate nearest neighbor fields to compute an initial motion field and use a robust algorithm to compute a set of similarity transformations as the motion candidates for segmentation. To account for deviations from similarity transformations, we add local deformations in the segmentation process. We also observe that small objects can be better recovered using translations as the motion candidates. We fuse the motion results obtained under similarity transformations and under translations together before a final refinement. Experimental validation shows that our method can successfully handle large displacement motions. Although we particularly focus on large displacement motions in this work, we make no sac- rifice in terms of overall performance. In particular, our method ranks at the top of the Middlebury benchmark.</p><p>4 0.2157006 <a title="362-tfidf-4" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>5 0.20649171 <a title="362-tfidf-5" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>Author: Deqing Sun, Jonas Wulff, Erik B. Sudderth, Hanspeter Pfister, Michael J. Black</p><p>Abstract: Layered models allow scene segmentation and motion estimation to be formulated together and to inform one another. Traditional layered motion methods, however, employ fairly weak models of scene structure, relying on locally connected Ising/Potts models which have limited ability to capture long-range correlations in natural scenes. To address this, we formulate a fully-connected layered model that enables global reasoning about the complicated segmentations of real objects. Optimization with fully-connected graphical models is challenging, and our inference algorithm leverages recent work on efficient mean field updates for fully-connected conditional random fields. These methods can be implemented efficiently using high-dimensional Gaussian filtering. We combine these ideas with a layered flow model, and find that the long-range connections greatly improve segmentation into figure-ground layers when compared with locally connected MRF models. Experiments on several benchmark datasets show that the method can re- cover fine structures and large occlusion regions, with good flow accuracy and much lower computational cost than previous locally-connected layered models.</p><p>6 0.20064031 <a title="362-tfidf-6" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>7 0.18802066 <a title="362-tfidf-7" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>8 0.18083267 <a title="362-tfidf-8" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>9 0.17620505 <a title="362-tfidf-9" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>10 0.17050643 <a title="362-tfidf-10" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>11 0.16790426 <a title="362-tfidf-11" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>12 0.16423962 <a title="362-tfidf-12" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>13 0.1584819 <a title="362-tfidf-13" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>14 0.1486236 <a title="362-tfidf-14" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>15 0.13734803 <a title="362-tfidf-15" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>16 0.12806383 <a title="362-tfidf-16" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>17 0.11817452 <a title="362-tfidf-17" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>18 0.11562056 <a title="362-tfidf-18" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>19 0.11556447 <a title="362-tfidf-19" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>20 0.10469939 <a title="362-tfidf-20" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.177), (2, 0.043), (3, -0.022), (4, 0.01), (5, -0.059), (6, 0.065), (7, -0.02), (8, -0.08), (9, 0.128), (10, 0.14), (11, 0.163), (12, 0.201), (13, 0.008), (14, 0.002), (15, 0.15), (16, -0.052), (17, -0.201), (18, 0.022), (19, 0.03), (20, -0.066), (21, -0.05), (22, 0.151), (23, 0.056), (24, -0.009), (25, -0.032), (26, -0.047), (27, -0.001), (28, 0.003), (29, -0.011), (30, 0.014), (31, -0.026), (32, 0.073), (33, 0.045), (34, -0.091), (35, 0.01), (36, 0.076), (37, 0.021), (38, 0.016), (39, -0.004), (40, 0.02), (41, -0.068), (42, -0.006), (43, -0.037), (44, 0.021), (45, 0.022), (46, 0.038), (47, -0.079), (48, 0.062), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96930921 <a title="362-lsi-1" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>2 0.82947671 <a title="362-lsi-2" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>Author: Jiangbo Lu, Hongsheng Yang, Dongbo Min, Minh N. Do</p><p>Abstract: Though many tasks in computer vision can be formulated elegantly as pixel-labeling problems, a typical challenge discouraging such a discrete formulation is often due to computational efficiency. Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. This paper presents a generic and fast computational framework for general multi-labeling problems called PatchMatch Filter (PMF). For the very first time, we explore effective and efficient strategies to weave together these two fundamental techniques developed in isolation, i.e., PatchMatch-based randomized search and efficient edge-aware image filtering. By decompositing an image into compact superpixels, we also propose superpixelbased novel search strategies that generalize and improve the original PatchMatch method. Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases.</p><p>3 0.73554039 <a title="362-lsi-3" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>Author: Deqing Sun, Jonas Wulff, Erik B. Sudderth, Hanspeter Pfister, Michael J. Black</p><p>Abstract: Layered models allow scene segmentation and motion estimation to be formulated together and to inform one another. Traditional layered motion methods, however, employ fairly weak models of scene structure, relying on locally connected Ising/Potts models which have limited ability to capture long-range correlations in natural scenes. To address this, we formulate a fully-connected layered model that enables global reasoning about the complicated segmentations of real objects. Optimization with fully-connected graphical models is challenging, and our inference algorithm leverages recent work on efficient mean field updates for fully-connected conditional random fields. These methods can be implemented efficiently using high-dimensional Gaussian filtering. We combine these ideas with a layered flow model, and find that the long-range connections greatly improve segmentation into figure-ground layers when compared with locally connected MRF models. Experiments on several benchmark datasets show that the method can re- cover fine structures and large occlusion regions, with good flow accuracy and much lower computational cost than previous locally-connected layered models.</p><p>4 0.69422621 <a title="362-lsi-4" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>Author: Zhuoyuan Chen, Hailin Jin, Zhe Lin, Scott Cohen, Ying Wu</p><p>Abstract: We present an optical flow algorithm for large displacement motions. Most existing optical flow methods use the standard coarse-to-fine framework to deal with large displacement motions which has intrinsic limitations. Instead, we formulate the motion estimation problem as a motion segmentation problem. We use approximate nearest neighbor fields to compute an initial motion field and use a robust algorithm to compute a set of similarity transformations as the motion candidates for segmentation. To account for deviations from similarity transformations, we add local deformations in the segmentation process. We also observe that small objects can be better recovered using translations as the motion candidates. We fuse the motion results obtained under similarity transformations and under translations together before a final refinement. Experimental validation shows that our method can successfully handle large displacement motions. Although we particularly focus on large displacement motions in this work, we make no sac- rifice in terms of overall performance. In particular, our method ranks at the top of the Middlebury benchmark.</p><p>5 0.62267834 <a title="362-lsi-5" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>6 0.61727506 <a title="362-lsi-6" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>7 0.61379486 <a title="362-lsi-7" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>8 0.57964337 <a title="362-lsi-8" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>9 0.5783456 <a title="362-lsi-9" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>10 0.56533784 <a title="362-lsi-10" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>11 0.56027609 <a title="362-lsi-11" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>12 0.55359161 <a title="362-lsi-12" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>13 0.54017603 <a title="362-lsi-13" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>14 0.5382517 <a title="362-lsi-14" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>15 0.53309417 <a title="362-lsi-15" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>16 0.53202194 <a title="362-lsi-16" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>17 0.52004904 <a title="362-lsi-17" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>18 0.50074279 <a title="362-lsi-18" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>19 0.49485174 <a title="362-lsi-19" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>20 0.49471492 <a title="362-lsi-20" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.094), (16, 0.047), (22, 0.011), (26, 0.073), (33, 0.283), (47, 0.02), (57, 0.061), (67, 0.047), (69, 0.027), (73, 0.1), (87, 0.115), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94175923 <a title="362-lda-1" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>2 0.93450117 <a title="362-lda-2" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<p>Author: Henrik Zimmer, Marcel Campen, Leif Kobbelt</p><p>Abstract: In the context of shape segmentation and retrieval object-wide distributions of measures are needed to accurately evaluate and compare local regions ofshapes. Lien et al. [16] proposed two point-wise concavity measures in the context of Approximate Convex Decompositions of polygons measuring the distance from a point to the polygon ’s convex hull: an accurate Shortest Path-Concavity (SPC) measure and a Straight Line-Concavity (SLC) approximation of the same. While both are practicable on 2D shapes, the exponential costs of SPC in 3D makes it inhibitively expensive for a generalization to meshes [14]. In this paper we propose an efficient and straight forward approximation of the Shortest Path-Concavity measure to 3D meshes. Our approximation is based on discretizing the space between mesh and convex hull, thereby reducing the continuous Shortest Path search to an efficiently solvable graph problem. Our approach works outof-the-box on complex mesh topologies and requires no complicated handling of genus. Besides presenting a rigorous evaluation of our method on a variety of input meshes, we also define an SPC-based Shape Descriptor and show its superior retrieval and runtime performance compared with the recently presented results on the Convexity Distribution by Lian et al. [12].</p><p>3 0.92665583 <a title="362-lda-3" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>Author: Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials, the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals, and that the intensity distribution of an intensity profile conveys information about the reflectance properties, when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations, we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.</p><p>4 0.92583144 <a title="362-lda-4" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>5 0.92464268 <a title="362-lda-5" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>6 0.92416608 <a title="362-lda-6" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>7 0.92389727 <a title="362-lda-7" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>8 0.92315418 <a title="362-lda-8" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>9 0.921314 <a title="362-lda-9" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>10 0.92073202 <a title="362-lda-10" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>11 0.92001557 <a title="362-lda-11" href="./cvpr-2013-Continuous_Inference_in_Graphical_Models_with_Polynomial_Energies.html">95 cvpr-2013-Continuous Inference in Graphical Models with Polynomial Energies</a></p>
<p>12 0.91933262 <a title="362-lda-12" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>13 0.91916138 <a title="362-lda-13" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>14 0.91903514 <a title="362-lda-14" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>15 0.91864514 <a title="362-lda-15" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>16 0.91858149 <a title="362-lda-16" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>17 0.91856813 <a title="362-lda-17" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>18 0.91817451 <a title="362-lda-18" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>19 0.91778463 <a title="362-lda-19" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>20 0.91769904 <a title="362-lda-20" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
