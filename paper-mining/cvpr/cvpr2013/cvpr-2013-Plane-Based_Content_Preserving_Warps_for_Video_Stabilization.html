<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-333" href="#">cvpr2013-333</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</h1>
<br/><p>Source: <a title="cvpr-2013-333-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhou_Plane-Based_Content_Preserving_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zihan Zhou, Hailin Jin, Yi Ma</p><p>Abstract: Recently, a new image deformation technique called content-preserving warping (CPW) has been successfully employed to produce the state-of-the-art video stabilization results in many challenging cases. The key insight of CPW is that the true image deformation due to viewpoint change can be well approximated by a carefully constructed warp using a set of sparsely constructed 3D points only. However, since CPW solely relies on the tracked feature points to guide the warping, it works poorly in large textureless regions, such as ground and building interiors. To overcome this limitation, in this paper we present a hybrid approach for novel view synthesis, observing that the textureless regions often correspond to large planar surfaces in the scene. Particularly, given a jittery video, we first segment each frame into piecewise planar regions as well as regions labeled as non-planar using Markov random fields. Then, a new warp is computed by estimating a single homography for regions belong to the same plane, while in- heriting results from CPW in the non-planar regions. We demonstrate how the segmentation information can be efficiently obtained and seamlessly integrated into the stabilization framework. Experimental results on a variety of real video sequences verify the effectiveness of our method.</p><p>Reference: <a title="cvpr-2013-333-reference" href="../cvpr2013_reference/cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Recently, a new image deformation technique called content-preserving warping (CPW) has been successfully employed to produce the state-of-the-art video stabilization results in many challenging cases. [sent-2, score-0.817]
</p><p>2 The key insight of CPW is that the true image deformation due to viewpoint change can be well approximated by a carefully constructed warp using a set of sparsely constructed 3D points only. [sent-3, score-0.16]
</p><p>3 However, since CPW solely relies on the tracked feature points to guide the warping, it works poorly in large textureless regions, such as ground and building interiors. [sent-4, score-0.195]
</p><p>4 To overcome this limitation, in this paper we present a hybrid approach for novel view synthesis, observing that the textureless regions often correspond to large planar surfaces in the scene. [sent-5, score-0.418]
</p><p>5 Particularly, given a jittery video, we first segment each frame into piecewise planar regions as well as regions labeled as non-planar using Markov random fields. [sent-6, score-0.597]
</p><p>6 Then, a new warp is computed by estimating a single homography for regions belong to the same plane, while in-  heriting results from CPW in the non-planar regions. [sent-7, score-0.198]
</p><p>7 We demonstrate how the segmentation information can be efficiently obtained and seamlessly integrated into the stabilization framework. [sent-8, score-0.655]
</p><p>8 Experimental results on a variety of real video sequences verify the effectiveness of our method. [sent-9, score-0.119]
</p><p>9 Introduction With the fast development of hand-held digital cameras, we have seen a dramatic increase in the amount of amateur videos shot over the past decade. [sent-11, score-0.127]
</p><p>10 However, very often people find their videos hard to watch, mainly due to the excessive amount of shake and undirected camera motions in the footage. [sent-12, score-0.133]
</p><p>11 Therefore, there has been an urgent demand in developing high-quality video stabilization algorithms, which are able to remove the undesirable jitters from amateur videos so that they look like to be taken under smooth, directed camera paths. [sent-13, score-0.897]
</p><p>12 In general, there are two major steps in stabilizing a jittery input video, namely (1) designing new smooth camera paths, and (2) synthesizing stabilized video frames accordHailin Jin  Yi Ma  Adobe  Microsoft Research Asia  hl j in@ adobe . [sent-14, score-0.468]
</p><p>13 Most existing methods [19, 10, 6, 15, 13] apply a full-frame 2D transformation to each input frame to obtain the stabilized output frame. [sent-18, score-0.284]
</p><p>14 Despite its computational efficiency and robustness, this approach is wellknown for its inability in handling the parallax effects of a non-degenarate scene and camera motion, as illustrated in Figure 1 (first row). [sent-19, score-0.181]
</p><p>15 In fact, in the ideal case one will need the dense 3D structures of the scene in order to create a novel view of it. [sent-20, score-0.117]
</p><p>16 Several attempts have been made along this direction [5, 7, 3], which rely on image-based rendering (IBR) to generate new images of a scene as seen along the smooth camera path. [sent-22, score-0.173]
</p><p>17 propose a novel method, namely contentpreserving warping (CPW), which instead uses the sparse 3D points obtained by any structure from motion system for synthesis. [sent-25, score-0.295]
</p><p>18 In practice, however, large textureless regions often exist in the scene, such as ground, building facades, and indoor walls, where  feature tracks are rare. [sent-28, score-0.225]
</p><p>19 Our key observation is that real scenes often exhibit strong structural regularities, in the form of one or more planar surfaces, which are largely ignored so far by existing methods. [sent-31, score-0.22]
</p><p>20 More importantly, these planar surfaces typically correspond to the textureless regions in the scene, which are problematic to CPW as well as many other methods. [sent-32, score-0.418]
</p><p>21 , homography) is too rigid to handle general motion and structures, resulting in large distortions in non-planar regions (e. [sent-38, score-0.134]
</p><p>22 Second row: Content-preserving warping preserves the non-planar structures well, but yields increasingly visible distortion in the textureless regions (i. [sent-41, score-0.319]
</p><p>23 Third row: Our plane-based warping is able to produce visually pleasing results by combining the strengths of both methods. [sent-44, score-0.117]
</p><p>24 Red line represents the boundary of planar and non-planar regions obtained by our video segmentation algorithm. [sent-45, score-0.445]
</p><p>25 away  Therefore, our goal is to develop a novel 3D stabilization method that can explicitly take advantage of the presence of (relatively large) planar surfaces in the scene. [sent-46, score-0.82]
</p><p>26 To this end, we propose to automatically detect large planes in the scene, and partition each frame into regions associated with each plane, as well as regions that are “non-planar”. [sent-47, score-0.393]
</p><p>27 Note that, since our ultimate goal is to improve the stabilization system and produce jitter-free videos, it is crucial for our segmentation algorithm to process the entire video in a short period of time, and obtain results which can be seamlessly integrated into the stabilization pipeline. [sent-48, score-1.357]
</p><p>28 To achieve this  goal, we develop a novel algorithm which directly works on the same uniform grid mesh that is employed by CPW, and only uses geometric cues for fast processing. [sent-49, score-0.148]
</p><p>29 This is contrary to the existing piecewise planar scene segmentation algorithms, which operate at the per-pixel level and rely on multiple low-level and high-level photometric cues. [sent-50, score-0.482]
</p><p>30 These methods are generally too slow for stabilization purposes, taking hours to process a video with a few hundred frames. [sent-51, score-0.701]
</p><p>31 We demonstrate that our algorithm is capable of processing the entire video in about 30 seconds, and obtaining results that are sufficient for stabilization. [sent-52, score-0.119]
</p><p>32 With the segmentation information, our new plane-based warping method computes a single homography for image regions that belong to the same plane, while borrowing the results of CPW for non-planar regions (Figure 1third row). [sent-53, score-0.339]
</p><p>33 In this way, we not only seamlessly integrate the information about planar structures of the scene into the stabilization framework, but also provide an unified framework for 2D-3D stabilization. [sent-54, score-0.937]
</p><p>34 When the scene is dominated by complex non-planar or dynamic structures, our method becomes CPW which is known to work well in such cases, whereas on the other end, if the scene contains a single large plane, it reduces to the robust and efficient 2D method. [sent-55, score-0.166]
</p><p>35 Related Work In general, depending on the level of scene geometry one recovers, existing video stabilization techniques can be roughly divided into two categories. [sent-58, score-0.748]
</p><p>36 Stabilization is then obtained by smoothing the parameters of 2D transformations followed by synthesizing a new video using the smoothed parameters. [sent-60, score-0.219]
</p><p>37 It is well known that 2D stabilization can only achieve limited smoothing before introducing noticeable artifacts to the output video. [sent-61, score-0.657]
</p><p>38 In order to fully handle general scene structure and camera motion, 3D stabilization methods [5, 7, 3, 16] attempt to recover true camera motion and scene structures via structure from motion (SFM) systems. [sent-64, score-1.038]
</p><p>39 Stabilization is subsequently done by smoothing the camera path in 3D and synthesizing a new video based on the smoothed path. [sent-65, score-0.284]
</p><p>40 The problem of segmenting video into motion layers that admit parametric transformation models was first studied in [25], and remains an active research topic in computer vision today. [sent-72, score-0.233]
</p><p>41 Given camera motion and 3D point cloud, early works on piecewiseplanar scene segmentation from multiple images [1, 26] are based on line grouping and plane sweeping, whose complexity is prohibitive beyond a few images. [sent-74, score-0.355]
</p><p>42 More recently, [2] and [24] both combine the idea of random sampling consensus (RANSAC) with photometric consistency check to obtain piecewise planar scene models. [sent-75, score-0.427]
</p><p>43 Finally, planes extracted from 3D point clouds or depth maps have been recently explored to improve the performance of multi-view stereo (MVS) systems [21, 8, 9, 20]. [sent-79, score-0.195]
</p><p>44 In summary,  ×  none of the existing methods meets our goal of obtaining satisfactory segmentation results within a few seconds for long video sequences. [sent-81, score-0.211]
</p><p>45 Overview of the Content-Preserving Warping Technique  Since our method is built upon the content-preserving warping (CPW) technique introduced in [16], in this section we give a brief review of it. [sent-83, score-0.117]
</p><p>46 In particular, it takes two sets of corresponding 2D points as input –Pˆ in the input frame, and P in the output frame and create a dense warp guided by the displacements from Pˆ to P. [sent-85, score-0.261]
</p><p>47 To create the dense warp, CPW first divides the original video frame Iˆ into an m n uniform grid mesh, represented –  by a set of NI ivnetroti acnes m Vˆ×  =n  { ˆvq}qN=1. [sent-87, score-0.265]
</p><p>48 The data term penalizes the difference in the output frame between the projected location of each point Pt and the location suggested by the estimated mesh V . [sent-90, score-0.222]
</p><p>49 The smoothness term measures the deviation of the estimated 2D transformation of each grid cell from a similarity transformation. [sent-99, score-0.19]
</p><p>50 The output frame is then generated using standard texture mapping algorithm according to V . [sent-110, score-0.126]
</p><p>51 222223990199  Finally we note that, according to the above discussion, the warp obtained by CPW tends to be close to a similarity transformation, especially in regions where features are rare or non-existing. [sent-111, score-0.133]
</p><p>52 However, similarity transformation cannot faithfully represent the projective effects of the scene, and hence may cause serious wobble effects in the stabilized videos. [sent-112, score-0.373]
</p><p>53 Fast Piecewise Planar and Non-Planar Scene Segmentation for Videos In this section, we propose a fast two-step approach to  automatically segment each video frame into piecewise planar and non-planar regions. [sent-115, score-0.569]
</p><p>54 First, we detect scene planes from 3D point cloud obtained by structure from motion using a robust multiple structure estimation algorithm called J-Linkage [23]. [sent-116, score-0.412]
</p><p>55 Second, we describe a novel video segmentation algorithm, which classifies each grid cell in the CPW framework into K + 1classes – one for each of the K detected planes, plus a “non-planar” class. [sent-117, score-0.271]
</p><p>56 Multiple Plane Detection Since real scenes often contain multiple planes as well as non-planar structures, we adopt a robust multiple structure estimation method called J-Linkage [23] to detect planes from 3D point cloud. [sent-122, score-0.389]
</p><p>57 Meanwhile, it has been shown in [23] that J-Linkage substantially outperforms other variants of RANSAC for multiple structure detection, such as sequential RANSAC and multi-RANSAC [29], in many real applications including 3D plane fitting. [sent-124, score-0.127]
</p><p>58 Figure 2 shows the result of applying J-Linkage to the 3D point cloud for an indoor video sequence taken by a person walking down the corridor with a hand-held camera (see Figure 3 for some input frames). [sent-140, score-0.258]
</p><p>59 In this example, three planes are detected, namely the ground and two side-walls. [sent-141, score-0.219]
</p><p>60 Although J-Linkage fails to detect the other two planes, namely the ceiling and front door, due to their small support sizes, we still consider the result successful as these two planes only occupy a very small portion of the video frames. [sent-142, score-0.34]
</p><p>61 A Markov Random Video Segmentation  Field Formulation  for  Once a set of dominant planes is detected, the next step is to perform piecewise planar and non-planar segmentation for each input frame. [sent-145, score-0.581]
</p><p>62 Given a set of K 3D planes, our goal is to assign a unique label li to each vertex pi ∈ V. [sent-156, score-0.121]
</p><p>63 (8) to dmax in order to prevent it from being dominated by a small number of poorly reconstructed 3D points. [sent-199, score-0.125]
</p><p>64 The function g(i, j) is designed to improve the estimation of label boundaries by imposing geometric constraints derived from multiple planes in the scene. [sent-209, score-0.17]
</p><p>65 First, for each pair of planes in the scene (if one exists), we compute the 2D intersection line L between them in each frame If. [sent-210, score-0.335]
</p><p>66 As one can see, our segmentation algorithm correctly identifies the large planar regions in a variety of indoor and outdoor scenes. [sent-216, score-0.359]
</p><p>67 This is mainly due to the uncertainty in 3D reconstruction, which decides the smallest possible threshold β one can choose to distinguish points on a plane from others. [sent-218, score-0.156]
</p><p>68 Nevertheless, we find that these errors have little effect on the final stabilization results, since the shifts in viewpoint are usually small for video stabilization. [sent-220, score-0.704]
</p><p>69 Additional results on piecewise planar and non-planar scene segmentation. [sent-226, score-0.427]
</p><p>70 piecewise planar scene segmentation algorithms take about 10 and 15 seconds on a desktop PC with 3. [sent-227, score-0.482]
</p><p>71 , planar surfaces) of the scene to produce high-quality stabilization results, especially in the cases where CPW per-  ×  forms poorly because of large textureless regions. [sent-232, score-0.991]
</p><p>72 In this section, we describe our plane-based stabilization algorithm in detail. [sent-233, score-0.558]
</p><p>73 Like other 3D stabilization methods, our plane-based method first applies structure from motion to recover the original camera motion and sparse 3D point cloud. [sent-234, score-0.763]
</p><p>74 To generate the stabilized camera path, we apply Gaussian filter to the original camera parameters. [sent-236, score-0.233]
</p><p>75 Each input frame is divided into a 64 36 grid mesh Vˆ = { ˆvq}qN=1 farnadm tehe i content-preserving warp gisr tihde mn computed. [sent-240, score-0.324]
</p><p>76 To incorporate information about the piecewise planar scene sntcruocrptourreast ein itnostabilization, we give a label, lq, to each vertex of the mesh according to the labels of its surrounding cells. [sent-244, score-0.57]
</p><p>77 For any vertex that lies on the segmentation boundary (hence the surrounding cells have more than one labels), we simply assign the smallest label to it. [sent-245, score-0.151]
</p><p>78 ,K  (12)  where Hk is the homography induced by the k-th plane between the input and output frames. [sent-250, score-0.202]
</p><p>79 The output frame is then obtained using standard texture mapping algorithms. [sent-251, score-0.126]
</p><p>80 Experiments We have tested our algorithm on 32 video sequences (see Figure 5) which consist of one or more large scene planes, including 5 videos that are used in [16] to demonstrate the performance of CPW. [sent-253, score-0.258]
</p><p>81 Among them, noticeable wobble effects can be seen in 18 results obtained by CPW, due to the lack of feature tracks in large planar regions. [sent-255, score-0.436]
</p><p>82 Meanwhile, our plane-based method succeeds in 30 of the 32 videos, generating satisfactory stabilization results. [sent-256, score-0.595]
</p><p>83 For the other two testing videos shown in Figure 6, our method is not able to completely remove the wobble effects, although it still produces better results than CPW. [sent-259, score-0.169]
</p><p>84 Therefore, J-Linkage fails to detect the ground plane in the case. [sent-261, score-0.157]
</p><p>85 Consequently, our segmentation algorithm incorrectly assigns the ground regions to the planes corresponding to the walls, causing undesirable artifacts in the stabilized video. [sent-262, score-0.432]
</p><p>86 ground is slightly curved, which confuses our plane detection and segmentation algorithms. [sent-271, score-0.185]
</p><p>87 As a result, a portion of the ground region is labeled as non-planar, hence the wobble effects remain in the output video. [sent-272, score-0.203]
</p><p>88 In fact, both cases reveal the dependency of our method’s performance on a few free parameters in the plane detection and segmentation algorithms, for which a set of fixed values is certainly not enough to handle all cases. [sent-273, score-0.183]
</p><p>89 Nevertheless, we have shown in this paper that, by exploiting scene structures such as the planar surfaces, our method significantly outperforms CPW in many challenging cases. [sent-274, score-0.337]
</p><p>90 Conclusion, Limitations, and Future Work In this paper we have described a novel method for video stabilization, which outperforms the state-of-the-art methods by taking advantage of the presence of large planes in the scene. [sent-276, score-0.289]
</p><p>91 In particular, we have proposed an efficient Markov random field formulation to segment each video frame into piecewise planar and non222333000533  planar regions. [sent-278, score-0.789]
</p><p>92 This level of scene understanding is shown to be ideal for generating high-quality jitter-free videos in a  variety of practical scenarios. [sent-279, score-0.139]
</p><p>93 Like CPW and many other 3D methods, our algorithm relies on structure from motion to get accurate information about the 3D scene structures and camera motions. [sent-280, score-0.263]
</p><p>94 Also, we do not address other common issues in video stabilization, including the smaller field of view, motion blur [19], and rolling shuttle effects [12]. [sent-282, score-0.255]
</p><p>95 Currently we use the robust model estimation package J-Linkage, but it leaves to the user to decide the minimum number of inliers for a valid model; hence it may fail when the number of reconstructed 3D points on the plane is extremely small. [sent-284, score-0.164]
</p><p>96 Automatic reconstruction of piecewise planar models from multiple views. [sent-292, score-0.381]
</p><p>97 A random sampling strategy for piecewise planar scene segmentation. [sent-296, score-0.427]
</p><p>98 Piecewise planar and non-planar stereo for urban scene reconstruction. [sent-361, score-0.316]
</p><p>99 Auto-directed video stabilization with robust l1 optimal camera paths. [sent-387, score-0.742]
</p><p>100 The multiransac algorithm and its application to detect planar homographies. [sent-523, score-0.247]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stabilization', 0.558), ('cpw', 0.492), ('planar', 0.22), ('planes', 0.17), ('piecewise', 0.136), ('video', 0.119), ('warping', 0.117), ('textureless', 0.105), ('plane', 0.105), ('stabilized', 0.103), ('wobble', 0.101), ('mesh', 0.096), ('frame', 0.094), ('warp', 0.082), ('lj', 0.08), ('xk', 0.076), ('vq', 0.073), ('scene', 0.071), ('videos', 0.068), ('gleicher', 0.067), ('synthesizing', 0.067), ('camera', 0.065), ('homography', 0.065), ('motion', 0.059), ('amateur', 0.059), ('transformation', 0.055), ('segmentation', 0.055), ('grid', 0.052), ('jin', 0.052), ('ransac', 0.051), ('regions', 0.051), ('warps', 0.05), ('qn', 0.047), ('vertex', 0.047), ('structures', 0.046), ('vf', 0.045), ('adobe', 0.045), ('effects', 0.045), ('contentpreserving', 0.045), ('jittery', 0.045), ('toldo', 0.045), ('zihan', 0.045), ('cell', 0.045), ('seamlessly', 0.042), ('surfaces', 0.042), ('pi', 0.041), ('cloud', 0.041), ('smoothness', 0.038), ('pages', 0.038), ('rendering', 0.037), ('poorly', 0.037), ('satisfactory', 0.037), ('tracks', 0.036), ('es', 0.036), ('epipolar', 0.036), ('regularities', 0.036), ('noticeable', 0.034), ('dmax', 0.033), ('indoor', 0.033), ('smoothing', 0.033), ('li', 0.033), ('output', 0.032), ('kwatra', 0.032), ('rolling', 0.032), ('reconstructed', 0.031), ('ij', 0.031), ('grundmann', 0.031), ('eij', 0.031), ('ge', 0.031), ('gf', 0.03), ('markov', 0.03), ('agarwala', 0.029), ('undesirable', 0.028), ('points', 0.028), ('viewpoint', 0.027), ('walls', 0.027), ('row', 0.027), ('detect', 0.027), ('cells', 0.026), ('stereo', 0.025), ('displacements', 0.025), ('ground', 0.025), ('reconstruction', 0.025), ('veksler', 0.025), ('preference', 0.025), ('period', 0.025), ('projective', 0.024), ('namely', 0.024), ('hours', 0.024), ('oth', 0.024), ('dominated', 0.024), ('distortions', 0.024), ('curless', 0.023), ('deformation', 0.023), ('smallest', 0.023), ('free', 0.023), ('structure', 0.022), ('steedly', 0.022), ('defi', 0.022), ('tewdog', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="333-tfidf-1" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>Author: Zihan Zhou, Hailin Jin, Yi Ma</p><p>Abstract: Recently, a new image deformation technique called content-preserving warping (CPW) has been successfully employed to produce the state-of-the-art video stabilization results in many challenging cases. The key insight of CPW is that the true image deformation due to viewpoint change can be well approximated by a carefully constructed warp using a set of sparsely constructed 3D points only. However, since CPW solely relies on the tracked feature points to guide the warping, it works poorly in large textureless regions, such as ground and building interiors. To overcome this limitation, in this paper we present a hybrid approach for novel view synthesis, observing that the textureless regions often correspond to large planar surfaces in the scene. Particularly, given a jittery video, we first segment each frame into piecewise planar regions as well as regions labeled as non-planar using Markov random fields. Then, a new warp is computed by estimating a single homography for regions belong to the same plane, while in- heriting results from CPW in the non-planar regions. We demonstrate how the segmentation information can be efficiently obtained and seamlessly integrated into the stabilization framework. Experimental results on a variety of real video sequences verify the effectiveness of our method.</p><p>2 0.32894567 <a title="333-tfidf-2" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>Author: Dennis Park, C. Lawrence Zitnick, Deva Ramanan, Piotr Dollár</p><p>Abstract: We describe novel but simple motion features for the problem of detecting objects in video sequences. Previous approaches either compute optical flow or temporal differences on video frame pairs with various assumptions about stabilization. We describe a combined approach that uses coarse-scale flow and fine-scale temporal difference features. Our approach performs weak motion stabilization by factoring out camera motion and coarse object motion while preserving nonrigid motions that serve as useful cues for recognition. We show results for pedestrian detection and human pose estimation in video sequences, achieving state-of-the-art results in both. In particular, given a fixed detection rate our method achieves a five-fold reduction in false positives over prior art on the Caltech Pedestrian benchmark. Finally, we perform extensive diagnostic experiments to reveal what aspects of our system are crucial for good performance. Proper stabilization, long time-scale features, and proper normalization are all critical.</p><p>3 0.21853727 <a title="333-tfidf-3" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>4 0.15960576 <a title="333-tfidf-4" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>Author: Cheng Li, Kris M. Kitani</p><p>Abstract: We address the task of pixel-level hand detection in the context of ego-centric cameras. Extracting hand regions in ego-centric videos is a critical step for understanding handobject manipulation and analyzing hand-eye coordination. However, in contrast to traditional applications of hand detection, such as gesture interfaces or sign-language recognition, ego-centric videos present new challenges such as rapid changes in illuminations, significant camera motion and complex hand-object manipulations. To quantify the challenges and performance in this new domain, we present a fully labeled indoor/outdoor ego-centric hand detection benchmark dataset containing over 200 million labeled pixels, which contains hand images taken under various illumination conditions. Using both our dataset and a publicly available ego-centric indoors dataset, we give extensive analysis of detection performance using a wide range of local appearance features. Our analysis highlights the effectiveness of sparse features and the importance of modeling global illumination. We propose a modeling strategy based on our findings and show that our model outperforms several baseline approaches.</p><p>5 0.11322965 <a title="333-tfidf-5" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>Author: Julien P.C. Valentin, Sunando Sengupta, Jonathan Warrell, Ali Shahrokni, Philip H.S. Torr</p><p>Abstract: Semantic reconstruction of a scene is important for a variety of applications such as 3D modelling, object recognition and autonomous robotic navigation. However, most object labelling methods work in the image domain and fail to capture the information present in 3D space. In this work we propose a principled way to generate object labelling in 3D. Our method builds a triangulated meshed representation of the scene from multiple depth estimates. We then define a CRF over this mesh, which is able to capture the consistency of geometric properties of the objects present in the scene. In this framework, we are able to generate object hypotheses by combining information from multiple sources: geometric properties (from the 3D mesh), and appearance properties (from images). We demonstrate the robustness of our framework in both indoor and outdoor scenes. For indoor scenes we created an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. For outdoor scenes, we created ground truth object labellings for the KITTI odometry dataset (stereo image sequence). We observe a signifi- cant speed-up in the inference stage by performing labelling on the mesh, and additionally achieve higher accuracies.</p><p>6 0.10747349 <a title="333-tfidf-6" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>7 0.10171624 <a title="333-tfidf-7" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>8 0.10013673 <a title="333-tfidf-8" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>9 0.095945925 <a title="333-tfidf-9" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>10 0.084369607 <a title="333-tfidf-10" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>11 0.083694704 <a title="333-tfidf-11" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>12 0.082329355 <a title="333-tfidf-12" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>13 0.082288876 <a title="333-tfidf-13" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>14 0.079993583 <a title="333-tfidf-14" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>15 0.078884929 <a title="333-tfidf-15" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>16 0.07818611 <a title="333-tfidf-16" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>17 0.077879213 <a title="333-tfidf-17" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>18 0.076664217 <a title="333-tfidf-18" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>19 0.07545428 <a title="333-tfidf-19" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>20 0.074689128 <a title="333-tfidf-20" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, 0.112), (2, 0.014), (3, -0.025), (4, -0.038), (5, -0.025), (6, -0.0), (7, -0.048), (8, -0.033), (9, 0.049), (10, 0.095), (11, 0.054), (12, 0.086), (13, -0.028), (14, 0.056), (15, -0.048), (16, 0.025), (17, 0.028), (18, -0.058), (19, 0.027), (20, -0.059), (21, -0.006), (22, -0.001), (23, -0.068), (24, -0.06), (25, -0.047), (26, -0.01), (27, 0.046), (28, 0.006), (29, 0.002), (30, -0.045), (31, 0.06), (32, -0.04), (33, 0.044), (34, -0.011), (35, 0.027), (36, -0.099), (37, 0.057), (38, -0.046), (39, -0.006), (40, -0.01), (41, 0.021), (42, -0.065), (43, -0.06), (44, 0.038), (45, -0.038), (46, -0.023), (47, -0.003), (48, -0.018), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92492473 <a title="333-lsi-1" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>Author: Zihan Zhou, Hailin Jin, Yi Ma</p><p>Abstract: Recently, a new image deformation technique called content-preserving warping (CPW) has been successfully employed to produce the state-of-the-art video stabilization results in many challenging cases. The key insight of CPW is that the true image deformation due to viewpoint change can be well approximated by a carefully constructed warp using a set of sparsely constructed 3D points only. However, since CPW solely relies on the tracked feature points to guide the warping, it works poorly in large textureless regions, such as ground and building interiors. To overcome this limitation, in this paper we present a hybrid approach for novel view synthesis, observing that the textureless regions often correspond to large planar surfaces in the scene. Particularly, given a jittery video, we first segment each frame into piecewise planar regions as well as regions labeled as non-planar using Markov random fields. Then, a new warp is computed by estimating a single homography for regions belong to the same plane, while in- heriting results from CPW in the non-planar regions. We demonstrate how the segmentation information can be efficiently obtained and seamlessly integrated into the stabilization framework. Experimental results on a variety of real video sequences verify the effectiveness of our method.</p><p>2 0.72854322 <a title="333-lsi-2" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>Author: Christian Richardt, Yael Pritch, Henning Zimmer, Alexander Sorkine-Hornung</p><p>Abstract: We present a solution for generating high-quality stereo panoramas at megapixel resolutions. While previous approaches introduced the basic principles, we show that those techniques do not generalise well to today’s high image resolutions and lead to disturbing visual artefacts. As our first contribution, we describe the necessary correction steps and a compact representation for the input images in order to achieve a highly accurate approximation to the required ray space. Our second contribution is a flow-based upsampling of the available input rays which effectively resolves known aliasing issues like stitching artefacts. The required rays are generated on the fly to perfectly match the desired output resolution, even for small numbers of input images. In addition, the upsampling is real-time and enables direct interactive control over the desired stereoscopic depth effect. In combination, our contributions allow the generation of stereoscopic panoramas at high output resolutions that are virtually free of artefacts such as seams, stereo discontinuities, vertical parallax and other mono-/stereoscopic shape distortions. Our process is robust, and other types of multiperspective panoramas, such as linear panoramas, can also benefit from our contributions. We show various comparisons and high-resolution results.</p><p>3 0.71533483 <a title="333-lsi-3" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>Author: Shaodi You, Robby T. Tan, Rei Kawakami, Katsushi Ikeuchi</p><p>Abstract: Raindrops adhered to a windscreen or window glass can significantly degrade the visibility of a scene. Detecting and removing raindrops will, therefore, benefit many computer vision applications, particularly outdoor surveillance systems and intelligent vehicle systems. In this paper, a method that automatically detects and removes adherent raindrops is introduced. The core idea is to exploit the local spatiotemporal derivatives ofraindrops. First, it detects raindrops based on the motion and the intensity temporal derivatives of the input video. Second, relying on an analysis that some areas of a raindrop completely occludes the scene, yet the remaining areas occludes only partially, the method removes the two types of areas separately. For partially occluding areas, it restores them by retrieving as much as possible information of the scene, namely, by solving a blending function on the detected partially occluding areas using the temporal intensity change. For completely occluding areas, it recovers them by using a video completion technique. Experimental results using various real videos show the effectiveness of the proposed method.</p><p>4 0.71306056 <a title="333-lsi-4" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>5 0.70422673 <a title="333-lsi-5" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>6 0.69100988 <a title="333-lsi-6" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>7 0.66869789 <a title="333-lsi-7" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>8 0.66768694 <a title="333-lsi-8" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>9 0.66540635 <a title="333-lsi-9" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>10 0.6541307 <a title="333-lsi-10" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>11 0.65251076 <a title="333-lsi-11" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>12 0.63971764 <a title="333-lsi-12" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>13 0.63963145 <a title="333-lsi-13" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>14 0.6356746 <a title="333-lsi-14" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>15 0.6307981 <a title="333-lsi-15" href="./cvpr-2013-Dynamic_Scene_Classification%3A_Learning_Motion_Descriptors_with_Slow_Features_Analysis.html">137 cvpr-2013-Dynamic Scene Classification: Learning Motion Descriptors with Slow Features Analysis</a></p>
<p>16 0.6238448 <a title="333-lsi-16" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>17 0.62355298 <a title="333-lsi-17" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>18 0.61893207 <a title="333-lsi-18" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>19 0.61844367 <a title="333-lsi-19" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>20 0.61070502 <a title="333-lsi-20" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.132), (16, 0.042), (24, 0.163), (26, 0.043), (28, 0.012), (33, 0.308), (59, 0.012), (67, 0.05), (69, 0.047), (76, 0.013), (87, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95997959 <a title="333-lda-1" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>Author: Shaodi You, Robby T. Tan, Rei Kawakami, Katsushi Ikeuchi</p><p>Abstract: Raindrops adhered to a windscreen or window glass can significantly degrade the visibility of a scene. Detecting and removing raindrops will, therefore, benefit many computer vision applications, particularly outdoor surveillance systems and intelligent vehicle systems. In this paper, a method that automatically detects and removes adherent raindrops is introduced. The core idea is to exploit the local spatiotemporal derivatives ofraindrops. First, it detects raindrops based on the motion and the intensity temporal derivatives of the input video. Second, relying on an analysis that some areas of a raindrop completely occludes the scene, yet the remaining areas occludes only partially, the method removes the two types of areas separately. For partially occluding areas, it restores them by retrieving as much as possible information of the scene, namely, by solving a blending function on the detected partially occluding areas using the temporal intensity change. For completely occluding areas, it recovers them by using a video completion technique. Experimental results using various real videos show the effectiveness of the proposed method.</p><p>2 0.92521483 <a title="333-lda-2" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>Author: Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang</p><p>Abstract: This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants, a new tree structure, ”Segment-Tree ”, is proposed for non-local matching cost aggregation. Conceptually, the segment-tree is constructed in a three-step process: first, the pixels are grouped into a set of segments with the reference color or intensity image; second, a tree graph is created for each segment; and in the final step, these independent segment graphs are linked to form the segment-tree structure. In practice, this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights, our method introduces some ’non-local’ decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities, and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree within two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable to previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore, the tree structure can be refined with the estimated disparities, which leads to consistent scene segmentation and significantly better aggregation results.</p><p>same-paper 3 0.91512543 <a title="333-lda-3" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>Author: Zihan Zhou, Hailin Jin, Yi Ma</p><p>Abstract: Recently, a new image deformation technique called content-preserving warping (CPW) has been successfully employed to produce the state-of-the-art video stabilization results in many challenging cases. The key insight of CPW is that the true image deformation due to viewpoint change can be well approximated by a carefully constructed warp using a set of sparsely constructed 3D points only. However, since CPW solely relies on the tracked feature points to guide the warping, it works poorly in large textureless regions, such as ground and building interiors. To overcome this limitation, in this paper we present a hybrid approach for novel view synthesis, observing that the textureless regions often correspond to large planar surfaces in the scene. Particularly, given a jittery video, we first segment each frame into piecewise planar regions as well as regions labeled as non-planar using Markov random fields. Then, a new warp is computed by estimating a single homography for regions belong to the same plane, while in- heriting results from CPW in the non-planar regions. We demonstrate how the segmentation information can be efficiently obtained and seamlessly integrated into the stabilization framework. Experimental results on a variety of real video sequences verify the effectiveness of our method.</p><p>4 0.91364962 <a title="333-lda-4" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>Author: Joseph Tighe, Svetlana Lazebnik</p><p>Abstract: This paper presents a system for image parsing, or labeling each pixel in an image with its semantic category, aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. The system combines region-level features with per-exemplar sliding window detectors. Per-exemplar detectors are better suited for our parsing task than traditional bounding box detectors: they perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. The proposed system achieves state-of-theart accuracy on three challenging datasets, the largest of which contains 45,676 images and 232 labels.</p><p>5 0.90724534 <a title="333-lda-5" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>Author: Jacinto C. Nascimento, Gustavo Carneiro</p><p>Abstract: The solution for the top-down segmentation of non-rigid visual objects using machine learning techniques is generally regarded as too complex to be solved in its full generality given the large dimensionality of the search space of the explicit representation ofthe segmentation contour. In order to reduce this complexity, theproblem is usually divided into two stages: rigid detection and non-rigid segmentation. The rationale is based on the fact that the rigid detection can be run in a lower dimensionality space (i.e., less complex and faster) than the original contour space, and its result is then used to constrain the non-rigid segmentation. In this paper, we propose the use of sparse manifolds to reduce the dimensionality of the rigid detection search space of current stateof-the-art top-down segmentation methodologies. The main goals targeted by this smaller dimensionality search space are the decrease of the search running time complexity and the reduction of the training complexity of the rigid detec- tor. These goals are attainable given that both the search and training complexities are function of the dimensionality of the rigid search space. We test our approach in the segmentation of the left ventricle from ultrasound images and lips from frontal face images. Compared to the performance of state-of-the-art non-rigid segmentation system, our experiments show that the use of sparse manifolds for the rigid detection leads to the two goals mentioned above.</p><p>6 0.90320683 <a title="333-lda-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.90166664 <a title="333-lda-7" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>8 0.90165502 <a title="333-lda-8" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>9 0.89990622 <a title="333-lda-9" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>10 0.89967811 <a title="333-lda-10" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>11 0.89887452 <a title="333-lda-11" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>12 0.89880866 <a title="333-lda-12" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>13 0.89863908 <a title="333-lda-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.89828521 <a title="333-lda-14" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>15 0.89810771 <a title="333-lda-15" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>16 0.89791322 <a title="333-lda-16" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>17 0.89773029 <a title="333-lda-17" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>18 0.89772224 <a title="333-lda-18" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>19 0.89770317 <a title="333-lda-19" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>20 0.89688575 <a title="333-lda-20" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
