<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>267 cvpr-2013-Least Soft-Threshold Squares Tracking</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-267" href="#">cvpr2013-267</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>267 cvpr-2013-Least Soft-Threshold Squares Tracking</h1>
<br/><p>Source: <a title="cvpr-2013-267-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Least_Soft-Threshold_Squares_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Dong Wang, Huchuan Lu, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods, the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution, which can be solved efficiently. Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.</p><p>Reference: <a title="cvpr-2013-267-reference" href="../cvpr2013_reference/cvpr-2013-Least_Soft-Threshold_Squares_Tracking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. [sent-6, score-0.366]
</p><p>2 Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. [sent-8, score-0.222]
</p><p>3 Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. [sent-9, score-0.506]
</p><p>4 In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. [sent-10, score-0.248]
</p><p>5 Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods. [sent-11, score-0.192]
</p><p>6 Introduction Visual tracking plays a critical role in computer vision that finds many practical applications (e. [sent-13, score-0.178]
</p><p>7 Although significant progress has been made in the past decades, developing a robust tracking algorithm is still a challenging problem due to numerous factors such as partial occlusion, illumination variation, pose change, complex motion, and background clutter. [sent-16, score-0.343]
</p><p>8 Generative methods focus on searching for the regions which are the most similar to the tracked targets, while discriminative methods cast tracking as a classification problem that distinguishes the tracked targets from the surrounding backgrounds. [sent-22, score-0.412]
</p><p>9 In this work, we propose a robust generative tracker which is able to handle partial occlusion and other challenging factors effectively. [sent-23, score-0.426]
</p><p>10 , a set of basis vectors from a subspace or a series of templates) to describe the tracked target. [sent-27, score-0.221]
</p><p>11 A given candidate sample is linearly represented by the dictionary, and the representation coefficient and reconstruction error are computed, from which the corresponding likelihood (belonging to the object class) is determined. [sent-28, score-0.197]
</p><p>12 [20] propose an incremental visual tracking (IVT) method which represents the tracked target by a low dimensional PCA subspace  (a set of PCA basis vectors) and assumes that the error is Gaussian distributed with small variances (i. [sent-30, score-0.537]
</p><p>13 Therefore, the representation coefficient can be obtained by a simple projection operator, which is equivalent to the ordinary least squares solution under the assumption that the dictionary atoms are orthogonal. [sent-33, score-0.528]
</p><p>14 The reconstruction error is computed by the objective function of the ordinary least squares methods. [sent-34, score-0.43]
</p><p>15 While the IVT method is effective to handle appearance change caused by illumination variation and pose variation, it is not robust to some challenging factors (e. [sent-35, score-0.169]
</p><p>16 , partial occlusion and background clutter) due to the following two reasons. [sent-37, score-0.139]
</p><p>17 First, ordinary least squares methods have been shown to be sensitive to outliers due to the formulation based on reconstruction error with Gaussian noise assumption. [sent-38, score-0.629]
</p><p>18 Second, the IVT method uses new observations to update the observation model without detecting outliers and processing them accordingly. [sent-39, score-0.243]
</p><p>19 Other recent tracking algorithms [16, 12] based on the Gaussian noise assumption or the ordinary least squares methods have similar problems as the IVT method. [sent-40, score-0.66]
</p><p>20 1 tracker that uses a series of target templates and trivial templates to model the tracked target, where the target templates are used to describe the object class to be tracked and trivial templates are used to deal with outliers (e. [sent-43, score-0.987]
</p><p>21 For tracking, a candidate sample can be sparsely represented by both tar-  get and trivial templates, and its corresponding likelihood is determined by the reconstruction error with respect to target templates. [sent-46, score-0.26]
</p><p>22 1 tracker in terms of both speed and accuracy by using accelerated proximal gradient algorithm [5], replacing raw pixel templates with orthogonal basis vectors [24, 26, 25], modeling the similarity between different candidates [31], to name a few. [sent-49, score-0.345]
</p><p>23 Although these algorithms consider outliers by using additional trivial templates, this formulation can be generalized with better understanding. [sent-50, score-0.151]
</p><p>24 In this work, we show that the linear regression with the Gaussian-Laplacian noise assumption is more effective in dealing with outliers for object tracking. [sent-51, score-0.348]
</p><p>25 In addition, from the viewpoint of linear regression, it is not suitable to estimate the likelihood based on the reconstruction error with respect to target templates. [sent-52, score-0.167]
</p><p>26 We present a novel distance function to compute the distance between a candidate and the object class. [sent-53, score-0.146]
</p><p>27 In this paper, we present a generative tracking algorithm based on linear regression. [sent-54, score-0.243]
</p><p>28 First, we introduce a novel linear regression method, Least Soft-thresold Squares (LSS), which assumes that the error vectors follow the i. [sent-56, score-0.148]
</p><p>29 Second, we present an efficient iter-  ation method to solve the LSS problem and propose a LSS distance to measure the dissimilarity between the observation vector and the dictionary. [sent-59, score-0.134]
</p><p>30 We note that the LSS method is related to the robust regression with the Huber loss function and is effective in detecting outliers. [sent-60, score-0.157]
</p><p>31 Compared with the least squares distance, the LSS distance is more effective in measuring the distance between the observation vector and the dictionary when outliers occur. [sent-61, score-0.671]
</p><p>32 Third, we design a generative tracker by using the LSS method, where the dictionary consists of PCA basis vectors. [sent-62, score-0.324]
</p><p>33 The observation likelihood of each candidate is computed based on the LSS distance. [sent-63, score-0.202]
</p><p>34 Furthermore, we update the tracker by using an effective update scheme. [sent-64, score-0.271]
</p><p>35 Numerous experiments on challenging image sequences with comparisons to state-ofthe-art tracking methods demonstrate the effectiveness of the proposed model and algorithm. [sent-65, score-0.219]
</p><p>36 The coefficient x can be obtained by maximizing the posteriori probability p (x|y), which is also equivalent to maximizing the joint likelihood probability p (x, y). [sent-82, score-0.165]
</p><p>37 Thus, the likelihood of the estimator (the joint probability of the error term e) is p (e) = fθ (ei). [sent-92, score-0.123]
</p><p>38 1), the MLE solution is equivalent to the ordinary le? [sent-102, score-0.135]
</p><p>39 If the error e follows the Laplacian distribution (ei ∈ L (0, σL)2), the MLE solution is equivalent to least absolute deviations (LAD) solution,  x? [sent-120, score-0.145]
</p><p>40 However, it is difficult to be solved by using either the simplex-based methods [6] or the iteratively reweighted least squares methods [22]. [sent-126, score-0.321]
</p><p>41 y = Ax  + n + s,  (4)  where the Gaussian component models small dense noise and the Laplacian one aims to handle outliers  3. [sent-136, score-0.199]
</p><p>42 , decomposing the noise into sparse and non-sparse ones [3, 8] for achieving robust motion estimation) have appeared in computer vision community. [sent-146, score-0.197]
</p><p>43 In this work, the proposed algorithm focuses on not only handling occlusion but also deriving a novel distance metric to compare the target candidate and the target template under the outlier condition, which will be presented later. [sent-148, score-0.376]
</p><p>44 Because of this, we treat the Laplacian noise term s as missing values with the same Laplacian prior, and therefore we can maximize the joint likelihood p (y, x, s) instead. [sent-173, score-0.204]
</p><p>45 Least Soft-thresold Squares Regression To maximize the joint likelihood of Eq. [sent-197, score-0.117]
</p><p>46 sa]sed = oarng tmhei standard least squares criterion and an ? [sent-206, score-0.289]
</p><p>47 As our algorithm consists of two main components: ordinary least squares and soft-thresholding operation, we denote it as the Least Soft-threshold Squares Regression method. [sent-240, score-0.395]
</p><p>48 Remark 1: The proposed least soft-threshold squares regression is equivalent to robust regression with the Huber loss function:  x = argmxin? [sent-254, score-0.525]
</p><p>49 We note the approach to compute robust regression with the Huber loss function is less efficient than the proposed method as the former is generally solved by using the iteratively reweighted least squares scheme which requires solving a weighted least squares problem (i. [sent-261, score-0.733]
</p><p>50 Robust line fitting by using the least soft-threshold squares (LSS) regression. [sent-268, score-0.289]
</p><p>51 Figure 1 shows an example of fitting a straight line when small Gaussian noise and outliers occur simultaneously. [sent-269, score-0.199]
</p><p>52 ; z10], A = [z, 1] and x = [a; b], we use the proposed least soft-threshold squares (LSS) method to estimate the parameter x. [sent-281, score-0.289]
</p><p>53 The matrix A is known as dictionary or basis matrix, and the vector ai is called an atom or basis  vector. [sent-295, score-0.16]
</p><p>54 For some vision applications (such as tracking), it requires not only to estimate the coefficient accurately but also to define a distance between a noisy observation and the dictionary or the subspace. [sent-296, score-0.238]
</p><p>55 The distance is usually defined to be inversely proportional to the maximum joint likelihood with respect to the coefficient x,  d∝(y −;lAog)mxaxp(y,x)  (10)  = −logmxaxp(y|x)p(x). [sent-298, score-0.182]
</p><p>56 Take the ordinary least squares method for example (i. [sent-299, score-0.395]
</p><p>57 Figure 2 illustrates a toy example of good and bad candidates for template matching with partial occlusion. [sent-337, score-0.21]
</p><p>58 In Table 2, we report the OLS distance and the LSS distance between the template and different candidates. [sent-340, score-0.147]
</p><p>59 ) template as t, the good candidate as yG and the bad candidate as yB respectively. [sent-348, score-0.218]
</p><p>60 In this example, dOLS (yB ; t) is smaller than dOLS (yG; t), which means the bad candidate is picked if the OLS distance is used. [sent-349, score-0.155]
</p><p>61 On the other hand, the good candidate is selected (dLSS (yG; t) < dLSS (yB ; t)) when the proposed LSS distance is used. [sent-350, score-0.1]
</p><p>62 Thus, we note that the proposed LSS distance is better than the OLS distance for handling outliers (e. [sent-351, score-0.204]
</p><p>63 A toy example of good and bad candidates for template matching. [sent-373, score-0.148]
</p><p>64 Least Soft-thresold Squares Tracking In this paper, visual tracking is treated as a dynamic Bayesian inference task with a hidden Markov model. [sent-375, score-0.178]
</p><p>65 , yt} up to the t-th frame, the aim is to estimate the target state variable xt by using the maximum a posteriori estimation,  x? [sent-379, score-0.209]
</p><p>66 p(xt|xt−1)p(xt−1|y1:t−1)xt−1, (16) where p (xt |xt−1) is the motion model that describes the state transition between consecutive frames, and p (yt |xt) is the observation model that estimates the likelihood of an observed image patch belonging to the object class. [sent-384, score-0.183]
</p><p>67 Observation Model: In this paper, we assume that the tracked target object is generated by a PCA subspace (spanned by U and centered at μ) with i. [sent-388, score-0.212]
</p><p>68 d GaussianLaplacian noise, y =  μ  + Uz + n + s,  (17)  where y denotes an observation vector, U represents a matrix of column basis vectors, z indicates the coefficients of basis vectors, n is the Gaussian noise component and s is  the Laplacian noise component. [sent-390, score-0.366]
</p><p>69 d Gaussian-Laplacian noise assumption, the distance between the vector y and the subspace (U, μ) is the least softthreshold squares distance, d(y;U,μ)  = mz,isn21 ? [sent-393, score-0.461]
</p><p>70 Thus, for each observation yi corresponding to a predicted state xi, we firstly solve the following optimization problem,  ? [sent-398, score-0.134]
</p><p>71 Then we reconstruct the observation vector 222333777533  by replacing the outliers with its corresponding parts of the mean vector μ,  yri=? [sent-476, score-0.2]
</p><p>72 , PCA basis vectors U and the mean vector μ) by using an incremental principal component analysis (PCA) method [20]. [sent-495, score-0.141]
</p><p>73 Experiments The proposed tracker is implemented in MATLAB and runs at 4 frames per second on a PC with Intel i7-3770 CPU (3. [sent-497, score-0.151]
</p><p>74 For each sequence,  ×  the location of the tracked target is manually labeled in the first frame. [sent-501, score-0.173]
</p><p>75 As a trade-off between effectiveness and speed, 600 particles are adopted and our tracker is incrementally updated every 5 frames. [sent-503, score-0.151]
</p><p>76 The challenging factors of these sequences include partial occlusion, illumination variation, pose change, background clutter and motion blur. [sent-517, score-0.202]
</p><p>77 We evaluate the proposed tracker against eleven state-of-the-art algorithms, including the FragT [1], IVT [20], MIL [4], VTD [15], TLD [14], APGL1 [5], MTT [3 1], LSAT [17], SCM [32], ASLSA [13] and OSPT [26] trackers. [sent-518, score-0.151]
</p><p>78 Given the tracking result (bounding box) of each frame RT and the corresponding ground truth bounding box RG, the overlap score is defined as score = Table 4 reports the average overlap rates, where larger average scores mean more accurate results. [sent-525, score-0.178]
</p><p>79 Qualitative Evaluation Severe Occlusion: We test several sequences (Occlusion1, Occlusion2, Caviar1, Caviar2, Caviar3, DavidOutdoor) with heavy or long-time partial occlusion, scale change and rotation. [sent-529, score-0.135]
</p><p>80 This can be attributed to two reasons: (1) the proposed LSS distance takes outliers (e. [sent-531, score-0.158]
</p><p>81 , occlusion) into account explicitly; and (2) the update scheme is able to avoid degrading the observation model by removing the outliers from new observed samples. [sent-533, score-0.243]
</p><p>82 The IVT method is sensitive to partial occlusion (Occlusion2, Caviar1, Caviar3, DavidOutdoor) since the OLS distance is not effective to handle outliers. [sent-535, score-0.219]
</p><p>83 Illumination Change: Figure 3 (d) shows the tracking results in the sequences (DavidIndoor, Car4, Singer1) with significant illumination variation, scale change and pose change. [sent-538, score-0.283]
</p><p>84 Due to the use of incremental PCA algorithm, the proposed tracker achieves good performance in dealing with the appearance change caused by light change. [sent-542, score-0.274]
</p><p>85 Background Clutter: Figure 3 (e) demonstrates the tracking results in the Car11, Deer and Football sequences with background clutter. [sent-544, score-0.219]
</p><p>86 These videos also pose other challenging factors including illumination variation (Car11), fast motion (Deer) and partial occlusion (Football). [sent-545, score-0.238]
</p><p>87 As the proposed LSS distance encourages good matching results when outliers occur, our tracker performs better than other methods in these videos (e. [sent-546, score-0.309]
</p><p>88 Fast Motion: Figure 3 (f) illustrates the tracking results on the Jumping, Owl and Face sequences. [sent-549, score-0.178]
</p><p>89 It is difficult to predict the locations of the tracked objects when they undergo abrupt motion. [sent-550, score-0.131]
</p><p>90 Furthermore, the appearance change caused by motion blur poses great challenges for capturing the tracked targets accurately and updating the observation models properly. [sent-551, score-0.288]
</p><p>91 Conclusion In this paper, we propose a Least Soft-thresold Squares (LSS) regression method that assumes the noise is Gaussian-Laplacian distributed, and apply it to object tracking. [sent-563, score-0.171]
</p><p>92 We derive a LSS distance to measure the difference between an observation sample and the dictionary. [sent-565, score-0.134]
</p><p>93 The LSS distance is effective in handling outliers and therefore provides an accurate match, which facilitates object tracking  (e. [sent-566, score-0.37]
</p><p>94 In addition, we develop a robust generative tracker based on the proposed LSS method and a simple update scheme. [sent-569, score-0.298]
</p><p>95 Both quantitative and qualitative evaluations on challenging image sequences show that the proposed tracker performs favorably against several state-of-the-art algorithms. [sent-570, score-0.192]
</p><p>96 Visual tracking via adaptive structural local sparse appearance model. [sent-669, score-0.178]
</p><p>97 222333777755  (a) Tracking results on sequence Oc lusion1 and Oc lusion2 with heavy oc lusion and in-plane rotation. [sent-677, score-0.102]
</p><p>98 (b) Tracking results on sequence Caviar1 and Caviar2 with partial oc lusion and scale change. [sent-678, score-0.164]
</p><p>99 Robust tracking using local sparse appearance model and k-selection. [sent-733, score-0.178]
</p><p>100 The estimation of laplace random vectors in additive white gaussian noise. [sent-774, score-0.097]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lss', 0.629), ('squares', 0.208), ('ols', 0.195), ('tracking', 0.178), ('ax', 0.153), ('tracker', 0.151), ('ivt', 0.144), ('xt', 0.137), ('ei', 0.123), ('outliers', 0.112), ('ordinary', 0.106), ('laplacian', 0.105), ('tld', 0.104), ('tracked', 0.101), ('observation', 0.088), ('noise', 0.087), ('si', 0.085), ('regression', 0.084), ('least', 0.081), ('occlusion', 0.077), ('yg', 0.076), ('deer', 0.076), ('templates', 0.075), ('dlss', 0.073), ('dols', 0.073), ('fnl', 0.073), ('gaussianlaplacian', 0.073), ('target', 0.072), ('pdf', 0.071), ('owl', 0.065), ('scm', 0.065), ('generative', 0.065), ('lu', 0.063), ('pca', 0.063), ('partial', 0.062), ('oc', 0.062), ('uz', 0.06), ('incremental', 0.06), ('likelihood', 0.06), ('huber', 0.06), ('mil', 0.058), ('dictionary', 0.056), ('bad', 0.055), ('template', 0.055), ('candidate', 0.054), ('football', 0.054), ('maxp', 0.052), ('mle', 0.052), ('basis', 0.052), ('aslas', 0.049), ('aslsa', 0.049), ('dalian', 0.049), ('davidindoor', 0.049), ('davidoutdoor', 0.049), ('erfc', 0.049), ('logmxaxp', 0.049), ('yb', 0.049), ('coefficient', 0.048), ('pages', 0.048), ('distance', 0.046), ('yi', 0.046), ('update', 0.043), ('lad', 0.043), ('xi', 0.042), ('id', 0.042), ('sequences', 0.041), ('lusion', 0.04), ('ucme', 0.04), ('mei', 0.04), ('gaussian', 0.039), ('robust', 0.039), ('subspace', 0.039), ('trivial', 0.039), ('candidates', 0.038), ('rced', 0.038), ('fl', 0.036), ('fifteen', 0.036), ('decomposing', 0.036), ('motion', 0.035), ('error', 0.035), ('effective', 0.034), ('reweighted', 0.032), ('xti', 0.032), ('change', 0.032), ('factors', 0.032), ('illumination', 0.032), ('targets', 0.032), ('jumping', 0.031), ('dealing', 0.031), ('yt', 0.031), ('mxin', 0.031), ('remark', 0.03), ('abrupt', 0.03), ('vectors', 0.029), ('additive', 0.029), ('equivalent', 0.029), ('maximize', 0.029), ('mea', 0.028), ('saffari', 0.028), ('joint', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="267-tfidf-1" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>Author: Dong Wang, Huchuan Lu, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods, the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution, which can be solved efficiently. Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.</p><p>2 0.23203981 <a title="267-tfidf-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.22821175 <a title="267-tfidf-3" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>4 0.20887761 <a title="267-tfidf-4" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>5 0.18724956 <a title="267-tfidf-5" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><p>6 0.17903142 <a title="267-tfidf-6" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>7 0.15922172 <a title="267-tfidf-7" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>8 0.14474605 <a title="267-tfidf-8" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>9 0.11461753 <a title="267-tfidf-9" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>10 0.11137909 <a title="267-tfidf-10" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>11 0.1103785 <a title="267-tfidf-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.10653303 <a title="267-tfidf-12" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>13 0.098164283 <a title="267-tfidf-13" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>14 0.09608674 <a title="267-tfidf-14" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>15 0.091965251 <a title="267-tfidf-15" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>16 0.086137921 <a title="267-tfidf-16" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>17 0.085699104 <a title="267-tfidf-17" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>18 0.083704881 <a title="267-tfidf-18" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>19 0.082164183 <a title="267-tfidf-19" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>20 0.078848407 <a title="267-tfidf-20" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, 0.003), (2, -0.068), (3, 0.001), (4, 0.005), (5, -0.044), (6, 0.158), (7, -0.144), (8, 0.099), (9, 0.148), (10, -0.071), (11, -0.053), (12, -0.138), (13, 0.076), (14, -0.043), (15, -0.026), (16, 0.001), (17, -0.009), (18, 0.037), (19, 0.025), (20, 0.048), (21, 0.009), (22, 0.045), (23, -0.062), (24, -0.014), (25, -0.036), (26, -0.089), (27, -0.019), (28, 0.048), (29, 0.04), (30, 0.073), (31, 0.032), (32, -0.016), (33, -0.0), (34, -0.078), (35, -0.03), (36, -0.033), (37, 0.027), (38, -0.02), (39, 0.006), (40, -0.019), (41, 0.003), (42, 0.008), (43, 0.014), (44, -0.024), (45, -0.004), (46, -0.007), (47, 0.044), (48, 0.015), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95856291 <a title="267-lsi-1" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>Author: Dong Wang, Huchuan Lu, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods, the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution, which can be solved efficiently. Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.</p><p>2 0.88037831 <a title="267-lsi-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.87803423 <a title="267-lsi-3" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>4 0.86180162 <a title="267-lsi-4" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><p>5 0.80317706 <a title="267-lsi-5" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>6 0.77740139 <a title="267-lsi-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.76267117 <a title="267-lsi-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.69701719 <a title="267-lsi-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.67579198 <a title="267-lsi-9" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>10 0.64919674 <a title="267-lsi-10" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>11 0.61379647 <a title="267-lsi-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.58986139 <a title="267-lsi-12" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>13 0.57400608 <a title="267-lsi-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.55449522 <a title="267-lsi-14" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>15 0.53472805 <a title="267-lsi-15" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>16 0.53027982 <a title="267-lsi-16" href="./cvpr-2013-Nonlinearly_Constrained_MRFs%3A_Exploring_the_Intrinsic_Dimensions_of_Higher-Order_Cliques.html">308 cvpr-2013-Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques</a></p>
<p>17 0.52827758 <a title="267-lsi-17" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>18 0.52754563 <a title="267-lsi-18" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>19 0.51897389 <a title="267-lsi-19" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>20 0.51865602 <a title="267-lsi-20" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.168), (16, 0.018), (26, 0.068), (30, 0.019), (33, 0.225), (36, 0.202), (39, 0.01), (67, 0.072), (69, 0.052), (87, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87062389 <a title="267-lda-1" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>2 0.85861385 <a title="267-lda-2" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>same-paper 3 0.84689331 <a title="267-lda-3" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>Author: Dong Wang, Huchuan Lu, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods, the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution, which can be solved efficiently. Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.</p><p>4 0.8394233 <a title="267-lda-4" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>5 0.83185005 <a title="267-lda-5" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>6 0.82385528 <a title="267-lda-6" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>7 0.81602138 <a title="267-lda-7" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>8 0.81554264 <a title="267-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.81402361 <a title="267-lda-9" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>10 0.81216151 <a title="267-lda-10" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>11 0.80888486 <a title="267-lda-11" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>12 0.80594462 <a title="267-lda-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.80453086 <a title="267-lda-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.80429476 <a title="267-lda-14" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>15 0.80280268 <a title="267-lda-15" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>16 0.80190575 <a title="267-lda-16" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>17 0.80163789 <a title="267-lda-17" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>18 0.80106801 <a title="267-lda-18" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>19 0.79911703 <a title="267-lda-19" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>20 0.79878914 <a title="267-lda-20" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
