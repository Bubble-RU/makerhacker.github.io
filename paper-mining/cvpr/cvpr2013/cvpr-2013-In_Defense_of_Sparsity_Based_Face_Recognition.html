<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>220 cvpr-2013-In Defense of Sparsity Based Face Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-220" href="#">cvpr2013-220</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>220 cvpr-2013-In Defense of Sparsity Based Face Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-220-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Deng_In_Defense_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Weihong Deng, Jiani Hu, Jun Guo</p><p>Abstract: The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. The experiments results on AR, FERET and FRGC databases validate that, if the proposed prototype plus variation representation model is applied, sparse coding plays a crucial role in face recognition, and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.</p><p>Reference: <a title="cvpr-2013-220-reference" href="../cvpr2013_reference/cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. [sent-3, score-0.457]
</p><p>2 A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. [sent-4, score-0.569]
</p><p>3 This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. [sent-5, score-0.51]
</p><p>4 Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. [sent-6, score-0.296]
</p><p>5 Introduction  The sparse representation-based classification (SRC) algorithm for face recognition was introduced by Wright et al. [sent-9, score-0.281]
</p><p>6 The key idea of that paper is a judicious choice of dictionary: representing the test image as a sparse linear combination of the the training images themselves. [sent-11, score-0.171]
</p><p>7 Motivated by the conditional equivalence of the sparsity measured by ? [sent-12, score-0.125]
</p><p>8 Finally, the test sample is classified by checking which class yields minimum representation error. [sent-16, score-0.149]
</p><p>9 The success of SRC has largely boosted the research of sparsity based face recognition. [sent-17, score-0.293]
</p><p>10 combined SRC with markov random fields to recognize the disguise face with large contiguous occlusion [20]. [sent-21, score-0.24]
</p><p>11 applied the sparse coding to jointly address blind image restoration and blurred face recognition [18]. [sent-25, score-0.308]
</p><p>12 introduced a discriminative dictionary learning method to improve the accuracy and efficiency of face recognition [17]. [sent-27, score-0.402]
</p><p>13 The sparse representation based face recognition assumes that the training images have been carefully controlled and that the number of samples per class is sufficiently large. [sent-29, score-0.504]
</p><p>14 [14] It is the purpose of this paper to challenge the common view that the sparsity based face recognition is inadequate with the uncontrolled training samples. [sent-31, score-0.595]
</p><p>15 The inferior performance of SRC can properly be traced to the training samples based dictionary that do not distinguish the classspecific prototype and the intra-class variation. [sent-32, score-0.472]
</p><p>16 It is shown in this paper that a simple variant of SRC, which represents the test sample as a sparse linear combination of the class centroid and the differences to the class centroid, leads to an enormous improvement under the uncontrolled training conditions. [sent-33, score-0.615]
</p><p>17 The Debates on SRC Denote the training samples of all k classes as the matrix A = [A1, A2 , . [sent-37, score-0.132]
</p><p>18 , Ak] ∈ Rd×n, where the sub-matrix  Ai ∈ Rd×ni stacks the training samples ofclass i. [sent-40, score-0.125]
</p><p>19 Then, the 333999779  linear representation of a testing sample y can be rewritten as y = Ax0 + z (1) where x0 is a sparse vector whose entries are zeros except those associated with the ith class, and z ∈ Rd is a noise ttehormse w asitsho cb ioautendde wdi energy ? [sent-41, score-0.139]
</p><p>20 2-regularized method, called collaborative representation based classification (CRC), had very competitive face recognition accuracy to the ? [sent-55, score-0.25]
</p><p>21 Based on their results, the sparsity based face recognition seems to be useful, but not necessary. [sent-57, score-0.324]
</p><p>22 It is possible that the sample size per class used in [19] is still not enough to directly ensemble a over-complete dictionary for the face recognition problem. [sent-61, score-0.521]
</p><p>23 How to design an over-complete dictionary with limited sample size per class is an essential problem for sparsity based face recognition. [sent-64, score-0.594]
</p><p>24 Prototype rithm  plus Variation Model and Algo-  The previous studies in [11] [13] [19] have revealed the limitations of sparsity based recognition when the training images are corrupted and the number of samples per class is insufficient. [sent-66, score-0.537]
</p><p>25 This section introduces a prototype plus variation (P+V) model and a corresponding sparsity based classification algorithm to address these limitations of SRC. [sent-67, score-0.512]
</p><p>26 We further assume that yp is sparsely generated using the model with a prototype dictionary (matrix) P = [P1, P2 , . [sent-73, score-0.4]
</p><p>27 , Pk] ∈ Rd×m, where the sub-matrix Pi ∈ Rd×mi stacks the mi prototypical bases of class i. [sent-76, score-0.199]
</p><p>28 If there are redundant and overcomplete facial variant bases in V , the combination coefficients in β0 are naturally sparse. [sent-81, score-0.189]
</p><p>29 In general, P+V model has two advantages over the traditional sparse model in (3): •  •  P+V model improves the robustness against the contPa+mVina mtoivdee training samples. [sent-84, score-0.142]
</p><p>30 By separating tth teh image contaminations to the variation matrix that is shared by all classes, the class-specific prototypes would become clean and natural, and thus the classification would not be deteriorated by the corrupted training sample. [sent-85, score-0.293]
</p><p>31 P+V model requires less samples per class to construct an over-complete dictionary. [sent-86, score-0.135]
</p><p>32 pAles sth pee rva crlaiasstio ton cmonatsrtirxu cist shared by all classes, the dictionary size of the class i is expanded from mi to mi + q. [sent-87, score-0.305]
</p><p>33 Once q is sufficiently large, the overcomplete dictionary for each class can be readily constructed. [sent-88, score-0.291]
</p><p>34 The illustrative examples of the prototype plus variation (P+V) model. [sent-90, score-0.344]
</p><p>35 A Superposed SRC Algorithm To show the strength of the P+V model, we propose a very simple classification algorithm according to this model and demonstrate its effectiveness on face recognition un-  der uncontrolled training conditions. [sent-95, score-0.491]
</p><p>36 Given a data set with multiple images per subject, the ni samples of subject i, stacked as vectors, form a matrix Ai ∈ Rd×ni ,i = 1, . [sent-96, score-0.147]
</p><p>37 As the prototypes are represented by class cen∈tr Roids, the variation matrix is naturally constructed by the sample based difference to the centroids as follows: V = [A1 − c1e1T, . [sent-112, score-0.284]
</p><p>38 , Ak − ckekT] ∈ Rd×n  (10)  where ci is the class centroid of class i. [sent-115, score-0.17]
</p><p>39 1 illustrates an typical examples of the prototype and variation matrices. [sent-117, score-0.249]
</p><p>40 When number of samples per class is insufficient, and in particular when only a single sample per class is available, the intra-class variation matrix would become collapsed. [sent-118, score-0.366]
</p><p>41 To address this difficult, one can acquire the variant bases from the generic subjects outside the gallery, as the P+V model has assumed that the intra-class variations of different subjects are sharable. [sent-119, score-0.219]
</p><p>42 The nonzero coefficients are expected to concentrate on the centroid of the same class as the test sample and on the related intra-class differences. [sent-121, score-0.194]
</p><p>43 Superposed Sparse Representation based Classification (SSRC) 1: Input: a matrix of training samples A = [A1, A2 , . [sent-123, score-0.132]
</p><p>44 Compute the prototype matrix P according to (9), and the variation matrix V according to (10). [sent-127, score-0.309]
</p><p>45 When the sample size per class is insufficient, the matrix V can be computed from a set of generic samples outside the gallery. [sent-128, score-0.244]
</p><p>46 2: Derive the projection matrix Φ ∈ Rd×p by applying DPCerAiv on hthee p training samples A Φ, a ∈nd R project the prototype and variation matrices to the p-dimensional space. [sent-129, score-0.381]
</p><p>47 new vector whose only nonzero entries are ∈the R entries in αˆ 1 that are associated with class i. [sent-177, score-0.125]
</p><p>48 Related Works and Discussions There are several previous methods that aim to improve the robustness of SRC by appending additional bases to the conventional dictionary of training images. [sent-181, score-0.379]
</p><p>49 addressed the disguise problem by adding a complete set of single-pixel based bases to the dictionary of SRC [15]. [sent-183, score-0.331]
</p><p>50 Yang and Zhang [16] used the Gabor features for SRC with a learned Gabor occlusion dictionary to reduce the computational cost. [sent-184, score-0.239]
</p><p>51 introduced Extended SRC (ESRC) method to address the undersampled problem of SRC by representing the typical facial variations in an additional dictionary [3]. [sent-186, score-0.296]
</p><p>52 These methods are effective to improve the robustness against the corruption ofthe test images, but they are still sensitive to the corruption of the training images. [sent-187, score-0.316]
</p><p>53 Comparative recognition rates of SSRC and other recognition methods. [sent-189, score-0.143]
</p><p>54 504910% % The proposed P+V model and the corresponding SSRC algorithm, for the first time, design the dictionary by the decomposition of the training samples into the separated parts of prototypes and variations. [sent-200, score-0.356]
</p><p>55 Therefore, the P+V model based classification is expected to be robust against the corruption of both the training and test images. [sent-201, score-0.212]
</p><p>56 [2] also aimed to address the training corruption problem, but they only filtered out the corruption by low-rank and sparse decomposition, without any concern of the typical intra-class variations in the dictionary setting. [sent-203, score-0.548]
</p><p>57 [13], SRC performs worse because the randomly selected training set contains corruption images occlusion that would break the sparsity assumption. [sent-232, score-0.342]
</p><p>58 However, one should not deny the the usefulness of the sparsity based recognition according to the above results, as we find that the discrimination power of sparse representation relies heavily on the suitable choice of dictionary. [sent-233, score-0.268]
</p><p>59 By simply re-designing the dictionary by the P+V model, the SSRC dramatically boost the sparsity based recognition accuracy to over 98%. [sent-236, score-0.38]
</p><p>60 The ESRC method, which appends an intra-class dictionary to the training samples, also increases the accuracy to about 97%, but using a much larger dictionary of 2600 bases. [sent-237, score-0.464]
</p><p>61 In total, there are 8 training images and 12 test images per person. [sent-241, score-0.162]
</p><p>62 In total, there are 8 training images and 12 test images per person. [sent-243, score-0.162]
</p><p>63 Sunglasses+Scarf: Seven neutral images and two corrupted images (aornfe: Sweivthe sunglasses aangde sth aen dot thwero w coitrhscarf) at session 1 are selected for training. [sent-244, score-0.404]
</p><p>64 The comparative recognition rates between SRC and SSRC on the AR data set with different kinds of corrupted training images. [sent-246, score-0.262]
</p><p>65 there are 9 training images and 17 test images (seven neutral images at session 2 plus the remaining ten occluded images) are available for this case. [sent-247, score-0.417]
</p><p>66 We vary the dimension of the eigenspace from 20 to 500, and compare the recognition performance of between SRC and SSRC. [sent-248, score-0.142]
</p><p>67 2 shows the comparative recognition rates between SRC and SSRC on the AR data set with different kinds of corrupted training im-  ages, and one can see from the figure that SSRC outperforms SRC by a margin about 6% to 12%, depending on the percentage of occlusion. [sent-251, score-0.262]
</p><p>68 Specifically, SRC performs better on the sunglasses scenario (about 84% accuracy with 20% occlusion) than the scarf scenario (about 80% accuracy with 40% occlusion), followed by the sunglasses+scarf scenario (about 78% accuracy). [sent-252, score-0.292]
</p><p>69 The performance of SRC deteriorates when the percentage of occlusion involved in the training images increases, and this is an observation consistent with the common criticism on SRC with uncontrolled training images [14]. [sent-253, score-0.407]
</p><p>70 Besides the boosted accuracy, SSRC displays the stability against various kinds of corruption in the training images. [sent-255, score-0.181]
</p><p>71 The average accuracies of the first five methods are cited from [2], of which the best-performed method, denoted as LR+SI+SRC, applied low-rank matrix recovery with structural incoherence to filter out the corruption of the training images. [sent-257, score-0.26]
</p><p>72 Comparative recognition rates of SSRC and other recognition methods. [sent-260, score-0.143]
</p><p>73 (a) The cropped images of some gallery images and corresponding probe images in the FERET database. [sent-281, score-0.222]
</p><p>74 (b) Example images of the differences to the class centroid computed from the  FRGC version 2 database. [sent-282, score-0.131]
</p><p>75 Recognition with Uncontrolled and Overcomplete Dictionary This experiment is designed to test the robustness of SSRC against complex facial variation in the real-world applications. [sent-286, score-0.172]
</p><p>76 fb probe set contains 1,195 images taken with an alterfnba tpivroeb bfeac sieatl expression. [sent-288, score-0.128]
</p><p>77 fc probe set contains 194 images taken under different lighting conditions. [sent-289, score-0.128]
</p><p>78 dup1 probe set contains 722 images taken in a different tdimupe1. [sent-290, score-0.128]
</p><p>79 dup2 probe set contains 234 images taken at least a year 2la pteror,b weh siecth c iosn a asiunbsse 23t o4f itmhea dup1 skeetn. [sent-291, score-0.128]
</p><p>80 As there is only a single sample per gallery class, we construct the intra-class variation matrix from the standard training image set of the FRGC Version 2 database [9], which contains 12,766 frontal images of 222 people taken in the uncontrolled conditions. [sent-296, score-0.564]
</p><p>81 Hence, in this experiment, the variation matrix is required to universally represent the complex facial variations under uncontrolled conditions. [sent-300, score-0.361]
</p><p>82 For each feature, we test the recognition performance in the reduced PCA dimension of 125, 250, and 1000 respectively. [sent-302, score-0.124]
</p><p>83 These results suggest that the P+V model is feasible for various feature representations, and thus it can be integrated with more informative features to address uncontrolled face recognition problem. [sent-319, score-0.434]
</p><p>84 It should be mentioned that similar experimental results has been reported on ESRC method [3], but its intra-class variant dictionary are constructed from the generic training  set of FERET database. [sent-321, score-0.318]
</p><p>85 In contrast, our experiment, for the first time, justifies the effectiveness of the sparsity based face recognition when the dictionary bases are collectedfrom the uncontrolled conditions that are independent from the test condition. [sent-325, score-0.886]
</p><p>86 2-norm regularization Over-complete Dictionary  with  Based on the results on the FERET database, we further investigate the role of sparsity in face recognition with an uncontrolled and over-complete dictionary. [sent-330, score-0.606]
</p><p>87 1-norm indeed play a crucial role in face recognition given an uncontrolled and over-complete dictionary. [sent-386, score-0.432]
</p><p>88 1-norm sparsity is different from that by Zhang et al. [sent-388, score-0.125]
</p><p>89 Indeed, both observations are valid, but under different dictionary settings. [sent-390, score-0.203]
</p><p>90 The dictionary of SSRC contains an over-complete set of intra-class variation bases, and most of which are irrelevant to the test sample. [sent-395, score-0.316]
</p><p>91 Conclusions It has been shown in this paper that a simple separation between the prototype and variation components leads to an enormous improvement on sparsity based face recognition under uncontrolled training conditions. [sent-404, score-0.868]
</p><p>92 1-norm regularization based sparse coding the algorithms accurately find out the intra-class variation bases from an over-complete dictionary that is constructed from uncontrolled generic images outside the gallery. [sent-408, score-0.801]
</p><p>93 Our preliminary results suggest that the proposed prototype plus variation model provides a widely applicable framework to address uncontrolled face recognition problem. [sent-409, score-0.778]
</p><p>94 Low-rank matrix recovery with structural incoherence for robust face recognition. [sent-428, score-0.205]
</p><p>95 Extended src: Undersampled face recognition via intraclass variant dictionary. [sent-434, score-0.252]
</p><p>96 Sparse representation for face recognition based on discriminative low-rank dictionary learning. [sent-453, score-0.432]
</p><p>97 Towards a practical face recognition system: robust registration and illumination by sparse representation. [sent-502, score-0.26]
</p><p>98 Gabor feature based sparse representation for face recognition with gabor occlusion dictionary. [sent-533, score-0.373]
</p><p>99 Close the loop: Joint blind image restoration and recognition with sparse representation prior. [sent-548, score-0.143]
</p><p>100 Sparse representation or collaborative representation: Which helps face recognition? [sent-554, score-0.177]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ssrc', 0.557), ('src', 0.485), ('uncontrolled', 0.213), ('dictionary', 0.203), ('feret', 0.176), ('prototype', 0.167), ('face', 0.147), ('sunglasses', 0.141), ('sparsity', 0.125), ('frgc', 0.106), ('corruption', 0.102), ('session', 0.099), ('plus', 0.095), ('bases', 0.095), ('esrc', 0.088), ('scarf', 0.088), ('wright', 0.085), ('variation', 0.082), ('probe', 0.079), ('neutral', 0.071), ('superposed', 0.071), ('comparative', 0.062), ('sparse', 0.061), ('class', 0.06), ('rd', 0.06), ('training', 0.058), ('wagner', 0.056), ('ar', 0.056), ('recognition', 0.052), ('corrupted', 0.051), ('prototypes', 0.051), ('centroid', 0.05), ('eigenspace', 0.049), ('regularization', 0.049), ('yv', 0.049), ('gallery', 0.047), ('gabor', 0.047), ('samples', 0.044), ('cited', 0.042), ('err', 0.042), ('dimension', 0.041), ('deng', 0.04), ('rates', 0.039), ('facial', 0.036), ('occlusion', 0.036), ('superposition', 0.035), ('undersampled', 0.035), ('cropped', 0.033), ('seven', 0.033), ('centroids', 0.033), ('criticized', 0.033), ('disguise', 0.033), ('per', 0.031), ('reproduction', 0.031), ('prevailing', 0.031), ('test', 0.031), ('ganesh', 0.031), ('matrix', 0.03), ('yp', 0.03), ('representation', 0.03), ('variant', 0.03), ('dimensional', 0.03), ('ization', 0.029), ('lr', 0.029), ('databases', 0.029), ('overcomplete', 0.028), ('sample', 0.028), ('incoherence', 0.028), ('taken', 0.028), ('ak', 0.028), ('generic', 0.027), ('yang', 0.027), ('coding', 0.026), ('database', 0.026), ('nonzero', 0.025), ('norm', 0.025), ('phillips', 0.025), ('zhang', 0.024), ('contiguous', 0.024), ('outside', 0.024), ('enormous', 0.024), ('symbols', 0.024), ('zhou', 0.024), ('robustness', 0.023), ('stacks', 0.023), ('intraclass', 0.023), ('address', 0.022), ('arxiv', 0.022), ('ni', 0.021), ('acquire', 0.021), ('classification', 0.021), ('scenario', 0.021), ('boosted', 0.021), ('mi', 0.021), ('images', 0.021), ('switching', 0.02), ('role', 0.02), ('entries', 0.02), ('argmin', 0.02), ('conditions', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="220-tfidf-1" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>Author: Weihong Deng, Jiani Hu, Jun Guo</p><p>Abstract: The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. The experiments results on AR, FERET and FRGC databases validate that, if the proposed prototype plus variation representation model is applied, sparse coding plays a crucial role in face recognition, and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.</p><p>2 0.36077049 <a title="220-tfidf-2" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>3 0.34922314 <a title="220-tfidf-3" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>4 0.19230494 <a title="220-tfidf-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.1724734 <a title="220-tfidf-5" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>6 0.1685847 <a title="220-tfidf-6" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>7 0.16092104 <a title="220-tfidf-7" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>8 0.16036566 <a title="220-tfidf-8" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>9 0.15249638 <a title="220-tfidf-9" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>10 0.1464929 <a title="220-tfidf-10" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>11 0.14182581 <a title="220-tfidf-11" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>12 0.11291073 <a title="220-tfidf-12" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>13 0.11006773 <a title="220-tfidf-13" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>14 0.10701125 <a title="220-tfidf-14" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>15 0.10671063 <a title="220-tfidf-15" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>16 0.10131292 <a title="220-tfidf-16" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>17 0.10101051 <a title="220-tfidf-17" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>18 0.0959002 <a title="220-tfidf-18" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>19 0.094027884 <a title="220-tfidf-19" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>20 0.092527874 <a title="220-tfidf-20" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, -0.121), (2, -0.198), (3, 0.197), (4, -0.044), (5, -0.073), (6, 0.038), (7, 0.0), (8, 0.133), (9, -0.06), (10, 0.066), (11, 0.007), (12, 0.051), (13, 0.07), (14, 0.028), (15, 0.029), (16, 0.039), (17, 0.022), (18, -0.035), (19, 0.05), (20, -0.028), (21, 0.041), (22, 0.024), (23, 0.052), (24, 0.035), (25, -0.048), (26, -0.055), (27, 0.028), (28, -0.055), (29, -0.012), (30, 0.045), (31, 0.045), (32, 0.019), (33, -0.08), (34, 0.004), (35, -0.012), (36, -0.065), (37, -0.001), (38, -0.032), (39, 0.019), (40, -0.113), (41, -0.011), (42, -0.007), (43, -0.054), (44, 0.027), (45, 0.051), (46, -0.053), (47, 0.031), (48, -0.036), (49, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94031328 <a title="220-lsi-1" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>Author: Weihong Deng, Jiani Hu, Jun Guo</p><p>Abstract: The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. The experiments results on AR, FERET and FRGC databases validate that, if the proposed prototype plus variation representation model is applied, sparse coding plays a crucial role in face recognition, and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.</p><p>2 0.85871124 <a title="220-lsi-2" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>3 0.81223661 <a title="220-lsi-3" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>4 0.70927298 <a title="220-lsi-4" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>5 0.68780422 <a title="220-lsi-5" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>6 0.68197662 <a title="220-lsi-6" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>7 0.66500294 <a title="220-lsi-7" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>8 0.6642369 <a title="220-lsi-8" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>9 0.65183836 <a title="220-lsi-9" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>10 0.61662447 <a title="220-lsi-10" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>11 0.58288515 <a title="220-lsi-11" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>12 0.58157045 <a title="220-lsi-12" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>13 0.57468444 <a title="220-lsi-13" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>14 0.57154721 <a title="220-lsi-14" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>15 0.57146353 <a title="220-lsi-15" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>16 0.55859089 <a title="220-lsi-16" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>17 0.54026687 <a title="220-lsi-17" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>18 0.51030105 <a title="220-lsi-18" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>19 0.49424976 <a title="220-lsi-19" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>20 0.49346921 <a title="220-lsi-20" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.099), (16, 0.032), (26, 0.057), (33, 0.235), (39, 0.042), (41, 0.235), (67, 0.074), (69, 0.046), (77, 0.013), (87, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80281723 <a title="220-lda-1" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>Author: Weihong Deng, Jiani Hu, Jun Guo</p><p>Abstract: The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. The experiments results on AR, FERET and FRGC databases validate that, if the proposed prototype plus variation representation model is applied, sparse coding plays a crucial role in face recognition, and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.</p><p>2 0.79640698 <a title="220-lda-2" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>Author: Paul Wohlhart, Martin Köstinger, Michael Donoser, Peter M. Roth, Horst Bischof</p><p>Abstract: The development of complex, powerful classifiers and their constant improvement have contributed much to the progress in many fields of computer vision. However, the trend towards large scale datasets revived the interest in simpler classifiers to reduce runtime. Simple nearest neighbor classifiers have several beneficial properties, such as low complexity and inherent multi-class handling, however, they have a runtime linear in the size of the database. Recent related work represents data samples by assigning them to a set of prototypes that partition the input feature space and afterwards applies linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper, we go a step beyond these approaches and purely focus on 1-nearest prototype classification, where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable, parameter free, avoids noise overfitting and, since during testing only comparisons to the derived prototypes are required, highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods, while even getting close to the results of more complex classifiers.</p><p>3 0.78581458 <a title="220-lda-3" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>4 0.76351792 <a title="220-lda-4" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>Author: Xiaobo Ren, Tony X. Han, Zhihai He</p><p>Abstract: We consider video object cut as an ensemble of framelevel background-foreground object classifiers which fuses information across frames and refine their segmentation results in a collaborative and iterative manner. Our approach addresses the challenging issues of modeling of background with dynamic textures and segmentation of foreground objects from cluttered scenes. We construct patch-level bagof-words background models to effectively capture the background motion and texture dynamics. We propose a foreground salience graph (FSG) to characterize the similarity of an image patch to the bag-of-words background models in the temporal domain and to neighboring image patches in the spatial domain. We incorporate this similarity information into a graph-cut energy minimization framework for foreground object segmentation. The background-foreground classification results at neighboring frames are fused together to construct a foreground probability map to update the graph weights. The resulting object shapes at neighboring frames are also used as constraints to guide the energy minimization process during graph cut. Our extensive experimental results and performance comparisons over a diverse set of challenging videos with dynamic scenes, including the new Change Detection Challenge Dataset, demonstrate that the proposed ensemble video object cut method outperforms various state-ofthe-art algorithms.</p><p>5 0.75872952 <a title="220-lda-5" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>Author: Byung-soo Kim, Shili Xu, Silvio Savarese</p><p>Abstract: In this paper we focus on the problem of detecting objects in 3D from RGB-D images. We propose a novel framework that explores the compatibility between segmentation hypotheses of the object in the image and the corresponding 3D map. Our framework allows to discover the optimal location of the object using a generalization of the structural latent SVM formulation in 3D as well as the definition of a new loss function defined over the 3D space in training. We evaluate our method using two existing RGB-D datasets. Extensive quantitative and qualitative experimental results show that our proposed approach outperforms state-of-theart as methods well as a number of baseline approaches for both 3D and 2D object recognition tasks.</p><p>6 0.75617367 <a title="220-lda-6" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>7 0.75459296 <a title="220-lda-7" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>8 0.7543363 <a title="220-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.75410432 <a title="220-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.75382811 <a title="220-lda-10" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>11 0.75344414 <a title="220-lda-11" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>12 0.7517674 <a title="220-lda-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.7499091 <a title="220-lda-13" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>14 0.74964488 <a title="220-lda-14" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>15 0.74896568 <a title="220-lda-15" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>16 0.74854869 <a title="220-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.74797153 <a title="220-lda-17" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>18 0.74766213 <a title="220-lda-18" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>19 0.74713498 <a title="220-lda-19" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>20 0.74642521 <a title="220-lda-20" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
