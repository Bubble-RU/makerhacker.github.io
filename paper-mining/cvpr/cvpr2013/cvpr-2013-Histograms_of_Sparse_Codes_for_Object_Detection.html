<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>204 cvpr-2013-Histograms of Sparse Codes for Object Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-204" href="#">cvpr2013-204</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>204 cvpr-2013-Histograms of Sparse Codes for Object Detection</h1>
<br/><p>Source: <a title="cvpr-2013-204-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ren_Histograms_of_Sparse_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xiaofeng Ren, Deva Ramanan</p><p>Abstract: Object detection has seen huge progress in recent years, much thanks to the heavily-engineered Histograms of Oriented Gradients (HOG) features. Can we go beyond gradients and do better than HOG? Weprovide an affirmative answer byproposing and investigating a sparse representation for object detection, Histograms of Sparse Codes (HSC). We compute sparse codes with dictionaries learned from data using K-SVD, and aggregate per-pixel sparse codes to form local histograms. We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. To keep training (and testing) efficient, we apply dimension reduction by computing SVD on learned models, and adopt supervised training where latent positions of roots and parts are given externally e.g. from a HOG-based detector. By learning and using local representations that are much more expressive than gradients, we demonstrate large improvements over the state of the art on the PASCAL benchmark for both root- only and part-based models.</p><p>Reference: <a title="cvpr-2013-204-reference" href="../cvpr2013_reference/cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We compute sparse codes with dictionaries learned from data using K-SVD, and aggregate per-pixel sparse codes to form local histograms. [sent-6, score-0.627]
</p><p>2 We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. [sent-7, score-0.144]
</p><p>3 To keep training (and testing) efficient, we apply dimension reduction by computing SVD on learned models, and adopt supervised training where latent positions of roots and parts are given externally e. [sent-8, score-0.42]
</p><p>4 There has been huge progress in object detection in recent years, much thanks to the celebrated Histograms of Oriented Gradients (HOG) features [8, 13]. [sent-14, score-0.133]
</p><p>5 We develop Histograms-of-Sparse-Codes (HSC), which represents local patches through learned sparse codes instead of gradients and outperforms HOG by a large margin in state-of-the-art sliding window detection. [sent-95, score-0.504]
</p><p>6 There are evidences that local features are most crucial for detection [23], and we may already be saturating the capacity of HOG [36]. [sent-99, score-0.131]
</p><p>7 In the wake of recent advances in feature learning [16, 1] and its successes in many vision problems such as recognition [19] and grouping [26], it is promising to consider employing local features automatically learned from data. [sent-101, score-0.222]
</p><p>8 However, feature learning for detection is a challenging problem, which has seen only limited successes so far [7, 9], partly because the massive number of windows one needs to scan. [sent-102, score-0.15]
</p><p>9 In this work, we show that indeed a local representation can be effectively learned for object detection, and the learned rich features outperform HOG by a large margin as demonstrated on the PASCAL and INRIA benchmarks. [sent-104, score-0.231]
</p><p>10 We compute per-pixel sparse codes using dictionaries learned through K-SVD, and aggregate them into “histograms” of sparse codes (HSC) in the spirit of HOG. [sent-105, score-0.627]
</p><p>11 For a fair comparison, we keep to the HOG-driven scanning window framework as much as possible, with identical settings for mixtures, parts, and training procedure. [sent-106, score-0.164]
</p><p>12 To enable efficient training (especially for part-based models), we use a supervised training strategy: instead of iterating over latent root and part locations in the semi-convex setting of DPM [13], we assume these locations are given and fixed (computed  with a HOG-based detector). [sent-107, score-0.198]
</p><p>13 We also apply dimension reduction using learned models to effectively compress the high dimensional sparse code representations. [sent-108, score-0.322]
</p><p>14 We validate the benefits of richer representation through the use of increasingly large dictionary sizes and patch sizes. [sent-117, score-0.416]
</p><p>15 To the best of our knowledge, our work is the first to show that dictionary-based features can replace and significantly outperform HOG for general object detection. [sent-118, score-0.133]
</p><p>16 Related Works Object detection: Many contemporary approaches for object detection have converged on the paradigm of linear SVMs trained on HOG features [8, 13, 5, 21], as evidenced by benchmark evaluations such as PASCAL [12]. [sent-120, score-0.133]
</p><p>17 Most approaches have explored model structure, either through nonparametric mixtures or exemplars [21, 10], compositional grammar structure [15], supervised correspondences [5, 2], and low-dimensional projections [29, 25]. [sent-121, score-0.113]
</p><p>18 A sampling of such descriptors include local binary patterns [17], integral channel features of gradients and color cues [11], RGB covariance features [28], and multiscale spatial pyramids [4]. [sent-126, score-0.194]
</p><p>19 We extensively compare to this approach and find that we are able to learn much richer structures on larger patches through sparse coding and achieve substantial improvements over HOG. [sent-131, score-0.374]
</p><p>20 Sparse coding is a popular way of learning feature representation [1, 24], commonly used in image classification settings [33, 6] but also explored for detection [18]. [sent-133, score-0.204]
</p><p>21 More recent uses of sparse coding are toward the pixel level, learning patch representations to replace SIFT features [3]. [sent-134, score-0.309]
</p><p>22 Such patch representations can be applied to other problems such as contour detection [26]. [sent-135, score-0.138]
</p><p>23 Feature Learning for Object Detection Histograms of Oriented Gradients (HOG) are highly specialized features engineered for object detection, extremely popular and used in virtually every object detection system. [sent-137, score-0.239]
</p><p>24 How to build a richer local representation that outperforms HOG is a key challenge for detection, which remains open despite efforts of designing features [17], learning them [9], or combining multiple features [30]. [sent-140, score-0.22]
</p><p>25 We seek to replace HOG with features automatically learned from data. [sent-141, score-0.134]
</p><p>26 In this section we will develop Histograms-of-SparseCodes (HSC), which resembles HOG but is based on welldeveloped sparse coding techniques that represent each local patch using a sparse set of codewords. [sent-143, score-0.284]
</p><p>27 Once per-pixel sparse codes are computed, we aggregate the codes into “histograms” on regular cells and use them to replace HOG in the standard Deformable Parts Model [13]. [sent-145, score-0.482]
</p><p>28 Local Representation via Sparse Coding We use K-SVD [1] for dictionary learning, a standard unsupervised dictionary learning algorithm that generalizes 333222444755  Figure 2: Dictionaries learned through K-SVD for three patch sizes. [sent-148, score-0.624]
</p><p>29 As patch size and dictionary size grow, increasingly complex patterns are represented in the dictio-  nary. [sent-149, score-0.402]
</p><p>30 Given a set of image patches Y = [y1, · · · , yn], K-SVD jointly finds a dictionary D = [d1, · · · , d,m·]· a·n ,dy an associated sparse code matrix X = [x1, · ,· · , ·x ,nd] by minimizing the reconstruction error mD,iXn? [sent-152, score-0.396]
</p><p>31 Given the dictionary D, computing the codes X can be efficiently solved using the greedy Orthogonal Matching Pursuit (OMP) [24]. [sent-164, score-0.379]
</p><p>32 Given the codes X, the dictionary D is updated sequentially by singular value decomposition. [sent-165, score-0.379]
</p><p>33 Once the dictionary D is learned, we again use Orthogonal Matching Pursuit to compute sparse codes at every pixel in an image pyramid. [sent-167, score-0.459]
</p><p>34 Examples of the dictionaries learned are shown in Fig. [sent-169, score-0.124]
</p><p>35 As the patch size and dictionary size grow, more and more inter-  esting structures are discovered (such as corners, thin lines, line endings, and high-frequency gratings). [sent-172, score-0.376]
</p><p>36 Aggregation into Histograms of Sparse Codes The sliding window framework of object detection divides an image into regular cells (8x8 pixels) and computes a feature vector of each cell, to be used in a convolutionbased window scanning. [sent-175, score-0.235]
</p><p>37 Let X be the sparse code computed at a pixel, whose dimension equals the dictionary size. [sent-177, score-0.415]
</p><p>38 l tTeh eva rleuseul |tx is| a (s oneme oi-f)d tehnes feo ufera stupraeti vector F on each cell averaging codes in a 16x16 neighborhood, which we call Histograms of Sparse Codes (HSC). [sent-179, score-0.181]
</p><p>39 Finally, we apply a power transform on each element of F F¯ = Fα  (2)  as is sometimes done in recognition settings [26]. [sent-181, score-0.112]
</p><p>40 The power transform makes the distribution of F’s values more uniform and increases the discriminative power of F. [sent-182, score-0.138]
</p><p>41 That is, each codeword iin the dictionary now has three values in the HSC: [ |xi | , max(xi, 0) , max(−xi, 0) ]  (3)  It is worth noting that there are very few ad-hoc design choices in these HSC features. [sent-188, score-0.269]
</p><p>42 This illustrates the power oflearning richer features on larger patches, which captures more information than gradients and has less need for manually designed transforms. [sent-190, score-0.275]
</p><p>43 Moreover, it is straightforward to change the settings, such as dictionary size, patch size or sparsity level, allowing the HSC features to adapt to the needs of different problems. [sent-191, score-0.41]
</p><p>44 HSC features capture oriented edges using learned patterns, and can better localize them in each cell (the edges can be off-center). [sent-194, score-0.143]
</p><p>45 Moreover, HSC features can represent richer patterns such as corners (the girl’s feet) or parallel lines (both horizontal and vertical in the negative image). [sent-195, score-0.154]
</p><p>46 mentation of the standard sliding window detection framework, following the DPM model of [13]. [sent-200, score-0.127]
</p><p>47 The computational cost is linear in the feature dimension of φ(I). [sent-211, score-0.111]
</p><p>48 The learning procedure needs to iterate over training the model and assigning latent variables in the positive images, resulting in an elaborate and slow  process, sometimes fragile due to the non-convex nature of the formulation. [sent-216, score-0.118]
</p><p>49 For general object detection, it is difficult to obtain extensive human labels, and we instead use the state-of-theart HOG-based detection system [14], where the outputs of their final detectors are used as “groundtruth”. [sent-220, score-0.121]
</p><p>50 By fixing the latent variables in the part-based model, we make a fair and direct comparison of detection using HSC vs HOG features. [sent-221, score-0.17]
</p><p>51 With latent variables fixed, learning the detection model can be defined as a convex quadratic program aβr,gξnm≥i0n s. [sent-222, score-0.124]
</p><p>52 This allows us to train our supervised models much faster than the latent hard-negative mining approach of [14], making it feasible to work with high dimensional appearance features in part-based models. [sent-227, score-0.16]
</p><p>53 Dimension Reduction using Learned Models For root-only experiments on PASCAL, we use a dictionary of 100 codes over 5x5 patches, resulting in a 300dimensional feature vector, an order of magnitude higher than HOG. [sent-230, score-0.41]
</p><p>54 We find it convenient to reduce the dimension down when training full part-based models. [sent-231, score-0.116]
</p><p>55 However, unsupervised dimension reduction, such as principal component analysis (PCA) on the data, tends not to work well for either gradient features or sparse codes. [sent-232, score-0.23]
</p><p>56 One way of doing proper dimension reduction in the SVM setting would be to consider joint optimization such as in the bilinear model of [25], but it requires an expensive iterative algorithm. [sent-233, score-0.172]
</p><p>57 We find a simple way of doing supervised dimension reduction making use of models we have learned for the rootonly case. [sent-234, score-0.247]
</p><p>58 Let us write each learned filter wim as an N nf matrix Wim, where N = nxny (the number ofa spatial ×cenlls in a part filter) and nf is the size of our HSC feature F. [sent-235, score-0.25]
</p><p>59 For INRIA, we use root-only models and evaluate the HSC settings such as dictionary size, sparsity level, patch size, and power transform. [sent-242, score-0.431]
</p><p>60 For PASCAL2007, we use both root-only and partbased models with supervised training, measure the improvements of HSC over HOG for the 20 classes, and compare to the state-of-the-art DPM system [14] which uses the same model but with additional tweaks (such as symmetry). [sent-243, score-0.129]
</p><p>61 This dataset is an ideal setting for studying local features and comparing to HOG, as it is what HOG was designed and optimized for, and training is straightforward (there is no need for mixture or latent positions for positive examples). [sent-247, score-0.141]
</p><p>62 This is an intriguing question and illustrates the difference between reconstructing signals (what sparse coding techniques are designed for) and extracting meaningful structures for recognition. [sent-253, score-0.163]
</p><p>63 4(a) shows the average precision on INRIA when we change the sparsity level along with the dictionary size using 5x5 patches. [sent-255, score-0.331]
</p><p>64 We observe that when the dictionary size is small, a patch cannot be well represented with a single codeword, and K > 1 (at least 2) seems to help. [sent-256, score-0.319]
</p><p>65 However, when the dictionary size grows and includes more structures in its codes, the K = 1curve catches up, and performs very well. [sent-257, score-0.278]
</p><p>66 Therefore we use K = 1in all the following experiments, which makes the HSC features behave indeed like histograms using a sparse code dictionary. [sent-258, score-0.194]
</p><p>67 Next we investigate whether our HSC features can capture richer structures using larger patches. [sent-260, score-0.187]
</p><p>68 4(b) shows the average precision as we change both the patch size and the dictionary size. [sent-262, score-0.355]
</p><p>69 It is encouraging to see that indeed the average precision greatly increases as we use larger patches (along with larger dictionary size). [sent-263, score-0.368]
</p><p>70 While 3x3 codes barely show an edge over  nraepcsoivgre0 . [sent-264, score-0.158]
</p><p>71 f8orm1 (a)  (b)  (c)  (d)  Figure 4: Investigating the use of sparse codes on INRIA. [sent-272, score-0.238]
</p><p>72 (a) Average precision (AP) of sparsity level vs dictionary size; sparsity=1 works well when the dictionary is large. [sent-273, score-0.599]
</p><p>73 (b) Patch size vs dictionary size; larger patches do code richer information but requires larger dictionaries. [sent-274, score-0.551]
</p><p>74 (d) Power transform significantly improves the discriminative power of the sparse code histograms. [sent-276, score-0.195]
</p><p>75 HOG, 5x5 and 7x7 codes work much better, and the trend  continues beyond 200 codewords. [sent-277, score-0.158]
</p><p>76 The ability to code and make use of larger patches shows the merits of our feature design and K-SVD learning comparing to the spherical k-medoids clustering in [9], which had considerable trouble with larger patches and observed decreases in accuracy going beyond the small size 3x3. [sent-279, score-0.354]
</p><p>77 With K = 1, one can also use K-means to learn a dictionary (after normalizing the magnitude of each patch). [sent-281, score-0.221]
</p><p>78 4(c) compares the detection accuracy with K-SVD vs K-means dictionaries on 5x5 patches. [sent-283, score-0.206]
</p><p>79 K-SVD dictionaries have a clear advantage over K-means, probably because the reconstruction coefficient in sparse coding allows for a single codeword to model more appearances including the change of sign. [sent-284, score-0.248]
</p><p>80 We use dictionary size 100 for 3x3 patches, 150 for 5x5, and 300 for 7x7. [sent-295, score-0.246]
</p><p>81 HSC-based detectors outperform HOG, especially with larger patch sizes, and are competitive with the state-of-the-art DPM system (with parts). [sent-302, score-0.156]
</p><p>82 We use a K-SVD dictionary of size 100 over 5x5 patches. [sent-311, score-0.246]
</p><p>83 With the expansion to to half-wave rectified codes, the feature dimension is 300. [sent-312, score-0.136]
</p><p>84 Our system does not handle the symmetry of filters explicitly, instead we flip the positive images and double the size of the training pool. [sent-313, score-0.128]
</p><p>85 Table 2(a) shows the average precision evaluation of our root-only models comparing the HSC features with HOG. [sent-315, score-0.108]
</p><p>86 4, we learn a projection of HSC features to a lower dimension (universally applied to all cells) by utilizing models learned in the root-only case, and integrate it into feature extraction. [sent-328, score-0.208]
</p><p>87 4 765439801reduScVeDd15m0aiotdenlsi205 INRIA  PASCAL  Figure 5: Comparing the effectiveness of dimension reduction: SVD-data is the standard way of unsupervised dimension reduction computing SVD on data; SVD-model computes SVD on learned root filters. [sent-335, score-0.324]
</p><p>88 We use feature dimension 100 (reduced from 300) for our part-based models. [sent-338, score-0.111]
</p><p>89 To facilitate efficient training of multiple classes as in PASCAL, we precompute the feature pyramids and cache them. [sent-349, score-0.13]
</p><p>90 g234 (a) Part-based models, with dimension reduction  Table 2: Results on the PASCAL2007 dataset. [sent-422, score-0.13]
</p><p>91 HSC and HOG results are from our supervised training system using identical settings and directly comparable. [sent-423, score-0.199]
</p><p>92 Figure 6: A few examples of HOG (left) vs HSC (right) based detection (root-only), showing top three candidates (in the order of red, green, blue). [sent-425, score-0.137]
</p><p>93 Discussions In this work we demonstrated that dictionary based features, learned from data unsupervisedly, can replace and outperform the hand-crafted HOG features for general object detection. [sent-430, score-0.409]
</p><p>94 Our studies show that large structures in large patches, when captured in a large dictionary, generally improve object detection, calling for future work on designing and learning even richer features. [sent-434, score-0.194]
</p><p>95 The sparse representation we use in the current HSC features are simple relative to what exits in the feature learning literature. [sent-435, score-0.179]
</p><p>96 There are a variety of more sophisticated schemes for coding, pooling and codebook learning that could potentially boost detection performance, and we believe this is a crucial direction toward solving the challenging detection problem under real-world conditions. [sent-436, score-0.18]
</p><p>97 K-SVD: An algorithm for designing overcomplete dictionaries for sparse 333222555200  [2] [3]  [4]  [5]  [6]  [7]  representation. [sent-442, score-0.171]
</p><p>98 Text detection and character  [8] [9] [10]  [11]  [12]  [13]  [14]  [15]  [16]  [17]  recognition in scene images with unsupervised feature learning. [sent-484, score-0.124]
</p><p>99 How important are deformable parts in the deformable parts model? [sent-503, score-0.142]
</p><p>100 Linear spatial pyramid matching using sparse coding for image classification. [sent-665, score-0.131]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hsc', 0.765), ('hog', 0.262), ('dictionary', 0.221), ('codes', 0.158), ('dpm', 0.11), ('inria', 0.096), ('richer', 0.088), ('sparse', 0.08), ('dimension', 0.08), ('patch', 0.073), ('vs', 0.072), ('pascal', 0.072), ('svd', 0.07), ('dictionaries', 0.069), ('detection', 0.065), ('gradients', 0.063), ('supervised', 0.062), ('patches', 0.061), ('power', 0.057), ('learned', 0.055), ('wim', 0.053), ('coding', 0.051), ('reduction', 0.05), ('sparsity', 0.049), ('codeword', 0.048), ('engineered', 0.046), ('pursuit', 0.044), ('neural', 0.043), ('nf', 0.043), ('bilinear', 0.042), ('features', 0.042), ('parts', 0.04), ('classes', 0.04), ('advances', 0.04), ('pages', 0.04), ('identical', 0.04), ('wmi', 0.038), ('cim', 0.038), ('histograms', 0.038), ('replace', 0.037), ('improvements', 0.037), ('filters', 0.037), ('training', 0.036), ('precision', 0.036), ('binning', 0.036), ('trouble', 0.036), ('virtually', 0.034), ('xi', 0.034), ('increasingly', 0.034), ('code', 0.034), ('caching', 0.034), ('omp', 0.034), ('latent', 0.033), ('sliding', 0.033), ('ap', 0.032), ('dikmen', 0.032), ('structures', 0.032), ('root', 0.031), ('settings', 0.031), ('feature', 0.031), ('deformable', 0.031), ('system', 0.03), ('person', 0.03), ('trainval', 0.03), ('comparing', 0.03), ('window', 0.029), ('outperform', 0.028), ('unsupervised', 0.028), ('keep', 0.028), ('successes', 0.028), ('designs', 0.027), ('intentionally', 0.027), ('aggregate', 0.027), ('mixtures', 0.027), ('object', 0.026), ('learning', 0.026), ('orthogonal', 0.026), ('intel', 0.026), ('domains', 0.026), ('larger', 0.025), ('girshick', 0.025), ('size', 0.025), ('rectified', 0.025), ('margin', 0.025), ('ren', 0.025), ('transform', 0.024), ('grammar', 0.024), ('patterns', 0.024), ('crucial', 0.024), ('ramanan', 0.023), ('pi', 0.023), ('dimensional', 0.023), ('cell', 0.023), ('oriented', 0.023), ('elaborate', 0.023), ('board', 0.023), ('pyramids', 0.023), ('codewords', 0.023), ('designing', 0.022), ('cells', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="204-tfidf-1" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>Author: Xiaofeng Ren, Deva Ramanan</p><p>Abstract: Object detection has seen huge progress in recent years, much thanks to the heavily-engineered Histograms of Oriented Gradients (HOG) features. Can we go beyond gradients and do better than HOG? Weprovide an affirmative answer byproposing and investigating a sparse representation for object detection, Histograms of Sparse Codes (HSC). We compute sparse codes with dictionaries learned from data using K-SVD, and aggregate per-pixel sparse codes to form local histograms. We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. To keep training (and testing) efficient, we apply dimension reduction by computing SVD on learned models, and adopt supervised training where latent positions of roots and parts are given externally e.g. from a HOG-based detector. By learning and using local representations that are much more expressive than gradients, we demonstrate large improvements over the state of the art on the PASCAL benchmark for both root- only and part-based models.</p><p>2 0.20852858 <a title="204-tfidf-2" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>3 0.19560589 <a title="204-tfidf-3" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>4 0.1893951 <a title="204-tfidf-4" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>5 0.18855542 <a title="204-tfidf-5" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>6 0.18197119 <a title="204-tfidf-6" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>7 0.16246039 <a title="204-tfidf-7" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>8 0.15973334 <a title="204-tfidf-8" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>9 0.1586145 <a title="204-tfidf-9" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>10 0.13700514 <a title="204-tfidf-10" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>11 0.1353281 <a title="204-tfidf-11" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>12 0.13030916 <a title="204-tfidf-12" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>13 0.12955584 <a title="204-tfidf-13" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>14 0.12476316 <a title="204-tfidf-14" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>15 0.12376072 <a title="204-tfidf-15" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>16 0.12219539 <a title="204-tfidf-16" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>17 0.12178358 <a title="204-tfidf-17" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>18 0.11411607 <a title="204-tfidf-18" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>19 0.10657949 <a title="204-tfidf-19" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>20 0.10516246 <a title="204-tfidf-20" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, -0.148), (2, -0.143), (3, 0.137), (4, -0.02), (5, -0.035), (6, 0.104), (7, 0.101), (8, -0.039), (9, -0.015), (10, -0.112), (11, -0.008), (12, 0.071), (13, -0.076), (14, 0.08), (15, 0.024), (16, 0.005), (17, 0.013), (18, -0.001), (19, 0.022), (20, 0.007), (21, 0.036), (22, 0.031), (23, -0.038), (24, -0.011), (25, 0.045), (26, -0.058), (27, -0.009), (28, -0.008), (29, 0.013), (30, -0.008), (31, -0.034), (32, 0.028), (33, 0.006), (34, 0.033), (35, 0.044), (36, 0.047), (37, -0.0), (38, -0.034), (39, 0.024), (40, 0.022), (41, -0.024), (42, -0.019), (43, -0.045), (44, 0.018), (45, -0.052), (46, -0.016), (47, -0.025), (48, 0.017), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94217074 <a title="204-lsi-1" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>Author: Xiaofeng Ren, Deva Ramanan</p><p>Abstract: Object detection has seen huge progress in recent years, much thanks to the heavily-engineered Histograms of Oriented Gradients (HOG) features. Can we go beyond gradients and do better than HOG? Weprovide an affirmative answer byproposing and investigating a sparse representation for object detection, Histograms of Sparse Codes (HSC). We compute sparse codes with dictionaries learned from data using K-SVD, and aggregate per-pixel sparse codes to form local histograms. We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. To keep training (and testing) efficient, we apply dimension reduction by computing SVD on learned models, and adopt supervised training where latent positions of roots and parts are given externally e.g. from a HOG-based detector. By learning and using local representations that are much more expressive than gradients, we demonstrate large improvements over the state of the art on the PASCAL benchmark for both root- only and part-based models.</p><p>2 0.79128909 <a title="204-lsi-2" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>3 0.7831018 <a title="204-lsi-3" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>4 0.76206094 <a title="204-lsi-4" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>5 0.74796933 <a title="204-lsi-5" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>6 0.74745005 <a title="204-lsi-6" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>7 0.72553664 <a title="204-lsi-7" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>8 0.70469725 <a title="204-lsi-8" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>9 0.69008195 <a title="204-lsi-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.67080802 <a title="204-lsi-10" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>11 0.66661555 <a title="204-lsi-11" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>12 0.66320431 <a title="204-lsi-12" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>13 0.65494913 <a title="204-lsi-13" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>14 0.64794761 <a title="204-lsi-14" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>15 0.64136469 <a title="204-lsi-15" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>16 0.63916218 <a title="204-lsi-16" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>17 0.6322884 <a title="204-lsi-17" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>18 0.62493831 <a title="204-lsi-18" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>19 0.62323904 <a title="204-lsi-19" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>20 0.6207543 <a title="204-lsi-20" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.126), (16, 0.031), (26, 0.054), (28, 0.026), (33, 0.269), (67, 0.126), (69, 0.064), (80, 0.018), (87, 0.062), (94, 0.12)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93174225 <a title="204-lda-1" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>Author: Hu Ding, Branislav Stojkovic, Ronald Berezney, Jinhui Xu</p><p>Abstract: Computing accurate and robust organizational patterns of chromosome territories inside the cell nucleus is critical for understanding several fundamental genomic processes, such as co-regulation of gene activation, gene silencing, X chromosome inactivation, and abnormal chromosome rearrangement in cancer cells. The usage of advanced fluorescence labeling and image processing techniques has enabled researchers to investigate interactions of chromosome territories at large spatial resolution. The resulting high volume of generated data demands for high-throughput and automated image analysis methods. In this paper, we introduce a novel algorithmic tool for investigating association patterns of chromosome territories in a population of cells. Our method takes as input a set of graphs, one for each cell, containing information about spatial interaction of chromosome territories, and yields a single graph that contains essential information for the whole population and stands as its structural representative. We formulate this combinato- rial problem as a semi-definite programming and present novel techniques to efficiently solve it. We validate our approach on both artificial and real biological data; the experimental results suggest that our approach yields a nearoptimal solution, and can handle large-size datasets, which are significant improvements over existing techniques.</p><p>2 0.92372787 <a title="204-lda-2" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>same-paper 3 0.92159671 <a title="204-lda-3" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>Author: Xiaofeng Ren, Deva Ramanan</p><p>Abstract: Object detection has seen huge progress in recent years, much thanks to the heavily-engineered Histograms of Oriented Gradients (HOG) features. Can we go beyond gradients and do better than HOG? Weprovide an affirmative answer byproposing and investigating a sparse representation for object detection, Histograms of Sparse Codes (HSC). We compute sparse codes with dictionaries learned from data using K-SVD, and aggregate per-pixel sparse codes to form local histograms. We intentionally keep true to the sliding window framework (with mixtures and parts) and only change the underlying features. To keep training (and testing) efficient, we apply dimension reduction by computing SVD on learned models, and adopt supervised training where latent positions of roots and parts are given externally e.g. from a HOG-based detector. By learning and using local representations that are much more expressive than gradients, we demonstrate large improvements over the state of the art on the PASCAL benchmark for both root- only and part-based models.</p><p>4 0.92063642 <a title="204-lda-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.92024064 <a title="204-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.91953874 <a title="204-lda-6" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>7 0.91864938 <a title="204-lda-7" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>8 0.91835475 <a title="204-lda-8" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>9 0.91794235 <a title="204-lda-9" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>10 0.91631949 <a title="204-lda-10" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>11 0.91369647 <a title="204-lda-11" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>12 0.91330242 <a title="204-lda-12" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>13 0.91326708 <a title="204-lda-13" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>14 0.91322678 <a title="204-lda-14" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>15 0.91273415 <a title="204-lda-15" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>16 0.9126215 <a title="204-lda-16" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>17 0.91191274 <a title="204-lda-17" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>18 0.91157144 <a title="204-lda-18" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>19 0.91062874 <a title="204-lda-19" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>20 0.90991312 <a title="204-lda-20" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
