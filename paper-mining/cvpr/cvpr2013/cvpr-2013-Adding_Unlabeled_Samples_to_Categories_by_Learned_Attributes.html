<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-36" href="#">cvpr2013-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</h1>
<br/><p>Source: <a title="cvpr-2013-36-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Choi_Adding_Unlabeled_Samples_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>Reference: <a title="cvpr-2013-36-reference" href="../cvpr2013_reference/cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attribut', 0.546), ('unlabel', 0.522), ('exempl', 0.265), ('categ', 0.26), ('wca', 0.185), ('ssl', 0.156), ('jca', 0.106), ('precid', 0.102), ('borrow', 0.095), ('bar', 0.09), ('rank', 0.082), ('jcv', 0.079), ('wac', 0.079), ('discovery', 0.078), ('label', 0.075), ('rastegar', 0.075), ('sampl', 0.067), ('pool', 0.067), ('orang', 0.065), ('train', 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="36-tfidf-1" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>2 0.55064523 <a title="36-tfidf-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.41997644 <a title="36-tfidf-3" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>Author: Arijit Biswas, Devi Parikh</p><p>Abstract: Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. “I think this is a forest, what do you think?”. If the learner is wrong, the supervisorprovides an explanation e.g. “No, this is too open to be a forest”. With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work, we propose three improvements over this set-up. First, we incorporate a weighting scheme that instead of making a hard decision reasons about the likelihood of an image being a negative example. Second, we do away with pre-trained attributes and instead learn the attribute models on the fly, alleviating overhead and restrictions of a pre-determined attribute vocabulary. Finally, we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. We demonstrate significant improvement in classification accuracy on faces and shoes. We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories.</p><p>4 0.35939619 <a title="36-tfidf-4" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>5 0.34176514 <a title="36-tfidf-5" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>6 0.32830623 <a title="36-tfidf-6" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>7 0.30116352 <a title="36-tfidf-7" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>8 0.29686281 <a title="36-tfidf-8" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>9 0.29229477 <a title="36-tfidf-9" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>10 0.28299096 <a title="36-tfidf-10" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>11 0.26011094 <a title="36-tfidf-11" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>12 0.24693263 <a title="36-tfidf-12" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>13 0.23699002 <a title="36-tfidf-13" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>14 0.20961083 <a title="36-tfidf-14" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>15 0.20289692 <a title="36-tfidf-15" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>16 0.20255473 <a title="36-tfidf-16" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>17 0.18322508 <a title="36-tfidf-17" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>18 0.17045316 <a title="36-tfidf-18" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>19 0.16754353 <a title="36-tfidf-19" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>20 0.16630548 <a title="36-tfidf-20" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, 0.016), (2, 0.267), (3, 0.093), (4, 0.36), (5, 0.13), (6, -0.175), (7, -0.252), (8, -0.21), (9, 0.078), (10, 0.044), (11, 0.054), (12, -0.066), (13, 0.012), (14, -0.034), (15, 0.047), (16, -0.027), (17, -0.026), (18, 0.04), (19, -0.172), (20, 0.067), (21, -0.043), (22, -0.081), (23, 0.1), (24, -0.078), (25, 0.085), (26, 0.04), (27, 0.064), (28, 0.05), (29, -0.027), (30, 0.013), (31, 0.012), (32, -0.01), (33, 0.029), (34, -0.055), (35, 0.026), (36, 0.074), (37, 0.017), (38, 0.046), (39, -0.007), (40, 0.018), (41, 0.033), (42, -0.013), (43, 0.147), (44, -0.015), (45, -0.045), (46, 0.071), (47, -0.007), (48, -0.073), (49, -0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9590162 <a title="36-lsi-1" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>2 0.89770478 <a title="36-lsi-2" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>Author: Arijit Biswas, Devi Parikh</p><p>Abstract: Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. “I think this is a forest, what do you think?”. If the learner is wrong, the supervisorprovides an explanation e.g. “No, this is too open to be a forest”. With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work, we propose three improvements over this set-up. First, we incorporate a weighting scheme that instead of making a hard decision reasons about the likelihood of an image being a negative example. Second, we do away with pre-trained attributes and instead learn the attribute models on the fly, alleviating overhead and restrictions of a pre-determined attribute vocabulary. Finally, we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. We demonstrate significant improvement in classification accuracy on faces and shoes. We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories.</p><p>3 0.86303234 <a title="36-lsi-3" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>4 0.8548916 <a title="36-lsi-4" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>5 0.83533031 <a title="36-lsi-5" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>6 0.79479945 <a title="36-lsi-6" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>7 0.79106146 <a title="36-lsi-7" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>8 0.75536865 <a title="36-lsi-8" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>9 0.71101701 <a title="36-lsi-9" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>10 0.709674 <a title="36-lsi-10" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>11 0.70337445 <a title="36-lsi-11" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>12 0.61119241 <a title="36-lsi-12" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>13 0.6048016 <a title="36-lsi-13" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>14 0.58978635 <a title="36-lsi-14" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>15 0.58205175 <a title="36-lsi-15" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>16 0.52795655 <a title="36-lsi-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.51910579 <a title="36-lsi-17" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>18 0.5175786 <a title="36-lsi-18" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>19 0.509206 <a title="36-lsi-19" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>20 0.50686955 <a title="36-lsi-20" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.049), (4, 0.087), (5, 0.156), (37, 0.445), (69, 0.011), (81, 0.042), (86, 0.049), (95, 0.011), (97, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96018767 <a title="36-lda-1" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>2 0.94207603 <a title="36-lda-2" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>Author: Heesoo Myeong, Kyoung Mu Lee</p><p>Abstract: We propose a novel nonparametric approach for semantic segmentation using high-order semantic relations. Conventional context models mainly focus on learning pairwise relationships between objects. Pairwise relations, however, are not enough to represent high-level contextual knowledge within images. In this paper, we propose semantic relation transfer, a method to transfer high-order semantic relations of objects from annotated images to unlabeled images analogous to label transfer techniques where label information are transferred. Wefirst define semantic tensors representing high-order relations of objects. Semantic relation transfer problem is then formulated as semi-supervised learning using a quadratic objective function of the semantic tensors. By exploiting low-rank property of the semantic tensors and employing Kronecker sum similarity, an efficient approximation algorithm is developed. Based on the predicted high-order semantic relations, we reason semantic segmentation and evaluate the performance on several challenging datasets.</p><p>3 0.93654817 <a title="36-lda-3" href="./cvpr-2013-Nonlinearly_Constrained_MRFs%3A_Exploring_the_Intrinsic_Dimensions_of_Higher-Order_Cliques.html">308 cvpr-2013-Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques</a></p>
<p>Author: Yun Zeng, Chaohui Wang, Stefano Soatto, Shing-Tung Yau</p><p>Abstract: This paper introduces an efficient approach to integrating non-local statistics into the higher-order Markov Random Fields (MRFs) framework. Motivated by the observation that many non-local statistics (e.g., shape priors, color distributions) can usually be represented by a small number of parameters, we reformulate the higher-order MRF model by introducing additional latent variables to represent the intrinsic dimensions of the higher-order cliques. The resulting new model, called NC-MRF, not only provides the flexibility in representing the configurations of higher-order cliques, but also automatically decomposes the energy function into less coupled terms, allowing us to design an efficient algorithmic framework for maximum a posteriori (MAP) inference. Based on this novel modeling/inference framework, we achieve state-of-the-art solutions to the challenging problems of class-specific image segmentation and template-based 3D facial expression tracking, which demonstrate the potential of our approach.</p><p>4 0.92323607 <a title="36-lda-4" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>Author: Xin Guo, Dong Liu, Brendan Jou, Mojun Zhu, Anni Cai, Shih-Fu Chang</p><p>Abstract: Object co-detection aims at simultaneous detection of objects of the same category from a pool of related images by exploiting consistent visual patterns present in candidate objects in the images. The related image set may contain a mixture of annotated objects and candidate objects generated by automatic detectors. Co-detection differs from the conventional object detection paradigm in which detection over each test image is determined one-by-one independently without taking advantage of common patterns in the data pool. In this paper, we propose a novel, robust approach to dramatically enhance co-detection by extracting a shared low-rank representation of the object instances in multiple feature spaces. The idea is analogous to that of the well-known Robust PCA [28], but has not been explored in object co-detection so far. The representation is based on a linear reconstruction over the entire data set and the low-rank approach enables effective removal of noisy and outlier samples. The extracted low-rank representation can be used to detect the target objects by spectral clustering. Extensive experiments over diverse benchmark datasets demonstrate consistent and significant performance gains of the proposed method over the state-of-the-art object codetection method and the generic object detection methods without co-detection formulations.</p><p>5 0.92163044 <a title="36-lda-5" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>Author: Shaokang Chen, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell</p><p>Abstract: Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once, with fixed clusters used for matching with other image sets. However, this may result in the two closest clusters to represent different characteristics of an object, due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem, we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace, we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error, we approach the nearest point on a Grassmann manifold. Experiments on Honda, ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques, such as Affine Hull based Image Set Distance (AHISD), Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).</p><p>same-paper 6 0.91672081 <a title="36-lda-6" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>7 0.87969536 <a title="36-lda-7" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>8 0.84258306 <a title="36-lda-8" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>9 0.8229149 <a title="36-lda-9" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>10 0.81578064 <a title="36-lda-10" href="./cvpr-2013-Auxiliary_Cuts_for_General_Classes_of_Higher_Order_Functionals.html">51 cvpr-2013-Auxiliary Cuts for General Classes of Higher Order Functionals</a></p>
<p>11 0.80580872 <a title="36-lda-11" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>12 0.80489057 <a title="36-lda-12" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>13 0.80212778 <a title="36-lda-13" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>14 0.79671949 <a title="36-lda-14" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>15 0.79533249 <a title="36-lda-15" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>16 0.792319 <a title="36-lda-16" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>17 0.79094243 <a title="36-lda-17" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>18 0.78972572 <a title="36-lda-18" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>19 0.78585476 <a title="36-lda-19" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>20 0.78228199 <a title="36-lda-20" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
