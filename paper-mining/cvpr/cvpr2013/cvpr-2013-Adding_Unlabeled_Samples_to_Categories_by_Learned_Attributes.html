<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-36" href="#">cvpr2013-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</h1>
<br/><p>Source: <a title="cvpr-2013-36-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Choi_Adding_Unlabeled_Samples_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>Reference: <a title="cvpr-2013-36-reference" href="../cvpr2013_reference/cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. [sent-5, score-0.707]
</p><p>2 In addition, we propose a method to stably capture example-specific attributes for a small sized training set. [sent-6, score-0.453]
</p><p>3 Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet. [sent-7, score-1.055]
</p><p>4 Building a good training set with minimal supervision is a core problem in training visual category recognition algorithms [1]. [sent-10, score-0.449]
</p><p>5 So, given a relatively small initial set of labeled samples from a category, we want to mine a large pool of unlabeled samples to identify visually different examples without human intervention. [sent-13, score-1.143]
</p><p>6 edu @  Semi-supervised learning (SSL) aims at labeling unlabeled images based on their underlying distribution shared with a few labeled samples [5, 17,21]. [sent-22, score-0.78]
</p><p>7 In SSL, it is assumed that the unlabeled images that are distributed around the labeled samples are highly likely to be members ofthe labeled category. [sent-23, score-0.901]
</p><p>8 However, if we need to dramatically change the decision boundary of a category to achieve good classification performance, it is unlikely that this can be done just by adding samples that are similar in the space in which the  original classifier is constructed. [sent-24, score-0.501]
</p><p>9 To expand the boundary of a category to an unseen region, we propose a method that selects unlabeled samples based on their attributes. [sent-25, score-0.961]
</p><p>10 The selected unlabeled samples are not always instances from the same category, but they can still improve category recognition accuracy, similar to [7, 10]. [sent-26, score-0.933]
</p><p>11 The categorywide attributes find samples that share a large number of discriminative attributes with the preponderance of training data. [sent-28, score-1.039]
</p><p>12 The example-specific attributes find samples that are highly predictive of the hard examples from a category - the ones poorly predicted by a leave one out protocol. [sent-29, score-0.919]
</p><p>13 We demonstrate that our augmented training set can significantly improve the recognition accuracy over a very small initial labeled training set, where the unlabeled samples are selected from a very large unlabeled image pool, e. [sent-30, score-1.436]
</p><p>14 We show the effectiveness of using attributes learned with auxiliary data to label unlabeled images without annotated attributes. [sent-34, score-0.939]
</p><p>15 We propose a framework that jointly identifies the unlabeled images and category wide attributes through an optimization that seeks high classification accuracy in both the original feature space and the attribute space. [sent-36, score-1.49]
</p><p>16 We propose a method to learn example specific attributes with a small sized training set, used with the proposed framework. [sent-38, score-0.475]
</p><p>17 We then combine the category wide and the example specific attributes to further im-  prove the quality of image selection by diversifying the variations of selected images. [sent-39, score-0.759]
</p><p>18 Section 4 describes our optimization framework for discovering category wide attributes and the unlabeled images as well as a method to capture exemplar specific attributes. [sent-42, score-1.47]
</p><p>19 proposed a novel active learning framework based on interactive communication between learners and supervisors (teachers) via attributes [13]. [sent-51, score-0.481]
</p><p>20 Semi-Supervised Learning Semi-supervised learning (SSL) adds unlabeled examples to a training set by modeling the distribution of features without supervision. [sent-53, score-0.681]
</p><p>21 proposed a SSL based scene category recognition framework using attributes, constrained by a category ontology [17]. [sent-59, score-0.568]
</p><p>22 They leverage the inter-class relationships as constraints for SSL using semantic attributes given by a category ontology as a priori. [sent-60, score-0.692]
</p><p>23 Our approach is similar to their work in terms of using attributes, but aims to discover attributes without any structured semantic prior. [sent-61, score-0.425]
</p><p>24 They assume that the images in a category are not diverse and adding all images from some selected category will help to build a better model for the target category. [sent-65, score-0.673]
</p><p>25 [14] propose discovering implicit attributes that are not necessarily semantic for category recognition. [sent-73, score-0.658]
</p><p>26 The discovered attributes preserve category-specific traits as well as their visual similarity by an iterative algorithm that learns discriminative hyperplanes with maxmargin and locality sensitive hashing criteria. [sent-74, score-0.605]
</p><p>27 Approach Overview Given a handful of labeled training examples per category, it is difficult to build a generalizable visual model of a category even with sophisticated classifiers [20]. [sent-76, score-0.643]
</p><p>28 To address the lack of variations of the few labeled examples, we expand the visual boundary of a category by adding unlabeled samples based on their attributes. [sent-77, score-1.116]
</p><p>29 The attribute description allows us to find examples that are visually different but similar in traits or characteristics [4, 8, 9]. [sent-78, score-0.492]
</p><p>30 Based on recent work on automatic discovery of attributes [14] and large scale category-labeled image datasets [2], we discover a rich set of attributes. [sent-79, score-0.493]
</p><p>31 These attributes are leaned using an auxiliary category-labeled dataset to avoid biasing the attribute models towards the few labeled examples. [sent-80, score-0.841]
</p><p>32 The motivation here is similar to what under-  lies the successful Classemes representation [18] which achieved good category recognition performance by representing samples by external data that consists of a large number of samples from various categories. [sent-81, score-0.543]
</p><p>33 Across the original visual feature space and the attribute space, we propose a framework that jointly selects the unlabeled images to be assigned to each category and the discriminative attribute representations of the categories based on either a category wide or exemplar based ranking criteria. [sent-82, score-2.12]
</p><p>34 1 presents the optimization framework for category wide addition of unlabeled samples to categories. [sent-85, score-0.916]
</p><p>35 This adds samples that share many discriminative attributes amongst themselves and the given labeled training data. [sent-86, score-0.801]
</p><p>36 The same framework can be applied to identify relevant unlabeled samples based on their attribute similarity to specific instances of the training data. [sent-87, score-0.99]
</p><p>37 This only involves a simple change to one term of the optimization, and is based on how ranks of unlabeled samples change as labeled samples are left out, one at a time, from the attribute based classifier. [sent-88, score-1.212]
</p><p>38 We refer to the first as a categorical analysis and the second as an exemplar analysis. [sent-90, score-0.479]
</p><p>39 Categorical Analysis We simultaneously discover discriminative attributes and images from the unlabeled data set in a joint optimization framework formulated in both visual feature space and attribute space with a max margin criterion for discriminativity. [sent-94, score-1.302]
</p><p>40 Also unlike [10], we do not need to learn the distributions of the unlabeled images in the original feature space. [sent-96, score-0.514]
</p><p>41 d to a category based on identifying discriminative attribute models. [sent-123, score-0.597]
</p><p>42 Since the problems of determining the discriminative attributes and selecting the subset of unlabeled data to assign to a category are coupled, we learn them jointly. [sent-124, score-1.213]
</p><p>43 Additionally, we want to mitigate against unlabeled samples being assigned to multiple categories, so a term M(·) is added to the optimization criteria tgoo erinefso,r scoe ath teatr. [sent-125, score-0.636]
</p><p>44 ,l}  Xn  X k=Xl+ 1  Ic,k ≤ γ,  =  Ic,k  M(I) XX Ic1 · Ic2, Xc16=Xc2  (1) Ic ∈ {0, 1} is the sample selection vector for category c, and∈ ∈ind {i0c,a1t}es i ws hthiceh s aumnlpalbee sleedle scatimopnle vse catorer s feolrec ctaetde gfoorry ya cs-, signment to the training set of category c. [sent-139, score-0.66]
</p><p>45 {TRz  }(2) TR essentia|lly choo{szes the }to|p γ responses of the }attribute classifier from the unlabeled set by the fifth constraint of Eq. [sent-155, score-0.507]
</p><p>46 At the first iteration, the initial value of I determined by training the attribute classifier wca on is the given labeled training set. [sent-165, score-0.798]
</p><p>47 For our purposes, though, we can accomplish the same thing by analyzing how the ranks of unlabeled samples change when a single sample is eliminated from the training set of the attribute SVM. [sent-179, score-1.068]
</p><p>48 If an unlabeled sample sees its rank drop sharply from its rank in the full-sample SVM, then the training sample dropped should have strong attribute similarity to the unlabeled sample. [sent-180, score-1.448]
</p><p>49 The leftmost column shows unlabeled samples sorted by their rank in the attribute classifier learned from that set. [sent-183, score-1.036]
</p><p>50 Then we construct leave one out attribute classifiers, and each column shows the new rankings of unlabeled samples when each image at the top of the column is eliminated from the training set. [sent-184, score-1.064]
</p><p>51 Eliminating the half orange (second sample, top row) from the training set  µ  reduces the rank of the globally best unlabeled sample from 1to 10. [sent-185, score-0.662]
</p><p>52 First, let wca be the attribute classifier for the current training set for category c (while the process is initialized based on the labeled training set, after each iteration we use the additional unlabeled samples added to the category to construct a new attribute classifier). [sent-186, score-2.213]
</p><p>53 Let wca,j¯ be the attribute classifier learned when the ith sample is removed from the training set. [sent-187, score-0.459]
</p><p>54 We next describe how we use the ranks of unlabeled samples in these two classifiers to modify TR in Eq. [sent-188, score-0.682]
</p><p>55 Basically, we are going to re-rank the unlabeled samples based on their rank changes from wca to wac,j¯. [sent-190, score-0.81]
</p><p>56 This can be accomplished by computing the following score based on rank changes, and sorting the unlabeled samples by this score:  ej(xi) = rgµ(xi) −rj(νxi),  (3)  where xi is a sample from the an unlabeled pool, rg (·) and rj (·) are the rank functions of wca and wca,j¯ respec(·ti)v aenlyd. [sent-192, score-1.399]
</p><p>57 The left most column is a list of unlabeled images ordered by confidence score by Rest of the columns are lists of unlabeled images ordered by each wac,i¯’s. [sent-214, score-1.061]
</p><p>58 The unlabeled image pool consists of images that are arbitrarily chosen from the entire 1,000 categories in the ILSVRC 2010 benchmark dataset, but includes at least 50 samples from each of the categories to be learned. [sent-219, score-0.879]
</p><p>59 For learning the attribute space and the mapper, it is expected that the attribute mapper should capture some attribute of the categories of interest. [sent-221, score-1.097]
</p><p>60 For this purpose, we use 50 labeled samples from 93 categories that are similar to the 11categories to learn the attribute space. [sent-222, score-0.667]
</p><p>61 Experiments The main goal of our method is to add unlabeled images to the initial training set in order to classify more test images correctly. [sent-224, score-0.63]
</p><p>62 For categorical attribute only, we mostly use γ = 50 except ones in Section 6. [sent-246, score-0.499]
</p><p>63 For combining exemplar and categorical attributes, we mostly use γ = 20 and γi = 3 except for Section 6. [sent-248, score-0.479]
</p><p>64 Qualitative Results Our method discovers examples that expand the visual coverage of a category by not only adding the examples from the same category but also examples from other categories. [sent-256, score-0.963]
</p><p>65 Figure 2 illustrates qualitative results on the category Dalmatian for both categorical and exemplar attributes analyses. [sent-257, score-1.137]
</p><p>66 The selected examples based on categorical attributes exhibit characteristics commonly found in the labeled examples such as dotted, four legged animal. [sent-258, score-1.037]
</p><p>67 The exemplar attributes, on the other hand, select examples that exhibit the characteristic of individual labeled training examples. [sent-259, score-0.589]
</p><p>68 Comparison with Other Selection Criteria Given our goal of selecting examples from a large unlabeled data with only a small number of labeled training samples, we do not compare with semi-supervised learning methods because they need more labeled data to model the distribution. [sent-262, score-0.932]
</p><p>69 ’ refers to our method of select examples using categorical attributes only. [sent-269, score-0.751]
</p><p>70 ‘E+C’ refers to addition using categorical and exemplar attributes. [sent-270, score-0.513]
</p><p>71 The size of the unlabeled dataset is roughly 3,000 from randomly chosen categories out of 1,000 categories. [sent-271, score-0.554]
</p><p>72 We compare to baseline algorithms which are applicable to the large unlabeled data scenario. [sent-272, score-0.491]
</p><p>73 However, our method identifies useful images in the unlabeled image pool and significantly improves mAP by 7. [sent-278, score-0.571]
</p><p>74 The added examples serve not only as positive samples for each category but also as negative samples for other categories. [sent-283, score-0.667]
</p><p>75 In addition, the exemplar attributes further improve the recognition accuracy. [sent-291, score-0.669]
</p><p>76 Note that the selected examples by categorical attributes display characteristics commonly found in the labeled training examples such as ‘dotted’, ‘four legged animal’ . [sent-297, score-1.099]
</p><p>77 In contrast, the exemplar attributes select the examples that  display the characteristic of individual example. [sent-298, score-0.794]
</p><p>78 Mean average precision (mAP) of 11 category by our method varying the number of unlabeled images selected. [sent-300, score-0.844]
</p><p>79 Set), the augmented set by our method using category wide attributes only (+ by C only) and categorical+exemplar attributes respectively. [sent-302, score-1.091]
</p><p>80 Red bars denote the purity  of selected images using category wide attributes only (+ by C only) and the green bars are obtained from categorical+exemplar attributes (+ by E+C). [sent-313, score-1.48]
</p><p>81 (The results using both exemplar and categorical attributes are similar so are omitted). [sent-317, score-0.87]
</p><p>82 Precision of Unlabeled Data The unlabeled data can be composed of images from many categories. [sent-320, score-0.492]
</p><p>83 The precision of the unlabeled data is defined as the ratio of size of the unlabeled images from extraneous categories to the size of the entire unlabeled image data. [sent-321, score-1.636]
</p><p>84 The larger the unlabeled data, the lower we expect its precision to be (imagine running a text based image search 888888777800888  Figure 5. [sent-322, score-0.554]
</p><p>85 Even the similar examples alone improve the category recognition accuracy compared to just using the initial labeled set. [sent-328, score-0.539]
</p><p>86 It is interesting to observe how robust our method is against the precision of unlabeled data. [sent-330, score-0.554]
</p><p>87 We start with an unlabeled set (550 images, 50 from each of the 11 categories) of precision 1. [sent-331, score-0.554]
</p><p>88 As shown in Figure 6, we observe that the accuracy improvement by our method using categorical attributes is quite stable even when precision is low. [sent-334, score-0.677]
</p><p>89 Mean average precision (mAP) as a function of precision of unlabeled data. [sent-354, score-0.639]
</p><p>90 Precision denotes the ratio of size of the unlabeled images from extraneous categories to the size of the entire unlabeled image data (size = 50,000). [sent-355, score-1.082]
</p><p>91 Comparison to Exemplar SVM We also compare the effectiveness of our proposed exemplar attributes discovery method (Sec. [sent-362, score-0.737]
</p><p>92 To stabilize the exemplar SVM scores, we employ 50,000 ex-  ternal negative samples to learn each exemplar SVM while we use the small original training set for our method. [sent-367, score-0.778]
</p><p>93 Our method outperforms the exemplar SVM in terms of category recognition accuracy by APs without the extra large negative example set (size = 50,000). [sent-372, score-0.545]
</p><p>94 ure 8 shows that our exemplar attribute discovery method outperforms the exemplar SVM by large margins even without the large negative example set. [sent-373, score-0.922]
</p><p>95 Conclusion  We proposed a method to select unlabeled images to learn classifiers based on learned attributes. [sent-375, score-0.602]
</p><p>96 The unlabeled images selected by our method do not necessarily belong to the category of interest but are similar in attributes. [sent-376, score-0.818]
</p><p>97 Our method does not require any annotated attribute set a priori but first builds an automatically learned attribute space. [sent-377, score-0.624]
</p><p>98 We formulate a joint optimization framework to select both images and the attributes for a category and solve it iteratively. [sent-378, score-0.711]
</p><p>99 In addition to the category wide attributes, we identify example specific attributes to diversify the selected images. [sent-379, score-0.782]
</p><p>100 From a large unlabeled data pool, the selected images improve category recognition accuracy significantly over accuracy obtained using the initial labeled training set. [sent-381, score-1.057]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('unlabeled', 0.469), ('attributes', 0.391), ('attribute', 0.298), ('exemplar', 0.278), ('category', 0.267), ('categorical', 0.201), ('wca', 0.161), ('samples', 0.138), ('ssl', 0.136), ('labeled', 0.124), ('purity', 0.107), ('bars', 0.1), ('examples', 0.095), ('jca', 0.092), ('mapper', 0.092), ('categories', 0.085), ('precision', 0.085), ('pool', 0.079), ('borrowing', 0.072), ('jcv', 0.069), ('wac', 0.069), ('discovery', 0.068), ('rastegari', 0.065), ('active', 0.064), ('training', 0.062), ('selected', 0.059), ('orange', 0.056), ('expand', 0.055), ('ilsvrc', 0.053), ('initial', 0.053), ('traits', 0.049), ('svm', 0.047), ('legged', 0.046), ('wcv', 0.046), ('xl', 0.046), ('ranks', 0.045), ('farhadi', 0.044), ('ic', 0.043), ('rank', 0.042), ('hyperplanes', 0.042), ('wide', 0.042), ('xn', 0.041), ('halved', 0.041), ('jx', 0.038), ('classifier', 0.038), ('intervention', 0.037), ('alc', 0.036), ('extraneous', 0.036), ('generalizable', 0.036), ('maxmargin', 0.036), ('discover', 0.034), ('transfer', 0.034), ('refers', 0.034), ('ontology', 0.034), ('imagenet', 0.034), ('adding', 0.034), ('sample', 0.033), ('parkash', 0.033), ('selects', 0.032), ('discriminative', 0.032), ('selecting', 0.032), ('vse', 0.031), ('borrow', 0.03), ('select', 0.03), ('ranked', 0.03), ('classifiers', 0.03), ('shrivastava', 0.03), ('adds', 0.029), ('visual', 0.029), ('supervision', 0.029), ('added', 0.029), ('aps', 0.028), ('leave', 0.028), ('learned', 0.028), ('map', 0.028), ('auxiliary', 0.028), ('ordered', 0.027), ('learning', 0.026), ('salakhutdinov', 0.026), ('characteristics', 0.026), ('margin', 0.026), ('discovers', 0.026), ('balancing', 0.026), ('hashing', 0.026), ('share', 0.025), ('dogs', 0.025), ('decision', 0.024), ('liblinear', 0.024), ('visually', 0.024), ('images', 0.023), ('xi', 0.023), ('identify', 0.023), ('column', 0.023), ('members', 0.023), ('eliminated', 0.023), ('tr', 0.022), ('baseline', 0.022), ('learn', 0.022), ('sharing', 0.022), ('rj', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="36-tfidf-1" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>2 0.4639644 <a title="36-tfidf-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.28560367 <a title="36-tfidf-3" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>4 0.28326738 <a title="36-tfidf-4" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>5 0.26881558 <a title="36-tfidf-5" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>6 0.26708922 <a title="36-tfidf-6" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>7 0.24896778 <a title="36-tfidf-7" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>8 0.23938777 <a title="36-tfidf-8" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>9 0.2316308 <a title="36-tfidf-9" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>10 0.22966821 <a title="36-tfidf-10" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>11 0.2232164 <a title="36-tfidf-11" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>12 0.21251012 <a title="36-tfidf-12" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>13 0.19476843 <a title="36-tfidf-13" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>14 0.18784209 <a title="36-tfidf-14" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>15 0.1824313 <a title="36-tfidf-15" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>16 0.1746887 <a title="36-tfidf-16" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>17 0.16742828 <a title="36-tfidf-17" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>18 0.14931652 <a title="36-tfidf-18" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>19 0.14872888 <a title="36-tfidf-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.14792281 <a title="36-tfidf-20" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, -0.202), (2, -0.076), (3, -0.04), (4, 0.181), (5, 0.152), (6, -0.358), (7, 0.068), (8, 0.127), (9, 0.23), (10, -0.046), (11, 0.07), (12, -0.058), (13, -0.019), (14, -0.012), (15, -0.036), (16, -0.067), (17, -0.168), (18, -0.053), (19, 0.109), (20, -0.089), (21, -0.09), (22, -0.063), (23, -0.025), (24, 0.081), (25, 0.032), (26, 0.018), (27, 0.007), (28, -0.012), (29, 0.019), (30, -0.081), (31, 0.027), (32, -0.029), (33, 0.037), (34, 0.008), (35, 0.02), (36, -0.006), (37, -0.109), (38, 0.046), (39, -0.125), (40, -0.062), (41, -0.026), (42, 0.079), (43, 0.078), (44, 0.064), (45, -0.026), (46, -0.066), (47, -0.123), (48, -0.053), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96825564 <a title="36-lsi-1" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>2 0.85591209 <a title="36-lsi-2" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>Author: Arijit Biswas, Devi Parikh</p><p>Abstract: Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. “I think this is a forest, what do you think?”. If the learner is wrong, the supervisorprovides an explanation e.g. “No, this is too open to be a forest”. With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work, we propose three improvements over this set-up. First, we incorporate a weighting scheme that instead of making a hard decision reasons about the likelihood of an image being a negative example. Second, we do away with pre-trained attributes and instead learn the attribute models on the fly, alleviating overhead and restrictions of a pre-determined attribute vocabulary. Finally, we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. We demonstrate significant improvement in classification accuracy on faces and shoes. We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories.</p><p>3 0.84001291 <a title="36-lsi-3" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>4 0.82249701 <a title="36-lsi-4" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>5 0.78529769 <a title="36-lsi-5" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>6 0.76828212 <a title="36-lsi-6" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>7 0.7668975 <a title="36-lsi-7" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>8 0.76653457 <a title="36-lsi-8" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>9 0.69064987 <a title="36-lsi-9" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>10 0.63292623 <a title="36-lsi-10" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>11 0.61248785 <a title="36-lsi-11" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>12 0.57653064 <a title="36-lsi-12" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>13 0.5671261 <a title="36-lsi-13" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>14 0.52891594 <a title="36-lsi-14" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>15 0.52263999 <a title="36-lsi-15" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>16 0.51930171 <a title="36-lsi-16" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>17 0.50415081 <a title="36-lsi-17" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>18 0.4795773 <a title="36-lsi-18" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>19 0.47929922 <a title="36-lsi-19" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>20 0.47168151 <a title="36-lsi-20" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.155), (16, 0.02), (26, 0.042), (28, 0.029), (33, 0.28), (36, 0.01), (67, 0.069), (69, 0.072), (77, 0.021), (82, 0.128), (87, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93161559 <a title="36-lda-1" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>Author: George Leifman, Ayellet Tal</p><p>Abstract: Colorization refers to the process of adding color to black & white images or videos. This paper extends the term to handle surfaces in three dimensions. This is important for applications in which the colors of an object need to be restored and no relevant image exists for texturing it. We focus on surfaces with patterns and propose a novel algorithm for adding colors to these surfaces. The user needs only to scribble a few color strokes on one instance of each pattern, and the system proceeds to automatically colorize the whole surface. For this scheme to work, we address not only the problem of colorization, but also the problem of pattern detection on surfaces.</p><p>2 0.93004954 <a title="36-lda-2" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>Author: Yan Wang, Rongrong Ji, Shih-Fu Chang</p><p>Abstract: Recent years have witnessed a growing interest in understanding the semantics of point clouds in a wide variety of applications. However, point cloud labeling remains an open problem, due to the difficulty in acquiring sufficient 3D point labels towards training effective classifiers. In this paper, we overcome this challenge by utilizing the existing massive 2D semantic labeled datasets from decadelong community efforts, such as ImageNet and LabelMe, and a novel “cross-domain ” label propagation approach. Our proposed method consists of two major novel components, Exemplar SVM based label propagation, which effectively addresses the cross-domain issue, and a graphical model based contextual refinement incorporating 3D constraints. Most importantly, the entire process does not require any training data from the target scenes, also with good scalability towards large scale applications. We evaluate our approach on the well-known Cornell Point Cloud Dataset, achieving much greater efficiency and comparable accuracy even without any 3D training data. Our approach shows further major gains in accuracy when the training data from the target scenes is used, outperforming state-ofthe-art approaches with far better efficiency.</p><p>3 0.92721754 <a title="36-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.91900992 <a title="36-lda-4" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>5 0.91897261 <a title="36-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.91628766 <a title="36-lda-6" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>7 0.91624254 <a title="36-lda-7" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>8 0.91600335 <a title="36-lda-8" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>9 0.91466731 <a title="36-lda-9" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>same-paper 10 0.91436249 <a title="36-lda-10" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>11 0.91429496 <a title="36-lda-11" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>12 0.91408753 <a title="36-lda-12" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>13 0.9140361 <a title="36-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.91316307 <a title="36-lda-14" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>15 0.91258192 <a title="36-lda-15" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>16 0.91188198 <a title="36-lda-16" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>17 0.91133189 <a title="36-lda-17" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>18 0.91122037 <a title="36-lda-18" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>19 0.91118395 <a title="36-lda-19" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>20 0.91115999 <a title="36-lda-20" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
