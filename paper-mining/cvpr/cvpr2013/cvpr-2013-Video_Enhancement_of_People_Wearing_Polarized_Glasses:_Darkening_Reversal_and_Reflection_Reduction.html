<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-454" href="#">cvpr2013-454</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</h1>
<br/><p>Source: <a title="cvpr-2013-454-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ye_Video_Enhancement_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>Reference: <a title="cvpr-2013-454-reference" href="../cvpr2013_reference/cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. [sent-6, score-0.18]
</p><p>2 However, the special glasses participants wear to see 3D can create distracting images. [sent-7, score-0.275]
</p><p>3 This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. [sent-8, score-0.2]
</p><p>4 More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. [sent-9, score-0.875]
</p><p>5 A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). [sent-10, score-0.505]
</p><p>6 The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. [sent-11, score-0.607]
</p><p>7 Introduction Three-dimensional videoconferencing aims to capture,  transmit and display people and their environments in 3D, thus creating an illusion that the remote participants are in the same room with you. [sent-14, score-0.244]
</p><p>8 In 3D-TV, the user only needs to consume the broadcasted content on a 3D display, which is often based on polarization or shutter glasses. [sent-17, score-0.29]
</p><p>9 If the user still wears polarized glasses or shutter glasses, the eye region will be too dark for the remote parties to tell his/her gaze orientations (Fig. [sent-19, score-1.0]
</p><p>10 One solution to the above issue is to develop autostereoscopic 3D displays, where no glasses is needed to perceive 3D. [sent-21, score-0.29]
</p><p>11 Such displays may include those based on lenticular screens [18], parallax barrier [11], projector arrays [2], rotating mirrors [13], eye tracking [27], etc. [sent-22, score-0.222]
</p><p>12 Currently, the majority of the 3D displays on the market are polarization or shutter glasses based. [sent-24, score-0.481]
</p><p>13 Motivated by the emerging trend of using polarized glasses for 3D viewing, in this paper, we study algorithms to enhance videos of people wearing polarized glasses for 3D video-  conferencing. [sent-25, score-1.07]
</p><p>14 The algorithm could be extended to shutter glasses, which have very similar light transmission rate as polarized glasses when their shutters are in “transparency” state. [sent-26, score-0.738]
</p><p>15 First, when lights are shone on a pair of polarized glasses, only about 40% of the lights actually go through. [sent-28, score-0.287]
</p><p>16 This causes the eye region to be darkened, which is unpleasant in videoconferencing. [sent-29, score-0.212]
</p><p>17 Second, for almost every pair of polarized 3D glasses we could buy from the shelf, it is strongly reflective (Fig. [sent-30, score-0.535]
</p><p>18 Since the eye region has already been darkened, such reflection further deteriorates the video quality. [sent-32, score-0.7]
</p><p>19 Adding anti-glare coating to the glasses may partially resolve this issue, although it could easily cost much more than the glasses themselves. [sent-33, score-0.496]
</p><p>20 In this paper, we present a solution that computationally perform darkening reversal and reflection reduction simultaneously. [sent-34, score-0.622]
</p><p>21 A stereo camera that also “wears” a pair of polarized glasses is adopted (Fig. [sent-35, score-0.613]
</p><p>22 We then propose a novel Bayesian model to describe the imaging process of the eye region including darkening and reflection, and infer the eye region based on Classification Expectation-Maximization (EM) [4]. [sent-37, score-0.529]
</p><p>23 Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement in image quality inside the eye region. [sent-38, score-0.285]
</p><p>24 (c) A sample image captured using regular camera without polarization filter. [sent-55, score-0.206]
</p><p>25 (d) A pair of images acquired by our stereo cameras mounted with polarization filters. [sent-56, score-0.236]
</p><p>26 eral layer decomposition problems, where one of the layer is partially observed and noisy. [sent-60, score-0.282]
</p><p>27 One particular example is reflection layer extraction from composite images, for instance to build an environment map, where the underlying layer maybe be observed by applying a polarizer to partially filter out the reflection. [sent-61, score-0.867]
</p><p>28 The Bayesian model for the imaging process of the eye region is described in Sec. [sent-66, score-0.247]
</p><p>29 It detects eyeglasses using a boosting based detector, and then adopts a statistical analysis and synthesis approach to remove eyeglasses based on training data. [sent-75, score-0.246]
</p><p>30 Since reflection is difficult to model with a set of training examples, the algorithm does not perform well under strong reflections. [sent-76, score-0.488]
</p><p>31 Reflection layer separation has in fact attracted a lot of research recently. [sent-77, score-0.174]
</p><p>32 Physics-based approaches usually rely on linear polarization of the reflected images. [sent-81, score-0.168]
</p><p>33 [14] captured multiple polarized images from a single view point with different polarization angles. [sent-83, score-0.488]
</p><p>34 Under the assumption that variance of gradients of the input images is proportional to the  magnitude of gradient of the reflection layer, a constrained minimization problem was formulated and solved to separate the reflection layer. [sent-84, score-0.976]
</p><p>35 Their more recent work [15] captured only three polarized images with polarizer angles separated by 45 degrees. [sent-85, score-0.352]
</p><p>36 The special relationship between images can then be utilized to extract the reflection layer. [sent-86, score-0.488]
</p><p>37 Since most 3D glasses are circularly polarized, these existing methods cannot be applied to our scenario. [sent-87, score-0.248]
</p><p>38 However, for polarized 3D glasses, small head movement can cause large changes in reflection, making temporal information difficult to extract. [sent-92, score-0.287]
</p><p>39 The last group of approaches incorporates various kinds of prior information about the scene in order to extract the reflection layer. [sent-93, score-0.488]
</p><p>40 The statistics between the background and the reflection layers can also be assumed to be independent. [sent-100, score-0.539]
</p><p>41 Since temporal information is unreliable in our scenario, our goal is to reverse the darkening effect and reduce the reflection solely from the spatial redundancy. [sent-107, score-0.558]
</p><p>42 To this end, we mount a pair of polarized 3D glasses on the cameras, which allows us to capture a partial reflection image directly due to polarization. [sent-108, score-1.023]
</p><p>43 1(d), consider the camera with left-handed circular polarization filter on it. [sent-110, score-0.173]
</p><p>44 It will not see the user’s eye with opposite polarization. [sent-111, score-0.175]
</p><p>45 In contrast, the camera with right-handed circular polarization will see both the eye region and the reflection for the same eye. [sent-113, score-0.873]
</p><p>46 In the following sections, we build a Bayesian model for the imaging process of the above setup, and present an optimization framework to enhance the eye region for our application. [sent-114, score-0.247]
</p><p>47 Let us consider the user’s right eye region in Fig. [sent-125, score-0.212]
</p><p>48 The bottom image is from another camera, and it contains both the background eye region and the reflection, which we denote Ic. [sent-128, score-0.212]
</p><p>49 Our goal is to recover the right eye region without polarized glasses for the same camera view of Ic, which we denote as Ib. [sent-129, score-0.794]
</p><p>50 A few hidden variables are introduced as follows: • Icb: the transmission layer of the composite image Ic; •• IIcr: the reflection layer of the composite image Ic; •• ITr: the mapping between the reflection layer Icr and Tthe observed reflection in alternative view Ir. [sent-133, score-2.203]
</p><p>51 (2), P(Ic |Icb, Icr), involves the composition of two layers : the tran|sImission layer (background layer) Icb and the reflection layer Icr. [sent-145, score-0.821]
</p><p>52 However, due to the polarization filters on our ×ste 3re moa cameras, tehvee rtr,a dnusefo rtmoa thtieon p obleatriwzeaetino nIc fbi taenrds Ib, named Tb, is slightly different from Tg, as illustrated in Fig. [sent-154, score-0.169]
</p><p>53 With our setup, the rest of the scene is recorded through a single polarization filter, while the eye regions behind the polarized eye-glasses are recorded through two filters. [sent-156, score-0.634]
</p><p>54 For the eye regions that can be seen, thanks to the same polarization direction of the glasses, the relative light loss with respect to the rest of the scene is smaller compared with Tg. [sent-157, score-0.368]
</p><p>55 2, which associates the observed reflection Ir in the alternative view with the reflection layer Icr of the composite image Ic. [sent-171, score-1.203]
</p><p>56 Due to view differences, spatially there is a non-linear warping between these two reflection images. [sent-172, score-0.561]
</p><p>57 Moreover, since the glasses area is relatively small, part of the reflection in one view is not observable from the other, as shown in Fig. [sent-173, score-0.791]
</p><p>58 Therefore, we partition the reflection layer Icr into two parts: Icor represents the part of reflection observed from the alternative view, which may be estimated from the observed reflection Ir through spatial warping. [sent-175, score-1.605]
</p><p>59 Red and yellow circles mark examples of observable and non-observable parts of the reflection layer. [sent-196, score-0.522]
</p><p>60 (x, y) in the reflection image Ir; and ω abstracts the non-linear warping. [sent-201, score-0.488]
</p><p>61 When either the scene point or the glasses moves, the scaling factor s for a single point on the glasses also changes (due to changes in in Fig. [sent-210, score-0.496]
</p><p>62 Since the reflection model involves non-linear spatial mapping ω, it is error-prone to estimate the modes of all  Iˆb  Initialization: Iterate until convergence:  E-step:  {Iˆcb,Iˆcr,Tˆr}  argmaxlog(P(Icb,Icr,Tr|Ic,Ir,Iˆb)) = argmaxlog? [sent-223, score-0.516]
</p><p>63 The reasons for choosing this particular cost function and weighting scheme are: (1) Human beings sense reflection mainly due to high frequency region of the reflection layer, e. [sent-284, score-1.013]
</p><p>64 × ×  Therefore the pixels of the reflection layer with higher gradients are assigned with higher weight as encoded in u(x, y). [sent-287, score-0.629]
</p><p>65 With the estimated disparities, the reflection image Ir can be spatially warped to align with the reflection layer Icr, denoted as Irω. [sent-293, score-1.138]
</p><p>66 In the rest of the paper, we denote the filtered reflection as Iˆωr for simplicity. [sent-302, score-0.488]
</p><p>67 {sl } (l denotes image color channel)T can cb eal ees ftiacmtoatresd s by combining Eq. [sent-306, score-0.17]
</p><p>68 We focus on the gradient difference between the composite image Ic and the estimated reflection layer Iˆcr, since the estimated transmission layer Iˆcb may contain reflection residue. [sent-320, score-1.44]
</p><p>69 ilarity between the transmission layer Iˆcb and the warped reflection image Iˆωr. [sent-344, score-0.767]
</p><p>70 If the similarity is high, there could be reflection  Figure5. [sent-345, score-0.488]
</p><p>71 (b) and (f) are the reflection image Ir and composite image Ic. [sent-348, score-0.553]
</p><p>72 residues in Iˆcb, thus the gradient difference between the composite image and the estimated reflection shall be further reduced. [sent-362, score-0.553]
</p><p>73 Reflection Layer The reflection layer, including both the observable and  nonIˆc-orb=serva rbglem pa xrIt,cr is={ upIcNdra,Itceord}? [sent-393, score-0.522]
</p><p>74 (9), the transmission layer can be obtained as:  Iˆcb= argmIcabx? [sent-453, score-0.258]
</p><p>75 However, it is impractical to perform pixelwise quantitative evaluation with images of subject with and without the polarized eyeglasses. [sent-503, score-0.308]
</p><p>76 The eye regions are manually defined for the first frame, and are then warped for all the rest images according to a 2D Homography calculated from the tracked markers (Fig. [sent-522, score-0.221]
</p><p>77 6(a), is then defined for skin detection with the eye regions excluded. [sent-525, score-0.255]
</p><p>78 Note that in the images captured with our camera setup, one of the eye regions for each view has only reflection. [sent-529, score-0.28]
</p><p>79 They are therefore not suitable for stereoscopic videoconferencing directly. [sent-530, score-0.18]
</p><p>80 Static Scenes: Two groups of images are acquired with regular stereo cameras (without polarization filters) and with our setup respectively. [sent-537, score-0.288]
</p><p>81 7(a) shows the first group, in which the dark eye regions as well as the reflection substantially compromise the visual quality. [sent-539, score-0.725]
</p><p>82 Together with the noisy reflection layers, they are used as input to our algorithm. [sent-544, score-0.488]
</p><p>83 When capturing (a) and (c) with the polarization filters on and off the cameras respectively, the subjects are asked to remain as still as possible. [sent-545, score-0.23]
</p><p>84 Image simulation for user studies: (a) captured with our setup; (b) simulated dark eye regions; (c) captured by a camera. [sent-547, score-0.39]
</p><p>85 7(b), the eye regions are enhanced through Tg−1 from (a) to achieve darkness reversal. [sent-550, score-0.301]
</p><p>86 By contrast, our approach performs both reflection reduction and darkness reversal, and achieves sig-  nificant improvement as demonstrated in Fig. [sent-552, score-0.567]
</p><p>87 Note the large differences of global illumination as well as the range of reflection strengths across images. [sent-554, score-0.488]
</p><p>88 Dynamic Scenes: In dynamic data set, the subjects generally move freely with various head motion and eye blinking. [sent-555, score-0.223]
</p><p>89 8, the simulated image (b) achieves very similar brightness in eye regions as that captured by a regular stereo camera pair (image (c)). [sent-572, score-0.365]
</p><p>90 However, as a side effect,  the reflection is also weakened, which makes the user study more favorable to the traditional setup. [sent-573, score-0.566]
</p><p>91 They are instructed to imagine that they are videoconferencing with the subject in the video. [sent-579, score-0.18]
</p><p>92 It  can not be directly applied to polarized eyeglasses made with cheap plastic filters, because the deformation in the plastic film causes large difference in the reflection across views (see supplemental materials). [sent-613, score-0.938]
</p><p>93 A precise 3D modeling of the reflection surface may help here. [sent-615, score-0.488]
</p><p>94 Our current implementation estimates the reflection parameters frame by frame. [sent-616, score-0.488]
</p><p>95 In future work, we plan to model the temporal relationship of the transmission layers Icb or the eye region image Ib explicitly to resolve this issue. [sent-618, score-0.38]
</p><p>96 We are also interested in applying our approach to shutter glasses, however the engineering hurdle to sync cameras with shutter glasses must be overcomed first. [sent-619, score-0.415]
</p><p>97 Conclusion We proposed a probabilistic approach for reflection reduction of polarized eyeglasses for the purpose of 3D videoconferencing, with our adapted hardware design. [sent-621, score-0.898]
</p><p>98 Our algorithm performs darkness reversal and reflection reduction effectively as demonstrated by the experiments, and substantially improves the visual quality of the images that are then used for 3D videoconferencing. [sent-622, score-0.631]
</p><p>99 Achieving eye contact in a oneto-many 3D video teleconferencing system. [sent-720, score-0.175]
</p><p>100 3D TV: A scalable system for –  [8]  [9] [10] [11]  [12] [13]  [14] [15] [16] [17] [18]  [19] [20]  [21] [22]  [23]  [24]  [25]  [26]  [27]  [28]  real-time acquistion, transmission and autostereoscopic display of dynamic scenes. [sent-753, score-0.213]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reflection', 0.488), ('icr', 0.296), ('polarized', 0.287), ('glasses', 0.248), ('icb', 0.222), ('icor', 0.207), ('ir', 0.199), ('ib', 0.189), ('eye', 0.175), ('icnr', 0.159), ('videoconferencing', 0.159), ('polarization', 0.147), ('cb', 0.145), ('layer', 0.141), ('eyeglasses', 0.123), ('transmission', 0.117), ('ic', 0.108), ('tg', 0.082), ('darkness', 0.079), ('user', 0.078), ('tb', 0.072), ('darkening', 0.07), ('composite', 0.065), ('shutter', 0.065), ('cr', 0.064), ('reversal', 0.064), ('sarel', 0.064), ('skin', 0.055), ('stereo', 0.052), ('setup', 0.052), ('warping', 0.052), ('layers', 0.051), ('tr', 0.046), ('logp', 0.044), ('autostereoscopic', 0.042), ('reflections', 0.042), ('dark', 0.037), ('region', 0.037), ('cameras', 0.037), ('transparent', 0.035), ('imaging', 0.035), ('em', 0.035), ('observable', 0.034), ('simulated', 0.034), ('sl', 0.034), ('separation', 0.033), ('captured', 0.033), ('argmaxlog', 0.032), ('argmiabx', 0.032), ('argmibaxlog', 0.032), ('darkened', 0.032), ('edcrc', 0.032), ('escr', 0.032), ('gai', 0.032), ('iocr', 0.032), ('nonobservable', 0.032), ('polarizer', 0.032), ('glass', 0.032), ('studies', 0.032), ('separating', 0.031), ('display', 0.03), ('ncr', 0.028), ('flickering', 0.028), ('csr', 0.028), ('modes', 0.028), ('remote', 0.028), ('participants', 0.027), ('hidden', 0.027), ('camera', 0.026), ('kentucky', 0.026), ('uky', 0.026), ('lenticular', 0.026), ('reversed', 0.026), ('irani', 0.025), ('regions', 0.025), ('ees', 0.025), ('wears', 0.025), ('dynamic', 0.024), ('subjects', 0.024), ('sr', 0.024), ('mao', 0.023), ('cor', 0.023), ('ica', 0.023), ('filters', 0.022), ('enhanced', 0.022), ('kong', 0.022), ('subject', 0.021), ('levin', 0.021), ('reflected', 0.021), ('view', 0.021), ('hallucinated', 0.021), ('stereoscopic', 0.021), ('cha', 0.021), ('light', 0.021), ('displays', 0.021), ('warped', 0.021), ('variables', 0.021), ('gaze', 0.02), ('plastic', 0.02), ('brightness', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="454-tfidf-1" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>2 0.2427225 <a title="454-tfidf-2" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>Author: Hyeongwoo Kim, Hailin Jin, Sunil Hadap, Inso Kweon</p><p>Abstract: We present a novel method to separate specular reflection from a single image. Separating an image into diffuse and specular components is an ill-posed problem due to lack of observations. Existing methods rely on a specularfree image to detect and estimate specularity, which however may confuse diffuse pixels with the same hue but a different saturation value as specular pixels. Our method is based on a novel observation that for most natural images the dark channel can provide an approximate specular-free image. We also propose a maximum a posteriori formulation which robustly recovers the specular reflection and chromaticity despite of the hue-saturation ambiguity. We demonstrate the effectiveness of the proposed algorithm on real and synthetic examples. Experimental results show that our method significantly outperforms the state-of-theart methods in separating specular reflection.</p><p>3 0.12858583 <a title="454-tfidf-3" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>4 0.10857733 <a title="454-tfidf-4" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<p>Author: Miaomiao Liu, Richard Hartley, Mathieu Salzmann</p><p>Abstract: This paper tackles the problem of reconstructing the shape of a smooth mirror surface from a single image. In particular, we consider the case where the camera is observing the reflection of a static reference target in the unknown mirror. We first study the reconstruction problem given dense correspondences between 3D points on the reference target and image locations. In such conditions, our differential geometry analysis provides a theoretical proof that the shape of the mirror surface can be uniquely recovered if the pose of the reference target is known. We then relax our assumptions by considering the case where only sparse correspondences are available. In this scenario, we formulate reconstruction as an optimization problem, which can be solved using a nonlinear least-squares method. We demonstrate the effectiveness of our method on both synthetic and real images.</p><p>5 0.10743577 <a title="454-tfidf-5" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>6 0.10669147 <a title="454-tfidf-6" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>7 0.066617988 <a title="454-tfidf-7" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>8 0.065162934 <a title="454-tfidf-8" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>9 0.060550123 <a title="454-tfidf-9" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>10 0.060263868 <a title="454-tfidf-10" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>11 0.058657493 <a title="454-tfidf-11" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>12 0.058352526 <a title="454-tfidf-12" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>13 0.0580514 <a title="454-tfidf-13" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>14 0.056597423 <a title="454-tfidf-14" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>15 0.053853072 <a title="454-tfidf-15" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>16 0.052961629 <a title="454-tfidf-16" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>17 0.052760322 <a title="454-tfidf-17" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>18 0.049873583 <a title="454-tfidf-18" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>19 0.047338367 <a title="454-tfidf-19" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>20 0.046101723 <a title="454-tfidf-20" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, 0.062), (2, 0.011), (3, 0.02), (4, -0.006), (5, -0.03), (6, -0.026), (7, 0.012), (8, 0.041), (9, 0.007), (10, 0.001), (11, -0.012), (12, 0.038), (13, -0.027), (14, -0.009), (15, 0.071), (16, 0.019), (17, 0.032), (18, 0.068), (19, 0.03), (20, 0.015), (21, -0.009), (22, -0.002), (23, -0.072), (24, -0.028), (25, -0.005), (26, 0.042), (27, 0.033), (28, 0.039), (29, 0.004), (30, -0.01), (31, -0.029), (32, 0.048), (33, -0.061), (34, 0.029), (35, -0.062), (36, -0.016), (37, 0.111), (38, -0.031), (39, -0.048), (40, -0.055), (41, 0.065), (42, -0.092), (43, 0.074), (44, -0.054), (45, 0.163), (46, -0.024), (47, 0.051), (48, -0.181), (49, -0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92799443 <a title="454-lsi-1" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>2 0.87330574 <a title="454-lsi-2" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>Author: Hyeongwoo Kim, Hailin Jin, Sunil Hadap, Inso Kweon</p><p>Abstract: We present a novel method to separate specular reflection from a single image. Separating an image into diffuse and specular components is an ill-posed problem due to lack of observations. Existing methods rely on a specularfree image to detect and estimate specularity, which however may confuse diffuse pixels with the same hue but a different saturation value as specular pixels. Our method is based on a novel observation that for most natural images the dark channel can provide an approximate specular-free image. We also propose a maximum a posteriori formulation which robustly recovers the specular reflection and chromaticity despite of the hue-saturation ambiguity. We demonstrate the effectiveness of the proposed algorithm on real and synthetic examples. Experimental results show that our method significantly outperforms the state-of-theart methods in separating specular reflection.</p><p>3 0.72689438 <a title="454-lsi-3" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>Author: Ilya Reshetouski, Alkhazur Manakov, Ayush Bandhari, Ramesh Raskar, Hans-Peter Seidel, Ivo Ihrke</p><p>Abstract: We investigate the problem of identifying the position of a viewer inside a room of planar mirrors with unknown geometry in conjunction with the room’s shape parameters. We consider the observations to consist of angularly resolved depth measurements of a single scene point that is being observed via many multi-bounce interactions with the specular room geometry. Applications of this problem statement include areas such as calibration, acoustic echo cancelation and time-of-flight imaging. We theoretically analyze the problem and derive sufficient conditions for a combination of convex room geometry, observer, and scene point to be reconstructable. The resulting constructive algorithm is exponential in nature and, therefore, not directly applicable to practical scenarios. To counter the situation, we propose theoretically devised geometric constraints that enable an efficient pruning of the solution space and develop a heuristic randomized search algorithm that uses these constraints to obtain an effective solution. We demonstrate the effectiveness of our algorithm on extensive simulations as well as in a challenging real-world calibration scenario.</p><p>4 0.55580306 <a title="454-lsi-4" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>5 0.54450834 <a title="454-lsi-5" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<p>Author: Miaomiao Liu, Richard Hartley, Mathieu Salzmann</p><p>Abstract: This paper tackles the problem of reconstructing the shape of a smooth mirror surface from a single image. In particular, we consider the case where the camera is observing the reflection of a static reference target in the unknown mirror. We first study the reconstruction problem given dense correspondences between 3D points on the reference target and image locations. In such conditions, our differential geometry analysis provides a theoretical proof that the shape of the mirror surface can be uniquely recovered if the pose of the reference target is known. We then relax our assumptions by considering the case where only sparse correspondences are available. In this scenario, we formulate reconstruction as an optimization problem, which can be solved using a nonlinear least-squares method. We demonstrate the effectiveness of our method on both synthetic and real images.</p><p>6 0.52357525 <a title="454-lsi-6" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>7 0.47088563 <a title="454-lsi-7" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>8 0.46108356 <a title="454-lsi-8" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>9 0.43631801 <a title="454-lsi-9" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>10 0.42590091 <a title="454-lsi-10" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>11 0.4258095 <a title="454-lsi-11" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>12 0.41670352 <a title="454-lsi-12" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>13 0.41553286 <a title="454-lsi-13" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>14 0.40249577 <a title="454-lsi-14" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>15 0.40149885 <a title="454-lsi-15" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>16 0.40063018 <a title="454-lsi-16" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>17 0.39401001 <a title="454-lsi-17" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>18 0.39174652 <a title="454-lsi-18" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>19 0.38644141 <a title="454-lsi-19" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>20 0.38474202 <a title="454-lsi-20" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.101), (16, 0.098), (21, 0.275), (26, 0.042), (33, 0.184), (67, 0.057), (69, 0.039), (72, 0.013), (87, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77445871 <a title="454-lda-1" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>2 0.730519 <a title="454-lda-2" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>3 0.681467 <a title="454-lda-3" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>Author: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar</p><p>Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In thispaper wepresent an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.</p><p>4 0.67851818 <a title="454-lda-4" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>5 0.67086804 <a title="454-lda-5" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>6 0.6574775 <a title="454-lda-6" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>7 0.65582252 <a title="454-lda-7" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>8 0.65311414 <a title="454-lda-8" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>9 0.64296508 <a title="454-lda-9" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>10 0.63877481 <a title="454-lda-10" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>11 0.63588041 <a title="454-lda-11" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>12 0.63556737 <a title="454-lda-12" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>13 0.63416994 <a title="454-lda-13" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>14 0.63378799 <a title="454-lda-14" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>15 0.63222557 <a title="454-lda-15" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>16 0.63211465 <a title="454-lda-16" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>17 0.63137335 <a title="454-lda-17" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>18 0.63099319 <a title="454-lda-18" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>19 0.62956518 <a title="454-lda-19" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>20 0.62819415 <a title="454-lda-20" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
