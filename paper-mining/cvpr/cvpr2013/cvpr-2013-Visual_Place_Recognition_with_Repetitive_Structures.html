<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>456 cvpr-2013-Visual Place Recognition with Repetitive Structures</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-456" href="#">cvpr2013-456</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>456 cvpr-2013-Visual Place Recognition with Repetitive Structures</h1>
<br/><p>Source: <a title="cvpr-2013-456-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Torii_Visual_Place_Recognition_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>Reference: <a title="cvpr-2013-456-reference" href="../cvpr2013_reference/cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. [sent-6, score-0.584]
</p><p>2 In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. [sent-9, score-0.496]
</p><p>3 We describe a representation of repeated structures suitable for scalable retrieval. [sent-10, score-0.515]
</p><p>4 It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. [sent-11, score-0.522]
</p><p>5 Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and  more recently proposed burstiness weighting. [sent-12, score-0.22]
</p><p>6 Introduction Given a query image of a particular street or a building, we seek to find one or more images in the geotagged database depicting the same place. [sent-14, score-0.39]
</p><p>7 The ability to visually recognize a place depicted in an image has a range of potential applications including automatic registration of images taken by a mobile phone for augmented reality applications [1] and accurate visual localization for robotics [7]. [sent-15, score-0.365]
</p><p>8 Scalable place recognition methods [3, 7, 18, 3 1, 37] often build on the efficient bag-of-visual-words representation developed for object and image retrieval [6, 13, 15, 24, 26, 40]. [sent-16, score-0.31]
</p><p>9 The detection is robust against local deformation of the repeated element and makes only weak assumptions on the spatial structure of the repetition. [sent-26, score-0.313]
</p><p>10 We develop a representation of repeated structures for efficient place recognition based on a simple modification of weights in the bag-of-visual-word model. [sent-27, score-0.728]
</p><p>11 extracted from each image in the database and quantized into a pre-computed vocabulary of visual words. [sent-28, score-0.311]
</p><p>12 Each image is represented by a sparse (weighted) frequency vector of visual words, which can be stored in an efficient inverted file indexing structure. [sent-29, score-0.344]
</p><p>13 At query time, after the visual words are extracted from the query image, the retrieval proceeds in two steps. [sent-30, score-0.777]
</p><p>14 In this work we develop a scalable representation for large-scale matching of repeated structures. [sent-35, score-0.443]
</p><p>15 While repeated structures often occur in man-made environments examples include building facades, fences, or road markings they are usually treated as nuisance and downweighted at the indexing stage [13, 18, 36, 39]. [sent-36, score-0.719]
</p><p>16 In contrast, we develop a simple but efficient representation of repeated structures –  –  and demonstrate its benefits for place recognition in urban environments. [sent-37, score-0.732]
</p><p>17 In detail, we first robustly detect repeated structures in images by finding spatially localized groups of visual words with similar appearance. [sent-38, score-0.807]
</p><p>18 Next, we modify the weights of the detected repeated visual words in the bag-of-visual-word model, where multiple occurrences of repeated elements in the same image provide a natural soft-assignment of features to visual words. [sent-39, score-1.314]
</p><p>19 In addition the contribution of repetitive structures is controlled to prevent dominating the matching score. [sent-40, score-0.445]
</p><p>20 After describing related work on finding and matching repeated structures (Section 1), we review in detail (Section 2) the common tf-idf visual word weighting scheme and its extensions to soft-assignment [27] and repeated structure suppression [13]. [sent-42, score-1.243]
</p><p>21 In Section 3 we describe our method for detecting repeated visual words in images. [sent-43, score-0.65]
</p><p>22 In Section 4, we describe the proposed model for scalable matching of repeated structures, and demonstrate its benefits for place recognition in section 5. [sent-44, score-0.687]
</p><p>23 Detecting repeated patterns in images is a well-studied problem. [sent-46, score-0.382]
</p><p>24 Repetitions are often detected based on an assumption of a single pattern repeated on a 2D (deformed) lattice [10, 19, 25]. [sent-47, score-0.369]
</p><p>25 Special attention has been paid to detecting planar patterns [35, 38] and in particular building facades [3, 9, 45], for which highly specialized grammar models, learnt from labelled data, were developed [23, 41]. [sent-48, score-0.31]
</p><p>26 Detecting planar repeated patterns can be useful for single view facade rectification [3] or even single-view 3D reconstruction [46]. [sent-49, score-0.417]
</p><p>27 However, the local ambiguity of repeated patterns often presents a significant challenge for geometric image matching [33, 38] and image retrieval [13]. [sent-50, score-0.544]
</p><p>28 [38] detect repeated patterns on building facades and then use the rectified repetition elements together with the spatial layout of the repetition grid to estimate the camera pose of a query image, given a database of building facades. [sent-52, score-1.035]
</p><p>29 Results are reported on a dataset of 5 query images and 9 building facades. [sent-53, score-0.252]
</p><p>30 [8] detect the repeated patterns in each image and represent the pattern using a single shift-invariant descriptor of the repeated element together with a simple descriptor of the 2D spatial layout. [sent-55, score-0.795]
</p><p>31 Their matching method is not scalable as they have to exhaustively compare repeated patterns in all images. [sent-56, score-0.512]
</p><p>32 In scalable image retrieval, Jegou et al [13] observe that repeated structures violate the feature independence assumption in the bag-of-visual-word model and test several schemes for down-weighting the influence of repeated patterns. [sent-57, score-0.873]
</p><p>33 Review of visual word weighting strategies In this section we first review the basic tf-idf weighting scheme proposed in text retrieval [32] and also commonly used for the bag-of-visual-words retrieval and place recognition [3, 6, 12, 13, 18, 24, 26, 40]. [sent-59, score-0.964]
</p><p>34 [13], which explicitly downweights repeated visual words in an image. [sent-61, score-0.683]
</p><p>35 Suppose there is a vocabulary of V visual words, then each image is represented by a vector vd = (t1, . [sent-64, score-0.241]
</p><p>36 The weighting is a product of two terms: the visual word frequency, nid/nd, and the inverse document (image) frequency, log N/Ni. [sent-72, score-0.478]
</p><p>37 The word frequency weights words occurring more often in a particular image higher (compared to visual word present/absent), whilst the inverse document frequency downweights visual words that appear often in the database, and therefore do not help to discriminate between different images. [sent-73, score-1.415]
</p><p>38 2 between the query vector√  √vq  and all image vectors  (3)  vd  in the  √v? [sent-79, score-0.23]
</p><p>39 database vectors are pre-normalized to unit L2 norm, equation (3) simplifies to the standard scalar product, which can be implemented efficiently using inverted file indexing schemes. [sent-85, score-0.246]
</p><p>40 Visual words generated through descriptor clustering often suffer from quantization errors, where local feature descriptors that should be matched but lie close to the Voronoi boundary are incorrectly assigned to different visual words. [sent-87, score-0.448]
</p><p>41 observe by counting visual word occurrences in a large corpus of 1M images that visual words  occurring multiple times in an image (e. [sent-97, score-0.809]
</p><p>42 on repeated structures) violate the assumption that visual word occurrences in an image are independent. [sent-99, score-0.844]
</p><p>43 Further they observe that the bursted visual words can negatively affect retrieval results. [sent-100, score-0.474]
</p><p>44 The intuition is that the contribution of visual words with a high number of occurrences towards the scalar product in equation (3) is too high. [sent-101, score-0.428]
</p><p>45 In the voting interpretation of the bag-of-visual-words model [12], bursted visual words vote multiple times for the same image. [sent-102, score-0.37]
</p><p>46 To see this, consider an example where a particular visual word occurs twice in the query and five times in a database image. [sent-103, score-0.643]
</p><p>47 Ignoring the normalization of the visual word vectors for simplicity, multiplying the number of occurrences as in (3) would result in 10 votes, whereas in practice only up to two matches (correspondences) can exist. [sent-104, score-0.486]
</p><p>48 propose to downweight the contribution of visual words occurring multiple times in an image, which is referred to as intraimage burrstiness. [sent-106, score-0.323]
</p><p>49 They experiment with different weighting strategies and empirically observe that down-weighting repeated visual words by multiplying the term frequency in equation (3) by factor √1nid, where nid is the number of occurrences, performs best. [sent-107, score-0.81]
</p><p>50 Similar strategies to discount repeated structures when matching images were also used in [36, 39]. [sent-108, score-0.501]
</p><p>51 also consider a more precise description of local invariant regions quantized into visual words using an additional binary signature [12] more pre-  cisely localizing the descriptor in the visual word Voronoi cell. [sent-110, score-0.678]
</p><p>52 Detection of repetitive structures The goal is to segment local invariant features detected in an image into localized groups of repetitive patterns and a layer of non-repeated features. [sent-115, score-0.77]
</p><p>53 Examples include detecting repeated patterns of windows on different building facades, as well as fences, road markings or trees in an image (see figure 2). [sent-116, score-0.597]
</p><p>54 Each SIFT descriptor is further assigned to the top K = 50 nearest visual words  from a pre-computed visual vocabulary (see section 5 for details). [sent-120, score-0.582]
</p><p>55 The features share at least one common visual word in their individual top K visual word assignments. [sent-129, score-0.686]
</p><p>56 In the following, we will call the detected feature groups “repttiles” for “tiles (regions) of repetitive features”. [sent-133, score-0.318]
</p><p>57 Figures 1 and 2 show a variety of examples of detected patterns of repeated features. [sent-134, score-0.438]
</p><p>58 The different repetitive patterns detected in each image are shown in different colors. [sent-139, score-0.343]
</p><p>59 Note the variety of detected repetitive structures such as different building  facades, trees, indoor objects, window tiles or floor patterns. [sent-141, score-0.502]
</p><p>60 Representing  repetitive structures  for scal-  able retrieval In this section we describe our image representation for efficient indexing taking into account the repetitive patterns. [sent-143, score-0.73]
</p><p>61 First, we aim at representing the presence of a repetition, rather than measuring the actual number of matching repeated elements. [sent-145, score-0.371]
</p><p>62 We take advantage of this fact and design a descriptor quantization procedure that adaptively soft-assigns local features with more repetitions in the image to fewer nearest cluster centers. [sent-147, score-0.319]
</p><p>63 The intuition is that the multiple examples of a repeated feature provide a natural and accurate soft-assignment to multiple visual words. [sent-148, score-0.438]
</p><p>64 wiTd i f T0 ≤ ≤ w wiidd< T  (5)  is obtained by thresholding weights wid by a threshold T. [sent-157, score-0.23]
</p><p>65 Note that the weighting described in equation (5) is similar to burstiness weighting, which down-weights repeating visual words. [sent-158, score-0.474]
</p><p>66 Here, however, we represent highly weighted (repeating) visual words with a constant T as the goal is to represent the occurrence (presence/absence) of the visual word, rather than measuring the actual number of occurrences (matches). [sent-159, score-0.553]
</p><p>67 Weight wid of the i-th visual word in image d is obtained by aggregating weights from adaptively soft-assigned features across the image taking into account the repeated image patterns. [sent-160, score-0.921]
</p><p>68 In particular, each feature f from the set Fd  of all features detected in image d is assigned to a kf-tuple Vf of indices of the kf nearest (in the feature space) visual words. [sent-161, score-0.383]
</p><p>69 k=f11[Vf(k) = i]2k1−1  (6)  where the indicator function 1[Vf (k) = i] is equal to 1if visual word iis present at the k-th position in Vf . [sent-166, score-0.343]
</p><p>70 This means that weight wid is obtained as the sum of contributions from all assignments of visual word iover all features in Fd. [sent-167, score-0.538]
</p><p>71 (7)  where kmax is the maximum number of assignments (kmax = 3 in all our experiments), and mf is the number of features in the repttile of f. [sent-171, score-0.243]
</p><p>72 a tis image mfeaaltluerstes i belonging attoe relatively larger repttiles are soft-assigned to fewer visual words as image repetitions provide a natural soft-assignment of the particular 88888644  Figure 3. [sent-185, score-0.514]
</p><p>73 Each row shows the query image (a), the best matching database image (b) correctly matched by the proposed method, and the best matching image (incorrect) using the baseline burstiness method [13] (c). [sent-187, score-0.636]
</p><p>74 The detected groups of repetitive features (“repttiles”) are overlaid over the image and color-coded according to the number of visual word assignments kf (red kf = 2, green kf = 1). [sent-188, score-1.185]
</p><p>75 Note that the number of soft-assignments for each feature is adapted to the size of the repttile, where features in bigger repttiles are assigned to  a smaller number of nearest visual words. [sent-190, score-0.28]
</p><p>76 This natural soft-assignment  is more precise and less ambiguous  than the standard soft-assignment to multiple nearest visual words [27] as will be demonstrated in the next section. [sent-192, score-0.327]
</p><p>77 The geotagged image database is formed by 254, 064 perspective images generated from 10, 586 Google Street View panoramas  of the Pittsburgh  ×  area downloaded from the the Internet. [sent-198, score-0.228]
</p><p>78 As testing query images, we use 24, 000 perspective images generated from 1, 000 panoramas randomly selected from 8, 999 panoramas of the Google Pittsburgh Research Data Set1 . [sent-205, score-0.35]
</p><p>79 up as the query images were captured in a different session than the database images and depict the same places from different viewpoints, under very different illumination conditions and, in some cases, in a different season. [sent-208, score-0.3]
</p><p>80 We build a visual vocabulary of 100,000 visual words by approximate k-means clustering [22, 26]. [sent-212, score-0.49]
</p><p>81 The vocabulary is built from features detected in a subset of 10, 000 randomly selected database images. [sent-213, score-0.242]
</p><p>82 We compare results of the proposed adaptive (soft-)assignment approach (Adaptive weights) with several baselines: the standard tf-idf weighting (tf-idf) [26], burstiness weights (brst-idf) [13], standard soft-assignment weights [27] (SA) and Fisher vector matching (FV) [16]. [sent-216, score-0.58]
</p><p>83 Each row shows the query image (a), the best matching database image (b) correctly matched by the proposed method, and the best matching image (incorrect) using [3] (c). [sent-221, score-0.416]
</p><p>84 of correctly recognized  (a) Locations of query (yellow dots) and database (gray dots) images. [sent-225, score-0.336]
</p><p>85 The query is correctly localized if at least one of the top N retrieved database images is within m meters from the ground truth position of the query. [sent-232, score-0.415]
</p><p>86 In the Pittsburgh database, since 97 % of wid are less or equal to 1, T = 1effectively downweights unnecessary bursted visual words. [sent-241, score-0.446]
</p><p>87 Next, we evaluate separately the benefits of the two com-  ponents of the proposed method with respect to the baseline burstiness weights: (i) thresholding using eq. [sent-244, score-0.258]
</p><p>88 We have also evaluated the proposed method on the San Francisco visual place recognition benchmark [3]. [sent-277, score-0.331]
</p><p>89 We have built a vocabulary of 100,000 visual words from upright RootSIFT [2] features extracted from 10,000 images randomly sampled from the San Francisco 1M image database [3]. [sent-278, score-0.471]
</p><p>90 We have not used the histogram equalization suggested by [3] as it did not im-  prove results using our visual word setup. [sent-279, score-0.343]
</p><p>91 The pattern of results is similar to the Pittsburgh data with our adaptive softassignment method (Adaptive weights) performing best and significantly better than the method of [3] underlying the importance ofhandling repetitive structures for place recognition in urban environments. [sent-284, score-0.657]
</p><p>92 Example place recognition results demonstrating benefits of the proposed approach are shown in figure 4. [sent-285, score-0.244]
</p><p>93 Conclusion In this work we have demonstrated that repeated structures in images are not a nuisance but can form a distinguishing feature for many places. [sent-289, score-0.496]
</p><p>94 We treat repeated visual words as significant visual events, which can be de-  tected and matched. [sent-290, score-0.723]
</p><p>95 This is achieved by robustly detecting repeated patterns of visual words in images, and adjusting their weights in the bag-of-visual-word representation. [sent-291, score-0.798]
</p><p>96 Multiple occurrences of repeated elements are used to provide a natural soft-assignment of features to visual words. [sent-292, score-0.581]
</p><p>97 The contribution of repetitive structures is controlled to prevent dominating the matching score. [sent-293, score-0.445]
</p><p>98 We have shown that the proposed representation achieves consistent improvements in place recognition performance in an urban environment. [sent-294, score-0.251]
</p><p>99 Detecting, localizing and grouping repeated scene elements from an image. [sent-412, score-0.313]
</p><p>100 Detecting and matching repeated patterns for automatic geo-tagging in urban environments. [sent-533, score-0.485]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('repeated', 0.313), ('burstiness', 0.22), ('repetitive', 0.218), ('word', 0.218), ('place', 0.206), ('jegou', 0.198), ('query', 0.194), ('words', 0.16), ('kf', 0.16), ('wid', 0.151), ('occurrences', 0.143), ('kmax', 0.142), ('facades', 0.131), ('structures', 0.13), ('visual', 0.125), ('pittsburgh', 0.121), ('repetitions', 0.116), ('repttiles', 0.113), ('database', 0.106), ('retrieval', 0.104), ('vf', 0.101), ('philbin', 0.099), ('francisco', 0.094), ('sivic', 0.088), ('weighting', 0.086), ('bursted', 0.085), ('downweights', 0.085), ('fences', 0.085), ('perdoch', 0.084), ('chum', 0.083), ('gps', 0.081), ('vocabulary', 0.08), ('frequency', 0.079), ('weights', 0.079), ('panoramas', 0.078), ('quantization', 0.076), ('scalable', 0.072), ('rootsift', 0.07), ('patterns', 0.069), ('markings', 0.066), ('san', 0.065), ('douze', 0.061), ('indexing', 0.06), ('isard', 0.058), ('tokyo', 0.058), ('matching', 0.058), ('adaptive', 0.058), ('building', 0.058), ('doubek', 0.057), ('repttile', 0.057), ('detected', 0.056), ('nuisance', 0.053), ('repetition', 0.053), ('detecting', 0.052), ('descriptor', 0.05), ('document', 0.049), ('frahm', 0.048), ('nid', 0.047), ('inverted', 0.046), ('street', 0.046), ('urban', 0.045), ('violate', 0.045), ('meters', 0.045), ('groups', 0.044), ('holidays', 0.044), ('procedural', 0.044), ('geotagged', 0.044), ('assignments', 0.044), ('repeating', 0.043), ('nearest', 0.042), ('torii', 0.042), ('schaffalitzky', 0.042), ('prague', 0.04), ('mikulik', 0.04), ('tiles', 0.04), ('vertices', 0.04), ('schindler', 0.039), ('road', 0.039), ('voronoi', 0.039), ('dominating', 0.039), ('google', 0.039), ('occurring', 0.038), ('benefits', 0.038), ('descriptors', 0.037), ('connected', 0.037), ('vd', 0.036), ('queries', 0.036), ('recognized', 0.036), ('facade', 0.035), ('localized', 0.035), ('text', 0.035), ('retrieved', 0.035), ('adaptively', 0.035), ('fisher', 0.035), ('sattler', 0.035), ('inria', 0.034), ('leibe', 0.034), ('file', 0.034), ('mobile', 0.034), ('recall', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="456-tfidf-1" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>2 0.28614867 <a title="456-tfidf-2" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>Author: Petr Gronát, Guillaume Obozinski, Josef Sivic, Tomáš Pajdla</p><p>Abstract: The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as onlyfewpositive training examples are availablefor each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. 2Center for Machine Perception, Faculty of Electrical Engineering 3WILLOW project, Laboratoire d’Informatique de l’E´cole Normale Sup e´rieure, ENS/INRIA/CNRS UMR 8548. 4Universit Paris-Est, LIGM (UMR CNRS 8049), Center for Visual Computing, Ecole des Ponts - ParisTech, 77455 Marne-la-Valle, France</p><p>3 0.24731888 <a title="456-tfidf-3" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>Author: Danfeng Qin, Christian Wengert, Luc Van_Gool</p><p>Abstract: Many recent object retrieval systems rely on local features for describing an image. The similarity between a pair of images is measured by aggregating the similarity between their corresponding local features. In this paper we present a probabilistic framework for modeling the feature to feature similarity measure. We then derive a query adaptive distance which is appropriate for global similarity evaluation. Furthermore, we propose a function to score the individual contributions into an image to image similarity within the probabilistic framework. Experimental results show that our method improves the retrieval accuracy significantly and consistently. Moreover, our result compares favorably to the state-of-the-art.</p><p>4 0.20796406 <a title="456-tfidf-4" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>5 0.17673953 <a title="456-tfidf-5" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>Author: Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian</p><p>Abstract: The Inverse Document Frequency (IDF) is prevalently utilized in the Bag-of-Words based image search. The basic idea is to assign less weight to terms with high frequency, and vice versa. However, the estimation of visual word frequency is coarse and heuristic. Therefore, the effectiveness of the conventional IDF routine is marginal, and far from optimal. To tackle thisproblem, thispaper introduces a novel IDF expression by the use of Lp-norm pooling technique. . edu . cn qit i @ c s an . ut s a . edu ? ? ? ? ? ? ? ? Carefully designed, the proposed IDF takes into account the term frequency, document frequency, the complexity of images, as well as the codebook information. Optimizing the IDF function towards optimal balancing between TF and pIDF weights yields the so-called Lp-norm IDF (pIDF). WpIDe sFho wwe ithghatts sth yeie clodsnv tehnetio son-acla IlDleFd i Ls a special case of our generalized version, and two novel IDFs, i.e. the average IDF and the max IDF, can also be derived from our formula. Further, by counting for the term-frequency in each image, the proposed Lp-norm IDF helps to alleviate the viismuaalg we,o trhde b purrosptionseesds phenomenon. Our method is evaluated through extensive experiments on three benchmark datasets (Oxford 5K, Paris 6K and Flickr 1M). We report a performance improvement of as large as 27.1% over the baseline approach. Moreover, since the Lp-norm IDF is computed offline, no extra computation or memory cost is introduced to the system at all.</p><p>6 0.12270431 <a title="456-tfidf-6" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>7 0.12084485 <a title="456-tfidf-7" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>8 0.12072351 <a title="456-tfidf-8" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>9 0.11506071 <a title="456-tfidf-9" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>10 0.11292444 <a title="456-tfidf-10" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>11 0.11283195 <a title="456-tfidf-11" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>12 0.1092441 <a title="456-tfidf-12" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>13 0.10791075 <a title="456-tfidf-13" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>14 0.10664477 <a title="456-tfidf-14" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>15 0.099594176 <a title="456-tfidf-15" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>16 0.099240869 <a title="456-tfidf-16" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>17 0.094846681 <a title="456-tfidf-17" href="./cvpr-2013-All_About_VLAD.html">38 cvpr-2013-All About VLAD</a></p>
<p>18 0.088653959 <a title="456-tfidf-18" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>19 0.087050945 <a title="456-tfidf-19" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>20 0.086895727 <a title="456-tfidf-20" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.196), (1, -0.042), (2, -0.003), (3, 0.012), (4, 0.081), (5, 0.011), (6, -0.094), (7, -0.068), (8, -0.09), (9, -0.062), (10, -0.056), (11, 0.044), (12, 0.109), (13, 0.046), (14, 0.025), (15, -0.176), (16, 0.091), (17, 0.039), (18, 0.111), (19, -0.187), (20, 0.206), (21, -0.048), (22, 0.035), (23, 0.086), (24, -0.067), (25, 0.001), (26, 0.067), (27, 0.015), (28, -0.032), (29, 0.027), (30, 0.115), (31, 0.009), (32, -0.075), (33, 0.066), (34, -0.046), (35, -0.006), (36, 0.041), (37, 0.055), (38, -0.058), (39, -0.059), (40, -0.039), (41, -0.074), (42, -0.036), (43, -0.009), (44, -0.12), (45, 0.063), (46, 0.032), (47, -0.03), (48, 0.017), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95604455 <a title="456-lsi-1" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>2 0.84098238 <a title="456-lsi-2" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>Author: Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian</p><p>Abstract: The Inverse Document Frequency (IDF) is prevalently utilized in the Bag-of-Words based image search. The basic idea is to assign less weight to terms with high frequency, and vice versa. However, the estimation of visual word frequency is coarse and heuristic. Therefore, the effectiveness of the conventional IDF routine is marginal, and far from optimal. To tackle thisproblem, thispaper introduces a novel IDF expression by the use of Lp-norm pooling technique. . edu . cn qit i @ c s an . ut s a . edu ? ? ? ? ? ? ? ? Carefully designed, the proposed IDF takes into account the term frequency, document frequency, the complexity of images, as well as the codebook information. Optimizing the IDF function towards optimal balancing between TF and pIDF weights yields the so-called Lp-norm IDF (pIDF). WpIDe sFho wwe ithghatts sth yeie clodsnv tehnetio son-acla IlDleFd i Ls a special case of our generalized version, and two novel IDFs, i.e. the average IDF and the max IDF, can also be derived from our formula. Further, by counting for the term-frequency in each image, the proposed Lp-norm IDF helps to alleviate the viismuaalg we,o trhde b purrosptionseesds phenomenon. Our method is evaluated through extensive experiments on three benchmark datasets (Oxford 5K, Paris 6K and Flickr 1M). We report a performance improvement of as large as 27.1% over the baseline approach. Moreover, since the Lp-norm IDF is computed offline, no extra computation or memory cost is introduced to the system at all.</p><p>3 0.79462653 <a title="456-lsi-3" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>Author: Danfeng Qin, Christian Wengert, Luc Van_Gool</p><p>Abstract: Many recent object retrieval systems rely on local features for describing an image. The similarity between a pair of images is measured by aggregating the similarity between their corresponding local features. In this paper we present a probabilistic framework for modeling the feature to feature similarity measure. We then derive a query adaptive distance which is appropriate for global similarity evaluation. Furthermore, we propose a function to score the individual contributions into an image to image similarity within the probabilistic framework. Experimental results show that our method improves the retrieval accuracy significantly and consistently. Moreover, our result compares favorably to the state-of-the-art.</p><p>4 0.76649952 <a title="456-lsi-4" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>Author: Petr Gronát, Guillaume Obozinski, Josef Sivic, Tomáš Pajdla</p><p>Abstract: The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as onlyfewpositive training examples are availablefor each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. 2Center for Machine Perception, Faculty of Electrical Engineering 3WILLOW project, Laboratoire d’Informatique de l’E´cole Normale Sup e´rieure, ENS/INRIA/CNRS UMR 8548. 4Universit Paris-Est, LIGM (UMR CNRS 8049), Center for Visual Computing, Ecole des Ponts - ParisTech, 77455 Marne-la-Valle, France</p><p>5 0.75453657 <a title="456-lsi-5" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>6 0.63415676 <a title="456-lsi-6" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>7 0.63155115 <a title="456-lsi-7" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>8 0.62677789 <a title="456-lsi-8" href="./cvpr-2013-All_About_VLAD.html">38 cvpr-2013-All About VLAD</a></p>
<p>9 0.61840695 <a title="456-lsi-9" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>10 0.61695623 <a title="456-lsi-10" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>11 0.58403403 <a title="456-lsi-11" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>12 0.55997157 <a title="456-lsi-12" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>13 0.55189639 <a title="456-lsi-13" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>14 0.52832562 <a title="456-lsi-14" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>15 0.52590907 <a title="456-lsi-15" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>16 0.51454854 <a title="456-lsi-16" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>17 0.51062727 <a title="456-lsi-17" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>18 0.50802606 <a title="456-lsi-18" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>19 0.50707227 <a title="456-lsi-19" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>20 0.49408692 <a title="456-lsi-20" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.064), (16, 0.013), (26, 0.042), (28, 0.013), (33, 0.347), (59, 0.213), (67, 0.11), (69, 0.041), (87, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95381004 <a title="456-lda-1" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>Author: Dror Sholomon, Omid David, Nathan S. Netanyahu</p><p>Abstract: In thispaper wepropose thefirst effective automated, genetic algorithm (GA)-based jigsaw puzzle solver. We introduce a novel procedure of merging two ”parent” solutions to an improved ”child” solution by detecting, extracting, and combining correctly assembled puzzle segments. The solver proposed exhibits state-of-the-art performance solving previously attempted puzzles faster and far more accurately, and also puzzles of size never before attempted. Other contributions include the creation of a benchmark of large images, previously unavailable. We share the data sets and all of our results for future testing and comparative evaluation of jigsaw puzzle solvers.</p><p>2 0.92268306 <a title="456-lda-2" href="./cvpr-2013-MKPLS%3A_Manifold_Kernel_Partial_Least_Squares_for_Lipreading_and_Speaker_Identification.html">276 cvpr-2013-MKPLS: Manifold Kernel Partial Least Squares for Lipreading and Speaker Identification</a></p>
<p>Author: Amr Bakry, Ahmed Elgammal</p><p>Abstract: Visual speech recognition is a challenging problem, due to confusion between visual speech features. The speaker identification problem is usually coupled with speech recognition. Moreover, speaker identification is important to several applications, such as automatic access control, biometrics, authentication, and personal privacy issues. In this paper, we propose a novel approach for lipreading and speaker identification. Wepropose a new approachfor manifold parameterization in a low-dimensional latent space, where each manifold is represented as a point in that space. We initially parameterize each instance manifold using a nonlinear mapping from a unified manifold representation. We then factorize the parameter space using Kernel Partial Least Squares (KPLS) to achieve a low-dimension manifold latent space. We use two-way projections to achieve two manifold latent spaces, one for the speech content and one for the speaker. We apply our approach on two public databases: AVLetters and OuluVS. We show the results for three different settings of lipreading: speaker independent, speaker dependent, and speaker semi-dependent. Our approach outperforms for the speaker semi-dependent setting by at least 15% of the baseline, and competes in the other two settings.</p><p>3 0.91315937 <a title="456-lda-3" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<p>Author: Eduard Trulls, Iasonas Kokkinos, Alberto Sanfeliu, Francesc Moreno-Noguer</p><p>Abstract: In this work we exploit segmentation to construct appearance descriptors that can robustly deal with occlusion and background changes. For this, we downplay measurements coming from areas that are unlikely to belong to the same region as the descriptor’s center, as suggested by soft segmentation masks. Our treatment is applicable to any image point, i.e. dense, and its computational overhead is in the order of a few seconds. We integrate this idea with Dense SIFT, and also with Dense Scale and Rotation Invariant Descriptors (SID), delivering descriptors that are densely computable, invariant to scaling and rotation, and robust to background changes. We apply our approach to standard benchmarks on large displacement motion estimation using SIFT-flow and widebaseline stereo, systematically demonstrating that the introduction of segmentation yields clear improvements.</p><p>same-paper 4 0.91080809 <a title="456-lda-4" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>5 0.91019613 <a title="456-lda-5" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>Author: Wenbin Li, Darren Cosker, Matthew Brown, Rui Tang</p><p>Abstract: In this paper we present a novel non-rigid optical flow algorithm for dense image correspondence and non-rigid registration. The algorithm uses a unique Laplacian Mesh Energy term to encourage local smoothness whilst simultaneously preserving non-rigid deformation. Laplacian deformation approaches have become popular in graphics research as they enable mesh deformations to preserve local surface shape. In this work we propose a novel Laplacian Mesh Energy formula to ensure such sensible local deformations between image pairs. We express this wholly within the optical flow optimization, and show its application in a novel coarse-to-fine pyramidal approach. Our algorithm achieves the state-of-the-art performance in all trials on the Garg et al. dataset, and top tier performance on the Middlebury evaluation.</p><p>6 0.89183062 <a title="456-lda-6" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>7 0.88547218 <a title="456-lda-7" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>8 0.86906117 <a title="456-lda-8" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>9 0.86769861 <a title="456-lda-9" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>10 0.86755025 <a title="456-lda-10" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>11 0.86726445 <a title="456-lda-11" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>12 0.86597627 <a title="456-lda-12" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>13 0.8652938 <a title="456-lda-13" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>14 0.86437631 <a title="456-lda-14" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>15 0.86434931 <a title="456-lda-15" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>16 0.86408055 <a title="456-lda-16" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>17 0.8632412 <a title="456-lda-17" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>18 0.86312979 <a title="456-lda-18" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>19 0.86306179 <a title="456-lda-19" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>20 0.86287844 <a title="456-lda-20" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
