<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 cvpr-2013-Finding Group Interactions in Social Clutter</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-172" href="#">cvpr2013-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 cvpr-2013-Finding Group Interactions in Social Clutter</h1>
<br/><p>Source: <a title="cvpr-2013-172-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Finding_Group_Interactions_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>Reference: <a title="cvpr-2013-172-reference" href="../cvpr2013_reference/cvpr-2013-Finding_Group_Interactions_in_Social_Clutter_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. [sent-6, score-0.955]
</p><p>2 Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. [sent-7, score-1.505]
</p><p>3 The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. [sent-8, score-1.329]
</p><p>4 Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. [sent-10, score-0.333]
</p><p>5 Introduction Social interactions are common, but they rarely take place in isolation. [sent-13, score-0.265]
</p><p>6 Conversations and other group interactions occur on busy streets, in crowded cafes, in conference halls, and in other types of social gatherings. [sent-14, score-0.613]
</p><p>7 In these situations, before a computer vision system can recognize distinctive group interactions, it must first detect them by distinguishing between participants and by-standers and by localizing them in time. [sent-15, score-0.506]
</p><p>8 This paper addresses this spatiotemporal detection problem for cases in which the agents in a large gathering can be reasonably detected and tracked. [sent-16, score-0.38]
</p><p>9 We consider group interactions broadly as distinctive space-time structural co-occurrence of individual actions. [sent-17, score-0.457]
</p><p>10 Given an exemplar video of an N-person social interaction, we seek to find similar interactions in a long input video with M > N approximately-tracked people. [sent-21, score-0.9]
</p><p>11 For each temporal frame in the exemplar, the N best-matching participants are identified separately in each temporal unit of the input, and the matches are assigned scores. [sent-22, score-0.823]
</p><p>12 Their interaction is then localized in time using an efficient branch-and-bound search. [sent-24, score-0.263]
</p><p>13 In a collection of hockey games, we might want all instances of a “three-on-one”, and in nature we might be interested in localizing instances of distinctive group interactions among populations of animals, insects, or bacteria. [sent-27, score-0.52]
</p><p>14 Given an exemplar video of a distinctive group interaction  involving a small handful of N agents, we detect and localize instances of similar interactions within a long video of a larger gathering of M ≥ N agents. [sent-32, score-1.119]
</p><p>15 Matching an exemplar interaction amounts to searching through space and time for ensembles that are similar in some sense. [sent-34, score-0.661]
</p><p>16 To use our matching approach for recognition, we simply match an input video against a labeled gallery of exemplars and then extract a class label or ranked list of labels from the resulting scored matches. [sent-36, score-0.334]
</p><p>17 Second, we expect that the same type of interaction can occur over different temporal extents and at variable rates within its temporal extent, so we want an approach insensitive to these “within-class” variations. [sent-40, score-0.721]
</p><p>18 First, the social descriptor-ensemble at each exemplar time unit is compared separately to each time unit of the input video, and the best-matching N participants in each unit are identified along with their matching score (yellow and gray lines in Fig. [sent-43, score-1.059]
</p><p>19 Third and finally, the temporal extent of the interaction is determined through an efficient branch-and-bound search. [sent-46, score-0.46]
</p><p>20 For analyzing interacting groups, previous approaches have considered cases in which: 1) there are no bystanders [11, 10, 3, 19, 21]; the interaction of interest is a priori localized in time [17, 4]; or both of these simultaneously [12, 20, 15]. [sent-50, score-0.366]
</p><p>21 A notable exception is [1], which like us, addresses the problem of localizing interactions in long videos that contain bystanders, albeit with a less flexible representation (more on this in Sec. [sent-51, score-0.391]
</p><p>22 Matching and Localizing Interactions We consider a video as a sequence of T temporal units that occur at a frequency equal to or less than the frame-rate of the raw video data. [sent-57, score-0.445]
</p><p>23 The duration of these T units is typically between one and a few raw video frames, and it is determined by the application-appropriate choice for temporal resolution of atomic action descriptors (e. [sent-58, score-0.484]
</p><p>24 Due to agent entry and exit, occlusions, and other tracking errors, not all M tracks will persist over all T frames, and some of the M tracks may correspond to short-lived false detections. [sent-62, score-0.511]
</p><p>25 There are TM per-time-unit dI-dimensional descriptors {fm,t} where fm,t encodes the mth agent’s activity aptto trism {ef uni}t tw h∈e [e1, f T] ; and TM(M − 1) pairwise tdivPi-tydim aten timsioen auln descriptors {gm,m? [sent-65, score-0.355]
</p><p>26 ,t encod-edsi mate ntismioen atl th dees cmrioptitoonrs a {ngd/or appearance gof agent m relative to agent m? [sent-67, score-0.384]
</p><p>27 Each exemplar video is processed in the very same way as the input video, so that an exemplar of N ≤ M participants over Sp tti vmide euon,it sos tish represented art o efa Nch ≤tim Me s ∈ [1, S] by the ensemble Ds ? [sent-83, score-1.006]
</p><p>28 Given a collection of exemplars and an input video, our matching strategy is as follows. [sent-88, score-0.26]
</p><p>29 For each exemplar D, we smeaatrcchhi through gthye input Qoll fowors t. [sent-89, score-0.348]
</p><p>30 he F optimal emxeamtchp,l identifying cthhe sherto uofg Nh t participants oanr dth localizing mtheatirc hin, tidereancttiifoynin time. [sent-90, score-0.371]
</p><p>31 Matching between Temporal Units The first step in our framework is to separately compute the correspondence between the N exemplar agents at each time s ∈ [1, S] and the optimal subset of N ≤ M of input agents a∈t [e1a,cSh ]t aimnde t h ∈e [p1t,i mT]a . [sent-98, score-0.908]
</p><p>32 binary pmreatsreixnt tW th,i sw Nhe-troe -tMhe cnomrr-ethsp entry wnm yi st one only w bhienna rtyhe m nattrhi exemplar agent is matched to the mth input agent. [sent-100, score-0.694]
</p><p>33 t The quality of a correspondence is measured by the similarity between the individual and pairwise descriptors of the N selected input agents and those of the N exemplar agents. [sent-107, score-0.895]
</p><p>34 (1) to be the dissimilarity between two instantaneous ensembles under a particular matching matrix W. [sent-118, score-0.341]
</p><p>35 wnm ∈ {0, 1}, W1 = 1, WT1 w  ≤ 1,  (2) where c is a MN 1 vector of distances between inwdihveidrueal c descriptors, ×d I1 (fm,t, fnD,s), ainstda cHe si sb a MeenN n×MN matrix of distances betwee)n, pairwise descriptors dP (gm,m? [sent-134, score-0.413]
</p><p>36 We achieve this through voting, with the intuition being that the optimal matching W∗ will occur relatively frequently among the instantaneous matches {Wt,s}. [sent-159, score-0.272]
</p><p>37 The first is the dissimilarity between the descriptorensemble of the exemplar and that of the matched input agents D(Qt , Ds). [sent-165, score-0.713]
</p><p>38 The second is a measure of temporal consistency, w,iDth the intuition being that if the N-subset of agents is matched at temporal pair (t, s) is correct, the same N-subset of agents should be matched for other pairs (t? [sent-166, score-1.04]
</p><p>39 ) in small temporal neighborhoods of the exemplar and input video. [sent-168, score-0.546]
</p><p>40 ∈N(t,s)  where N(t,s) is a temporal neighborhood of (t,s) in whi(c4h) we erenf Norc(te, sth)e is consistency aenigdh bito rish depicted sin) Fig. [sent-184, score-0.233]
</p><p>41 As a result, the voting procedure is shown in Algorithm 1, where in the last two steps we find among those matching matrices which receive a substantial number of supports from instantaneous matchings the best matching W∗ with the lowest average dissimilarity to the exemplar. [sent-189, score-0.369]
</p><p>42 1, where a thick matching line indicates a strong similarity (low weight v), and the agents receiving the lowest average weight are selected as participants. [sent-191, score-0.385]
</p><p>43 For this purpose, after the participants are dteertaecr--  mined through the best matching W∗, we recompute for all (t, s) pairs the dissimilarities under this best matching Dˆ(Qt, Ds , W∗), between the interaction of the individuDals( Qsel,eDcted by W∗ at time t and the exemplar at time s. [sent-195, score-0.975]
</p><p>44 We then compute D∗ (t) = mins Dˆ(Qt, Ds , W∗), the minimal dissimilarity of the input inDter(aQcti,oDn by the selected participants at time t to the entire exemplar, and s∗ (t) = arg mins Dˆ(Qt, Ds , W∗), the time in the exemplar at which the input Dat( tQim,eD Dt exhibits this maximum similarity. [sent-196, score-0.781]
</p><p>45 As interactions occur at variable rates within their temporal extent, we use a temporal pyramid to efficiently measure alignment in a way that also respects these variations. [sent-199, score-0.795]
</p><p>46 1  Let (ts , te) be the true, unknown starting and ending times of the detected interaction in the input video, and suppose that the input descriptor-ensemble over this interval exactly matches that of the exemplar. [sent-209, score-0.49]
</p><p>47 To determine good estimates for the interval (ts , te) we define a cost that is a product of the temporal alignment and visual similarity summed over the candidate interval: ? [sent-210, score-0.305]
</p><p>48 This means that the summand in (6) considered as a function of t assumes a negative value in the desired interval ts ≤ t ≤ te and a positive value otherwise, as denoted as q(t)≤ an td ≤ depicted in the bottom of Fig. [sent-216, score-0.414]
</p><p>49 4, the process can also handle moderately broken tracks by setting the descriptor values of missing temporal units to be sufficiently large (or small) so as not to be matched with any exemplar agents. [sent-223, score-0.769]
</p><p>50 Each row is an annotated two-cell exemplar with markers representing instantaneous descriptor-ensembles at each time unit. [sent-233, score-0.459]
</p><p>51 For discrimination between interaction categories, distances between ensembles of the same class (red circles and red squares) should be small whenever they occur in the same cell number; and distances for different classes (red vs. [sent-234, score-0.625]
</p><p>52 For effective and efficient temporal localization, distances between ensembles at labeled times and unlabeled “background” times (black circles) should be large, and all distances should be offset by −1. [sent-236, score-0.451]
</p><p>53 localization by ensuring that distances between labeled ensembles and unlabeled “background” ensembles are large. [sent-237, score-0.362]
</p><p>54 The combination of 1) and 2) leads to more accurate spatial localizations of participants (i. [sent-238, score-0.323]
</p><p>55 2), and induces the “quality function” conditions required for efficient temporal localization by branch-andbound (Sec. [sent-242, score-0.246]
</p><p>56 For each application scenario, we use a training set of exemplar videos—possibly having varying numbers of agents N—that are annotated with start/end times, category labels, N-agent correspondences between exemplars of the same category. [sent-246, score-0.789]
</p><p>57 This figure depicts three different exemplar videos in which  a subset of time units have been labeled as being distinctive interactions of two different classes. [sent-250, score-0.726]
</p><p>58 In this example, each labeled exemplar is shown as being divided into two cells; these correspond to the lowest level of the temporal pyramid described in Sec. [sent-251, score-0.587]
</p><p>59 The first three constraints in the list enhance discrimination between categories, while the last three enhance the accuracy of temporal localization. [sent-254, score-0.303]
</p><p>60 3) and occur roughly in the same temporal location within the interaction instances (i. [sent-259, score-0.475]
</p><p>61 , in the same cell of the lowest level of the temporal pyramids), together with their “ground-truth” matchings. [sent-261, score-0.277]
</p><p>62 For the classroom dataset, pairwise descriptors for groups comprised of (a) three or more participants, and (b) two participants. [sent-263, score-0.409]
</p><p>63 The datasets are very different from one another, with distinct types of individual and pairwise descriptors that are appropriate for that environment. [sent-276, score-0.267]
</p><p>64 In all experiments we use four-level temporal pyramids for the interactions and we set the time unit to be half the duration of the cells in the lowest level. [sent-277, score-0.603]
</p><p>65 The classroom is “interactive” because at various times throughout the lecture students are invited to engage in ad-hoc group discussions about problems provided by the instructor (see, e. [sent-282, score-0.297]
</p><p>66 (a)(b)(c) ROC curves for identifying the participants of an two-person, three-person, and four-person interactions using the proposed approach and baselines. [sent-290, score-0.549]
</p><p>67 (d) Temporal localization accuracies using the proposed approach with and without metric learning, using individual and/or pairwise descriptors. [sent-291, score-0.255]
</p><p>68 seating rows, and detecting them is a challenge because the number of by-standers is much larger than the number of participants (M is between 10 and 20 while N is between  2 and 4), video quality is limited (low light, 15fps), and the visual cues for interaction are quite subtle. [sent-292, score-0.601]
</p><p>69 The ability to automatically detect such interactions is important for education researchers, however, since it can help in understanding how students self-organize into groups, and which geometric configurations of groups lead to improved educational outcomes [5]. [sent-293, score-0.4]
</p><p>70 In consultation with education experts, we manually identified the participants and start/end times of all two-person, three-person, and four-person interactions, obtaining 254 two-person, 112 three-person, and 16 four-person interactions in total. [sent-295, score-0.593]
</p><p>71 We defined interaction categories based on the geometric configurations of the participants: three categories for 2-person interactions (same row; different rows with left agent in front; different rows with right agent in front) and four categories for 3-person interactions. [sent-296, score-0.993]
</p><p>72 The annotated interactions range from a few seconds to tens-ofseconds in length. [sent-299, score-0.3]
</p><p>73 Also, for each split of the data we manually eliminate the false detections and tracks  two-person and three-person interactions (Individual and/or pairwise descriptors, with or without metric learning (ML)). [sent-301, score-0.566]
</p><p>74 We begin by looking at accuracy of detection, where we ignore the inferred interaction categories and simply measure the systems ability to detect when an interaction has occurred. [sent-320, score-0.488]
</p><p>75 Using all parts of the system yields the best results, and we note that performance improves as the number of participants N increases. [sent-324, score-0.284]
</p><p>76 The latter is due to the fact that interaction patterns are more salient when more pairwise information is available. [sent-325, score-0.331]
</p><p>77 Examples of social interaction detection and matching on the classroom interaction database. [sent-327, score-0.783]
</p><p>78 Each row is an example of detecting a salient interaction from an input. [sent-328, score-0.255]
</p><p>79 (a) the input; (b) detected social interaction; (c-1) to (c-3) top three associated database exemplars that support the detection. [sent-329, score-0.334]
</p><p>80 (Due to the small number of 4-person interactions in our dataset, we did not de-  fine categories for them. [sent-332, score-0.305]
</p><p>81 6 shows the average true positive rates versus false positives when further classifying detected interactions into the three or four categories. [sent-334, score-0.406]
</p><p>82 Finally, we investigate the temporal localization performance, for which we compute the ratio of the intersection to the union of the estimated interval and the annotated interval, and we show the averages in Fig. [sent-341, score-0.388]
</p><p>83 In the fourth row, a three-person interaction is correctly identified even though the third associated exemplar is from a different category (two looking right). [sent-346, score-0.612]
</p><p>84 In the other rows, twoperson and three-person interactions are correctly detected and matched with exemplars. [sent-347, score-0.348]
</p><p>85 We follow the protocol defined in previous work [21, 1]: 20% of available interaction annotations are used as exemplars for training, and the remaining (non-annotated) sequences are used for testing. [sent-351, score-0.354]
</p><p>86 For our system, we consider one database exemplar at a time, compute its maximal response over the input video, and claim a true positive only when both the class-label and the identified participants are simultaneously correct. [sent-365, score-0.676]
</p><p>87 Otherwise a false positive is indicated for that exemplar class. [sent-366, score-0.364]
</p><p>88 Next we study detection in terms ofboth temporal localization and participant identification. [sent-368, score-0.323]
</p><p>89 For temporal localization, we follow the protocol of [1] by indicating a true-positive when there is correct classification and more than a 50% ratio between the intersection and union of the estimated temporal interval and the ground-truth. [sent-369, score-0.503]
</p><p>90 We achieve a slightly smaller area under ROC curve than the two baselines, as shown in Table 2, but point out that differences are hard to interpret because the temporal boundaries are somewhat ambiguous for the consecutively-executed interactions in the dataset. [sent-370, score-0.463]
</p><p>91 We attribute this to the fact that we explicitly discriminate 222777222866  interactions and participants in the form of tracks of bounding boxes, while [1] does not do so but simply explains an input using a non-discriminative generative model. [sent-372, score-0.714]
</p><p>92 It is interesting to see the pairwise descriptor plays a more crucial role for this dataset: A significant performance drop arises when we only consider individual action descriptors. [sent-380, score-0.263]
</p><p>93 Classification accuracies and false positive (FP) rates comparison on UT-Interaction dataset for evaluating the effectiveness of different components of the proposed approach: Individual and/or pairwise descriptors, with or without metric learning (ML). [sent-382, score-0.25]
</p><p>94 As an application to agents other than humans, we also evaluate our approach in the mouse dataset of [2]. [sent-390, score-0.338]
</p><p>95 We introduced a voting-based approach for detecting and localizing small-group interactions within larger social gatherings. [sent-393, score-0.546]
</p><p>96 Since it operates on agent tracks, it is also quite flexible and can be applied in many different multi-agent scenarios, provided that the environment-specific individual descriptor and the environment-specific pairwise descriptor are properly defined. [sent-395, score-0.452]
</p><p>97 We represent group interactions as collections of individual and pairwise descriptors (1st and 2nd order), and our results suggest that this is effective for groups of up to four agents. [sent-397, score-0.73]
</p><p>98 Higher-order interaction descriptors may play a more important role for larger interacting groups, and this may be a useful future research direction as new datasets become available. [sent-398, score-0.38]
</p><p>99 We use a simple combination of descriptor collection and temporal pyramid, but one could imagine using a (learned) tree of space-time parts, analogous to how spatial parts-based models are used for object detection. [sent-400, score-0.311]
</p><p>100 A chains model for localizing participants of group activities in videos. [sent-406, score-0.466]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exemplar', 0.312), ('participants', 0.284), ('agents', 0.28), ('interactions', 0.265), ('interaction', 0.224), ('temporal', 0.198), ('agent', 0.192), ('qt', 0.171), ('social', 0.163), ('ts', 0.161), ('exemplars', 0.13), ('ensembles', 0.125), ('instantaneous', 0.112), ('te', 0.111), ('classroom', 0.111), ('interval', 0.107), ('pairwise', 0.107), ('descriptors', 0.103), ('tracks', 0.099), ('group', 0.095), ('students', 0.091), ('localizing', 0.087), ('ds', 0.085), ('participant', 0.077), ('wnm', 0.075), ('harvard', 0.074), ('units', 0.07), ('cvx', 0.066), ('distances', 0.064), ('lmnn', 0.064), ('video', 0.062), ('matching', 0.061), ('gathering', 0.059), ('collections', 0.059), ('mouse', 0.058), ('individual', 0.057), ('occur', 0.053), ('interacting', 0.053), ('unit', 0.053), ('false', 0.052), ('action', 0.051), ('bystanders', 0.05), ('porfilio', 0.05), ('localization', 0.048), ('rates', 0.048), ('voting', 0.048), ('descriptor', 0.048), ('dp', 0.046), ('matches', 0.046), ('roc', 0.045), ('gallery', 0.045), ('conversations', 0.044), ('lowest', 0.044), ('groups', 0.044), ('identified', 0.044), ('comprised', 0.044), ('mn', 0.043), ('cells', 0.043), ('opencv', 0.043), ('metric', 0.043), ('dissimilarity', 0.043), ('matched', 0.042), ('tw', 0.042), ('minw', 0.041), ('detected', 0.041), ('distinctive', 0.04), ('categories', 0.04), ('videos', 0.039), ('localized', 0.039), ('permuted', 0.039), ('localizations', 0.039), ('triples', 0.039), ('extent', 0.038), ('turned', 0.038), ('entry', 0.037), ('busy', 0.037), ('enhance', 0.037), ('individuals', 0.036), ('input', 0.036), ('mins', 0.035), ('cell', 0.035), ('annotated', 0.035), ('depicted', 0.035), ('head', 0.034), ('vote', 0.033), ('boxes', 0.033), ('pyramid', 0.033), ('dissimilarities', 0.033), ('collection', 0.033), ('array', 0.033), ('tracking', 0.032), ('category', 0.032), ('analogous', 0.032), ('detecting', 0.031), ('discrimination', 0.031), ('wt', 0.03), ('bounding', 0.03), ('dh', 0.03), ('fragmented', 0.03), ('circles', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="172-tfidf-1" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>2 0.218779 <a title="172-tfidf-2" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>3 0.20338495 <a title="172-tfidf-3" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>4 0.19812562 <a title="172-tfidf-4" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>5 0.18120486 <a title="172-tfidf-5" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>6 0.15836537 <a title="172-tfidf-6" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>7 0.13936734 <a title="172-tfidf-7" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>8 0.13860925 <a title="172-tfidf-8" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>9 0.13460454 <a title="172-tfidf-9" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>10 0.13063289 <a title="172-tfidf-10" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>11 0.12723722 <a title="172-tfidf-11" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>12 0.12582998 <a title="172-tfidf-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.1243679 <a title="172-tfidf-13" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>14 0.12057802 <a title="172-tfidf-14" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>15 0.11838978 <a title="172-tfidf-15" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>16 0.11545405 <a title="172-tfidf-16" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>17 0.11483506 <a title="172-tfidf-17" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>18 0.11320576 <a title="172-tfidf-18" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>19 0.1116079 <a title="172-tfidf-19" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>20 0.10964483 <a title="172-tfidf-20" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.25), (1, -0.077), (2, -0.013), (3, -0.123), (4, -0.035), (5, 0.001), (6, -0.048), (7, -0.041), (8, 0.036), (9, -0.012), (10, 0.077), (11, -0.041), (12, 0.095), (13, 0.021), (14, 0.017), (15, -0.043), (16, 0.064), (17, 0.059), (18, 0.048), (19, -0.107), (20, -0.045), (21, 0.023), (22, 0.034), (23, 0.036), (24, 0.022), (25, -0.018), (26, 0.051), (27, -0.079), (28, -0.011), (29, -0.01), (30, 0.013), (31, -0.034), (32, 0.056), (33, -0.015), (34, 0.023), (35, -0.044), (36, 0.13), (37, 0.002), (38, 0.071), (39, 0.034), (40, 0.052), (41, -0.023), (42, -0.071), (43, 0.036), (44, 0.233), (45, -0.088), (46, -0.1), (47, -0.107), (48, -0.061), (49, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94685119 <a title="172-lsi-1" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>2 0.79557699 <a title="172-lsi-2" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>3 0.71187377 <a title="172-lsi-3" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>4 0.65695405 <a title="172-lsi-4" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>5 0.56889176 <a title="172-lsi-5" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>6 0.54095036 <a title="172-lsi-6" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>7 0.53776699 <a title="172-lsi-7" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>8 0.49396059 <a title="172-lsi-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.49021858 <a title="172-lsi-9" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>10 0.4824751 <a title="172-lsi-10" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>11 0.47686014 <a title="172-lsi-11" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>12 0.47422737 <a title="172-lsi-12" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>13 0.45815977 <a title="172-lsi-13" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>14 0.45232981 <a title="172-lsi-14" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>15 0.44983876 <a title="172-lsi-15" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>16 0.44404826 <a title="172-lsi-16" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>17 0.44258177 <a title="172-lsi-17" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>18 0.43832082 <a title="172-lsi-18" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>19 0.43611073 <a title="172-lsi-19" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>20 0.43063968 <a title="172-lsi-20" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.098), (16, 0.02), (26, 0.058), (33, 0.215), (67, 0.076), (69, 0.396), (87, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88594776 <a title="172-lda-1" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>Author: Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: 3D volumetric reasoning is important for truly understanding a scene. Humans are able to both segment each object in an image, and perceive a rich 3D interpretation of the scene, e.g., the space an object occupies, which objects support other objects, and which objects would, if moved, cause other objects to fall. We propose a new approach for parsing RGB-D images using 3D block units for volumetric reasoning. The algorithm fits image segments with 3D blocks, and iteratively evaluates the scene based on block interaction properties. We produce a 3D representation of the scene based on jointly optimizing over segmentations, block fitting, supporting relations, and object stability. Our algorithm incorporates the intuition that a good 3D representation of the scene is the one that fits the data well, and is a stable, self-supporting (i.e., one that does not topple) arrangement of objects. We experiment on several datasets including controlled and real indoor scenarios. Results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation.</p><p>same-paper 2 0.86947912 <a title="172-lda-2" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>3 0.84924531 <a title="172-lda-3" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>4 0.83880818 <a title="172-lda-4" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>5 0.8340891 <a title="172-lda-5" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>6 0.82320732 <a title="172-lda-6" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>7 0.80893368 <a title="172-lda-7" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>8 0.7721405 <a title="172-lda-8" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>9 0.7420457 <a title="172-lda-9" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>10 0.70418775 <a title="172-lda-10" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>11 0.68950874 <a title="172-lda-11" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>12 0.68233079 <a title="172-lda-12" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>13 0.6755023 <a title="172-lda-13" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>14 0.66903156 <a title="172-lda-14" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>15 0.66750413 <a title="172-lda-15" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>16 0.66633123 <a title="172-lda-16" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>17 0.6656009 <a title="172-lda-17" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>18 0.66136974 <a title="172-lda-18" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>19 0.65838796 <a title="172-lda-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.6540519 <a title="172-lda-20" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
