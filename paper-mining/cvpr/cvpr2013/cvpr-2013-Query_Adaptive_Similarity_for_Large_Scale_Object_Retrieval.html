<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-343" href="#">cvpr2013-343</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</h1>
<br/><p>Source: <a title="cvpr-2013-343-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Qin_Query_Adaptive_Similarity_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Danfeng Qin, Christian Wengert, Luc Van_Gool</p><p>Abstract: Many recent object retrieval systems rely on local features for describing an image. The similarity between a pair of images is measured by aggregating the similarity between their corresponding local features. In this paper we present a probabilistic framework for modeling the feature to feature similarity measure. We then derive a query adaptive distance which is appropriate for global similarity evaluation. Furthermore, we propose a function to score the individual contributions into an image to image similarity within the probabilistic framework. Experimental results show that our method improves the retrieval accuracy significantly and consistently. Moreover, our result compares favorably to the state-of-the-art.</p><p>Reference: <a title="cvpr-2013-343-reference" href="../cvpr2013_reference/cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch } on  ,  Abstract Many recent object retrieval systems rely on local features for describing an image. [sent-4, score-0.254]
</p><p>2 The similarity between a pair of images is measured by aggregating the similarity between their corresponding local features. [sent-5, score-0.388]
</p><p>3 In this paper we present a probabilistic framework for modeling the feature to feature similarity measure. [sent-6, score-0.359]
</p><p>4 We then derive a query adaptive distance which is appropriate for global similarity evaluation. [sent-7, score-0.79]
</p><p>5 Furthermore, we propose a function to score the individual contributions into an image to image similarity within the probabilistic framework. [sent-8, score-0.217]
</p><p>6 Most of the recent state-of-the-art large scale image retrieval systems rely on local features, in particular the SIFT descriptor [14] and its variants. [sent-14, score-0.23]
</p><p>7 We then derive a measure that is adaptive to the query feature. [sent-19, score-0.491]
</p><p>8 We show - both on simulated and real data - that the Euclidean distance density distribution is highly query dependent and that our model adapts the original distance accordingly. [sent-20, score-0.678]
</p><p>9 The expected distance to the non-corresponding features can be used to adapt the original distance and can be efficiently estimated by introducing a small set of random features as negative examples. [sent-22, score-0.402]
</p><p>10 Furthermore, we derive a global similarity function that scores the feature to feature similarities. [sent-23, score-0.362]
</p><p>11 Despite its simplicity, experimental results on standard benchmarks show that our method improves the retrieval accuracy consistently and significantly and compares favorably to the state-of-the-art. [sent-26, score-0.266]
</p><p>12 Results in a large scale image retrieval system are presented in Section 5 and compared with the state-of-the-art. [sent-32, score-0.23]
</p><p>13 Related Work Most of the recent works addressing the image similarity problem in image retrieval can be roughly grouped into three categories. [sent-34, score-0.407]
</p><p>14 Feature-feature similarity The first group mainly works on establishing local feature correspondence. [sent-35, score-0.251]
</p><p>15 In order to reduce quantization artifacts, [20] proposed to assign each feature to multiple visual words. [sent-39, score-0.199]
</p><p>16 Additionally, product quantization [12] was used to esti111666001088  mate the pairwise Euclidean distance between features, and the top k nearest neighbors of a query feature is considered as matches. [sent-41, score-0.779]
</p><p>17 Recently, several researchers have addressed the problem of the Euclidean distance not being the optimal similarity measure in most situations. [sent-42, score-0.299]
</p><p>18 Intra-image similarity The second group focuses on effectively weighting the similarity of a feature pair considering its relationship to other matched pairs. [sent-45, score-0.459]
</p><p>19 Inter-image similarity Finally, the third group addresses the problem of how to improve the retrieval performance by exploiting additional information contained in other images in the database, that depict the same object as the query image. [sent-54, score-0.748]
</p><p>20 That is, after retrieving a set of spatially verified database images, this new set is used to query the system again to increase recall. [sent-56, score-0.369]
</p><p>21 In [22], a set of relevant images is constructed using k-reciprocal nearest neighbors, and the similarity score is evaluated on how similar a database image is to this set. [sent-57, score-0.292]
</p><p>22 By formulating the feature-feature matching problem in a probabilistic framework, we propose an adaptive similarity to each query feature, and a similarity function to approximate the quantitative result. [sent-59, score-0.848]
</p><p>23 Although the idea of adapting similarity by dissimilarity has already been exploited in [11][17], we propose to measure dissimilarity by mean distance of the query to a set of random features, while theirs use k nearest neighbors (kNN). [sent-60, score-0.761]
</p><p>24 Considering the large amount of data in a typical large scale image retrieval system, it is impractical to compute the pairwise distances between high-dimensional original feature vectors. [sent-64, score-0.35]
</p><p>25 Our Approach In this section, we present a theoretical framework for modeling the visual similarity between a pair of features, given a pairwise measurement. [sent-68, score-0.257]
</p><p>26 We then derive an analytical model for computing the accuracy of the similarity estimation in order to compare different similarity measures. [sent-69, score-0.4]
</p><p>27 Since the distribution of the Euclidean distance varies enormously from one query feature to another, we propose to normalize the distance locally to obtain similar degree of measurement across queries. [sent-71, score-0.778]
</p><p>28 Furthermore, using the adaptive measure, we quantitatively analyze  the similarity function on the simulated data and propose a function to approximate the quantitative result. [sent-72, score-0.403]
</p><p>29 A probabilistic view of similarity estimation We are interested in modeling the visual similarity between features based on a pairwise measurement. [sent-76, score-0.503]
</p><p>30 Let us denote as xi the local feature vectors from a query image and as Y = {y1, . [sent-77, score-0.566]
</p><p>31 F oufrt lohceramlo ferae-, let m(xi , yj) denote a pairwise measurement between xi and yj . [sent-84, score-0.771]
</p><p>32 Instead of considering whether yj is similar to xi and how similar they look, we want to evaluate how likely yj belongs to T(xi) given a measure m. [sent-86, score-1.03]
</p><p>33 This can be modeled as follows f(xi , yj) = p(yj ∈ T(xi) | m(xi, yj )) (1) For simplicity, we denote mj = m(xi , yj), Ti = T(xi), and Fi = F(xi). [sent-87, score-0.601]
</p><p>34 T∈h Terefore, p(yj ∈ Ti) and p(yj ∈ Fi) only depend on the query feature xi. [sent-91, score-0.388]
</p><p>35 In contrast, p(mj | yj ∈ Ti) and p(mj | yj ∈ Fi) are the probability density yfun∈cti oTns of the distr|ib yutio∈n Fof mj, for {yj | y ∈ Ti} and {yj | y ∈ Fi}. [sent-92, score-0.852]
</p><p>36 Estimation accuracy Since the pairwise measurement between features is the only observation for our model, it is essential to estimate its reliability. [sent-100, score-0.224]
</p><p>37 In other words, the better the measurement distinguishes the true correspondences from the false ones, the more accurately the feature similarity based on it can be estimated. [sent-102, score-0.407]
</p><p>38 Query adaptive distance  It has been observed that the Euclidean distance is not an appropriate measurement for similarity [21, 16, 11]. [sent-124, score-0.67]
</p><p>39 As an example, Figure 2 depicts the distributions of the Euclidean distance of the corresponding and non corresponding features for the two different interest points shown in Figure 1. [sent-126, score-0.234]
</p><p>40 It can be seen, that the Euclidean distance separates the matching from the non-matching features quite well in the local neighborhood of a given query feature xi. [sent-129, score-0.564]
</p><p>41 Distribution of the Euclidean distance for two points from the simulated data. [sent-132, score-0.208]
</p><p>42 Another property can also be observed in Figure 2: if a feature has a large distance to its correspondences, it also has a large distance to the non-matching features. [sent-136, score-0.337]
</p><p>43 It is intractable to estimate the distance distribution between all feature and their correspondences, but it is simple to estimate the expected distance to non-corresponding features. [sent-138, score-0.343]
</p><p>44 Since the non-corresponding features are independent from the query, a set of randomly sampled, thus unrelated features can be used to represent the set of noncorrespondent features to each query. [sent-139, score-0.202]
</p><p>45 Moreover, if we assume the distance distribution of the non-corresponding set to follow a normal distribution N(μ, σ), then the estimattioo nfo error ao fn oirtsm mean rbiabsuetdio on a sμu,bσs)e,t hfoelnlow thse eansotimthearnormal distribution N(0, σ/N), with N the size of the subsneotr. [sent-140, score-0.221]
</p><p>46 The probability that an unknown feature matches to the query one when observing their distance z can be modeled as,  p(T|z)= {N1T+×N pTF(zN×|T p ×)( z p+ |(NzTF |) T}×−)1p(z|F)  (14)  with NT and NF the number of corresponding and noncorresponding pairs respectively. [sent-144, score-0.566]
</p><p>47 Figure 3 illustrates how the adaptive distance recovers more correct matches compared to the Euclidean distance. [sent-147, score-0.253]
</p><p>48 Similarity function In this section, we show how to derive a globally appropriate feature similarity in a quantitative manner. [sent-155, score-0.291]
</p><p>49 After having established the distance distribution of the query adaptive distance in the previous section, the only unknown in Equation 5 remains As discussed in Sect∈ioTn 3. [sent-156, score-0.723]
</p><p>50 The resulting curves follow an inverse sigmoid form such that the similarity is 1for dn → 0 and 0 if dn → 1. [sent-159, score-0.36]
</p><p>51 For the reason of simplicity, we choose to approximate the similarity function as f(xi, yj) = exp(−α dn (xi, yj)4) (15) As can be seen in Figure 4, this curve is flatter and covers approximately the full range of possible values for c. [sent-166, score-0.27]
</p><p>52 Overall method In this section we will integrate the query adaptive distance measurement and the similarity function presented 111666111311  0. [sent-171, score-0.868]
</p><p>53 52 ftraluese c oorr erpeopnodnedenntt ppaairisrs  ftraluese c oorr erpeopnodnedenntt ppaairisrs  (a) Euclidean Distance (b) Query Adaptive Distance Figure 3. [sent-183, score-0.472]
</p><p>54 The comparison of our adaptive distance to the Euclidean distance on dataset D. [sent-184, score-0.372]
</p><p>55 Let the visual similarity between the query image q = {x1, . [sent-214, score-0.497]
</p><p>56 1  with f(xi, yj) the pairwise feature similarity as in Equation 15. [sent-229, score-0.3]
</p><p>57 For retrieval, we use a standard bag-of-words inverted  file. [sent-231, score-0.238]
</p><p>58 However, in order to have an estimation of the pairwise distance d(xi, yj) between query and database features, we add a product quantization scheme as in [12] and select the same parameters as the original author. [sent-232, score-0.665]
</p><p>59 000 Voronoi cells according to a coarse quantization codebook Kc. [sent-234, score-0.198]
</p><p>60 All features lcoocradtiendg i tno t ah ec same qVuoarnotnizoai cioenll are grouped into the same inverted list. [sent-235, score-0.295]
</p><p>61 Each feature is further quantized with respect to its coarse quantization centroid. [sent-236, score-0.225]
</p><p>62 That is, the residual between the feature and its closest centroid is equally split into m = 8 parts and each part is separately quantized according to a product quantization codebook Kp with Np = 256 centtoro aid psr. [sent-237, score-0.243]
</p><p>63 Tdhucent, q euaacnhti fzeaattiuorne c cios deenbcooodked K using its related image identifier and a set of quantization codes, and is stored in its corresponding inverted list. [sent-238, score-0.414]
</p><p>64 We select random features from Flickr and add 100 of them to each inverted list. [sent-239, score-0.345]
</p><p>65 For performance reasons, we make sure that the random features are added to the inverted list before adding the database vectors. [sent-240, score-0.451]
</p><p>66 At query time, all inverted lists whose related coarse quantization centers are in the k nearest neighborhood of the query vector are scanned. [sent-241, score-1.142]
</p><p>67 Then, the query adaptive distance dn (xi, yj) to each database vector can directly be computed as in Equation 13. [sent-243, score-0.712]
</p><p>68 In order to reduce unnecessary computation even more, a threshold β is used  ×  to quickly drop features whose Euclidean distance is larger than β Nd(xi) . [sent-244, score-0.206]
</p><p>69 Random feature dataset preparation Random images from Flickr (however different from the codebook training dataset) are used to generate the random feature dataset. [sent-274, score-0.236]
</p><p>70 There are two parameters in our method: the number of random features in each inverted list, and the cut-  ×  off threshold β for filtering out features whose contribution is negligible. [sent-278, score-0.402]
</p><p>71 The influence of the number of the random features Table 1 shows the retrieval performance by varying the number of random features for each inverted list. [sent-279, score-0.699]
</p><p>72 This supports the assumption, that the mean distance of a query feature to the dissimilar features can be robustly estimated even with a small number of random features. [sent-281, score-0.671]
</p><p>73 We select 100 random features per inverted list throughout the rest of this paper. [sent-282, score-0.426]
</p><p>74 The influence of the cut-off threshold β Table 2 shows that features with a distance larger than β Nd(xi) with  Length50100500100010000 mAP0. [sent-283, score-0.226]
</p><p>75 Influence of the size of the random feature set for inverted list on Oxford5k  β0. [sent-289, score-0.413]
</p><p>76 Effectiveness of our method Local adaptive distance In order to compare the adaptive distance function to the Euclidean distance, we use a threshold for separating matching and non-matching features. [sent-309, score-0.506]
</p><p>77 4 shows the retrieval performance for a varying threshold both for the Euclidean distance as well as for the adaptive distance. [sent-311, score-0.45]
</p><p>78 Overall, the best mAP using the adaptive distance is 3% better than the Euclidean distance. [sent-312, score-0.253]
</p><p>79 Furthermore, the adaptive distance is less sensitive when selecting a non-optimal threshold. [sent-313, score-0.253]
</p><p>80 Comparison of our adaptive distance with Euclidean distance on Oxford5k dataset Contributions of other steps In order to justify the contribution of other steps that are contained in our method, we evaluate the performance of our method by taking them out of the pipeline. [sent-316, score-0.433]
</p><p>81 With multi-assignment only on the query side, mAP can increase from 0. [sent-321, score-0.317]
</p><p>82 MA denotes the number of inverted lists that are traversed per query feature. [sent-325, score-0.611]
</p><p>83 As expected, multi-assignment (scanning of several inverted lists) reduces the quantization artifacts and improves the performance consistently, however, in exchange for more computational load. [sent-329, score-0.366]
</p><p>84 We choose to use reciprocal nearest neighbors (RNN) [22], for the reason that it can be easily integrated on top of a retrieval system independently from the image similarity function. [sent-331, score-0.472]
</p><p>85 Considering that RNN tries to exploit additional information contained in other relevant database images, which are scarce in Holidays (in average only 2 to 3 relevant database images per query), it is difficult for query expansion methods to perform much better. [sent-334, score-0.448]
</p><p>86 Retrieval Performance by using top k nearest neighbor as similar features [12] We first compare the performance of our method to [12] which relies on using the top k nearest neighbors of the Euclidean distance for selecting the similar features of a query. [sent-343, score-0.416]
</p><p>87 In all of the previous experiments, each feature costs 12 bytes of memory. [sent-371, score-0.238]
</p><p>88 Specifically, 4 bytes is used for the image identifier and 8 bytes for the quantization codes. [sent-372, score-0.51]
</p><p>89 As [11] mainly show results using more bytes for feature encoding, we also compare our method to theirs with more bytes per feature. [sent-373, score-0.405]
</p><p>90 As shown in Table 6, using more bytes further improves the retrieval results. [sent-374, score-0.364]
</p><p>91 In all experiments, we compare favorably to the stateof-the-art by exploiting a simple similarity function without any parameter tuning for each dataset. [sent-376, score-0.252]
</p><p>92 g for Oxford5k, we observe that our method is 30% faster than the original prod-  uct quantization algorithm[12] while traversing the inverted lists, for the reason that our method requires no heap structure. [sent-386, score-0.366]
</p><p>93 However, for a large scale experiment, we observe similar timing of our method to theirs as each inverted list contains a very long list of database features, and thus the computation of the Euclidean distance will dominate the computational time. [sent-387, score-0.582]
</p><p>94 Conclusion In this paper, we present a probabilistic framework for the feature to feature similarity for high-dimensional local features such as SIFT. [sent-389, score-0.416]
</p><p>95 We then propose a query adaptive feature to feature distance measurement and derive a global image to image similarity function. [sent-390, score-1.05]
</p><p>96 Total recall: Automatic query expansion with a generative feature model for object retrieval. [sent-422, score-0.388]
</p><p>97 Object retrieval with large vocabularies and fast spatial matching. [sent-509, score-0.24]
</p><p>98 Lost in quantization: Improving particular object retrieval in large scale image databases. [sent-517, score-0.23]
</p><p>99 Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors. [sent-532, score-0.257]
</p><p>100 Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking. [sent-541, score-0.377]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yj', 0.426), ('query', 0.317), ('inverted', 0.238), ('retrieval', 0.197), ('similarity', 0.18), ('xi', 0.178), ('rnn', 0.177), ('mj', 0.175), ('bytes', 0.167), ('euclidean', 0.159), ('burstiness', 0.137), ('adaptive', 0.134), ('quantization', 0.128), ('distance', 0.119), ('measurement', 0.118), ('holidays', 0.114), ('egou', 0.113), ('douze', 0.095), ('dn', 0.09), ('simulated', 0.089), ('chum', 0.087), ('yyjj', 0.078), ('acc', 0.074), ('feature', 0.071), ('ti', 0.07), ('philbin', 0.069), ('distractor', 0.063), ('fi', 0.062), ('isard', 0.06), ('nearest', 0.06), ('datasetours', 0.059), ('erpeopnodnedenntt', 0.059), ('ftraluese', 0.059), ('noncorresponding', 0.059), ('oorr', 0.059), ('ppaairisrs', 0.059), ('tfii', 0.059), ('toefd', 0.059), ('flickr', 0.059), ('features', 0.057), ('lists', 0.056), ('list', 0.054), ('database', 0.052), ('asift', 0.052), ('affine', 0.052), ('sivic', 0.051), ('bow', 0.051), ('influence', 0.05), ('random', 0.05), ('pairwise', 0.049), ('pp', 0.049), ('identifier', 0.048), ('furthermore', 0.044), ('codebook', 0.044), ('perdoch', 0.044), ('knn', 0.043), ('vocabularies', 0.043), ('nd', 0.042), ('paris', 0.041), ('derive', 0.04), ('qin', 0.04), ('voronoi', 0.04), ('favorably', 0.04), ('simplicity', 0.039), ('correspondences', 0.038), ('dz', 0.038), ('probabilistic', 0.037), ('neighbors', 0.035), ('justify', 0.034), ('distribution', 0.034), ('scale', 0.033), ('tuning', 0.032), ('burden', 0.032), ('dominate', 0.032), ('distributions', 0.031), ('ma', 0.031), ('unrelated', 0.031), ('moreover', 0.03), ('codebooks', 0.03), ('october', 0.03), ('roughly', 0.03), ('drop', 0.03), ('dissimilar', 0.03), ('nf', 0.029), ('consistently', 0.029), ('sim', 0.029), ('quantity', 0.028), ('property', 0.028), ('june', 0.028), ('neighbor', 0.028), ('pair', 0.028), ('rotating', 0.028), ('depict', 0.027), ('pointed', 0.027), ('non', 0.027), ('contained', 0.027), ('supports', 0.027), ('throughout', 0.027), ('hamming', 0.026), ('coarse', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="343-tfidf-1" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>Author: Danfeng Qin, Christian Wengert, Luc Van_Gool</p><p>Abstract: Many recent object retrieval systems rely on local features for describing an image. The similarity between a pair of images is measured by aggregating the similarity between their corresponding local features. In this paper we present a probabilistic framework for modeling the feature to feature similarity measure. We then derive a query adaptive distance which is appropriate for global similarity evaluation. Furthermore, we propose a function to score the individual contributions into an image to image similarity within the probabilistic framework. Experimental results show that our method improves the retrieval accuracy significantly and consistently. Moreover, our result compares favorably to the state-of-the-art.</p><p>2 0.24731888 <a title="343-tfidf-2" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>3 0.22082824 <a title="343-tfidf-3" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>4 0.21264677 <a title="343-tfidf-4" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>Author: Petr Gronát, Guillaume Obozinski, Josef Sivic, Tomáš Pajdla</p><p>Abstract: The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as onlyfewpositive training examples are availablefor each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. 2Center for Machine Perception, Faculty of Electrical Engineering 3WILLOW project, Laboratoire d’Informatique de l’E´cole Normale Sup e´rieure, ENS/INRIA/CNRS UMR 8548. 4Universit Paris-Est, LIGM (UMR CNRS 8049), Center for Visual Computing, Ecole des Ponts - ParisTech, 77455 Marne-la-Valle, France</p><p>5 0.16912414 <a title="343-tfidf-5" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>Author: Baoyuan Wu, Yifan Zhang, Bao-Gang Hu, Qiang Ji</p><p>Abstract: In this paper, we focus on face clustering in videos. Given the detected faces from real-world videos, we partition all faces into K disjoint clusters. Different from clustering on a collection of facial images, the faces from videos are organized as face tracks and the frame index of each face is also provided. As a result, many pairwise constraints between faces can be easily obtained from the temporal and spatial knowledge of the face tracks. These constraints can be effectively incorporated into a generative clustering model based on the Hidden Markov Random Fields (HMRFs). Within the HMRF model, the pairwise constraints are augmented by label-level and constraint-level local smoothness to guide the clustering process. The parameters for both the unary and the pairwise potential functions are learned by the simulated field algorithm, and the weights of constraints can be easily adjusted. We further introduce an efficient clustering framework specially for face clustering in videos, considering that faces in adjacent frames of the same face track are very similar. The framework is applicable to other clustering algorithms to significantly reduce the computational cost. Experiments on two face data sets from real-world videos demonstrate the significantly improved performance of our algorithm over state-of-theart algorithms.</p><p>6 0.15882164 <a title="343-tfidf-6" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>7 0.14943351 <a title="343-tfidf-7" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>8 0.13915625 <a title="343-tfidf-8" href="./cvpr-2013-Bilinear_Programming_for_Human_Activity_Recognition_with_Unknown_MRF_Graphs.html">62 cvpr-2013-Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</a></p>
<p>9 0.13230455 <a title="343-tfidf-9" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>10 0.12899666 <a title="343-tfidf-10" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>11 0.12658541 <a title="343-tfidf-11" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>12 0.12497072 <a title="343-tfidf-12" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>13 0.12408274 <a title="343-tfidf-13" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>14 0.12359503 <a title="343-tfidf-14" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>15 0.11096448 <a title="343-tfidf-15" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>16 0.11059418 <a title="343-tfidf-16" href="./cvpr-2013-All_About_VLAD.html">38 cvpr-2013-All About VLAD</a></p>
<p>17 0.11019818 <a title="343-tfidf-17" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>18 0.10785292 <a title="343-tfidf-18" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>19 0.10491274 <a title="343-tfidf-19" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>20 0.10468608 <a title="343-tfidf-20" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, -0.039), (2, -0.028), (3, 0.024), (4, 0.099), (5, 0.015), (6, -0.095), (7, -0.138), (8, -0.122), (9, -0.091), (10, -0.047), (11, 0.047), (12, 0.08), (13, 0.062), (14, -0.019), (15, -0.151), (16, 0.059), (17, 0.013), (18, 0.073), (19, -0.171), (20, 0.135), (21, -0.069), (22, -0.031), (23, 0.123), (24, 0.012), (25, -0.048), (26, -0.011), (27, 0.006), (28, 0.02), (29, 0.046), (30, 0.142), (31, -0.044), (32, -0.014), (33, 0.068), (34, -0.0), (35, -0.046), (36, 0.051), (37, -0.016), (38, -0.037), (39, -0.033), (40, -0.007), (41, 0.021), (42, 0.101), (43, 0.099), (44, -0.106), (45, 0.024), (46, 0.03), (47, 0.043), (48, 0.084), (49, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94439709 <a title="343-lsi-1" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>Author: Danfeng Qin, Christian Wengert, Luc Van_Gool</p><p>Abstract: Many recent object retrieval systems rely on local features for describing an image. The similarity between a pair of images is measured by aggregating the similarity between their corresponding local features. In this paper we present a probabilistic framework for modeling the feature to feature similarity measure. We then derive a query adaptive distance which is appropriate for global similarity evaluation. Furthermore, we propose a function to score the individual contributions into an image to image similarity within the probabilistic framework. Experimental results show that our method improves the retrieval accuracy significantly and consistently. Moreover, our result compares favorably to the state-of-the-art.</p><p>2 0.77932459 <a title="343-lsi-2" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>3 0.7684235 <a title="343-lsi-3" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>4 0.75289917 <a title="343-lsi-4" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>Author: Petr Gronát, Guillaume Obozinski, Josef Sivic, Tomáš Pajdla</p><p>Abstract: The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as onlyfewpositive training examples are availablefor each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. 2Center for Machine Perception, Faculty of Electrical Engineering 3WILLOW project, Laboratoire d’Informatique de l’E´cole Normale Sup e´rieure, ENS/INRIA/CNRS UMR 8548. 4Universit Paris-Est, LIGM (UMR CNRS 8049), Center for Visual Computing, Ecole des Ponts - ParisTech, 77455 Marne-la-Valle, France</p><p>5 0.65738916 <a title="343-lsi-5" href="./cvpr-2013-All_About_VLAD.html">38 cvpr-2013-All About VLAD</a></p>
<p>Author: unkown-author</p><p>Abstract: The objective of this paper is large scale object instance retrieval, given a query image. A starting point of such systems is feature detection and description, for example using SIFT. The focus of this paper, however, is towards very large scale retrieval where, due to storage requirements, very compact image descriptors are required and no information about the original SIFT descriptors can be accessed directly at run time. We start from VLAD, the state-of-the art compact descriptor introduced by J´ egou et al. [8] for this purpose, and make three novel contributions: first, we show that a simple change to the normalization method significantly improves retrieval performance; second, we show that vocabulary adaptation can substantially alleviate problems caused when images are added to the dataset after initial vocabulary learning. These two methods set a new stateof-the-art over all benchmarks investigated here for both mid-dimensional (20k-D to 30k-D) and small (128-D) descriptors. Our third contribution is a multiple spatial VLAD representation, MultiVLAD, that allows the retrieval and local- ization of objects that only extend over a small part of an image (again without requiring use of the original image SIFT descriptors).</p><p>6 0.6483016 <a title="343-lsi-6" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>7 0.64556432 <a title="343-lsi-7" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>8 0.64540839 <a title="343-lsi-8" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>9 0.60148007 <a title="343-lsi-9" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>10 0.59777653 <a title="343-lsi-10" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>11 0.57600027 <a title="343-lsi-11" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>12 0.55508357 <a title="343-lsi-12" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>13 0.52000707 <a title="343-lsi-13" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>14 0.51587427 <a title="343-lsi-14" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>15 0.50356466 <a title="343-lsi-15" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>16 0.50269318 <a title="343-lsi-16" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>17 0.49892393 <a title="343-lsi-17" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>18 0.49636269 <a title="343-lsi-18" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>19 0.48778725 <a title="343-lsi-19" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>20 0.48682413 <a title="343-lsi-20" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.091), (16, 0.046), (26, 0.052), (28, 0.015), (33, 0.348), (59, 0.012), (67, 0.085), (69, 0.039), (77, 0.012), (87, 0.079), (91, 0.157)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95472831 <a title="343-lda-1" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>Author: Nikolaos Pitelis, Chris Russell, Lourdes Agapito</p><p>Abstract: In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image.</p><p>2 0.95343482 <a title="343-lda-2" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>3 0.93858856 <a title="343-lda-3" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>4 0.93476981 <a title="343-lda-4" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>5 0.92790139 <a title="343-lda-5" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>same-paper 6 0.92337668 <a title="343-lda-6" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>7 0.92003143 <a title="343-lda-7" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>8 0.9184463 <a title="343-lda-8" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>9 0.91785079 <a title="343-lda-9" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>10 0.91652608 <a title="343-lda-10" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>11 0.91623557 <a title="343-lda-11" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>12 0.91569632 <a title="343-lda-12" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>13 0.91567826 <a title="343-lda-13" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>14 0.91561604 <a title="343-lda-14" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>15 0.91522908 <a title="343-lda-15" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>16 0.91517204 <a title="343-lda-16" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>17 0.9151175 <a title="343-lda-17" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>18 0.9150095 <a title="343-lda-18" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>19 0.91495222 <a title="343-lda-19" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>20 0.91494381 <a title="343-lda-20" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
