<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 cvpr-2013-FasT-Match: Fast Affine Template Matching</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-162" href="#">cvpr2013-162</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 cvpr-2013-FasT-Match: Fast Affine Template Matching</h1>
<br/><p>Source: <a title="cvpr-2013-162-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Korman_FasT-Match_Fast_Affine_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>Reference: <a title="cvpr-2013-162-reference" href="../cvpr2013_reference/cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('net', 0.467), ('templ', 0.424), ('sad', 0.271), ('parallelogram', 0.259), ('affin', 0.244), ('transform', 0.243), ('asift', 0.164), ('sublinear', 0.152), ('tsur', 0.111), ('zurich', 0.098), ('match', 0.095), ('graysc', 0.087), ('degrad', 0.086), ('closest', 0.085), ('theorem', 0.082), ('mikolajczyk', 0.078), ('fuh', 0.074), ('quadril', 0.074), ('reichm', 0.074), ('dt', 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="162-tfidf-1" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>2 0.18078332 <a title="162-tfidf-2" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>3 0.17948049 <a title="162-tfidf-3" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>Author: Matthias Dantone, Juergen Gall, Christian Leistner, Luc Van_Gool</p><p>Abstract: In this work, we address the problem of estimating 2d human pose from still images. Recent methods that rely on discriminatively trained deformable parts organized in a tree model have shown to be very successful in solving this task. Within such a pictorial structure framework, we address the problem of obtaining good part templates by proposing novel, non-linear joint regressors. In particular, we employ two-layered random forests as joint regressors. The first layer acts as a discriminative, independent body part classifier. The second layer takes the estimated class distributions of the first one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This results in a pose estimation framework that takes dependencies between body parts already for joint localization into account and is thus able to circumvent typical ambiguities of tree structures, such as for legs and arms. In the experiments, we demonstrate that our body parts dependent joint regressors achieve a higher joint localization accuracy than tree-based state-of-the-art methods.</p><p>4 0.14472336 <a title="162-tfidf-4" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>Author: Elhanan Elboer, Michael Werman, Yacov Hel-Or</p><p>Abstract: The graph Laplacian operator, which originated in spectral graph theory, is commonly used for learning applications such as spectral clustering and embedding. In this paper we explore the Laplacian distance, a distance function related to the graph Laplacian, and use it for visual search. We show that previous techniques such as Matching by Tone Mapping (MTM) are particular cases of the Laplacian distance. Generalizing the Laplacian distance results in distance measures which are tolerant to various visual distortions. A novel algorithm based on linear decomposition makes it possible to compute these generalized distances efficiently. The proposed approach is demonstrated for tone mapping invariant, outlier robust and multimodal template matching.</p><p>5 0.12381315 <a title="162-tfidf-5" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>Author: Adrien Bartoli, Toby Collins</p><p>Abstract: It has been shown that a surface deforming isometrically can be reconstructed from a single image and a template 3D shape. Methods from the literature solve this problem efficiently. However, they all assume that the camera model is calibrated, which drastically limits their applicability. We propose (i) a general variational framework that applies to (calibrated and uncalibrated) general camera models and (ii) self-calibrating 3D reconstruction algorithms for the weak-perspective and full-perspective camera models. In the former case, our algorithm returns the normal field and camera ’s scale factor. In the latter case, our algorithm returns the normal field, depth and camera ’s focal length. Our algorithms are the first to achieve deformable 3D reconstruction including camera self-calibration. They apply to much more general setups than existing methods. Experimental results on simulated and real data show that our algorithms give results with the same level of accuracy as existing methods (which use the true focal length) on perspective images, and correctly find the normal field on affine images for which the existing methods fail.</p><p>6 0.11264094 <a title="162-tfidf-6" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>7 0.10510368 <a title="162-tfidf-7" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>8 0.092011638 <a title="162-tfidf-8" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>9 0.090472296 <a title="162-tfidf-9" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>10 0.08906284 <a title="162-tfidf-10" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>11 0.087680556 <a title="162-tfidf-11" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>12 0.080521867 <a title="162-tfidf-12" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>13 0.080319583 <a title="162-tfidf-13" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<p>14 0.077838905 <a title="162-tfidf-14" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>15 0.077496327 <a title="162-tfidf-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.076197721 <a title="162-tfidf-16" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>17 0.075851209 <a title="162-tfidf-17" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>18 0.074512109 <a title="162-tfidf-18" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>19 0.074348591 <a title="162-tfidf-19" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>20 0.070125028 <a title="162-tfidf-20" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, -0.034), (2, -0.032), (3, -0.005), (4, 0.004), (5, 0.027), (6, -0.02), (7, 0.024), (8, 0.022), (9, -0.036), (10, -0.025), (11, -0.018), (12, -0.099), (13, -0.042), (14, 0.011), (15, -0.041), (16, 0.033), (17, -0.046), (18, -0.057), (19, -0.032), (20, 0.022), (21, -0.033), (22, -0.003), (23, 0.041), (24, -0.026), (25, -0.01), (26, -0.019), (27, -0.015), (28, -0.1), (29, 0.057), (30, 0.027), (31, 0.03), (32, 0.061), (33, 0.043), (34, -0.04), (35, -0.02), (36, 0.026), (37, 0.142), (38, -0.028), (39, 0.037), (40, 0.036), (41, -0.036), (42, -0.068), (43, -0.029), (44, -0.056), (45, 0.059), (46, -0.093), (47, 0.074), (48, -0.011), (49, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93589181 <a title="162-lsi-1" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>2 0.78459299 <a title="162-lsi-2" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>Author: Elhanan Elboer, Michael Werman, Yacov Hel-Or</p><p>Abstract: The graph Laplacian operator, which originated in spectral graph theory, is commonly used for learning applications such as spectral clustering and embedding. In this paper we explore the Laplacian distance, a distance function related to the graph Laplacian, and use it for visual search. We show that previous techniques such as Matching by Tone Mapping (MTM) are particular cases of the Laplacian distance. Generalizing the Laplacian distance results in distance measures which are tolerant to various visual distortions. A novel algorithm based on linear decomposition makes it possible to compute these generalized distances efficiently. The proposed approach is demonstrated for tone mapping invariant, outlier robust and multimodal template matching.</p><p>3 0.70156807 <a title="162-lsi-3" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>4 0.67383754 <a title="162-lsi-4" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>5 0.64665562 <a title="162-lsi-5" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>Author: Mayank Bansal, Kostas Daniilidis</p><p>Abstract: We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night), age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixellevel descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art.</p><p>6 0.63566226 <a title="162-lsi-6" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>7 0.63078564 <a title="162-lsi-7" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>8 0.61560678 <a title="162-lsi-8" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>9 0.60128921 <a title="162-lsi-9" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>10 0.58799249 <a title="162-lsi-10" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>11 0.56640434 <a title="162-lsi-11" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>12 0.56276685 <a title="162-lsi-12" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>13 0.55928367 <a title="162-lsi-13" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>14 0.54973263 <a title="162-lsi-14" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>15 0.5468927 <a title="162-lsi-15" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<p>16 0.53613842 <a title="162-lsi-16" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>17 0.53349531 <a title="162-lsi-17" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>18 0.52915251 <a title="162-lsi-18" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>19 0.52576673 <a title="162-lsi-19" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>20 0.52058655 <a title="162-lsi-20" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.078), (4, 0.167), (5, 0.128), (37, 0.083), (70, 0.012), (81, 0.069), (86, 0.033), (88, 0.204), (89, 0.018), (95, 0.015), (97, 0.118)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83959627 <a title="162-lda-1" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>2 0.81374305 <a title="162-lda-2" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>Author: Amaury Dame, Victor A. Prisacariu, Carl Y. Ren, Ian Reid</p><p>Abstract: We propose a formulation of monocular SLAM which combines live dense reconstruction with shape priors-based 3D tracking and reconstruction. Current live dense SLAM approaches are limited to the reconstruction of visible surfaces. Moreover, most of them are based on the minimisation of a photo-consistency error, which usually makes them sensitive to specularities. In the 3D pose recovery literature, problems caused by imperfect and ambiguous image information have been dealt with by using prior shape knowledge. At the same time, the success of depth sensors has shown that combining joint image and depth information drastically increases the robustness of the classical monocular 3D tracking and 3D reconstruction approaches. In this work we link dense SLAM to 3D object pose and shape recovery. More specifically, we automatically augment our SLAMsystem with object specific identity, together with 6D pose and additional shape degrees of freedom for the object(s) of known class in the scene, combining im- age data and depth information for the pose and shape recovery. This leads to a system that allows for full scaled 3D reconstruction with the known object(s) segmented from the scene. The segmentation enhances the clarity, accuracy and completeness of the maps built by the dense SLAM system, while the dense 3D data aids the segmentation process, yieldingfaster and more reliable convergence than when using 2D image data alone.</p><p>3 0.78681874 <a title="162-lda-3" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>Author: Armand Joulin, Sing Bing Kang</p><p>Abstract: An anaglyph is a single image created by selecting complementary colors from a stereo color pair; the user can perceive depth by viewing it through color-filtered glasses. We propose a technique to reconstruct the original color stereo pair given such an anaglyph. We modified SIFT-Flow and use it to initially match the different color channels across the two views. Our technique then iteratively refines the matches, selects the good matches (which defines the “anchor” colors), and propagates the anchor colors. We use a diffusion-based technique for the color propagation, and added a step to suppress unwanted colors. Results on a variety of inputs demonstrate the robustness of our technique. We also extended our method to anaglyph videos by using optic flow between time frames.</p><p>4 0.78610498 <a title="162-lda-4" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>Author: Zhuoyuan Chen, Hailin Jin, Zhe Lin, Scott Cohen, Ying Wu</p><p>Abstract: We present an optical flow algorithm for large displacement motions. Most existing optical flow methods use the standard coarse-to-fine framework to deal with large displacement motions which has intrinsic limitations. Instead, we formulate the motion estimation problem as a motion segmentation problem. We use approximate nearest neighbor fields to compute an initial motion field and use a robust algorithm to compute a set of similarity transformations as the motion candidates for segmentation. To account for deviations from similarity transformations, we add local deformations in the segmentation process. We also observe that small objects can be better recovered using translations as the motion candidates. We fuse the motion results obtained under similarity transformations and under translations together before a final refinement. Experimental validation shows that our method can successfully handle large displacement motions. Although we particularly focus on large displacement motions in this work, we make no sac- rifice in terms of overall performance. In particular, our method ranks at the top of the Middlebury benchmark.</p><p>5 0.7826097 <a title="162-lda-5" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>6 0.7819345 <a title="162-lda-6" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>7 0.77996933 <a title="162-lda-7" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>8 0.77955759 <a title="162-lda-8" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>9 0.77840644 <a title="162-lda-9" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>10 0.7779097 <a title="162-lda-10" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>11 0.77726841 <a title="162-lda-11" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>12 0.77688485 <a title="162-lda-12" href="./cvpr-2013-Whitened_Expectation_Propagation%3A_Non-Lambertian_Shape_from_Shading_and_Shadow.html">466 cvpr-2013-Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow</a></p>
<p>13 0.77560812 <a title="162-lda-13" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>14 0.77551085 <a title="162-lda-14" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>15 0.77276546 <a title="162-lda-15" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>16 0.77105892 <a title="162-lda-16" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>17 0.77094674 <a title="162-lda-17" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>18 0.77077854 <a title="162-lda-18" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>19 0.77073318 <a title="162-lda-19" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>20 0.76991296 <a title="162-lda-20" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
