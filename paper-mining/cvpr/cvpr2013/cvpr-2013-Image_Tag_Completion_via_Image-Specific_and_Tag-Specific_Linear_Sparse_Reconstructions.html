<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-213" href="#">cvpr2013-213</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</h1>
<br/><p>Source: <a title="cvpr-2013-213-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lin_Image_Tag_Completion_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>Reference: <a title="cvpr-2013-213-reference" href="../cvpr2013_reference/cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. [sent-8, score-0.908]
</p><p>2 Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i. [sent-9, score-0.303]
</p><p>3 Generally, such large-scale social images are associated with userprovided textual tags for describing their semantic content, which are widely utilized for facilitating image management. [sent-20, score-0.315]
</p><p>4 However, due to the time-consuming tagging process and the arbitrariness of user tagging behaviours, the userprovided tags probably contain imprecise ones, and they are usually incomplete, as also revealed in [1, 11]. [sent-21, score-0.76]
</p><p>5 The imprecision and incompleteness of user-provided tags probably leads to performance degradations of various tag-dependent applications like tag based image retrieval, etc. [sent-22, score-0.934]
</p><p>6 Therefore, recently tag refinement, including denoising and completion, has become an attractive subject of many ongoing researches. [sent-23, score-0.694]
</p><p>7 However, previous work on tag refinement, as referred to in related work, focused more on denoising but less on completion, even though incompleteness can also introduce serious negative effects. [sent-24, score-0.72]
</p><p>8 Given an incomplete initial tagging matrix, tag completion is to fill it up by identifying more correct associations between images and tags. [sent-25, score-1.187]
</p><p>9 In this paper we propose a novel tag completion scheme denoted as LSR and tackle the problem from both perspectives of image (i. [sent-26, score-0.945]
</p><p>10 Specifically, each image and tag  is optimally reconstructed with remaining ones under constraints of sparsity, and then the reconstruction values from both perspectives are normalized and merged for predicting the relevance of unlabelled tags. [sent-31, score-0.918]
</p><p>11 Regarding the imagespecific reconstruction, both low-level image features and high-level tagging row vectors are considered. [sent-32, score-0.318]
</p><p>12 As for the tag-specific reconstruction, we mainly consider their corresponding column vectors in the initial tagging matrix, which essentially mines their concurrence for seeking unlabelled high-confidence tags with initially labelled ones within an image. [sent-33, score-0.702]
</p><p>13 Therefore, the proposed LSR is a unified framework merging image-image similarity, image-tag association and tag-tag concurrence for tag completion. [sent-34, score-0.743]
</p><p>14 ∙  ∙  We propose an effective tag completion scheme via image-specific a enfdfe tag-specific mlinpleeatrio sparse reconstructions, considering and merging various contextual information. [sent-36, score-0.98]
</p><p>15 We propose to perform tag completion for each row aWnde pcroolupmosen separately, i tnasgte caodm opfl performing global refinement for the tagging matrix, enabling LSR to complete both existing datasets (i. [sent-37, score-1.228]
</p><p>16 Related Work As tag completion is to add high-confidence candidate tags to a given image, it is intuitive to compare it with image auto-annotation and tag recommendation. [sent-48, score-1.78]
</p><p>17 Tag recommendation [11, 14, 5] is a trade-off between autoannotation and manual tagging, which is to recommend semantically related tags to a user while he is annotating an image online. [sent-56, score-0.324]
</p><p>18 Zwol [11] introduced a generic tag recommendation method that deploys the collective knowledge residing in images. [sent-60, score-0.724]
</p><p>19 [5] formulated tag recommendation as a maximum a posteriori (MAP) problem using a visual folksonomy. [sent-63, score-0.739]
</p><p>20 Though with similar goals, image auto-annotation and  tag completion are still different, since the majority of existing auto-annotation methods are founded on the assumption that images in training set are completely annotated with appropriate tags. [sent-64, score-0.929]
</p><p>21 But what tag completion faces is just a dataset made up of partially annotated images, and it is supposed to add missing related tags to each image. [sent-66, score-1.177]
</p><p>22 In this sense, tag completion seems more difficult, as no extra perfect training set is available. [sent-67, score-0.908]
</p><p>23 As for tag recommendation methods, they are generally designed to work online and prefer to incorporating feedback from labellers, while tag completion can be automatically done offline with looser requirements of real-time performance. [sent-68, score-1.632]
</p><p>24 As mentioned previously, tag completion is included in tag refinement framework, which has recently become an attractive subject of many ongoing researches [12, 4, 6, 16, 9, 7]. [sent-69, score-1.647]
</p><p>25 [4] utilized neighbour voting to learn the relevance of each tag, and then differentiated noisy tags from correct ones. [sent-72, score-0.284]
</p><p>26 [6] performed tag denoising according to the consistency between “visual similarity” and “semantic similarity” in images, and then enriched denoised tags with their synonyms and hypernyms in WordNet. [sent-75, score-0.904]
</p><p>27 formulated the tag refinement problem as a decomposition of the initial tagging matrix into a low-rank refined matrix and a sparse error matrix, with the optimization objective of low-rank, content consistency, tag correlation and error sparsity. [sent-78, score-1.714]
</p><p>28 [9]  constructed semantic unities with pairs of associated tag and image, and further proposed a hyper-graph model for tag clustering and refinement. [sent-81, score-1.319]
</p><p>29 By reviewing previous researches on tag refinement, we realize that they focused more on tag denoising but less on tag completion. [sent-82, score-1.993]
</p><p>30 [16, 9, 7]) are declared to be unified frameworks, unifying with tag denoising can probably introduce risks to the performance of tag completion, due to the difficulty in controlling the degree of denoising. [sent-85, score-1.365]
</p><p>31 Therefore, recently researchers began to pay more attention to tag completion and even treat it as an independent problem. [sent-86, score-0.908]
</p><p>32 [13] proposed to address the problem by searching for the optimal tagging matrix consistent with both observed tags and visual similarities. [sent-89, score-0.494]
</p><p>33 [8] formulated the tag completion procedure as a non-negative data factorization problem and tackled it by embedding various contextual information like within-image and cross-image relations, etc. [sent-92, score-0.939]
</p><p>34 image-specific and tag-specific linear sparse reconstructions, from which corresponding tag completion results based on reconstruction values are output for further normalization and integration. [sent-97, score-1.023]
</p><p>35 Then according to the merged completion result, unlabelled tags with higher reconstruction values are selected. [sent-98, score-0.683]
</p><p>36 The sparsity constraints are attributed to the observation that generally an image contains a few objects and a tag connotes a few levels of meaning, and usually corresponding objects or levels of meaning are redundantly contained or implied in the context. [sent-100, score-0.669]
</p><p>37 In image-specific reconstruction, both low-level image features and high-level tagging row vectors are considered and integrated into a unified objective function. [sent-101, score-0.307]
</p><p>38 In tag-specific reconstruction, we perform linear sparse reconstruction for each column vector of the tagging matrix with remaining ones. [sent-102, score-0.385]
</p><p>39 It should be noticed that the correlations of tagging vectors utilized in image-specific and tag-specific reconstructions are totally different. [sent-103, score-0.356]
</p><p>40 Hence imagespecific reconstruction mainly utilizes the visual similarity and semantic similarity between images, while tag-specific reconstruction mines the concurrence between tags. [sent-105, score-0.33]
</p><p>41 With both separate linear sparse reconstructions, LSR further normalizes and merges their respective tag completion results. [sent-106, score-0.966]
</p><p>42 Here we adopt a weighted linear combination strategy as follows, which is well demonstrated by experi111666111977  from both perspectives of image (upper dotted square) and tag (lower dotted square), and then normalizes and merges corresponding results. [sent-107, score-0.716]
</p><p>43 Ω = 훿푇  + (1 − 훿) 푅  (1)  푅  where Ω is the expected final result, 푇 and are respectively the normalized completion results from imagespecific and tag-specific reconstructions, and 훿 is a weighting parameter in (0, 1). [sent-109, score-0.34]
</p><p>44 Then based on Ω, corresponding unlabelled tags with higher reconstruction values are added to each incompletely annotated image. [sent-110, score-0.375]
</p><p>45 Image-Specific Reconstruction Given the incomplete initial tagging matrix 퐷푚×푛, where 푚 and 푛 respectively denote the number of images and tags, image-specific linear sparse reconstruction is to perform tag completion from the perspective of row. [sent-113, score-1.326]
</p><p>46 As mentioned previously, both low-level features and highlevel tagging vectors are considered. [sent-114, score-0.268]
</p><p>47 highlevel tagging vectors, we introduce a group sparse structure for the reconstruction weights as [15], which is attributed to the observation that images associated with an identical tag probably share more common semantic content and thus form a group. [sent-127, score-1.087]
</p><p>48 Here we denote the 푖th group of reconstruction weights as 푔푖 = {훽휅(푖,1) , 훽휅(푖,2) , ⋅ ⋅ ⋅ , 훽휅(푖,∣푔푖 ∣) } , where 휅 (푖, 푗) is the index= o {f훽 훽the 푗th weight o⋅⋅f⋅ ⋅t,h훽e 푖th group winh tehree weighting vector Note that each tag corresponds to a group of reconstruction weights, i. [sent-128, score-0.929]
</p><p>49 It is reasonable since only a few tags are associated with the target image (i. [sent-135, score-0.224]
</p><p>50 inter-group sparsity), and images in the same group are all supposed to contribute to the reconstruction if the corresponding tag would be associated (i. [sent-137, score-0.778]
</p><p>51 labelled tags) of the initial tagging vector, since they are ensured while the zero ones (i. [sent-143, score-0.326]
</p><p>52 low-level features and high-level tagging vectors, and 휈 is a tuning factor for penalizing the difference between 훼 and 훽. [sent-150, score-0.296]
</p><p>53 low-level features and high-level tagging vectors are supposed to be consistent (i. [sent-155, score-0.29]
</p><p>54 Then the optimal 훼 and 훽 can be merged for obtaining a reconstructed tagging vector 푡′ for the target image, as shown in formula (5). [sent-160, score-0.373]
</p><p>55 By performing linear sparse reconstructions for all to-becompleted images, an image-specific reconstructed tagging matrix 푇푚×푛 consisting of all 푡′푇 can be output for further normalization and integration. [sent-162, score-0.391]
</p><p>56 Tag-Specific Reconstruction Tag-specific linear sparse reconstruction is to perform tag completion for the incomplete initial tagging matrix 퐷푚×푛 from the perspective of column. [sent-166, score-1.326]
</p><p>57 Here we denote the tagging column vector in 퐷 of a to-be-completed tag as 휏푚× 1, and the dictionary matrix consisting of other tagging column vectors as 푅ˆ푚× (푛−1) . [sent-167, score-1.226]
</p><p>58 ×  Ψ = 푚푖푛훾  ∥푊′  (휏 −푅ˆ훾)  ∥22 + 휉∥훾∥1  (6)  where 훾(푛−1) 1 is the objective weˆighting vector with each element representing the weight of corresponding tag in the reconstruction, and 휉 is a tuning factor for penalizing the non-sparsity of 훾. [sent-169, score-0.72]
</p><p>59 Additionally, 푊′ is a diagonal weighting matrix for the reconstruction residuals of all entries in 휏, which is defined in the same way as 푊 in formula (3). [sent-170, score-0.224]
</p><p>60 The tag-specific objective function can also be demonstrated to be convex and thus there exists a global optimal 훾, which can then be utilized to obtain a reconstructed tagging column vector 푟′ = 푅ˆ훾 for the target tag. [sent-171, score-0.326]
</p><p>61 With all tags reconstructed, a tag-specific reconstructed tagging matrix 푅푚×푛 consisting of all 푟′ can be output for further normalization, which in our experiments is to normalize the maximal value as 1. [sent-172, score-0.512]
</p><p>62 Then the normalized imagespecific and tag-specific tagging matrix, i. [sent-173, score-0.296]
</p><p>63 Instead of performing global optimization for the tagging matrix as most previous work, LSR performs tag completion for each image and tag separately, enabling itself to complete both existing datasets (i. [sent-196, score-1.84]
</p><p>64 푡 퐹,  111666112199  Corel5kFlickr30Concepts  tags are given in the format “mean / maximum”. [sent-209, score-0.224]
</p><p>65 With accurate manual annotations, the labelled tags of each image in Corel5k are relatively complete and contain little noise, making it an ideal evaluation benchmark for tag completion. [sent-215, score-0.952]
</p><p>66 To perform tag completion, we randomly delete 40% of the associated tags for all images in both datasets, ensuring that each image has at least one tag deleted and finally contains at least one tag. [sent-219, score-1.55]
</p><p>67 Therefore, we strike out images originally associated with only one tag and finally obtain two refinement datasets with statistics shown in table 1. [sent-220, score-0.708]
</p><p>68 Due to the high cost of manual judgements for tag completion results, we take the originally labelled tags of each image as ground truth for both datasets, and measure the completion results regarding the missing ground-truth tags (i. [sent-224, score-1.758]
</p><p>69 The experimental results of tag completion are 1The 30 non-abstract concepts are: aircraft, ball, beach, bike, bird, book, bridge, car, chair, child, clock, countryside, dog, door, fire, fish, flower, house, kite, lamp, mountain, mushroom, pen, rabbit, river, sky, sun, tower, train, tree. [sent-232, score-0.908]
</p><p>70 Parameter Settings Before applying LSR on both datasets, we use the validate set of Corel5k for tuning parameters and analysing their corresponding influences on tag completion result. [sent-248, score-0.994]
</p><p>71 5 for equally weighting the roles of feature vectors and tagging vectors. [sent-251, score-0.298]
</p><p>72 Moreover, in sub-figure [푎4] the tag com2The features include: Color Correlogram, Color Layout, CEDD, Edge Histogram, FCTH, JCD, Jpeg Coefficient Histogram, RGB Color Histogram, Scalable Color, SURF with Bag-of-Words model. [sent-270, score-0.648]
</p><p>73 Moreover, it can be concluded that the optimal merged completion result outperforms that of mere image-specific (i. [sent-312, score-0.353]
</p><p>74 훿 = 0) reconstruction, which demonstrates the effectiveness of performing tag completion from both perspectives. [sent-316, score-0.922]
</p><p>75 Furthermore, we investigate how the tag completion performance varies with the iterations in the solution processes for optimizing image-specific and tag-specific objective functions, as illustrated in Fig. [sent-317, score-0.93]
</p><p>76 For ease of presentation, we show the merged completion result. [sent-319, score-0.308]
</p><p>77 It can be seen that with the iterations increasing, the tag completion performance achieves continuous improvement and tends to be stable after about 400 iterations, which is adopted as the upper bound for iterations in following experiments. [sent-320, score-0.908]
</p><p>78 JEC [10] and TagProp [3]), tag recommendation approaches (i. [sent-328, score-0.724]
</p><p>79 Vote+ [11] and Folksonomy [5]) and recently proposed unified tag refinement frameworks of denoising and completion (i. [sent-330, score-1.04]
</p><p>80 LSR img feat and LSR img tvec separately represent two variants that perform tag completion utilizing only low-level features or high-level tagging vectors for image-specific reconstruction, i. [sent-338, score-1.483]
</p><p>81 LSR img and LSR tag are variants that respectively utilize mere image-specific or tagspecific linear sparse reconstruction, i. [sent-341, score-0.879]
</p><p>82 formula (4) and (6), of which the former is the integration of LSR img feat  and LSR img tvec. [sent-343, score-0.324]
</p><p>83 LSR is the proposed integrated scheme merging LSR img and LSR tag (i. [sent-344, score-0.781]
</p><p>84 Note that all the completion results are measured on the same test sets. [sent-347, score-0.26]
</p><p>85 Tag completion results on Corel5k in terms of 퐴푃@푁, 퐴푅@푁 and 퐶@푁 with  푁 in {1, 2, 3}. [sent-355, score-0.26]
</p><p>86 Among  TagProp are image laeuttioon-an rnesoutaltstio onn m Coetrhelo5dks, Vno teterm+ asn odf Folksonomy are tag dre c퐶o@m푁me wndiathtio 푁n  the baselines, JEC and  approaches, wAhmiolen LgR th aen bda SseUlinGe are EuCni faiendd  tag refinement frameworks of denoising and completion. [sent-356, score-1.411]
</p><p>87 1) Most variants of LSR consistently outperform the image auto-annotation, tag recommendation and tag refinement baselines, providing a demonstration for their effectiveness. [sent-361, score-1.454]
</p><p>88 2) LSR img outperforms both LSR img feat and LSR img tvec, which validates the necessity of considering both low-level features and high-level tagging vectors in image-specific linear sparse reconstruction, and using their semantic consistency for performance enhancement. [sent-362, score-0.685]
</p><p>89 3) LSR outperforms both LSR img and LSR tag, which provides further evidence for the effectiveness of performing tag completion from both perspectives of image and  tag. [sent-363, score-1.063]
</p><p>90 5) Initially labelled tags are important for tag completion, as validated by that LSR img feat achieves the worst performance while other tag-dependent variants perform much better. [sent-365, score-1.118]
</p><p>91 Regarding experiments on the much larger real-world Flickr30Concepts, we take 퐴푃@4, 퐴푅@4 and 퐶@4 as performance measurements, since the mean number of missing tags for each image in Flickr30Concepts is 3. [sent-366, score-0.247]
</p><p>92 Note that we cannot obtain the completion results of SUG on Flickr30Concepts due to its high computational cost to calculate the eigenvalues of the normalized Laplacian matrix of a large hypergraph. [sent-368, score-0.284]
</p><p>93 To compare with the most recently published tag completion methods (i. [sent-371, score-0.922]
</p><p>94 Yet boosted by tag-specific reconstruction, the merged completion results (i. [sent-421, score-0.308]
</p><p>95 LSR and 휋LSR) are still acceptable, which verifies the robustness of LSR and the necessity of performing tag completion from both image and tag perspectives. [sent-423, score-1.585]
</p><p>96 Specifically, we recover all the missing tags in training sets of both datasets. [sent-427, score-0.247]
</p><p>97 Conclusions In this paper we propose an effective scheme denoted as LSR for automatic image tag completion, using imagespecific and tag-specific linear sparse reconstructions. [sent-431, score-0.725]
</p><p>98 And it achieves the state-of-the-art performance in extensive experiments conducted on both benchmark dataset and web images for tag completion. [sent-433, score-0.648]
</p><p>99 Image tag refinement along the ‘what’ dimension using tag categorization and neighbor voting. [sent-461, score-1.356]
</p><p>100 Image tag refinement towards low-rank, content-tag prior and error sparsity. [sent-549, score-0.708]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tag', 0.648), ('lsr', 0.52), ('completion', 0.26), ('tagging', 0.246), ('tags', 0.224), ('img', 0.104), ('reconstruction', 0.088), ('labelled', 0.08), ('recommendation', 0.076), ('unlabelled', 0.063), ('reconstructions', 0.062), ('dlc', 0.061), ('formula', 0.061), ('refinement', 0.06), ('imagespecific', 0.05), ('concurrence', 0.049), ('jec', 0.049), ('merged', 0.048), ('tagprop', 0.047), ('dictionary', 0.04), ('feat', 0.04), ('regarding', 0.039), ('perspectives', 0.037), ('tmc', 0.037), ('incomplete', 0.033), ('inductive', 0.032), ('denoising', 0.032), ('tuning', 0.031), ('deleted', 0.03), ('mere', 0.03), ('weighting', 0.03), ('merging', 0.029), ('transductive', 0.028), ('sparse', 0.027), ('baselines', 0.027), ('utilized', 0.026), ('autoannotation', 0.024), ('cep', 0.024), ('fcth', 0.024), ('folksonomy', 0.024), ('incompleteness', 0.024), ('jcd', 0.024), ('lick', 0.024), ('neve', 0.024), ('rnsson', 0.024), ('sigurbj', 0.024), ('sug', 0.024), ('tagspecific', 0.024), ('userprovided', 0.024), ('utilize', 0.024), ('matrix', 0.024), ('missing', 0.023), ('frameworks', 0.023), ('semantic', 0.023), ('objective', 0.022), ('vectors', 0.022), ('neighbours', 0.022), ('supposed', 0.022), ('variants', 0.022), ('completely', 0.021), ('sparsity', 0.021), ('entries', 0.021), ('validate', 0.02), ('group', 0.02), ('probably', 0.02), ('penalizing', 0.019), ('makadia', 0.019), ('tvec', 0.019), ('reconstructed', 0.018), ('facilitating', 0.018), ('mines', 0.018), ('neighbour', 0.018), ('degradations', 0.018), ('knn', 0.018), ('influences', 0.018), ('separately', 0.018), ('tsinghua', 0.017), ('unified', 0.017), ('researches', 0.017), ('analysing', 0.017), ('beijing', 0.016), ('merges', 0.016), ('relevance', 0.016), ('contextual', 0.016), ('though', 0.016), ('former', 0.015), ('liu', 0.015), ('measurements', 0.015), ('weights', 0.015), ('formulated', 0.015), ('concluded', 0.015), ('optimized', 0.015), ('necessity', 0.015), ('normalizes', 0.015), ('convex', 0.014), ('performing', 0.014), ('guillaumin', 0.014), ('ongoing', 0.014), ('utilizes', 0.014), ('published', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="213-tfidf-1" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>2 0.51325178 <a title="213-tfidf-2" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>3 0.48007959 <a title="213-tfidf-3" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>4 0.10760769 <a title="213-tfidf-4" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>5 0.071215838 <a title="213-tfidf-5" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>6 0.064093143 <a title="213-tfidf-6" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>7 0.053941537 <a title="213-tfidf-7" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>8 0.04926585 <a title="213-tfidf-8" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>9 0.047415428 <a title="213-tfidf-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.041152202 <a title="213-tfidf-10" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>11 0.040978279 <a title="213-tfidf-11" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>12 0.040473543 <a title="213-tfidf-12" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>13 0.040224425 <a title="213-tfidf-13" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>14 0.039801806 <a title="213-tfidf-14" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>15 0.039562706 <a title="213-tfidf-15" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>16 0.039309297 <a title="213-tfidf-16" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>17 0.037066314 <a title="213-tfidf-17" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>18 0.037057623 <a title="213-tfidf-18" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>19 0.035991333 <a title="213-tfidf-19" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>20 0.035892226 <a title="213-tfidf-20" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.088), (1, -0.034), (2, -0.056), (3, 0.092), (4, 0.009), (5, -0.03), (6, 0.0), (7, 0.051), (8, -0.043), (9, 0.036), (10, 0.019), (11, 0.015), (12, -0.013), (13, 0.032), (14, 0.015), (15, -0.035), (16, 0.024), (17, 0.04), (18, -0.07), (19, -0.047), (20, 0.052), (21, 0.036), (22, -0.025), (23, 0.086), (24, -0.012), (25, -0.003), (26, 0.286), (27, 0.099), (28, 0.647), (29, -0.2), (30, -0.087), (31, -0.006), (32, 0.156), (33, 0.032), (34, -0.095), (35, 0.037), (36, -0.016), (37, 0.001), (38, 0.05), (39, 0.02), (40, 0.045), (41, 0.003), (42, 0.057), (43, -0.079), (44, -0.05), (45, -0.031), (46, 0.034), (47, -0.037), (48, -0.026), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96825898 <a title="213-lsi-1" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>2 0.92471272 <a title="213-lsi-2" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>3 0.70626956 <a title="213-lsi-3" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>4 0.19254327 <a title="213-lsi-4" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>5 0.14411902 <a title="213-lsi-5" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>Author: Kevin Tang, Rahul Sukthankar, Jay Yagnik, Li Fei-Fei</p><p>Abstract: The ubiquitous availability of Internet video offers the vision community the exciting opportunity to directly learn localized visual concepts from real-world imagery. Unfortunately, most such attempts are doomed because traditional approaches are ill-suited, both in terms of their computational characteristics and their inability to robustly contend with the label noise that plagues uncurated Internet content. We present CRANE, a weakly supervised algorithm that is specifically designed to learn under such conditions. First, we exploit the asymmetric availability of real-world training data, where small numbers of positive videos tagged with the concept are supplemented with large quantities of unreliable negative data. Second, we ensure that CRANE is robust to label noise, both in terms of tagged videos that fail to contain the concept as well as occasional negative videos that do. Finally, CRANE is highly parallelizable, making it practical to deploy at large scale without sacrificing the quality of the learned solution. Although CRANE is general, this paper focuses on segment annotation, where we show state-of-the-art pixel-level segmentation results on two datasets, one of which includes a training set of spatiotemporal segments from more than 20,000 videos.</p><p>6 0.14160873 <a title="213-lsi-6" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>7 0.13368879 <a title="213-lsi-7" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>8 0.13294189 <a title="213-lsi-8" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>9 0.12964197 <a title="213-lsi-9" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>10 0.12847763 <a title="213-lsi-10" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>11 0.1257769 <a title="213-lsi-11" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>12 0.12567408 <a title="213-lsi-12" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>13 0.12012398 <a title="213-lsi-13" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>14 0.11872046 <a title="213-lsi-14" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>15 0.11856036 <a title="213-lsi-15" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>16 0.11476783 <a title="213-lsi-16" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>17 0.11325996 <a title="213-lsi-17" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>18 0.11318635 <a title="213-lsi-18" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>19 0.11295117 <a title="213-lsi-19" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>20 0.11261456 <a title="213-lsi-20" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.099), (16, 0.024), (26, 0.047), (33, 0.169), (46, 0.307), (67, 0.053), (69, 0.056), (80, 0.017), (87, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70395023 <a title="213-lda-1" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>2 0.61237925 <a title="213-lda-2" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>Author: Yingying Zhu, Nandita M. Nayak, Amit K. Roy-Chowdhury</p><p>Abstract: In thispaper, rather than modeling activities in videos individually, we propose a hierarchical framework that jointly models and recognizes related activities using motion and various context features. This is motivated from the observations that the activities related in space and time rarely occur independently and can serve as the context for each other. Given a video, action segments are automatically detected using motion segmentation based on a nonlinear dynamical model. We aim to merge these segments into activities of interest and generate optimum labels for the activities. Towards this goal, we utilize a structural model in a max-margin framework that jointly models the underlying activities which are related in space and time. The model explicitly learns the duration, motion and context patterns for each activity class, as well as the spatio-temporal relationships for groups of them. The learned model is then used to optimally label the activities in the testing videos using a greedy search method. We show promising results on the VIRAT Ground Dataset demonstrating the benefit of joint modeling and recognizing activities in a wide-area scene.</p><p>3 0.58473837 <a title="213-lda-3" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>4 0.58453727 <a title="213-lda-4" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>5 0.57795864 <a title="213-lda-5" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>Author: Bo Zheng, Yibiao Zhao, Joey C. Yu, Katsushi Ikeuchi, Song-Chun Zhu</p><p>Abstract: In this paper, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.</p><p>6 0.57730186 <a title="213-lda-6" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>7 0.57722598 <a title="213-lda-7" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>8 0.57676506 <a title="213-lda-8" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>9 0.57540715 <a title="213-lda-9" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>10 0.57449651 <a title="213-lda-10" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>11 0.57412642 <a title="213-lda-11" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>12 0.57354766 <a title="213-lda-12" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>13 0.57300538 <a title="213-lda-13" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>14 0.57292742 <a title="213-lda-14" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>15 0.57175344 <a title="213-lda-15" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>16 0.57131386 <a title="213-lda-16" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>17 0.57119358 <a title="213-lda-17" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>18 0.57081974 <a title="213-lda-18" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>19 0.57071859 <a title="213-lda-19" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>20 0.5705744 <a title="213-lda-20" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
