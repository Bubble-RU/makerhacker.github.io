<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-198" href="#">cvpr2013-198</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</h1>
<br/><p>Source: <a title="cvpr-2013-198-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhong_Handling_Noise_in_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>Reference: <a title="cvpr-2013-198-reference" href="../cvpr2013_reference/cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. [sent-2, score-0.656]
</p><p>2 However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. [sent-4, score-0.902]
</p><p>3 We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. [sent-5, score-0.608]
</p><p>4 Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. [sent-6, score-1.04]
</p><p>5 Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. [sent-7, score-1.174]
</p><p>6 Finally, we reconstruct the blur kernel using inverse Radon transform. [sent-8, score-0.628]
</p><p>7 Experimental results on synthetic and real data show that our algorithm achieves higher quality results than  previous approaches on blurry and noisy images. [sent-9, score-0.327]
</p><p>8 In this work, we do not make this assumption and aim to restore a sharp image from a blurry and noisy input. [sent-19, score-0.315]
</p><p>9 2Adobe  Research  Many single image blind deconvolution methods have been recently proposed [4, 6, 8–10, 13, 14, 16, 20]. [sent-21, score-0.372]
</p><p>10 Although they generally work well when the input image is noise-free, their performance degrades rapidly when the noise level increases. [sent-22, score-0.295]
</p><p>11 Specifically, the blur kernel estimation step in previous deblurring approaches is often too fragile to reliably  estimate the blur kernel when the image is contaminated with noise, as shown in Fig. [sent-23, score-1.461]
</p><p>12 Even assuming that an accurate blur kernel can be estimated, the amplified image noise and ringing artifacts generated from the non-blind deconvolution also significantly degrade the results [5, 11, 21, 22]. [sent-25, score-1.255]
</p><p>13 To handle noisy inputs in single image deblurring, Tai and Lin [17] first apply an existing denoising package [1] as preprocessing, and then estimate the blur kernel and the latent image from the denoised result. [sent-26, score-1.092]
</p><p>14 However, applying existing denoising methods is likely to damage, at least partially, the detailed blur information that one can extract from the input image, thereby leading to a biased kernel estimation. [sent-28, score-0.902]
</p><p>15 2, we illustrate that standard denoising methods, from bilateral filtering to more advanced approaches such as Non-Local Means [3] and BM3D [7], have negative impacts on the accuracy of kernel estimation. [sent-30, score-0.59]
</p><p>16 In this paper, we propose a new approach for estimating an accurate blur kernel from a noisy blurry image. [sent-31, score-0.878]
</p><p>17 However, we carefully design the denoising filters and deblurring procedures in such a way that the estimated kernel is not affected by the denoising filters. [sent-33, score-1.036]
</p><p>18 Our approach is derived from the key observation that if a directional low-pass linear filter is applied to the input  image, it can reduce the noise level greatly, while the frequency content, including essential blur information, along the orthogonal direction is not affected. [sent-35, score-1.048]
</p><p>19 We use this property to estimate 1D projections of the desired blur kernel to the orthogonal directions of these filters. [sent-36, score-0.708]
</p><p>20 These projections, also known as the Radon transform, will not be affected by applying directional low-pass filters to the input image, except for the noise reduction. [sent-37, score-0.617]
</p><p>21 (a) Synthetic input image with 5% noise and the ground truth kernel (overlayed). [sent-40, score-0.47]
</p><p>22 we apply a series of directional low-pass filters at different orientations, and estimate a slice of kernel projection from each image. [sent-46, score-0.684]
</p><p>23 Finally, we reconstruct the blur kernel using the inverse Radon transform. [sent-48, score-0.628]
</p><p>24 Once a good kernel is obtained, we incorporate denoising filtering into the final deconvolution process to suppress noise and obtain a high-quality latent image. [sent-49, score-1.093]
</p><p>25 Side effects of denoising as preprocessing  Before introducing our approach, we first analyze the negative impact of employing denoising as preprocessing on kernel estimation. [sent-52, score-0.781]
</p><p>26 In single image deblurring, a blurry and noisy input image b is usually modeled as: b  =  ? [sent-53, score-0.302]
</p><p>27 , k and n represent the latent sharp image, blur kernel, and additive noise, respectively, ∗ is the convolution operator. [sent-55, score-0.504]
</p><p>28 Thus, the noise n in the input image will be amplified at most by the condition number κ(LTL) for kernel estimation, where LTL is often called the deconvolution matrix and has a block-circulant-with-circulant-block (BCCB) structure [12]. [sent-87, score-0.774]
</p><p>29 4 shows that the upper bound on the error in the estimated kernel is proportional to the amplitude of the noise in input image. [sent-89, score-0.548]
</p><p>30 Building on this result, one can attempt to apply sophisticated denoising filter to the blurry image to reduce the noise amplitude, hoping that this will improve the kernel estimate. [sent-90, score-0.971]
</p><p>31 However, denoising filters also alter the profile of edges, e. [sent-91, score-0.364]
</p><p>32 This information is critical to accurate kernel estimation, and as we shall see, the benefits of the noise reduction are often outweighed by the artifacts caused by the profile alteration. [sent-94, score-0.529]
</p><p>33 However, the kernel estimation then becomes: kg  = argminkg ? [sent-97, score-0.418]
</p><p>34 = k ∗ Gg, (5) where k is the blur kernel for the original input image and  2  2  kg is the optimal solution after Gaussian denoising. [sent-107, score-0.733]
</p><p>35 5 shows that the estimated kernel kg is a blurred version of the actual kernel k. [sent-109, score-0.616]
</p><p>36 This result comes from the initial noise reduction and is independent of the kernel estimation method. [sent-111, score-0.465]
</p><p>37 666111113  (a) Input  (b) True kernel  (c) No denoising  (d) Gaussian filter  (e) Bilateral filter  (f) Non-local means  (g) BM3D  Figure 2. [sent-112, score-0.585]
</p><p>38 (c) the blur kernel estimated without applying any denoising method to the input image (a). [sent-116, score-0.954]
</p><p>39 (e)-(g) the estimated blur kernels after applying different denoising filters. [sent-117, score-0.755]
</p><p>40 Although more sophisticated denoising methods are better at preserving high frequencies, denoising remains an open problem for which no perfect solution exists. [sent-119, score-0.493]
</p><p>41 Since no information about the blur kernel can be observed in uniform regions of the blurry image, edges are the main source of information that drives deblurring algorithms either implicitly or explicitly, e. [sent-120, score-0.977]
</p><p>42 Even small degradations introduced by state-of-the-art denoising techniques can have a strong impact on deblurring results as shown in Fig. [sent-123, score-0.458]
</p><p>43 05 when the intensity range is [0, 1] , and then use Cho and Lee’s method [4] to estimate the blur kernel. [sent-128, score-0.422]
</p><p>44 The recent approach of Tai and Lin [17] first applies  an existing commercial denoising package (NeatImage [1]) to the input image, then iteratively applies a motion-aware non-local mean filtering and deblurring to refine the results. [sent-130, score-0.626]
</p><p>45 Although special treatment has been added into the process, both the commercial denoising package and the non-local means filter have the same negative impacts on kernel estimation as we will show in Sec. [sent-131, score-0.651]
</p><p>46 Our experiments showed that even state-ofthe-art denoising filters do have negative impacts on kernel estimation. [sent-136, score-0.593]
</p><p>47 In this section, we resolve this problem by using directional blur and the Radon transform to estimate the kernel. [sent-137, score-0.726]
</p><p>48 Our approach reduces the noise without degrading blur information, thereby producing better kernels. [sent-138, score-0.591]
</p><p>49 Applying directional filters We now show that directional low-pass filters can be applied to an image without affecting its Radon transform, while decreasing its noise level. [sent-141, score-0.873]
</p><p>50 We consider the directional low-pass filter fθ:  I(p) ∗ fθ=c1? [sent-142, score-0.296]
</p><p>51 5, the kernel that we estimate from the filtered image bθ = b ∗ fθ is: kθ  =  k ∗ fθ . [sent-151, score-0.307]
</p><p>52 (7)  Similarly to filtering with a 2D Gaussian Gg, applying fθ averages pixels and reduces the noise level. [sent-152, score-0.304]
</p><p>53 Since fθ filters only along the direction θ, it has nearly no influence on the blur information in the orthogonal direction. [sent-153, score-0.576]
</p><p>54 We exploit this property to estimate the projection of the original kernel k along the direction θ. [sent-154, score-0.364]
</p><p>55 Thus, the projection of kernel kθ along  the projection direction θ is: Rθ? [sent-163, score-0.357]
</p><p>56 9 shows fθ has no impact on the Radon transform of the blur kernel to the orthogonal direction of the filter. [sent-176, score-0.773]
</p><p>57 The algorithm We now explain how we recover the sharp image, with the kernel estimation first, and then the deconvolution step. [sent-182, score-0.573]
</p><p>58 (a)(b)(c) are the synthetic image before adding noise, after adding noise, and after applying a directional filter(θ = 3π/4), respectively. [sent-185, score-0.328]
</p><p>59 (d)(e)(f) are the corresponding estimated blur kernels and their Radon transforms in the same direction. [sent-186, score-0.484]
</p><p>60 Note that the estimated kernel in (f) is largely damaged by the directional filter, but its Radon transform is the same as the one in (d). [sent-187, score-0.573]
</p><p>61 1  Noise-aware kernel estimation  Based on the above analysis, we apply a directional blur fθ, estimate the combined blur kernel kθ, and then project it along the same direction of the filter to get the corresponding Radon transform. [sent-190, score-1.651]
</p><p>62 Finally, we compute the 2D kernel using the inverse Radon transform [18]. [sent-192, score-0.326]
</p><p>63 The advantage of this strategy is that it greatly reduces noise when applying fθ, while keeping the computed Radon transform intact. [sent-193, score-0.324]
</p><p>64 This is not the case in practice,  and even with state-of-the-art kernel estimation techniques, recovering kθ from bθ, which is a blurry image convolved with an additional directional blur, has proven to be challenging. [sent-196, score-0.696]
</p><p>65 For a more reliable kernel estimation, we adopt the multiscale blind deconvolution framework commonly used in previous approaches [4, 20]. [sent-198, score-0.614]
</p><p>66 Since noise is largely removed by image downsizing, we apply an existing approach by Cho and Lee [4] to estimate the blur kernels ki and latent images ? [sent-203, score-0.798]
</p><p>67 Only for the full resolution layer b0, we apply the directional filter fθ and then estimate the Algorithm 1 Multiscale noise-aware blind deconvolution Input: The pyramid {b0, b1, . [sent-205, score-0.742]
</p><p>68 , bn} by down-sampling the input blurry apnydra noisy image b, whe}re b yb0 d o=w bn. [sent-208, score-0.302]
</p><p>69 3: repeat 4: Apply Nf directional filters to the input image b0, each filter has a direction of i· π/Nf, i= 1, . [sent-219, score-0.474]
</p><p>70 0 based on the new k0 using a noise-aware nonblind deconvolution approach. [sent-232, score-0.382]
</p><p>71 10: With the final estimated kernel k0, use the final deconvolution method described in Sec. [sent-234, score-0.585]
</p><p>72 kernel using the robust deconvolution technique described later in this section. [sent-239, score-0.489]
</p><p>73 We apply directional filters in different orientations to the input image. [sent-273, score-0.423]
</p><p>74 From each filtered image a corresponding kernel is computed first, then projected along the same direction to generate the correct radon transform of the true kernel. [sent-274, score-0.805]
</p><p>75 The final blur kernel k0 is reconstructed using inverse Radon transform [6]. [sent-275, score-0.724]
</p><p>76 1 contains much less noise due to image downsizing, incorporating this term can effectively reduce the noise level in ? [sent-279, score-0.452]
</p><p>77 This non-blind deconvolution step is an intermediate step in blur kernel estimation that produces sufficiently accurate images at a limited computational cost. [sent-281, score-0.919]
</p><p>78 In the next section, we describe a more sophisticated non-blind deconvolution algorithm for generating high-quality final latent image given the estimated kernel. [sent-282, score-0.444]
</p><p>79 To deal with severe noise, we will only use previous methods to estimate blur kernels from bn to b2 in Step 1 of the algorithm, and then apply noise-aware kernel estimation from Step 2 to 9 to the last two layers b1 and b0. [sent-285, score-0.833]
</p><p>80 We applied this modified version of the algorithm to examples with 10% noise (Gaussian noise with standard deviation of 0. [sent-286, score-0.43]
</p><p>81 [6] also use the Radon transform to recover the blur kernel. [sent-290, score-0.45]
</p><p>82 2  Final noise-aware nonblind deconvolution  Once an accurate k0 is estimated, we use it to estimate a good latent image ? [sent-298, score-0.526]
</p><p>83 Comparison results of our final noise-aware nonblind deconvolution with other recent nonblind deconvolution methods. [sent-303, score-0.786]
</p><p>84 This is in sharp contrast to Tai and Lin’s method [17] where denoising and kernel estimation interfere with each other. [sent-307, score-0.537]
</p><p>85 We apply directional filters along 36 regularly sampled orientations, that is, one sample every 5◦ . [sent-352, score-0.378]
</p><p>86 Three input blurry image examples with different levels of noise are shown in (a),(b),(c). [sent-368, score-0.441]
</p><p>87 (d) and (e) are the ground truth blur kernels from Levin et. [sent-369, score-0.432]
</p><p>88 (f-k) show the estimated kernels and the latent images of Tai and Lin’s method and our method with 5% noise and 10% noise. [sent-373, score-0.4]
</p><p>89 Synthetic data extent σf  We first conducted experiments on images that we convolved with a known blur kernel and to which we added noise in a controlled fashion. [sent-384, score-0.836]
</p><p>90 6), where the latent sharp images were blurred using two blur kernels provided by Levin et al. [sent-389, score-0.588]
</p><p>91 The comparison shows that visually our estimated blur kernels are closer to the ground truth, and our estimated latent images contain more details and less ringing artifacts. [sent-395, score-0.682]
</p><p>92 Comparisons with other methods We also conducted experiments to explore how noise affects the performance of other state-of-the-art single-image blind deconvolution methods. [sent-397, score-0.587]
</p><p>93 Using the “Aque” image and the blur kernel shown in Fig. [sent-398, score-0.593]
</p><p>94 We then applied different blind deconvolution methods to these test images, and measure the PSNR  666111557  Noise5%PSNR10%5%SSIM10%  TableAC1hq. [sent-400, score-0.372]
</p><p>95 The PSNR curves of various blind deconvolution algorithms, including Goldstein and Fattal [9], Cho and Lee [4], Cho et al. [sent-408, score-0.372]
</p><p>96 [15] and our method, on the 10 synthetic test images with noise level from 1% to 10%, generated by the “Aque” image and the kernel shown in Fig. [sent-410, score-0.517]
</p><p>97 In this paper, we propose a new single image blind deconvolution method that is more robust to noise than previous approaches. [sent-443, score-0.587]
</p><p>98 Our method uses directional filters to reduce the noise while keeping the blur information in their orthogonal direction intact. [sent-444, score-1.0]
</p><p>99 By applying a series of such directional filters, we showed how to recover correct 1D projections of the kernel in all directions, which we use to estimate an accurate blur kernel using the inverse Radon transform. [sent-445, score-1.232]
</p><p>100 Motion-aware noise filtering for deblurring of noisy and blurry images. [sent-581, score-0.729]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('radon', 0.408), ('blur', 0.376), ('deconvolution', 0.272), ('denoising', 0.236), ('directional', 0.23), ('tai', 0.221), ('kernel', 0.217), ('noise', 0.215), ('deblurring', 0.196), ('blurry', 0.188), ('aque', 0.132), ('cho', 0.127), ('nonblind', 0.11), ('ltl', 0.102), ('kg', 0.102), ('blind', 0.1), ('filters', 0.099), ('lin', 0.095), ('chalet', 0.088), ('gg', 0.08), ('levin', 0.078), ('latent', 0.077), ('noisy', 0.076), ('transform', 0.074), ('ringing', 0.069), ('nlm', 0.068), ('filter', 0.066), ('argminkg', 0.066), ('synthetic', 0.063), ('downsizing', 0.058), ('psnr', 0.058), ('kernels', 0.056), ('bn', 0.054), ('filtering', 0.054), ('estimated', 0.052), ('sharp', 0.051), ('goldstein', 0.049), ('estimate', 0.046), ('filtered', 0.044), ('inours', 0.044), ('linourstai', 0.044), ('linourstaii', 0.044), ('xpsf', 0.044), ('bilateral', 0.042), ('impacts', 0.041), ('direction', 0.041), ('orthogonal', 0.039), ('fattal', 0.039), ('argmink', 0.039), ('projection', 0.039), ('input', 0.038), ('shake', 0.037), ('package', 0.036), ('andd', 0.036), ('dimitris', 0.036), ('exposure', 0.035), ('inverse', 0.035), ('applying', 0.035), ('zoran', 0.034), ('ey', 0.034), ('estimation', 0.033), ('preprocessing', 0.033), ('nf', 0.033), ('amplified', 0.032), ('cos', 0.031), ('projections', 0.03), ('degrade', 0.03), ('ssim', 0.03), ('profile', 0.029), ('buades', 0.029), ('convolved', 0.028), ('apply', 0.028), ('blurred', 0.028), ('orientations', 0.028), ('lee', 0.028), ('ab', 0.027), ('amplitude', 0.026), ('handheld', 0.026), ('adobe', 0.026), ('weiss', 0.026), ('impact', 0.026), ('multiscale', 0.025), ('series', 0.025), ('comparisons', 0.025), ('psf', 0.025), ('joshi', 0.024), ('shall', 0.024), ('artifacts', 0.023), ('severe', 0.023), ('level', 0.022), ('applies', 0.022), ('final', 0.022), ('commercial', 0.022), ('sophisticated', 0.021), ('along', 0.021), ('handling', 0.021), ('photos', 0.021), ('durand', 0.021), ('accurate', 0.021), ('rapidly', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="198-tfidf-1" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>2 0.51523107 <a title="198-tfidf-2" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>3 0.42629659 <a title="198-tfidf-3" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>4 0.40353554 <a title="198-tfidf-4" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>5 0.36939919 <a title="198-tfidf-5" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>Author: Li Xu, Shicheng Zheng, Jiaya Jia</p><p>Abstract: We show in this paper that the success of previous maximum a posterior (MAP) based blur removal methods partly stems from their respective intermediate steps, which implicitly or explicitly create an unnatural representation containing salient image structures. We propose a generalized and mathematically sound L0 sparse expression, together with a new effective method, for motion deblurring. Our system does not require extra filtering during optimization and demonstrates fast energy decreasing, making a small number of iterations enough for convergence. It also provides a unifiedframeworkfor both uniform andnon-uniform motion deblurring. We extensively validate our method and show comparison with other approaches with respect to convergence speed, running time, and result quality.</p><p>6 0.36912107 <a title="198-tfidf-6" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>7 0.2942872 <a title="198-tfidf-7" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>8 0.28734183 <a title="198-tfidf-8" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>9 0.28130028 <a title="198-tfidf-9" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>10 0.21830252 <a title="198-tfidf-10" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>11 0.16149221 <a title="198-tfidf-11" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>12 0.15863661 <a title="198-tfidf-12" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>13 0.15156546 <a title="198-tfidf-13" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>14 0.15106219 <a title="198-tfidf-14" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>15 0.12094612 <a title="198-tfidf-15" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>16 0.11565858 <a title="198-tfidf-16" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>17 0.10958868 <a title="198-tfidf-17" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>18 0.086961284 <a title="198-tfidf-18" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>19 0.085209675 <a title="198-tfidf-19" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>20 0.078866132 <a title="198-tfidf-20" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, 0.201), (2, -0.058), (3, 0.19), (4, -0.159), (5, 0.553), (6, 0.073), (7, -0.04), (8, 0.03), (9, -0.021), (10, -0.036), (11, -0.06), (12, -0.039), (13, -0.042), (14, -0.027), (15, -0.006), (16, 0.082), (17, -0.025), (18, 0.048), (19, 0.04), (20, 0.029), (21, 0.065), (22, 0.026), (23, 0.029), (24, -0.025), (25, -0.031), (26, 0.002), (27, -0.052), (28, 0.007), (29, -0.002), (30, -0.008), (31, -0.044), (32, 0.007), (33, -0.015), (34, -0.012), (35, 0.015), (36, 0.022), (37, -0.009), (38, 0.008), (39, 0.004), (40, -0.003), (41, -0.022), (42, 0.019), (43, -0.026), (44, 0.009), (45, 0.048), (46, 0.008), (47, 0.016), (48, -0.002), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96733642 <a title="198-lsi-1" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>2 0.92520732 <a title="198-lsi-2" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>3 0.91653228 <a title="198-lsi-3" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>4 0.89447194 <a title="198-lsi-4" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>5 0.87584352 <a title="198-lsi-5" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>Author: Yi Zhang, Keigo Hirakawa</p><p>Abstract: We propose a notion of double discrete wavelet transform (DDWT) that is designed to sparsify the blurred image and the blur kernel simultaneously. DDWT greatly enhances our ability to analyze, detect, and process blur kernels and blurry images—the proposed framework handles both global and spatially varying blur kernels seamlessly, and unifies the treatment of blur caused by object motion, optical defocus, and camera shake. To illustrate the potential of DDWT in computer vision and image processing, we develop example applications in blur kernel estimation, deblurring, and near-blur-invariant image feature extraction.</p><p>6 0.85607398 <a title="198-lsi-6" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>7 0.84876883 <a title="198-lsi-7" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>8 0.78007817 <a title="198-lsi-8" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>9 0.75337094 <a title="198-lsi-9" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>10 0.70283997 <a title="198-lsi-10" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>11 0.5935424 <a title="198-lsi-11" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>12 0.49416271 <a title="198-lsi-12" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>13 0.41135153 <a title="198-lsi-13" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>14 0.39778578 <a title="198-lsi-14" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>15 0.39097977 <a title="198-lsi-15" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>16 0.35884029 <a title="198-lsi-16" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>17 0.35456812 <a title="198-lsi-17" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>18 0.34693685 <a title="198-lsi-18" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>19 0.33155921 <a title="198-lsi-19" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>20 0.31239182 <a title="198-lsi-20" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.341), (16, 0.024), (26, 0.07), (28, 0.012), (33, 0.201), (44, 0.164), (67, 0.024), (69, 0.017), (87, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8992095 <a title="198-lda-1" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>2 0.89275485 <a title="198-lda-2" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>3 0.89158797 <a title="198-lda-3" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>4 0.89127743 <a title="198-lda-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.88494498 <a title="198-lda-5" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>6 0.87339389 <a title="198-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>same-paper 7 0.86841065 <a title="198-lda-7" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>8 0.8653121 <a title="198-lda-8" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>9 0.86048555 <a title="198-lda-9" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>10 0.841672 <a title="198-lda-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.82243401 <a title="198-lda-11" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>12 0.80135989 <a title="198-lda-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.79602748 <a title="198-lda-13" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>14 0.78966707 <a title="198-lda-14" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>15 0.78541136 <a title="198-lda-15" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>16 0.77771419 <a title="198-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.77674502 <a title="198-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.77399671 <a title="198-lda-18" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>19 0.76709789 <a title="198-lda-19" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>20 0.76438665 <a title="198-lda-20" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
