<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-17" href="#">cvpr2013-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</h1>
<br/><p>Source: <a title="cvpr-2013-17-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Schuler_A_Machine_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>Reference: <a title="cvpr-2013-17-reference" href="../cvpr2013_reference/cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A machine learning  approach  for non-blind image deconvolution  Christian J. [sent-1, score-0.233]
</p><p>2 Abstract Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. [sent-10, score-0.388]
</p><p>3 Currently, the most successful meth-  ods involve a regularized inversion of the blur in Fourier domain as a first step. [sent-12, score-0.373]
</p><p>4 A blurry image y is given by y = x ∗ v + n, where x is the tAru bel underlying (non-blurry) image, v i+s nth,e w point spread function (PSF) describing the blur and n is noise, usually assumed to be additive, white and Gaussian (AWG) noise. [sent-24, score-0.4]
</p><p>5 The inversion of the blurring process is called image deconvolution and is ill-posed in the presence of noise. [sent-25, score-0.332]
</p><p>6 While most methods are well-engineered algorithms, we ask the question: Is it possible to automatically learn an image deconvolution procedure? [sent-30, score-0.233]
</p><p>7 Contributions: We present an image deconvolution procedure that is learned on a large dataset of natural images with a multi-layer perceptron (MLP). [sent-32, score-0.259]
</p><p>8 We compare our approach to other methods on a large dataset of synthetically blurred images, and obtain state-of-the-art results for all tested blur kernels. [sent-33, score-0.259]
</p><p>9 Related Work Image deconvolution methods can be broadly separated into two classes. [sent-38, score-0.233]
</p><p>10 , and EPLL seek a maximum a posteriori (MAP) estimate of the clean image x, given a blurry (and noisy) version y and the PSF v. [sent-45, score-0.197]
</p><p>11 The second category of methods apply a regularized inversion of the blur, followed by a denoising procedure. [sent-55, score-0.302]
</p><p>12 In Fourier domain, the inversion of the blur can be seen as a pointwise division by the blur kernel. [sent-56, score-0.574]
</p><p>13 Hence, these methods address deconvolution as a denoising problem. [sent-58, score-0.387]
</p><p>14 Unfortunately, most denoising methods are designed to remove AWG noise [23, 12, 9]. [sent-59, score-0.319]
</p><p>15 Deconvolution via denoising requires the denoising algorithm to be able to remove colored noise (non-flat power spectrum of the noise, not to be confused with color noise of RGB images). [sent-60, score-0.649]
</p><p>16 [14]) have been shown to achieve good deconvolution results. [sent-63, score-0.233]
</p><p>17 Some approaches to denoising rely on learning, where learning can involve learning a probabilistic model of natural images [24], or of smaller natural image patches [3 1]. [sent-65, score-0.232]
</p><p>18 In that case, denoising can be achieved using a maximum a pos-  teriori method. [sent-66, score-0.182]
</p><p>19 In [16], it is shown that convolutional neural networks can achieve good image denoising results for AWG noise. [sent-68, score-0.31]
</p><p>20 More recently, it was shown that a type ofneural network based on stacked denoising auto-encoders [29] can achieve good results in image denoising for AWG noise as well as for “blind” image inpainting (when the positions of the pixels to be inpainted are unknown) [30]. [sent-69, score-0.455]
</p><p>21 Also recently, plain neural networks achieved state-ofthe-art results in image denoising for AWG noise, provided the neural nets have enough capacity and that sufficient training data is provided [3, 4]. [sent-70, score-0.439]
</p><p>22 It was also shown that plain neural networks can achieve good results on other types of noise, such as noise resembling stripes, salt-and-pepper noise, JPEG-artifacts and mixed Poisson-Gaussian noise. [sent-71, score-0.315]
</p><p>23 Differences and similarities to our work: We address the deconvolution problem as a denoising problem and therefore take an approach that is in line with [10, 11, 14], but different from [18]. [sent-72, score-0.387]
</p><p>24 Method  The most direct way to deconvolve images with neural networks is to train them directly on blurry/clean patch pairs. [sent-78, score-0.214]
</p><p>25 Instead, our method relies on two steps: (i) a regularized inversion of the blur in Fourier domain and (ii) a denoising step using a neural network. [sent-80, score-0.607]
</p><p>26 Direct deconvolution The goal of this step is to make the blurry image sharper. [sent-84, score-0.388]
</p><p>27 In our model, the underlying true (sharp) image x is blurred with a PSF v and corrupted with AWG noise n with standard deviation σ: y = v ∗ x + n. [sent-86, score-0.236]
</p><p>28 asurement of the blur kernel is corrupted by AWG, a further term β? [sent-96, score-0.301]
</p><p>29 Illustration of the effect of the regularized blur inversion. [sent-116, score-0.274]
</p><p>30 The result z of the  regularized inversion is the sum of a corrupted image xcorrupted and colored noise ncolored. [sent-118, score-0.448]
</p><p>31 Other methods [10, 11, 14] attempt to remove ncolored but ignore the noise in xcorrupted, whereas our method learns to denoise z and therefore addresses both problems. [sent-119, score-0.225]
</p><p>32 Using the regularized inverse R, we can estimate the Fourier transform of the true image by the so-called direct deconvolution (following [15]) Z = R ? [sent-123, score-0.338]
</p><p>33 ve Hrseen cise given by the sum of the colored noise image R ? [sent-133, score-0.176]
</p><p>34 We therefore see that methods trying to remove the colored noise component R ? [sent-140, score-0.215]
</p><p>35 t W Re propose as step (aici)t a procedure that removes the colored noise and additional  ××  image artifacts. [sent-142, score-0.176]
</p><p>36 Artifact removal by MLPs A multi-layer perceptron (MLP) is a neural network that processes multivariate input via several hidden layers and outputs multivariate output. [sent-147, score-0.269]
</p><p>37 (392, 2047, 2047, 2047, 2047, 132) describes an MLP with four hidden layers (each having 2047 nodes) and patches of size 39 39 as input, and of size 13 13 as output. [sent-151, score-0.164]
</p><p>38 Training procedure: Our goal is to learn an MLP that maps corrupted input patches to clean output patches. [sent-156, score-0.238]
</p><p>39 2, the clean image x is blurred by the PSF v and additionally corrupted by noise n. [sent-160, score-0.278]
</p><p>40 In this case φ is equivalent to the linear blur model in Equation (1). [sent-161, score-0.225]
</p><p>41 We apply the direct deconvolution to φ(x) to obtain the image z = F−1(R ? [sent-164, score-0.289]
</p><p>42 The free parameters of the MLP are learned on such pairs of corrupted and clean image patches from z and x, using stochastic gradient descent [19]. [sent-168, score-0.196]
</p><p>43 The resulting image (showing characteristic artifacts) is then chopped into overlapping patches and each patch is processed separately by the trained MLP. [sent-174, score-0.165]
</p><p>44 If the regularization in the direct deconvolution is weak, strong artifacts are created, leading to bad results. [sent-186, score-0.354]
</p><p>45 Figure 4 shows that the results tend to improve with longer training times, but that the choice of the MLP’s architecture as well as of the regularization strength α during direct deconvolution is important. [sent-194, score-0.343]
</p><p>46 In our experiments, we use α = 20 for the direct deconvolution and (392 , 4 2047, 132) for the architecture. [sent-198, score-0.289]
</p><p>47 (d) Square blur (box blur) with size 19 19 and AWG noise Swqiutha σ b=l u0r. [sent-218, score-0.351]
</p><p>48 (e) Motion blur from [21] and AWG noise with σ = 0. [sent-220, score-0.351]
</p><p>49 Scenario (e) uses a motion blur recorded in [21], which is easier to deblur. [sent-227, score-0.225]
</p><p>50 Since only the methods using an image prior would be able to treat the boundary conditions correctly, we use circular convolution in all methods but exclude the borders of the images in the evaluation (we cropped by half the size of the blur kernel). [sent-241, score-0.225]
</p><p>51 In Figure 5 we see that in smooth areas, IDD-BM3D [11] and DEB-BM3D [10] produce artifacts resembling the PSF (square blur), whereas our method does 1 1 10 0 06 67 80 8  ]B  (a) Gaussian blur σ=1 . [sent-245, score-0.335]
</p><p>52 0  (d) Square blur 19x19  (e) Motion blur  d[  AWG noise σ=0. [sent-248, score-0.576]
</p><p>53 look “grainy” and the results achieved by EPLL [3 1] look more blurry than those achieved by our method. [sent-258, score-0.277]
</p><p>54 Table 1 summarizes the results achieved on 11 standard test images for denoising [9], downsampled to 128 128 pixels. [sent-301, score-0.182]
</p><p>55 The other methods rank differently depending on noise and blur strength. [sent-304, score-0.351]
</p><p>56 In the supplementary material we demonstrate that the MLP is optimal only for the noise level it was trained on, but still achieves good results if used at the wrong noise level. [sent-306, score-0.307]
</p><p>57 Poisson noise For scenario (c) we also consider Poisson noise with equivalent average variance. [sent-307, score-0.252]
</p><p>58 Poisson noise is approximately equivalent to additive Gaussian noise, where the variance of the noise depends on the intensity of the underlying pixel. [sent-308, score-0.279]
</p><p>59 Averaged over the 500 images in the Berkeley dataset, the results achieved with an MLP trained on this type of noise are slightly better (0. [sent-310, score-0.209]
</p><p>60 The fact that our results become somewhat better is consistent with the finding that equivalent Poisson noise is slightly easier to remove [22]. [sent-313, score-0.165]
</p><p>61 We note that even though the improvement is slight, this result shows that MLPs are able to automatically adapt to a new noise type, whereas methods that are not based on learning would ideally have to be engineered to cope with a new noise type (e. [sent-314, score-0.325]
</p><p>62 Qualitative results on a real photograph To test the performance of our method in a real-world setting, we remove defocus blur from a photograph. [sent-319, score-0.322]
</p><p>63 In order to make the defocus blur approximately  constant over the image plane, the lens is stopped down to f/5. [sent-322, score-0.311]
</p><p>64 The randomness in the size of the pillbox PSF expresses that we don’t know the exact blur and a pillbox is only an approximation. [sent-330, score-0.353]
</p><p>65 The variance of readout noise is independent of the expected illumination, but photon shot noise scales linearly with the mean, and pixel non-uniformity causes a quadratic increase in variance [1]. [sent-334, score-0.252]
</p><p>66 To generate the input to the MLP we pre-process each of the four channels generated by the Bayer pattern via direct deconvolution using a pillbox of the corresponding size at this resolution (radius 9. [sent-339, score-0.376]
</p><p>67 Following [5], we call weights connecting the input to the first hidden layer feature detectors and weights connecting the last layer to the output feature generators, both of which can be represented as patches. [sent-353, score-0.281]
</p><p>68 Finding an input pattern maximizing the activation of a specific hidden unit can be performed using activation maximization [13]. [sent-355, score-0.273]
</p><p>69 The first MLP is trained on patches tha,t4 are pre-processed with direct deconvolution, whereas the second MLP is trained on the blurry image patches themselves (i. [sent-357, score-0.505]
</p><p>70 Eight feature detectors of an MLP trained to remove a square blur. [sent-361, score-0.195]
</p><p>71 The MLP was trained on patches pre-processed with direct deconvolution. [sent-362, score-0.189]
</p><p>72 A potential explanation for this surprising observation is that these feature detectors focus on artifacts created by the regularized inversion of the blur. [sent-372, score-0.267]
</p><p>73 We perform the same analysis on the MLP trained on blurry patches, see Figure 7. [sent-373, score-0.21]
</p><p>74 The shape ofthe blur is evident 1 1 10 0 07 7 702 0  Figure 7. [sent-374, score-0.225]
</p><p>75 Eight feature detectors of an MLP trained to remove a square blur. [sent-375, score-0.195]
</p><p>76 The MLP was trained on the blurry patches themselves (i. [sent-376, score-0.288]
</p><p>77 In some feature detectors, the shape of the blur is not evident (the  three rightmost). [sent-381, score-0.253]
</p><p>78 We also observe that all features are large compared to the size of the output patch (the output patches are three times smaller than the input patches). [sent-382, score-0.171]
</p><p>79 This was not the case for the MLP trained with pre-processing (Figure 6) and is explained by the fact that in the blurry inputs, information is very spread out. [sent-383, score-0.23]
</p><p>80 We clearly see that the direct deconvolution has the effect of making the information more local. [sent-384, score-0.289]
</p><p>81 Analysis of the feature generators: We now analyze the feature generators learned by the MLPs. [sent-385, score-0.28]
</p><p>82 We will compare the feature generators to the input patterns maximizing the activation of their corresponding unit. [sent-386, score-0.367]
</p><p>83 feature generators (bottom row) in an MLP trained on pre-processed patches. [sent-390, score-0.307]
</p><p>84 Figure 8 shows eight feature generators (bottom row) along with their corresponding input features (top row) maximizing the activation of the same hidden unit. [sent-394, score-0.451]
</p><p>85 We repeat the analysis for the MLP trained on blurry patches (i. [sent-398, score-0.288]
</p><p>86 Figure 9 shows eight feature generators (middle row) along with their corresponding input features (top row). [sent-401, score-0.304]
</p><p>87 This time, the features found with activation maximization look different from their corresponding feature generators. [sent-402, score-0.164]
</p><p>88 However, the feature detectors look remarkably similar to the feature generators convolved with the PSF (bottom row). [sent-403, score-0.39]
</p><p>89 We interpret this observation as follows: If the MLP detects a blurry version of a certain feature in the input, it copies the (non-blurry) feature into the output. [sent-404, score-0.232]
</p><p>90 feature generators (middle row) in an MLP trained on blurry patches (i. [sent-407, score-0.54]
</p><p>91 The input patterns look like the feature generators convolved with the PSF (bottom row). [sent-410, score-0.339]
</p><p>92 This was possible by looking at the weights connecting the input to the first hidden layer and the weights connecting the last hidden layer to the output, as well as through the use of activation maximization [13]. [sent-414, score-0.318]
</p><p>93 We have seen that the MLP trained on blurry patches has to learn large feature detectors, because the information in the input is very spread-out. [sent-415, score-0.339]
</p><p>94 For both MLPs, the feature generators look similar: Many resemble Gabor filters or blobs. [sent-417, score-0.306]
</p><p>95 We were also able to answer the question: Which inputs cause the individual feature generators to activate? [sent-419, score-0.298]
</p><p>96 Roughly speaking, in the case of the MLP trained on pre-processed patches, the inputs have to look like the feature generators themselves, whereas in the case of the MLP trained on blurry patches, the inputs have to look like the feature generators convolved with the PSF. [sent-420, score-0.94]
</p><p>97 Finally, by directly learning the mapping from corrupted patches to clean patches, we handle both types of artifacts introduced by the direct deconvolution, instead of being limited to removing colored noise. [sent-427, score-0.347]
</p><p>98 A limitation of our approach is that each MLP has to be trained on only one blur kernel: Results achieved with MLPs trained on several blur kernels are inferior to those achieved with MLPs trained on a single blur kernel. [sent-430, score-0.896]
</p><p>99 However, in this case the deblurring quality is currently more limited by errors in the blur estimation than in the non-blind deconvolution step. [sent-432, score-0.485]
</p><p>100 Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. [sent-669, score-0.352]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mlp', 0.671), ('awg', 0.256), ('deconvolution', 0.233), ('blur', 0.225), ('generators', 0.224), ('mlps', 0.224), ('blurry', 0.155), ('denoising', 0.154), ('psf', 0.152), ('noise', 0.126), ('inversion', 0.099), ('neural', 0.08), ('patches', 0.078), ('corrupted', 0.076), ('activation', 0.072), ('epll', 0.071), ('pillbox', 0.064), ('krishnan', 0.061), ('fourier', 0.061), ('levin', 0.057), ('direct', 0.056), ('hidden', 0.055), ('trained', 0.055), ('schuler', 0.053), ('colored', 0.05), ('regularized', 0.049), ('agnadu', 0.048), ('asswiang', 0.048), ('iwthit', 0.048), ('nlouris', 0.048), ('perceptrons', 0.048), ('shta', 0.048), ('xcorrupted', 0.048), ('networks', 0.046), ('detectors', 0.046), ('burger', 0.045), ('engineered', 0.045), ('artifacts', 0.045), ('clean', 0.042), ('poisson', 0.039), ('remove', 0.039), ('resembling', 0.037), ('tanh', 0.037), ('foe', 0.035), ('katkovnik', 0.035), ('architecture', 0.034), ('blurred', 0.034), ('look', 0.033), ('removal', 0.033), ('patch', 0.032), ('sorted', 0.032), ('ncolored', 0.032), ('maximization', 0.031), ('convolved', 0.031), ('layers', 0.031), ('lens', 0.03), ('convolutional', 0.03), ('competitors', 0.029), ('defocus', 0.029), ('photograph', 0.029), ('eight', 0.029), ('bayer', 0.028), ('whereas', 0.028), ('feature', 0.028), ('achieved', 0.028), ('approximately', 0.027), ('square', 0.027), ('deblurring', 0.027), ('plain', 0.026), ('perceptron', 0.026), ('ieee', 0.025), ('seem', 0.025), ('dabov', 0.025), ('nets', 0.025), ('division', 0.025), ('foi', 0.024), ('inputs', 0.023), ('deep', 0.023), ('input', 0.023), ('answer', 0.023), ('gy', 0.023), ('restoration', 0.022), ('blind', 0.022), ('deblur', 0.022), ('harmeling', 0.022), ('network', 0.021), ('canon', 0.021), ('copies', 0.021), ('resemble', 0.021), ('connecting', 0.021), ('formation', 0.02), ('layer', 0.02), ('gaussian', 0.02), ('blurs', 0.02), ('question', 0.02), ('spread', 0.02), ('maximizing', 0.02), ('regularization', 0.02), ('berkeley', 0.019), ('output', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="17-tfidf-1" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>2 0.28292668 <a title="17-tfidf-2" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>3 0.28130028 <a title="17-tfidf-3" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>4 0.23951781 <a title="17-tfidf-4" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>Author: James Gregson, Felix Heide, Matthias B. Hullin, Mushfiqur Rouf, Wolfgang Heidrich</p><p>Abstract: We present a novel stochastic framework for non-blind deconvolution based on point samples obtained from random walks. Unlike previous methods that must be tailored to specific regularization strategies, the new Stochastic Deconvolution method allows arbitrary priors, including nonconvex and data-dependent regularizers, to be introduced and tested with little effort. Stochastic Deconvolution is straightforward to implement, produces state-of-the-art results and directly leads to a natural boundary condition for image boundaries and saturated pixels.</p><p>5 0.21913378 <a title="17-tfidf-5" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>6 0.20004231 <a title="17-tfidf-6" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>7 0.17823525 <a title="17-tfidf-7" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>8 0.17521016 <a title="17-tfidf-8" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>9 0.16145857 <a title="17-tfidf-9" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>10 0.1609146 <a title="17-tfidf-10" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>11 0.11682513 <a title="17-tfidf-11" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>12 0.10453233 <a title="17-tfidf-12" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>13 0.099313021 <a title="17-tfidf-13" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>14 0.096477747 <a title="17-tfidf-14" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>15 0.079398222 <a title="17-tfidf-15" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>16 0.072987638 <a title="17-tfidf-16" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>17 0.072611682 <a title="17-tfidf-17" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>18 0.065136053 <a title="17-tfidf-18" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>19 0.057353493 <a title="17-tfidf-19" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>20 0.056674249 <a title="17-tfidf-20" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, 0.101), (2, -0.038), (3, 0.117), (4, -0.087), (5, 0.328), (6, 0.048), (7, -0.01), (8, 0.027), (9, -0.033), (10, -0.021), (11, -0.039), (12, -0.01), (13, -0.05), (14, -0.014), (15, 0.017), (16, 0.036), (17, -0.007), (18, 0.091), (19, 0.048), (20, 0.023), (21, 0.027), (22, 0.018), (23, -0.037), (24, -0.042), (25, -0.011), (26, 0.018), (27, -0.068), (28, 0.0), (29, -0.005), (30, 0.019), (31, -0.03), (32, -0.02), (33, 0.009), (34, -0.013), (35, 0.008), (36, -0.017), (37, -0.022), (38, -0.01), (39, -0.033), (40, -0.007), (41, -0.019), (42, 0.007), (43, -0.05), (44, 0.013), (45, 0.059), (46, -0.017), (47, 0.025), (48, 0.028), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92643291 <a title="17-lsi-1" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>2 0.9206503 <a title="17-lsi-2" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>3 0.87192142 <a title="17-lsi-3" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>4 0.8558268 <a title="17-lsi-4" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>5 0.84572864 <a title="17-lsi-5" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>6 0.80965322 <a title="17-lsi-6" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>7 0.79665983 <a title="17-lsi-7" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>8 0.78137201 <a title="17-lsi-8" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>9 0.78124028 <a title="17-lsi-9" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>10 0.61688417 <a title="17-lsi-10" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>11 0.60608274 <a title="17-lsi-11" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>12 0.51435834 <a title="17-lsi-12" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>13 0.48873904 <a title="17-lsi-13" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>14 0.48219585 <a title="17-lsi-14" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>15 0.47623831 <a title="17-lsi-15" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>16 0.4565621 <a title="17-lsi-16" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>17 0.42832649 <a title="17-lsi-17" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>18 0.39830095 <a title="17-lsi-18" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>19 0.35226318 <a title="17-lsi-19" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>20 0.34923661 <a title="17-lsi-20" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.191), (16, 0.046), (26, 0.047), (28, 0.017), (33, 0.189), (49, 0.234), (67, 0.055), (69, 0.037), (77, 0.011), (80, 0.014), (87, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78554928 <a title="17-lda-1" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>2 0.78143543 <a title="17-lda-2" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>Author: Fei Chen, Huimin Yu, Roland Hu, Xunxun Zeng</p><p>Abstract: In this paper we introduce a new shape-driven approach for object segmentation. Given a training set of shapes, we first use deep Boltzmann machine to learn the hierarchical architecture of shape priors. This learned hierarchical architecture is then used to model shape variations of global and local structures in an energetic form. Finally, it is applied to data-driven variational methods to perform object extraction of corrupted data based on shape probabilistic representation. Experiments demonstrate that our model can be applied to dataset of arbitrary prior shapes, and can cope with image noise and clutter, as well as partial occlusions.</p><p>same-paper 3 0.7753765 <a title="17-lda-3" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>Author: Christian J. Schuler, Harold Christopher Burger, Stefan Harmeling, Bernhard Schölkopf</p><p>Abstract: Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non- blind deconvolution. Currently, the most successful meth- ods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step ap- proach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-ofthe-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.</p><p>4 0.75559384 <a title="17-lda-4" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>5 0.7529037 <a title="17-lda-5" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>6 0.75138628 <a title="17-lda-6" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>7 0.74918991 <a title="17-lda-7" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>8 0.74848574 <a title="17-lda-8" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>9 0.7446813 <a title="17-lda-9" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>10 0.74330342 <a title="17-lda-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.7428118 <a title="17-lda-11" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>12 0.73888731 <a title="17-lda-12" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>13 0.73848397 <a title="17-lda-13" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>14 0.73553526 <a title="17-lda-14" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>15 0.73464441 <a title="17-lda-15" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>16 0.72849268 <a title="17-lda-16" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>17 0.72775722 <a title="17-lda-17" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>18 0.72361535 <a title="17-lda-18" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>19 0.72185236 <a title="17-lda-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.72163159 <a title="17-lda-20" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
