<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-83" href="#">cvpr2013-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</h1>
<br/><p>Source: <a title="cvpr-2013-83-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chang_Classification_of_Tumor_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Hang Chang, Alexander Borowsky, Paul Spellman, Bahram Parvin</p><p>Abstract: Image-based classification oftissue histology, in terms of different components (e.g., normal signature, categories of aberrant signatures), provides a series of indices for tumor composition. Subsequently, aggregation of these indices in each whole slide image (WSI) from a large cohort can provide predictive models of clinical outcome. However, the performance of the existing techniques is hindered as a result of large technical and biological variations that are always present in a large cohort. In this paper, we propose two algorithms for classification of tissue histology based on robust representations of morphometric context, which are built upon nuclear level morphometric features at various locations and scales within the spatial pyramid matching (SPM) framework. These methods have been evaluated on two distinct datasets of different tumor types collected from The Cancer Genome Atlas (TCGA), and the experimental results indicate that our methods are (i) extensible to different tumor types; (ii) robust in the presence of wide technical and biological variations; (iii) invariant to different nuclear segmentation strategies; and (iv) scalable with varying training sample size. In addition, our experiments suggest that enforcing sparsity, during the construction of morphometric context, further improves the performance of the system.</p><p>Reference: <a title="cvpr-2013-83-reference" href="../cvpr2013_reference/cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , normal signature, categories of aberrant signatures), provides a series of indices for tumor composition. [sent-13, score-0.194]
</p><p>2 Subsequently, aggregation of these indices in each whole slide image (WSI) from a large cohort can provide predictive models of clinical outcome. [sent-14, score-0.156]
</p><p>3 However, the performance of the existing techniques is hindered as a result of large technical and biological variations that are always present in a large cohort. [sent-15, score-0.107]
</p><p>4 In this paper, we propose two algorithms for classification of tissue histology based on robust representations of morphometric context, which are built upon nuclear level morphometric features at various locations and scales within the spatial pyramid matching (SPM) framework. [sent-16, score-1.778]
</p><p>5 In addition, our experiments suggest that enforcing sparsity, during the construction of morphometric context, further improves the performance of the system. [sent-18, score-0.563]
</p><p>6 Introduction Histology sections provide a wealth of information about the tissue architecture that contains multiple cell types at different states of cell cycles. [sent-20, score-0.281]
</p><p>7 These sections are often ∗This work was supported by NIH U24 CA1437991 carried out at Lawrence Berkeley National Laboratory under Contract No. [sent-21, score-0.044]
</p><p>8 stained with hematoxylin and eosin (H&E;) stains, which label DNA (e. [sent-23, score-0.051]
</p><p>9 Abberations in the histology architecture often lead to disease progression. [sent-26, score-0.175]
</p><p>10 It is desirable to quantify indices associated with these abberations since they can be tested against the clinical outcome, e. [sent-27, score-0.061]
</p><p>11 Even though there are inter- and intra- observer variations [7], a trained pathologist always uses rich content (e. [sent-30, score-0.051]
</p><p>12 , various cell types, cellular organization, cell state and health), in context, to characterize tumor architecture. [sent-32, score-0.308]
</p><p>13 Due to the effectiveness of our representations, our methods achieve excellent performance even with small number of training samples across different segmentation strategies and independent datasets of tumors. [sent-34, score-0.097]
</p><p>14 These characteristics dramatically improve the (i) effectiveness of our techniques when applied to a large cohort, and (ii) extensibility to other cell-based assays. [sent-36, score-0.062]
</p><p>15 Related Work For the analysis of the H&E; stained sections, several excellent reviews can be found in [13, 8]. [sent-41, score-0.051]
</p><p>16 Fundamentally, the trend has been based either on nuclear segmentation and corresponding morphometric representaion, or patch-based 222222000311  Figure 1. [sent-42, score-0.865]
</p><p>17 Computational workflows: (a) Morphometric nonlinear kernel SPM; (b) Sparse Morphometric linear SPM. [sent-43, score-0.056]
</p><p>18 In both approaches, the nuclear segmentation could be based on any of the existing methods. [sent-44, score-0.302]
</p><p>19 representation of the histology sections that aids in clinical association. [sent-45, score-0.23]
</p><p>20 For example, a recent study indicates that detailed segmentation and multivariate representation of nuclear features from H&E; stained sections can predict DCIS recurrence [1] in patients with more than one nuclear grade. [sent-46, score-0.678]
</p><p>21 The major challenge for tissue classification is the large amounts of technical and biological variations in the data, which typically results in techniques that are tumor type specific. [sent-47, score-0.425]
</p><p>22 In the context of image categorization research, the traditional bag of features (BoF) model has been widely studied and improved through different variations, e. [sent-49, score-0.038]
</p><p>23 , modeling of co-occurrence of descriptors based on generative methods [4, 3, 20, 24], improving dictionary construction through discriminative learning [9, 22], modeling the spatial layout of local descriptors based on spatial pyramid matching kernel (SPM) [18]. [sent-51, score-0.197]
</p><p>24 At the same time, SPM partially captures context because of its hierarchical nature. [sent-54, score-0.038]
</p><p>25 Motivated by the works of [18, 28], we encode morphometric signatures, at different locations and scales, within the SPM framework. [sent-55, score-0.563]
</p><p>26 The end results are highly robust and effective systems across multiple tumor types with limited number of training samples. [sent-56, score-0.237]
</p><p>27 Approaches The computational workflows for the proposed methods are shown in Figure 1, where the nuclear segmentation can be based on any of the existing methods for delineating nuclei from background. [sent-58, score-0.397]
</p><p>28 For some tissue images and their corresponding nuclear segmentation, let:  1. [sent-59, score-0.382]
</p><p>29 N be the number of morphometric descriptors extracted from each segmented nucleus, e. [sent-61, score-0.563]
</p><p>30 X be the set of morphometric descriptors for all segmented nuclei, where X = [x1, . [sent-64, score-0.563]
</p><p>31 Morphometric SPM)  nonlinear  kernel SPM (MK-  In this approach, we utilize the nuclear morphometric information within the SPM framework to construct the morphometric context at various locations and scales for tissue image representation and classification. [sent-72, score-1.648]
</p><p>32 are the K nuclear morphometric types to be learned by the following optimization: ? [sent-78, score-0.846]
</p><p>33 =1||xm− zmD||2  (1)  subject to card(zm) = 1, |zm | = 1, zm ? [sent-80, score-0.088]
</p><p>34 indicates the assignment of the nuclear morphometric type, card(zm) is a cardinality constraint enforcing only one nonzero element  of zm, zm ? [sent-85, score-0.909]
</p><p>35 Construct spatial histogram as the descriptor for the morphometric context for SPM [18]. [sent-89, score-0.62]
</p><p>36 This is done by repeatedly subdividing an image and computing the histograms of different nuclear morphometric types over the resulting subregions. [sent-90, score-0.846]
</p><p>37 As a result, the spatial histogram, H, is formed by concatenating the appropriately weighted histograms of all nuclear morphometric types at all resolutions. [sent-91, score-0.865]
</p><p>38 In practice, the intersection kernel and χ2 kernel have been found to be the most suitable for histogram representations [28]. [sent-99, score-0.094]
</p><p>39 are the K nuclear morphmetric types to be learned by the following sparse coding optimization: ? [sent-115, score-0.351]
</p><p>40 Construct spatial pyramid representation as the descriptor of morphometric context for the linear SPM [28]. [sent-120, score-0.67]
</p><p>41 Let Z be the sparse codes calculated through Equation 3 for a descriptor set X. [sent-121, score-0.058]
</p><p>42 Based on pre-learned and fixed dictionary D, the image descriptor is computed based on a pre-determined pooling function as follows, f = P(Z)  (4)  In our implementation, P is selected to be the max pooling function on the absolute sparse codes fj = max{ |z1j | , |z2j |, . [sent-122, score-0.369]
</p><p>43 , |zMj |} (5) where fj is the j-th element of f, zij is the matrix element at i-th row and j-th column of Z, and M is the number of nuclei in the region. [sent-125, score-0.14]
</p><p>44 The choice of max pooling procedure is justified by biophysical evidence in the visual cortex [25], algorithms in image categorization [28], and our experimental comparison with other common pooling strategies (see Table 7). [sent-126, score-0.219]
</p><p>45 Similar to the construction of SPM, the pooled features from  various locations and scales are then concatenated to form a spatial pyramid representation ofthe image, and a linear SPM kernel is applied as follows,  κ(fi,fj) = fi? [sent-127, score-0.131]
</p><p>46 1  (6)  where fi and fj are spatial pyramid representations for image Ii and Ij, respectively, ? [sent-141, score-0.144]
</p><p>47 fj, and fil (s, t) and fjl (s,, ,t )r are cthtiev max pooling st fatistics of the sparse codes in the (s, t)-th segment of image Ii and Ij in the scale level l, respectively. [sent-144, score-0.184]
</p><p>48 SMLSPM: the linear SPM that uses linear kernel on spatial-pyramid pooling of morphometric sparse codes;  2. [sent-151, score-0.713]
</p><p>49 MKSPM: the nonlinear kernel SPM that uses spatialpyramid histograms of morphometric features and χ2 kernels; 3. [sent-152, score-0.647]
</p><p>50 ScSPM [28]: the linear SPM that uses linear kernel on spatial-pyramid pooling of SIFT sparse codes; 4. [sent-153, score-0.15]
</p><p>51 KSPM [18]: the nonlinear kernel SPM that uses spatial-pyramid histograms of SIFT features and χ2 kernels; 5. [sent-154, score-0.056]
</p><p>52 Comparison of average segmentation performance among MRGC [6], SRCD [5], and OTGR. [sent-162, score-0.044]
</p><p>53 MRGC [6]: A multi-reference graph cut approach for nuclear segmentation in histology tissue sections; 2. [sent-165, score-0.582]
</p><p>54 SRCD [5]: A single-reference color decomposition approach for nuclear segmentation in histology tissue sections; 3. [sent-166, score-0.582]
</p><p>55 OTGR: A simple Otsu thresholding [23] approach nuclear segmentation in histology tissue sections. [sent-167, score-0.582]
</p><p>56 our implementation, nuclear mask was generated applying Otsu thresholding on gray-scale image, refined by geometric reasoning [27]. [sent-168, score-0.258]
</p><p>57 for In by and reap-  A comparison of the segmentation performance, for the above methods, is quoted from [6], and listed in Table 1, and the computed morphometric features are listed in Table 2. [sent-170, score-0.676]
</p><p>58 Additionally, for all five methods, we fixed the level of pyramid to be 3, and used linear SVM for classification. [sent-177, score-0.05]
</p><p>59 GBM Dataset The GBM dataset contains 3 classes: Tumor, Necrosis, and Transition to Necrosis, which were curated from whole MRGCSRCDOTGR SMLSPM92. [sent-182, score-0.059]
</p><p>60 Performance of SMLSPM and MKSPM on the GBM dataset based on three different segmentation approaches, where the number of training images per category was fixed to be 160, and the dictionary sizes for SMLSPM and MKSPM were fixed to be 1024 and 512, respectively, to achieve optimal performance. [sent-207, score-0.179]
</p><p>61 Performance of SMLSPM and MKSPM on the KIRC dataset based on three different segmentation approaches, where the number of training images per category was fixed to be 280, and the dictionary sizes for both SMLSPM and MKSPM were fixed to be 512, to achieve the optimal performance. [sent-233, score-0.179]
</p><p>62 slide images (WSI) scanned with a 20X objective (0. [sent-234, score-0.062]
</p><p>63 The number of images per category are 628, 428 and 324, respectively. [sent-237, score-0.045]
</p><p>64 In this experiment, we torsatin imeda on 4 a0r,e 8 100 a00nd× 11600 images per category and tested on the rest, with three different dictionary sizes: 256, 512 and 1024. [sent-239, score-0.117]
</p><p>65 For SMLSPM and MKSPM, we also evaluated the performance based on three different segmentation strategies: MRGC, SRCD and OTGR. [sent-241, score-0.044]
</p><p>66 KIRC Dataset  The KIRC dataset contains 3 classes: Tumor, Normal, and Stromal, which were curated from whole slide images (WSI) scanned with a 40X objective (0. [sent-245, score-0.121]
</p><p>67 The number of images per category are 568, 796 and 784, respectively. [sent-248, score-0.045]
</p><p>68 In this experiment, we tirmaaingeeds on e7 10,0 10400 × ×an 1d0 20800 p images per category and tested  on the rest, with three different dictionary sizes: 256, 5 12 and 1024. [sent-250, score-0.117]
</p><p>69 For SMLSPM and MKSPM, we also evaluated the performance based on three different segmentation strategies: MRGC, SRCD and OTGR. [sent-252, score-0.044]
</p><p>70 , train222222000644  background is defined to be outside the nuclear region, but inside the bounding box of nuclear boundary. [sent-257, score-0.516]
</p><p>71 Comparison of performance for SMLSPM using different pooling strategies on the GBM and KIRC datasets. [sent-286, score-0.117]
</p><p>72 For GBM, the number of training images per category was fixed to be 160, and the dictionary size was fixed to be 1024; for KIRC, the number of training images per category was fixed to be 280, and the dictionary size was fixed to be 512. [sent-287, score-0.27]
</p><p>73 Since GBM and KIRC are two vastly different tumor types with significantly different signatures, we suggest that the consistency in performance assures extensibility to different tumor types. [sent-291, score-0.452]
</p><p>74 Robustness in the presence of large amounts of technical and biological variations. [sent-292, score-0.087]
</p><p>75 Performance of different methods on the GBM dataset, where SMLSPM and MKSPM were evaluated based on the segmentation  method: MRGC [6]. [sent-473, score-0.044]
</p><p>76 With respect to the KIRC dataset, shown in Table 5, the performance of our methods, based on 70 training samples per category, is comparable to the performance of ScSPM, KSPM and CTSPM based on 280 training samples per category. [sent-478, score-0.074]
</p><p>77 Given the fact that TCGA datasets contain large amount of technical and biological variations [17, 19], these results clearly indicate the robustness of our morphometric context representation, which dramatically improved the reliability of our approaches. [sent-479, score-0.731]
</p><p>78 Ta-  ×  bles 4 and 6 indicate that the performance of our approaches are almost invariant to different segmentation algorithms, given the fact that the segmentation performance itself varies a lot, as shown in Table 1. [sent-482, score-0.088]
</p><p>79 More importantly, even with the simplest segmentation strategy OTGR, SMLSPM outperforms the methods in [28, 18] on both datasets, and MRSPM outperforms the methods in [28, 18] on the GBM dataset, while generating comparable results on the KIRC dataset. [sent-483, score-0.044]
</p><p>80 Given the fact that, in a lot of studies, both nuclear segmentation and tissue classification are necessary components, the MethodDictionarySize=256DictionarySize=512DictionarySize=1024 280 trainingSMLSPM98. [sent-484, score-0.426]
</p><p>81 Performance of different methods on the KIRC dataset, where SMLSPM and MKSPM were evaluated based on the segmentation  mehtod: MRGC [6]. [sent-664, score-0.044]
</p><p>82 use of pre-computed morphometric features dramatically improve the efficiency by avoiding extra feature extraction steps. [sent-665, score-0.606]
</p><p>83 Our experiments show that the morphometric context representation in SMLSPM works well with linear SVMs, which dramatically improves the scalability of training and the speed of testing. [sent-671, score-0.642]
</p><p>84 This is very important for the analyzing large cohort of whole slide images. [sent-672, score-0.126]
</p><p>85 To study the impact of pooling strategies on the SMLSPM method, we also provide an experimental comparison among max pooling and two other common pooling methods, which are defined as follows,  Sqrt Abs  :fj=? [sent-673, score-0.301]
</p><p>86 As shown in Table 7, the max pooling strategy outperforms the other two, which is probably due to its robustness to local translations. [sent-680, score-0.102]
</p><p>87 Conclusion and Future Work In this paper, we proposed two spatial pyramid matching approaches based on morphometric features and morphometric sparse code, respectively, for tissue image classification. [sent-683, score-1.35]
</p><p>88 By modeling the context of the morphometric information, these methods outperform traditional ones which were typically based on pixel- or patch-level features. [sent-684, score-0.601]
</p><p>89 Due to i) the effectiveness of our morphometric context representations; and ii) the important role of cellular context for the study of different cell assays, proposed methods are suggested to be extendable to image classification tasks for different cell assays. [sent-686, score-0.753]
</p><p>90 Future work will be focused on accelerating the sparse coding through the sparse auto encoder [15], utilizing supervised dictionary learning [21] for possible improvement, and further validating our methods on other tissue types and other cell assays. [sent-687, score-0.364]
</p><p>91 Effect of quantitative 222222000977  [2]  [3]  [4]  [5]  nuclear features on recurrence of ductal carcinoma in situ (dcis) of breast. [sent-697, score-0.312]
</p><p>92 Automatic identification and delineation of germ layer components in h&e; stained images of teratomas derived from human and nonhuman primate embryonic stem cells. [sent-707, score-0.051]
</p><p>93 Automated cancer diagnosis based on histopathological images: A systematic survey, 2009. [sent-754, score-0.116]
</p><p>94 Image denoising via sparse and redundant representations over learned dictionaries. [sent-758, score-0.051]
</p><p>95 The pyramid match kernel: discriminative classification with sets of image features. [sent-790, score-0.05]
</p><p>96 Time efficient sparse analysis of histopathological whole slide images. [sent-807, score-0.173]
</p><p>97 Fast infer–  [13]  [14]  [15]  [16]  [17]  [18]  [19]  [20]  [21] [22]  [23]  ence in sparse coding algorithms with applications to object recognition. [sent-813, score-0.068]
</p><p>98 Biological interpretation of morphological patterns in histopathological whole slide images. [sent-829, score-0.142]
</p><p>99 Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. [sent-835, score-0.05]
</p><p>100 Linear spatial pyramid matching using sparse coding for image classification. [sent-904, score-0.137]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('morphometric', 0.563), ('mkspm', 0.313), ('nuclear', 0.258), ('smlspm', 0.25), ('kspm', 0.208), ('spm', 0.206), ('tumor', 0.194), ('ctspm', 0.188), ('kirc', 0.188), ('gbm', 0.167), ('scspm', 0.166), ('histology', 0.156), ('tissue', 0.124), ('mrgc', 0.11), ('zm', 0.088), ('pooling', 0.082), ('srcd', 0.078), ('dictionary', 0.072), ('nuclei', 0.064), ('biological', 0.063), ('histopathological', 0.063), ('slide', 0.062), ('fj', 0.055), ('cancer', 0.053), ('stained', 0.051), ('pyramid', 0.05), ('isbi', 0.048), ('cohort', 0.047), ('otgr', 0.047), ('spellman', 0.047), ('tcga', 0.047), ('wsi', 0.047), ('cell', 0.044), ('segmentation', 0.044), ('sections', 0.044), ('curated', 0.042), ('extensibility', 0.039), ('context', 0.038), ('coding', 0.037), ('kernel', 0.037), ('strategies', 0.035), ('sparse', 0.031), ('abberations', 0.031), ('borowsky', 0.031), ('carcinoma', 0.031), ('mrgcsrcdotgr', 0.031), ('necrosis', 0.031), ('pathologist', 0.031), ('quoted', 0.031), ('secondrow', 0.031), ('workflows', 0.031), ('zmd', 0.031), ('dk', 0.03), ('clinical', 0.03), ('proceedings', 0.03), ('health', 0.029), ('signatures', 0.028), ('spatialpyramid', 0.028), ('biomedicine', 0.028), ('genome', 0.028), ('codes', 0.027), ('ow', 0.027), ('conference', 0.027), ('ii', 0.026), ('category', 0.026), ('dcis', 0.026), ('cellular', 0.026), ('otsu', 0.026), ('scales', 0.025), ('types', 0.025), ('liblinear', 0.024), ('oregon', 0.024), ('fil', 0.024), ('kernels', 0.024), ('technical', 0.024), ('recurrence', 0.023), ('dramatically', 0.023), ('extensible', 0.021), ('atlas', 0.021), ('bioinformatics', 0.021), ('card', 0.021), ('construct', 0.021), ('zij', 0.021), ('max', 0.02), ('variations', 0.02), ('pages', 0.02), ('avoiding', 0.02), ('representations', 0.02), ('nonlinear', 0.019), ('spatial', 0.019), ('nih', 0.019), ('disease', 0.019), ('listed', 0.019), ('xm', 0.019), ('berkeley', 0.019), ('per', 0.019), ('lawrence', 0.018), ('training', 0.018), ('chang', 0.018), ('whole', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="83-tfidf-1" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>Author: Hang Chang, Alexander Borowsky, Paul Spellman, Bahram Parvin</p><p>Abstract: Image-based classification oftissue histology, in terms of different components (e.g., normal signature, categories of aberrant signatures), provides a series of indices for tumor composition. Subsequently, aggregation of these indices in each whole slide image (WSI) from a large cohort can provide predictive models of clinical outcome. However, the performance of the existing techniques is hindered as a result of large technical and biological variations that are always present in a large cohort. In this paper, we propose two algorithms for classification of tissue histology based on robust representations of morphometric context, which are built upon nuclear level morphometric features at various locations and scales within the spatial pyramid matching (SPM) framework. These methods have been evaluated on two distinct datasets of different tumor types collected from The Cancer Genome Atlas (TCGA), and the experimental results indicate that our methods are (i) extensible to different tumor types; (ii) robust in the presence of wide technical and biological variations; (iii) invariant to different nuclear segmentation strategies; and (iv) scalable with varying training sample size. In addition, our experiments suggest that enforcing sparsity, during the construction of morphometric context, further improves the performance of the system.</p><p>2 0.080347039 <a title="83-tfidf-2" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>3 0.07076215 <a title="83-tfidf-3" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Peng Wang, Jingdong Wang, Gang Zeng, Weiwei Xu, Hongbin Zha, Shipeng Li</p><p>Abstract: In visual recognition tasks, the design of low level image feature representation is fundamental. The advent of local patch features from pixel attributes such as SIFT and LBP, has precipitated dramatic progresses. Recently, a kernel view of these features, called kernel descriptors (KDES) [1], generalizes the feature design in an unsupervised fashion and yields impressive results. In this paper, we present a supervised framework to embed the image level label information into the design of patch level kernel descriptors, which we call supervised kernel descriptors (SKDES). Specifically, we adopt the broadly applied bag-of-words (BOW) image classification pipeline and a large margin criterion to learn the lowlevel patch representation, which makes the patch features much more compact and achieve better discriminative ability than KDES. With this method, we achieve competitive results over several public datasets comparing with stateof-the-art methods.</p><p>4 0.069311403 <a title="83-tfidf-4" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>5 0.066028737 <a title="83-tfidf-5" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>6 0.061532103 <a title="83-tfidf-6" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>7 0.061151497 <a title="83-tfidf-7" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>8 0.060527515 <a title="83-tfidf-8" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>9 0.056507371 <a title="83-tfidf-9" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>10 0.055696756 <a title="83-tfidf-10" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>11 0.054143064 <a title="83-tfidf-11" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>12 0.054015577 <a title="83-tfidf-12" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>13 0.051999077 <a title="83-tfidf-13" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>14 0.051252771 <a title="83-tfidf-14" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>15 0.049254224 <a title="83-tfidf-15" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>16 0.047147591 <a title="83-tfidf-16" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>17 0.04590914 <a title="83-tfidf-17" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>18 0.045249104 <a title="83-tfidf-18" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>19 0.044077847 <a title="83-tfidf-19" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>20 0.043238521 <a title="83-tfidf-20" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.09), (1, -0.043), (2, -0.053), (3, 0.066), (4, -0.005), (5, -0.01), (6, 0.008), (7, 0.029), (8, -0.042), (9, 0.004), (10, 0.004), (11, -0.002), (12, 0.006), (13, -0.016), (14, 0.023), (15, 0.009), (16, -0.006), (17, 0.006), (18, 0.01), (19, 0.043), (20, 0.0), (21, 0.019), (22, 0.024), (23, -0.006), (24, -0.018), (25, 0.077), (26, -0.021), (27, 0.007), (28, -0.017), (29, 0.014), (30, -0.009), (31, -0.002), (32, -0.039), (33, -0.016), (34, 0.015), (35, -0.001), (36, 0.011), (37, 0.047), (38, -0.007), (39, 0.007), (40, -0.043), (41, -0.019), (42, 0.009), (43, 0.021), (44, 0.047), (45, -0.03), (46, -0.002), (47, 0.005), (48, 0.005), (49, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89076537 <a title="83-lsi-1" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>Author: Hang Chang, Alexander Borowsky, Paul Spellman, Bahram Parvin</p><p>Abstract: Image-based classification oftissue histology, in terms of different components (e.g., normal signature, categories of aberrant signatures), provides a series of indices for tumor composition. Subsequently, aggregation of these indices in each whole slide image (WSI) from a large cohort can provide predictive models of clinical outcome. However, the performance of the existing techniques is hindered as a result of large technical and biological variations that are always present in a large cohort. In this paper, we propose two algorithms for classification of tissue histology based on robust representations of morphometric context, which are built upon nuclear level morphometric features at various locations and scales within the spatial pyramid matching (SPM) framework. These methods have been evaluated on two distinct datasets of different tumor types collected from The Cancer Genome Atlas (TCGA), and the experimental results indicate that our methods are (i) extensible to different tumor types; (ii) robust in the presence of wide technical and biological variations; (iii) invariant to different nuclear segmentation strategies; and (iv) scalable with varying training sample size. In addition, our experiments suggest that enforcing sparsity, during the construction of morphometric context, further improves the performance of the system.</p><p>2 0.7123065 <a title="83-lsi-2" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>3 0.69982713 <a title="83-lsi-3" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Peng Wang, Jingdong Wang, Gang Zeng, Weiwei Xu, Hongbin Zha, Shipeng Li</p><p>Abstract: In visual recognition tasks, the design of low level image feature representation is fundamental. The advent of local patch features from pixel attributes such as SIFT and LBP, has precipitated dramatic progresses. Recently, a kernel view of these features, called kernel descriptors (KDES) [1], generalizes the feature design in an unsupervised fashion and yields impressive results. In this paper, we present a supervised framework to embed the image level label information into the design of patch level kernel descriptors, which we call supervised kernel descriptors (SKDES). Specifically, we adopt the broadly applied bag-of-words (BOW) image classification pipeline and a large margin criterion to learn the lowlevel patch representation, which makes the patch features much more compact and achieve better discriminative ability than KDES. With this method, we achieve competitive results over several public datasets comparing with stateof-the-art methods.</p><p>4 0.68666214 <a title="83-lsi-4" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>5 0.66759396 <a title="83-lsi-5" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>6 0.65829509 <a title="83-lsi-6" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>7 0.65586382 <a title="83-lsi-7" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>8 0.65505129 <a title="83-lsi-8" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>9 0.65381831 <a title="83-lsi-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.65288562 <a title="83-lsi-10" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>11 0.65144104 <a title="83-lsi-11" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>12 0.64640099 <a title="83-lsi-12" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>13 0.63406831 <a title="83-lsi-13" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>14 0.61785471 <a title="83-lsi-14" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>15 0.61736029 <a title="83-lsi-15" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>16 0.61018133 <a title="83-lsi-16" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>17 0.59259301 <a title="83-lsi-17" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>18 0.59235996 <a title="83-lsi-18" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>19 0.58798575 <a title="83-lsi-19" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>20 0.58170897 <a title="83-lsi-20" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.084), (16, 0.025), (26, 0.021), (28, 0.012), (33, 0.189), (43, 0.444), (67, 0.036), (69, 0.032), (76, 0.014), (87, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.65643185 <a title="83-lda-1" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>Author: Hang Chang, Alexander Borowsky, Paul Spellman, Bahram Parvin</p><p>Abstract: Image-based classification oftissue histology, in terms of different components (e.g., normal signature, categories of aberrant signatures), provides a series of indices for tumor composition. Subsequently, aggregation of these indices in each whole slide image (WSI) from a large cohort can provide predictive models of clinical outcome. However, the performance of the existing techniques is hindered as a result of large technical and biological variations that are always present in a large cohort. In this paper, we propose two algorithms for classification of tissue histology based on robust representations of morphometric context, which are built upon nuclear level morphometric features at various locations and scales within the spatial pyramid matching (SPM) framework. These methods have been evaluated on two distinct datasets of different tumor types collected from The Cancer Genome Atlas (TCGA), and the experimental results indicate that our methods are (i) extensible to different tumor types; (ii) robust in the presence of wide technical and biological variations; (iii) invariant to different nuclear segmentation strategies; and (iv) scalable with varying training sample size. In addition, our experiments suggest that enforcing sparsity, during the construction of morphometric context, further improves the performance of the system.</p><p>2 0.55640775 <a title="83-lda-2" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>Author: Luc Oth, Paul Furgale, Laurent Kneip, Roland Siegwart</p><p>Abstract: Rolling Shutter (RS) cameras are used across a wide range of consumer electronic devices—from smart-phones to high-end cameras. It is well known, that if a RS camera is used with a moving camera or scene, significant image distortions are introduced. The quality or even success of structure from motion on rolling shutter images requires the usual intrinsic parameters such as focal length and distortion coefficients as well as accurate modelling of the shutter timing. The current state-of-the-art technique for calibrating the shutter timings requires specialised hardware. We present a new method that only requires video of a known calibration pattern. Experimental results on over 60 real datasets show that our method is more accurate than the current state of the art.</p><p>3 0.50498766 <a title="83-lda-3" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>Author: Wangmeng Zuo, Lei Zhang, Chunwei Song, David Zhang</p><p>Abstract: Image denoising is a classical yet fundamental problem in low level vision, as well as an ideal test bed to evaluate various statistical image modeling methods. One of the most challenging problems in image denoising is how to preserve the fine scale texture structures while removing noise. Various natural image priors, such as gradient based prior, nonlocal self-similarity prior, and sparsity prior, have been extensively exploited for noise removal. The denoising algorithms based on these priors, however, tend to smooth the detailed image textures, degrading the image visual quality. To address this problem, in this paper we propose a texture enhanced image denoising (TEID) method by enforcing the gradient distribution of the denoised image to be close to the estimated gradient distribution of the original image. A novel gradient histogram preservation (GHP) algorithm is developed to enhance the texture structures while removing noise. Our experimental results demonstrate that theproposed GHP based TEID can well preserve the texture features of the denoised images, making them look more natural.</p><p>4 0.47313654 <a title="83-lda-4" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>5 0.44073895 <a title="83-lda-5" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>6 0.44037247 <a title="83-lda-6" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>7 0.44033062 <a title="83-lda-7" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>8 0.44002625 <a title="83-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.4399505 <a title="83-lda-9" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>10 0.43992981 <a title="83-lda-10" href="./cvpr-2013-Tensor-Based_Human_Body_Modeling.html">426 cvpr-2013-Tensor-Based Human Body Modeling</a></p>
<p>11 0.43972561 <a title="83-lda-11" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>12 0.43966976 <a title="83-lda-12" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>13 0.43965936 <a title="83-lda-13" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>14 0.43963429 <a title="83-lda-14" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>15 0.43932468 <a title="83-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.43920884 <a title="83-lda-16" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>17 0.43917263 <a title="83-lda-17" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>18 0.43908188 <a title="83-lda-18" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>19 0.43893638 <a title="83-lda-19" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>20 0.43890575 <a title="83-lda-20" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
