<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-323" href="#">cvpr2013-323</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</h1>
<br/><p>Source: <a title="cvpr-2013-323-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Berg_POOF_Part-Based_One-vs.-One_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: From a set ofimages in aparticular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.</p><p>Reference: <a title="cvpr-2013-323-reference" href="../cvpr2013_reference/cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract From a set ofimages in aparticular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). [sent-3, score-0.287]
</p><p>2 In contrast to basiclevel recognition, in which we need to distinguish basiclevel categories such as chair and car from each other, the fine-grained categorization problem asks us to distinguish subordinate-level categories such as office chair and kitchen chair from each other. [sent-9, score-0.333]
</p><p>3 One relatively well-studied example of fine-grained visual categorization is species or breed recognition. [sent-10, score-0.59]
</p><p>4 Many of the most accurate approaches to fine-grained visual categorization are based on detecting and extracting features from particular parts of the objects. [sent-11, score-0.271]
</p><p>5 For example, in dog breed classification one may extract features from the nose and base of the ears [16, 23]. [sent-12, score-0.365]
</p><p>6 Face recognition is an extreme case of fine-grained visual categorization in which the “subcategories” are individual instances, and the best face recognition methods extract features from locations determined by finding facial landmarks such as the corners of the eyes [3, 3 1, 36]. [sent-13, score-0.414]
</p><p>7 Fine-grained visual categorization also conveniently enables part-based approaches, because objects within the same basic-level category will often have the “same” parts [25], allowing for easier comparison. [sent-18, score-0.206]
</p><p>8 For example since all dogs have noses,  it is natural in dog breed recognition to attempt to detect and extract features from the nose. [sent-19, score-0.256]
</p><p>9 A straightforward approach to part-based recognition is to extract some of these features at the part locations and build a classifier. [sent-22, score-0.207]
</p><p>10 We start with a dataset of images in the domain, labeled by class and with part locations. [sent-26, score-0.165]
</p><p>11 For any pair of classes, for any pair of parts, we extract some low-level features in a grid of cells that covers the two parts, and train a linear classifier to distinguish the two classes from each other. [sent-27, score-0.392]
</p><p>12 ) The weights assigned  by this classifier to different cells of the grid indicate the most discriminative region around these parts for this pair of classes. [sent-29, score-0.353]
</p><p>13 Learning a Part-based One-vs-One Feature (POOF) for bird species identification. [sent-32, score-0.495]
</p><p>14 Given (a) a reference dataset of images labeled with class (species) and part locations, a POOF is defined by specifying two classes, one part for feature extraction, another part for alignment, and a low-level “base feature. [sent-33, score-0.378]
</p><p>15 ” (b) Samples of the two chosen classes are taken from the dataset and (c) aligned to put the two chosen parts in fixed locations. [sent-34, score-0.237]
</p><p>16 (d) The aligned images are divided into cells at multiple scales, from which the base feature is extracted. [sent-35, score-0.295]
</p><p>17 Finally, a classifier is trained on the base feature values from just the support region. [sent-38, score-0.244]
</p><p>18 30% on the localized species categorization benchmark, quadrupling the accuracy reported in [27]. [sent-44, score-0.54]
</p><p>19 We demonstrate that POOFs reduce the need for large training sets, showing that in the face domain they can be used as extremely effective intermediate features for tasks such as attribute labeling. [sent-45, score-0.349]
</p><p>20 –  •  •  –  While each POOF is only known to be discriminative for the two classes used in its definition, we find that collections of POOFs are useful not only for classification into the classes in the reference dataset, but for other tasks in the same domain. [sent-46, score-0.313]
</p><p>21 We show examples in two domains, bird species and faces. [sent-47, score-0.495]
</p><p>22 Instance-level recognition can be seen as the “finest-grained” categorization, and is most commonly seen as face recognition or image search. [sent-50, score-0.167]
</p><p>23 [6, 30, 34]) or by perform-  ing an alignment step based on parts, then extracting features from fixed locations (e. [sent-53, score-0.163]
</p><p>24 [3] in particular takes an approach similar to ours in using binary onevs-one classifiers trained on a reference set as discriminative features for the domain. [sent-56, score-0.279]
</p><p>25 Moving up a step from instance-level recognition, subordinate category recognition has been explored mostly in the context of species or breed recognition. [sent-59, score-0.534]
</p><p>26 Many authors have reported bird species identification results on the CaltechUCSD Birds Dataset [27], using the idea of parts in one way or another. [sent-60, score-0.621]
</p><p>27 [8, 32, 33, 35] in different ways attempt to find parts of the image that are discriminative without explicit part labels, but cannot achieve the accuracy of a supervised part-based approach. [sent-61, score-0.191]
</p><p>28 [9] defines a set of just 999995555566444  two coarse parts (the head and body) used to align the images, but do not use fine-scale part locations to define their features. [sent-63, score-0.202]
</p><p>29 [14] and [15] train attribute classifiers based on a set of classes with labeled attributes, then apply the attribute classifiers to novel classes, in the domains of faces and animal species respectively. [sent-67, score-1.0]
</p><p>30 These methods restrict the shape of the feature support region to one of several symmetric configurations, while our method allows any shape of descriptor, up to the resolution of our base feature grid. [sent-75, score-0.277]
</p><p>31 Part-Based One-vs-One Features Our method requires as input a reference dataset of im-  ages belonging to the domain under study, annotated with class labels and part locations. [sent-77, score-0.227]
</p><p>32 It can also be a separate dataset labeled with classes different from those in the classification task. [sent-81, score-0.209]
</p><p>33 In the current implementation we use two base features: gradient direction histograms and color histograms. [sent-101, score-0.256]
</p><p>34 The POOF will be learned based on the reference images of classes iand j. [sent-105, score-0.181]
</p><p>35 We tile the cropped images with a grid of feature cells, and extract the base feature from each cell. [sent-110, score-0.344]
</p><p>36 We do multiple tilings, each using grid cells of a different size, and so extracting features at a different scale. [sent-111, score-0.205]
</p><p>37 For the tiling at each scale, we train a linear support vector machine to distinguish class i from class j, based on the concatenation of the base feature values over the grid. [sent-113, score-0.366]
</p><p>38 The trained SVM weight vector gives weights to every dimension of the base feature in every grid cell. [sent-115, score-0.283]
</p><p>39 By thresholding these weights, we obtain a mask on the aligned images  Tfi,,aj,b  that defines the grid cells that are most discriminative between class iand j. [sent-117, score-0.279]
</p><p>40 Starting with the grid cell containing part f as a seed, we find the maximum connected component of grid cells above the threshold in each tiling. [sent-119, score-0.268]
</p><p>41 This will act as a mask on the aligned image, defining at each scale a discriminative region around part f. [sent-120, score-0.159]
</p><p>42 By restricting the region to a connected component of f, we force POOFs with different feature parts to use different regions, encouraging diversity across the set of POOFs. [sent-121, score-0.184]
</p><p>43 The low-level feature associated with Tfi,,aj,b is the concatenation of the base feature at the masked cells in all the tilings. [sent-123, score-0.408]
</p><p>44 Using this feature and all aligned images of classes iand j, we train another linear SVM. [sent-124, score-0.21]
</p><p>45 Bird species classification accuracy on the full 200species CUBS benchmark. [sent-128, score-0.438]
</p><p>46 The new image is aligned by similarity to put parts f and a in standard locations, then the base-level feature is extracted from just the masked cells of the tilings at each scale. [sent-130, score-0.357]
</p><p>47 We use two scales of grid for the base feature extraction, with 8 x 8 and 16 x 16-pixel cells. [sent-140, score-0.235]
</p><p>48 For the “gradhist” variant, we extract an 8-bin gradient direction histogram from each grid cell, then concatenate the histograms over all cells (or in the final Tfi,,aj,b, over just the masked cells). [sent-144, score-0.33]
</p><p>49 Bird species classification accuracy on the “birdlets” subset of 14 woodpeckers and vireos defined in [9]. [sent-148, score-0.461]
</p><p>50 This gives us a nine-bin unsigned gradient direction histogram, an 18-bin signed gradient orientation histogram, and 4 normalization constants, for, in total, a 31-dimensional feature for each grid cell. [sent-149, score-0.256]
</p><p>51 The color centers are obtained by running k-means in RGB space on the pixels in the aligned and cropped region for all the images in the reference set, so the color centers are a function of f and a. [sent-154, score-0.224]
</p><p>52 This has the effect of masking out half of the region in Step 4 (which is further reduced when we restrict the region to a connected component contiguous with part f). [sent-156, score-0.16]
</p><p>53 1, we consider bird species identification, building a set of POOFs from the training set, and applying them to recognition. [sent-160, score-0.527]
</p><p>54 2 we apply our method to face verification on unseen face pairs, building POOFs on a set of faces of different people than the test faces, demonstrating that our features learn to discriminate over the domain of images in general and not just over the particular classes from which they are built. [sent-162, score-0.615]
</p><p>55 2 to attribute classification, and find that they are useful even when the classification task is on a different type 999995555588666  of classes (attributes) than the classes on which they were  learned (subject identities). [sent-165, score-0.297]
</p><p>56 The images are split into training and test sets, with about 30 images per species in the training set, and the remainder in the test set. [sent-170, score-0.503]
</p><p>57 The authors propose several benchmarks for species recognition and part detection. [sent-171, score-0.466]
</p><p>58 Here, we evaluate on the “localized species categorization” benchmark, in which the part locations for all images are provided to the algorithm, and the task is, given the species labels on the training images, to determine the species of the test images. [sent-172, score-1.346]
</p><p>59 There are very few images in the dataset with all fifteen parts visible. [sent-174, score-0.162]
</p><p>60 In particular, most birds have only one eye and one wing visible. [sent-175, score-0.241]
</p><p>61 This gives us a dataset in which almost all of the images have the left eye labeled (a few images have neither eye visible). [sent-178, score-0.228]
</p><p>62 There are 200 classes, twelve parts, and two base features, yielding ? [sent-181, score-0.165]
</p><p>63 For each image, we rank the 200 species from highest to lowest classifier response. [sent-187, score-0.437]
</p><p>64 Taking the top ranked species for each image, we achieve a classification accuracy of 68. [sent-188, score-0.438]
</p><p>65 73% using the gradhist variant of the gradient feature, or 73. [sent-189, score-0.234]
</p><p>66 While the localized species categorization protocol defined in [27] uses the ground truth part locations, this does not give automatic classification performance. [sent-191, score-0.66]
</p><p>67 To evaluate automatic classification, we rerun the experiment using automatically detected part locations on the test data in place of the ground truth locations. [sent-192, score-0.159]
</p><p>68 We use part locations from the part detector of [2] on images cropped to the bounding boxes of the birds, allowing us to compare with previous work that uses the bounding boxes but not the part labels. [sent-193, score-0.237]
</p><p>69 The rate at which the correct species is in the top k ranked species is shown in Figure 2. [sent-199, score-0.79]
</p><p>70 Our rank-1 classification accuracy on this subset using the gradhist variant is 80. [sent-201, score-0.262]
</p><p>71 To show the benefit of the POOFs, we contrast our onevs-all species classifiers with classifiers trained in a similar way, but without the POOFs. [sent-206, score-0.586]
</p><p>72 The POOFs are built using histograms of gradient direction and color over spatial grids  covering the parts as the base features. [sent-207, score-0.351]
</p><p>73 For comparison, we build species classifiers that operate directly on the concatenation of these base features over all twelve parts. [sent-208, score-0.734]
</p><p>74 As with the POOF-based species classifiers, these classifiers are linear SVMs. [sent-209, score-0.478]
</p><p>75 Baseline accuracy on the localized species categorization benchmark reported in [27] is 17. [sent-212, score-0.565]
</p><p>76 73% mean average precision on the birdlets subset using the earlier version of the dataset (our mAP with HOG on the birdlets subset is 85. [sent-222, score-0.315]
</p><p>77 Face Verification In face verification, we are given two face images, of people not encountered at any training stage, and must determine whether they are two images of the same person or images of two different people. [sent-235, score-0.27]
</p><p>78 Because we must deal with previously unseen faces, there is no training set of images belonging to the classes we will be faced with at test time, as there was in the previous example, where we could learn our features based on the training set. [sent-236, score-0.252]
</p><p>79 Here, we learn the features from a set of face images entirely separate from the evaluation dataset, in the belief that the features we discover are generally applicable to the face domain. [sent-237, score-0.352]
</p><p>80 The Labeled Faces in the Wild (LFW) [12] is the standard face verification dataset and benchmark, containing 6,000 face pairs and a ten-fold cross-validation protocol for algorithm evaluation. [sent-238, score-0.367]
</p><p>81 30% [3], using a sep-  arate reference dataset of images labeled with identity to train a set of “Tom-vs-Pete” classifiers, which are then used as feature extractors feeding a higher level classifier. [sent-240, score-0.246]
</p><p>82 We use this same reference dataset to learn POOFs, and also transform the images with the “identity-preserving alignment” from that work as a preprocessing step, based on part detections from the detector of [2]. [sent-241, score-0.189]
</p><p>83 The reference dataset, from [3], consists of 20,639 face images, downloaded from the internet, spanning 120 subject identities. [sent-242, score-0.198]
</p><p>84 This image pair feature is extracted from the training folds to train a same-vs-different classifier that makes the verification decision. [sent-248, score-0.262]
</p><p>85 40%across the ten folds using the gradhist variant, or 92. [sent-256, score-0.173]
</p><p>86 The most important difference is that our method is general, where they carefully choose the support regions for the Tom-vs-Pete classifiers based on knowledge of face recognition. [sent-260, score-0.202]
</p><p>87 Figure 6 compares the result from the POOFs with a result using the base features alone, showing, as in Figure 2 for bird species recognition, a substantial boost due to the POOFs. [sent-263, score-0.675]
</p><p>88 For each attribute, the first row gives the baseline accuracy obtained by training directly on the low-level base features (color and gradient direction histograms), and the second row gives accuracies using our POOFs. [sent-441, score-0.359]
</p><p>89 [14] downloaded face images from the Internet, labeled them with at-  tributes such as gender, race, age, and hair color, and used these labels to train attribute classifiers based on low-level features such as raw pixel color and gradients. [sent-451, score-0.472]
</p><p>90 We use this same dataset to train a set of attribute classifiers based on POOFs. [sent-452, score-0.262]
</p><p>91 have made available both human labels and the results of their attribute classifiers for 19 binary attributes on the 7701 images in View 2 of LFW. [sent-454, score-0.282]
</p><p>92 Although the classes in this task (attributes) are of a different type from those in the previous experiment (identities), we remain in the face domain, and so expect the POOFs we learned there to be useful here. [sent-456, score-0.19]
</p><p>93 ) To build attribute classifiers, we simply extract our 10,000 POOF scores from the attribute training images, and use these feature vectors to train a linear SVM for each attribute. [sent-460, score-0.361]
</p><p>94 One of the benefits of POOFs is that by incorporating knowledge of the domain learned from the reference set, which is not labeled with attributes, they reduce the need for a large attribute-labeled training set. [sent-461, score-0.214]
</p><p>95 The results on the test set are shown in Table 1, using the gradhist variant of the gradient orientation base feature. [sent-463, score-0.393]
</p><p>96 As before, we also show the performance of classifiers built directly on the low-level base features. [sent-464, score-0.22]
</p><p>97 Labeled faces in the wild: A database for studying face recognition in unconstrained environments. [sent-577, score-0.195]
</p><p>98 Leafsnap: A computer vision system for automatic plant species identification. [sent-592, score-0.425]
</p><p>99 Describable visual attributes for face verification and image search. [sent-603, score-0.3]
</p><p>100 Learning to detect unseen object classes by between-class attribute transfer. [sent-611, score-0.207]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('poofs', 0.626), ('species', 0.395), ('poof', 0.302), ('gradhist', 0.151), ('base', 0.137), ('birds', 0.122), ('face', 0.119), ('birdlets', 0.117), ('attribute', 0.112), ('categorization', 0.111), ('bird', 0.1), ('parts', 0.095), ('verification', 0.094), ('attributes', 0.087), ('breed', 0.084), ('classifiers', 0.083), ('cells', 0.082), ('reference', 0.079), ('classes', 0.071), ('lfw', 0.064), ('wing', 0.064), ('masked', 0.061), ('labeled', 0.06), ('locations', 0.06), ('grid', 0.058), ('eye', 0.055), ('gender', 0.053), ('faces', 0.052), ('wah', 0.051), ('asian', 0.05), ('discriminative', 0.049), ('branson', 0.048), ('concatenation', 0.048), ('dogs', 0.047), ('part', 0.047), ('variant', 0.045), ('lumbi', 0.043), ('tilings', 0.043), ('features', 0.043), ('domain', 0.043), ('classification', 0.043), ('classifier', 0.042), ('feature', 0.04), ('alignment', 0.038), ('basiclevel', 0.038), ('gradient', 0.038), ('welinder', 0.037), ('kumar', 0.036), ('cropped', 0.036), ('hog', 0.036), ('aligned', 0.036), ('accuracies', 0.036), ('dataset', 0.035), ('belhumeur', 0.035), ('leg', 0.034), ('identities', 0.034), ('localized', 0.034), ('extract', 0.033), ('restrict', 0.033), ('train', 0.032), ('tiling', 0.032), ('unsigned', 0.032), ('fifteen', 0.032), ('training', 0.032), ('jacobs', 0.031), ('histograms', 0.031), ('iand', 0.031), ('farrell', 0.031), ('subordinate', 0.031), ('identification', 0.031), ('distinguish', 0.031), ('automatic', 0.03), ('berg', 0.029), ('wild', 0.029), ('subcategories', 0.029), ('chair', 0.028), ('perona', 0.028), ('learn', 0.028), ('twelve', 0.028), ('schroff', 0.028), ('region', 0.027), ('direction', 0.027), ('contiguous', 0.026), ('dog', 0.025), ('benchmark', 0.025), ('trained', 0.025), ('columbia', 0.024), ('reporting', 0.024), ('recognition', 0.024), ('unseen', 0.024), ('subset', 0.023), ('gives', 0.023), ('cell', 0.023), ('color', 0.023), ('class', 0.023), ('extracting', 0.022), ('test', 0.022), ('folds', 0.022), ('restricting', 0.022), ('svm', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="323-tfidf-1" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: From a set ofimages in aparticular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.</p><p>2 0.26573202 <a title="323-tfidf-2" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>Author: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman</p><p>Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependentfeatures computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.</p><p>3 0.24593593 <a title="323-tfidf-3" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>Author: Anelia Angelova, Shenghuo Zhu</p><p>Abstract: We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also ‘zoom in ’ on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios [4, 21]. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging largescale flower dataset, containing 578 species of flowers and 250,000 images.</p><p>4 0.14367646 <a title="323-tfidf-4" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>5 0.14136329 <a title="323-tfidf-5" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>Author: Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</p><p>Abstract: Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently, and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query, which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions without explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discrimina- tive binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improveperformance significantly over several strong baselines, while being an order of magnitude faster than exhaustively searching through all possible conjunctions.</p><p>6 0.14134277 <a title="323-tfidf-6" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>7 0.12039205 <a title="323-tfidf-7" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>8 0.11715969 <a title="323-tfidf-8" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>9 0.1165674 <a title="323-tfidf-9" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>10 0.11460759 <a title="323-tfidf-10" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>11 0.10905342 <a title="323-tfidf-11" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>12 0.10545578 <a title="323-tfidf-12" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>13 0.10489377 <a title="323-tfidf-13" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>14 0.099209107 <a title="323-tfidf-14" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>15 0.097601034 <a title="323-tfidf-15" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>16 0.094910875 <a title="323-tfidf-16" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>17 0.093704492 <a title="323-tfidf-17" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>18 0.092535242 <a title="323-tfidf-18" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>19 0.091826148 <a title="323-tfidf-19" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>20 0.09112455 <a title="323-tfidf-20" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, -0.113), (2, -0.043), (3, -0.023), (4, 0.1), (5, 0.054), (6, -0.123), (7, 0.025), (8, 0.149), (9, -0.009), (10, -0.021), (11, 0.003), (12, 0.046), (13, -0.0), (14, 0.036), (15, -0.017), (16, 0.012), (17, -0.062), (18, -0.041), (19, 0.056), (20, -0.024), (21, 0.037), (22, 0.047), (23, -0.029), (24, 0.051), (25, 0.064), (26, 0.04), (27, 0.052), (28, -0.051), (29, -0.046), (30, 0.009), (31, 0.092), (32, 0.051), (33, 0.028), (34, 0.122), (35, 0.006), (36, 0.013), (37, -0.04), (38, -0.037), (39, 0.106), (40, -0.016), (41, -0.005), (42, -0.055), (43, -0.072), (44, -0.018), (45, -0.026), (46, 0.113), (47, 0.093), (48, -0.123), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90860462 <a title="323-lsi-1" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: From a set ofimages in aparticular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.</p><p>2 0.73434591 <a title="323-lsi-2" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>Author: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman</p><p>Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependentfeatures computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.</p><p>3 0.66354644 <a title="323-lsi-3" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>4 0.64129519 <a title="323-lsi-4" href="./cvpr-2013-Fine-Grained_Crowdsourcing_for_Fine-Grained_Recognition.html">174 cvpr-2013-Fine-Grained Crowdsourcing for Fine-Grained Recognition</a></p>
<p>Author: Jia Deng, Jonathan Krause, Li Fei-Fei</p><p>Abstract: Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles ” that reveals discriminative features humans use. The player’s goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ( “bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.</p><p>5 0.60487866 <a title="323-lsi-5" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>Author: Anelia Angelova, Shenghuo Zhu</p><p>Abstract: We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also ‘zoom in ’ on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios [4, 21]. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging largescale flower dataset, containing 578 species of flowers and 250,000 images.</p><p>6 0.59719729 <a title="323-lsi-6" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>7 0.59693062 <a title="323-lsi-7" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>8 0.57457536 <a title="323-lsi-8" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>9 0.57290924 <a title="323-lsi-9" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>10 0.57187003 <a title="323-lsi-10" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>11 0.56915152 <a title="323-lsi-11" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>12 0.56554377 <a title="323-lsi-12" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>13 0.56535065 <a title="323-lsi-13" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>14 0.56494522 <a title="323-lsi-14" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>15 0.56228489 <a title="323-lsi-15" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>16 0.55184746 <a title="323-lsi-16" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>17 0.55130458 <a title="323-lsi-17" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>18 0.55007517 <a title="323-lsi-18" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>19 0.53713453 <a title="323-lsi-19" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>20 0.52756429 <a title="323-lsi-20" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.105), (16, 0.019), (19, 0.019), (26, 0.048), (27, 0.049), (33, 0.27), (67, 0.071), (69, 0.043), (77, 0.018), (80, 0.012), (83, 0.184), (87, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91534811 <a title="323-lda-1" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>Author: Amit Agrawal, Srikumar Ramalingam</p><p>Abstract: Imaging systems consisting of a camera looking at multiple spherical mirrors (reflection) or multiple refractive spheres (refraction) have been used for wide-angle imaging applications. We describe such setups as multi-axial imaging systems, since a single sphere results in an axial system. Assuming an internally calibrated camera, calibration of such multi-axial systems involves estimating the sphere radii and locations in the camera coordinate system. However, previous calibration approaches require manual intervention or constrained setups. We present a fully automatic approach using a single photo of a 2D calibration grid. The pose of the calibration grid is assumed to be unknown and is also recovered. Our approach can handle unconstrained setups, where the mirrors/refractive balls can be arranged in any fashion, not necessarily on a grid. The axial nature of rays allows us to compute the axis of each sphere separately. We then show that by choosing rays from two or more spheres, the unknown pose of the calibration grid can be obtained linearly and independently of sphere radii and locations. Knowing the pose, we derive analytical solutions for obtaining the sphere radius and location. This leads to an interesting result that 6-DOF pose estimation of a multi-axial camera can be done without the knowledge of full calibration. Simulations and real experiments demonstrate the applicability of our algorithm.</p><p>2 0.89398265 <a title="323-lda-2" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>Author: Ramakrishna Kakarala, Prabhu Kaliamoorthi, Vittal Premachandran</p><p>Abstract: We show that bilateral symmetry plane estimation for three-dimensional (3-D) shapes may be carried out accurately, and efficiently, in the spherical harmonic domain. Our methods are valuable for applications where spherical harmonic expansion is already employed, such as 3-D shape registration, morphometry, and retrieval. We show that the presence of bilateral symmetry in the 3-D shape is equivalent to a linear phase structure in the corresponding spherical harmonic coefficients, and provide algorithms for estimating the orientation of the symmetry plane. The benefit of using spherical harmonic phase is that symmetry estimation reduces to matching a compact set of descriptors, without the need to solve a correspondence problem. Our methods work on point clouds as well as large-scale mesh models of 3-D shapes.</p><p>3 0.88167065 <a title="323-lda-3" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>Author: Raffay Hamid, Dennis Decoste, Chih-Jen Lin</p><p>Abstract: We present a robust and efficient technique for matching dense sets of points undergoing non-rigid spatial transformations. Our main intuition is that the subset of points that can be matched with high confidence should be used to guide the matching procedure for the rest. We propose a novel algorithm that incorporates these high-confidence matches as a spatial prior to learn a discriminative subspace that simultaneously encodes both the feature similarity as well as their spatial arrangement. Conventional subspace learning usually requires spectral decomposition of the pair-wise distance matrix across the point-sets, which can become inefficient even for moderately sized problems. To this end, we propose the use of random projections for approximate subspace learning, which can provide significant time improvements at the cost of minimal precision loss. This efficiency gain allows us to iteratively find and remove high-confidence matches from the point sets, resulting in high recall. To show the effectiveness of our approach, we present a systematic set of experiments and results for the problem of dense non-rigid image-feature matching.</p><p>same-paper 4 0.86974633 <a title="323-lda-4" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: From a set ofimages in aparticular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.</p><p>5 0.86057287 <a title="323-lda-5" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>Author: Manmohan Chandraker, Dikpal Reddy, Yizhou Wang, Ravi Ramamoorthi</p><p>Abstract: We present a theory that addresses the problem of determining shape from the (small or differential) motion of an object with unknown isotropic reflectance, under arbitrary unknown distant illumination, , for both orthographic and perpsective projection. Our theory imposes fundamental limits on the hardness of surface reconstruction, independent of the method involved. Under orthographic projection, we prove that three differential motions suffice to yield an invariant that relates shape to image derivatives, regardless of BRDF and illumination. Under perspective projection, we show that four differential motions suffice to yield depth and a linear constraint on the surface gradient, with unknown BRDF and lighting. Further, we delineate the topological classes up to which reconstruction may be achieved using the invariants. Finally, we derive a general stratification that relates hardness of shape recovery to scene complexity. Qualitatively, our invariants are homogeneous partial differential equations for simple lighting and inhomogeneous for complex illumination. Quantitatively, our framework shows that the minimal number of motions required to resolve shape is greater for more complex scenes. Prior works that assume brightness constancy, Lambertian BRDF or a known directional light source follow as special cases of our stratification. We illustrate with synthetic and real data how potential reconstruction methods may exploit our framework.</p><p>6 0.85918778 <a title="323-lda-6" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>7 0.84736788 <a title="323-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.84533638 <a title="323-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.84529889 <a title="323-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.84448922 <a title="323-lda-10" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>11 0.84390122 <a title="323-lda-11" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>12 0.8438803 <a title="323-lda-12" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>13 0.84325844 <a title="323-lda-13" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>14 0.8423599 <a title="323-lda-14" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>15 0.84207636 <a title="323-lda-15" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>16 0.84203523 <a title="323-lda-16" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>17 0.84188056 <a title="323-lda-17" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>18 0.84184003 <a title="323-lda-18" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>19 0.84159935 <a title="323-lda-19" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>20 0.8414253 <a title="323-lda-20" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
