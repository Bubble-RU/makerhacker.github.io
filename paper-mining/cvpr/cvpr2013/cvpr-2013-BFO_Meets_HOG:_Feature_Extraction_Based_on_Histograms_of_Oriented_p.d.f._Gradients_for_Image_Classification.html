<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-53" href="#">cvpr2013-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</h1>
<br/><p>Source: <a title="cvpr-2013-53-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kobayashi_BFO_Meets_HOG_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>Reference: <a title="cvpr-2013-53-reference" href="../cvpr2013_reference/cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. [sent-7, score-0.241]
</p><p>2 Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p. [sent-9, score-0.379]
</p><p>3 f gradients via orientation coding followed by aggregation of the orientation codes. [sent-24, score-0.843]
</p><p>4 In the experiments on object recog-  nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods. [sent-26, score-0.226]
</p><p>5 The image classification is frequently addressed in the framework of bag-of-features (BoF) [9] owing to the advances of the local descriptors such as SIFT [23]. [sent-30, score-0.223]
</p><p>6 BoF is based on the local descriptors densely extracted in an image which are further coded into visual words and pro-  (1)Inputimage (2)LocaSlIFdTescpariepto128rs  (3) p. [sent-31, score-0.513]
</p><p>7 At dense spatial grid points in an input image (1), a plenty of SIFT local descriptors are extracted (2). [sent-36, score-0.301]
</p><p>8 f) in the descriptor space is estimated by applying kernel density estimator to those descriptors (3). [sent-39, score-0.353]
</p><p>9 Those codes are aggregated around respective visual words (5) and the aggregated codes are finally concatenated into the image feature vector (6). [sent-43, score-0.669]
</p><p>10 duces as the image feature the histogram of the visual words that appear in the image. [sent-44, score-0.222]
</p><p>11 Thus, BoF mainly consists of the four procedures extracting local descriptors, coding them into words, aggregating (pooling) the words into the histogram, and classifying the histogram feature vector. [sent-45, score-0.579]
</p><p>12 7 7 7 4 4 4 75 575  In the other direction, the BoF has been recently extended to the methods aggregating vectors [16, 24, 35, 17], not the word codes. [sent-47, score-0.231]
</p><p>13 The method of super vector coding [35] was proposed afterwards to approximate the (nonlinear) classifier function by piece-wise linear models. [sent-49, score-0.566]
</p><p>14 Although those two methods are defined in different scenarios, they actually result in the same procedure that aggregates the difference vectors between local descriptors and their nearby visual word centers. [sent-50, score-0.363]
</p><p>15 The image features are obtained by concatenating those difference vectors aggregated around respective words, in contrast to the standard BoF which simply counts the occurrence ofthe visual words in the histograms. [sent-51, score-0.404]
</p><p>16 This vector aggregation based method significantly improves the performance on object recognition [16, 24, 35] compared to the BoF approach. [sent-52, score-0.241]
</p><p>17 f is estimated by applying kernel density estimator [3 1] to the densely extracted local descriptors without assuming any specific probabilistic models such as GMM. [sent-61, score-0.429]
</p><p>18 Through computing the gradients, the mean shift vectors [7] are naturally induced and those vectors are coded in terms of their orientations. [sent-72, score-0.245]
</p><p>19 Those orientation codes are finally aggregated around respective visual words into the histograms similarly to the above-mentioned methods, Fisher kernel [16, 24] and super vector coding [35] which are also shown to be a special case of the proposed method (Sec. [sent-73, score-1.221]
</p><p>20 Proposed method In the BoF framework [9], an image is represented by a plenty (bag) of local descriptors densely extracted in the image, and then is finally characterized by a histogram of visual words quantizing the underlying probability distribution of the local descriptors. [sent-77, score-0.591]
</p><p>21 Probability density function in BoF From an input image, N local descriptors, such as SIFT descriptors [23], are extracted at dense spatial positions with various scales; those are denoted by xi ∈ Rd, i = 1, · · · , N. [sent-87, score-0.412]
</p><p>22 to W discretely represent tohsee image, we apply k soern fearl density estimator [3 1] to obtain the following (continuous) probability density function (p. [sent-90, score-0.244]
</p><p>23 , tfe (sz t)h e= ( Cd,h exp(−2zh) wofiitleh the bandwidth parameter h, say h = 0. [sent-97, score-0.176]
</p><p>24 f gradients The gradients, especially their orientations, effectively characterize the “shape” of the p. [sent-106, score-0.2]
</p><p>25 (z), the derivative ofthe profile function fw (hze)r,e ewgh(izc)h =is a−ls2of the profile [7]. [sent-119, score-0.238]
</p><p>26 f gradient vectors themselves since the gradient orientation information is canceled out via summation. [sent-122, score-0.449]
</p><p>27 Thus, we consider the orientation coding of the p. [sent-123, score-0.413]
</p><p>28 The orientation coding is usually applied to image gradients such as in HOG[10] and SIFT[23]. [sent-126, score-0.497]
</p><p>29 The orientation of the image gradients is coded based on a lot of the bases (bins) that uniformly cover 2-D spatial orientations, forming overcomplete set to describe any oriented gradients. [sent-127, score-0.566]
</p><p>30 However, it is infeasible to use those overcomplete set of (uniform) bases for coding high-dimensional gradient orientations; e. [sent-128, score-0.508]
</p><p>31 Thereby, we obtain the d orthonormal basis (eigen) vectors, uj ,j = 1, ··· , d, uj? [sent-141, score-0.151]
</p><p>32 Along each basis vector, we can c1o,·ns·i·d ,edr ,twuo orientations, positive and negative ones, which totally provides 2d orientation bins by C(v; {uj}jd=1) = [max(u1? [sent-143, score-0.223]
</p><p>33 This coding produces rather sparse orientation codes in which at most d components are nonzero, and the code has a unit sum for ? [sent-149, score-0.559]
</p><p>34 tation coding, uth}e gradient vectors, especially oriented in opposite directions, would be canceled out via aggregation. [sent-159, score-0.226]
</p><p>35 rP sCimAp produces et ohem eigenvalues ej as well as the eigenvectors uj employed for the orientation bases. [sent-171, score-0.355]
</p><p>36 The rare orientations would be more discriminative than the common ones [28], and thus the weighting (4) improves the discriminative power. [sent-197, score-0.269]
</p><p>37 f gradient orientation codes The orientation codes (4) are finally aggregated around the visual words which are basis points (cluster centers) in the local descriptor space Rd. [sent-202, score-0.951]
</p><p>38 We define the aggregation in the following continuous form as is the case with the p. [sent-203, score-0.195]
</p><p>39 dx,  (5)  where W(x, y) is the weighting function indicating how the local descriptor x contributes to the word μ as defined in the later, and the magnitude and the orientation of the p. [sent-212, score-0.415]
</p><p>40 To reduce tghrea dcieonntti vnueocutosr f ∇ormp into a tractable discrete one, it should  be noted that the local descriptors xi , i= 1, · · · , N are assumed to be randomly sampled according t o1 ,th··e· p. [sent-215, score-0.289]
</p><p>41 (7)  This is a summation weighted by the inverse of the probability pf (xi). [sent-236, score-0.37]
</p><p>42 The formulation (7) favorably suppresses the effect of the frequent local descriptors of high probability which are common across the categories and less discriminative for classification [28]. [sent-237, score-0.384]
</p><p>43 proximately applied to the normalization since pf (x) ≈ pg(x). [sent-246, score-0.364]
</p><p>44 Note that the normalized gradient (x) is identical to the mean shift vector [7] which  ∇ˆpf  has be∇enp usually used for clustering and its favorable properties are discussed in [7]. [sent-247, score-0.248]
</p><p>45 By introducing the normalized gradient (8) into (7), we finally obtain the aggregation form to construct features as the histogram of the oriented p. [sent-248, score-0.361]
</p><p>46 Let μk , k = 1, · · · , M be the k-th visual word center, and the aggregat1io,·n· a·r ,oMund b μk ies given by  dk=N1? [sent-251, score-0.148]
</p><p>47 (9) These features around the respective visual words are concatenated into the final feature vector; d = [d1? [sent-259, score-0.295]
</p><p>48 We define the weighting function W(x, μ) from the local descriptor x to the visual word μ. [sent-267, score-0.295]
</p><p>49 We simply apply the following method based on the distance ratio, though the other word coding methods [12, 15, 33, 32] are also applicable. [sent-268, score-0.379]
</p><p>50 of spatial pyramid matching [19], we employ the spatial pooling using spatial bins (partitions) in order to roughly take into account the spatial alignment in the image. [sent-298, score-0.259]
</p><p>51 f of densely extracted local descriptors in the BoF framework. [sent-313, score-0.231]
</p><p>52 f is composed of (discrete) samples of local descriptors and thus defined in a high dimensional space, while HOG [10]/SIFT [23] directly deals with an image, a (continuous) image pixel function well defined in 2-D spatial dimensions. [sent-318, score-0.199]
</p><p>53 In the BoF framework, some methods based on vector aggregation have been proposed, showing promising performances on object recognition, such as Fisher kernel [16, 24] and super vector coding [35]. [sent-320, score-1.013]
</p><p>54 The Fisher kernel [16] is derived from the information geometry [2] with Gaussian mixture model (GMM), and the super coding [35] is motivated by the piece-wise linear approximation of the (nonlinear) classification function. [sent-321, score-0.654]
</p><p>55 These methods eventually result in aggregating the difference vectors xi −μk around the visual word μk as shown in Table 2. [sent-322, score-0.397]
</p><p>56 f pf(x)image pixel function I(r)  variable  locaxl d ∈es Rcrdiptor  orientation code by aggregation at  PCxA ∈ b Rases visual words  2-D pors ∈itio Rn2 vector predefined o ∈ri Rentation bins predefined spatial bins  Table 2. [sent-326, score-0.737]
</p><p>57 Super vector coding [35] has a (scalar) parameter s. [sent-332, score-0.308]
</p><p>58 f gradient vector ∇pf (μk) in (2), or mean shift vector ∇ˆpf (μk) einn (8), atto trh ∇e wpord μk; thus, we can say that the Fi∇shper kernel [16] and super coding [35] are viewed as the special case of the proposed method. [sent-355, score-0.856]
</p><p>59 The gradient vectors only at those sparse word points would be less discriminative for characterizing the distribution of local descriptors; especially, the information of the distribution around the word is canceled  out by simply aggregating those difference vectors as shown in Fig. [sent-356, score-0.655]
</p><p>60 The Fisher kernel [16, 24] additionally employs the variances of the difference vector in order to compensate it, but the integration of those different kinds of statistical quantities is itself difficult, though those are simply concatenated in [16, 24]. [sent-358, score-0.169]
</p><p>61 f) of the local descriptors by means of the histograms of the p. [sent-361, score-0.16]
</p><p>62 f gradient orientation which is densely computed at every sample point, as shown in Fig. [sent-363, score-0.281]
</p><p>63 The SIFT local descriptors [23] are extracted at dense spatial grid points in 4 pixel step with three scales of {16, 24, 32} pixels. [sent-368, score-0.23]
</p><p>64 Visual words are o wbtiathin tehdr by applying 1k6-,m2e4a,n3s2 clustering tisou one omridl-s lion local descriptors which are randomly sampled from the training images. [sent-369, score-0.321]
</p><p>65 2), the bases uj and the weights ej are obtained by applying PCA to all the gradient vectors in the training images. [sent-372, score-0.431]
</p><p>66 While the aggregated difference vectors, denoted by the large arrows, improperly coincide for different distributions (a), the aggregation (histogram) of the orientation codes discriminatively describes the distribution around the word (b). [sent-377, score-0.661]
</p><p>67 Due to the curse of dimensionality [26], such adaptive bandwidth selection becomes less effective in the higher-dimensional space, since the data samples are sparsely distributed around each sample point. [sent-395, score-0.172]
</p><p>68 Performance analysis on PASCAL-VOC2007 (a) Bandwidth in the profile f (b) Orientation coding h = 10. [sent-414, score-0.381]
</p><p>69 2, those orientations are coded by using the PCA basis vectors. [sent-459, score-0.229]
</p><p>70 In such a case of complete set of orientation bases, which is smaller than overcomplete one, the data-driven bases provided by PCA effectively code the orientations. [sent-462, score-0.376]
</p><p>71 The third issue is related to the weighting of the orientation codes as described in the last paragraph of Sec. [sent-464, score-0.325]
</p><p>72 4  formance is improved by the weighting which suppresses the orientations commonly occurring across the categories while enhancing the less-frequent but discriminative ones. [sent-492, score-0.357]
</p><p>73 f gradient ∇ˆpf that corresponds to the mean-shift vector [7] is naturally ∇inpduced via the aggregation considering sampling procedure as described in Sec. [sent-498, score-0.331]
</p><p>74 The method employing ∇pf amounts to the aggregation (5) weighted by tphleo probability pf which would highly enhance the sam-  ples (local descriptors) frequently found in the image. [sent-504, score-0.565]
</p><p>75 On the other hand, the normalized gradient ∇ˆpf leads to the uniform aggregation (5) over x; even the less-frequent samples which would be discriminative are fairly treated. [sent-506, score-0.319]
</p><p>76 We show the performances on various numbers of words M ∈ {128, 256, 512, 1024} oinn Fig. [sent-511, score-0.296]
</p><p>77 4u sw nituhm comparison dtos Mthe ∈m {et1h2o8d,s2 5o6f, 5Fi1s2h,e1r0 k2e4r}nel [24], super vector coding [35] and the standard BoF. [sent-512, score-0.566]
</p><p>78 The proposed method produces stably high performances even on the small amount of words, exhibiting significant improvement over the BoF and the other two methods. [sent-513, score-0.171]
</p><p>79 The proposed feature is twice the dimensionality of super vector coding [35] when the same number of words are used, but our method produces superior performances to [35] under the same dimensionality; e. [sent-514, score-0.926]
</p><p>80 , the proposed method of 256 words is superior to super vector coding of 512 words. [sent-516, score-0.755]
</p><p>81 f gradient orientations more effectively characterizes the distribution of the local descriptors, compared to the difference vectors. [sent-519, score-0.275]
</p><p>82 [5] Fisher kernel (256 words) [24] super coding (1024 words) [35] ours (256 words)  83. [sent-534, score-0.591]
</p><p>83 Since the performance is sufficiently improved by 256 words, in the following experiments, we apply the proposed method with 256 words and similarly Fisher kernel with 256 words and super vector coding with 1024 words. [sent-546, score-0.959]
</p><p>84 We can see that the proposed method significantly outperforms the others including Fisher kernel [24] and super vector coding [35]; the proposed method improves the performance by 17% over the recently developed method [4]. [sent-596, score-0.637]
</p><p>85 The vector aggregation based methods effectively work compared to the other methods; especially, the proposed method significantly outperforms others, though it is comparable to the super vector coding [35]. [sent-605, score-0.85]
</p><p>86 However, it should be noted that the dimensionality of the proposed feature with 256 words is halfofthat in super vector coding with 1024 words, which speeds up the classification. [sent-606, score-0.755]
</p><p>87 The proposed method significantly outperforms the other methods [13, 18, 29, 12]; the performances are improved by 3 ∼ 5% over the method [13] fwohrmicahn uses multiple types o 3f f∼ea 5tu%res o vwerhi tlhee t mhee proposed method is based on single type of SIFT local descriptors. [sent-635, score-0.166]
</p><p>88 These experimental results demonstrate  that the proposed method produces favorable performances compared to the other existing methods on various tasks of object recognition and scene/event classification using various datasets. [sent-637, score-0.301]
</p><p>89 Thus, the parameter setting, especially the bandwidth h = 0. [sent-639, score-0.165]
</p><p>90 We obtain greater performance improvements over the methods of super vector coding [35] and Fisher kernel [24] on PASCAL-VOC2007, MIT-Scene and Caltech-256 datasets. [sent-641, score-0.637]
</p><p>91 This is because the performances on the remaining datasets of Scene-1 5 and UIUC-sports are saturated in the proposed method and super coding [35]. [sent-642, score-0.681]
</p><p>92 Those two datasets contain smaller number of categories, rendering rather easier classification tasks than the other three datasets; actually, the performances produced by the methods on Scene-1 5 and UIUC-sports are much higher (about 90%) than those on the other three datasets. [sent-643, score-0.224]
</p><p>93 The proposed method improves the performances more significantly on the more challenging (difficult) datasets due to its high discriminative power. [sent-644, score-0.195]
</p><p>94 In the framework of BoF  which extracts a plenty of local descriptors from an image, the proposed method is built upon the probability density function (p. [sent-647, score-0.348]
</p><p>95 f) obtained by applying kernel density estimator to those local descriptors. [sent-649, score-0.229]
</p><p>96 f, which are subsequently coded and aggregated into the orientation histograms. [sent-654, score-0.321]
</p><p>97 The variable bandwidth mean shift and data-driven scale selection. [sent-699, score-0.183]
</p><p>98 Local features are not lonely - laplacian sparse coding for image classification. [sent-726, score-0.262]
</p><p>99 Linear spatial pyramid matching using sparse coding for image classification. [sent-854, score-0.33]
</p><p>100 Image classification using super-vector coding of local image descriptors. [sent-867, score-0.356]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pf', 0.334), ('bof', 0.311), ('ppff', 0.296), ('coding', 0.262), ('super', 0.258), ('aggregation', 0.195), ('fisher', 0.165), ('words', 0.161), ('orientation', 0.151), ('bandwidth', 0.138), ('performances', 0.135), ('descriptors', 0.129), ('uj', 0.123), ('profile', 0.119), ('bases', 0.118), ('word', 0.117), ('orientations', 0.111), ('xi', 0.101), ('gradient', 0.09), ('weighting', 0.09), ('coded', 0.09), ('codes', 0.084), ('gradients', 0.084), ('density', 0.081), ('aggregated', 0.08), ('pca', 0.077), ('dixit', 0.072), ('kernel', 0.071), ('plenty', 0.071), ('winner', 0.071), ('pages', 0.07), ('xx', 0.068), ('favorable', 0.067), ('jd', 0.066), ('classification', 0.063), ('canceled', 0.063), ('aggregating', 0.059), ('categories', 0.057), ('vectors', 0.055), ('sift', 0.054), ('characterize', 0.046), ('estimator', 0.046), ('vector', 0.046), ('oriented', 0.046), ('ej', 0.045), ('shift', 0.045), ('hog', 0.045), ('bins', 0.044), ('kobayashi', 0.044), ('effectively', 0.043), ('respective', 0.043), ('bandwidths', 0.042), ('gmm', 0.041), ('dx', 0.041), ('densely', 0.04), ('category', 0.04), ('spatial', 0.039), ('overcomplete', 0.038), ('say', 0.038), ('xxii', 0.037), ('probability', 0.036), ('procedures', 0.036), ('produces', 0.036), ('ud', 0.036), ('classifications', 0.036), ('indoor', 0.035), ('sl', 0.034), ('discriminative', 0.034), ('around', 0.034), ('suppresses', 0.034), ('geometrical', 0.033), ('boureau', 0.033), ('quattoni', 0.032), ('local', 0.031), ('extracted', 0.031), ('visual', 0.031), ('comaniciu', 0.031), ('enhancing', 0.031), ('normalization', 0.03), ('table', 0.03), ('pooling', 0.03), ('histogram', 0.03), ('bo', 0.03), ('iw', 0.03), ('study', 0.029), ('pyramid', 0.029), ('somewhat', 0.029), ('basis', 0.028), ('superior', 0.028), ('noted', 0.028), ('especially', 0.027), ('ri', 0.027), ('event', 0.027), ('trials', 0.026), ('kinds', 0.026), ('descriptor', 0.026), ('code', 0.026), ('datasets', 0.026), ('yu', 0.026), ('concatenated', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="53-tfidf-1" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>2 0.24748753 <a title="53-tfidf-2" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>3 0.13878104 <a title="53-tfidf-3" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Peng Wang, Jingdong Wang, Gang Zeng, Weiwei Xu, Hongbin Zha, Shipeng Li</p><p>Abstract: In visual recognition tasks, the design of low level image feature representation is fundamental. The advent of local patch features from pixel attributes such as SIFT and LBP, has precipitated dramatic progresses. Recently, a kernel view of these features, called kernel descriptors (KDES) [1], generalizes the feature design in an unsupervised fashion and yields impressive results. In this paper, we present a supervised framework to embed the image level label information into the design of patch level kernel descriptors, which we call supervised kernel descriptors (SKDES). Specifically, we adopt the broadly applied bag-of-words (BOW) image classification pipeline and a large margin criterion to learn the lowlevel patch representation, which makes the patch features much more compact and achieve better discriminative ability than KDES. With this method, we achieve competitive results over several public datasets comparing with stateof-the-art methods.</p><p>4 0.13824329 <a title="53-tfidf-4" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>5 0.13440576 <a title="53-tfidf-5" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>Author: Qiang Chen, Zheng Song, Rogerio Feris, Ankur Datta, Liangliang Cao, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In recent years, efficiency of large-scale object detection has arisen as an important topic due to the exponential growth in the size of benchmark object detection datasets. Most current object detection methods focus on improving accuracy of large-scale object detection with efficiency being an afterthought. In this paper, we present the Efficient Maximum Appearance Search (EMAS) model which is an order of magnitude faster than the existing state-of-the-art large-scale object detection approaches, while maintaining comparable accuracy. Our EMAS model consists of representing an image as an ensemble of densely sampled feature points with the proposed Pointwise Fisher Vector encoding method, so that the learnt discriminative scoring function can be applied locally. Consequently, the object detection problem is transformed into searching an image sub-area for maximum local appearance probability, thereby making EMAS an order of magnitude faster than the traditional detection methods. In addition, the proposed model is also suitable for incorporating global context at a negligible extra computational cost. EMAS can also incorporate fusion of multiple features, which greatly improves its performance in detecting multiple object categories. Our experiments show that the proposed algorithm can perform detection of 1000 object classes in less than one minute per image on the Image Net ILSVRC2012 dataset and for 107 object classes in less than 5 seconds per image for the SUN09 dataset using a single CPU.</p><p>6 0.12381136 <a title="53-tfidf-6" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>7 0.11601079 <a title="53-tfidf-7" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>8 0.1142453 <a title="53-tfidf-8" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>9 0.11200292 <a title="53-tfidf-9" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>10 0.10807315 <a title="53-tfidf-10" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>11 0.10791075 <a title="53-tfidf-11" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>12 0.10055079 <a title="53-tfidf-12" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>13 0.099796318 <a title="53-tfidf-13" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>14 0.097939603 <a title="53-tfidf-14" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>15 0.097513646 <a title="53-tfidf-15" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>16 0.095464878 <a title="53-tfidf-16" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>17 0.092188157 <a title="53-tfidf-17" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>18 0.091459163 <a title="53-tfidf-18" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>19 0.090091527 <a title="53-tfidf-19" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>20 0.089973986 <a title="53-tfidf-20" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, -0.069), (2, -0.05), (3, 0.063), (4, 0.008), (5, 0.018), (6, -0.069), (7, -0.023), (8, -0.083), (9, -0.028), (10, -0.05), (11, -0.028), (12, 0.044), (13, -0.03), (14, 0.051), (15, -0.012), (16, -0.044), (17, -0.008), (18, 0.103), (19, -0.044), (20, 0.059), (21, 0.015), (22, 0.1), (23, -0.035), (24, -0.033), (25, 0.193), (26, -0.049), (27, 0.097), (28, -0.042), (29, -0.063), (30, 0.009), (31, 0.054), (32, -0.067), (33, 0.029), (34, -0.004), (35, -0.007), (36, -0.048), (37, 0.14), (38, 0.102), (39, -0.102), (40, -0.015), (41, -0.079), (42, 0.022), (43, -0.03), (44, 0.045), (45, -0.101), (46, 0.009), (47, -0.05), (48, -0.096), (49, -0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95380867 <a title="53-lsi-1" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>2 0.85647708 <a title="53-lsi-2" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>3 0.7780751 <a title="53-lsi-3" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>Author: Peng Wang, Jingdong Wang, Gang Zeng, Weiwei Xu, Hongbin Zha, Shipeng Li</p><p>Abstract: In visual recognition tasks, the design of low level image feature representation is fundamental. The advent of local patch features from pixel attributes such as SIFT and LBP, has precipitated dramatic progresses. Recently, a kernel view of these features, called kernel descriptors (KDES) [1], generalizes the feature design in an unsupervised fashion and yields impressive results. In this paper, we present a supervised framework to embed the image level label information into the design of patch level kernel descriptors, which we call supervised kernel descriptors (SKDES). Specifically, we adopt the broadly applied bag-of-words (BOW) image classification pipeline and a large margin criterion to learn the lowlevel patch representation, which makes the patch features much more compact and achieve better discriminative ability than KDES. With this method, we achieve competitive results over several public datasets comparing with stateof-the-art methods.</p><p>4 0.71468759 <a title="53-lsi-4" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>Author: Bin Zhao, Eric P. Xing</p><p>Abstract: Many vision tasks require a multi-class classifier to discriminate multiple categories, on the order of hundreds or thousands. In this paper, we propose sparse output coding, a principled way for large-scale multi-class classification, by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem. Specifically, sparse output coding is composed of two steps: efficient coding matrix learning with scalability to thousands of classes, and probabilistic decoding. Empirical results on object recognition and scene classification demonstrate the effectiveness ofour proposed approach.</p><p>5 0.69727641 <a title="53-lsi-5" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>Author: Xavier Boix, Michael Gygli, Gemma Roig, Luc Van_Gool</p><p>Abstract: The representation of local image patches is crucial for the good performance and efficiency of many vision tasks. Patch descriptors have been designed to generalize towards diverse variations, depending on the application, as well as the desired compromise between accuracy and efficiency. We present a novel formulation of patch description, that serves such issues well. Sparse quantization lies at its heart. This allows for efficient encodings, leading to powerful, novel binary descriptors, yet also to the generalization of existing descriptors like SIFTorBRIEF. We demonstrate the capabilities of our formulation for both keypoint matching and image classification. Our binary descriptors achieve state-of-the-art results for two keypoint matching benchmarks, namely those by Brown [6] and Mikolajczyk [18]. For image classification, we propose new descriptors that perform similar to SIFT on Caltech101 [10] and PASCAL VOC07 [9].</p><p>6 0.68768692 <a title="53-lsi-6" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>7 0.67758316 <a title="53-lsi-7" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>8 0.67273033 <a title="53-lsi-8" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>9 0.66885549 <a title="53-lsi-9" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>10 0.66884023 <a title="53-lsi-10" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>11 0.64718384 <a title="53-lsi-11" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>12 0.64473706 <a title="53-lsi-12" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>13 0.64205086 <a title="53-lsi-13" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>14 0.64031941 <a title="53-lsi-14" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>15 0.6375457 <a title="53-lsi-15" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>16 0.62941051 <a title="53-lsi-16" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>17 0.62873387 <a title="53-lsi-17" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>18 0.62036455 <a title="53-lsi-18" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>19 0.61630446 <a title="53-lsi-19" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>20 0.60115343 <a title="53-lsi-20" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.109), (16, 0.033), (26, 0.045), (28, 0.018), (33, 0.332), (35, 0.151), (39, 0.013), (67, 0.083), (69, 0.066), (87, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96338534 <a title="53-lda-1" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>Author: Xavier Boix, Michael Gygli, Gemma Roig, Luc Van_Gool</p><p>Abstract: The representation of local image patches is crucial for the good performance and efficiency of many vision tasks. Patch descriptors have been designed to generalize towards diverse variations, depending on the application, as well as the desired compromise between accuracy and efficiency. We present a novel formulation of patch description, that serves such issues well. Sparse quantization lies at its heart. This allows for efficient encodings, leading to powerful, novel binary descriptors, yet also to the generalization of existing descriptors like SIFTorBRIEF. We demonstrate the capabilities of our formulation for both keypoint matching and image classification. Our binary descriptors achieve state-of-the-art results for two keypoint matching benchmarks, namely those by Brown [6] and Mikolajczyk [18]. For image classification, we propose new descriptors that perform similar to SIFT on Caltech101 [10] and PASCAL VOC07 [9].</p><p>2 0.94040197 <a title="53-lda-2" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>Author: Xiaobai Liu, Liang Lin, Alan L. Yuille</p><p>Abstract: In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.</p><p>same-paper 3 0.93148577 <a title="53-lda-3" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>4 0.92248183 <a title="53-lda-4" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>Author: Wongun Choi, Yu-Wei Chao, Caroline Pantofaru, Silvio Savarese</p><p>Abstract: Visual scene understanding is a difficult problem interleaving object detection, geometric reasoning and scene classification. We present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the 3D Geometric Phrase Model which captures the semantic and geometric relationships between objects whichfrequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings from a single image, while also improving individual object detections.</p><p>5 0.91982102 <a title="53-lda-5" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>Author: Yingying Zhu, Nandita M. Nayak, Amit K. Roy-Chowdhury</p><p>Abstract: In thispaper, rather than modeling activities in videos individually, we propose a hierarchical framework that jointly models and recognizes related activities using motion and various context features. This is motivated from the observations that the activities related in space and time rarely occur independently and can serve as the context for each other. Given a video, action segments are automatically detected using motion segmentation based on a nonlinear dynamical model. We aim to merge these segments into activities of interest and generate optimum labels for the activities. Towards this goal, we utilize a structural model in a max-margin framework that jointly models the underlying activities which are related in space and time. The model explicitly learns the duration, motion and context patterns for each activity class, as well as the spatio-temporal relationships for groups of them. The learned model is then used to optimally label the activities in the testing videos using a greedy search method. We show promising results on the VIRAT Ground Dataset demonstrating the benefit of joint modeling and recognizing activities in a wide-area scene.</p><p>6 0.91905963 <a title="53-lda-6" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>7 0.91898483 <a title="53-lda-7" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>8 0.91830373 <a title="53-lda-8" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>9 0.91803783 <a title="53-lda-9" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>10 0.91798258 <a title="53-lda-10" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>11 0.9173876 <a title="53-lda-11" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>12 0.91731751 <a title="53-lda-12" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>13 0.9172141 <a title="53-lda-13" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>14 0.91717243 <a title="53-lda-14" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>15 0.91716802 <a title="53-lda-15" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>16 0.9171524 <a title="53-lda-16" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>17 0.91669267 <a title="53-lda-17" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>18 0.91639906 <a title="53-lda-18" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>19 0.91631764 <a title="53-lda-19" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>20 0.91604394 <a title="53-lda-20" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
