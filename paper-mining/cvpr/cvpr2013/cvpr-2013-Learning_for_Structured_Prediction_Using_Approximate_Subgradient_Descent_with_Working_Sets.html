<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-262" href="#">cvpr2013-262</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</h1>
<br/><p>Source: <a title="cvpr-2013-262-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lucchi_Learning_for_Structured_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Aurélien Lucchi, Yunpeng Li, Pascal Fua</p><p>Abstract: We propose a working set based approximate subgradient descent algorithm to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM. We focus on the setting of general graphical models, such as loopy MRFs and CRFs commonly used in image segmentation, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM’s cutting plane algorithm as well as reducing the robustness of existing subgradient based methods. We show that the proposed method obtains better approximate subgradients through the use of working sets, leading to improved convergence properties and increased reliability. Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, which can be used to reduce learning time at only a small cost of performance. We demonstrate the strength of our method empirically on the segmentation of a new publicly available electron microscopy dataset as well as the popular MSRC data set and show state-of-the-art results.</p><p>Reference: <a title="cvpr-2013-262-reference" href="../cvpr2013_reference/cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We show that the proposed method obtains better approximate subgradients through the use of working sets, leading to improved convergence properties and increased reliability. [sent-3, score-0.679]
</p><p>2 Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, which can be used to reduce learning time at only a small cost of performance. [sent-4, score-0.448]
</p><p>3 We demonstrate the strength of our method empirically on the segmentation of a new publicly available  electron microscopy dataset as well as the popular MSRC data set and show state-of-the-art results. [sent-5, score-0.178]
</p><p>4 Introduction Markov random fields (MRF) [2] and conditional random fields (CRF) [11] are among the most widely used models in computer vision. [sent-7, score-0.107]
</p><p>5 A particularly successful large-margin formulation is the structured support vector machine (SSVM) [28], where the learning objective is to minimize a regularized hinge loss due to the violation of a set of soft margin constraints. [sent-11, score-0.491]
</p><p>6 This can be solved iteratively using the SSVM cutting plane algorithm [28] or by solving the equivalent unconstrained optimization problem using subgradient based methods [19, 15, 17, 30]. [sent-12, score-0.614]
</p><p>7 Both approaches require finding at each iteration the most violated constraint, namely the la-  beling that maximizes the margin-sensitive hinge loss [28], which is necessary for obtaining a valid cutting plane or a true subgradient of the objective. [sent-13, score-1.017]
</p><p>8 Finding such constraints is, however, intractable in loopy graphical models, such as the MRFs and CRFs usually used in image segmentation. [sent-14, score-0.281]
</p><p>9 In this paper, we propose to use a working set of constraints to increase robustness of approximate subgradient descent based learning. [sent-17, score-0.953]
</p><p>10 The resulting algorithm is particularly suited for minimizing the margin-sensitive hinge loss in the SSVM formulation [28] when the most violated constraints and hence the resulting subgradients are not exact. [sent-18, score-0.752]
</p><p>11 We show that the proposed method is able to obtain better subgradient approximations by computing them with respect to the whole working set, as opposed to existing approaches where only the last constraint is considered. [sent-19, score-0.701]
</p><p>12 Therefore, we are able to obtain sufficiently reliable ap-  proximate subgradients even when those due to individual constraints are noisy. [sent-20, score-0.35]
</p><p>13 This further enables us to replace the most violated constraints with randomly sampled labelings, thus avoiding the need to perform inference at all during 111999888755  learning. [sent-21, score-0.404]
</p><p>14 The use of sampling leads to decreased learning time while still maintaining good levels of performance. [sent-22, score-0.139]
</p><p>15 We discuss the prior work in Section 2 and provide the background on the large-margin framework and learning techniques based on subgradient descent in Section 3. [sent-25, score-0.552]
</p><p>16 Section 4 describes our working set based algorithm in detail and analyze its properties. [sent-26, score-0.308]
</p><p>17 Related work Maximum margin learning of CRFs was first formulated in the max-margin Markov networks (M3N) [26], whose objective is to minimize a margin-sensitive hinge loss between the ground-truth labeling and all other labelings for each training example. [sent-29, score-0.56]
</p><p>18 This is especially appealing for learning CRFs with loopy structures, due to its more objective-driven nature and its complete bypassing of the partition function that presents a major challenge to maximum likelihood based approaches. [sent-30, score-0.205]
</p><p>19 In M3N this is handled by rewriting the QP dual in terms of a polynomial number of marginal variables, which can then be solved by a coordinate descent method analogous to the sequential minimal optimization (SMO) [18]. [sent-32, score-0.152]
</p><p>20 However, solving such a QP is not tractable for loopy CRFs with high tree widths that are often needed in many computer vision tasks and even solving it approximately can become overwhelmingly expensive on large graphs. [sent-33, score-0.231]
</p><p>21 Structured SVMs (SSVM) [28] optimize the same kind of objective as M3N, while allowing for a more general class of loss functions. [sent-34, score-0.165]
</p><p>22 It employs a cutting plane algorithm to iteratively solve a series of increasingly larger QPs, which makes learning more scalable. [sent-35, score-0.202]
</p><p>23 However, the cutting plane algorithm requires the computation of the most violated constraints, namely the labeling maximizing the hinge loss [28]. [sent-36, score-0.627]
</p><p>24 This involves performing the loss augmented inference [25], which makes it intractable on loopy CRFs. [sent-37, score-0.486]
</p><p>25 Though approximate constraints can be used [5], they make the cutting plane algorithm susceptible to premature termination and can lead to catastrophic failure. [sent-38, score-0.325]
</p><p>26 Moreover, solving the QP can become slow as the set of constraints grows larger after each iteration, especially when the dimensionality of the feature space is also high. [sent-39, score-0.114]
</p><p>27 An alternative to solving the quadratic program deterministically is to employ stochastic gradient or subgradient descent. [sent-40, score-0.471]
</p><p>28 In the context of structured prediction, learning can be achieved by finding a convex-concave saddle-point and solving it with a dual extra-gradient method [27]. [sent-42, score-0.192]
</p><p>29 In [19] max-margin learning is solved as an unconstrained optimization problem and subgradients are used to approximate the gradient in the resulting non-differentiable problem. [sent-43, score-0.398]
</p><p>30 However, the soundness of these methods heavily depends on the assumption that a valid subgradient is obtained at each iteration. [sent-46, score-0.424]
</p><p>31 Hence they become much less reliable when the subgradients are noisy due to inexact inference, as is the case for loopy CRFs. [sent-47, score-0.406]
</p><p>32 The recently proposed SampleRank [29] avoids performing inference altogether during learning; Instead it samples labelings at random using Markov chain Monte Carlo (MCMC). [sent-48, score-0.257]
</p><p>33 Though achieving notable speed improvement, the method does not in fact optimize the actual hinge loss but rather a loose upper bound on it. [sent-50, score-0.315]
</p><p>34 Given its parameters w, a CRF predicts the labeling Y for a given input X by maximizing some score function Sw : X Y → R, i. [sent-54, score-0.112]
</p><p>35 , Yˆ = argmaxSw(Y ) Y ∈Y  = argmaxwTΨ(X, Y ) Y ∈Y  (1)  The score is usually expressed as a linear function of w and can be written as wTΨ(X, Y ), where the vector Ψ(X, Y ) is the feature map corresponding to the input X and the labeling Y . [sent-56, score-0.112]
</p><p>36 The fundamental properties of random fields imply that the feature map Ψ(X, Y ) and hence the score Sw decompose into sums over individual nodes and edges for any pairwise CRFs [2]. [sent-57, score-0.114]
</p><p>37 Discriminative Learning Discriminative learning uses the labeled training data to learn the CRF parameters so that the inferred labeling of the CRF is “close” to that of the ground truth, defined as yielding a low loss. [sent-61, score-0.17]
</p><p>38 Yn)∈Dl(Xn,Yn,w) + R(w),(2)  where lis the surrogate loss function and R(w) is the regularizer (typically the L2 norm). [sent-67, score-0.185]
</p><p>39 The most common choice of l is the hinge loss, as used in [26, 28], which will be described later on in this section. [sent-68, score-0.15]
</p><p>40 Note that the definition of the surrogate loss l depends on the score function Sw, since the goal of learning is to make the maximizer of Sw a desirable output for the given input. [sent-69, score-0.274]
</p><p>41 Max-margin Formulation The max-margin approach is a particular instance of discriminative learning, where parameter learning is formulated as a quadratic program (QP) with soft margin constraints [28]:  wm,ξi≥n021||w| 2+ Cn? [sent-72, score-0.202]
</p><p>42 ∀n : Sw(Yn) ≥ Y m ∈aYxn(Sw(Y ) + Δ(Yn,Y )) − ξn,  where Yn is the set of all possible labelings for example n, wtheh croen Ystant C controls the trade-off between margin and training error, and the task loss Δ measures the closeness of any inferred labeling Y to the ground truth labeling Yn. [sent-75, score-0.433]
</p><p>43 The QP can be converted to an unconstrained optimization problem by incorporating the soft constraints directly into the objective function, yielding: min L(w) = w  (4)  mwin21C||w||2+n? [sent-76, score-0.139]
</p><p>44 2 where the hinge loss is used as the surrogate loss l, i. [sent-80, score-0.475]
</p><p>45 4 can be minimized via stochastic subgradient descent, similar to [19, 15]. [sent-89, score-0.436]
</p><p>46 This class of methods iteratively computes and steps in the opposite direction of a subgradient vector with respect to a example Xn chosen by picking an index n ∈ {1. [sent-90, score-0.427]
</p><p>47 (7)  A subgradient of the convex function f : W → R at w is dAef sinuebdg as a evnetct oofr t g, scuocnhv ethxa ftu ∀w? [sent-96, score-0.393]
</p><p>48 (8)  The set of all subgradients at w is called the subdifferential at w and is denoted ∂f(w). [sent-100, score-0.344]
</p><p>49 A valid subgradient g(Yn, Y∗ , w) with respect to the parameter w can always be computed as the gradient of f(w) at Y∗ . [sent-102, score-0.424]
</p><p>50 Hence for the hinge loss, it can be computed as:  ∂f(Yn∂,wY∗,w)= ψ(Y∗) − ψ(Yn) +wC. [sent-103, score-0.15]
</p><p>51 (10) For loopy CRFs, however, true subgradients of the hinge loss cannot always be obtained due to the intractability of loss-augmented inference. [sent-110, score-0.728]
</p><p>52 This can lead to erratic behavior due to noisy subgradient estimates and loss of performance. [sent-111, score-0.572]
</p><p>53 Estimating Subgradient Using Working Sets Our algorithm aims at better estimating an approximate subgradient of f(Yn, Y, w) by using working sets of constraints, denoted An, for learning loopy CRFs where exact isntrfaeirnetnsc,e d eisn oitnetrdaAc table. [sent-113, score-0.961]
</p><p>54 It first solves the loss-augmented inference to find a constraint Y∗ and add it to the working set An. [sent-115, score-0.51]
</p><p>55 It then steps in the opposite direction of the approxiAmate subgradient computed as an average over the set of violated constraints belonging to An. [sent-116, score-0.584]
</p><p>56 lHatenedce c ounnslitkraei ndtsua ble averaging mAethods [17, 30] that aggregate over all previous subgradients, our algorithm only considers the subset of active, namely violated, constraints 111999888977  Algorithm 1 1:INPUTS :  2: D : Training set of N examples. [sent-117, score-0.105]
</p><p>57 Therefore all subgradients are computed with respect to the parameters at the current iteration, as opposed to using their historical values. [sent-134, score-0.271]
</p><p>58 This produces more meaningful descent directions, as evidenced by the results in Section 5. [sent-135, score-0.116]
</p><p>59 8 cannot be guaranteed for loopy CRFs, interesting results can still be obtained even if one can only find an approximate ? [sent-138, score-0.192]
</p><p>60 -subgradients, and we show below how this is achieved through the use of working sets. [sent-157, score-0.308]
</p><p>61 , gm ∈ Rd be the approximate subgradients with respect to example (Xn, Yn) of L for labelings in the working set An that still violates) t ohfe margin ceolinnsgtsra i nnt t haet a given git seertat Aion. [sent-161, score-0.765]
</p><p>62 Let δi = gi −μi be the difference between approximate ? [sent-163, score-0.108]
</p><p>63 , 0 in this case, as the number of violated constraints in the working set m increases: Pr? [sent-180, score-0.499]
</p><p>64 |∂f(Y∂nw,Y(t,w)(t))  ←  17: z ← z − η(t)g(t) 18: endz ←for 19: w(t+1) ← z 20: end for  * atomic update *  Algorithm 1 solves the loss-augmented inference to generate new constraints, which can be expensive to compute. [sent-220, score-0.27]
</p><p>65 The analysis presented in Section 4 suggests that it is possible to use a sampling method instead of the loss-augmented inference to obtain new constraints, and under similar assumptions the average subgradient ¯g still converges to a valid subgradient. [sent-221, score-0.695]
</p><p>66 Based on this observation, we propose an adaptation of Algorithm 1 that uses sampling instead of solving the loss-augmented inference. [sent-222, score-0.131]
</p><p>67 We also replace the standard update of Algorithm 1 by a sequence of atomic updates that has been shown to improve the speed of convergence [29]. [sent-224, score-0.123]
</p><p>68 Concerning the practicality, we would like to point out that the working set does not lead to a significant increase in memory as we only need to store the feature maps rather than the whole labellings. [sent-225, score-0.308]
</p><p>69 ∈ T hVe corresponds t,oE a superpixel anendd tshoe rteh aist an edge d(ei, ji) ∈ ∈∈ VE bcoertwreseepno ntdwso tnoo dae ssu ip aenrpdi j ilf a atnhed corresponding superpixels are adjacent niond tehse i image. [sent-233, score-0.193]
</p><p>70 L theet Y = {yi} for i ∈ V denote the labeling of the CRF which assigns a }c flaosrs ila ∈be Vl yi tnoo etaec thh en loadbee li. [sent-234, score-0.121]
</p><p>71 To ensure good performance across all classes, we adopt a loss function that weighs errors for a given class inversely proportional to the frequency with which it appears in the training data. [sent-250, score-0.172]
</p><p>72 Methods In the following, we will compare our learning methods (referred as Working sets + inference and Working sets + sampling) with the following baselines. [sent-253, score-0.268]
</p><p>73 We also experimented with averaging all past subgradients [17, 30], which did not produce meaningful results for our task. [sent-254, score-0.297]
</p><p>74 •  SSVM –The cutting plane algorithm described in [28]. [sent-261, score-0.159]
</p><p>75 SGD + inference solve the loss-augmented inference using graph-cuts or belief-propagation. [sent-263, score-0.35]
</p><p>76 –  SGD + sampling – Instead of performing inference, use DM +CM saCm ptol sample sctoenasdtr oaifn ptse ffroormm a gdi isntrfeibruetnicoen, targeting the loss-augmented score. [sent-265, score-0.096]
</p><p>77 [3 1]  Original Ground truth SGD + inference [19] Working set + inference  67 92 80 82 89 97 86 83 86 79 94 96 85 35 98 70 86 78 55 62 23 77. [sent-304, score-0.35]
</p><p>78 The quantitative results show that the working set of constraints improves the average score regardless of whether inference or sampling was used during learning. [sent-313, score-0.704]
</p><p>79 Electron Microscopy Dataset Here, we perform mitochondria segmentation in 3D using the large image stack from Fig. [sent-319, score-0.17]
</p><p>80 It is written as:  Jaccard index =True Positive + FaTlrsuee P Poossiti tvivee + False Negative The segmentation process begins by over-segmenting the volume using SLIC supervoxels [1]. [sent-326, score-0.13]
</p><p>81 Segmentation performance measured with the Jaccard index for the mitochondria EM dataset. [sent-334, score-0.117]
</p><p>82 SVM  Lucchi  SSVM  SampleRank  [13]  [28]  [29]  + sampling SGD  + inference SGD  [19]  Working sampling  Original features  73. [sent-337, score-0.367]
</p><p>83 7%  Ground truthLinear SVMSGD + inference [19]Working set + inference Figure 3. [sent-352, score-0.35]
</p><p>84 The computational overhead reported in the brackets is the increase in time resulting from the working set. [sent-356, score-0.335]
</p><p>85 EM SampleRank [29] SGD + Sampling Working set + Sampling SGD + inference [19] Working set + inference  MSRC  2524s 2481s 2619s (+5. [sent-358, score-0.35]
</p><p>86 nTh ien increased reliability due to the use of working sets leads to higher scores for both the inference and sampling methods. [sent-367, score-0.604]
</p><p>87 The inference version of our algorithm outperforms the previous state-of-the-art [13]. [sent-368, score-0.175]
</p><p>88 Time analysis We conducted a time analysis of the standard subgradient method of [19] against the 2 versions of the algorithm introduced in this paper. [sent-371, score-0.393]
</p><p>89 As shown in Table 3, the sampling method is much faster than solving the loss-augmented inference to find the most violated constraint. [sent-372, score-0.418]
</p><p>90 We can see that the computational overhead due to the working set is  (a) Training set, EM(b) Test set, EM (c) Train g set, MSRC(d) Test set, MSRC  Figure 4. [sent-373, score-0.335]
</p><p>91 Evolution of the training and test scores (Jaccard index for EM and average score for MSRC) as a function of the number of iterations t. [sent-374, score-0.112]
</p><p>92 We report results for the sampling method with and without working set in green and blue respectively. [sent-375, score-0.404]
</p><p>93 of the order of 5% for the sampling method and less than 10% when solving the loss-augmenting inference to find the most-violated constraint. [sent-376, score-0.306]
</p><p>94 The curves clearly show that the working set of constraints leads to a much higher score on both the training and test sets. [sent-379, score-0.465]
</p><p>95 Conclusion We have presented a working set based approximate subgradient descent method for learning graphical models for structured prediction. [sent-381, score-1.026]
</p><p>96 Our method is particularly appealing for learning large CRFs with loops, which are common in computer vision tasks, since under these circumstances the use working sets of constraints produces better subgradient estimates and higher-quality solutions. [sent-382, score-0.875]
</p><p>97 More-  over the method allows us to use sampling to replace the more expensive inference step without much performance loss, leading to significantly lower learning time. [sent-384, score-0.378]
</p><p>98 Efficient inference in fully connected crfs with gaussian edge potentials. [sent-431, score-0.347]
</p><p>99 Linear convergence of epsilon-subgradient descent methods for a class of convex functions. [sent-509, score-0.159]
</p><p>100 Dual averaging methods for regularized stochastic learning and online optimization. [sent-578, score-0.112]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subgradient', 0.393), ('working', 0.308), ('subgradients', 0.271), ('samplerank', 0.22), ('sgd', 0.203), ('yn', 0.198), ('msrc', 0.185), ('inference', 0.175), ('crfs', 0.172), ('hinge', 0.15), ('sw', 0.145), ('loss', 0.14), ('loopy', 0.135), ('crf', 0.133), ('ssvm', 0.128), ('descent', 0.116), ('violated', 0.112), ('cutting', 0.111), ('sampling', 0.096), ('jaccard', 0.089), ('qp', 0.083), ('mitochondria', 0.083), ('labelings', 0.082), ('constraints', 0.079), ('structured', 0.078), ('taskar', 0.075), ('subdifferential', 0.073), ('electron', 0.068), ('em', 0.067), ('lucchi', 0.067), ('labeling', 0.066), ('slic', 0.063), ('xn', 0.057), ('approximate', 0.057), ('segmentation', 0.057), ('tnoo', 0.055), ('microscopy', 0.053), ('gi', 0.051), ('vij', 0.05), ('plane', 0.048), ('cuts', 0.048), ('margin', 0.047), ('score', 0.046), ('surrogate', 0.045), ('kernelized', 0.045), ('learning', 0.043), ('convergence', 0.043), ('stochastic', 0.043), ('superpixels', 0.043), ('bounded', 0.043), ('atomic', 0.042), ('mrfs', 0.042), ('ssu', 0.041), ('fields', 0.04), ('supervoxels', 0.039), ('erratic', 0.039), ('replace', 0.038), ('icml', 0.038), ('smith', 0.037), ('intractable', 0.036), ('dual', 0.036), ('solving', 0.035), ('prediction', 0.035), ('belief', 0.035), ('ladick', 0.034), ('index', 0.034), ('propagation', 0.033), ('soft', 0.033), ('true', 0.032), ('training', 0.032), ('graphical', 0.031), ('relational', 0.031), ('valid', 0.031), ('superpixel', 0.03), ('mcmc', 0.03), ('susceptible', 0.03), ('march', 0.03), ('achanta', 0.03), ('stack', 0.03), ('epfl', 0.03), ('yielding', 0.029), ('optimality', 0.028), ('sums', 0.028), ('segmentations', 0.028), ('conditional', 0.027), ('overhead', 0.027), ('unconstrained', 0.027), ('appealing', 0.027), ('solves', 0.027), ('expensive', 0.026), ('averaging', 0.026), ('markov', 0.026), ('optimize', 0.025), ('sets', 0.025), ('knott', 0.024), ('qps', 0.024), ('prematurely', 0.024), ('usin', 0.024), ('ilf', 0.024), ('thhaet', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999851 <a title="262-tfidf-1" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>Author: Aurélien Lucchi, Yunpeng Li, Pascal Fua</p><p>Abstract: We propose a working set based approximate subgradient descent algorithm to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM. We focus on the setting of general graphical models, such as loopy MRFs and CRFs commonly used in image segmentation, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM’s cutting plane algorithm as well as reducing the robustness of existing subgradient based methods. We show that the proposed method obtains better approximate subgradients through the use of working sets, leading to improved convergence properties and increased reliability. Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, which can be used to reduce learning time at only a small cost of performance. We demonstrate the strength of our method empirically on the segmentation of a new publicly available electron microscopy dataset as well as the popular MSRC data set and show state-of-the-art results.</p><p>2 0.1535102 <a title="262-tfidf-2" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>Author: Steve Branson, Oscar Beijbom, Serge Belongie</p><p>Abstract: unkown-abstract</p><p>3 0.14156234 <a title="262-tfidf-3" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>4 0.12639996 <a title="262-tfidf-4" href="./cvpr-2013-Continuous_Inference_in_Graphical_Models_with_Polynomial_Energies.html">95 cvpr-2013-Continuous Inference in Graphical Models with Polynomial Energies</a></p>
<p>Author: Mathieu Salzmann</p><p>Abstract: In this paper, we tackle the problem of performing inference in graphical models whose energy is a polynomial function of continuous variables. Our energy minimization method follows a dual decomposition approach, where the global problem is split into subproblems defined over the graph cliques. The optimal solution to these subproblems is obtained by making use of a polynomial system solver. Our algorithm inherits the convergence guarantees of dual decomposition. To speed up optimization, we also introduce a variant of this algorithm based on the augmented Lagrangian method. Our experiments illustrate the diversity of computer vision problems that can be expressed with polynomial energies, and demonstrate the benefits of our approach over existing continuous inference methods.</p><p>5 0.12240036 <a title="262-tfidf-5" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>Author: Matthieu Guillaumin, Luc Van_Gool, Vittorio Ferrari</p><p>Abstract: Pairwise discrete energies defined over graphs are ubiquitous in computer vision. Many algorithms have been proposed to minimize such energies, often concentrating on sparse graph topologies or specialized classes of pairwise potentials. However, when the graph is fully connected and the pairwise potentials are arbitrary, the complexity of even approximate minimization algorithms such as TRW-S grows quadratically both in the number of nodes and in the number of states a node can take. Moreover, recent applications are using more and more computationally expensive pairwise potentials. These factors make it very hard to employ fully connected models. In this paper we propose a novel, generic algorithm to approximately minimize any discrete pairwise energy function. Our method exploits tractable sub-energies to filter the domain of the function. The parameters of the filter are learnt from instances of the same class of energies with good candidate solutions. Compared to existing methods, it efficiently handles fully connected graphs, with many states per node, and arbitrary pairwise potentials, which might be expensive to compute. We demonstrate experimentally on two applications that our algorithm is much more efficient than other generic minimization algorithms such as TRW-S, while returning essentially identical solutions.</p><p>6 0.12120759 <a title="262-tfidf-6" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>7 0.11575573 <a title="262-tfidf-7" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>8 0.10787129 <a title="262-tfidf-8" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>9 0.099348389 <a title="262-tfidf-9" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>10 0.093508072 <a title="262-tfidf-10" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>11 0.091215573 <a title="262-tfidf-11" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>12 0.08611118 <a title="262-tfidf-12" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>13 0.085796893 <a title="262-tfidf-13" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>14 0.084812589 <a title="262-tfidf-14" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>15 0.083399169 <a title="262-tfidf-15" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>16 0.081802979 <a title="262-tfidf-16" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>17 0.079786867 <a title="262-tfidf-17" href="./cvpr-2013-A_Comparative_Study_of_Modern_Inference_Techniques_for_Discrete_Energy_Minimization_Problems.html">6 cvpr-2013-A Comparative Study of Modern Inference Techniques for Discrete Energy Minimization Problems</a></p>
<p>18 0.078958668 <a title="262-tfidf-18" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>19 0.078278661 <a title="262-tfidf-19" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>20 0.074970931 <a title="262-tfidf-20" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, -0.009), (2, 0.003), (3, 0.004), (4, 0.105), (5, 0.024), (6, 0.051), (7, 0.051), (8, -0.085), (9, 0.004), (10, 0.098), (11, -0.023), (12, -0.051), (13, 0.015), (14, -0.11), (15, 0.084), (16, 0.025), (17, -0.024), (18, 0.071), (19, 0.002), (20, -0.033), (21, -0.054), (22, -0.048), (23, 0.008), (24, 0.037), (25, 0.026), (26, -0.088), (27, 0.092), (28, -0.003), (29, -0.049), (30, 0.045), (31, -0.007), (32, 0.025), (33, -0.043), (34, -0.033), (35, -0.028), (36, -0.014), (37, -0.034), (38, -0.03), (39, 0.031), (40, 0.019), (41, -0.003), (42, 0.001), (43, -0.023), (44, 0.013), (45, -0.044), (46, 0.057), (47, 0.007), (48, 0.018), (49, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93641537 <a title="262-lsi-1" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>Author: Aurélien Lucchi, Yunpeng Li, Pascal Fua</p><p>Abstract: We propose a working set based approximate subgradient descent algorithm to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM. We focus on the setting of general graphical models, such as loopy MRFs and CRFs commonly used in image segmentation, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM’s cutting plane algorithm as well as reducing the robustness of existing subgradient based methods. We show that the proposed method obtains better approximate subgradients through the use of working sets, leading to improved convergence properties and increased reliability. Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, which can be used to reduce learning time at only a small cost of performance. We demonstrate the strength of our method empirically on the segmentation of a new publicly available electron microscopy dataset as well as the popular MSRC data set and show state-of-the-art results.</p><p>2 0.81496257 <a title="262-lsi-2" href="./cvpr-2013-A_Comparative_Study_of_Modern_Inference_Techniques_for_Discrete_Energy_Minimization_Problems.html">6 cvpr-2013-A Comparative Study of Modern Inference Techniques for Discrete Energy Minimization Problems</a></p>
<p>Author: Jörg H. Kappes, Bjoern Andres, Fred A. Hamprecht, Christoph Schnörr, Sebastian Nowozin, Dhruv Batra, Sungwoong Kim, Bernhard X. Kausler, Jan Lellmann, Nikos Komodakis, Carsten Rother</p><p>Abstract: Seven years ago, Szeliski et al. published an influential study on energy minimization methods for Markov random fields (MRF). This study provided valuable insights in choosing the best optimization technique for certain classes of problems. While these insights remain generally useful today, the phenominal success of random field models means that the kinds of inference problems we solve have changed significantly. Specifically, the models today often include higher order interactions, flexible connectivity structures, large label-spaces of different cardinalities, or learned energy tables. To reflect these changes, we provide a modernized and enlarged study. We present an empirical comparison of 24 state-of-art techniques on a corpus of 2,300 energy minimization instances from 20 diverse computer vision applications. To ensure reproducibility, we evaluate all methods in the OpenGM2 framework and report extensive results regarding runtime and solution quality. Key insights from our study agree with the results of Szeliski et al. for the types of models they studied. However, on new and challenging types of models our findings disagree and suggest that polyhedral methods and integer programming solvers are competitive in terms of runtime and solution quality over a large range of model types.</p><p>3 0.77813113 <a title="262-lsi-3" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>Author: Roman Shapovalov, Dmitry Vetrov, Pushmeet Kohli</p><p>Abstract: This paper addresses the problem of semantic segmentation of 3D point clouds. We extend the inference machines framework of Ross et al. by adding spatial factors that model mid-range and long-range dependencies inherent in the data. The new model is able to account for semantic spatial context. During training, our method automatically isolates and retains factors modelling spatial dependencies between variables that are relevant for achieving higher prediction accuracy. We evaluate the proposed method by using it to predict 1 7-category semantic segmentations on sets of stitched Kinect scans. Experimental results show that the spatial dependencies learned by our method significantly improve the accuracy of segmentation. They also show that our method outperforms the existing segmentation technique of Koppula et al.</p><p>4 0.7640692 <a title="262-lsi-4" href="./cvpr-2013-Continuous_Inference_in_Graphical_Models_with_Polynomial_Energies.html">95 cvpr-2013-Continuous Inference in Graphical Models with Polynomial Energies</a></p>
<p>Author: Mathieu Salzmann</p><p>Abstract: In this paper, we tackle the problem of performing inference in graphical models whose energy is a polynomial function of continuous variables. Our energy minimization method follows a dual decomposition approach, where the global problem is split into subproblems defined over the graph cliques. The optimal solution to these subproblems is obtained by making use of a polynomial system solver. Our algorithm inherits the convergence guarantees of dual decomposition. To speed up optimization, we also introduce a variant of this algorithm based on the augmented Lagrangian method. Our experiments illustrate the diversity of computer vision problems that can be expressed with polynomial energies, and demonstrate the benefits of our approach over existing continuous inference methods.</p><p>5 0.76118064 <a title="262-lsi-5" href="./cvpr-2013-A_Principled_Deep_Random_Field_Model_for_Image_Segmentation.html">24 cvpr-2013-A Principled Deep Random Field Model for Image Segmentation</a></p>
<p>Author: Pushmeet Kohli, Anton Osokin, Stefanie Jegelka</p><p>Abstract: We discuss a model for image segmentation that is able to overcome the short-boundary bias observed in standard pairwise random field based approaches. To wit, we show that a random field with multi-layered hidden units can encode boundary preserving higher order potentials such as the ones used in the cooperative cuts model of [11] while still allowing for fast and exact MAP inference. Exact inference allows our model to outperform previous image segmentation methods, and to see the true effect of coupling graph edges. Finally, our model can be easily extended to handle segmentation instances with multiple labels, for which it yields promising results.</p><p>6 0.7437861 <a title="262-lsi-6" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>7 0.73631138 <a title="262-lsi-7" href="./cvpr-2013-Towards_Efficient_and_Exact_MAP-Inference_for_Large_Scale_Discrete_Computer_Vision_Problems_via_Combinatorial_Optimization.html">436 cvpr-2013-Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization</a></p>
<p>8 0.73233128 <a title="262-lsi-8" href="./cvpr-2013-Universality_of_the_Local_Marginal_Polytope.html">448 cvpr-2013-Universality of the Local Marginal Polytope</a></p>
<p>9 0.72994006 <a title="262-lsi-9" href="./cvpr-2013-Nonlinearly_Constrained_MRFs%3A_Exploring_the_Intrinsic_Dimensions_of_Higher-Order_Cliques.html">308 cvpr-2013-Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques</a></p>
<p>10 0.71131027 <a title="262-lsi-10" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>11 0.70984685 <a title="262-lsi-11" href="./cvpr-2013-Auxiliary_Cuts_for_General_Classes_of_Higher_Order_Functionals.html">51 cvpr-2013-Auxiliary Cuts for General Classes of Higher Order Functionals</a></p>
<p>12 0.70071715 <a title="262-lsi-12" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>13 0.69870019 <a title="262-lsi-13" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>14 0.6948874 <a title="262-lsi-14" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>15 0.66090387 <a title="262-lsi-15" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>16 0.65955204 <a title="262-lsi-16" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>17 0.65067464 <a title="262-lsi-17" href="./cvpr-2013-Discrete_MRF_Inference_of_Marginal_Densities_for_Non-uniformly_Discretized_Variable_Space.html">128 cvpr-2013-Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space</a></p>
<p>18 0.63518035 <a title="262-lsi-18" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>19 0.63231331 <a title="262-lsi-19" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<p>20 0.62730515 <a title="262-lsi-20" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.158), (16, 0.023), (22, 0.016), (26, 0.043), (33, 0.301), (64, 0.172), (67, 0.032), (69, 0.042), (80, 0.013), (87, 0.1), (94, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90620786 <a title="262-lda-1" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>Author: Aurélien Lucchi, Yunpeng Li, Pascal Fua</p><p>Abstract: We propose a working set based approximate subgradient descent algorithm to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM. We focus on the setting of general graphical models, such as loopy MRFs and CRFs commonly used in image segmentation, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM’s cutting plane algorithm as well as reducing the robustness of existing subgradient based methods. We show that the proposed method obtains better approximate subgradients through the use of working sets, leading to improved convergence properties and increased reliability. Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, which can be used to reduce learning time at only a small cost of performance. We demonstrate the strength of our method empirically on the segmentation of a new publicly available electron microscopy dataset as well as the popular MSRC data set and show state-of-the-art results.</p><p>2 0.90267146 <a title="262-lda-2" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>Author: Simon Hadfield, Richard Bowden</p><p>Abstract: Action recognition in unconstrained situations is a difficult task, suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent emergence of 3D data, both in broadcast content, and commercial depth sensors, provides the possibility to overcome this issue. This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use ofthe 3D data, andfive new interestpoint detection strategies are alsoproposed, that extend to the 3D data. Our evaluation compares all 4 feature descriptors, using 7 different types of interest point, over a variety of threshold levels, for the Hollywood3D dataset. We make the dataset including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community.</p><p>3 0.89818919 <a title="262-lda-3" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>Author: Cunzhao Shi, Chunheng Wang, Baihua Xiao, Yang Zhang, Song Gao, Zhong Zhang</p><p>Abstract: Scene text recognition has inspired great interests from the computer vision community in recent years. In this paper, we propose a novel scene text recognition method using part-based tree-structured character detection. Different from conventional multi-scale sliding window character detection strategy, which does not make use of the character-specific structure information, we use part-based tree-structure to model each type of character so as to detect and recognize the characters at the same time. While for word recognition, we build a Conditional Random Field model on the potential character locations to incorporate the detection scores, spatial constraints and linguistic knowledge into one framework. The final word recognition result is obtained by minimizing the cost function defined on the random field. Experimental results on a range of challenging public datasets (ICDAR 2003, ICDAR 2011, SVT) demonstrate that the proposed method outperforms stateof-the-art methods significantly bothfor character detection and word recognition.</p><p>4 0.89269072 <a title="262-lda-4" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>Author: Steve Branson, Oscar Beijbom, Serge Belongie</p><p>Abstract: unkown-abstract</p><p>5 0.89130175 <a title="262-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.88937384 <a title="262-lda-6" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>7 0.88892692 <a title="262-lda-7" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>8 0.88872617 <a title="262-lda-8" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>9 0.88787729 <a title="262-lda-9" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>10 0.88770032 <a title="262-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>11 0.88723719 <a title="262-lda-11" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>12 0.88666254 <a title="262-lda-12" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>13 0.88633478 <a title="262-lda-13" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>14 0.88631362 <a title="262-lda-14" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>15 0.88612092 <a title="262-lda-15" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>16 0.88552761 <a title="262-lda-16" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>17 0.88541156 <a title="262-lda-17" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>18 0.88534033 <a title="262-lda-18" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>19 0.88529384 <a title="262-lda-19" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>20 0.88517296 <a title="262-lda-20" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
