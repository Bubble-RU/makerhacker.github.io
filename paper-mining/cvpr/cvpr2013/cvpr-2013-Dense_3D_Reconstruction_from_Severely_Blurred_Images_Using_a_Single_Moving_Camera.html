<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-108" href="#">cvpr2013-108</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</h1>
<br/><p>Source: <a title="cvpr-2013-108-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lee_Dense_3D_Reconstruction_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: Motion blur frequently occurs in dense 3D reconstruction using a single moving camera, and it degrades the quality of the 3D reconstruction. To handle motion blur caused by rapid camera shakes, we propose a blur-aware depth reconstruction method, which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. Motion blur is dependent on 3D geometry, thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. The estimated depth is then converted intopixel-wise blur kernels, and non-uniform motion blur is easily removed with low computational cost. The obtained blur kernel is depth-dependent, thus it effectively addresses scene-depth variation, which is a challenging problem in conventional non-uniform deblurring methods.</p><p>Reference: <a title="cvpr-2013-108-reference" href="../cvpr2013_reference/cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Motion blur frequently occurs in dense 3D reconstruction using a single moving camera, and it degrades the quality of the 3D reconstruction. [sent-4, score-0.823]
</p><p>2 To handle motion blur caused by rapid camera shakes, we propose a blur-aware depth reconstruction method, which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. [sent-5, score-1.816]
</p><p>3 Motion blur is dependent on 3D geometry, thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. [sent-6, score-1.953]
</p><p>4 The estimated depth is then converted intopixel-wise blur kernels, and non-uniform motion blur is easily removed with low computational cost. [sent-7, score-1.841]
</p><p>5 The obtained blur kernel is depth-dependent, thus it effectively addresses scene-depth variation, which is a challenging problem in conventional non-uniform deblurring methods. [sent-8, score-1.044]
</p><p>6 Introduction  Motion blur in images is an undesirable effect in various computer vision algorithms. [sent-10, score-0.651]
</p><p>7 In particular, motion blur is a critical issue in the correspondence problem because motion blur destroys the structure details of images. [sent-11, score-1.723]
</p><p>8 Among these reconstruction algorithms, dense reconstruction algorithms [6, 15, 16, 22], which reconstruct dense 3D structures from a single moving camera, frequently suffer from severe motion blur due to camera shakes because the camera keeps moving by human hands or mobile robots. [sent-16, score-1.589]
</p><p>9 (d) Deblurred image by using the estimated depth-dependent blur kernel. [sent-38, score-0.657]
</p><p>10 However, motion blur degrades the resolution of input images in a blurred direction, and classical dense correspondence algorithms based on brightness or gradient constancy fail to obtain correct pixel correspondences. [sent-40, score-1.183]
</p><p>11 To handle motion blur for 3D reconstruction, deblurring methods, particularly video deblurring [3, 13, 21, 24], can be used by recovering input images. [sent-41, score-1.491]
</p><p>12 However, most highquality deblurring methods are inadequate for fast dense reconstruction systems, because these methods typically entail high computational cost but cannot handle scene-depth variation in blur kernel estimation. [sent-42, score-1.209]
</p><p>13 Therefore, we propose a blur-handling method for 3D reconstruction, in which blur kernel and depth of pixel are simultaneously estimated by adopting their dependency on each other. [sent-43, score-1.122]
</p><p>14 A blur kernel from camera shake can be interpreted as a trajectory of a projected 3D scene point by camera motion 222777333  during exposure time. [sent-44, score-1.413]
</p><p>15 Thus, the pixel-wise blur kernel can be determined in a closed form when camera motion, exposure time, and scene depth are given. [sent-45, score-1.336]
</p><p>16 In other words, estimating the scene depth is equivalent to estimating the pixelwise blur kernel when camera motion and exposure time are known. [sent-46, score-1.565]
</p><p>17 These values are available in general dense reconstruction systems, where exposure time can be obtained from camera hardware and camera motion can be estimated by camera localization method. [sent-47, score-1.056]
</p><p>18 In the proposed method, camera motion is estimated by image registration method between a reference image and an warped observed image using a reconstructed depth map, similarly to other 3D reconstruction algorithms [16]. [sent-48, score-0.94]
</p><p>19 Although the estimated camera motion has errors, the proposed method can generate a more reliable depth map than the conventional depth reconstruction methods that do not consider motion blur, as compared in Fig. [sent-49, score-1.467]
</p><p>20 The estimated depth map can be converted into pixel-wise blur kernels by using 3D geometry, and non-uniform deblurring can then be easily achieved, as shown in Fig. [sent-51, score-1.398]
</p><p>21 The proposed blur kernel estimation explicitly considers scene depth, thus it can provide improved deblurring results compared with previous image or video deblurring methods that disregard scene depth variation. [sent-53, score-1.779]
</p><p>22 Related Work Motion blur from camera shakes, rather than from object motion, has been solved in many studies by considering camera geometry. [sent-56, score-0.943]
</p><p>23 This means that most methods that utilize camera geometry disregard the effect of scene depth variation. [sent-60, score-0.613]
</p><p>24 We briefly review related studies on blur kernel estimation utilizing either camera geometry or scene geometry. [sent-61, score-0.971]
</p><p>25 The relationship between the camera geometry and motion blur has been studied in multiple image deblurring [2, 13] and single image deblurring [7, 8, 25] to address a method for removing nonuniform motion blur attributed to camera shakes. [sent-63, score-2.697]
</p><p>26 In multiple image deblurring, camera motion is parameterized by homography under the assumption of constant scene depth, and blur kernels are derived from the estimated homographies. [sent-64, score-1.174]
</p><p>27 In single image deblurring, non-uniform motion blur is represented by a finite number of basis functions that related to camera motion or homography, and blur kernel is solved efficiently with respect to these basis functions. [sent-65, score-1.943]
</p><p>28 However, the above methods do not consider the effect of scene depth variation, which is an important factor that contribute to the non-uniformity of motion blur. [sent-66, score-0.596]
</p><p>29 [11] explicitly utilize a camera motion by estimating the camera motion from inertial measurement  sensors. [sent-68, score-0.856]
</p><p>30 Their camera is equipped with accelerometers and gyroscopes, and six degrees of freedom (DOF) camera motion is estimated from the sensors and it generates accurate non-uniform blur kernels. [sent-69, score-1.229]
</p><p>31 While typical image only-based blur estimation methods have limited range of measurable kernel size because they utilize image priors which are valid only for a small region, [11] can handle large size of blur kernels with the aid of additional sensors. [sent-70, score-1.486]
</p><p>32 Thus, this method is valid only for negligible depth variation or limited types of camera motion, such as pure rotation. [sent-72, score-0.528]
</p><p>33 To address the depth variation in blur kernel estimation, Xu and Jia [26] combined depth reconstruction by using stereopsis with blur kernel estimation. [sent-74, score-2.2]
</p><p>34 Since motion blur in stereo image pair is almost identical, a scene depth is easily estimated by classical stereo matching algorithm and the result is used in their depth-dependent blur kernel estimation. [sent-75, score-1.996]
</p><p>35 Their depthdependent blur kernel estimation can be extended to single image deblurring, however, camera motion is limited to translation in single image cases. [sent-76, score-1.137]
</p><p>36 In-depth studies on the relationship between scene depth and motion blur were conducted in [5, 17], which are closely related to our proposed method. [sent-77, score-1.196]
</p><p>37 However, these methods  differ from our method; [5] assumes sideways translational camera motion unlike the proposed method which deals with arbitrary camera motion, a reference unblurred image is required in [17] while all input images can be blurred in our method. [sent-79, score-0.869]
</p><p>38 The proposed method considers both camera motion and the effect of scene depth variation in handling motion blur. [sent-80, score-1.07]
</p><p>39 Blur-Aware Depth Reconstruction We convert two image blur kernel estimation problem into a depth estimation problem by utilizing camera motion obtained from camera localization algorithm in 3D reconstruction. [sent-83, score-1.655]
</p><p>40 This section explains the two image motion blur estimation strategy and then presents a method that converts the blur kernel estimation problem into a depth estimation problem. [sent-84, score-1.986]
</p><p>41 Finally, the two image depth reconstruction process will be extended to multiple image depth reconstruction. [sent-85, score-0.761]
</p><p>42 m idle:  Synthesized input images In−1 and In, the estimated blur kernels represented by motion vectors, and their commutative convolution results. [sent-145, score-1.063]
</p><p>43 Motion blur estimation from two images  Estimation of motion blur kernels from two images utilizes the idea that applying the blur kernel of each image to the other image results in the same cumulatively blurred images [19, 20]. [sent-149, score-2.51]
</p><p>44 Let In−1 and In be two consecutive blurred images in an observed sequence, which have latent unblurred images Ln−1 and Ln, as well as blur kernels Kn−1 and Kn, respectively. [sent-150, score-0.988]
</p><p>45 The blurred image by the pixel-wise blur kernel Kn (x, y) is represented as follows: In (x, y) = (Ln ⊗ Kn (x, y)) (x, y) ,  (1)  where ⊗ denotes the convolution operator that corresponds to blur operation, and (x, y) represents a pixel coordinate. [sent-151, score-1.564]
</p><p>46 We omit the pixel coordinate notation (x, y) for images In and Ln as well as the blur kernel Kn for notational simplicity. [sent-152, score-0.784]
</p><p>47 An example of estimated blur kernels and their  ? [sent-154, score-0.728]
</p><p>48 The blur kernel K corresponds to a part of pixel motion v in an exposure time. [sent-179, score-1.114]
</p><p>49 (3), but the dependency of blur kernel on the warping functions can reduce the number of actual unknowns. [sent-185, score-0.843]
</p><p>50 Without motion blur, the warped image Ln from Ln−1 by a small motion vn can be approximated by the second-order expansion [14]:  Ln = Wn−1,n(Ln−1)  ≈ Ln−1+ JLn−1vn+21vn? [sent-188, score-0.671]
</p><p>51 When motion blur is considered in the image warping between two images as shown in Fig. [sent-190, score-1.001]
</p><p>52 (5), which represents the parametrization of the blurred image I using motion vector v, the by objective function that satisfies condition (3) can be formulated by using only the motion vectors vn−1 and vn. [sent-200, score-0.666]
</p><p>53 (8) approximates the blurred appearance of In−1 by the blur kernel of In, and the second term approximates the warped and blurred appearance of In, by warping Wn−−11,n and the blur kernel of In−1, respectively. [sent-215, score-1.862]
</p><p>54 Motion blur estimation to depth estimation Although the objective function is reduced to determining pixel-wise motion vectors vn−1 and vn, this problem remains ill-posed because only one pixel correspondence is  given for the quadratic equation (8) of two variables. [sent-218, score-1.381]
</p><p>55 The ambiguities in motion or blur kernel estimation given two images has been addressed in several previous works [10, 19, 20]. [sent-220, score-0.991]
</p><p>56 For example, the directions of the blur kernels of two images are assumed to be known [20]; otherwise, additional input images are used to refine the motion vectors of the two blurry images [19]. [sent-221, score-1.048]
</p><p>57 The proposed method utilizes a camera motion and exposure time as additional constraints to resolve the ambiguity in motion estimation. [sent-222, score-0.761]
</p><p>58 The use of camera motion has a similar advantage as that of using known blur directions in [20]. [sent-223, score-1.009]
</p><p>59 However, the assumption of known camera motion is more general than the assumption of known blur direction because the former can address non-uniform blur kernels and any type of pixel motion, such as curved pixel motion caused by camera rotation. [sent-224, score-2.233]
</p><p>60 When camera motion and exposure time are known, the estimation of pixel-wise blur kernels from two images is converted into an estimation of pixel-wise depth value. [sent-225, score-1.633]
</p><p>61 In the proposed method, exposure time τo and τc are provided by camera hardware, and camera pose at τ = τc is obtained from the registration-based camera localization algorithm. [sent-226, score-0.607]
</p><p>62 Let Pτn ∈ SE(3) be the six DOF camera pose at time τ for the nth image, which is represented by the special Euclidean group in three dimensions, and let d be the inverse depth of pixel (the pixel coordinate notation is also omitted for simplicity) with respect to the unblurred reference image L0. [sent-227, score-0.778]
</p><p>63 We utilize inverse depth, which is a reciprocal of  depth, because inverse depth has better convergence property in estimation than the original depth [4]. [sent-228, score-0.78]
</p><p>64 The 2D motion path of the projected pixel point (xτn , ynτ) at time τ corresponding to inverse depth d is represented as follows: (xτn, ynτ) = h(K((Pτn)−1 X=  d1K−1  ·  X)),  (9)  · (x,y,1)? [sent-229, score-0.671]
</p><p>65 Thus, the kernel estimation problem is reformulated into an estimation problem of inverse depth d. [sent-238, score-0.522]
</p><p>66 (9) shows that the pixel motions vn−1 = (xτnc−1, yτnc−1) − (xτnc−2, yτnc−2) and vn (xτnc yτnc) (xτnc−1 , yτnc−1) are functions of inverse depth d. [sent-240, score-0.597]
</p><p>67 To solve the objective function by means of the convex optimization framework, we linearize the relationship between the pixel  =  ,  motions  vn−1  , vn and a small update value of depth Δd  using the Jacobian matrices Jvn−1= Jvn =  ? [sent-243, score-0.609]
</p><p>68 (12) Therefore, the motion blur estimation problem is now represented by the depth estimation problem. [sent-257, score-1.241]
</p><p>69 Depth reconstruction using multiple images The proposed two view depth reconstruction can be easily extended to multiple view depth reconstruction in a manner similar to that of other multiple image reconstruction 222777666  methods [16, 22]. [sent-260, score-1.171]
</p><p>70 The objective function for the depth reconstruction of multiple images is defined as the minimization of  the sum of the differences between the first image I1 and the other images In considering their blurred appearances. [sent-262, score-0.65]
</p><p>71 The method for building blur kernel K from depth d will be described in Section 4. [sent-316, score-1.02]
</p><p>72 Algorithm 1 Warping and updating for depth reconstruction 1:Initialization: d =d¯ 2: repeat  3: 4: 5:  Resize images and depth map to finer level for n = 2 to N do In? [sent-317, score-0.797]
</p><p>73 1) enables handling of a curved motion path caused by camera rotation. [sent-320, score-0.491]
</p><p>74 Consequently, the proposed depth-based blur model can address more general motion blur compared with [19], where the blur kernel was assumed to be linear. [sent-321, score-2.168]
</p><p>75 Deblurring by using Estimated Depth This section describes building blur kernels from the estimated depth for deconvolution-based image deblurring. [sent-323, score-1.043]
</p><p>76 Similar to the projective motion path model in [23], we represent the blur kernel Kn at pixel (x, y) as a set of pixel  positions {(xτi , yτi ) , i ∈ 0, . [sent-324, score-1.102]
</p><p>77 , M}, which corresponds to the motion path of pixel (x, y) during exposure time as well as the weight kn (xτi , yτi ) for each pixel position. [sent-327, score-0.753]
</p><p>78 i=0  The weight of blur kernel should satisfy the constraint ? [sent-332, score-0.705]
</p><p>79 (9), we interpolate an intermediate camera pose Pτni from the input camera poses Pτnc−1 and Pτnc on the manifold of SE(3) as follows:  Pτni= exp(τ1c(τoτcM− τoi)ΔP) · Pτnc−1,  (19)  where ΔP is the camera motion between two input images, such that ΔP = log(Pτnc · (Pτnc−1)−1). [sent-337, score-0.756]
</p><p>80 The blur kernel generated by this method is used for image warping in depth reconstruction as well as deblurring after obtaining the final depth map. [sent-338, score-1.917]
</p><p>81 Given that the pixel’s motion path in images for 3D reconstruction is uncomplicated, a small number of Richardson-Lucy iterations (less than 50) are sufficient to obtain satisfactory deblurring results. [sent-341, score-0.716]
</p><p>82 Experiments In the experiment, the analysis of several important parameters is initially presented, then the comparative evaluations of the proposed method with other methods with respect to depth reconstruction, optical flow estimation, and deblurring then follow. [sent-343, score-0.735]
</p><p>83 Analysis of the initial depth value The initial value of depth for the proposed depth reconstruction is important, because the depth estimation is solved by variational optimization combined with a coarseto-fine scheme. [sent-347, score-1.526]
</p><p>84 We can verify that the optimization is not excessively sensitive to the initial value and converges to similar results for an arbitrary initial depth value only if the initial depth is not extremely far from the true value. [sent-443, score-0.714]
</p><p>85 However, this is invalid when motion blur occurs in the image sequence. [sent-449, score-0.846]
</p><p>86 With motion blur, finding the pixel correspondences becomes more difficult as the number of image increases because motion blur varies for each image. [sent-450, score-1.137]
</p><p>87 Meanwhile, the proposed blur-handled depth reconstruction provides a more accurate depth map as the number of input images increases. [sent-451, score-0.816]
</p><p>88 Comparison of depth reconstruction results The blur-robustness of the proposed algorithm is verified by comparing the depth reconstruction results with the conventional variational depth reconstruction implemented by removing the blur-handling parts of the proposed method. [sent-454, score-1.403]
</p><p>89 From top to bottom: Input images, variational depth reconstruction without blur handling, and the proposed blur-robust reconstruction. [sent-460, score-1.102]
</p><p>90 Comparison of optical flow results  The effectiveness of the proposed blur handling is demonstrated by comparing the optical flow results, i. [sent-481, score-0.884]
</p><p>91 We convert the estimated depth into motion vectors by using Eq. [sent-484, score-0.602]
</p><p>92 7 with the average endpoint error (EPE), and deblurring results from the estimated motion vectors are shown to verify the optical flow accuracy. [sent-489, score-0.707]
</p><p>93 By re-parameterizing the optical flow to depth, the proposed method is found to be capable of handling more complex shape of motion blur and thus achieves improved results. [sent-490, score-1.006]
</p><p>94 Comparison of deblurring results Finally, the deblurring results for real image data by the proposed method and the multiple image deblurring method from [24] are presented in Fig. [sent-493, score-0.939]
</p><p>95 The input image has a significant depth variations in a vertical direction, which cannot be addressed by conventional video deblurring methods. [sent-495, score-0.673]
</p><p>96 On the other hand, the proposed method successfully removes the motion blur by using the depth-aware blur kernels. [sent-497, score-1.463]
</p><p>97 The approximation technique for blurred appearance of image was successfully combined with the depth map estimation framework based on the variational optimization. [sent-500, score-0.554]
</p><p>98 Our geometry-combined blur estimation enabled handling of scene depth variation and large blur kernels, which are difficult in traditional image-only-based deblurring methods. [sent-501, score-2.019]
</p><p>99 ESM-blur: Handling & rendering blur in 3d tracking and augmentation. [sent-657, score-0.617]
</p><p>100 Richardson-lucy deblurring for scenes under a projective motion path. [sent-692, score-0.56]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blur', 0.617), ('depth', 0.315), ('deblurring', 0.313), ('kn', 0.256), ('motion', 0.229), ('vn', 0.181), ('camera', 0.163), ('blurred', 0.141), ('warping', 0.138), ('reconstruction', 0.131), ('exposure', 0.118), ('ln', 0.118), ('unblurred', 0.107), ('ajin', 0.093), ('kernel', 0.088), ('nc', 0.08), ('kernels', 0.071), ('wn', 0.066), ('pixel', 0.062), ('optical', 0.057), ('ajv', 0.056), ('hln', 0.056), ('jvn', 0.056), ('handling', 0.053), ('flow', 0.05), ('shakes', 0.043), ('blurry', 0.043), ('jacobian', 0.042), ('joshi', 0.041), ('estimated', 0.04), ('estimation', 0.04), ('inertial', 0.04), ('inverse', 0.039), ('convolution', 0.039), ('variational', 0.039), ('jinvn', 0.037), ('jln', 0.037), ('scene', 0.035), ('warped', 0.032), ('utilize', 0.032), ('correspondence', 0.031), ('commutative', 0.031), ('dense', 0.031), ('reference', 0.03), ('objective', 0.029), ('variation', 0.029), ('avn', 0.029), ('ereg', 0.029), ('hessian', 0.029), ('initial', 0.028), ('geometry', 0.028), ('favaro', 0.027), ('seoul', 0.027), ('snu', 0.026), ('conventional', 0.026), ('path', 0.026), ('nonuniform', 0.025), ('cho', 0.025), ('moving', 0.025), ('synthesized', 0.025), ('coarsest', 0.025), ('converted', 0.023), ('disregard', 0.023), ('huber', 0.023), ('deblurred', 0.022), ('newcombe', 0.022), ('derivation', 0.022), ('update', 0.022), ('utilizes', 0.022), ('rms', 0.021), ('dof', 0.021), ('valid', 0.021), ('parametrization', 0.02), ('tai', 0.02), ('curved', 0.02), ('zitnick', 0.019), ('kang', 0.019), ('input', 0.019), ('live', 0.019), ('homography', 0.019), ('jin', 0.019), ('stereo', 0.019), ('degrades', 0.019), ('map', 0.019), ('hardware', 0.018), ('consecutive', 0.018), ('projective', 0.018), ('vectors', 0.018), ('classical', 0.017), ('effect', 0.017), ('images', 0.017), ('un', 0.017), ('epe', 0.017), ('idle', 0.017), ('uncomplicated', 0.017), ('cumulatively', 0.017), ('accelerometers', 0.017), ('acuracy', 0.017), ('dehomogenization', 0.017), ('edm', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="108-tfidf-1" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: Motion blur frequently occurs in dense 3D reconstruction using a single moving camera, and it degrades the quality of the 3D reconstruction. To handle motion blur caused by rapid camera shakes, we propose a blur-aware depth reconstruction method, which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. Motion blur is dependent on 3D geometry, thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. The estimated depth is then converted intopixel-wise blur kernels, and non-uniform motion blur is easily removed with low computational cost. The obtained blur kernel is depth-dependent, thus it effectively addresses scene-depth variation, which is a challenging problem in conventional non-uniform deblurring methods.</p><p>2 0.64074999 <a title="108-tfidf-2" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>3 0.53051031 <a title="108-tfidf-3" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>4 0.49291316 <a title="108-tfidf-4" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>5 0.43363908 <a title="108-tfidf-5" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>Author: Li Xu, Shicheng Zheng, Jiaya Jia</p><p>Abstract: We show in this paper that the success of previous maximum a posterior (MAP) based blur removal methods partly stems from their respective intermediate steps, which implicitly or explicitly create an unnatural representation containing salient image structures. We propose a generalized and mathematically sound L0 sparse expression, together with a new effective method, for motion deblurring. Our system does not require extra filtering during optimization and demonstrates fast energy decreasing, making a small number of iterations enough for convergence. It also provides a unifiedframeworkfor both uniform andnon-uniform motion deblurring. We extensively validate our method and show comparison with other approaches with respect to convergence speed, running time, and result quality.</p><p>6 0.36912107 <a title="108-tfidf-6" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>7 0.35120651 <a title="108-tfidf-7" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>8 0.32453093 <a title="108-tfidf-8" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>9 0.2879453 <a title="108-tfidf-9" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>10 0.25404602 <a title="108-tfidf-10" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>11 0.17823525 <a title="108-tfidf-11" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>12 0.16462903 <a title="108-tfidf-12" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>13 0.16374347 <a title="108-tfidf-13" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>14 0.14768314 <a title="108-tfidf-14" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>15 0.14015898 <a title="108-tfidf-15" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>16 0.13574305 <a title="108-tfidf-16" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>17 0.13508578 <a title="108-tfidf-17" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>18 0.12868777 <a title="108-tfidf-18" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>19 0.12675303 <a title="108-tfidf-19" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>20 0.125018 <a title="108-tfidf-20" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.215), (1, 0.384), (2, -0.024), (3, 0.162), (4, -0.257), (5, 0.523), (6, 0.06), (7, 0.013), (8, 0.064), (9, 0.042), (10, -0.023), (11, 0.002), (12, 0.034), (13, 0.113), (14, 0.075), (15, -0.045), (16, -0.034), (17, 0.042), (18, -0.149), (19, -0.043), (20, -0.001), (21, -0.09), (22, -0.007), (23, 0.023), (24, 0.023), (25, 0.005), (26, 0.007), (27, 0.062), (28, -0.001), (29, 0.014), (30, -0.025), (31, 0.061), (32, 0.036), (33, 0.006), (34, 0.024), (35, -0.032), (36, 0.005), (37, -0.002), (38, -0.001), (39, -0.018), (40, 0.022), (41, -0.018), (42, -0.041), (43, 0.061), (44, -0.025), (45, -0.063), (46, -0.019), (47, -0.006), (48, 0.012), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96053505 <a title="108-lsi-1" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>same-paper 2 0.94135904 <a title="108-lsi-2" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: Motion blur frequently occurs in dense 3D reconstruction using a single moving camera, and it degrades the quality of the 3D reconstruction. To handle motion blur caused by rapid camera shakes, we propose a blur-aware depth reconstruction method, which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. Motion blur is dependent on 3D geometry, thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. The estimated depth is then converted intopixel-wise blur kernels, and non-uniform motion blur is easily removed with low computational cost. The obtained blur kernel is depth-dependent, thus it effectively addresses scene-depth variation, which is a challenging problem in conventional non-uniform deblurring methods.</p><p>3 0.90248489 <a title="108-lsi-3" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>Author: Yi Zhang, Keigo Hirakawa</p><p>Abstract: We propose a notion of double discrete wavelet transform (DDWT) that is designed to sparsify the blurred image and the blur kernel simultaneously. DDWT greatly enhances our ability to analyze, detect, and process blur kernels and blurry images—the proposed framework handles both global and spatially varying blur kernels seamlessly, and unifies the treatment of blur caused by object motion, optical defocus, and camera shake. To illustrate the potential of DDWT in computer vision and image processing, we develop example applications in blur kernel estimation, deblurring, and near-blur-invariant image feature extraction.</p><p>4 0.88556224 <a title="108-lsi-4" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>5 0.81975591 <a title="108-lsi-5" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>Author: Li Xu, Shicheng Zheng, Jiaya Jia</p><p>Abstract: We show in this paper that the success of previous maximum a posterior (MAP) based blur removal methods partly stems from their respective intermediate steps, which implicitly or explicitly create an unnatural representation containing salient image structures. We propose a generalized and mathematically sound L0 sparse expression, together with a new effective method, for motion deblurring. Our system does not require extra filtering during optimization and demonstrates fast energy decreasing, making a small number of iterations enough for convergence. It also provides a unifiedframeworkfor both uniform andnon-uniform motion deblurring. We extensively validate our method and show comparison with other approaches with respect to convergence speed, running time, and result quality.</p><p>6 0.80147028 <a title="108-lsi-6" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>7 0.77076745 <a title="108-lsi-7" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>8 0.76415998 <a title="108-lsi-8" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>9 0.62401426 <a title="108-lsi-9" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>10 0.60570014 <a title="108-lsi-10" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>11 0.56275326 <a title="108-lsi-11" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>12 0.54755712 <a title="108-lsi-12" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>13 0.4619641 <a title="108-lsi-13" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>14 0.45202044 <a title="108-lsi-14" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>15 0.45011643 <a title="108-lsi-15" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>16 0.44502249 <a title="108-lsi-16" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>17 0.44270203 <a title="108-lsi-17" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>18 0.42596543 <a title="108-lsi-18" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>19 0.40905368 <a title="108-lsi-19" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>20 0.3956033 <a title="108-lsi-20" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.282), (16, 0.031), (24, 0.091), (26, 0.049), (33, 0.312), (39, 0.026), (67, 0.021), (69, 0.021), (87, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97338784 <a title="108-lda-1" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>2 0.97107148 <a title="108-lda-2" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>3 0.97098482 <a title="108-lda-3" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>4 0.96862233 <a title="108-lda-4" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>5 0.96854615 <a title="108-lda-5" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>6 0.96698123 <a title="108-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.96653533 <a title="108-lda-7" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>8 0.96586311 <a title="108-lda-8" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>9 0.96466398 <a title="108-lda-9" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>10 0.96448708 <a title="108-lda-10" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>11 0.95677912 <a title="108-lda-11" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>12 0.94692183 <a title="108-lda-12" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>13 0.94226992 <a title="108-lda-13" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>14 0.94096017 <a title="108-lda-14" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>same-paper 15 0.93698883 <a title="108-lda-15" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>16 0.9353562 <a title="108-lda-16" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>17 0.93467098 <a title="108-lda-17" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>18 0.93178982 <a title="108-lda-18" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>19 0.93029785 <a title="108-lda-19" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>20 0.92831761 <a title="108-lda-20" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
