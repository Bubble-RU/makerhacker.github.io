<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-261" href="#">cvpr2013-261</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</h1>
<br/><p>Source: <a title="cvpr-2013-261-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zeng_Learning_by_Associating_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>Reference: <a title="cvpr-2013-261-reference" href="../cvpr2013_reference/cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We study in this paper the problem of learning classifiers from ambiguously labeled images. [sent-13, score-0.589]
</p><p>2 For instance, in the collection of new images, each image contains some samples of interest (e. [sent-14, score-0.18]
</p><p>3 , human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. [sent-16, score-0.319]
</p><p>4 The task is to learn classifiers from these ambiguously labeled images and generalize to new images. [sent-17, score-0.556]
</p><p>5 An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. [sent-18, score-0.272]
</p><p>6 Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. [sent-20, score-1.265]
</p><p>7 If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. [sent-21, score-0.51]
</p><p>8 By leveraging such a low-rank assump-  tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. [sent-22, score-0.426]
</p><p>9 The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. [sent-23, score-0.333]
</p><p>10 Introduction Learning classifiers for recognition purposes generally requires intensive labor work of labeling/annotating a large amount of training data. [sent-26, score-0.158]
</p><p>11 For example, in face recognition [28, 32, 13, 10], it is well known that collecting training samples with manual annotation for precise face alignment is the key to achieve high recognition accuracy. [sent-27, score-0.302]
</p><p>12 On the other hand, however, an unlimited number of images/videos with accompanying captions are freely available from the Internet, e. [sent-28, score-0.147]
</p><p>13 , images containing human faces and their associated text captions from the news websites. [sent-30, score-0.325]
</p><p>14 It becomes possible to avoid the intensive labor work if we can train good classifiers using these freely available data in the wild. [sent-31, score-0.213]
</p><p>15 The main difficulty comes from the ambiguous association between samples in images and their labels in the corresponding image captions, as illustrated in Fig. [sent-33, score-0.537]
</p><p>16 Learning classifiers from the ambiguously labeled data  falls in the category of ambiguous learning. [sent-35, score-0.801]
</p><p>17 The ambiguous association between samples and labels make the learning task more challenging than that in standard supervised learning. [sent-36, score-0.57]
</p><p>18 For example, Multiple Instance Learning (MIL) has been proposed [1, 6, 25, 33] to learn classifiers from ambiguously labeled data, in which an image is treated as a bag, and the bag is labeled as positive if it contains at least one true positive instance, and negative otherwise. [sent-38, score-0.633]
</p><p>19 MIL essentially learns a classifier for each class of samples by iteratively estimating the instance label by some predefined losses. [sent-39, score-0.287]
</p><p>20 To explore the relations between samples and their ambiguous annotations, co-occurrence model [2, 3, 30, 20] has been proposed to infer their correspondences using the Expectation Maximization. [sent-40, score-0.554]
</p><p>21 Iterative clustering and learning approach was also proposed in [4] to assign human faces to named entities. [sent-41, score-0.177]
</p><p>22 In [12, 24], an ambiguous loss was proposed to learn a discriminant function for classification. [sent-42, score-0.33]
</p><p>23 [ News From Washington Post ]  BryantandAndrewBynumhavebe n named  starters  Western  Conference  at gua rd and center  All-Star  respectively. [sent-46, score-0.081]
</p><p>24 This is Bryant's 14th time starting the league's annual showcase game. [sent-47, score-0.029]
</p><p>25 Sample photos from news websites and the corresponding  text caption. [sent-50, score-0.174]
</p><p>26 In general, most of the news websites do not provide  the face-name correspondence, hence it is a challenging task for the standard supervised learning method to automatically perform face  recognition on such freely available data. [sent-51, score-0.323]
</p><p>27 vant samples as background class, 2) uniqueness constraint - samples of the same class cannot simultaneously appear in an image except the background class (e. [sent-52, score-0.752]
</p><p>28 , multiple faces of the same person cannot appear in an image), and 3) nonpairing constraint - samples of different classes and their true labels cannot consistently appear together across the training images (e. [sent-54, score-0.45]
</p><p>29 , the faces from two subjects will not always co-occur in most of the images). [sent-56, score-0.105]
</p><p>30 With these assumptions in mind, the task of ambiguous learning is essentially to model the ambiguous relations between samples and labels both within each image and across the image set. [sent-57, score-0.902]
</p><p>31 A good approach should be able to make use of all constraints available in the sample-label relations in a principled way. [sent-58, score-0.17]
</p><p>32 To this end, we propose a novel framework to address the ambiguous learning problem. [sent-60, score-0.278]
</p><p>33 We are particularly interested in the problem of face recognition using ambiguously labeled images. [sent-61, score-0.551]
</p><p>34 Our framework is motivated by the observation that samples from the same class, assuming intra-class variations are reduced within a certain level, can be characterized by a low-dimensional subspace embedded in the ambient space. [sent-62, score-0.18]
</p><p>35 [10] showed that face images of the same person can be represented as a low-rank matrix. [sent-64, score-0.061]
</p><p>36 Based on this low-rank assumption, our framework  simultaneously optimizes a partial permutation matrix (PPM) for each of the training images by rank minimization. [sent-65, score-0.139]
</p><p>37 The PPMs are formulated so that after optimization, they can associate samples of the same classes from different images to form low-rank matrices. [sent-66, score-0.209]
</p><p>38 To address the intra-class variations, a sparse error term for each class is also introduced to achieve better robustness. [sent-67, score-0.075]
</p><p>39 The obtained PPMs can be used as indicators to assign the labels to samples in each image. [sent-68, score-0.294]
</p><p>40 Indeed, our method relies on the facts that PPMs are formulated and optimized so that the intrinsic constraints from both the intra-image and inter-image sample-label relations can be explored. [sent-69, score-0.158]
</p><p>41 For the intra-image relations, the PPM is constrained to simultaneously and exclusively assign one label to one sample in each image, where other priors could also be incorporated. [sent-70, score-0.081]
</p><p>42 For the inter-image relations, the PPMs are simultaneously optimized by rank minimization so that the aforementioned non-pairing assumption (3) in ambiguous learning can be used. [sent-71, score-0.393]
</p><p>43 Once the sample-label correspon-  dences are established, standard supervised learning methods can be applied to perform the prediction on unseen data. [sent-75, score-0.072]
</p><p>44 Related work Learning visual classifiers from caption-accompanying images has been an active topic in computer vision [1, 2, 20, 3 1, 24], of which learning face classifiers from such data is of particular interest [3, 18, 12, 24]. [sent-78, score-0.226]
</p><p>45 There are a few methods that explicitly take face-name (sample-label) correspondences into account. [sent-79, score-0.037]
</p><p>46 The work in [18] first iteratively clusters faces using EM based on face similarity and constraints from the caption. [sent-81, score-0.203]
</p><p>47 Based on these clusters, a weighted bipartite graph modelling the null assignment (i. [sent-82, score-0.158]
</p><p>48 , faces that are not assigned to any names and names that are not assigned to any faces) and caption constraints is constructed for face-name assignment. [sent-84, score-0.491]
</p><p>49 On the other hand, Support Vector Machine (SVM) based methods directly learn discriminant classifiers using the ambiguously labeled data. [sent-85, score-0.585]
</p><p>50 [12] proposed a max-margin for777777000000977797  mulation by introducing an ambiguous 0/1 loss to replace the loss in the standard SVM formulation, in which they defined the ambiguous 0/1 loss as 0 if the predicted name  is in the image caption, and 1otherwise. [sent-87, score-0.658]
</p><p>51 Based on this ambiguous loss, they defined a convex loss that penalized the prediction of names as the ones not present in the caption. [sent-88, score-0.372]
</p><p>52 This formulation did not consider the uniqueness constraint, hence it generally cannot perform well for images with multiple faces. [sent-89, score-0.108]
</p><p>53 [24] extended the idea of ambiguous loss for images with multiple faces, in which they enforced the uniqueness constraint by assigning names to faces at a set level (via labeling vectors) in each image. [sent-91, score-0.646]
</p><p>54 Recently, low-rank property of a set of linearly correlated images shows its usefulness in many computer vision problems, such as subspace segmentation [22], face recognition [10], multi-label image classification [7], image alignment [27] and image segmentation [11]. [sent-92, score-0.061]
</p><p>55 On the other hand, PPM has been popularly used for feature point correspondence with unsupervised learning [26, 34]. [sent-93, score-0.067]
</p><p>56 Our method in this paper is essentially motivated by these pioneering works. [sent-94, score-0.032]
</p><p>57 However, with a new formulation of lowrank matrices and PPM constraints, we show that our proposed method fits well for the ambiguous learning task. [sent-95, score-0.278]
</p><p>58 The proposed framework We formally define the problem of learning from ambiguously labeled images as follows. [sent-97, score-0.523]
</p><p>59 Each image has different , lneucmtiobenr o off N samples f Iro,m· ·d·i s,tIinctive classes, and there are K¯ classes in total. [sent-99, score-0.18]
</p><p>60 More precisely, we assume there are Kn samples from the nth image, and they are from different  classes. [sent-100, score-0.247]
</p><p>61 Hence, the nth image is represented as Fn = [f1n, ··· , fnKn] ∈ Rd×Kn. [sent-102, score-0.067]
</p><p>62 Associated with the nth image is a binary ve]ct ∈or Rtn ∈ {0, 1}K¯ representing the labels appearing in the caption o {f0 t,h1e} nth image: tn (i) = 1 if the label of the ith class appears in the image caption, and 0 otherwise. [sent-103, score-0.581]
</p><p>63 In the following, we first introduce how the low-rank assumption of the matrix formed by the samples from the same class can be used to simultaneously optimize a set of PPMs for assigning labels to the samples in the ambiguously labeled training images. [sent-105, score-1.108]
</p><p>64 Low-rank assumption same class  for samples from the  Face images of the same individual are commonly assumed to reside in a low-dimensional subspace [28, 10]. [sent-108, score-0.29]
</p><p>65 Put it in another way, if we place sufficient face samples from the same class into a matrix, this matrix should be approximately low-rank. [sent-109, score-0.316]
</p><p>66 Denote F¯i = ··· ,¯fini] ∈ Rd×ni as the matrix containing ni samples f,r·o·m· the i ∈th class, i∈ {1, . [sent-110, score-0.209]
</p><p>67 When these samples are human faces, then iF¯i ∈ s h{1ou,. [sent-114, score-0.18]
</p><p>68 a rHeo hwumevaenr, tahcee sd,i tshtrein-  [f¯i1,  ×  bution and ground-truth labels of these ni samples in the N training images are unknown. [sent-118, score-0.342]
</p><p>69 In our ambiguous learning tasks, we show next how this low-rank assumption can be used to seek the sample-label correspondences. [sent-119, score-0.313]
</p><p>70 Sample-label correspondences via PPM Given N training images, our first objective is to find the sample-label correspondences for all samples from K¯ classes. [sent-122, score-0.254]
</p><p>71 In this work, we use partial permutation matrix (PPM) [26, 3 1, 34] to model such correspondences. [sent-123, score-0.059]
</p><p>72 The first row in (1) enforces that only labels appearing in the caption can be assigned to samples in the image In. [sent-133, score-0.509]
</p><p>73 The second row in (1) is designed ptloe satisfy t ihme non-redundancy and uniqueness constraints. [sent-134, score-0.14]
</p><p>74 { PNote∈ thPat }PPM has been used in [3 1] for ambiguous learning. [sent-136, score-0.245]
</p><p>75 However, their work did not enforce the uniqueness constraint when using PMMs. [sent-137, score-0.138]
</p><p>76 Given {Fn}nN=1, there exist the PPMs such that samples of the same c}lass can be identified and columnly corresponded in {FnPn ∈ or equivalently, the K¯ sub matrices {LF1, . [sent-138, score-0.209]
</p><p>77 , LTK¯]T are rank deficient, where vec(·) is an operator that vectoriazrees a nmka tdreixfi by concatenating ·it)s cso alunm opn evreacttoorr tsh. [sent-145, score-0.067]
</p><p>78 aBta vseecdt on our low-rank assumption for samples from the same class, the sample-label correspondence problem can be formulated as the following problem: ? [sent-146, score-0.278]
</p><p>79 Considering intra-class variations and inevitable data noise or corruption, the above low-rank assumption is likely to be violated. [sent-153, score-0.035]
</p><p>80 1 is the l1norm and λ > 0 is a trade-off parameter twhahte rbea ? [sent-168, score-0.032]
</p><p>81 Modeling for the background samples In practical ambiguously labeled images from Internet, there are many irrelevant or background samples that cooccur with the samples we are interested in. [sent-173, score-1.03]
</p><p>82 In line with the convention in ambiguous learning [24, 18], we call these background samples as samples of null class. [sent-174, score-0.84]
</p><p>83 Without loss of generality, we let the K¯th class be the null class. [sent-175, score-0.289]
</p><p>84 Note that enforcing low-rank and sparse constraints on samples of the null class is inappropriate. [sent-176, score-0.45]
</p><p>85 In addition, there might be no true labels appearing in image captions for samples from the null class, we again take the convention to set tn (K¯) = 0. [sent-177, score-0.639]
</p><p>86 Moreover, to avoid the trivial solution that all samples are assigned to the null class, we assume that at least one sample per image is not associated with the null class. [sent-178, score-0.496]
</p><p>87 Similar to the PPM  definition in (s1 a), ct h×e cfi irsdte row imn (t2ri)x prohibits our hmee PthPoMd from choosing a label not appeared in the image caption except the null class. [sent-187, score-0.455]
</p><p>88 The second row enforces that at least one label from the caption must be chosen to avoid the trivial solution that assigning all the samples to the null class. [sent-188, score-0.576]
</p><p>89 The third and forth rows in (2) enforce the uniqueness and non-redundancy constraints respectively. [sent-189, score-0.145]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ppm', 0.465), ('ambiguously', 0.413), ('ppms', 0.286), ('ambiguous', 0.245), ('caption', 0.207), ('pn', 0.184), ('samples', 0.18), ('null', 0.158), ('news', 0.128), ('nn', 0.123), ('kn', 0.116), ('uniqueness', 0.108), ('faces', 0.105), ('captions', 0.092), ('relations', 0.092), ('labeled', 0.077), ('class', 0.075), ('labels', 0.075), ('vec', 0.073), ('ckobam', 0.072), ('fnpn', 0.072), ('inik', 0.072), ('rdk', 0.072), ('shenghua', 0.072), ('names', 0.071), ('qn', 0.067), ('nth', 0.067), ('lk', 0.066), ('classifiers', 0.066), ('zn', 0.064), ('face', 0.061), ('permutation', 0.059), ('loss', 0.056), ('freely', 0.055), ('admm', 0.053), ('labor', 0.051), ('rd', 0.049), ('appearing', 0.047), ('websites', 0.046), ('ek', 0.046), ('fn', 0.045), ('ei', 0.045), ('convention', 0.044), ('tn', 0.043), ('mil', 0.043), ('simultaneously', 0.042), ('principled', 0.041), ('intensive', 0.041), ('ic', 0.041), ('assign', 0.039), ('unseen', 0.039), ('rank', 0.038), ('association', 0.037), ('singapore', 0.037), ('correspondences', 0.037), ('constraints', 0.037), ('assumption', 0.035), ('correspondence', 0.034), ('ik', 0.034), ('learning', 0.033), ('xn', 0.032), ('bryant', 0.032), ('victory', 0.032), ('vant', 0.032), ('hmee', 0.032), ('president', 0.032), ('twhahte', 0.032), ('summit', 0.032), ('adsc', 0.032), ('denot', 0.032), ('fini', 0.032), ('ihme', 0.032), ('iki', 0.032), ('kui', 0.032), ('nise', 0.032), ('starters', 0.032), ('essentially', 0.032), ('internet', 0.032), ('assigning', 0.031), ('constraint', 0.03), ('rn', 0.03), ('appear', 0.03), ('showcase', 0.029), ('debate', 0.029), ('ector', 0.029), ('policy', 0.029), ('opn', 0.029), ('rtn', 0.029), ('bution', 0.029), ('tahcee', 0.029), ('corresponded', 0.029), ('prohibits', 0.029), ('cfi', 0.029), ('dmitry', 0.029), ('deficient', 0.029), ('lass', 0.029), ('tk', 0.029), ('discriminant', 0.029), ('formulated', 0.029), ('ni', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="261-tfidf-1" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>2 0.27535522 <a title="261-tfidf-2" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>Author: Yi-Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa, P. Jonathon Phillips</p><p>Abstract: We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification, where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.</p><p>3 0.13412981 <a title="261-tfidf-3" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>Author: Paul Bodesheim, Alexander Freytag, Erik Rodner, Michael Kemmler, Joachim Denzler</p><p>Abstract: Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in com- prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.</p><p>4 0.096280374 <a title="261-tfidf-4" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>Author: Baoyuan Wu, Yifan Zhang, Bao-Gang Hu, Qiang Ji</p><p>Abstract: In this paper, we focus on face clustering in videos. Given the detected faces from real-world videos, we partition all faces into K disjoint clusters. Different from clustering on a collection of facial images, the faces from videos are organized as face tracks and the frame index of each face is also provided. As a result, many pairwise constraints between faces can be easily obtained from the temporal and spatial knowledge of the face tracks. These constraints can be effectively incorporated into a generative clustering model based on the Hidden Markov Random Fields (HMRFs). Within the HMRF model, the pairwise constraints are augmented by label-level and constraint-level local smoothness to guide the clustering process. The parameters for both the unary and the pairwise potential functions are learned by the simulated field algorithm, and the weights of constraints can be easily adjusted. We further introduce an efficient clustering framework specially for face clustering in videos, considering that faces in adjacent frames of the same face track are very similar. The framework is applicable to other clustering algorithms to significantly reduce the computational cost. Experiments on two face data sets from real-world videos demonstrate the significantly improved performance of our algorithm over state-of-theart algorithms.</p><p>5 0.081711866 <a title="261-tfidf-5" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>6 0.077331819 <a title="261-tfidf-6" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>7 0.075153351 <a title="261-tfidf-7" href="./cvpr-2013-Procrustean_Normal_Distribution_for_Non-rigid_Structure_from_Motion.html">341 cvpr-2013-Procrustean Normal Distribution for Non-rigid Structure from Motion</a></p>
<p>8 0.07323771 <a title="261-tfidf-8" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>9 0.069506451 <a title="261-tfidf-9" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>10 0.066158131 <a title="261-tfidf-10" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>11 0.064406767 <a title="261-tfidf-11" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>12 0.061259612 <a title="261-tfidf-12" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>13 0.060824301 <a title="261-tfidf-13" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>14 0.059502214 <a title="261-tfidf-14" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>15 0.05894845 <a title="261-tfidf-15" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>16 0.058916472 <a title="261-tfidf-16" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>17 0.056980304 <a title="261-tfidf-17" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>18 0.055702757 <a title="261-tfidf-18" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>19 0.054916367 <a title="261-tfidf-19" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>20 0.054056555 <a title="261-tfidf-20" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.137), (1, -0.046), (2, -0.059), (3, 0.046), (4, 0.05), (5, -0.005), (6, -0.023), (7, -0.039), (8, 0.046), (9, -0.024), (10, 0.044), (11, -0.014), (12, -0.005), (13, 0.011), (14, -0.032), (15, -0.014), (16, -0.003), (17, -0.023), (18, -0.013), (19, -0.029), (20, -0.029), (21, -0.016), (22, -0.036), (23, 0.023), (24, 0.087), (25, -0.046), (26, -0.007), (27, 0.039), (28, -0.025), (29, -0.031), (30, -0.043), (31, 0.054), (32, -0.052), (33, -0.002), (34, -0.011), (35, -0.011), (36, -0.072), (37, -0.038), (38, 0.047), (39, -0.045), (40, 0.028), (41, -0.004), (42, 0.018), (43, 0.05), (44, 0.044), (45, 0.051), (46, 0.036), (47, 0.026), (48, 0.018), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92420036 <a title="261-lsi-1" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>2 0.71940559 <a title="261-lsi-2" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>Author: Martin Bäuml, Makarand Tapaswi, Rainer Stiefelhagen</p><p>Abstract: We address the problem of person identification in TV series. We propose a unified learning framework for multiclass classification which incorporates labeled and unlabeled data, and constraints between pairs of features in the training. We apply the framework to train multinomial logistic regression classifiers for multi-class face recognition. The method is completely automatic, as the labeled data is obtained by tagging speaking faces using subtitles and fan transcripts of the videos. We demonstrate our approach on six episodes each of two diverse TV series and achieve state-of-the-art performance.</p><p>3 0.65526623 <a title="261-lsi-3" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>Author: Yi-Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa, P. Jonathon Phillips</p><p>Abstract: We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification, where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.</p><p>4 0.63223428 <a title="261-lsi-4" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>5 0.63004011 <a title="261-lsi-5" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>Author: Paul Bodesheim, Alexander Freytag, Erik Rodner, Michael Kemmler, Joachim Denzler</p><p>Abstract: Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in com- prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.</p><p>6 0.62851685 <a title="261-lsi-6" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>7 0.61477327 <a title="261-lsi-7" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>8 0.61196482 <a title="261-lsi-8" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>9 0.61059773 <a title="261-lsi-9" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>10 0.60290915 <a title="261-lsi-10" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>11 0.59986973 <a title="261-lsi-11" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>12 0.59652621 <a title="261-lsi-12" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>13 0.57814997 <a title="261-lsi-13" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>14 0.56587142 <a title="261-lsi-14" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>15 0.55998009 <a title="261-lsi-15" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>16 0.55875063 <a title="261-lsi-16" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>17 0.553132 <a title="261-lsi-17" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>18 0.54554302 <a title="261-lsi-18" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>19 0.53527147 <a title="261-lsi-19" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>20 0.52789831 <a title="261-lsi-20" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.064), (16, 0.03), (26, 0.036), (28, 0.325), (33, 0.249), (67, 0.068), (69, 0.048), (87, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80375493 <a title="261-lda-1" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>2 0.79207945 <a title="261-lda-2" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>Author: Austin Abrams, Kylia Miskell, Robert Pless</p><p>Abstract: Shadows encode a powerful geometric cue: if one pixel casts a shadow onto another, then the two pixels are colinear with the lighting direction. Given many images over many lighting directions, this constraint can be leveraged to recover the depth of a scene from a single viewpoint. For outdoor scenes with solar illumination, we term this the episolar constraint, which provides a convex optimization to solve for the sparse depth of a scene from shadow correspondences, a method to reduce the search space when finding shadow correspondences, and a method to geometrically calibrate a camera using shadow constraints. Our method constructs a dense network of nonlocal constraints which complements recent work on outdoor photometric stereo and cloud based cues for 3D. We demonstrate results across a variety of time-lapse sequences from webcams “in . wu st l. edu (b)(c) the wild.”</p><p>same-paper 3 0.79007035 <a title="261-lda-3" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>4 0.77217299 <a title="261-lda-4" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>5 0.76689219 <a title="261-lda-5" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>Author: Minh Hoai, Andrew Zisserman</p><p>Abstract: The objective of this work is to learn sub-categories. Rather than casting this as a problem of unsupervised clustering, we investigate a weakly supervised approach using both positive and negative samples of the category. We make the following contributions: (i) we introduce a new model for discriminative sub-categorization which determines cluster membership for positive samples whilst simultaneously learning a max-margin classifier to separate each cluster from the negative samples; (ii) we show that this model does not suffer from the degenerate cluster problem that afflicts several competing methods (e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discover interpretable sub-categories in various datasets. The model is evaluated experimentally over various datasets, and itsperformance advantages over k-means and Latent SVM are demonstrated. We also stress test the model and show its resilience in discovering sub-categories as the parameters are varied.</p><p>6 0.75827914 <a title="261-lda-6" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>7 0.72312367 <a title="261-lda-7" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>8 0.72196233 <a title="261-lda-8" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>9 0.68192697 <a title="261-lda-9" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>10 0.67607653 <a title="261-lda-10" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>11 0.67204452 <a title="261-lda-11" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>12 0.67204028 <a title="261-lda-12" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>13 0.67057633 <a title="261-lda-13" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>14 0.66722739 <a title="261-lda-14" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>15 0.66676098 <a title="261-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.66652805 <a title="261-lda-16" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>17 0.66641486 <a title="261-lda-17" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>18 0.66542095 <a title="261-lda-18" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>19 0.66538459 <a title="261-lda-19" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>20 0.66403729 <a title="261-lda-20" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
