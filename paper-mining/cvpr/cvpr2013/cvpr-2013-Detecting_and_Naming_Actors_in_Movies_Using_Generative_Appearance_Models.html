<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-120" href="#">cvpr2013-120</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</h1>
<br/><p>Source: <a title="cvpr-2013-120-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Gandhi_Detecting_and_Naming_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Vineet Gandhi, Remi Ronfard</p><p>Abstract: We introduce a generative model for learning person and costume specific detectors from labeled examples. We demonstrate the model on the task of localizing and naming actors in long video sequences. More specifically, the actor’s head and shoulders are each represented as a constellation of optional color regions. Detection can proceed despite changes in view-point and partial occlusions. We explain how to learn the models from a small number of labeled keyframes or video tracks, and how to detect novel appearances of the actors in a maximum likelihood framework. We present results on a challenging movie example, with 81% recall in actor detection (coverage) and 89% precision in actor identification (naming).</p><p>Reference: <a title="cvpr-2013-120-reference" href="../cvpr2013_reference/cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We demonstrate the model on the task of localizing and naming actors in long video sequences. [sent-4, score-0.467]
</p><p>2 We explain how to learn the models from a small number of labeled keyframes or video tracks, and how to detect novel appearances of the actors in a maximum likelihood framework. [sent-7, score-0.508]
</p><p>3 We present results on a challenging movie example, with 81% recall in actor detection (coverage) and 89% precision in actor identification (naming). [sent-8, score-1.586]
</p><p>4 Introduction Detecting and naming actors in movies is important for content-based indexing and retrieval of movie scenes  and can also be used to support statistical analysis of film style. [sent-10, score-0.571]
</p><p>5 Additionally, detecting and naming actors in unedited footage can be useful for post-production. [sent-11, score-0.527]
</p><p>6 Methods for learning such features are desired to improve the recall and precision of actor detection in long movie scenes where the appearance of actors is consistent over time. [sent-13, score-1.289]
</p><p>7 Contributions We propose a complete framework to learn view-independent actor models using maximally stable color regions (MSCR) [10] with a novel clustering algorithm. [sent-14, score-0.853]
</p><p>8 The actor’s head and shoulders are represented as constellations of color blobs where the appearance of each blob is represented in a 9 dimensional space combining color, size, shape and position relative to the actor’s coordinate system, together with a frequency term. [sent-15, score-0.714]
</p><p>9 fr k-nearest neighbours corresponding to the actor by just using the appearance of the blobs in the model. [sent-19, score-1.072]
</p><p>10 The second  stage is a sliding window search for the best localization of the actor in position and scale. [sent-20, score-0.893]
</p><p>11 By repeating those two steps for all actors at all sizes, we obtain detection windows and actor names that maximize the posterior likelihood of each video frame. [sent-21, score-1.222]
</p><p>12 First, we briefly review related work in generic and specific actor detection in movies. [sent-23, score-0.756]
</p><p>13 Related work Much previous work on actor detection and recognition has been based on face detection. [sent-29, score-0.781]
</p><p>14 In one of their examples, they report that 42% of actor appearances are frontal, 21% profile and 37% are actors facing away from the camera. [sent-34, score-1.176]
</p><p>15 Indeed, generic meth-  ods for detecting upper-body or full-body actors have been proposed by Dalal et al. [sent-36, score-0.412]
</p><p>16 While such methods can potentially increase the coverage of actor detectors by detecting actors in profile and back views, they also suffer from the higher variability of actor appearances in such views. [sent-39, score-2.002]
</p><p>17 (c) The blobs chosen for training shown with the head and the torso partition. [sent-46, score-0.44]
</p><p>18 (d) The color blobs scaled and shifted to the normalized actor coordinate system which is represented by the red axis. [sent-47, score-1.051]
</p><p>19 While the work in [18] and [15] focuses on first obtaining tracks and later classifying or hand labeling them, our method can directly perform actor specific detections on individual frames. [sent-49, score-0.771]
</p><p>20 Our actor model is simpler since we only model two body parts (head and shoulders)  but each part is an arbitrarily complex constellation model of color blobs. [sent-54, score-0.824]
</p><p>21 It is difficult to apply such models to actor appearances because interest points and their image features are typically  Figure 2. [sent-58, score-0.772]
</p><p>22 Training images from different viewpoints are merged to obtain an actor model. [sent-59, score-0.75]
</p><p>23 We resolve this issue by representing parts of actors with color regions rather than local features. [sent-61, score-0.449]
</p><p>24 We extend such previous work to the more difficult problem of ”re-detecting” actors where the generic detectors fail due to variations in pose and viewpoints, partial occlusions, etc. [sent-63, score-0.4]
</p><p>25 Generative model In this section, we introduce our generative model for the appearance of actors and describe a method for learning the model from a small number of individual keyframes or short video tracks. [sent-67, score-0.517]
</p><p>26 Our model is designed to incorporate the costume of the actor and to be robust to changes in viewpoint and pose. [sent-68, score-0.76]
</p><p>27 We make one important assumption that the actor is in an upright position and that both the head and the shoulders are visible. [sent-69, score-0.87]
</p><p>28 As a result, we model the actor with two image windows for the head and shoulders, in a normalized coordinate system with the origin at the actor’s neck, and with unit size set to twice the height of the actor’s eyes relative to the origin. [sent-70, score-0.834]
</p><p>29 Appearance models for all 8 actors in the movie ”Rope” [11]  extends from (−1, −1) to (1, 0) and the shoulder region exteexntdesn dfrso fmro (m− (1−, 10), −to1 ()1 t,o o3 (). [sent-75, score-0.523]
</p><p>30 More specifically, we associate with each actor a visual vocabulary of color blobs Ci described in terms of their normalized coordinates xi, yi, sizes si, colors ci and shapes mi, and their frequencies Hi. [sent-77, score-1.072]
</p><p>31 Color blobs above the actor’s origin are labeled as ”head” features and color blobs under the origin are labeled as ”shoulder” features. [sent-78, score-0.68]
</p><p>32 Formally, our generative model for each actor consists in the following three steps: 1. [sent-81, score-0.749]
</p><p>33 Choose screen location and window size for the actor on the screen, using the detections in the previous  frame as a prior. [sent-82, score-0.873]
</p><p>34 Maximally stable color regions The maximally stable color regions (MSCR) feature is a color extension of the maximally stable extremal region (MSER) feature [10]. [sent-91, score-0.355]
</p><p>35 These approximated ellipses are termed as color blobs in the later part ofthe text. [sent-98, score-0.34]
</p><p>36 The actual choice of samples is not very important, as long as we cover the entire range of appearances ofthe actor (we try to keep the training set equally sampled across different views i. [sent-104, score-0.811]
</p><p>37 Ideally a sequence of each actor performing a 360 degree turn would be sufficient to build such models. [sent-107, score-0.711]
</p><p>38 We then collect the color blobs in all training windows, center and resize them, and assign them to ac333777000866  (a)(b) Figure 4. [sent-110, score-0.34]
</p><p>39 Example of two independent randomly generated blob images given the actor and background models. [sent-111, score-0.894]
</p><p>40 We cluster the blobs for all actors using a constrained agglomerative clustering. [sent-114, score-0.753]
</p><p>41 For every actor in n training images we get n set of blobs (f1, f2 , . [sent-115, score-1.011]
</p><p>42 , fn) with varying number of blobs in each set, where each blob is represented as a 9 dimensional vector in normalized actor coordinates. [sent-119, score-1.194]
</p><p>43 We then compute pairwise matching between those clusters and the blobs in the next set f2. [sent-122, score-0.334]
</p><p>44 Note that the number of clusters per actor is variable. [sent-132, score-0.745]
</p><p>45 As a result, actors with more complex appearances can be represented with a larger number of clusters. [sent-133, score-0.443]
</p><p>46 The appearance models for eight different actors are shown in Fig 3. [sent-134, score-0.414]
</p><p>47 Our framework searches for actors over a variety of scales, from foreground (larger scales) to background (smaller scales). [sent-137, score-0.382]
</p><p>48 For each actor we first perform a search space reduction using kNN-search. [sent-138, score-0.731]
</p><p>49 2: for each actor a do 3: for each scale s do 4: Normalize image features w. [sent-146, score-0.731]
</p><p>50 7: for each position (x, y) do 8: Find blob indices Jhead and Jshoulders 9: Compute mij using blob indices and inverted indices  score(x, y, s, a) = ? [sent-154, score-0.644]
</p><p>51 This gives us a initial set of blobs B over which we perform a refinement step using kNN search given the actor model and the particular scale. [sent-159, score-1.056]
</p><p>52 Firstly for each blob within the sliding window we only require to compare it with its corresponding entries in the inverted index table instead of doing an exhaustive search. [sent-169, score-0.373]
</p><p>53 (d) Refined set of blobs for the given actor at given scale, based on  appearance. [sent-178, score-1.011]
</p><p>54 (g) The corresponding matched cluster centers in the given actor model. [sent-181, score-0.781]
</p><p>55 Sliding window search We now proceed to define the detection scores for all actors at all positions and scales using a sliding window approach. [sent-189, score-0.635]
</p><p>56 Each actor detection score is based on the likelihood that the image in the sliding window was generated by the actor model using the previous frame detections as prior information. [sent-190, score-1.699]
</p><p>57 In practice we compute MSCR features in the best available scale and then shift and scale the blobs respectively while searching at different scales. [sent-191, score-0.34]
</p><p>58 During recognition, we similarly normalize the size, shape and position of blobs relative to the sliding window. [sent-193, score-0.405]
</p><p>59 This ensures that all computations are performed in reduced actor coordinates. [sent-194, score-0.711]
</p><p>60 We represent B as the set of all blobs detected in the image and Ca as the set of cluster centers in the model for a given actor a. [sent-196, score-1.095]
</p><p>61 Given a sliding window at position (x, y) and scale s, we find all blobs centered within the sliding window and assign the blobs indices Jhead and Jshoulders. [sent-197, score-0.953]
</p><p>62 The term P(Bj , mij , a) is the similarity function between the model cluster Cia and the corresponding matched blob Bj in nine dimensional space (position, size, color and shape), which is defined as follows:  P(Bj,mij,a) =? [sent-204, score-0.385]
</p><p>63 where Cia is the center for cluster iin the actor model and Σia is its covariance matrix. [sent-207, score-0.783]
</p><p>64 A distinctive feature of our detection framework is that it requires us to find a partial assignment mij between blobs in the the sliding window and clusters in the model. [sent-208, score-0.602]
</p><p>65 More precisely, we compute  such that each blob in the sliding window is assigned to at most one cluster, each cluster is assigned to at most one b? [sent-209, score-0.366]
</p><p>66 this method produces significantly better results than computing the average score over all possible blobto-cluster assignments, where the same blob may be assigned to multiple clusters, and the same cluster to multiple blobs, which is prone to detection errors. [sent-219, score-0.312]
</p><p>67 where, l1,1 and l1,0 measures the probability that the same actor is observed in consecutive frames. [sent-223, score-0.711]
</p><p>68 When the actor is not present in the previous frame, all positions in next frame are equally probable. [sent-224, score-0.743]
</p><p>69 When the actor is present in both frames, we assume the new position to be close to the previous position, within some covariance term Σpos. [sent-225, score-0.762]
</p><p>70 Comparison of results on recall and precision for actor detection using Upper body detector(UBD), Color Blob detector(CBD) and Combined method (UBD-CBD) are in fact independent on the choice of actor or movie. [sent-227, score-1.539]
</p><p>71 Benefiting from the fact that an actor can only appear  once on a frame, we then search for the best possible positions [x∗ (a) , y∗ (a) , s∗ (a)] which maximizes the total score over all actors. [sent-234, score-0.763]
</p><p>72 Dataset As mentioned earlier the previous work in actor specific models have focused on obtaining tracks and then performing classification on the obtained tracks. [sent-241, score-0.734]
</p><p>73 Our method on the other hand can perform direct detection using actor specific models, even on keyframes which is a much harder task than classifying tracks and we target the scenario where it is difficult to obtain face or upper body tracks. [sent-243, score-0.916]
</p><p>74 Comparison of recall with increasing number of actors in UBD, CBD and combined cases keyframes at equal intervals, a frame every 10 seconds from the movie ”Rope” [11] by Alfred Hitchcock. [sent-247, score-0.579]
</p><p>75 This dataset presents significant scale and viewpoint variations for each actor with presence of motion and focus blur. [sent-248, score-0.731]
</p><p>76 The lighting changes considerably during the movie and the clothing appearance of all the actors remains consistent which makes it suitable for our experiments. [sent-250, score-0.526]
</p><p>77 There are 8 different actors in the entire movie (except the initial victim and Alfred Hitchcock himself). [sent-252, score-0.469]
</p><p>78 The number of appearances per actor vary between 38 and 275. [sent-254, score-0.772]
</p><p>79 All 443 frames were hand labeled with the names and screen locations for all actors to serve as ground truth. [sent-255, score-0.458]
</p><p>80 Actor identification results for all 8 actors in Rope dataset (percentages). [sent-259, score-0.382]
</p><p>81 We ran our detection and recognition algorithm using the built actor models, on all 443 frames and compared the results with the ground truth. [sent-261, score-0.756]
</p><p>82 Fig 6 shows the results on recall and precision of actor detection using the proposed method (CBD) and it is compared with the state of the art Upper Body Detector (UBD)2 and the combined case where we merged the detections obtained from both the methods individually. [sent-262, score-0.844]
</p><p>83 Results demonstrate an increase in recall from about 57 percent in UBD to 70 percent in proposed Color blob detector (CBD) to about 81 percent in combined approach for a similar precision. [sent-263, score-0.323]
</p><p>84 Some detection results from Rope dataset using proposed method CBD (in red) with recognized actor names on top left, UBD (in yellow) and not detected by both (in green). [sent-270, score-0.828]
</p><p>85 Notice how our method is able to detect and identify the actors in the presence of multiple actors with partial occlusions [e. [sent-271, score-0.782]
</p><p>86 (e)], merging of foreground blobs with the background due to low illumination [e. [sent-283, score-0.3]
</p><p>87 Fig 7 we plot recall rates for both UBD and CBD and combined case, with different number of actors present in the frame and it shows that the proposed method gives consistent results with varying number of actors. [sent-293, score-0.446]
</p><p>88 Recognition results on the detected actors are presented in Table 5. [sent-294, score-0.414]
</p><p>89 As can be seen, our method not only increases the average recall rate for all actors, but also correctly names all actors with an average precision of 89 percent despite the large number of back views and partial occlusions. [sent-296, score-0.549]
</p><p>90 Some of the example detections results are shown in Fig 8 and they demonstrate how our method performs well even with severe cases of occlusions, viewpoint scale and pose variations in a multi actor scenario. [sent-297, score-0.768]
</p><p>91 In 9(a) the blobs in the torso region of the undetected actor gets merged with the background, heavy blur causes mis-detection in 9(b), torso is largely occluded in 9(c). [sent-300, score-1.173]
</p><p>92 In fourth instance 9(d), the hand gets detected as the head and blobs below as torso, leading to a false detection. [sent-301, score-0.41]
</p><p>93 Conclusions We have presented a generative appearance model for detecting and naming actors in movies that can be learned from a small number of training examples. [sent-307, score-0.584]
</p><p>94 We have shown that low dimensional features like MSCR that were previously used for actor re-identification can also support actor detection, even in difficult multiple actors scenarios. [sent-308, score-1.804]
</p><p>95 Results show significant increase in coverage (recall) for actor detection maintaining high precision. [sent-309, score-0.803]
</p><p>96 To our knowledge, this is the first time that a generative appearance model is  demonstrated on the task of detecting and recognizing actors from arbitrary viewpoints. [sent-310, score-0.482]
</p><p>97 Our method also appears to be a good candidate for tracking multiple actors constantly changing viewpoints and occluding each other in long video sequences such as ”Rope”, which include important application scenarios, such as unedited, raw video footage and recordings of live performances. [sent-311, score-0.46]
</p><p>98 We also plan to investigate weakly supervised methods by extracting actor labels from temporally aligned movie scripts [17, 2]. [sent-312, score-0.825]
</p><p>99 One obvious limitation of our method is that it only handles cases where the appearance of actors does not change much over time. [sent-313, score-0.414]
</p><p>100 In future work, we are planning to investigate extensions with mutually-exclusive appearances per actor, so that actors can change their appearances and costumes over time. [sent-314, score-0.504]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('actor', 0.711), ('actors', 0.382), ('blobs', 0.3), ('blob', 0.183), ('mscr', 0.12), ('rope', 0.114), ('fig', 0.098), ('ubd', 0.098), ('mij', 0.092), ('movie', 0.087), ('cbd', 0.087), ('head', 0.078), ('sliding', 0.074), ('naming', 0.066), ('torso', 0.062), ('appearances', 0.061), ('idx', 0.058), ('window', 0.057), ('shoulder', 0.054), ('ronfard', 0.053), ('cluster', 0.052), ('shoulders', 0.05), ('costume', 0.049), ('bj', 0.049), ('cia', 0.048), ('coverage', 0.047), ('keyframes', 0.046), ('detection', 0.045), ('maximally', 0.044), ('names', 0.04), ('color', 0.04), ('brandon', 0.04), ('singleton', 0.04), ('knn', 0.04), ('indices', 0.04), ('body', 0.04), ('views', 0.039), ('generative', 0.038), ('detections', 0.037), ('screen', 0.036), ('percent', 0.036), ('movies', 0.036), ('inverted', 0.035), ('optional', 0.035), ('clusters', 0.034), ('constellation', 0.033), ('alfred', 0.033), ('janet', 0.033), ('jhead', 0.033), ('ljk', 0.033), ('remi', 0.033), ('appearance', 0.032), ('detected', 0.032), ('recall', 0.032), ('score', 0.032), ('frame', 0.032), ('stable', 0.031), ('position', 0.031), ('detecting', 0.03), ('neighbours', 0.029), ('unedited', 0.029), ('sivic', 0.028), ('regions', 0.027), ('scripts', 0.027), ('vineet', 0.027), ('pt', 0.026), ('upper', 0.026), ('clothing', 0.025), ('benefiting', 0.025), ('grenoble', 0.025), ('windows', 0.025), ('face', 0.025), ('refinement', 0.025), ('exhaustive', 0.024), ('tracks', 0.023), ('person', 0.022), ('ca', 0.022), ('pictorial', 0.022), ('profile', 0.022), ('ci', 0.021), ('coherence', 0.021), ('back', 0.02), ('covariance', 0.02), ('shots', 0.02), ('eichner', 0.02), ('footage', 0.02), ('inria', 0.02), ('scale', 0.02), ('search', 0.02), ('failure', 0.02), ('origin', 0.02), ('viewpoints', 0.02), ('agglomerative', 0.019), ('merged', 0.019), ('video', 0.019), ('leonardis', 0.019), ('blur', 0.019), ('matched', 0.018), ('occlusions', 0.018), ('detectors', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="120-tfidf-1" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>Author: Vineet Gandhi, Remi Ronfard</p><p>Abstract: We introduce a generative model for learning person and costume specific detectors from labeled examples. We demonstrate the model on the task of localizing and naming actors in long video sequences. More specifically, the actor’s head and shoulders are each represented as a constellation of optional color regions. Detection can proceed despite changes in view-point and partial occlusions. We explain how to learn the models from a small number of labeled keyframes or video tracks, and how to detect novel appearances of the actors in a maximum likelihood framework. We present results on a challenging movie example, with 81% recall in actor detection (coverage) and 89% precision in actor identification (naming).</p><p>2 0.17448376 <a title="120-tfidf-2" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>3 0.14251798 <a title="120-tfidf-3" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>Author: Michalis Raptis, Leonid Sigal</p><p>Abstract: In this paper, we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework, where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and minefor hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive with the state of the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.</p><p>4 0.091508612 <a title="120-tfidf-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.07982672 <a title="120-tfidf-5" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>6 0.074610248 <a title="120-tfidf-6" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>7 0.071149871 <a title="120-tfidf-7" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>8 0.068064362 <a title="120-tfidf-8" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>9 0.06716378 <a title="120-tfidf-9" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>10 0.063474417 <a title="120-tfidf-10" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>11 0.062759846 <a title="120-tfidf-11" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>12 0.062185071 <a title="120-tfidf-12" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>13 0.060961932 <a title="120-tfidf-13" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>14 0.05876305 <a title="120-tfidf-14" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>15 0.057608809 <a title="120-tfidf-15" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>16 0.055364255 <a title="120-tfidf-16" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>17 0.055300426 <a title="120-tfidf-17" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>18 0.054928415 <a title="120-tfidf-18" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>19 0.054369621 <a title="120-tfidf-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.054338988 <a title="120-tfidf-20" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, -0.021), (2, 0.008), (3, -0.056), (4, -0.018), (5, 0.009), (6, 0.031), (7, -0.01), (8, 0.042), (9, -0.025), (10, -0.019), (11, 0.005), (12, 0.006), (13, -0.003), (14, 0.029), (15, -0.0), (16, 0.025), (17, 0.006), (18, -0.009), (19, -0.034), (20, 0.023), (21, 0.042), (22, -0.037), (23, 0.054), (24, -0.003), (25, -0.014), (26, 0.012), (27, -0.021), (28, -0.02), (29, -0.049), (30, 0.027), (31, 0.043), (32, 0.011), (33, 0.029), (34, 0.026), (35, -0.005), (36, 0.001), (37, 0.043), (38, 0.018), (39, -0.054), (40, -0.027), (41, 0.02), (42, -0.03), (43, -0.002), (44, -0.004), (45, -0.033), (46, -0.002), (47, -0.019), (48, 0.038), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87441713 <a title="120-lsi-1" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>Author: Vineet Gandhi, Remi Ronfard</p><p>Abstract: We introduce a generative model for learning person and costume specific detectors from labeled examples. We demonstrate the model on the task of localizing and naming actors in long video sequences. More specifically, the actor’s head and shoulders are each represented as a constellation of optional color regions. Detection can proceed despite changes in view-point and partial occlusions. We explain how to learn the models from a small number of labeled keyframes or video tracks, and how to detect novel appearances of the actors in a maximum likelihood framework. We present results on a challenging movie example, with 81% recall in actor detection (coverage) and 89% precision in actor identification (naming).</p><p>2 0.67593974 <a title="120-lsi-2" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>3 0.6564936 <a title="120-lsi-3" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>4 0.64926028 <a title="120-lsi-4" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>Author: Varun Ramakrishna, Takeo Kanade, Yaser Sheikh</p><p>Abstract: The human body is structurally symmetric. Tracking by detection approaches for human pose suffer from double counting, where the same image evidence is used to explain two separate but symmetric parts, such as the left and right feet. Double counting, if left unaddressed can critically affect subsequent processes, such as action recognition, affordance estimation, and pose reconstruction. In this work, we present an occlusion aware algorithm for tracking human pose in an image sequence, that addresses the problem of double counting. Our key insight is that tracking human pose can be cast as a multi-target tracking problem where the ”targets ” are related by an underlying articulated structure. The human body is modeled as a combination of singleton parts (such as the head and neck) and symmetric pairs of parts (such as the shoulders, knees, and feet). Symmetric body parts are jointly tracked with mutual exclusion constraints to prevent double counting by reasoning about occlusion. We evaluate our algorithm on an outdoor dataset with natural background clutter, a standard indoor dataset (HumanEva-I), and compare against a state of the art pose estimation algorithm.</p><p>5 0.6226939 <a title="120-lsi-5" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>6 0.62154055 <a title="120-lsi-6" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>7 0.60216963 <a title="120-lsi-7" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>8 0.59237784 <a title="120-lsi-8" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>9 0.58838302 <a title="120-lsi-9" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>10 0.58173651 <a title="120-lsi-10" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>11 0.57875913 <a title="120-lsi-11" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>12 0.57861292 <a title="120-lsi-12" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>13 0.57479393 <a title="120-lsi-13" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>14 0.55486721 <a title="120-lsi-14" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>15 0.55479664 <a title="120-lsi-15" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>16 0.55385584 <a title="120-lsi-16" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>17 0.55246395 <a title="120-lsi-17" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>18 0.54351389 <a title="120-lsi-18" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>19 0.53342301 <a title="120-lsi-19" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>20 0.53114218 <a title="120-lsi-20" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.076), (16, 0.034), (26, 0.053), (28, 0.018), (33, 0.235), (62, 0.011), (67, 0.105), (69, 0.046), (80, 0.023), (87, 0.059), (93, 0.251)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84054101 <a title="120-lda-1" href="./cvpr-2013-Hyperbolic_Harmonic_Mapping_for_Constrained_Brain_Surface_Registration.html">208 cvpr-2013-Hyperbolic Harmonic Mapping for Constrained Brain Surface Registration</a></p>
<p>Author: Rui Shi, Wei Zeng, Zhengyu Su, Hanna Damasio, Zhonglin Lu, Yalin Wang, Shing-Tung Yau, Xianfeng Gu</p><p>Abstract: Automatic computation of surface correspondence via harmonic map is an active research field in computer vision, computer graphics and computational geometry. It may help document and understand physical and biological phenomena and also has broad applications in biometrics, medical imaging and motion capture. Although numerous studies have been devoted to harmonic map research, limited progress has been made to compute a diffeomorphic harmonic map on general topology surfaces with landmark constraints. This work conquer this problem by changing the Riemannian metric on the target surface to a hyperbolic metric, so that the harmonic mapping is guaranteed to be a diffeomorphism under landmark constraints. The computational algorithms are based on the Ricci flow method and the method is general and robust. We apply our algorithm to study constrained human brain surface registration problem. Experimental results demonstrate that, by changing the Riemannian metric, the registrations are always diffeomorphic, and achieve relative high performance when evaluated with some popular cortical surface registration evaluation standards.</p><p>same-paper 2 0.80914605 <a title="120-lda-2" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>Author: Vineet Gandhi, Remi Ronfard</p><p>Abstract: We introduce a generative model for learning person and costume specific detectors from labeled examples. We demonstrate the model on the task of localizing and naming actors in long video sequences. More specifically, the actor’s head and shoulders are each represented as a constellation of optional color regions. Detection can proceed despite changes in view-point and partial occlusions. We explain how to learn the models from a small number of labeled keyframes or video tracks, and how to detect novel appearances of the actors in a maximum likelihood framework. We present results on a challenging movie example, with 81% recall in actor detection (coverage) and 89% precision in actor identification (naming).</p><p>3 0.7761566 <a title="120-lda-3" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>Author: Kota Hara, Rama Chellappa</p><p>Abstract: We present a hierarchical method for human pose estimation from a single still image. In our approach, a dependency graph representing relationships between reference points such as bodyjoints is constructed and thepositions of these reference points are sequentially estimated by a successive application of multidimensional output regressions along the dependency paths, starting from the root node. Each regressor takes image features computed from an image patch centered on the current node ’s position estimated by the previous regressor and is specialized for estimating its child nodes ’ positions. The use of the dependency graph allows us to decompose a complex pose estimation problem into a set of local pose estimation problems that are less complex. We design a dependency graph for two commonly used human pose estimation datasets, the Buffy Stickmen dataset and the ETHZ PASCAL Stickmen dataset, and demonstrate that our method achieves comparable accuracy to state-of-the-art results on both datasets with significantly lower computation time than existing methods. Furthermore, we propose an importance weighted boosted re- gression trees method for transductive learning settings and demonstrate the resulting improved performance for pose estimation tasks.</p><p>4 0.76310921 <a title="120-lda-4" href="./cvpr-2013-A_Principled_Deep_Random_Field_Model_for_Image_Segmentation.html">24 cvpr-2013-A Principled Deep Random Field Model for Image Segmentation</a></p>
<p>Author: Pushmeet Kohli, Anton Osokin, Stefanie Jegelka</p><p>Abstract: We discuss a model for image segmentation that is able to overcome the short-boundary bias observed in standard pairwise random field based approaches. To wit, we show that a random field with multi-layered hidden units can encode boundary preserving higher order potentials such as the ones used in the cooperative cuts model of [11] while still allowing for fast and exact MAP inference. Exact inference allows our model to outperform previous image segmentation methods, and to see the true effect of coupling graph edges. Finally, our model can be easily extended to handle segmentation instances with multiple labels, for which it yields promising results.</p><p>5 0.76178831 <a title="120-lda-5" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>6 0.74999571 <a title="120-lda-6" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>7 0.73748809 <a title="120-lda-7" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>8 0.73701173 <a title="120-lda-8" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>9 0.7329306 <a title="120-lda-9" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>10 0.73284382 <a title="120-lda-10" href="./cvpr-2013-Area_Preserving_Brain_Mapping.html">44 cvpr-2013-Area Preserving Brain Mapping</a></p>
<p>11 0.73200005 <a title="120-lda-11" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>12 0.73176235 <a title="120-lda-12" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>13 0.73043585 <a title="120-lda-13" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>14 0.7292735 <a title="120-lda-14" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>15 0.72865772 <a title="120-lda-15" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>16 0.72837991 <a title="120-lda-16" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>17 0.72772241 <a title="120-lda-17" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>18 0.72698855 <a title="120-lda-18" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>19 0.72673488 <a title="120-lda-19" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>20 0.72636771 <a title="120-lda-20" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
