<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-236" href="#">cvpr2013-236</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</h1>
<br/><p>Source: <a title="cvpr-2013-236-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/He_K-Means_Hashing_An_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>Reference: <a title="cvpr-2013-236-reference" href="../cvpr2013_reference/cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 K-means Hashing: an Affinity-Preserving Quantization Method for Learning Binary Compact Codes Kaiming He Fang Wen Microsoft Research Asia  Jian Sun  Abstract In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. [sent-1, score-0.766]
</p><p>2 The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. [sent-2, score-0.59]
</p><p>3 Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. [sent-3, score-0.7]
</p><p>4 In this paper, we present a hashing method adopting the k-means quantization. [sent-4, score-0.59]
</p><p>5 The distance between the cells is approximated by the Hamming distance of the cell indices. [sent-6, score-0.241]
</p><p>6 Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods. [sent-8, score-0.65]
</p><p>7 Introduction Approximate nearest neighbors (ANN) search is wide-  ly applied in image/video retrieval [27, 11], recognition [28], image classification [23, 2], pose estimation [25], and many other computer vision problems. [sent-10, score-0.148]
</p><p>8 An issue of particular interest in ANN search is to use compact representations to approximate the data distances (or their ranking orders). [sent-11, score-0.205]
</p><p>9 We observe these compact encoding methods commonly involve two stages: (i) quantization - the feature space is partitioned into a number of non-overlapping cells with a unique index (code) for each cell; and (ii) distance computation based on the indices. [sent-13, score-0.527]
</p><p>10 Existing Hamming-based methods (mostly termed as hashing) quantize the space using hyperplanes [1, 13, 29, 5, 19] or kernelized hyperplanes [3 1, 14, 15]. [sent-14, score-0.278]
</p><p>11 dh(1c01y,c01 )  (a) hashing  (b) k-means  (c) our method  Figure 1: Compact encoding methods. [sent-18, score-0.65]
</p><p>12 A black line denotes a partition boundary, a circle denotes a k-means center, and a cross denotes a sample vector. [sent-19, score-0.115]
</p><p>13 Here d denotes Euclidean distance, and dh denotes Hamming-based distance. [sent-20, score-0.251]
</p><p>14 and is often determined by the sign of one hashing function. [sent-22, score-0.59]
</p><p>15 The distance between two samples is approximated by the Hamming distance between their indices (Fig. [sent-23, score-0.248]
</p><p>16 K-means is a more adaptive quantization method than those using hyperplanes, and is optimal in the sense of minimizing quantization error [7]. [sent-26, score-0.6]
</p><p>17 The distance between two samples is approximated by the distance between the k-means centers (Fig. [sent-27, score-0.168]
</p><p>18 1(b)), which can be read from pre-computed lookup tables given the center indices. [sent-28, score-0.179]
</p><p>19 The product quantization [7, 10] is a way of applying k-means-based quantization for a larger number of bits (e. [sent-29, score-0.772]
</p><p>20 The lookup-based methods like [3, 10] has been shown more accurate than some Hamming methods with the same code-length, thanks  1The Hamming distance is defined as the number of different bits between two binary codes. [sent-33, score-0.256]
</p><p>21 222999333866  to the adaptive k-means quantization and the more flexible distance lookup. [sent-36, score-0.352]
</p><p>22 However, the lookup-based distance computation is slower than the Hamming distance computation2. [sent-37, score-0.2]
</p><p>23 Hamming methods also have the advantage that the distance computation is problem-independent: they involve only an encoding stage but no decoding stage (i. [sent-38, score-0.176]
</p><p>24 We propose a novel scheme: we partition the feature space by k-means-based quantization, but approximate the distance by the Hamming distance between the cell indices (Fig. [sent-45, score-0.355]
</p><p>25 A naive solution would be first to quantize the space using k-means and then assign distance-preserving indices to the cells. [sent-48, score-0.211]
</p><p>26 To this end, we propose a novel quantization algorithm called Affinity-Preserving K-means which simultaneously takes both quantization and distance approximation into  account. [sent-50, score-0.62]
</p><p>27 Our method, named as K-means Hashing (KMH), enjoys the benefits of adaptive k-means quantization and fast Hamming distance computation. [sent-53, score-0.403]
</p><p>28 We point out that the terminology “hashing” in [22] refers to the classical hashing strategy of distributing data into buckets so that similar data would collide in the same bucket. [sent-55, score-0.701]
</p><p>29 In this paper we follow the terminology in many other recent papers [1, 3 1, 14, 29, 19, 15] where “hashing” refers to compact binary encoding with Hamming distance computation. [sent-56, score-0.255]
</p><p>30 Basic Model Our basic model is to quantize the feature space in a kmeans fashion and compute the approximate distance via the Hamming distance of the cell indices. [sent-61, score-0.323]
</p><p>31 Following the classical vector quantization (VQ) [7], we map a d-dimensional vector x ∈ Rd to another vector qw(ex )m a∈p C a d=- {imcie n| ci ∈al R vedc, t0o r≤ x xi ∈ ∈≤ R k − 1}. [sent-62, score-0.4]
</p><p>32 Given b bits for indexing, there are 2In our experiments, it is 10-20 times slower per distance computation in product quantization than in hashing methods. [sent-65, score-1.21]
</p><p>33 VQ assigns any vector to its nearest codeword in the codebook. [sent-67, score-0.154]
</p><p>34 Usually the codewords are given by the k-means centers, as they provide minimal quantization error [7]. [sent-68, score-0.438]
</p><p>35 VQ approximates the distance between any two vectors x and y by the distance of their codewords: d(x, y)  ? [sent-69, score-0.204]
</p><p>36 The above notation highlights that the distance computation solely depends on the indices: it can be read from a k-by-k pre-computed lookup table d(·, ·). [sent-75, score-0.236]
</p><p>37 of fast Hamming distance computation, we approximate the lookup-based distance using the Hamming distance: −  d(ci(x) , ci(y) )  ? [sent-77, score-0.209]
</p><p>38 dh (i(x) , i(y))  (2)  where dh is defined as a Hamming-based distance between any two indices iand j: dh(i,j) ? [sent-78, score-0.546]
</p><p>39 The square root is essential: it relates our method to orthogonal hashing methods (Sec. [sent-81, score-0.651]
</p><p>40 The usage of s is because the Euclidean distance d can be in arbitrary range, while the Hamming distance h is constrained in [0, b] given b bits. [sent-86, score-0.168]
</p><p>41 In sum, given a codebook C we approximate the distance d(xI,n ny s)u through d ah c c(iod(xe)b , oio(kyC)) w(see ea Fig. [sent-90, score-0.176]
</p><p>42 e Itth ies doibsvtainoucse that the indexing of the codewords affects the approximate distance. [sent-92, score-0.148]
</p><p>43 A Naive Two-step Method A naive two-step solution to the above model would be: first quantize via k-means with k = 2b codewords, and then assign optimal indices to the codewords. [sent-95, score-0.211]
</p><p>44 (4)  This equation minimizes the difference between two k-by-k affinity matrices d(·, ·) and dh (·, ·). [sent-113, score-0.295]
</p><p>45 This is because the k-means quantization would generate an affinity matrix d(·, ·) of arbitrary range. [sent-123, score-0.372]
</p><p>46 The affinity fitting error as in (4) is a quantity not concerned in the first-step k-means quantization. [sent-128, score-0.143]
</p><p>47 This motivates us to simultaneously minimize the quantization error and the affinity error. [sent-129, score-0.411]
</p><p>48 The classical k-means algorithm [18] minimizes the average quantization error Equan of the training samples:  Equan=1n? [sent-130, score-0.35]
</p><p>49 (5) In the clas-  wsichaelr ek- Sme isan ths eth tirsa error sise tm winitihm nize sda by an ExpectationMaximization (EM) alike algorithm: alternatively assign the sample indices i(x) and update the codewords {ci}. [sent-134, score-0.267]
</p><p>50 We consider the affinity error Eaff between all sample pairs:  Eaff=n12x? [sent-138, score-0.143]
</p><p>51 Putting the quantization error and affinity error together, we minimize the following objective function: E  = Equan + λEaff,  (7)  where λ is a fixed weight (in this paper we use 10). [sent-154, score-0.45]
</p><p>52 c Thh sample x is assigned to its nearest codeword in the codebook {ci}. [sent-160, score-0.205]
</p><p>53 So we sequentially optimize each individual codeword cj with other {ci}i? [sent-164, score-0.134]
</p><p>54 The above iterative algorithm needs to initialize the indices i(x), codebook C, and scale s in (3). [sent-175, score-0.155]
</p><p>55 To obtain the corresponding codebook C and the scale s, we need to build a relation binegtw coeedne existing hashing cmaeleth so,d ws ean nde our om beutihlodd a. [sent-178, score-0.668]
</p><p>56 2: repeat 3: Assignment: for ∀x ∈ S update i(x) by x’s nearest Acosdseigwnomrde’ns ti:n dfoerx. [sent-191, score-0.118]
</p><p>57 Relation to Existing Methods Our method would become classical vector quantization methods or hashing methods if we relax or strengthen some constraints. [sent-195, score-0.901]
</p><p>58 If we allow to use pre-computed lookup tables d(·, ·), we can remove ttohe affinity error tuetermd lEooafkf uinp ( ta7)b by setting = 0. [sent-198, score-0.292]
</p><p>59 Figure 2: Relation between vector quantization and orthogonal hashing. [sent-214, score-0.329]
</p><p>60 The partition boundaries of the cells are orthogonal hyperplanes (lines in 2-d). [sent-217, score-0.202]
</p><p>61 If we set λ = ∞ in (7) so d(·, ·) and dh (·, ·) must be idenIftic wale, minimizing (i7n) ( 7is) equivalent atnod dth de hashing umstet bheo idcalled Iterative Quantization (ITQ) [5]. [sent-219, score-0.806]
</p><p>62 If the two lookup tables d(·, ·) and dh (·, ·) are identical, the k = 2b codewtabolredss md(u·,st·) ) b aen tdak den( f·,r·o)m a rthee i dveenrtteicxael,s othfe a kb-d =im 2ensional hyper-cube. [sent-223, score-0.34]
</p><p>63 It is easy {tor see trhea bt- tdhiem partition boortuhnodgaornieasl bofa stehes resulting cte isl s e are orthogonal hyperplanes (e. [sent-229, score-0.17]
</p><p>64 The Euclidean distance between any two codewords equals to the Hamming-based distance: d2(ci, cj) = s2 · h(i,j) = dh2(i,j). [sent-233, score-0.191]
</p><p>65 λ A =s ∞ sius equivalent ntog minimizing teh feu quantization error λw. [sent-240, score-0.332]
</p><p>66 A Geometric View As discussed above, any orthogonal hashing method (e. [sent-254, score-0.651]
</p><p>67 , ITQ [5] and PCAH [29, 5]) can be considered as a vector quantization method using the vertexes of a rotated hyper-cube as the codewords. [sent-256, score-0.302]
</p><p>68 Although ITQ has the minimal quantization error Equan among orthogonal hashing methods, our method 222999334199  achieves smaller Equan thanks to the stretching, as shown in the tables in Fig. [sent-262, score-1.041]
</p><p>69 We also evaluate the empirical mean distance error Edist:  Edist=n12x? [sent-264, score-0.123]
</p><p>70 Though the methods are given 3 bits and at most 23=8 clusters, both ITQ and our method divide the data into roughly two clusters (colored in Fig. [sent-279, score-0.169]
</p><p>71 The codewords of these two clusters are on the diagonal of the hyper-cube, having the max possible Hamming distance (=3 here). [sent-281, score-0.221]
</p><p>72 Although it seems inefficient to use 3 bits to encode two clusters, it is worthwhile if preserving distance is the concern. [sent-282, score-0.223]
</p><p>73 The product quantization (PQ) method [10] addresses this issue by separately training k-means in the subspaces of a product space. [sent-288, score-0.51]
</p><p>74 Any codeword c in the product space RD is a concatenation of M sub-codewords drawn from the M sub-codebooks. [sent-301, score-0.214]
</p><p>75 (1)), PQ approximates the distance between two vectors by the distance between codewords4: d(x, y)  ? [sent-307, score-0.204]
</p><p>76 In PQ the distance in (11) is computed through M separate k-by-k lookup tables . [sent-314, score-0.233]
</p><p>77 In this case the Hamming distance still approximates the Euclidean distance. [sent-340, score-0.12]
</p><p>78 Following the common criterion in [3 1, 29, 3, 5] that the bits should be independent, we expect the subspaces in our method to be independent. [sent-351, score-0.187]
</p><p>79 We follow the search strategy of Hamming ranking commonly adopted in many hashing methods [1, 3 1, 29, 19, 5, 6]. [sent-381, score-0.676]
</p><p>80 The recall is defined as the fraction of retrieved true nearest neighbors to the total number of true nearest neighbors. [sent-389, score-0.197]
</p><p>81 a Off-line, the training time of our method when B = 64 bits is about 3 minutes in SIFT1M (b = 4) and 20 minutes in GIST1M (b = 8), using an unoptimized Matlab code. [sent-396, score-0.139]
</p><p>82 01ms) is comparable with other hyperplane-based hashing methods, and is ignorable compared with the Hamming ranking time (1. [sent-398, score-0.648]
</p><p>83 Comparisons with the naive two-step method The naive two-step method in Sec. [sent-400, score-0.116]
</p><p>84 4 shows the comparison between this method and our method, in SIFT1M with 64 bits (M = 16 subspaces). [sent-407, score-0.139]
</p><p>85 4 shows that the naive method is inferior even it exhaustively fits the two affinity matrices. [sent-409, score-0.162]
</p><p>86 The affinity error can be large when fitting an Hamming affinity matrix to an arbitrary matrix. [sent-411, score-0.247]
</p><p>87 We also compare with a semi-supervised method - minimal loss hashing (MLH) [19], for which we use 10,000 samples to generate the pseudo-labels. [sent-413, score-0.614]
</p><p>88 6 show the comparisons  in the two  222999444311  Figure 5: ANN search performance of six hashing methods on SIFT1M using 32, 64, and 128-bit codes. [sent-417, score-0.644]
</p><p>89 In this figure, K=10 Euclidean nearest neighbors are considered as the ground truth. [sent-418, score-0.12]
</p><p>90 Figure 6: ANN search performance of six hashing methods on GIST1M using 32, 64, and 128-bit codes. [sent-420, score-0.618]
</p><p>91 In this figure, K=10 Euclidean nearest neighbors are considered as the ground truth. [sent-421, score-0.12]
</p><p>92 This implies that reducing the quantization error is a reasonable objective. [sent-428, score-0.307]
</p><p>93 Performance under various K In a recent paper [30] it has been noticed that when evaluating hashing methods, the threshold (like K nearest neighbors in our experiment setting) determining the ground truth nearest neighbors can be of particular impact. [sent-430, score-0.852]
</p><p>94 Discussion and Conclusion We have proposed a k-means-based binary compact encoding method. [sent-438, score-0.144]
</p><p>95 Unlike most hashing methods using hyperplanes to quantize, our method enjoys the adaptivity of k222999444422  GIST1M 64-bit  Figure 7: Comparisons in GIST1M with 64 bits under different metric of ground truth nearest neighbors (K=1, 100, 1000). [sent-439, score-0.984]
</p><p>96 Our Affinity-Preserving K-means algorithm allows to approximate the Euclidean distance between codewords without lookup tables, so our method also enjoys the advantages of Hamming distance computation. [sent-443, score-0.457]
</p><p>97 Experiments have shown that our method outperforms many hashing methods. [sent-444, score-0.59]
</p><p>98 Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. [sent-448, score-0.708]
</p><p>99 Transform coding for fast approximate nearest neighbor search in high dimensions. [sent-459, score-0.146]
</p><p>100 Mobile product search with bag of hash bits and boundary reranking. [sent-492, score-0.319]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hashing', 0.59), ('hamming', 0.379), ('quantization', 0.268), ('dh', 0.191), ('equan', 0.16), ('itq', 0.147), ('bits', 0.139), ('eaff', 0.138), ('pcah', 0.124), ('codewords', 0.107), ('affinity', 0.104), ('product', 0.097), ('lookup', 0.09), ('ci', 0.089), ('distance', 0.084), ('hyperplanes', 0.084), ('indices', 0.08), ('codeword', 0.077), ('nearest', 0.077), ('quantize', 0.073), ('kmh', 0.069), ('pq', 0.062), ('orthogonal', 0.061), ('encoding', 0.06), ('bit', 0.06), ('tables', 0.059), ('ranking', 0.058), ('naive', 0.058), ('euclidean', 0.057), ('cj', 0.057), ('codes', 0.056), ('hash', 0.055), ('vq', 0.053), ('jegou', 0.053), ('compact', 0.051), ('codebook', 0.051), ('enjoys', 0.051), ('ann', 0.049), ('subspaces', 0.048), ('commend', 0.046), ('edist', 0.046), ('eeqduisatn', 0.046), ('mlh', 0.046), ('popcnt', 0.046), ('neighbors', 0.043), ('classical', 0.043), ('locality', 0.041), ('approximate', 0.041), ('update', 0.041), ('buckets', 0.041), ('ccube', 0.041), ('cell', 0.041), ('concatenation', 0.04), ('error', 0.039), ('million', 0.038), ('adc', 0.038), ('stretching', 0.038), ('rd', 0.038), ('douze', 0.037), ('kernelized', 0.037), ('approximates', 0.036), ('principal', 0.035), ('vertexes', 0.034), ('sdc', 0.034), ('binary', 0.033), ('pca', 0.033), ('notice', 0.033), ('norouzi', 0.032), ('computation', 0.032), ('cells', 0.032), ('qm', 0.031), ('wij', 0.031), ('read', 0.03), ('clusters', 0.03), ('denotes', 0.03), ('query', 0.03), ('pages', 0.03), ('putting', 0.03), ('lsh', 0.029), ('competitors', 0.028), ('search', 0.028), ('bases', 0.028), ('distances', 0.027), ('relation', 0.027), ('suboptimal', 0.027), ('terminology', 0.027), ('kulis', 0.026), ('comparisons', 0.026), ('assignment', 0.025), ('partition', 0.025), ('rt', 0.025), ('minimizing', 0.025), ('subspace', 0.024), ('minimal', 0.024), ('initialize', 0.024), ('day', 0.024), ('sy', 0.024), ('decomposing', 0.022), ('perronnin', 0.022), ('noticed', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="236-tfidf-1" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>2 0.48371351 <a title="236-tfidf-2" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>3 0.48213521 <a title="236-tfidf-3" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>Author: Yue Lin, Rong Jin, Deng Cai, Shuicheng Yan, Xuelong Li</p><p>Abstract: Recent studies have shown that hashing methods are effective for high dimensional nearest neighbor search. A common problem shared by many existing hashing methods is that in order to achieve a satisfied performance, a large number of hash tables (i.e., long codewords) are required. To address this challenge, in this paper we propose a novel approach called Compressed Hashing by exploring the techniques of sparse coding and compressed sensing. In particular, we introduce a sparse coding scheme, based on the approximation theory of integral operator, that generate sparse representation for high dimensional vectors. We then project sparse codes into a low dimensional space by effectively exploring the Restricted Isometry Property (RIP), a key property in compressed sensing theory. Both of the theoretical analysis and the empirical studies on two large data sets show that the proposed approach is more effective than the state-of-the-art hashing algorithms.</p><p>4 0.45930171 <a title="236-tfidf-4" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>5 0.44080302 <a title="236-tfidf-5" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>Author: Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van_den_Hengel, Zhenmin Tang</p><p>Abstract: Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original highdimensional data. The complexity of these models, and the problems with out-of-sample data, havepreviously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE [29] outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths.</p><p>6 0.32859409 <a title="236-tfidf-6" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>7 0.2316862 <a title="236-tfidf-7" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>8 0.21623328 <a title="236-tfidf-8" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>9 0.20780377 <a title="236-tfidf-9" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>10 0.17227888 <a title="236-tfidf-10" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>11 0.12359503 <a title="236-tfidf-11" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>12 0.094792932 <a title="236-tfidf-12" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>13 0.087523147 <a title="236-tfidf-13" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>14 0.074956 <a title="236-tfidf-14" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>15 0.07282228 <a title="236-tfidf-15" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>16 0.069710836 <a title="236-tfidf-16" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>17 0.063679837 <a title="236-tfidf-17" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>18 0.063144647 <a title="236-tfidf-18" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>19 0.059880234 <a title="236-tfidf-19" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>20 0.05611968 <a title="236-tfidf-20" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, -0.039), (2, -0.052), (3, 0.068), (4, 0.117), (5, 0.012), (6, -0.102), (7, -0.304), (8, -0.371), (9, -0.115), (10, -0.342), (11, 0.017), (12, 0.074), (13, 0.27), (14, 0.03), (15, 0.18), (16, 0.075), (17, 0.068), (18, -0.1), (19, 0.065), (20, -0.167), (21, 0.007), (22, -0.068), (23, -0.005), (24, 0.043), (25, -0.076), (26, 0.03), (27, -0.089), (28, -0.019), (29, -0.0), (30, 0.018), (31, 0.029), (32, 0.013), (33, 0.024), (34, 0.019), (35, -0.051), (36, -0.027), (37, 0.005), (38, 0.007), (39, -0.017), (40, 0.023), (41, 0.032), (42, 0.005), (43, 0.02), (44, 0.006), (45, 0.053), (46, -0.026), (47, -0.059), (48, 0.016), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96326202 <a title="236-lsi-1" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>2 0.9203555 <a title="236-lsi-2" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>3 0.85787249 <a title="236-lsi-3" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>Author: Yue Lin, Rong Jin, Deng Cai, Shuicheng Yan, Xuelong Li</p><p>Abstract: Recent studies have shown that hashing methods are effective for high dimensional nearest neighbor search. A common problem shared by many existing hashing methods is that in order to achieve a satisfied performance, a large number of hash tables (i.e., long codewords) are required. To address this challenge, in this paper we propose a novel approach called Compressed Hashing by exploring the techniques of sparse coding and compressed sensing. In particular, we introduce a sparse coding scheme, based on the approximation theory of integral operator, that generate sparse representation for high dimensional vectors. We then project sparse codes into a low dimensional space by effectively exploring the Restricted Isometry Property (RIP), a key property in compressed sensing theory. Both of the theoretical analysis and the empirical studies on two large data sets show that the proposed approach is more effective than the state-of-the-art hashing algorithms.</p><p>4 0.8028695 <a title="236-lsi-4" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>Author: Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van_den_Hengel, Zhenmin Tang</p><p>Abstract: Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original highdimensional data. The complexity of these models, and the problems with out-of-sample data, havepreviously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE [29] outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths.</p><p>5 0.75912315 <a title="236-lsi-5" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>6 0.71963215 <a title="236-lsi-6" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>7 0.60947818 <a title="236-lsi-7" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>8 0.54222465 <a title="236-lsi-8" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>9 0.44545317 <a title="236-lsi-9" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>10 0.38407242 <a title="236-lsi-10" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>11 0.3572984 <a title="236-lsi-11" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>12 0.32127923 <a title="236-lsi-12" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>13 0.30784169 <a title="236-lsi-13" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>14 0.23894426 <a title="236-lsi-14" href="./cvpr-2013-Graph-Laplacian_PCA%3A_Closed-Form_Solution_and_Robustness.html">191 cvpr-2013-Graph-Laplacian PCA: Closed-Form Solution and Robustness</a></p>
<p>15 0.21509327 <a title="236-lsi-15" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>16 0.2144967 <a title="236-lsi-16" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>17 0.21156631 <a title="236-lsi-17" href="./cvpr-2013-All_About_VLAD.html">38 cvpr-2013-All About VLAD</a></p>
<p>18 0.21108226 <a title="236-lsi-18" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>19 0.2087291 <a title="236-lsi-19" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>20 0.2065614 <a title="236-lsi-20" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.09), (16, 0.039), (26, 0.046), (33, 0.257), (36, 0.296), (67, 0.077), (69, 0.047), (87, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83580881 <a title="236-lda-1" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>2 0.8047089 <a title="236-lda-2" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>3 0.77448183 <a title="236-lda-3" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>4 0.77113444 <a title="236-lda-4" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>5 0.76778948 <a title="236-lda-5" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>6 0.75456482 <a title="236-lda-6" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>7 0.74101502 <a title="236-lda-7" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>8 0.72847444 <a title="236-lda-8" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>9 0.72687155 <a title="236-lda-9" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>10 0.72090644 <a title="236-lda-10" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>11 0.71526152 <a title="236-lda-11" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>12 0.71100253 <a title="236-lda-12" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>13 0.70210123 <a title="236-lda-13" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>14 0.70099264 <a title="236-lda-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.70029205 <a title="236-lda-15" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>16 0.70025533 <a title="236-lda-16" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>17 0.69987857 <a title="236-lda-17" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>18 0.69970399 <a title="236-lda-18" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>19 0.69932568 <a title="236-lda-19" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>20 0.69930035 <a title="236-lda-20" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
