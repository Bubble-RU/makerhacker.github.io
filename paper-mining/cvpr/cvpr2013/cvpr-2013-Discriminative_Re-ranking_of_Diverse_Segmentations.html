<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-132" href="#">cvpr2013-132</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</h1>
<br/><p>Source: <a title="cvpr-2013-132-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yadollahpour_Discriminative_Re-ranking_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Payman Yadollahpour, Dhruv Batra, Gregory Shakhnarovich</p><p>Abstract: This paper introduces a two-stage approach to semantic image segmentation. In the first stage a probabilistic model generates a set of diverse plausible segmentations. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. While our proposed approach already achieves state-of-the-art results (48.1%) on the challenging VOC 2012 dataset, our machine and human analyses suggest that even larger gains are possible with such an approach.</p><p>Reference: <a title="cvpr-2013-132-reference" href="../cvpr2013_reference/cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In the first stage a probabilistic model generates a set of diverse plausible segmentations. [sent-2, score-0.36]
</p><p>2 The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. [sent-4, score-0.457]
</p><p>3 A semantic segmentation algorithm must deal with tremendous amount of uncertainty – from inter and intra object occlusion and varying appear–  –  ance, lighting & pose. [sent-13, score-0.14]
</p><p>4 Unfortunately, idealized models that reason about (the distribution over) all possible segmentations jointly with all confounding factors in a fully probabilistic setting are typically computationally intractable. [sent-14, score-0.284]
</p><p>5 In Stage 1 diverse segmentations are computed from a tractable probabilistic model. [sent-20, score-0.463]
</p><p>6 Even though the most probable segmentation from Stage 1 is incorrect, the set of segmentations does contain an accurate solution, which the re-ranker is able to score to the top. [sent-23, score-0.403]
</p><p>7 This feed-forward process captures rich dependencies between pixels and regions, but errors are accumulated and propagated from one stage to the next. [sent-26, score-0.132]
</p><p>8 We propose a two-stage 111999222311  model where the first stage is a tractable probabilistic model that reasons about an exponentially large output-space and makes a joint prediction but crucially outputs a diverse set of plausible segmentations, not just a single one. [sent-28, score-0.434]
</p><p>9 The second stage in our approach is a discriminative re-ranker that is free to exploit arbitrarily complex features, and attempts to pick out the best segmentation from this set. [sent-29, score-0.287]
</p><p>10 Thinking about semantic segmentation as a two-stage DivMBEST+RERANK process has several key advantages: –  •  •  Global Optimization over a Simple Model. [sent-33, score-0.14]
</p><p>11 The first stage of this approach is able to perform global optimization over all variables of interest, in a tractable albeit imperfect model to find a small set (? [sent-34, score-0.191]
</p><p>12 a t1 0le)a ostf one of these solutions is highly accurate. [sent-38, score-0.17]
</p><p>13 The re-ranker is  free to compute arbitrarily complex features that are not amenable to tractable inference and could not be added to the probabilistic model in the first stage. [sent-41, score-0.155]
</p><p>14 Specifically, for the re-ranker the goal is no longer to use features than can identify generic good segmentations, rather to use features that can help it discriminate good solutions from bad ones within a small set. [sent-45, score-0.263]
</p><p>15 While this paradigm is broadly applicable, we pick semantic segmentation as a case study in this paper. [sent-48, score-0.171]
</p><p>16 Our main technical contribution is a discriminative reranking formulation for semantic segmentation. [sent-49, score-0.15]
</p><p>17 In order to generate this set of segmentations, we build on our previous work [2], which produces diverse M-Best solutions from any probabilistic model. [sent-55, score-0.405]
</p><p>18 For the first stage of our approach, we analyze two different semantic segmen-  tation probabilistic models Automatic Labelling Environment (ALE) [18, 20] and Second Order Pooling (O2P) [4] and find that DivMBEST+RERANK results in significant improvements for both of them. [sent-56, score-0.242]
</p><p>19 ’, perhaps more progress can be made by answering a simpler question – ‘Given two plausible segmentations for an image, can we tell a good segmentation from a bad segmentation? [sent-60, score-0.411]
</p><p>20 From the human analyses, we find that people are surprisingly good at picking a good segmentation from a bad segmentation by looking at the segmentations alone. [sent-63, score-0.471]
</p><p>21 The idea of pruning possible solutions in successive stages has been central to many vision systems, including the seminal cascaded architecture of Viola and Jones [28] and more recent work [24, 29]. [sent-71, score-0.205]
</p><p>22 Our approach on the other hand is a shallow cascade, with a powerful first stage that performs an exponentially large pruning: from all possible segmentations to a small list of size M (? [sent-73, score-0.397]
</p><p>23 eO fuirrs tre stage is computationally efficient and successful at producing a very small list with at least one high-quality solution. [sent-77, score-0.132]
</p><p>24 Categoryindependent segmentation has long been thought of as a preprocessing stage for higher vision tasks. [sent-79, score-0.22]
</p><p>25 In contrast, stage 1in our work produces holistic proposals, i. [sent-84, score-0.165]
</p><p>26 Stage 1 of our approach is related to a problem studied in the graphical mod111999222422  els literature called M-Best MAP [13, 22, 30], which involves finding the top M most probable solutions in a probabilistic model. [sent-89, score-0.277]
</p><p>27 Unfortunately, since there is no emphasis on diversity, such solutions are typically minor perturbations of each other. [sent-90, score-0.235]
</p><p>28 This paper builds on our recent work, called DivMBEST [2], which produces diverse M-Best solutions. [sent-91, score-0.177]
</p><p>29 Diversity in solutions is crucial in re-ranking because we don’t want to pick from a set of solutions that are simply minor perturbations of each other but rather ones that present whole alternative explanations. [sent-92, score-0.436]
</p><p>30 Our previous work [2] mostly focused on interactive applications where these diverse M-Best solutions could simply be shown to a user/expert. [sent-93, score-0.314]
</p><p>31 Discriminative re-ranking of multiple solutions is a dominant paradigm in domains like speech [9, 10] and natural language processing [8, 25]. [sent-96, score-0.271]
</p><p>32 Recall that the first stage is a Conditional Random Field (CRF) that produces a diverse set of segmentations and the second stage re-ranks this set and then picks the top scoring segmentation. [sent-101, score-0.715]
</p><p>33 A segmentation y is a set of discrete random variables, representing the category assigned to each labelling unit (pixel or superpixel or region), i. [sent-112, score-0.189]
</p><p>34 The quality of the predicted segmentation is measured by a loss function yˆ) that denotes the cost of predicting ˆy when the ground-truth is In Pascal VOC [12], this loss would be the standard 1−inteurnsieoctnion measure, averaged over wmaousklds bofe athlle ec sattaengdoarireds 1. [sent-125, score-0.182]
</p><p>35 Stage 1: Producing Diverse Segmentations Let us first describe how we generate multiple segmentations from the CRF in stage 1. [sent-128, score-0.358]
</p><p>36 θu  θuv  pressing compatibility of labels yu and yv at adjacent vertices. [sent-144, score-0.134]
</p><p>37 To generate a set of segmentations, we utilize our previous work called DivMBEST [2], which produces diverse M-Best solutions from any probabilistic model that allows for efficient MAP computation. [sent-162, score-0.405]
</p><p>38 (2) Here λ = {λm | m ∈ [M −1] } is the set of Lagrange multHipelrieers λ, w=h i{cλh de|t merm ∈in [eM th −e w1]e}ig ish tth oef s stehte o pfe Lnaagltrya nimgepo msueldfor violating the diversity constraints. [sent-181, score-0.135]
</p><p>39 This makes it really efficient to produce DivMBEST solutions in stage 1. [sent-209, score-0.302]
</p><p>40 Stage 2: Re-ranking Diverse Segmentations We now describe our proposed approach for re-ranking the diverse set of segmentations produced by stage 1. [sent-210, score-0.502]
</p><p>41 y(iM)}  Let Yi = denote the set of M segmentations fo=r i {myage i. [sent-214, score-0.226]
</p><p>42 The }in dpuetn ottoe stage e2t oaft t Mrain s-etgimmee nis? [sent-215, score-0.132]
</p><p>43 The accuracy of solution yi∗ forms an upper-bound on the re-ranker performance since we are committed to picking one solution from Yi. [sent-228, score-0.136]
</p><p>44 The reranker assigns a score to each segmentation in the set, i. [sent-233, score-0.193]
</p><p>45 Inference in the re-ranker consists of finding the highest scoring solu-  ygit,  ygit  tion, yˆi = argmaxy∈Yi Sr (y). [sent-237, score-0.157]
</p><p>46 The re-ranking features ψ need not be the same as the CRF features φ, and can be quite complex, because inference in the re-ranker merely involves extracting the features on a small set of solutions, taking a dot-product with the weights and sorting according to the resulting score. [sent-238, score-0.116]
</p><p>47 Also notice that the features are a function of both the image xi and the segmentation yi. [sent-239, score-0.115]
</p><p>48 Thus, we can compute features like size of various categories, connectivity of the label masks, relative location of label masks and other such quantities that are functions of global statistics of the segmentation and thus intractable to include in the first stage. [sent-240, score-0.206]
</p><p>49 the task loss of segmentation yˆi relative to the best segmentation in this set yi∗ . [sent-249, score-0.223]
</p><p>50 For instance, consider two images i,j with two segmentations each, whose accuracies are Acc(Yi) = {95%, 75%} and Acc(Yj) = {40%, 35%} respectively. [sent-251, score-0.255]
</p><p>51 , on set j and ignore set ibecause both solutions in Yj have high loss  L(yigt,  w. [sent-257, score-0.261]
</p><p>52 ≥ 1 −L(yξigit,y)  (4b)  ξi ≥ 0  ∀y ∈ Yi \ yi∗,  (4c)  Intuitively, we can see that the constraint (4b) tries to maximize the (soft) margin between the score of the oracle solution and all other solutions in the set. [sent-276, score-0.526]
</p><p>53 Thus if in addition to yi∗ there are other good solutions in the set, the margin for such solutions will not be tightly enforced. [sent-278, score-0.371]
</p><p>54 On the other hand, the margin between yi∗ and bad solutions will be very strictly enforced. [sent-279, score-0.24]
</p><p>55 We now provide a detailed analysis of both stages of our DivMBEST+RERANK approach – Section 4 analyzes stage 1 and Section 5 analyzes stage 2. [sent-282, score-0.318]
</p><p>56 Analyzing Diverse Segmentations In this section, we provide details of the CRFs used to produce multiple segmentations and characterize the diversity achieved in these segmentations. [sent-284, score-0.361]
</p><p>57 Specifically, we investigate the sources of diversity, and attempt to quantify the extent to which diversity enables potential gain in accuracy over the MAP solution. [sent-285, score-0.135]
</p><p>58 CRFs: ALE and O2P We used two different models for semantic segmentation the Associative Hierarchical CRF [18] (implemented as the Automatic Labeling Environment, ALE) and the SecondOrder Pooling (O2P) model of Carreira et al. [sent-288, score-0.14]
</p><p>59 For both models, DivMBEST is able to reuse the respective MAP inference algorithms to produce a diverse set of segmentations. [sent-295, score-0.179]
</p><p>60 Diversity and Oracles For the analysis reported in this subsection, we used the VOC 2012 t rain and val sets. [sent-300, score-0.116]
</p><p>61 ALE and O2P models were trained on VOC2012 t rain, and the models were used to produce 10 segmentations for each image in val. [sent-301, score-0.226]
</p><p>62 Since ground-truth is known for VOC val images, we can find the oracle accuracy, i. [sent-306, score-0.321]
</p><p>63 Oracle accuracy with ALE solutions show a similar increase. [sent-314, score-0.17]
</p><p>64 To put these oracle numbers in context, we ask what is the the best possible segmentation that could be constructed with the 150 CPMC segments. [sent-315, score-0.32]
</p><p>65 we only need 10 DivMBEST solutions to reach to 60. [sent-320, score-0.17]
</p><p>66 We now turn to empirical analysis that quantifies the amount of diversity in these solutions, and how that affects the oracle performance. [sent-323, score-0.367]
</p><p>67 The first question we address is: how much diversity do the DivMBEST solutions contain over MAP? [sent-326, score-0.305]
</p><p>68 Thus, on average at least one out of 10  DivMBEST solutions for O2P overlaps MAP by only 10%. [sent-331, score-0.17]
</p><p>69 Of course, diversity is useful only if it brings in improved quality, and our next goal is to assess this. [sent-332, score-0.135]
</p><p>70 We computed the covering of MAP by the oracle for every image, and found that on average this covering is less than 61% for O2P and 55% for ALE. [sent-333, score-0.282]
</p><p>71 Thus, we can conclude that the oracle segmentations are not simply minor perturbations of the MAP. [sent-335, score-0.523]
</p><p>72 Diving a bit deeper we can investigate the modes of this diversity: how is the oracle different from the MAP? [sent-337, score-0.232]
</p><p>73 The analysis above tells us that the set of regions in the oracle tends to be very different from MAP. [sent-338, score-0.232]
</p><p>74 But perhaps the oracle simply contains a better set of masks for the same categories present in the MAP? [sent-339, score-0.294]
</p><p>75 We show in the supplementary materials that this is not the case: if we find the best labeling of any of the DivMBEST solutions restricted to the set of categories present in MAP, we obtain performance significantly inferior to that of the true oracle. [sent-340, score-0.17]
</p><p>76 Thus, we can conclude that there are clear differences in both the labels and segments of the oracle segmentations compared to the MAP. [sent-341, score-0.5]
</p><p>77 (5 dimensions) Diversity features measure average per pixel agreement of y with the majority vote by the diverse set (weighted or 111999222755  unweighted by the model scores). [sent-349, score-0.171]
</p><p>78 We use outputs of object detectors from [21] to get detector-based segmentations D1, D2, wtorhser fer meac [h2 1p]i xtoel g eist daessteigctnoerd-b ab yse md asejogmriteyn tvaottioe osn D de,tDection scores (thresholded & un-thresholded). [sent-351, score-0.226]
</p><p>79 We compute bmyax D/median/min of the detection score (with and without thresholding) for every category in y (120 dims); the average overlap between category masks in D1, D2 and in y (2 dims); and pixelwise average mdeateskctso irn s Dcor,eDs for categories in y (2 dims). [sent-353, score-0.198]
</p><p>80 Segment features measure the geometric properties of the  segments in y: perimeter, area, and the ratio of the two; computed separately for segments in every class and for the entire foreground (63 dimensions). [sent-355, score-0.111]
</p><p>81 Relative location of the centroids of masks for each category pair (420 dimensions). [sent-356, score-0.11]
</p><p>82 (420 dimensions) All the features above are independent of the image x; the following features rely on image measurements as well as properties of the solution y. [sent-359, score-0.107]
</p><p>83 We also compute recall by the gPb map of the category boundaries in the y; this produces a 10 dimensional feature for ten equally spaced precision values. [sent-362, score-0.142]
</p><p>84 We stress that most of these features rely on higher-order information that would be intractable to incorporate into the CRF model used in stage 1. [sent-368, score-0.188]
</p><p>85 However, evaluating these features on M segmentations is easy, which allows us to use them at the re-ranking stage. [sent-370, score-0.253]
</p><p>86 We also compared against randomly selecting one-out-of-M solutions (Rand). [sent-378, score-0.17]
</p><p>87 2 shows the behavior of the reranker on VOC 2012 val: (a) shows the number of images in which the oracle solution was originally at rank M. [sent-392, score-0.407]
</p><p>88 We can see that there is a heavy tail in the distribution, indicating that high-quality solutions are often found near the bottom of the list; (b) shows the number of images where the re-ranker predicts solution M. [sent-393, score-0.309]
</p><p>89 We can see a much lighter tail, suggesting that the re-ranker ‘plays it safe’ and predicts MAP very frequently; (c) shows a scatter plot of re-ranker score vs solution accuracy. [sent-394, score-0.209]
</p><p>90 We selected 150 images from the VOC 2012 validation set where the MAP segmentation was neither the worst nor the best segmentation. [sent-399, score-0.127]
</p><p>91 Subjects were not shown the image and had to pick the better segmentation simply by looking at labelings with category names annotated. [sent-401, score-0.167]
</p><p>92 Figure 2: Statistics on VOC 2012 val with O2P model: (a),(b) show the number of images in which the oracle / top-reranked solution was originally at rank M. [sent-473, score-0.431]
</p><p>93 We can see that there is a heavy tail in the oracle distribution, but a much lighter tail in the re-ranker, suggesting that the re-ranker “plays it safe” and predicts MAP very frequently; (c) shows a scatter plot of re-ranker score vs solution accuracy. [sent-474, score-0.547]
</p><p>94 Conclusions We have presented a two-stage hybrid approach to segmentation: produce a set of diverse solutions from a generative model, then re-rank them using a discriminative reranker. [sent-506, score-0.35]
</p><p>95 Our detailed analysis, applied to two models (ALE and O2P) shows that the set of solutions obtained in stage 1 contains segmentations dramatically more accurate than the single MAP solution, and that the sources of diversity are non-trivial. [sent-507, score-0.663]
</p><p>96 With the re-ranker trained using a novel structured SVM formulation, we obtain state of the art results on VOC 2012 segmentation te st set. [sent-508, score-0.116]
</p><p>97 Chief among our future work directions is to continue closing the gap between what is achieved by the re-ranker and what is possible based on our oracle analysis of the diverse solution sets. [sent-509, score-0.429]
</p><p>98 The gap and the actual values of the oracle suggest that efforts of CRF modelling community may be misguided the bottleneck is not optimization algorithms for probabilistic models, rather the bottleneck is the absence of rich features that can tell a dog from a cat. [sent-510, score-0.317]
</p><p>99 An efficient algorithm for finding the m most probable configurations in probabilistic expert systems. [sent-667, score-0.107]
</p><p>100 Using multiple segmentations to discover objects and their extent in image collections. [sent-680, score-0.226]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('divmbest', 0.589), ('rerank', 0.305), ('oracle', 0.232), ('segmentations', 0.226), ('solutions', 0.17), ('yigt', 0.153), ('ale', 0.15), ('diverse', 0.144), ('diversity', 0.135), ('stage', 0.132), ('voc', 0.13), ('ygit', 0.109), ('yu', 0.094), ('uv', 0.093), ('val', 0.089), ('crf', 0.089), ('segmentation', 0.088), ('dims', 0.087), ('cpmc', 0.085), ('limage', 0.065), ('reranker', 0.065), ('yi', 0.064), ('reranking', 0.062), ('masks', 0.062), ('map', 0.061), ('probabilistic', 0.058), ('gpb', 0.055), ('speech', 0.053), ('labelling', 0.053), ('tail', 0.053), ('solution', 0.053), ('semantic', 0.052), ('carreira', 0.051), ('probable', 0.049), ('category', 0.048), ('language', 0.048), ('pascal', 0.048), ('scoring', 0.048), ('loss', 0.047), ('analyses', 0.045), ('ibecause', 0.044), ('orst', 0.044), ('segments', 0.042), ('yv', 0.04), ('associative', 0.04), ('score', 0.04), ('bad', 0.039), ('yadollahpour', 0.039), ('worst', 0.039), ('exponentially', 0.039), ('batra', 0.038), ('dissimilarity', 0.037), ('discriminative', 0.036), ('argmaxy', 0.036), ('binning', 0.036), ('perturbations', 0.035), ('tractable', 0.035), ('pruning', 0.035), ('inference', 0.035), ('potentials', 0.034), ('produces', 0.033), ('joachims', 0.033), ('predicts', 0.033), ('dimensions', 0.032), ('lighter', 0.032), ('instructive', 0.032), ('answering', 0.032), ('ladicky', 0.032), ('pick', 0.031), ('winning', 0.031), ('margin', 0.031), ('km', 0.031), ('minor', 0.03), ('picking', 0.03), ('thinking', 0.03), ('proposals', 0.03), ('crfs', 0.029), ('accuracies', 0.029), ('intractable', 0.029), ('safe', 0.029), ('ssvm', 0.029), ('rank', 0.029), ('unary', 0.028), ('structured', 0.028), ('originally', 0.028), ('ver', 0.028), ('acc', 0.027), ('scatter', 0.027), ('features', 0.027), ('analyzes', 0.027), ('exploration', 0.027), ('rain', 0.027), ('plausible', 0.026), ('covering', 0.025), ('band', 0.025), ('pooling', 0.024), ('albeit', 0.024), ('vs', 0.024), ('russell', 0.024), ('thresholded', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="132-tfidf-1" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>Author: Payman Yadollahpour, Dhruv Batra, Gregory Shakhnarovich</p><p>Abstract: This paper introduces a two-stage approach to semantic image segmentation. In the first stage a probabilistic model generates a set of diverse plausible segmentations. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. While our proposed approach already achieves state-of-the-art results (48.1%) on the challenging VOC 2012 dataset, our machine and human analyses suggest that even larger gains are possible with such an approach.</p><p>2 0.14925963 <a title="132-tfidf-2" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>Author: Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun</p><p>Abstract: In this paper we are interested in how semantic segmentation can help object detection. Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional HOG filters as well as a set of novel segmentation features. Thus our model “blends ” between the detector and segmentation models. Since our features can be computed very efficiently given the segments, we maintain the same complexity as the original DPM [14]. We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector [12] on all classes, achieving 13% higher average AP. When employing the parts, we outperform the original DPM [14] in 19 out of 20 classes, achieving an improvement of 8% AP. Furthermore, we outperform the previous state-of-the-art on VOC’10 test by 4%.</p><p>3 0.12426825 <a title="132-tfidf-3" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>Author: David Weiss, Ben Taskar</p><p>Abstract: We propose SCALPEL, a flexible method for object segmentation that integrates rich region-merging cues with mid- and high-level information about object layout, class, and scale into the segmentation process. Unlike competing approaches, SCALPEL uses a cascade of bottom-up segmentation models that is capable of learning to ignore boundaries early on, yet use them as a stopping criterion once the object has been mostly segmented. Furthermore, we show how such cascades can be learned efficiently. When paired with a novel method that generates better localized shapepriors than our competitors, our method leads to a concise, accurate set of segmentation proposals; these proposals are more accurate on the PASCAL VOC2010 dataset than state-of-the-art methods that use re-ranking to filter much larger bags of proposals. The code for our algorithm is available online.</p><p>4 0.12387475 <a title="132-tfidf-4" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>5 0.10895583 <a title="132-tfidf-5" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>6 0.093959898 <a title="132-tfidf-6" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>7 0.092385434 <a title="132-tfidf-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.08611118 <a title="132-tfidf-8" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>9 0.084077373 <a title="132-tfidf-9" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>10 0.082151636 <a title="132-tfidf-10" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>11 0.080718644 <a title="132-tfidf-11" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>12 0.080356307 <a title="132-tfidf-12" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>13 0.080034725 <a title="132-tfidf-13" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>14 0.079462163 <a title="132-tfidf-14" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>15 0.075688131 <a title="132-tfidf-15" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>16 0.073485732 <a title="132-tfidf-16" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>17 0.070054829 <a title="132-tfidf-17" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>18 0.070020549 <a title="132-tfidf-18" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>19 0.067312092 <a title="132-tfidf-19" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>20 0.067143694 <a title="132-tfidf-20" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, -0.029), (2, 0.029), (3, -0.02), (4, 0.107), (5, 0.032), (6, 0.032), (7, 0.073), (8, -0.066), (9, -0.006), (10, 0.064), (11, -0.022), (12, -0.014), (13, 0.013), (14, -0.039), (15, 0.04), (16, 0.052), (17, -0.02), (18, -0.007), (19, 0.013), (20, -0.008), (21, -0.017), (22, 0.023), (23, 0.023), (24, 0.039), (25, 0.007), (26, -0.035), (27, 0.042), (28, 0.004), (29, -0.074), (30, 0.012), (31, -0.034), (32, -0.043), (33, -0.008), (34, -0.014), (35, 0.021), (36, 0.014), (37, -0.032), (38, -0.058), (39, 0.068), (40, -0.045), (41, 0.045), (42, 0.033), (43, 0.03), (44, -0.009), (45, -0.007), (46, 0.041), (47, 0.011), (48, 0.055), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94729275 <a title="132-lsi-1" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>Author: Payman Yadollahpour, Dhruv Batra, Gregory Shakhnarovich</p><p>Abstract: This paper introduces a two-stage approach to semantic image segmentation. In the first stage a probabilistic model generates a set of diverse plausible segmentations. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. While our proposed approach already achieves state-of-the-art results (48.1%) on the challenging VOC 2012 dataset, our machine and human analyses suggest that even larger gains are possible with such an approach.</p><p>2 0.84318262 <a title="132-lsi-2" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>3 0.81481403 <a title="132-lsi-3" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>4 0.80704004 <a title="132-lsi-4" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>Author: David Weiss, Ben Taskar</p><p>Abstract: We propose SCALPEL, a flexible method for object segmentation that integrates rich region-merging cues with mid- and high-level information about object layout, class, and scale into the segmentation process. Unlike competing approaches, SCALPEL uses a cascade of bottom-up segmentation models that is capable of learning to ignore boundaries early on, yet use them as a stopping criterion once the object has been mostly segmented. Furthermore, we show how such cascades can be learned efficiently. When paired with a novel method that generates better localized shapepriors than our competitors, our method leads to a concise, accurate set of segmentation proposals; these proposals are more accurate on the PASCAL VOC2010 dataset than state-of-the-art methods that use re-ranking to filter much larger bags of proposals. The code for our algorithm is available online.</p><p>5 0.78686059 <a title="132-lsi-5" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>Author: Aurélien Lucchi, Yunpeng Li, Pascal Fua</p><p>Abstract: We propose a working set based approximate subgradient descent algorithm to minimize the margin-sensitive hinge loss arising from the soft constraints in max-margin learning frameworks, such as the structured SVM. We focus on the setting of general graphical models, such as loopy MRFs and CRFs commonly used in image segmentation, where exact inference is intractable and the most violated constraints can only be approximated, voiding the optimality guarantees of the structured SVM’s cutting plane algorithm as well as reducing the robustness of existing subgradient based methods. We show that the proposed method obtains better approximate subgradients through the use of working sets, leading to improved convergence properties and increased reliability. Furthermore, our method allows new constraints to be randomly sampled instead of computed using the more expensive approximate inference techniques such as belief propagation and graph cuts, which can be used to reduce learning time at only a small cost of performance. We demonstrate the strength of our method empirically on the segmentation of a new publicly available electron microscopy dataset as well as the popular MSRC data set and show state-of-the-art results.</p><p>6 0.76605272 <a title="132-lsi-6" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>7 0.73949838 <a title="132-lsi-7" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>8 0.73541337 <a title="132-lsi-8" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>9 0.73347604 <a title="132-lsi-9" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>10 0.71880341 <a title="132-lsi-10" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>11 0.71375126 <a title="132-lsi-11" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>12 0.70898056 <a title="132-lsi-12" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>13 0.7072559 <a title="132-lsi-13" href="./cvpr-2013-A_Principled_Deep_Random_Field_Model_for_Image_Segmentation.html">24 cvpr-2013-A Principled Deep Random Field Model for Image Segmentation</a></p>
<p>14 0.69615972 <a title="132-lsi-14" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>15 0.67762399 <a title="132-lsi-15" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>16 0.66943479 <a title="132-lsi-16" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>17 0.66809905 <a title="132-lsi-17" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>18 0.66449714 <a title="132-lsi-18" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>19 0.65013844 <a title="132-lsi-19" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>20 0.63605309 <a title="132-lsi-20" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.119), (16, 0.019), (26, 0.03), (28, 0.014), (33, 0.233), (47, 0.172), (67, 0.079), (69, 0.105), (80, 0.015), (87, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91007096 <a title="132-lda-1" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>Author: Nikolaos Kyriazis, Antonis Argyros</p><p>Abstract: In several hand-object(s) interaction scenarios, the change in the objects ’ state is a direct consequence of the hand’s motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.</p><p>2 0.87514532 <a title="132-lda-2" href="./cvpr-2013-Towards_Efficient_and_Exact_MAP-Inference_for_Large_Scale_Discrete_Computer_Vision_Problems_via_Combinatorial_Optimization.html">436 cvpr-2013-Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization</a></p>
<p>Author: Jörg Hendrik Kappes, Markus Speth, Gerhard Reinelt, Christoph Schnörr</p><p>Abstract: Discrete graphical models (also known as discrete Markov random fields) are a major conceptual tool to model the structure of optimization problems in computer vision. While in the last decade research has focused on fast approximative methods, algorithms that provide globally optimal solutions have come more into the research focus in the last years. However, large scale computer vision problems seemed to be out of reach for such methods. In this paper we introduce a promising way to bridge this gap based on partial optimality and structural properties of the underlying problem factorization. Combining these preprocessing steps, we are able to solve grids of size 2048 2048 in less than 90 seconds. On the hitherto unsolva2b04le8 C×h2i0ne4s8e character dataset of Nowozin et al. we obtain provably optimal results in 56% of the instances and achieve competitive runtimes on other recent benchmark problems. While in the present work only generalized Potts models are considered, an extension to general graphical models seems to be feasible.</p><p>same-paper 3 0.87503433 <a title="132-lda-3" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>Author: Payman Yadollahpour, Dhruv Batra, Gregory Shakhnarovich</p><p>Abstract: This paper introduces a two-stage approach to semantic image segmentation. In the first stage a probabilistic model generates a set of diverse plausible segmentations. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. While our proposed approach already achieves state-of-the-art results (48.1%) on the challenging VOC 2012 dataset, our machine and human analyses suggest that even larger gains are possible with such an approach.</p><p>4 0.86406159 <a title="132-lda-4" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>5 0.84611428 <a title="132-lda-5" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>6 0.84460151 <a title="132-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.84306473 <a title="132-lda-7" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>8 0.83827823 <a title="132-lda-8" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>9 0.83784908 <a title="132-lda-9" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>10 0.83770508 <a title="132-lda-10" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>11 0.83586252 <a title="132-lda-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.83455962 <a title="132-lda-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.83371013 <a title="132-lda-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.83355582 <a title="132-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.83302093 <a title="132-lda-15" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>16 0.83289343 <a title="132-lda-16" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>17 0.83068639 <a title="132-lda-17" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>18 0.83055359 <a title="132-lda-18" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>19 0.82946217 <a title="132-lda-19" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>20 0.82859397 <a title="132-lda-20" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
