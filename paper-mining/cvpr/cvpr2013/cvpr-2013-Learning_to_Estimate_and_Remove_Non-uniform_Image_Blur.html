<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-265" href="#">cvpr2013-265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</h1>
<br/><p>Source: <a title="cvpr-2013-265-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Couzinie-Devy_Learning_to_Estimate_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>Reference: <a title="cvpr-2013-265-reference" href="../cvpr2013_reference/cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. [sent-2, score-0.914]
</p><p>2 The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. [sent-3, score-1.64]
</p><p>3 Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. [sent-4, score-0.838]
</p><p>4 Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. [sent-5, score-1.18]
</p><p>5 The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. [sent-6, score-0.24]
</p><p>6 Short exposures and small apertures can be used to limit motion blur and increase depth of field, but this may result in noisy images, especially under low light conditions. [sent-12, score-0.856]
</p><p>7 It is therefore desirable to model the blurring process, and use the image content itself to estimate the corresponding parameters and restore a sharp image. [sent-13, score-0.254]
</p><p>8 This problem is known as blind deblurring (or blind deconvolution), and it is the topic of this presentation. [sent-14, score-0.574]
</p><p>9 1In contrast, non-blind deblurring refers to the simpler (but still quite challenging) problem of recovering the sharp image when the blur parameters are known. [sent-16, score-1.283]
</p><p>10 Two images demonstrating defocus and motion blur, with an out-of-focus swan in the foreground (left), and a moving  bus before a static background (right) respectively. [sent-18, score-0.243]
</p><p>11 Related Work There have been many attempts in the past to solve the image deblurring problem. [sent-25, score-0.358]
</p><p>12 Amongst these, it is commonly assumed that the blur kernel is spatially uniform [4, 7, 15, 18, 26, 37, 38], which allows it to be estimated from global image evidence. [sent-26, score-0.868]
</p><p>13 [27] argue that it is desirable to first estimate the blur kernel before using it to deblur the image. [sent-28, score-0.817]
</p><p>14 Statistical gradient priors [33], sharp edge assumptions [22, 36], and non-convex regularization [24] have also been imposed on the latent sharp image for blur estimation. [sent-31, score-1.159]
</p><p>15 Although these approaches may give impressive  results, they assume that the blur kernel is uniform which, as demonstrated by Figure 1, is not realistic in many settings involving out-of-focus regions or blur due to moving objects. [sent-32, score-1.619]
</p><p>16 1 1 10 0 07 7 735 3  The uniform kernel assumption has recently been relaxed in several blind deblurring methods that assume instead that blur is mostly due to camera rotation, which is realistic for camera shake in long exposures [6, 8, 16, 17, 21, 35]. [sent-33, score-1.453]
</p><p>17 In this case, the blurry image can be seen as an integral over time of images related to each other by homographies [34, 35]. [sent-34, score-0.256]
</p><p>18 An effective framework has also been proposed in [17] to approximate the spatially-varying blur kernels by combining a set of localized uniform blur kernels. [sent-35, score-1.555]
</p><p>19 Such works handle a specific type of non-uniform blur, where a global camera motion constraint can be imposed over the kernels, which simplifies the problem of kernel estimation. [sent-36, score-0.201]
</p><p>20 Proposed Approach We propose a method for joint image segmentation and deblurring under defocus and linear (say, horizontal) motion blur. [sent-40, score-0.594]
</p><p>21 [28] detect blurry regions, but do not  estimate the exact kernels that affect them. [sent-43, score-0.305]
</p><p>22 The algorithms proposed in [1, 8] rely on multiple blurry images or video frames to reduce the ambiguity of motion blur estimation and segmentation. [sent-44, score-1.098]
</p><p>23 [5] show interesting results for separating the blur and sharp regions in an image, but do not address deblurring itself, which is equally challenging. [sent-46, score-1.319]
</p><p>24 Dai and Wu [9] and Levin [25] rely on different local spectral or gradient cues, as well as natural image statistics for motion blur estimation. [sent-47, score-0.839]
</p><p>25 In summary, previous approaches to our deblurring problem either (i) fall short in the estimation or the deblurring step, (ii) require multiple images, or (iii) consider a limited set of blur types (e. [sent-49, score-1.48]
</p><p>26 We aim to overcome these limitations, and cast the estimation of the global (non-uniform) image blur as a multilabel energy minimization problem (Section 2). [sent-52, score-0.914]
</p><p>27 The energy is the sum of unary terms corresponding to learned local blur kernel estimators (Section 2. [sent-53, score-1.009]
</p><p>28 Its global minimum is found using Ishikawa’s method by exploiting the natural order of discretized blur values for linear motions and defocus. [sent-55, score-0.838]
</p><p>29 Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics (Section 3). [sent-56, score-1.18]
</p><p>30 The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. [sent-57, score-0.24]
</p><p>31 Estimating the Image Blur We show in this section that estimating the non-uniform blur of an image can be cast as a segmentation problem, where uniform regions correspond to homogeneous blur strength. [sent-61, score-1.628]
</p><p>32 Local (but noisy) blur estimators are learned using logistic regression. [sent-62, score-0.878]
</p><p>33 A robust global estimate of the image blur is then obtained by combining the corresponding local estimates with smoothness constraints in a multi-label energy minimization framework, where labels correspond to integer (rounded) blur strengths. [sent-63, score-1.688]
</p><p>34 Since integer labels admit a natural order, it is then possible to find the global minimum of the energy using appropriate smoothness terms and Ishikawa’s method [10, 19]. [sent-64, score-0.18]
</p><p>35 Learning Local Blur Estimators For simplicity, we model horizontal blur as a moving average, and defocus by a Gaussian filter. [sent-67, score-0.969]
</p><p>36 Training data is obtained by (globally) blurring a set of natural sharp images for each value of σ. [sent-73, score-0.248]
</p><p>37 We represent the local grey level pattern around each pixel in a blurry image by a feature vector x of dimension L 1obtained by pooling the responses of a fixed bank of L multi-scale filters. [sent-75, score-0.281]
</p><p>38 2 The filters used in our framework are a combination of 64 Gabor filters and of atoms of a dictionary learned on blurry and sharp natural images since these have been shown to prove useful in many image restoration tasks [11]. [sent-78, score-0.698]
</p><p>39 The dictionary is learned such that some of its atoms represent blurry patches. [sent-79, score-0.388]
</p><p>40 This is achieved by first learning a small dictionary [11] from blurry image patches alone. [sent-80, score-0.318]
</p><p>41 We then learn the complete dictionary, where the initial atoms are fixed to those learned from blurry patches, with sharp image patches. [sent-81, score-0.531]
</p><p>42 Figure 2 (left) shows an illustration of dictionary-based filters learned for the horizontal motion blur case. [sent-82, score-0.992]
</p><p>43 Note that the atoms shown here in the top few rows correspond to blurry patches. [sent-83, score-0.313]
</p><p>44 +  2Our filters are designed to give zero values on uniform patches since the overall grey level is irrelevant for blur estimation. [sent-84, score-0.9]
</p><p>45 Left: a dictionary learned on blurry (horizontal motion blur in this case) and sharp natural images. [sent-88, score-1.355]
</p><p>46 In practice, we divide the useful range [0, Σ] of blur values into K intervals Ik = [σk−1 , σk] for k = 1, . [sent-94, score-0.76]
</p><p>47 3The observant reader may have noticed that since the k estimators are learned independently, the function fk − fk−1 is not guaranteed to be positive. [sent-108, score-0.203]
</p><p>48 Given a fixed number P of integer labels, we split the blur  parameter space into P bins and predict a bin for each pixel. [sent-110, score-0.805]
</p><p>49 The function U is the unary cost of assigning label yi to the feature xi, as derived in the previous section, and B is a pairwise smoothness term that ensures that nearby pixels have consistent blur values. [sent-124, score-0.886]
</p><p>50 After obtaining the optimal kernel labels yi for each pixel i, the local blur for the pixel will be represented by the motion or defocus kernel with corresponding value σi. [sent-128, score-1.1]
</p><p>51 Deblurring the Image We now address the problem ofdeblurring the blurry image. [sent-130, score-0.256]
</p><p>52 Given the blur kernel estimated for each pixel in the previous section, we can construct a non-uniform blur kernel matrix Kˆ. [sent-131, score-1.58]
</p><p>53 Using this model, the blurry image f = Kˆg + u + μ, where f and g are the blurry and sharp images in vector form respectively, and Kˆg denotes the spatially-varying blurring process in matrix form. [sent-133, score-0.739]
</p><p>54 By further assuming that the error term u is sparsely distributed in the 1 1 10 0 07 7 757 5  with only the unary cost, blur estimation with unary and binary costs, which corresponds to the global minimum of the energy function (3). [sent-136, score-0.998]
</p><p>55 image domain, we estimate the sharp image g by optimizing:  mg,iun12||Kˆg+u−f||22+λ1(||F1g||αα+||F2g||αα)+λ2||u||1, (5) where | |Fig| |αα = ? [sent-137, score-0.212]
</p><p>56 Experiments Obtaining a quantitative evaluation of algorithms for spatially-varying blur is a difficult task. [sent-155, score-0.74]
</p><p>57 The results of [5] are shown for both its steps, using blur cues only (which is comparable to our method), and with blur and object cues. [sent-162, score-1.511]
</p><p>58 We also tested our multi-label framework, which handles images with multiple blur levels, in a binary setting, where there are exactly two labels – one to describe the sharp regions, and another for the blurry areas. [sent-163, score-1.259]
</p><p>59 way of obtaining a ground truth to evaluate the blur estimation or the deblurring results. [sent-164, score-1.122]
</p><p>60 Thus, we have built a synthetic dataset, where the region blurred and the strength of the blur are known. [sent-166, score-0.878]
</p><p>61 We evaluate our approach for non-uniform blind deblurring at several levels. [sent-169, score-0.466]
</p><p>62 First, we evaluate our energy formulation for blur prediction (Section 2. [sent-170, score-0.815]
</p><p>63 Given this blur estimation, we then evaluate the proposed deconvolution method, and compare with the popular Richardson-Lucy algorithm [29, 32]. [sent-172, score-0.855]
</p><p>64 We also compare the quality of the resulting sharp image with that obtained from two baseline deblurring algorithms based on [13, 33]. [sent-173, score-0.543]
</p><p>65 Datasets For a quantitative evaluation of our blur prediction and deblurring methods, we introduce a synthetic dataset. [sent-176, score-1.202]
</p><p>66 It  consists of 4 sharp images, which were subjected to different levels of horizontal and Gaussian blurs. [sent-177, score-0.363]
</p><p>67 In essence, this produces ground truth (blur estimate) segmentations and corresponding blurred images for each sharp image. [sent-181, score-0.23]
</p><p>68 From top to bottom: input image, estimated blur (regions shown in red) with unary and binary costs, the result of [5] using only blur cues, and [5] with blur as well as object cues. [sent-185, score-2.304]
</p><p>69 that this synthetic dataset may not very accurate, in particular, near the blur boundaries. [sent-187, score-0.812]
</p><p>70 4 In our experiments, we used a bank of 64 Gabor filters, with different orientations and frequencies, and a dictionary of 320 atoms learned on a set of blurry and sharp natural images to generate the feature set for an image. [sent-193, score-0.594]
</p><p>71 Here we show the average values for three images from our synthetic dataset, each subjected to six levels of horizontal blur. [sent-206, score-0.27]
</p><p>72 Blur Estimation  We evaluated our local blur estimators (i. [sent-209, score-0.82]
</p><p>73 In the horizontal blur case, the task is to predict one value from the set σi = {1 3 5 7 9 11 13}, and in the Gaussian blur case, we us=ed a 1se 3t o5f 7 79 9 bl 1u1rs 1o3f} }v,a arinadn ciens t uheni Gfoarumslsyia spread between 0 and 4. [sent-212, score-1.579]
</p><p>74 We predicted the blur at each pixel individually, with an accuracy of 72% and 62% in the horizontal and Gaussian blurs respectively. [sent-213, score-0.892]
</p><p>75 A visualization of blur prediction on an entire image using unary costs alone, i. [sent-214, score-0.856]
</p><p>76 We introduce the smoothness term B (4) to ensure that nearby pixels have consistent blur values. [sent-218, score-0.792]
</p><p>77 The method by [5] uses a two-step process: (i) blur cues are first used to construct an initial blur estimate segmentation; and then (ii) a color model is learned for each region to yield the final segmentation. [sent-223, score-1.571]
</p><p>78 Since our approach only uses blur cues, it would be fair to compare it with the results from step (i). [sent-225, score-0.74]
</p><p>79 Our framework handles images with multiple blur levels  (see Figure 3 (right) for example, which shows three distinct blur regions). [sent-229, score-1.548]
</p><p>80 We tested this generic framework in a binary setting, where only one blur level is assumed, with the other label corresponding to sharp regions, similar to [5]. [sent-230, score-0.946]
</p><p>81 PSNR values obtained with our method and two stateof-the-art uniform blind deconvolution algorithms on the bus, car, and horse images from our synthetic dataset (see Figures 4, 5). [sent-235, score-0.368]
</p><p>82 We provide a good approximation for uniform (horizontal) blur regions with a manually marked bounding box enclosing the blurry object. [sent-236, score-1.11]
</p><p>83 Our method, despite the lack of such ‘ground truth’ blur regions, performs better than the two other methods here. [sent-237, score-0.74]
</p><p>84 2-label approach shows a performance comparable to [5] with object cues, and outperforms it when only blur cues are used. [sent-241, score-0.771]
</p><p>85 Deblurring Given the computed non-uniform blur in an image, we estimate the sharp image with our deblurring method. [sent-244, score-1.31]
</p><p>86 Since there appear to be no deconvolution methods that handle gracefully non-uniform blurs considered here, we adapted three methods to make our baseline comparisons. [sent-245, score-0.189]
</p><p>87 We show this comparison as average PSNR values, computed on three synthetic images with six strengths of blur each, in Table 2. [sent-247, score-0.812]
</p><p>88 We also compared our deblurring results with two state-of-the-art uniform blind deblurring algorithms [13, 33]. [sent-248, score-0.877]
</p><p>89 We observe that our method, which requires no such ‘ground truth’ blur regions, outperforms [13] significantly, and is comparable to or better than [33]. [sent-251, score-0.74]
</p><p>90 In Table 4 we compare the PSNR values of blurred and estimated sharp image. [sent-252, score-0.25]
</p><p>91 We also evaluated our blur estimation and deconvolution methods qualitatively on real images from other [5, 25], as well our own datasets. [sent-253, score-0.879]
</p><p>92 A selection of these results are  shown in Figures 6, 7, and 8 for horizontal motion and defocus blurs. [sent-254, score-0.307]
</p><p>93 Average PSNR values on the synthetic dataset for horizontal and Gaussian blur types. [sent-259, score-0.931]
</p><p>94 Discussion We presented a novel approach for first estimating nonuniform blur caused by horizontal motion or defocus, and then the sharp (deconvolved) image. [sent-264, score-1.131]
</p><p>95 A promising direction future work is the construction of a tree structure estimator to be able to handle two different types of blur in the same image. [sent-267, score-0.74]
</p><p>96 Blind motion deblurring from a single image using sparse approximation. [sent-295, score-0.436]
</p><p>97 Horizontal blur: Sample deblurring results on two real images from [5] and on one synthetic image. [sent-352, score-0.43]
</p><p>98 From left to right: blurry image, deblurred image, close-up corresponding to the boxes shown in red. [sent-447, score-0.32]
</p><p>99 Richardson-lucy deblurring for scenes under a projective motion path. [sent-499, score-0.436]
</p><p>100 Deconvolving psfs for a better motion deblurring using multiple images. [sent-526, score-0.436]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blur', 0.74), ('deblurring', 0.358), ('blurry', 0.256), ('sharp', 0.185), ('defocus', 0.13), ('deconvolution', 0.115), ('blind', 0.108), ('horizontal', 0.099), ('estimators', 0.08), ('motion', 0.078), ('chakrabarti', 0.076), ('synthetic', 0.072), ('fk', 0.064), ('deblurred', 0.064), ('unary', 0.063), ('psnr', 0.057), ('atoms', 0.057), ('blurs', 0.053), ('uniform', 0.053), ('kernel', 0.05), ('shake', 0.048), ('ishikawa', 0.047), ('subjected', 0.047), ('blurred', 0.045), ('energy', 0.043), ('blurring', 0.042), ('dictionary', 0.042), ('filters', 0.042), ('wk', 0.042), ('cho', 0.038), ('levin', 0.038), ('exposures', 0.038), ('integer', 0.037), ('handles', 0.036), ('regions', 0.036), ('bus', 0.035), ('vi', 0.033), ('rieure', 0.033), ('learned', 0.033), ('smoothness', 0.033), ('levels', 0.032), ('pages', 0.032), ('prediction', 0.032), ('motions', 0.032), ('normale', 0.032), ('cast', 0.031), ('yi', 0.031), ('cues', 0.031), ('regularization', 0.03), ('gabor', 0.03), ('fig', 0.03), ('multilabel', 0.029), ('nonuniform', 0.029), ('camera', 0.029), ('bins', 0.028), ('graphics', 0.028), ('segmentation', 0.028), ('restored', 0.027), ('rescaling', 0.027), ('ik', 0.027), ('fergus', 0.027), ('estimate', 0.027), ('mathematics', 0.026), ('reader', 0.026), ('enclosing', 0.025), ('logistic', 0.025), ('sup', 0.025), ('grey', 0.025), ('global', 0.025), ('estimation', 0.024), ('krishnan', 0.023), ('joshi', 0.023), ('ji', 0.023), ('say', 0.023), ('tai', 0.023), ('liblinear', 0.022), ('kernels', 0.022), ('minimization', 0.022), ('natural', 0.021), ('transactions', 0.021), ('dai', 0.021), ('adapted', 0.021), ('costs', 0.021), ('strength', 0.021), ('labels', 0.021), ('binary', 0.021), ('equation', 0.02), ('quantitatively', 0.02), ('values', 0.02), ('durand', 0.02), ('patches', 0.02), ('restoration', 0.02), ('imposed', 0.019), ('term', 0.019), ('toolboxes', 0.019), ('pon', 0.019), ('uhe', 0.019), ('zontal', 0.019), ('rumpf', 0.019), ('kro', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="265-tfidf-1" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>2 0.64074999 <a title="265-tfidf-2" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: Motion blur frequently occurs in dense 3D reconstruction using a single moving camera, and it degrades the quality of the 3D reconstruction. To handle motion blur caused by rapid camera shakes, we propose a blur-aware depth reconstruction method, which utilizes a pixel correspondence that is obtained by considering the effect of motion blur. Motion blur is dependent on 3D geometry, thus parameterizing blurred appearance of images with scene depth given camera motion is possible and a depth map can be accurately estimated from the blur-considered pixel correspondence. The estimated depth is then converted intopixel-wise blur kernels, and non-uniform motion blur is easily removed with low computational cost. The obtained blur kernel is depth-dependent, thus it effectively addresses scene-depth variation, which is a challenging problem in conventional non-uniform deblurring methods.</p><p>3 0.59192854 <a title="265-tfidf-3" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>4 0.55344868 <a title="265-tfidf-4" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>5 0.51523107 <a title="265-tfidf-5" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>6 0.49068743 <a title="265-tfidf-6" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>7 0.48806381 <a title="265-tfidf-7" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>8 0.39086765 <a title="265-tfidf-8" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>9 0.28292668 <a title="265-tfidf-9" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>10 0.23065838 <a title="265-tfidf-10" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>11 0.13266714 <a title="265-tfidf-11" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>12 0.1044974 <a title="265-tfidf-12" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>13 0.097436741 <a title="265-tfidf-13" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>14 0.09305834 <a title="265-tfidf-14" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>15 0.08122959 <a title="265-tfidf-15" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>16 0.079345502 <a title="265-tfidf-16" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>17 0.071127005 <a title="265-tfidf-17" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>18 0.063805178 <a title="265-tfidf-18" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>19 0.059983872 <a title="265-tfidf-19" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>20 0.0581921 <a title="265-tfidf-20" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, 0.234), (2, -0.07), (3, 0.222), (4, -0.216), (5, 0.691), (6, 0.122), (7, -0.016), (8, 0.048), (9, 0.016), (10, -0.006), (11, -0.034), (12, -0.013), (13, 0.021), (14, -0.003), (15, -0.005), (16, 0.129), (17, 0.001), (18, -0.03), (19, 0.005), (20, 0.0), (21, -0.012), (22, 0.016), (23, 0.074), (24, -0.01), (25, -0.045), (26, 0.052), (27, 0.011), (28, -0.004), (29, 0.015), (30, 0.029), (31, 0.043), (32, -0.016), (33, 0.014), (34, 0.049), (35, 0.012), (36, 0.001), (37, -0.001), (38, 0.024), (39, -0.021), (40, -0.006), (41, 0.019), (42, 0.007), (43, 0.03), (44, 0.014), (45, -0.02), (46, -0.015), (47, 0.013), (48, 0.017), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96417701 <a title="265-lsi-1" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>Author: Florent Couzinié-Devy, Jian Sun, Karteek Alahari, Jean Ponce</p><p>Abstract: This paper addresses the problem of restoring images subjected to unknown and spatially varying blur caused by defocus or linear (say, horizontal) motion. The estimation of the global (non-uniform) image blur is cast as a multilabel energy minimization problem. The energy is the sum of unary terms corresponding to learned local blur estimators, and binary ones corresponding to blur smoothness. Its global minimum is found using Ishikawa ’s method by exploiting the natural order of discretized blur values for linear motions and defocus. Once the blur has been estimated, the image is restored using a robust (non-uniform) deblurring algorithm based on sparse regularization with global image statistics. The proposed algorithm outputs both a segmentation of the image into uniform-blur layers and an estimate of the corresponding sharp image. We present qualitative results on real images, and use synthetic data to quantitatively compare our approach to the publicly available implementation of Chakrabarti et al. [5].</p><p>2 0.93536538 <a title="265-lsi-2" href="./cvpr-2013-Blur_Processing_Using_Double_Discrete_Wavelet_Transform.html">68 cvpr-2013-Blur Processing Using Double Discrete Wavelet Transform</a></p>
<p>Author: Yi Zhang, Keigo Hirakawa</p><p>Abstract: We propose a notion of double discrete wavelet transform (DDWT) that is designed to sparsify the blurred image and the blur kernel simultaneously. DDWT greatly enhances our ability to analyze, detect, and process blur kernels and blurry images—the proposed framework handles both global and spatially varying blur kernels seamlessly, and unifies the treatment of blur caused by object motion, optical defocus, and camera shake. To illustrate the potential of DDWT in computer vision and image processing, we develop example applications in blur kernel estimation, deblurring, and near-blur-invariant image feature extraction.</p><p>3 0.90720427 <a title="265-lsi-3" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>4 0.88763243 <a title="265-lsi-4" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>Author: Lin Zhong, Sunghyun Cho, Dimitris Metaxas, Sylvain Paris, Jue Wang</p><p>Abstract: State-of-the-art single image deblurring techniques are sensitive to image noise. Even a small amount of noise, which is inevitable in low-light conditions, can degrade the quality of blur kernel estimation dramatically. The recent approach of Tai and Lin [17] tries to iteratively denoise and deblur a blurry and noisy image. However, as we show in this work, directly applying image denoising methods often partially damages the blur information that is extracted from the input image, leading to biased kernel estimation. We propose a new method for handling noise in blind image deconvolution based on new theoretical and practical insights. Our key observation is that applying a directional low-pass filter to the input image greatly reduces the noise level, while preserving the blur information in the orthogonal direction to the filter. Based on this observation, our method applies a series of directional filters at different orientations to the input image, and estimates an accurate Radon transform of the blur kernel from each filtered image. Finally, we reconstruct the blur kernel using inverse Radon transform. Experimental results on synthetic and real data show that our algorithm achieves higher quality results than previous approaches on blurry and noisy images. 1</p><p>5 0.88735455 <a title="265-lsi-5" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>Author: Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, Stefan Roth</p><p>Abstract: Non-blind deblurring is an integral component of blind approaches for removing image blur due to camera shake. Even though learning-based deblurring methods exist, they have been limited to the generative case and are computationally expensive. To this date, manually-defined models are thus most widely used, though limiting the attained restoration quality. We address this gap by proposing a discriminative approach for non-blind deblurring. One key challenge is that the blur kernel in use at test time is not known in advance. To address this, we analyze existing approaches that use half-quadratic regularization. From this analysis, we derive a discriminative model cascade for image deblurring. Our cascade model consists of a Gaussian CRF at each stage, based on the recently introduced regression tree fields. We train our model by loss minimization and use synthetically generated blur kernels to generate training data. Our experiments show that the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur.</p><p>6 0.85668594 <a title="265-lsi-6" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>7 0.77857274 <a title="265-lsi-7" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>8 0.74628031 <a title="265-lsi-8" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>9 0.74308991 <a title="265-lsi-9" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>10 0.69130045 <a title="265-lsi-10" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>11 0.6749543 <a title="265-lsi-11" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>12 0.32570106 <a title="265-lsi-12" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>13 0.30332062 <a title="265-lsi-13" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>14 0.28846219 <a title="265-lsi-14" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>15 0.27340865 <a title="265-lsi-15" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>16 0.25721258 <a title="265-lsi-16" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>17 0.25150427 <a title="265-lsi-17" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>18 0.22739939 <a title="265-lsi-18" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>19 0.22645661 <a title="265-lsi-19" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>20 0.22243772 <a title="265-lsi-20" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.317), (16, 0.028), (26, 0.032), (28, 0.013), (33, 0.295), (55, 0.068), (67, 0.044), (69, 0.033), (87, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98536253 <a title="265-lda-1" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>2 0.98500401 <a title="265-lda-2" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>3 0.98366886 <a title="265-lda-3" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>4 0.98262686 <a title="265-lda-4" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>5 0.98175758 <a title="265-lda-5" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>6 0.97925776 <a title="265-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.97714055 <a title="265-lda-7" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>8 0.97148389 <a title="265-lda-8" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>9 0.96597028 <a title="265-lda-9" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>10 0.96265608 <a title="265-lda-10" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>11 0.95572203 <a title="265-lda-11" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>12 0.93865687 <a title="265-lda-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.93792009 <a title="265-lda-13" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>14 0.93162555 <a title="265-lda-14" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>15 0.92828476 <a title="265-lda-15" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>16 0.92154384 <a title="265-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.92048264 <a title="265-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.91772556 <a title="265-lda-18" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>19 0.91293043 <a title="265-lda-19" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>20 0.91248369 <a title="265-lda-20" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
