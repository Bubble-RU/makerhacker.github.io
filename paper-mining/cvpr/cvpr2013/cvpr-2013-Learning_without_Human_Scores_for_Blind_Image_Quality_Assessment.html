<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-266" href="#">cvpr2013-266</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</h1>
<br/><p>Source: <a title="cvpr-2013-266-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xue_Learning_without_Human_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Wufeng Xue, Lei Zhang, Xuanqin Mou</p><p>Abstract: General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in thefields of image processing, vision and machine learning. Stateof-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map.</p><p>Reference: <a title="cvpr-2013-266-reference" href="../cvpr2013_reference/cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in thefields of image processing, vision and machine learning. [sent-20, score-0.48]
</p><p>2 Stateof-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. [sent-21, score-0.607]
</p><p>3 However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. [sent-22, score-0.498]
</p><p>4 An interesting question is then: can we learn for effective BIQA without using human scored images? [sent-23, score-0.255]
</p><p>5 We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. [sent-25, score-0.813]
</p><p>6 Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. [sent-26, score-0.475]
</p><p>7 These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. [sent-27, score-0.954]
</p><p>8 It not only has comparable  accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map. [sent-29, score-0.697]
</p><p>9 Introduction With the ubiquitous use of digital imaging devices (e. [sent-31, score-0.081]
</p><p>10 , digital cameras and camera phones) and the rapid development of internet service, digital images have been becoming one of the most popular types of media in our daily life. [sent-33, score-0.09]
</p><p>11 The quality of those images can be deteriorated due to noise corruption, blur, JPEG or JPEG 2000 compression, etc. [sent-35, score-0.305]
</p><p>12 However, in most scenarios we do not have the source of the distorted image, and consequently how to evaluate blindly the quality of an image has been becoming increasingly important [22]. [sent-36, score-0.561]
</p><p>13 The current blind image quality assessment (BIQA) methods can be classified into two categories: distortion specific methods [1, 8, 9, 18, 25] and distortion independent methods [4, 10, 13, 14, 16, 17,21,27]. [sent-37, score-0.701]
</p><p>14 The former category estimates the quality of an image by quantifying the particular artifacts induced by the distortion process, and usually works well for one specific type of distortion. [sent-38, score-0.411]
</p><p>15 The latter category often refers to the general purpose BIQA, which is clearly a much more challenging task than the former category due to the lack of distortion information. [sent-39, score-0.158]
</p><p>16 In this paper we focus on the general purpose BIQA methods. [sent-40, score-0.024]
</p><p>17 Most of the state-of-the-art BIQA methods [4, 10, 13, 14, 16, 17, 21, 27] learn to estimate the image quality from training samples whose human subjective quality scores are available, e. [sent-41, score-0.841]
</p><p>18 Generally speaking, all these methods follow a two-step framework: feature extraction and model regression by human scores. [sent-44, score-0.113]
</p><p>19 [13] first uses a support vector machine (SVM) to detect the distortion type and then uses a support vector regression (SVR) [20] model specified to that distortion for BIQA. [sent-46, score-0.311]
</p><p>20 trained a probabilistic model for BIQA based on the contrast and structural features such as kurtosis and anisotropy in the DCT domain [16]. [sent-48, score-0.047]
</p><p>21 The BIQA metric in [21] extracts three sets of features based on the statistics of natural images, distortion textures and blur/noise. [sent-49, score-0.134]
</p><p>22 Three regression models are then trained for each feature set and finally a weighted combination of them is used to estimate the image quality. [sent-50, score-0.043]
</p><p>23 A summarization of the used features and the regression algorithms in recently developed BIQA methods can be found in [27]. [sent-51, score-0.043]
</p><p>24 The mostly widely used algorithm for regression is the SVR with a radial basis function as kernel. [sent-52, score-0.043]
</p><p>25 In [4], the sparse representation based classifier firstly developed in face recognition literature [26] was used to regress the image quality score. [sent-53, score-0.299]
</p><p>26 First of all, all these methods need a large amount of human scored images for training. [sent-55, score-0.221]
</p><p>27 They are randomly selected from the Berkeley Segmentation database [7]. [sent-58, score-0.026]
</p><p>28 Reference images in the (b) LIVE database [19]; (c) TID2008 database [15]; and (d) CSIQ database [6]. [sent-59, score-0.078]
</p><p>29 Second, these methods usually learn a mapping function (e. [sent-61, score-0.034]
</p><p>30 This makes the BIQA process a black box and the relationship between features and quality score implicit. [sent-66, score-0.363]
</p><p>31 None of these methods can provide a local quality map of the distorted image, which is much desirable to understand the good and bad quality regions of the input image. [sent-67, score-0.784]
</p><p>32 Intuitively, one interesting question is can we develop an effective and efficient BIQA algorithm but without using human scored images for training? [sent-69, score-0.221]
</p><p>33 ever proposed such an algorithm by conducting probabilistic latent semantic analysis (pLSA) on the statistical features of a large collection of pristine and distorted image patches. [sent-71, score-0.251]
</p><p>34 The uncovered latent quality factors are then applied to the image patches of the test image to infer a quality score. [sent-72, score-0.654]
</p><p>35 However, this method does not perform well compared with those methods learning with human scoring information. [sent-73, score-0.07]
</p><p>36 In this paper, we present a novel solution to BIQA using no human scored images in learning. [sent-74, score-0.221]
</p><p>37 They key is that we propose a quality-aware clustering (QAC) method to learn a set of quality-aware centroids and use them as the codebook to infer the quality of an image patch so that the quality of the whole image can be determined. [sent-75, score-0.855]
</p><p>38 With some reference and distorted images (but without human score), we partition them into overlapped patches and use a percentile pooling strategy to estimate the quality ofeach patch. [sent-76, score-0.987]
</p><p>39 According to the estimated quality level, the patches are grouped into different groups, and QAC is applied to each group to learn the quality-aware centroids. [sent-77, score-0.408]
</p><p>40 In the testing stage, each patch of the distorted image is compared to the learned qualityaware centroids, and a simple weighted average operation is used to assign a score to it. [sent-78, score-0.436]
</p><p>41 The perceptual quality score of the whole image can then be figured out by summing over all patches. [sent-79, score-0.435]
</p><p>42 Our experimental results validate that it has comparable accuracy to those state-of-the-art methods learning from human scored images. [sent-81, score-0.242]
</p><p>43 First, it shows that even without using human scored images for training, we are still able to develop effective BIQA algorithms. [sent-83, score-0.221]
</p><p>44 Second, it builds an explicit relationship between the image feature and the quality score, and could provide a local quality map of the input image, which is not achievable by all the other BIQA methods. [sent-84, score-0.554]
</p><p>45 Third, the proposed QAC is very fast and can work in  real-time, making it applicable to devices with limited computational resources (e. [sent-85, score-0.029]
</p><p>46 At last, QAC has a very high linearity to human perception of image quality. [sent-88, score-0.173]
</p><p>47 The learning of quality-aware centroids by QAC is described in detail in Section 2. [sent-90, score-0.139]
</p><p>48 Then how to use the learned centroids to perform blind quality estimation is described in Section 3. [sent-91, score-0.488]
</p><p>49 Learning dataset generation Our method works on image patches and aims to learn a set of quality-aware centroids for blind image quality assessment (BIQA). [sent-97, score-0.659]
</p><p>50 To this end, we need some reference and distorted images for training but do not need to know the human subjective scores of the distorted images. [sent-98, score-0.764]
</p><p>51 Considering that the existing IQA databases [6, 15, 19] will be used to evaluate and compare the different BIQA algorithms in the experiments, we do not use them in our method to better validate the generality and database-independency of our approach. [sent-99, score-0.055]
</p><p>52 Instead, we randomly selected from the Berkeley image database [7] ten source images (please refer to Fig. [sent-100, score-0.056]
</p><p>53 1(a)), which have different scenes from the images in the databases [6, 15, 19] that will be used in our experiments (please refer to Fig. [sent-101, score-0.034]
</p><p>54 The four most common types of distortions are simulated: Gaussian noise, Gaussian blur, JPEG compression and JPEG2000 compression. [sent-107, score-0.066]
</p><p>55 These four distortion types are also the ones TID2008, LIVE and CSIQ databases have in common. [sent-108, score-0.168]
</p><p>56 Finally, we obtain a 120 distorted images and 10 reference images. [sent-111, score-0.281]
</p><p>57 of the three quality levels should make sure that ty distribution of the resulted samples in the next balanced. [sent-112, score-0.325]
</p><p>58 Patch quality estimation and normalization With the simulated dataset which has no human subjective quality score, we aim to learn a set of quality-aware centroids for BIQA. [sent-115, score-1.002]
</p><p>59 We partition the reference and distorted images into many overlapped patches. [sent-118, score-0.356]
</p><p>60 Denote by xi a patch of one reference image and by di the distorted version of it. [sent-119, score-0.445]
</p><p>61 One key problem in our method is how to assign a perceptual quality to di. [sent-120, score-0.349]
</p><p>62 To this end, we can first use the similarity function in some state-of-the-art full-reference image quality assessment (FR-IQA) method, such as SSIM [23] and  FSIM [29], to calculate the similarity between xi and di. [sent-121, score-0.397]
</p><p>63 By this way, the dependency on human score are removed. [sent-122, score-0.156]
</p><p>64 The similarity score si can reflect the quality of di to some extent, and it ranges from 0 to 1. [sent-124, score-0.505]
</p><p>65 In FR-IQA, we usually simply take si as the local quality score of di, and average all si in one image as the final quality score of this image. [sent-125, score-0.868]
</p><p>66 Such a simple strategy works well for FR-IQA since the availability of reference image. [sent-126, score-0.097]
</p><p>67 Y-axis denotes the pre-  diction score by IQA models. [sent-129, score-0.086]
</p><p>68 Note that the mean values of the lowest 10% predicted quality scores shows much better linearity to the human subjective scores. [sent-130, score-0.623]
</p><p>69 is to learn for performing BIQA, and taking si as the quality score of di will have some problem. [sent-131, score-0.539]
</p><p>70 Suppose that the real human scored quality of a distorted image d is s, if we take si as the local quality score of its patch di, then the average of all si can be very different from s, leading to much bias in the learning stage. [sent-132, score-1.29]
</p><p>71 To solve this problem, we must normalize si in order to make the average of all si in an image as close to its overall perceptual quality as possible. [sent-133, score-0.512]
</p><p>72 It is known that the similarity functions in FR-IQA methods can only give a nonlinear monotonic prediction of the human subjective score [23, 29]. [sent-134, score-0.328]
</p><p>73 The red round point shows the FR-IQA results by FSIM with average pooling versus the subjective score with a two-order polynomial fitting. [sent-137, score-0.308]
</p><p>74 It is this nonlinearity that often makes the estimated quality score deviate from the human perception. [sent-138, score-0.455]
</p><p>75 On the other hand, it has been found that in an image, the predicted quality of the worst local areas has a good linearity to human perception [12, 24]. [sent-139, score-0.478]
</p><p>76 3 shows the worst 10% percentile pooling results of FSIM versus the subjective score, which has much better linearity. [sent-141, score-0.39]
</p><p>77 Based on this finding, we propose a percentile pooling procedure to normalize si. [sent-142, score-0.232]
</p><p>78 In particular, we divide si by a constant C such that the average quality of all patches in an image will equal to the  percentile pooling result. [sent-143, score-0.612]
</p><p>79 Denote by Ω the set of patch indices of an image, and by Ωp the set of indices of the 10% lowest quality patches. [sent-144, score-0.4]
</p><p>80 The normalization factor C is calculated as:  C =10? [sent-145, score-0.027]
</p><p>81 Quality-aware clustering With the patch quality normalization strategy in Section 2. [sent-151, score-0.406]
</p><p>82 2, finally we can have a set of patches {di} and their norm2. [sent-152, score-0.053]
</p><p>83 2al,iz fienda quality scores { ac sie}t, o bfa pseadtc on {wdhi}ch a nthde t qualityaware clustering can sb e { cco}n,du bcasteedd. [sent-153, score-0.425]
</p><p>84 1; bottom row: 3 clusters on quality level ql = 1. [sent-157, score-0.386]
</p><p>85 {ci} in hand, we can group {di} into groups of similar quality, }an ind hthanend ,c wluesct earn nt ghroosue patches itno tghreo same quality group into different clusters based on their local structures. [sent-158, score-0.451]
</p><p>86 Since ci is a real-value number between 0 and 1, we first uniformly quantize ci into L levels, denoted by ql = l/L, l = 1, 2, . [sent-159, score-0.15]
</p><p>87 Then the patches having the same quality level are grouped into the same group, denoted by Gl . [sent-163, score-0.351]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('biqa', 0.696), ('qac', 0.281), ('quality', 0.277), ('distorted', 0.23), ('scored', 0.151), ('subjective', 0.151), ('percentile', 0.14), ('centroids', 0.139), ('distortion', 0.134), ('fsim', 0.127), ('csiq', 0.095), ('score', 0.086), ('ql', 0.084), ('assessment', 0.084), ('jpeg', 0.079), ('blind', 0.072), ('perceptual', 0.072), ('si', 0.071), ('di', 0.071), ('pooling', 0.071), ('linearity', 0.071), ('human', 0.07), ('compression', 0.066), ('live', 0.065), ('qualityaware', 0.063), ('svr', 0.061), ('patch', 0.057), ('iqa', 0.056), ('patches', 0.053), ('jiaotong', 0.052), ('phones', 0.052), ('reference', 0.051), ('overlapped', 0.049), ('blur', 0.048), ('regression', 0.043), ('rec', 0.042), ('gl', 0.038), ('xi', 0.036), ('databases', 0.034), ('learn', 0.034), ('ci', 0.033), ('perception', 0.032), ('scores', 0.032), ('digital', 0.031), ('ten', 0.03), ('devices', 0.029), ('facebook', 0.028), ('tiom', 0.028), ('deteriorated', 0.028), ('bfa', 0.028), ('ddii', 0.028), ('saad', 0.028), ('sie', 0.028), ('worst', 0.028), ('becoming', 0.028), ('simulated', 0.027), ('resulted', 0.027), ('normalization', 0.027), ('xci', 0.026), ('ifm', 0.026), ('earn', 0.026), ('blindly', 0.026), ('cco', 0.026), ('moorthy', 0.026), ('availability', 0.026), ('partition', 0.026), ('database', 0.026), ('berkeley', 0.025), ('clusters', 0.025), ('clustering', 0.025), ('uncovered', 0.024), ('plsa', 0.024), ('kurtosis', 0.024), ('flicker', 0.024), ('itno', 0.024), ('purpose', 0.024), ('codebook', 0.023), ('attracting', 0.023), ('anisotropy', 0.023), ('group', 0.023), ('infer', 0.023), ('tahgee', 0.022), ('polytechnic', 0.022), ('lowest', 0.022), ('indices', 0.022), ('ssim', 0.022), ('nonlinearity', 0.022), ('regress', 0.022), ('service', 0.022), ('grouped', 0.021), ('normalize', 0.021), ('thr', 0.021), ('mittal', 0.021), ('ubiquitous', 0.021), ('monotonic', 0.021), ('conducting', 0.021), ('validate', 0.021), ('levels', 0.021), ('strategy', 0.02), ('corruption', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="266-tfidf-1" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>Author: Wufeng Xue, Lei Zhang, Xuanqin Mou</p><p>Abstract: General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in thefields of image processing, vision and machine learning. Stateof-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map.</p><p>2 0.15556975 <a title="266-tfidf-2" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>3 0.13945036 <a title="266-tfidf-3" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>4 0.070783794 <a title="266-tfidf-4" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>5 0.069417119 <a title="266-tfidf-5" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>6 0.056761213 <a title="266-tfidf-6" href="./cvpr-2013-Learning_to_Estimate_and_Remove_Non-uniform_Image_Blur.html">265 cvpr-2013-Learning to Estimate and Remove Non-uniform Image Blur</a></p>
<p>7 0.052696656 <a title="266-tfidf-7" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>8 0.051290601 <a title="266-tfidf-8" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>9 0.044065557 <a title="266-tfidf-9" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>10 0.043941721 <a title="266-tfidf-10" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>11 0.043400697 <a title="266-tfidf-11" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>12 0.041814934 <a title="266-tfidf-12" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>13 0.041164517 <a title="266-tfidf-13" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>14 0.040528774 <a title="266-tfidf-14" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>15 0.040383529 <a title="266-tfidf-15" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>16 0.03931766 <a title="266-tfidf-16" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>17 0.039196264 <a title="266-tfidf-17" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>18 0.038428441 <a title="266-tfidf-18" href="./cvpr-2013-Unnatural_L0_Sparse_Representation_for_Natural_Image_Deblurring.html">449 cvpr-2013-Unnatural L0 Sparse Representation for Natural Image Deblurring</a></p>
<p>19 0.038349081 <a title="266-tfidf-19" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>20 0.037962671 <a title="266-tfidf-20" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.099), (1, 0.005), (2, -0.012), (3, 0.029), (4, -0.004), (5, 0.066), (6, -0.001), (7, -0.007), (8, -0.004), (9, -0.031), (10, -0.016), (11, 0.009), (12, 0.027), (13, -0.025), (14, -0.02), (15, -0.02), (16, 0.021), (17, 0.005), (18, 0.056), (19, 0.004), (20, 0.019), (21, 0.022), (22, -0.026), (23, -0.042), (24, -0.028), (25, 0.031), (26, -0.045), (27, -0.007), (28, 0.01), (29, -0.062), (30, -0.068), (31, -0.005), (32, -0.036), (33, 0.031), (34, -0.044), (35, 0.073), (36, -0.004), (37, -0.045), (38, -0.045), (39, 0.019), (40, 0.035), (41, 0.005), (42, 0.017), (43, -0.0), (44, 0.034), (45, -0.006), (46, -0.012), (47, -0.055), (48, 0.025), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89698386 <a title="266-lsi-1" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>Author: Wufeng Xue, Lei Zhang, Xuanqin Mou</p><p>Abstract: General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in thefields of image processing, vision and machine learning. Stateof-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map.</p><p>2 0.69678628 <a title="266-lsi-2" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>3 0.67878777 <a title="266-lsi-3" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>4 0.64157021 <a title="266-lsi-4" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>Author: Xiaogang Chen, Sing Bing Kang, Jie Yang, Jingyi Yu</p><p>Abstract: Patch-based methods such as Non-Local Means (NLM) and BM3D have become the de facto gold standard for image denoising. The core of these approaches is to use similar patches within the image as cues for denoising. The operation usually requires expensive pair-wise patch comparisons. In this paper, we present a novel fast patch-based denoising technique based on Patch Geodesic Paths (PatchGP). PatchGPs treat image patches as nodes and patch differences as edge weights for computing the shortest (geodesic) paths. The path lengths can then be used as weights of the smoothing/denoising kernel. We first show that, for natural images, PatchGPs can be effectively approximated by minimum hop paths (MHPs) that generally correspond to Euclidean line paths connecting two patch nodes. To construct the denoising kernel, we further discretize the MHP search directions and use only patches along the search directions. Along each MHP, we apply a weightpropagation scheme to robustly and efficiently compute the path distance. To handle noise at multiple scales, we conduct wavelet image decomposition and apply PatchGP scheme at each scale. Comprehensive experiments show that our approach achieves comparable quality as the state-of-the-art methods such as NLM and BM3D but is a few orders of magnitude faster.</p><p>5 0.62918234 <a title="266-lsi-5" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>Author: Maria Zontak, Inbar Mosseri, Michal Irani</p><p>Abstract: Recurrence of small clean image patches across different scales of a natural image has been successfully used for solving ill-posed problems in clean images (e.g., superresolution from a single image). In this paper we show how this multi-scale property can be extended to solve ill-posed problems under noisy conditions, such as image denoising. While clean patches are obscured by severe noise in the original scale of a noisy image, noise levels drop dramatically at coarser image scales. This allows for the unknown hidden clean patches to “naturally emerge ” in some coarser scale of the noisy image. We further show that patch recurrence across scales is strengthened when using directional pyramids (that blur and subsample only in one direction). Our statistical experiments show that for almost any noisy image patch (more than 99%), there exists a “good” clean version of itself at the same relative image coordinates in some coarser scale of the image.This is a strong phenomenon of noise-contaminated natural images, which can serve as a strong prior for separating the signal from the noise. Finally, incorporating this multi-scale prior into a simple denoising algorithm yields state-of-the-art denois- ing results.</p><p>6 0.59547448 <a title="266-lsi-6" href="./cvpr-2013-A_Machine_Learning_Approach_for_Non-blind_Image_Deconvolution.html">17 cvpr-2013-A Machine Learning Approach for Non-blind Image Deconvolution</a></p>
<p>7 0.56867045 <a title="266-lsi-7" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>8 0.56700414 <a title="266-lsi-8" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>9 0.56441969 <a title="266-lsi-9" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>10 0.55835778 <a title="266-lsi-10" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>11 0.52930403 <a title="266-lsi-11" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>12 0.52695411 <a title="266-lsi-12" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>13 0.50891292 <a title="266-lsi-13" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>14 0.49935418 <a title="266-lsi-14" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>15 0.49832803 <a title="266-lsi-15" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>16 0.49827209 <a title="266-lsi-16" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>17 0.49645942 <a title="266-lsi-17" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>18 0.49429885 <a title="266-lsi-18" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>19 0.49405053 <a title="266-lsi-19" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>20 0.49078411 <a title="266-lsi-20" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.09), (16, 0.011), (26, 0.03), (33, 0.356), (60, 0.248), (67, 0.045), (69, 0.066), (87, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91456145 <a title="266-lda-1" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>2 0.88225955 <a title="266-lda-2" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>same-paper 3 0.87736964 <a title="266-lda-3" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>Author: Wufeng Xue, Lei Zhang, Xuanqin Mou</p><p>Abstract: General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in thefields of image processing, vision and machine learning. Stateof-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map.</p><p>4 0.83525008 <a title="266-lda-4" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>Author: Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</p><p>Abstract: How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What defines these spatiotemporal patches is their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore, these patches can be used as a discriminative vocabulary for action classification where they demonstrate stateof-the-art performance on UCF50 and Olympics datasets.</p><p>5 0.83521563 <a title="266-lda-5" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: Estimating geographic location from images is a challenging problem that is receiving recent attention. In contrast to many existing methods that primarily model discriminative information corresponding to different locations, we propose joint learning of information that images across locations share and vary upon. Starting with generative and discriminative subspaces pertaining to domains, which are obtained by a hierarchical grouping of images from adjacent locations, we present a top-down approach that first models cross-domain information transfer by utilizing the geometry ofthese subspaces, and then encodes the model results onto individual images to infer their location. We report competitive results for location recognition and clustering on two public datasets, im2GPS and San Francisco, and empirically validate the utility of various design choices involved in the approach.</p><p>6 0.83499563 <a title="266-lda-6" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>7 0.83481479 <a title="266-lda-7" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>8 0.83425206 <a title="266-lda-8" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>9 0.83424085 <a title="266-lda-9" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>10 0.83418506 <a title="266-lda-10" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>11 0.83385497 <a title="266-lda-11" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>12 0.83346081 <a title="266-lda-12" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>13 0.83335531 <a title="266-lda-13" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>14 0.83331496 <a title="266-lda-14" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>15 0.83316892 <a title="266-lda-15" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>16 0.83302569 <a title="266-lda-16" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>17 0.83302116 <a title="266-lda-17" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>18 0.83287168 <a title="266-lda-18" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>19 0.83281827 <a title="266-lda-19" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>20 0.83262849 <a title="266-lda-20" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
