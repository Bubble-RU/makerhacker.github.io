<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-210" href="#">cvpr2013-210</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</h1>
<br/><p>Source: <a title="cvpr-2013-210-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Illumination_Estimation_Based_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Bing Li, Weihua Xiong, Weiming Hu, Houwen Peng</p><p>Abstract: Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods.</p><p>Reference: <a title="cvpr-2013-210-reference" href="../cvpr2013_reference/cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. [sent-6, score-0.336]
</p><p>2 Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. [sent-7, score-0.4]
</p><p>3 However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. [sent-8, score-0.572]
</p><p>4 In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. [sent-9, score-1.158]
</p><p>5 For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. [sent-10, score-0.635]
</p><p>6 The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods. [sent-11, score-0.491]
</p><p>7 Introduction The color signals of any object from an imaging device are determined by three factors: the color of light incident on the scene, the surface reflectance of the object, and sensor sensitivity function of the camera [7] [8]. [sent-13, score-0.366]
</p><p>8 Therefore, the color of same surface will usually appear differently under varying light sources. [sent-14, score-0.187]
</p><p>9 In contrast, the human beings have the ability to “see” a surface as having the same color independent of variations of the illumination, which is called “Color Constancy” [16]. [sent-15, score-0.152]
</p><p>10 Computational color constancy is targeted for providing the same sort of color stability in the context of computer vision [1], and its central issue is to build up an optimal illumination estimation model. [sent-16, score-0.764]
</p><p>11 Most early studies treat an image as a bag of pixels with RGB values and give out the illumination estimation model without considering the underlying semantic content expressed by the pixels’ arrangement. [sent-28, score-0.392]
</p><p>12 The unsupervised DD methods, such as Grey World (GW)[1 1], maxRGB [24], Shades of Grey (SoG)[18], and Edge-based method [29] (also called Grey Edge, GE), etc, predefine fixed illumination estimation models based on certain hypotheses for all images. [sent-31, score-0.3]
</p><p>13 Once the model is fixed in a DD method, the illumination colors of all the test images are computed out using the same model. [sent-34, score-0.299]
</p><p>14 In order to avoid the fixed model problem, many researchers focus on the model selection or combination for illumination estimation. [sent-36, score-0.226]
</p><p>15 Recent years have witnessed a rise in applying image content analysis to guide illumination estimation. [sent-37, score-0.342]
</p><p>16 [20], which selects the most appropriate unsupervised DD method based on natu1 1 14 4 42 2 213 1  ral texture statistics and scene semantics of the test image (NIS). [sent-41, score-0.16]
</p><p>17 [10] propose to use the indoor/outdoor scene classification for choosing the most appropriate estimation method (IO). [sent-45, score-0.141]
</p><p>18 [30] use high level visual information for improving illumination estimation (HVI), in which an image is modeled as a mixture of semantic classes, such as sky, grass, road, and building. [sent-47, score-0.311]
</p><p>19 Then they evaluate several different illumination estimation models on the likelihood of its semantic content in correspondence with prior knowledge of the world, and produce the final output that results in the most likely semantic composition of the image. [sent-48, score-0.392]
</p><p>20 According to the analysis on the CD methods, we obtain the following observations: • Since most existing CD methods are combinational Sminetcheod ms,o sthte eirx performance tish inevitably mafbfiencatetido by the DD methods used for combination. [sent-49, score-0.18]
</p><p>21 Although the high level scene content is useful for iAllluthmoiungathio tnh ee hstigimha tlieovnel, sacuetnoem catoicnatel scene ecfounltfe notr classification, such as 3D stage classification or indoor/outdoor classification, is another difficult and unsolved computer vision problem. [sent-52, score-0.333]
</p><p>22 Our work According to the observations above, this paper proposes a novel bilayer sparse coding model (BSC) for illumination estimation that integrates the high level content cues and low level color features into a unified supervised framework. [sent-55, score-1.129]
</p><p>23 The proposed BSC method models illumination estimation as an image similarity problem and considers low level color distribution and high level scene category simultaneously. [sent-56, score-0.626]
</p><p>24 Our work is primarily inspired by two hypotheses: (1) The images with similar color distributions are preferable to be captured under the similar light colors; and (2) the scenes belonging to the same high level category have the similar illumination conditions [10]. [sent-57, score-0.485]
</p><p>25 This is because the varying range of light colors in a certain type of scene is often limited. [sent-58, score-0.154]
</p><p>26 So the BSC method is to directly estimate the illumination color of the test image based on the training images that are similar to the test image from both color and scene viewpoints. [sent-64, score-0.744]
</p><p>27 •  •  The BSC method need not explicitly classify the scene iTnhtoe predefined din ndeeodor /nooutt edxopolri or o ctlhaessr scene ccaetneegories. [sent-65, score-0.182]
</p><p>28 Instead, it integrates the high level scene content similarity into the supervised illumination estimation procedure so as to avoid negative impact of incorrect hard scene classification. [sent-66, score-0.648]
</p><p>29 i uSitinocne tihne t sparse coding mise a ondo-m lieosde inl learning algorithm. [sent-69, score-0.232]
</p><p>30 Compared with most existing methods that always use a prefixed model(or a limited model set for selection) for all the test images, our BSC algorithm adaptively learns a individualized model for each test image according to its color and scene cues. [sent-70, score-0.333]
</p><p>31 Sparse Coding Preliminaries Before introducing the details of our model, we start with a brief overview of sparse coding that is the basis of the pro-  posed algorithm. [sent-72, score-0.232]
</p><p>32 The goal of sparse coding is to sparsely represent input vectors approximately as a weighted linear combination of a number of “basis vectors”. [sent-74, score-0.232]
</p><p>33 , unn i]n p∈u tR vke×ctno,r sparse coding aims to find a spa? [sent-79, score-0.232]
</p><p>34 Fortunately, recent results [31] show that, if the solution is sparse enough, the sparse representation can be recovered by the following convex ? [sent-92, score-0.18]
</p><p>35 Bilayer Sparse Coding for Illumination Estimation In this section, we firstly propose bilayer sparse coding model (BSC) for illumination estimation; then discuss color feature and scene feature used in BSC; and finally give out an optimization algorithm for BSC. [sent-104, score-0.922]
</p><p>36 , IN, the color feature vector of the image Ii is Ci ∈ Rd. [sent-112, score-0.152]
</p><p>37 Here, color feature Ci can be binarized 2D/3D chromaticity histogram that has been proved to be effective for many supervised color constancy algorithms [17] [12][32]. [sent-113, score-0.762]
</p><p>38 For any test image Iy with color feature Cy ∈ Rd, we can linearly reconstruct its color feature using th∈e training images under the sparse coding framework, as: mγin? [sent-114, score-0.614]
</p><p>39 , γN]T is a N-dimensional coefficient vector that indicates the reconstruction weight associated with each training image. [sent-124, score-0.169]
</p><p>40 From viewpoint of color gamut, the Eq(3) is actually to reconstruct the color gamut of the test image using color gamut  of all the training images. [sent-125, score-0.952]
</p><p>41 The sparse code γ can also be viewed as the color correlation coefficient between Iy and each training image. [sent-126, score-0.395]
</p><p>42 2  Sparse Coding for Scene Category Similarity  Generally speaking, a typical type of scene is determined by a bag of certain objects and their co-occurrence relationships [23]. [sent-129, score-0.124]
</p><p>43 The test image Iy is also segmented into ny objects Iy1, Iy2, . [sent-142, score-0.153]
</p><p>44 The scene category similarity analysis here is to reco∈ns Rtruct the ny objects in the test image by using the n1 + n2 + . [sent-149, score-0.251]
</p><p>45 Considering co-occurrence property of objects in the same image, we should try to reconstruct the objects in the test image using those objects from the same training image. [sent-153, score-0.177]
</p><p>46 Therefore, we introduce the multi-task joint sparse  vnyy  Figure 1. [sent-154, score-0.135]
</p><p>47 Sparse reconstruction of image’s scene content: (A) test images Iy and its segmented objects. [sent-155, score-0.214]
</p><p>48 , ny) is a reconstruction coefficient vector of the jth object in Iy associated with all the objects in I1. [sent-159, score-0.169]
</p><p>49 , ny) is reconstruction coefficient vector of the jth object in Iy associated with all the objects in IN. [sent-163, score-0.169]
</p><p>50 The multi-task joint sparse representation can be regarded as a combinational model of group Lasso and multi-task Lasso by penalizing the sum of ? [sent-166, score-0.27]
</p><p>51 2 norms of the blocks of coefficients associated with each covariate group (objects in each training image) across different reconstruction tasks (object reconstruction in the test image)[33]. [sent-167, score-0.174]
</p><p>52 For any test object Iyj in the test image Iy, if ∈ Rni denotes the reconstruction coefficient associated wi∈th Rthe objects Ii1, Ii2 , . [sent-168, score-0.259]
</p><p>53 , Winy] ∈ Rni to represent the reconstruction coefficient ma]tr ∈ix Rof all the objects in Iy associated with all the objects in the image Ii . [sent-174, score-0.202]
</p><p>54 The joint sparse representation of all the objects in the test image can be formulated as [33]:  mWinj? [sent-176, score-0.168]
</p><p>55 is the sparse coefficient matrix for all the objects in the test image; β is the regularization coefficient. [sent-192, score-0.256]
</p><p>56 Their outputs are between (0, 1] and can be viewed as the costs in sparse color reconstruction and sparse scene content reconstruction. [sent-260, score-0.587]
</p><p>57 In the color layer, it tends to select the images with lower f( ? [sent-261, score-0.152]
</p><p>58 2,1 norm of the scene reconstruction coefficient Wi, to nreocromns ? [sent-263, score-0.25]
</p><p>59 Comparing Eq(5) with Eq(3) can tell us that the γ in BSC model contains not only color correlation but also scene content correlation information. [sent-279, score-0.423]
</p><p>60 4  Illumination Estimation  The coefficient γ in Eq(5), which represents the correlation between the test image and all training images, is used for illumination estimation. [sent-284, score-0.424]
</p><p>61 To remove the shading effect, the ground truth illumination color value ei = (Ri, Gi, Bi)T of the training image Ii is mapped into 2D chromaticity space through: li = cient vector γ is? [sent-285, score-0.595]
</p><p>62 gy)T  So the final illumination chromaticity ly= (ry, of the test image can be estimated as the weighted average of the illumination values of all the training images as: ly = L γˆ,  L = [l1, l2, . [sent-294, score-0.736]
</p><p>63 In the color reconstruction layer, we consider 3D color sp? [sent-300, score-0.352]
</p><p>64 ace as [32]: two chromaticity values, defined as (r,g)T =  ? [sent-301, score-0.184]
</p><p>65 e chromaticity space (r, g)T is equally partitioned along each component into 50 equal parts yields 2500 bins. [sent-306, score-0.184]
</p><p>66 The intensity L is quantized into 25 equal steps [32][9], so the 3D color histograms consist of 62,500 (50 50 25) bins [32]. [sent-307, score-0.152]
</p><p>67 Each image is represented as a 0 b0in (a5r0iz×ed5 03D× chromaticity histogram, ien i sw rhepicrhe s’e e1n n’ or ’0’ indicates the presence or absence of the corresponding chromaticity and intensity in the image. [sent-308, score-0.368]
</p><p>68 Since 0 ≤ r+g ≤ 1, a compact 3yDan chromaticity histogram can ebe0 ≤obt ra+inged ≤ by discarding the space with r+g > 1. [sent-309, score-0.205]
</p><p>69 In the scene layer, the SIFT descriptor [26] that is widely applied to scene classification, is used as object’s visual feature under the Bag-of-Word (BoW) model [2]. [sent-310, score-0.182]
</p><p>70 Considering that the scene layer is to find the training images with both similar scene contents and similar illumination conditions to the test image, color SIFT descriptor on r-g chromaticity space is used as scene feature. [sent-311, score-0.991]
</p><p>71 However, if the value of γ is fixed, the optimization in scene layer is just a multi-task joint sparse coding, which  can be effectively solved via the ? [sent-318, score-0.259]
</p><p>72 On the other hand, if the coefficient matrix W is given, the optimization in color layer is just a general sparse coding with a cost constrain that can also be solved by the ? [sent-320, score-0.55]
</p><p>73 2,1 mixednorm APG algorithm to optimize the bilayer sparse coding as shown in Algorithm 1. [sent-323, score-0.52]
</p><p>74 The second one includes the real1 1 14 4 42 2 246 4  Algorithm 1 Pseudo-code for bilayer sparse coding optimization. [sent-336, score-0.453]
</p><p>75 Input: The color feature Ciand scene feature Vi= [v1i , vi2 , . [sent-337, score-0.243]
</p><p>76 , vnii] ∈ Rm×ni of each training image, the color fea∈ture Cy and scene feature Vy = [v1y , vy2 , . [sent-340, score-0.276]
</p><p>77 , vnyy] of the test image,the regularization coefficient λ and β, the threshold ε. [sent-343, score-0.133]
</p><p>78 The BSC method is compared with some leading illumination estimation methods, including GW [11], maxRGB [24], Grey Edge (0th, 1st, 2nd-order)[29], Gamut Mapping [22], Spatio-Spectral [13], SVR[32], HVI [30] and NIS[20]. [sent-370, score-0.276]
</p><p>79 The binarized 3D color histogram is also used in the SVR method. [sent-372, score-0.203]
</p><p>80 In order to further validate the effect of the scene category for illumination estimation, the single color layer in BSC (denoted as SSC) excluding any scene cue is also used in comparison. [sent-379, score-0.675]
</p><p>81 For each image in the image sets, the ground truth chromaticity of the light source ea = (ra, ga, ba) is known. [sent-387, score-0.25]
</p><p>82 ,(10) where ey • ea is the dot product of ey and the ea ; and ? [sent-395, score-0.132]
</p><p>83 The worst-25% (or best-25%) indicates the mean angular error of the largest (or smallest) 25% angular errors on test images. [sent-401, score-0.211]
</p><p>84 The two facts imply that high level scene category cues can indeed improve the illumination estimation. [sent-497, score-0.412]
</p><p>85 Furthermore, the SSC outperforms SVR method, which shows that the sparse coding technique is a good alternative learning tool for illumination estimation. [sent-498, score-0.458]
</p><p>86 Since a matte grey sphere ball is mounted onto the video camera to obtain the ground truth illumination of each image; in order to ensure that the grey ball has no effect on our results, the grey sphere is masked during experiments. [sent-503, score-0.502]
</p><p>87 The proposed BSC method outperforms all other methods, even better than the combinational method NIS and HVI. [sent-513, score-0.18]
</p><p>88 The SSC method also achieves much better performance than all the other methods except NIS and HVI, which again implies the effect of the sparse code technique for illumination estimation. [sent-515, score-0.316]
</p><p>89 The fact that the BSC method outperforms SSC further confirms the effect of scene category cues for illumination estimation. [sent-516, score-0.377]
</p><p>90 Conclusion Image’s high level content cue has been evidenced to be helpful for improving the illumination estimation. [sent-518, score-0.377]
</p><p>91 However, most prevailing methods using high level content cues can be viewed as combinational methods. [sent-519, score-0.389]
</p><p>92 In this paper, we integrate image’s color distribution and scene content analysis into a unified bilayer sparse coding framework for illumination estimation. [sent-520, score-1.038]
</p><p>93 The experiments on real-world image sets show that the mutually constrained combination can improve the accuracy of illumination estimation. [sent-521, score-0.226]
</p><p>94 A comparison of computational color constancy algorithms-part 1: Methodology and 11111444442222268866  Table 2. [sent-558, score-0.336]
</p><p>95 A comparison of computational color constancy algorithms-part 2: Experiments with image data. [sent-637, score-0.336]
</p><p>96 Estimating the scene illumination chromaticity using a neural network. [sent-661, score-0.501]
</p><p>97 Color by correlation: A simple, unifying framework for color constancy. [sent-689, score-0.152]
</p><p>98 Color constancy using natural image statistics and scene semantics. [sent-709, score-0.275]
</p><p>99 Generalized gamut mapping using image derivative structuresfor color constancy. [sent-722, score-0.4]
</p><p>100 Color constancy algorithms: psychophysical evaluation on a new dataset. [sent-760, score-0.184]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bsc', 0.584), ('illumination', 0.226), ('bilayer', 0.221), ('gamut', 0.209), ('constancy', 0.184), ('chromaticity', 0.184), ('combinational', 0.18), ('gijsenij', 0.157), ('color', 0.152), ('coding', 0.142), ('hvi', 0.139), ('dd', 0.122), ('content', 0.116), ('iy', 0.112), ('eq', 0.11), ('grey', 0.092), ('scene', 0.091), ('sparse', 0.09), ('maxrgb', 0.09), ('nis', 0.09), ('ssc', 0.089), ('coefficient', 0.088), ('gw', 0.087), ('svr', 0.087), ('angular', 0.083), ('layer', 0.078), ('cd', 0.068), ('apg', 0.067), ('ciurea', 0.067), ('mixednorm', 0.067), ('reprocessed', 0.067), ('sfu', 0.066), ('nlpr', 0.066), ('wi', 0.062), ('estimation', 0.05), ('reconstruction', 0.048), ('tip', 0.047), ('gehler', 0.046), ('barnard', 0.046), ('lasso', 0.046), ('ny', 0.045), ('ge', 0.045), ('bianco', 0.045), ('cardei', 0.045), ('houwen', 0.045), ('inii', 0.045), ('vnii', 0.045), ('vnyy', 0.045), ('vyj', 0.045), ('test', 0.045), ('canon', 0.043), ('weijer', 0.041), ('endc', 0.04), ('supervised', 0.039), ('mapping', 0.039), ('bow', 0.039), ('gevers', 0.038), ('xiong', 0.038), ('category', 0.037), ('ofmorp', 0.037), ('finlayson', 0.037), ('cy', 0.036), ('light', 0.035), ('level', 0.035), ('prevailing', 0.035), ('ey', 0.035), ('rm', 0.034), ('shades', 0.033), ('vik', 0.033), ('objects', 0.033), ('etc', 0.033), ('training', 0.033), ('correlation', 0.032), ('io', 0.032), ('gy', 0.032), ('ea', 0.031), ('binarized', 0.03), ('recomputed', 0.03), ('segmented', 0.03), ('cn', 0.03), ('al', 0.028), ('rni', 0.028), ('colors', 0.028), ('gamma', 0.027), ('imaging', 0.027), ('tpami', 0.026), ('rthe', 0.025), ('ii', 0.025), ('unsupervised', 0.024), ('lights', 0.024), ('ry', 0.023), ('cues', 0.023), ('proximal', 0.023), ('norm', 0.023), ('sift', 0.022), ('colour', 0.022), ('shi', 0.022), ('pages', 0.022), ('ly', 0.022), ('histogram', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="210-tfidf-1" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>Author: Bing Li, Weihua Xiong, Weiming Hu, Houwen Peng</p><p>Abstract: Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods.</p><p>2 0.10514297 <a title="210-tfidf-2" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>3 0.10467098 <a title="210-tfidf-3" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>4 0.096702866 <a title="210-tfidf-4" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>Author: Cheng Li, Kris M. Kitani</p><p>Abstract: We address the task of pixel-level hand detection in the context of ego-centric cameras. Extracting hand regions in ego-centric videos is a critical step for understanding handobject manipulation and analyzing hand-eye coordination. However, in contrast to traditional applications of hand detection, such as gesture interfaces or sign-language recognition, ego-centric videos present new challenges such as rapid changes in illuminations, significant camera motion and complex hand-object manipulations. To quantify the challenges and performance in this new domain, we present a fully labeled indoor/outdoor ego-centric hand detection benchmark dataset containing over 200 million labeled pixels, which contains hand images taken under various illumination conditions. Using both our dataset and a publicly available ego-centric indoors dataset, we give extensive analysis of detection performance using a wide range of local appearance features. Our analysis highlights the effectiveness of sparse features and the importance of modeling global illumination. We propose a modeling strategy based on our findings and show that our model outperforms several baseline approaches.</p><p>5 0.094464369 <a title="210-tfidf-5" href="./cvpr-2013-Scalable_Sparse_Subspace_Clustering.html">379 cvpr-2013-Scalable Sparse Subspace Clustering</a></p>
<p>Author: Xi Peng, Lei Zhang, Zhang Yi</p><p>Abstract: In this paper, we address two problems in Sparse Subspace Clustering algorithm (SSC), i.e., scalability issue and out-of-sample problem. SSC constructs a sparse similarity graph for spectral clustering by using ?1-minimization based coefficients, has achieved state-of-the-art results for image clustering and motion segmentation. However, the time complexity of SSC is proportion to the cubic of problem size such that it is inefficient to apply SSC into large scale setting. Moreover, SSC does not handle with out-ofsample data that are not used to construct the similarity graph. For each new datum, SSC needs recalculating the cluster membership of the whole data set, which makes SSC is not competitive in fast online clustering. To address the problems, this paper proposes out-of-sample extension of SSC, named as Scalable Sparse Subspace Clustering (SSSC), which makes SSC feasible to cluster large scale data sets. The solution of SSSC adopts a ”sampling, clustering, coding, and classifying” strategy. Extensive experimental results on several popular data sets demonstrate the effectiveness and efficiency of our method comparing with the state-of-the-art algorithms.</p><p>6 0.08871302 <a title="210-tfidf-6" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>7 0.087552674 <a title="210-tfidf-7" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>8 0.076883674 <a title="210-tfidf-8" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>9 0.075204864 <a title="210-tfidf-9" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>10 0.074611224 <a title="210-tfidf-10" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>11 0.073616646 <a title="210-tfidf-11" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>12 0.068403222 <a title="210-tfidf-12" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>13 0.06714429 <a title="210-tfidf-13" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>14 0.063470773 <a title="210-tfidf-14" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>15 0.063243158 <a title="210-tfidf-15" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>16 0.0587501 <a title="210-tfidf-16" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>17 0.056921653 <a title="210-tfidf-17" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>18 0.056755189 <a title="210-tfidf-18" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>19 0.056347031 <a title="210-tfidf-19" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>20 0.055516753 <a title="210-tfidf-20" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.151), (1, 0.025), (2, -0.037), (3, 0.074), (4, -0.001), (5, -0.034), (6, -0.029), (7, 0.012), (8, -0.007), (9, -0.005), (10, -0.026), (11, -0.065), (12, 0.003), (13, -0.013), (14, 0.028), (15, 0.024), (16, 0.002), (17, -0.004), (18, 0.005), (19, -0.016), (20, 0.074), (21, 0.02), (22, 0.017), (23, -0.066), (24, -0.013), (25, 0.018), (26, 0.032), (27, 0.037), (28, -0.019), (29, 0.001), (30, 0.01), (31, 0.024), (32, -0.027), (33, -0.032), (34, -0.052), (35, -0.013), (36, -0.061), (37, 0.121), (38, 0.055), (39, -0.068), (40, -0.148), (41, -0.019), (42, 0.017), (43, -0.004), (44, -0.005), (45, -0.026), (46, 0.011), (47, -0.009), (48, -0.062), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93466347 <a title="210-lsi-1" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>Author: Bing Li, Weihua Xiong, Weiming Hu, Houwen Peng</p><p>Abstract: Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods.</p><p>2 0.70520419 <a title="210-lsi-2" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>Author: Antony Lam, Imari Sato</p><p>Abstract: Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally, we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.</p><p>3 0.68071502 <a title="210-lsi-3" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>4 0.66175961 <a title="210-lsi-4" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>5 0.65798908 <a title="210-lsi-5" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>6 0.63807374 <a title="210-lsi-6" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>7 0.61839938 <a title="210-lsi-7" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>8 0.61427844 <a title="210-lsi-8" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>9 0.61210507 <a title="210-lsi-9" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>10 0.60223067 <a title="210-lsi-10" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>11 0.60202354 <a title="210-lsi-11" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>12 0.58647293 <a title="210-lsi-12" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>13 0.58087516 <a title="210-lsi-13" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>14 0.57317775 <a title="210-lsi-14" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>15 0.57128221 <a title="210-lsi-15" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>16 0.56292063 <a title="210-lsi-16" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>17 0.56257409 <a title="210-lsi-17" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>18 0.55467695 <a title="210-lsi-18" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>19 0.55429977 <a title="210-lsi-19" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>20 0.54360688 <a title="210-lsi-20" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.092), (16, 0.051), (26, 0.039), (28, 0.011), (33, 0.265), (67, 0.034), (69, 0.046), (80, 0.307), (87, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83280945 <a title="210-lda-1" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>2 0.80418682 <a title="210-lda-2" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>same-paper 3 0.78784764 <a title="210-lda-3" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>Author: Bing Li, Weihua Xiong, Weiming Hu, Houwen Peng</p><p>Abstract: Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods.</p><p>4 0.78182244 <a title="210-lda-4" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: In this paper we consider the challenging problem of articulated human pose estimation in still images. We observe that despite high variability of the body articulations, human motions and activities often simultaneously constrain the positions of multiple body parts. Modelling such higher order part dependencies seemingly comes at a cost of more expensive inference, which resulted in their limited use in state-of-the-art methods. In this paper we propose a model that incorporates higher order part dependencies while remaining efficient. We achieve this by defining a conditional model in which all body parts are connected a-priori, but which becomes a tractable tree-structured pictorial structures model once the image observations are available. In order to derive a set of conditioning variables we rely on the poselet-based features that have been shown to be effective for people detection but have so far found limited application for articulated human pose estimation. We demon- strate the effectiveness of our approach on three publicly available pose estimation benchmarks improving or being on-par with state of the art in each case.</p><p>5 0.77856004 <a title="210-lda-5" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>6 0.75299531 <a title="210-lda-6" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>7 0.73333228 <a title="210-lda-7" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>8 0.7264418 <a title="210-lda-8" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>9 0.71064723 <a title="210-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.70791596 <a title="210-lda-10" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>11 0.70134228 <a title="210-lda-11" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>12 0.700297 <a title="210-lda-12" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>13 0.69933498 <a title="210-lda-13" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>14 0.69906569 <a title="210-lda-14" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>15 0.69891524 <a title="210-lda-15" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>16 0.69871658 <a title="210-lda-16" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>17 0.69853294 <a title="210-lda-17" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>18 0.69823617 <a title="210-lda-18" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>19 0.69793838 <a title="210-lda-19" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>20 0.69667697 <a title="210-lda-20" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
