<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-221" href="#">cvpr2013-221</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</h1>
<br/><p>Source: <a title="cvpr-2013-221-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Incorporating_Structural_Alternatives_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan</p><p>Abstract: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the andnodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC2007and UIUCPeople, and it achieves state-of-the-arts performance.</p><p>Reference: <a title="cvpr-2013-221-reference" href="../cvpr2013_reference/cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i. [sent-2, score-0.154]
</p><p>2 These capabilities enable us to account well for structural variabilities while preserving the model compact. [sent-7, score-0.151]
</p><p>3 For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e. [sent-9, score-0.389]
</p><p>4 , leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. [sent-11, score-0.217]
</p><p>5 The objective of this work is to develop a novel hierarchical and reconfigurable model for multiclass object recognition, in the form of an And-Or graph representation, as Fig. [sent-17, score-0.382]
</p><p>6 Parts of the model for sheep and horse are shown. [sent-27, score-0.211]
</p><p>7 The squares in bottom represent the leaf-nodes, which can be shared among different classes(e. [sent-28, score-0.126]
</p><p>8 the leaf-node for localizing legs are shared between sheep and horse). [sent-30, score-0.263]
</p><p>9 The or-nodes over bottom are used to activate their children leaf-nodes, tackling the appearance variability. [sent-31, score-0.203]
</p><p>10 However, the structural configura-  tions of these models are mainly fixed, e. [sent-35, score-0.151]
</p><p>11 Inspired by And-Or graph models in [13, 27, 7, 23], we develop the “switch variables”, namely or-nodes, to specify alternative compositions in hierarchy. [sent-38, score-0.142]
</p><p>12 In detection, the or-nodes are used to activate its children leaf-nodes (i. [sent-39, score-0.203]
</p><p>13 It worths mentioning that the association of or-nodes with its children leafnodes can be automatically determined in model training. [sent-42, score-0.279]
</p><p>14 1, the sheep head is localized by the leaf-node that is activated by its parent or-node. [sent-44, score-0.285]
</p><p>15 In the context of multiclass object recognition, existing systems commonly treat different classes as unrelated entities. [sent-46, score-0.44]
</p><p>16 According to acknowledged studies [20, 18, 16], sharing information among different classes can boost model performance in general and alleviate the requirement of a large amount of training data. [sent-47, score-0.25]
</p><p>17 It inspires us to make structure shared in the And-Or graph model, for adapting the task of multiclass recognition. [sent-50, score-0.452]
</p><p>18 In our method, the leaf-nodes are sharable among different classes so that we keep the model compact to represent multiple object categories. [sent-51, score-0.217]
</p><p>19 1, the part of feet in category horse and sheep have  similar appearances, and thus can be both detected by the leaf-node shared across the two classes. [sent-53, score-0.337]
</p><p>20 The key contribution of this work is a novel And-Or graph model for multiclass object recognition, by addressing the both above issues. [sent-54, score-0.382]
</p><p>21 The leafnodes (denoted by squares) in the bottom are discriminative classifiers for detecting object parts. [sent-57, score-0.233]
</p><p>22 The or-nodes (denoted by dashed circles) over in the third layer are used to activate one of its children leaf-nodes in detection, which are allowed to slightly perturb for capturing deformations. [sent-58, score-0.29]
</p><p>23 The root-node at top is for switching multiclass recognition, which is also an ornode. [sent-60, score-0.316]
</p><p>24 In addition, we define the collaborative edges (denoted by curve connections) to encode intraclass (part-level) relations, and interclass contexts are modeled in the similar way as the edges connect the and-nodes also. [sent-61, score-0.24]
</p><p>25 In our method, we propose a novel algorithm for this problem, namely Dynamical Structural Optimization (DSO), motivated by the recently proposed structural optimization methods [24, 11]. [sent-63, score-0.151]
</p><p>26 In this step, we produce leaf-  nodes associated with their parent or-nodes and make leafnodes shared across classes. [sent-67, score-0.394]
</p><p>27 Due to large variance among classes, it would be intractable to train the classes altogether by pooling all samples from different classes into a bag. [sent-69, score-0.279]
</p><p>28 Then we train the models for object classes in each group. [sent-71, score-0.222]
</p><p>29 For example, we can easily decide to put sheep and horses into one group and train the multiclass model by sharing. [sent-72, score-0.509]
</p><p>30 And the collaborative edges are also learned during this step. [sent-74, score-0.195]
</p><p>31 Related Work Traditional multiclass object detectors are trained in a one-vs-all manner, where each object category are trained independently. [sent-76, score-0.461]
</p><p>32 A pioneer work [20] is proposed to learn shared features among classes and improve the classifier in both effectiveness and efficiency. [sent-78, score-0.239]
</p><p>33 The efficiency can be significantly im-  proved by integrating taxonomies with object hierarchy [8]. [sent-82, score-0.151]
</p><p>34 To tackle realistic challenges in object recognition, many deformable part-based methods are developed by latent structural learning recently [6, 26, 19]. [sent-83, score-0.265]
</p><p>35 These models are also extended to multiclass recognition and detection [4, 17, 16, 15]. [sent-84, score-0.334]
</p><p>36 [16] present the multiclass Hough Forest combing with the part-based models; Desai et al. [sent-86, score-0.271]
</p><p>37 Its general idea, using And/Or nodes to account for structural compositions and variabilities in hierarchy, has been applied in several vision tasks, e. [sent-90, score-0.238]
</p><p>38 [7] propose to train the And-Or graph for multiclass shape-based detection in a generative way, and extensively discuss the learning strategies. [sent-95, score-0.442]
</p><p>39 Motivated by these works, we propose an alternating way to discriminatively train the And-Or graph model for multiclass object recognition, and achieve superior performances. [sent-96, score-0.435]
</p><p>40 And-Or Graph Model  ××  Our multiclass object model is constructed in the form of an And-Or graph G = (V, E), where V contains three types Aofn dn-oOders, g rEa represents t,hEe) ,co wllhaebroera Vtiv ceo edges. [sent-98, score-0.382]
</p><p>41 are local classifiers for object parts, and they can be shared among different classes. [sent-121, score-0.182]
</p><p>42 Specifically, if a leaf-node is affiliated to the j-th or-node, it is also possible to be shared by the or-nodes in other classes indexed by j + 9 k, where tkh e∈ o r{-n1,o d2. [sent-122, score-0.28]
</p><p>43 We define a feature for object deformation as (Pr, Pj ) = (dx, dy, dx2, dy2), where (dx, dy) represents the displacement of Uj relative to its anchor position that is determined by the position of its parent Pr. [sent-151, score-0.147]
</p><p>44 e edge: Teh cehreil are two types of collaborative edges in our model, representing the spatial cooccurrence between different leaf-nodes as well as between different and-nodes. [sent-178, score-0.195]
</p><p>45 For the collaborative edges between leaf-nodes, we introduce a 4-bin binary feature ψl (Pi, Pi? [sent-179, score-0.195]
</p><p>46 ) represents one of the relations: clockwise, anti-clockwise, near and far between two leafnodes Li and Li? [sent-182, score-0.177]
</p><p>47 Then we use  two bins to indicate either clockwise or anti-clockwise for the angle between the dashed line and the red line. [sent-190, score-0.144]
</p><p>48 We thus define the response of the collaborative edge between two leaf-nodes as, Γil,i? [sent-191, score-0.224]
</p><p>49 The response of the collaborative edge between two and-nodes is defined as, Γra,r? [sent-203, score-0.224]
</p><p>50 Inference Given an image, the task for inference is to localize all the multiclass objects with the model. [sent-212, score-0.271]
</p><p>51 It is a procedure integrating the local testing and binding testing as follows. [sent-232, score-0.154]
</p><p>52 Local testing: For a subgraph model rooted at Ar (i. [sent-233, score-0.128]
</p><p>53 ch(j)Ril(X,Pj) · Vi− Rjs(Pr,Pj)),  (7)  where Ril (X, Pj) represents the leaf-node response, and we can share these responses among different classes by calculating them at the beginning of inference. [sent-240, score-0.158]
</p><p>54 subgraph rooted at Ar: Sg(X, r, Pr) = mVax(Slr(X,P? [sent-278, score-0.128]
</p><p>55 We define a instance set INS = {(k, y)} indicating that yk = y for a∀( ikns, yta)n ∈e sIeNtS I NanSd = yk =k, y0) }oth inedriwciastien. [sent-309, score-0.398]
</p><p>56 y,y  rameters for each object group; (ii) combining models and learning collaborative edges. [sent-352, score-0.206]
</p><p>57 To reduce the computational cost for model sharing, we first divide the object classes into several groups as a datadriven initialization, and train the multiclass model for each group. [sent-353, score-0.493]
</p><p>58 3(d)); A leaf-node is shared as it can capture the similar appearances for other classes (Fig. [sent-359, score-0.239]
</p><p>59 During the detection, each Tik extracts a set of image patches from different samples, and we group these patches into a cluster Ωik. [sent-367, score-0.274]
</p><p>60 In each of the new sets, we describe the image patches with the HOG descriptor and group them into several clusters by using ISODATA algorithm with Euclidean distance. [sent-370, score-0.136]
</p><p>61 T T(jw, ok )c >as σ, sw jh aenrde 333333333755  The model structure after the first iteration; (b) A new leaf-node is created to recognize the head of sheep; (c) A leaf-node for sheep leg is  shared with the horse; (d) A leaf-node for horse leg is removed. [sent-374, score-0.337]
</p><p>62 (III) Based on the calculated M, we assign the classes that( are possibly s thhaerce dal cinultoat ethde M same group nS . [sent-376, score-0.161]
</p><p>63 Optimization Formulation Given an object group S, we train a multiclass model without collaborative edges, which is a procedure integrating structure reconfiguration and parameter estimation. [sent-384, score-0.696]
</p><p>64 a Abetl sth teh e b oebgjiencnitn cgla ossfe etrsa,i annidng y, w=e − −in1it liaableizles tthhee multiclass model with m = |S| and-nodes and one leafnmoudleti cfolars sea mcho doerl-n woditeh. [sent-395, score-0.271]
</p><p>65 The function (13) can be learned by applying structural SVM with latent variables,  mωin21? [sent-417, score-0.209]
</p><p>66 N=1[my,Hax(ω · φ(Xk,y,H) + L(yk,y)) − mHax(ω · φ(Xk , yk , H))] , (15) where C is a penalty parameter set as 0. [sent-420, score-0.199]
</p><p>67 005 empirically, and we define the loss function L(yk, y) = 0 when yk = y, aanndd Lw(ey dke , fiyn) = th 1e loifs yk y. [sent-421, score-0.441]
</p><p>68 We first find a hyperplane qt to upper bound −g(ω) in (17), − g(ω) ≤ −g(ωt) + (ω − ωt) · qt ,∀ω. [sent-438, score-0.184]
</p><p>69 (18) We calculate qt by finding the optimal latent variables H? [sent-439, score-0.199]
</p><p>70 model by structural reconfiguration and sharing, and it is p? [sent-448, score-0.269]
</p><p>71 i, we group the patches detected via it from all samples into a cluster Ωi, and the size of these patches is (hi , wi). [sent-454, score-0.274]
</p><p>72 After the clustering, the leaf-nodes are reconfigured as: If a cluster is newly generated, we create a new leaf-node accordingly; we remove a leaf-node if there are few image patches in the corresponding cluster. [sent-459, score-0.138]
</p><p>73 For a cluster Ωi, if there are images patches localized by Uj(in step(I)), we associate the leaf-node Li to Uj . [sent-460, score-0.138]
</p><p>74 Thus Li is shared by different classes for different associations. [sent-461, score-0.239]
</p><p>75 Recall that the HOG vector of an image patch is part of φ, and the patches in the same cluster are represented with the same bins in φ. [sent-463, score-0.186]
</p><p>76 , ,f aors e (aa)ch a sample ahfotwers clustering as φd(Xk , yk , H? [sent-486, score-0.199]
</p><p>77 Model Combination After training the multiclass models for each object group in {S1, . [sent-514, score-0.375]
</p><p>78 A toy example of feature adjustment according to structural clustering. [sent-525, score-0.151]
</p><p>79 (a)shows the feature vectors generated after Step (I), whose value is indicated by the intensities of bins; (b)shows the structural re-clustering: The feature ? [sent-527, score-0.151]
</p><p>80 For simplicity, we shorten the responses for leaf-node, or-node deformation and and-node as (k) = (X, Rjs(k) = and Ryak = Ryak (X, Given an image X, the objective function S(X) of multiclass recognition defined in Eq. [sent-538, score-0.316]
</p><p>81 =1 where the first two terms represent the local testing score, the next two represent the binding testing score, and the last one accounts for edge responses between and-nodes. [sent-567, score-0.199]
</p><p>82 For the training, we collect a set of images containing multiclass objects, each of which is labeled with Y = {y1, . [sent-568, score-0.271]
</p><p>83 (k) , Rjs (k) and Ryak as part of the input feature, and train the parameters β, αl and αa by standard structural SVM. [sent-577, score-0.204]
</p><p>84 The detection accuracy is calculated as [23]: only the detection with the highest score on the image is considered. [sent-621, score-0.126]
</p><p>85 The detectors for and-nodes and leafnodes are shown in Fig. [sent-628, score-0.255]
</p><p>86 Note that some of the leafnodes are shared for capturing similar appearances. [sent-630, score-0.303]
</p><p>87 The two detectors are generated when recognizing the images beside them. [sent-633, score-0.161]
</p><p>88 The results show that our model can generate alterable detectors to adapt diverse object appearances and poses. [sent-634, score-0.134]
</p><p>89 Evaluation for Model Sharing To analyze the effectiveness of sharing leaf-nodes, we disable the process for model sharing in training so that we obtain the simplified non-sharing And-Or graph model, named “Ours(sim)”. [sent-658, score-0.243]
</p><p>90 Conclusion This paper introduces a novel method for multiclass object detection and recognition, in the form of And-Or graph. [sent-674, score-0.39]
</p><p>91 (a) shows parts of the model with two classes (views), in which we visualize the detectors (in the form of HOG patterns) for the and-nodes and leaf-nodes based on the learned parameters, alone with the example images recognized by the model. [sent-680, score-0.191]
</p><p>92 (b) visualizes two detectors that are composed by 9 activated leaf-nodes. [sent-681, score-0.135]
</p><p>93 The two detectors are generated, respectively, when recognizing the images beside them. [sent-682, score-0.161]
</p><p>94 plane bicycle bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv Avg. [sent-684, score-0.264]
</p><p>95 Zisserman, The pascal visual object classes (voc) challenge, Int’l J. [sent-836, score-0.25]
</p><p>96 Joachims, Learning structural svms with latent variables, In ICML, 2009. [sent-862, score-0.209]
</p><p>97 Zisserman, Incremental learning of object detectors using a visual shape alphabet, In CVPR, 2006. [sent-883, score-0.134]
</p><p>98 Tenenbaum, Learning to share visual appearance for multiclass object detection, In CVPR, 2011. [sent-895, score-0.327]
</p><p>99 Freeman, Sharing visual features for multiclass and multiview object detection, IEEE TPAMI, 29(5):854-869, 2007. [sent-916, score-0.327]
</p><p>100 Freeman, Latent hierarchical structural learning for object detection, In CVPR, 2010. [sent-940, score-0.207]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('multiclass', 0.271), ('rjs', 0.235), ('yk', 0.199), ('leafnodes', 0.177), ('pr', 0.175), ('ril', 0.169), ('uj', 0.168), ('structural', 0.151), ('collaborative', 0.15), ('sheep', 0.137), ('shared', 0.126), ('reconfiguration', 0.118), ('ryak', 0.118), ('uiuc', 0.113), ('classes', 0.113), ('pi', 0.111), ('pk', 0.111), ('children', 0.102), ('activate', 0.101), ('tik', 0.097), ('sharing', 0.094), ('qt', 0.092), ('parent', 0.091), ('qtd', 0.088), ('patches', 0.088), ('compositions', 0.087), ('dynamical', 0.087), ('beside', 0.083), ('pascal', 0.081), ('js', 0.08), ('voc', 0.078), ('detectors', 0.078), ('response', 0.074), ('horse', 0.074), ('ar', 0.074), ('rra', 0.072), ('dso', 0.072), ('sim', 0.072), ('aps', 0.072), ('ch', 0.071), ('afterwards', 0.07), ('td', 0.066), ('rooted', 0.065), ('aofn', 0.065), ('vj', 0.064), ('subgraph', 0.063), ('detection', 0.063), ('binding', 0.06), ('andnode', 0.059), ('fuiml', 0.059), ('isodata', 0.059), ('mpajx', 0.059), ('rju', 0.059), ('latent', 0.058), ('pj', 0.057), ('activated', 0.057), ('object', 0.056), ('hog', 0.056), ('xk', 0.056), ('sg', 0.055), ('vr', 0.055), ('graph', 0.055), ('hierarchy', 0.055), ('people', 0.054), ('il', 0.054), ('train', 0.053), ('switch', 0.053), ('clockwise', 0.052), ('myax', 0.052), ('wik', 0.052), ('yuille', 0.051), ('cluster', 0.05), ('ck', 0.05), ('activation', 0.049), ('variables', 0.049), ('bins', 0.048), ('mumford', 0.048), ('sharable', 0.048), ('opelt', 0.048), ('group', 0.048), ('testing', 0.047), ('vi', 0.046), ('responses', 0.045), ('switching', 0.045), ('edges', 0.045), ('lin', 0.045), ('ra', 0.044), ('dashed', 0.044), ('zhu', 0.043), ('sra', 0.043), ('perturb', 0.043), ('acknowledged', 0.043), ('dke', 0.043), ('parsing', 0.043), ('hik', 0.042), ('zisserman', 0.042), ('indexed', 0.041), ('visualized', 0.041), ('taxonomies', 0.04), ('razavi', 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="221-tfidf-1" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan</p><p>Abstract: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the andnodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC2007and UIUCPeople, and it achieves state-of-the-arts performance.</p><p>2 0.13815819 <a title="221-tfidf-2" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>3 0.1137055 <a title="221-tfidf-3" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>Author: Fang Wang, Yi Li</p><p>Abstract: Simple tree models for articulated objects prevails in the last decade. However, it is also believed that these simple tree models are not capable of capturing large variations in many scenarios, such as human pose estimation. This paper attempts to address three questions: 1) are simple tree models sufficient? more specifically, 2) how to use tree models effectively in human pose estimation? and 3) how shall we use combined parts together with single parts efficiently? Assuming we have a set of single parts and combined parts, and the goal is to estimate a joint distribution of their locations. We surprisingly find that no latent variables are introduced in the Leeds Sport Dataset (LSP) during learning latent trees for deformable model, which aims at approximating the joint distributions of body part locations using minimal tree structure. This suggests one can straightforwardly use a mixed representation of single and combined parts to approximate their joint distribution in a simple tree model. As such, one only needs to build Visual Categories of the combined parts, and then perform inference on the learned latent tree. Our method outperformed the state of the art on the LSP, both in the scenarios when the training images are from the same dataset and from the PARSE dataset. Experiments on animal images from the VOC challenge further support our findings.</p><p>4 0.11152041 <a title="221-tfidf-4" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>Author: Xin Guo, Dong Liu, Brendan Jou, Mojun Zhu, Anni Cai, Shih-Fu Chang</p><p>Abstract: Object co-detection aims at simultaneous detection of objects of the same category from a pool of related images by exploiting consistent visual patterns present in candidate objects in the images. The related image set may contain a mixture of annotated objects and candidate objects generated by automatic detectors. Co-detection differs from the conventional object detection paradigm in which detection over each test image is determined one-by-one independently without taking advantage of common patterns in the data pool. In this paper, we propose a novel, robust approach to dramatically enhance co-detection by extracting a shared low-rank representation of the object instances in multiple feature spaces. The idea is analogous to that of the well-known Robust PCA [28], but has not been explored in object co-detection so far. The representation is based on a linear reconstruction over the entire data set and the low-rank approach enables effective removal of noisy and outlier samples. The extracted low-rank representation can be used to detect the target objects by spectral clustering. Extensive experiments over diverse benchmark datasets demonstrate consistent and significant performance gains of the proposed method over the state-of-the-art object codetection method and the generic object detection methods without co-detection formulations.</p><p>5 0.10907472 <a title="221-tfidf-5" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>6 0.10895496 <a title="221-tfidf-6" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>7 0.10701468 <a title="221-tfidf-7" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>8 0.10484955 <a title="221-tfidf-8" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>9 0.10153499 <a title="221-tfidf-9" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>10 0.10139334 <a title="221-tfidf-10" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>11 0.099972755 <a title="221-tfidf-11" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>12 0.098875813 <a title="221-tfidf-12" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>13 0.098583743 <a title="221-tfidf-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.096108235 <a title="221-tfidf-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.09462814 <a title="221-tfidf-15" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>16 0.092229582 <a title="221-tfidf-16" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>17 0.091783091 <a title="221-tfidf-17" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>18 0.091137156 <a title="221-tfidf-18" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>19 0.089282088 <a title="221-tfidf-19" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>20 0.085540585 <a title="221-tfidf-20" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, -0.073), (2, -0.005), (3, -0.027), (4, 0.08), (5, 0.038), (6, 0.051), (7, 0.068), (8, -0.028), (9, -0.037), (10, -0.061), (11, -0.009), (12, -0.023), (13, -0.087), (14, -0.0), (15, -0.014), (16, 0.013), (17, 0.029), (18, 0.02), (19, -0.021), (20, -0.037), (21, 0.025), (22, 0.054), (23, -0.05), (24, 0.059), (25, 0.026), (26, 0.01), (27, -0.024), (28, 0.001), (29, -0.001), (30, -0.014), (31, 0.019), (32, 0.028), (33, -0.001), (34, 0.04), (35, -0.007), (36, -0.007), (37, -0.012), (38, -0.01), (39, -0.064), (40, 0.016), (41, -0.037), (42, -0.032), (43, 0.006), (44, 0.028), (45, -0.022), (46, 0.039), (47, -0.004), (48, 0.021), (49, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96251988 <a title="221-lsi-1" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan</p><p>Abstract: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the andnodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC2007and UIUCPeople, and it achieves state-of-the-arts performance.</p><p>2 0.85714012 <a title="221-lsi-2" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>Author: Xi Song, Tianfu Wu, Yunde Jia, Song-Chun Zhu</p><p>Abstract: This paper presents a method of learning reconfigurable And-Or Tree (AOT) models discriminatively from weakly annotated data for object detection. To explore the appearance and geometry space of latent structures effectively, we first quantize the image lattice using an overcomplete set of shape primitives, and then organize them into a directed acyclic And-Or Graph (AOG) by exploiting their compositional relations. We allow overlaps between child nodes when combining them into a parent node, which is equivalent to introducing an appearance Or-node implicitly for the overlapped portion. The learning of an AOT model consists of three components: (i) Unsupervised sub-category learning (i.e., branches of an object Or-node) with the latent structures in AOG being integrated out. (ii) Weaklysupervised part configuration learning (i.e., seeking the globally optimal parse trees in AOG for each sub-category). To search the globally optimal parse tree in AOG efficiently, we propose a dynamic programming (DP) algorithm. (iii) Joint appearance and structural parameters training under latent structural SVM framework. In experiments, our method is tested on PASCAL VOC 2007 and 2010 detection , benchmarks of 20 object classes and outperforms comparable state-of-the-art methods.</p><p>3 0.83259982 <a title="221-lsi-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.8307184 <a title="221-lsi-4" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>Author: Jian Dong, Wei Xia, Qiang Chen, Jianshi Feng, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In this paper, we introduce a subcategory-aware object classification framework to boost category level object classification performance. Motivated by the observation of considerable intra-class diversities and inter-class ambiguities in many current object classification datasets, we explicitly split data into subcategories by ambiguity guided subcategory mining. We then train an individual model for each subcategory rather than attempt to represent an object category with a monolithic model. More specifically, we build the instance affinity graph by combining both intraclass similarity and inter-class ambiguity. Visual subcategories, which correspond to the dense subgraphs, are detected by the graph shift algorithm and seamlessly integrated into the state-of-the-art detection assisted classification framework. Finally the responses from subcategory models are aggregated by subcategory-aware kernel regression. The extensive experiments over the PASCAL VOC 2007 and PASCAL VOC 2010 databases show the state-ofthe-art performance from our framework.</p><p>5 0.82219642 <a title="221-lsi-5" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>Author: Qiang Chen, Zheng Song, Rogerio Feris, Ankur Datta, Liangliang Cao, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In recent years, efficiency of large-scale object detection has arisen as an important topic due to the exponential growth in the size of benchmark object detection datasets. Most current object detection methods focus on improving accuracy of large-scale object detection with efficiency being an afterthought. In this paper, we present the Efficient Maximum Appearance Search (EMAS) model which is an order of magnitude faster than the existing state-of-the-art large-scale object detection approaches, while maintaining comparable accuracy. Our EMAS model consists of representing an image as an ensemble of densely sampled feature points with the proposed Pointwise Fisher Vector encoding method, so that the learnt discriminative scoring function can be applied locally. Consequently, the object detection problem is transformed into searching an image sub-area for maximum local appearance probability, thereby making EMAS an order of magnitude faster than the traditional detection methods. In addition, the proposed model is also suitable for incorporating global context at a negligible extra computational cost. EMAS can also incorporate fusion of multiple features, which greatly improves its performance in detecting multiple object categories. Our experiments show that the proposed algorithm can perform detection of 1000 object classes in less than one minute per image on the Image Net ILSVRC2012 dataset and for 107 object classes in less than 5 seconds per image for the SUN09 dataset using a single CPU.</p><p>6 0.81016904 <a title="221-lsi-6" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>7 0.77030283 <a title="221-lsi-7" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>8 0.76933134 <a title="221-lsi-8" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>9 0.7569204 <a title="221-lsi-9" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>10 0.74708319 <a title="221-lsi-10" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>11 0.74701834 <a title="221-lsi-11" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>12 0.73747581 <a title="221-lsi-12" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>13 0.73729724 <a title="221-lsi-13" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>14 0.73671156 <a title="221-lsi-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.73495048 <a title="221-lsi-15" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>16 0.73319453 <a title="221-lsi-16" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>17 0.73172587 <a title="221-lsi-17" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>18 0.72579521 <a title="221-lsi-18" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>19 0.72335249 <a title="221-lsi-19" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>20 0.71380228 <a title="221-lsi-20" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.112), (16, 0.041), (26, 0.059), (28, 0.016), (33, 0.244), (39, 0.016), (52, 0.162), (67, 0.098), (69, 0.078), (77, 0.019), (80, 0.016), (87, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8947103 <a title="221-lda-1" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>same-paper 2 0.89378041 <a title="221-lda-2" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan</p><p>Abstract: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the andnodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC2007and UIUCPeople, and it achieves state-of-the-arts performance.</p><p>3 0.88591599 <a title="221-lda-3" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>4 0.88218391 <a title="221-lda-4" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>5 0.85855627 <a title="221-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.8555702 <a title="221-lda-6" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>7 0.85475624 <a title="221-lda-7" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>8 0.85318577 <a title="221-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.85297704 <a title="221-lda-9" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>10 0.85290915 <a title="221-lda-10" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>11 0.85281122 <a title="221-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.85232955 <a title="221-lda-12" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>13 0.85139495 <a title="221-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.8513028 <a title="221-lda-14" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>15 0.85071409 <a title="221-lda-15" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>16 0.85068136 <a title="221-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.85000861 <a title="221-lda-17" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>18 0.84948593 <a title="221-lda-18" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>19 0.84943312 <a title="221-lda-19" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>20 0.84871125 <a title="221-lda-20" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
