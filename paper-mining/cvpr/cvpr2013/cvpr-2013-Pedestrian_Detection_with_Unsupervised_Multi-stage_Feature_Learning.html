<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-328" href="#">cvpr2013-328</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</h1>
<br/><p>Source: <a title="cvpr-2013-328-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sermanet_Pedestrian_Detection_with_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>Reference: <a title="cvpr-2013-328-reference" href="../cvpr2013_reference/cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. [sent-4, score-0.751]
</p><p>2 The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage. [sent-5, score-0.712]
</p><p>3 All existing state-of-the-art methods use a combination of hand-crafted features such as Integral Channel Fea-  tures [9], HoG [5] and their variations [13, 33] and combinations [38], followed by a trainable classifier such as SVM [13, 28], boosted classifiers [9] or random forests [7]. [sent-9, score-0.139]
</p><p>4 While low-level features can be designed by hand with good success, mid-level features that combine low-level features are difficult to engineer without the help of some sort of learning procedure. [sent-10, score-0.211]
</p><p>5 Multi-stage recognizers that learn hierarchies of features tuned to the task at hand can be trained end-to-end with little prior knowledge. [sent-11, score-0.166]
</p><p>6 Convolutional Networks (ConvNets) [23] are examples of such hierarchical systems with end-to-end feature learning that are trained in a supervised fashion. [sent-12, score-0.279]
</p><p>7 ×  Figure 1: 128 9 9 filters trained on grayscale INRIA images using Algorithm 1. [sent-14, score-0.164]
</p><p>8 The main contribution of this paper is to show that the ConvNet model, with a few important twists, consistently yields state of the art and competitive results on all major pedestrian detection benchmarks. [sent-18, score-0.396]
</p><p>9 The system uses unsupervised convolutional sparse auto-encoders to pre-train features at all levels from the relatively small INRIA dataset [5], and end-to-end supervised training to train the classifier and fine-tune the features in an integrated fashion. [sent-19, score-0.742]
</p><p>10 Additionally, multi-stage features with layer-skipping connections enable output stages to combine global shape detectors with local motif detectors. [sent-20, score-0.193]
</p><p>11 Processing speed in pedestrian detection has recently seen great progress, enabling real-time operation without sacrificing quality. [sent-21, score-0.326]
</p><p>12 While processing speed is not the focus of this paper, features and classifier approximations introducedby [8] and [3] may be applicable to deep learning models for faster detection, in addition to GPU optimizations. [sent-23, score-0.203]
</p><p>13 Learning Feature Hierarchies Much of the work on pedestrian detection have focused on designing representative and powerful features [5, 9, 8, 38]. [sent-25, score-0.383]
</p><p>14 In this work, we show that generic feature learning algorithms can produce successful feature extractors that can achieve state-of-the-art results. [sent-26, score-0.173]
</p><p>15 Supervised learning of end-to-end systems on images have been shown to work well when there is abundant labeled samples [23], including for detection tasks [37, 29, 15, 3 1, 14, 36]. [sent-27, score-0.145]
</p><p>16 In this case, one can resort to designing useful features by using domain knowledge, or an alternative way is to use unsupervised learning algorithms. [sent-29, score-0.253]
</p><p>17 Recently unsupervised learning algorithms have been demonstrated to produce good features for generic object recognition problems [24, 25, 18, 20]. [sent-30, score-0.253]
</p><p>18 In [16], it was shown that unsupervised learning can be used to train deep hierarchical models and the final representation achieved is actually useful for a variety of different tasks [32, 24, 4]. [sent-31, score-0.3]
</p><p>19 In this work, we also follow a similar approach and train a generic unsupervised model at each layer using the output representation from the layer before. [sent-32, score-0.594]
</p><p>20 This process is then followed by supervised updates to the whole hierarchical system using label information. [sent-33, score-0.157]
</p><p>21 ×  Figure 2: A subset of 7 7 second layer filters trained on grayscale INRIA images using Algorithm 2. [sent-34, score-0.364]
</p><p>22 Each row in the figure shows filters that connect to a common output feature map. [sent-35, score-0.126]
</p><p>23 Hierarchical Model  A hierarchical feature extraction system consists of multiple levels of feature extractors that perform the same filtering and non-linear transformation functions in successive layers. [sent-41, score-0.209]
</p><p>24 In this work we use sparse convolutional feature hierarchies as proposed in [20]. [sent-43, score-0.41]
</p><p>25 Each layer of the unsupervised model contains a convolutional sparse coding algorithm and a predictor function that can be used for fast inference. [sent-44, score-0.891]
</p><p>26 After the last layer a classifier is used to map the feature representations into class labels. [sent-45, score-0.279]
</p><p>27 Both the sparse coding dictionary and the pre-  dictor function do not contain any hard-coded parameter and are trained from the input data. [sent-46, score-0.286]
</p><p>28 Each layer is trained in an unsupervised manner using the representation from previous layer (or the input image for the initial layer) separately. [sent-48, score-0.622]
</p><p>29 After the whole multi-stage system is trained in a layer-wise fashion, the complete architecture followed by a classifier is fine-tuned using labeled data. [sent-49, score-0.224]
</p><p>30 The exact form of s(z) depends on the particular sparse coding algorithm that is used, here, we use the ? [sent-56, score-0.157]
</p><p>31 All sparse modeling algorithms that adopt the dictionary matrix D exploit this property and perform a coordinate descent like minimization process where each variable is updated in succession. [sent-69, score-0.133]
</p><p>32 Following [30] many authors have used sparse dictionary learning to represent images [27, 1, 19]. [sent-70, score-0.173]
</p><p>33 However, most of the sparse coding models use small image patches as input x to learn the dictionary D and then apply the resulting model to every overlapping patch location on the full image. [sent-71, score-0.22]
</p><p>34 The 333666222755  particular predictor function we use is similar to a single layer ConvNet of the following form:  ××  f(x; g, k, b) = ˜z = { z˜j }j=1. [sent-74, score-0.323]
</p><p>35 In this formulation x is a p p grayscale input image, k ∈ Rn×m×m is a set of 2D filters where each filter is kj ∈ Rm×m, g ∈ Rn and b ∈ Rn are vectors with n elements, the predictor output z˜ ∈ Rn×p−m+1 ×p−m+1 is a set of feature maps where each of z˜j is of size p −m + 1 p −m + 1. [sent-77, score-0.339]
</p><p>36 Considering this general predictor function, the final form of the convolutional unsupervised energy for grayscale inputs is as follows: ECPSD  EConvSC  EPred  = EConvSC + βEPred = ? [sent-78, score-0.586]
</p><p>37 The unsupervised learning procedure is a two step coordinate descent process. [sent-96, score-0.196]
</p><p>38 The inference procedure requires us to carry out the sparse coding problem solution. [sent-98, score-0.157]
</p><p>39 We apply the FISTA algorithm in the image domain adopting the convolutional formulation. [sent-101, score-0.255]
</p><p>40 For color images or other multi-modal feature representations, the input x is a set of feature maps indexed by iand the representation z is a set of feature maps indexed by j for each input map i. [sent-102, score-0.126]
</p><p>41 Thus, the predictor function in Algorithm 1is defined as:  ˜zj= gj× tanh⎝⎛i? [sent-105, score-0.123]
</p><p>42 1  (9)  For a fully connected layer, all the input features are connected to all the output features, however it is also common to use sparse connection maps to reduce the number of parameters. [sent-114, score-0.165]
</p><p>43 The online training algorithm for unsupervised training of a single layer is: Algorithm 1 Single layer unsupervised training. [sent-115, score-0.778]
</p><p>44 Non-Linear Transformations Once the unsupervised learning for a single stage is completed, the next stage is trained on the feature representation from the previous one. [sent-121, score-0.384]
</p><p>45 In order to obtain the feature representation for the next stage, we use the predictor function f(x) followed by non-linear transformations and pooling. [sent-122, score-0.21]
</p><p>46 Absolute Value Rectification is applied component-wise to the whole feature output from f(x) in order to avoid cancellation problems in contrast normalization and pooling steps. [sent-124, score-0.135]
</p><p>47 Once a single layer of the network is trained, the features for training a successive layer is extracted using the predictor function followed by non-linear transformations. [sent-135, score-0.694]
</p><p>48 Detailed procedure of training an N layer hierarchical model is explained in Algorithm 2. [sent-136, score-0.268]
</p><p>49 The first layer features can be easily displayed in the parameter space since the parameterspace and the input space is same, however visualizing the second and higher level features in the input space can only be possible when only 333666222866  Algorithm 2 Multi-layer unsupervised training. [sent-137, score-0.47]
</p><p>50 N}, ηi) Set: i = 1, X1 = x, lcn(x) using equations 10-1 1, ds(X, w, s) as the down-sampling operator using boxcar kernel of size w w and stride of size s in both directions. [sent-140, score-0.128]
</p><p>51 z˜  ˜ z = |l˜c z|n( z˜)  Xi+1 = ds( z˜, i= i+ 1 until i= N end function  wi,  si)  Figure 3: A multi-scale convolutional network. [sent-143, score-0.255]
</p><p>52 The bottom row in which the 1st stage output is branched, subsampled again and merged into the classifier input provides a multistage component to the classifier stage. [sent-145, score-0.237]
</p><p>53 However, since we use absolute value rectification and local contrast normalization operations mapping the second layer features onto input space is not possible. [sent-148, score-0.453]
</p><p>54 In Figure 2 we show a subset of 1664 second layer features in the parameter space. [sent-149, score-0.257]
</p><p>55 Supervised Training After the unsupervised learning of the hierarchical feature extraction system is completed using Algorithm 2, we append a classifier function, usually in the form of a linear logistic regression, and perform stochastic online training using labeled data. [sent-152, score-0.384]
</p><p>56 Multi-Stage Features ConvNets are usually organized in a strictly feedforward manner where one layer only takes the output of the previous layer as input. [sent-155, score-0.438]
</p><p>57 Contrary to [12], the output of the first stage is branched after the non-linear transformations and pooling/subsampling operations rather than before. [sent-159, score-0.127]
</p><p>58 On the Y channel, we use 32 7 7 features followed by absolute value rectification, contrast normalization and 3 3 subsampling. [sent-163, score-0.207]
</p><p>59 On the subsampled UV channels, we extract 6 5 5 features followed by absolute value rectification and contrast normalization, skipping the usual subsampling step since it was performed beforehand. [sent-164, score-0.314]
</p><p>60 The second layer feature extraction takes 38 feature maps and produces 68 output features using 2040 9 9 features. [sent-166, score-0.379]
</p><p>61 A randomly selected 20% of the connections in mapping from input features to output features is removed to limit the computational requirements and break the symmetry [23]. [sent-167, score-0.195]
</p><p>62 The output of the second layer features are then transformed using absolute value rectification and contrast normalization followed by 2 2 subsampling. [sent-168, score-0.536]
</p><p>63 Greatest improvements are obtained for pedestrian detection and traffic-sign classification while only minimal gains are obtained for house numbers classification, a less complex task. [sent-171, score-0.419]
</p><p>64 Bootstrapping  Bootstrapping is typically used in detection settings by multiple phases of extracting the most offending negative answers and adding these samples to the existing dataset while training. [sent-174, score-0.169]
</p><p>65 For this purpose, we extract 3000 negative samples per bootstrapping pass and limit the number  of most offending answers to 5 for each image. [sent-175, score-0.218]
</p><p>66 We perform 3 bootstrapping passes in addition to the original training phase (i. [sent-176, score-0.161]
</p><p>67 21%%% Table 1:  Error rates improvements of multi-stage features over single-stage features for different types of objects detection and classification. [sent-192, score-0.199]
</p><p>68 Improvements are significant for multi-scale and textured objects such as traffic signs and pedestrians but minimal for house  numbers. [sent-193, score-0.133]
</p><p>69 modification is to avoid false positives that are due to pedestrian body parts. [sent-194, score-0.412]
</p><p>70 Instead of changing the criteria, we actively modify our training set before each bootstrapping phase. [sent-196, score-0.161]
</p><p>71 We include body part images that cause false positive detection into our bootstrapping image set. [sent-197, score-0.237]
</p><p>72 Experiments We evaluate our system on 5 standard pedestrian detection datasets. [sent-200, score-0.367]
</p><p>73 We also demonstrate improvements brought by unsupervised training and multistage features. [sent-202, score-0.278]
</p><p>74 In the following we name our model ConvNet with variants of unsupervised (Convnet-U) and fullysupervised training (Convnet-F) and multi-stage features (Convnet-U-MS and ConvNet-F-MS). [sent-203, score-0.301]
</p><p>75 Data Preparation The ConvNet is trained on the INRIA pedestrian dataset [5]. [sent-206, score-0.347]
</p><p>76 Each pedestrian image is mirrored along the horizontal axis to expand the dataset. [sent-212, score-0.281]
</p><p>77 Note that the unsupervised training phase is performed on this initial data before the bootstrapping phase. [sent-220, score-0.317]
</p><p>78 Evaluation Protocol During testing and bootstrapping phases using the INRIA dataset, the images are both up-sampled and subsampled. [sent-223, score-0.162]
</p><p>79 To ensure a fair comparison, we separated systems trained on INRIA (the majority) from systems trained on TUD-MotionPairs and the only system trained on Caltech in table 2. [sent-255, score-0.359]
</p><p>80 For clarity, only systems trained on INRIA were represented in Figure 5, however all results for all systems are still reported in table 2. [sent-256, score-0.186]
</p><p>81 miss rate versus false positives per image (FPPI), on the fixed INRIA dataset and rank algorithms along two measures: the error rate at 1 FPPI and the area under curve (AUC) rate in the [0, 1] FPPI range. [sent-261, score-0.169]
</p><p>82 html#inria 333666223088  false positives per image  false positives per image Figure 4: DET curves on the fixed-INRIA dataset for large pedestrians measure report false positives per image (FPPI) against miss rate. [sent-268, score-0.511]
</p><p>83 vidual contributions  of unsupervised  learning  (ConvNet-  U) and multi-stage features learning (ConvNet-F-MS) and their combination (ConvNet-U-MS) compared to the fullysupervised system without multi-stage  features (ConvNet-  F). [sent-271, score-0.446]
</p><p>84 1% error rate, unsupervised  learning exhibits  the most improvements compared to the baseline ConvNetF (23. [sent-273, score-0.236]
</p><p>85 Multi-stage  features  without  unsupervised  learning reach 17. [sent-275, score-0.253]
</p><p>86 Extensive results comparison of all major pedestrian datasets and published systems is provided in Table 2. [sent-278, score-0.341]
</p><p>87 We suspect the ConvNet with multi-stage features trained at high-resolution is more sensitive to resolution loss than other methods. [sent-283, score-0.123]
</p><p>88 Discussion We have introduced a new feature learning model with an application to pedestrian detection. [sent-286, score-0.363]
</p><p>89 We used the method of [20] as a baseline, and extended it by combining high and low resolution features in the model, and by learning features on the color channels of the input. [sent-288, score-0.154]
</p><p>90 The resulting model provides state of the art or competitive results on most measures of all publicly available datasets. [sent-290, score-0.125]
</p><p>91 Small-scale pedestrian measures can be improved in future work by training multiple scale models relying less on highresolution details. [sent-291, score-0.369]
</p><p>92 While computational speed was not the focus and hence was not reported here, our model was successfully used with near real-time speed in a haptic belt system [22] using parallel hardware. [sent-292, score-0.145]
</p><p>93 The ConvNet system yields state-of-the-art or competitive results on most datasets and measures, except for the low resolutions measures on the Caltech dataset because of higher reliance on high-resolution cues than other methods. [sent-325, score-0.166]
</p><p>94 DET curves plot false positives per image (FPPI) against miss rate. [sent-466, score-0.212]
</p><p>95 We report the multiple measures introduced by [10] for all major pedestrian datasets. [sent-469, score-0.336]
</p><p>96 Convolutional face finder: A neural architecture for fast and robust face detection. [sent-558, score-0.124]
</p><p>97 Fast inference in sparse coding algorithms with applications to object recognition. [sent-587, score-0.157]
</p><p>98 Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. [sent-641, score-0.344]
</p><p>99 Convolutional neural networks applied to house numbers digit classification. [sent-708, score-0.186]
</p><p>100 New fea-  tures and insights for pedestrian detection. [sent-734, score-0.281]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('convnet', 0.468), ('pedestrian', 0.281), ('convolutional', 0.255), ('inria', 0.245), ('auc', 0.215), ('layer', 0.2), ('unsupervised', 0.156), ('bootstrapping', 0.128), ('predictor', 0.123), ('daimler', 0.098), ('kavukcuoglu', 0.091), ('rectification', 0.091), ('neural', 0.089), ('fppi', 0.088), ('coding', 0.087), ('econvsc', 0.083), ('caltech', 0.08), ('pedestrians', 0.08), ('fista', 0.073), ('convnets', 0.073), ('sermanet', 0.073), ('stride', 0.073), ('competitive', 0.07), ('sparse', 0.07), ('deep', 0.069), ('doll', 0.068), ('det', 0.067), ('ranzato', 0.067), ('positives', 0.067), ('trained', 0.066), ('false', 0.064), ('dictionary', 0.063), ('wp', 0.062), ('systems', 0.06), ('ki', 0.059), ('features', 0.057), ('belt', 0.055), ('boxcar', 0.055), ('chintala', 0.055), ('epred', 0.055), ('flesiraxe', 0.055), ('fullysupervised', 0.055), ('ltdes', 0.055), ('motif', 0.055), ('offending', 0.055), ('unsup', 0.055), ('measures', 0.055), ('normalization', 0.055), ('house', 0.053), ('zj', 0.053), ('grayscale', 0.052), ('uv', 0.05), ('absolute', 0.05), ('branched', 0.049), ('multistage', 0.049), ('extractors', 0.049), ('haptic', 0.049), ('twists', 0.049), ('wrt', 0.049), ('filters', 0.046), ('followed', 0.045), ('detection', 0.045), ('courant', 0.045), ('networks', 0.044), ('connections', 0.043), ('editors', 0.043), ('lecun', 0.043), ('hierarchies', 0.043), ('tanh', 0.043), ('plot', 0.043), ('feature', 0.042), ('system', 0.041), ('gi', 0.041), ('chnftrs', 0.041), ('crosstalk', 0.041), ('learning', 0.04), ('improvements', 0.04), ('stage', 0.04), ('stacked', 0.04), ('output', 0.038), ('bi', 0.038), ('miss', 0.038), ('kj', 0.038), ('classifier', 0.037), ('boxes', 0.037), ('network', 0.036), ('subsampled', 0.036), ('supervised', 0.036), ('answers', 0.035), ('dz', 0.035), ('subsampling', 0.035), ('conference', 0.035), ('hierarchical', 0.035), ('architecture', 0.035), ('rni', 0.034), ('reasonable', 0.034), ('nyu', 0.034), ('phases', 0.034), ('gj', 0.034), ('training', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="328-tfidf-1" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>2 0.25121817 <a title="328-tfidf-2" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>3 0.23635291 <a title="328-tfidf-3" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>Author: Junjie Yan, Xucong Zhang, Zhen Lei, Shengcai Liao, Stan Z. Li</p><p>Abstract: The serious performance decline with decreasing resolution is the major bottleneck for current pedestrian detection techniques [14, 23]. In this paper, we take pedestrian detection in different resolutions as different but related problems, and propose a Multi-Task model to jointly consider their commonness and differences. The model contains resolution aware transformations to map pedestrians in different resolutions to a common space, where a shared detector is constructed to distinguish pedestrians from background. For model learning, we present a coordinate descent procedure to learn the resolution aware transformations and deformable part model (DPM) based detector iteratively. In traffic scenes, there are many false positives located around vehicles, therefore, we further build a context model to suppress them according to the pedestrian-vehicle relationship. The context model can be learned automatically even when the vehicle annotations are not available. Our method reduces the mean miss rate to 60% for pedestrians taller than 30 pixels on the Caltech Pedestrian Benchmark, which noticeably outperforms previous state-of-the-art (71%).</p><p>4 0.23525368 <a title="328-tfidf-4" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>5 0.2241556 <a title="328-tfidf-5" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>Author: Rodrigo Benenson, Markus Mathias, Tinne Tuytelaars, Luc Van_Gool</p><p>Abstract: The current state of the art solutions for object detection describe each class by a set of models trained on discovered sub-classes (so called “components ”), with each model itself composed of collections of interrelated parts (deformable models). These detectors build upon the now classic Histogram of Oriented Gradients+linear SVM combo. In this paper we revisit some of the core assumptions in HOG+SVM and show that by properly designing the feature pooling, feature selection, preprocessing, and training methods, it is possible to reach top quality, at least for pedestrian detections, using a single rigid component. We provide experiments for a large design space, that give insights into the design of classifiers, as well as relevant information for practitioners. Our best detector is fully feed-forward, has a single unified architecture, uses only histograms of oriented gradients and colour information in monocular static images, and improves over 23 other methods on the INRIA, ETHand Caltech-USA datasets, reducing the average miss-rate over HOG+SVM by more than 30%.</p><p>6 0.193424 <a title="328-tfidf-6" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>7 0.18929546 <a title="328-tfidf-7" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>8 0.16835059 <a title="328-tfidf-8" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>9 0.16659339 <a title="328-tfidf-9" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>10 0.1591661 <a title="328-tfidf-10" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>11 0.15393814 <a title="328-tfidf-11" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>12 0.1400113 <a title="328-tfidf-12" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>13 0.12219539 <a title="328-tfidf-13" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>14 0.12187497 <a title="328-tfidf-14" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>15 0.11487284 <a title="328-tfidf-15" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>16 0.11304732 <a title="328-tfidf-16" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>17 0.1098374 <a title="328-tfidf-17" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>18 0.10241737 <a title="328-tfidf-18" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>19 0.10106079 <a title="328-tfidf-19" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>20 0.099722579 <a title="328-tfidf-20" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.232), (1, -0.077), (2, -0.064), (3, 0.062), (4, 0.038), (5, 0.006), (6, 0.113), (7, 0.039), (8, 0.019), (9, -0.038), (10, -0.104), (11, -0.1), (12, 0.137), (13, -0.231), (14, 0.094), (15, 0.089), (16, -0.184), (17, 0.078), (18, 0.1), (19, 0.06), (20, -0.022), (21, -0.08), (22, -0.123), (23, 0.028), (24, -0.095), (25, 0.062), (26, 0.017), (27, 0.078), (28, 0.027), (29, 0.088), (30, 0.008), (31, -0.014), (32, -0.004), (33, -0.017), (34, -0.056), (35, 0.048), (36, -0.031), (37, 0.067), (38, 0.061), (39, -0.008), (40, -0.032), (41, -0.059), (42, 0.077), (43, -0.003), (44, -0.041), (45, 0.012), (46, -0.013), (47, 0.031), (48, -0.001), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92748553 <a title="328-lsi-1" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>2 0.80056715 <a title="328-lsi-2" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xingyu Zeng, Xiaogang Wang</p><p>Abstract: Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. The difficulty is added when several pedestrians overlap in images and occlude each other. We observe, however, that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the CaltechTrain dataset, the Caltech-Test dataset and the ETHdataset. Including mutual visibility leads to 4% −8% improvements on mluudlitnipglem ubteunaclh vmiasibrki ditayta lesaedtss.</p><p>3 0.75659436 <a title="328-lsi-3" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>Author: Junjie Yan, Xucong Zhang, Zhen Lei, Shengcai Liao, Stan Z. Li</p><p>Abstract: The serious performance decline with decreasing resolution is the major bottleneck for current pedestrian detection techniques [14, 23]. In this paper, we take pedestrian detection in different resolutions as different but related problems, and propose a Multi-Task model to jointly consider their commonness and differences. The model contains resolution aware transformations to map pedestrians in different resolutions to a common space, where a shared detector is constructed to distinguish pedestrians from background. For model learning, we present a coordinate descent procedure to learn the resolution aware transformations and deformable part model (DPM) based detector iteratively. In traffic scenes, there are many false positives located around vehicles, therefore, we further build a context model to suppress them according to the pedestrian-vehicle relationship. The context model can be learned automatically even when the vehicle annotations are not available. Our method reduces the mean miss rate to 60% for pedestrians taller than 30 pixels on the Caltech Pedestrian Benchmark, which noticeably outperforms previous state-of-the-art (71%).</p><p>4 0.74525219 <a title="328-lsi-4" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>5 0.74174434 <a title="328-lsi-5" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>Author: Rodrigo Benenson, Markus Mathias, Tinne Tuytelaars, Luc Van_Gool</p><p>Abstract: The current state of the art solutions for object detection describe each class by a set of models trained on discovered sub-classes (so called “components ”), with each model itself composed of collections of interrelated parts (deformable models). These detectors build upon the now classic Histogram of Oriented Gradients+linear SVM combo. In this paper we revisit some of the core assumptions in HOG+SVM and show that by properly designing the feature pooling, feature selection, preprocessing, and training methods, it is possible to reach top quality, at least for pedestrian detections, using a single rigid component. We provide experiments for a large design space, that give insights into the design of classifiers, as well as relevant information for practitioners. Our best detector is fully feed-forward, has a single unified architecture, uses only histograms of oriented gradients and colour information in monocular static images, and improves over 23 other methods on the INRIA, ETHand Caltech-USA datasets, reducing the average miss-rate over HOG+SVM by more than 30%.</p><p>6 0.69766021 <a title="328-lsi-6" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>7 0.68864143 <a title="328-lsi-7" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>8 0.66553658 <a title="328-lsi-8" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>9 0.64946043 <a title="328-lsi-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.61010402 <a title="328-lsi-10" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>11 0.60966271 <a title="328-lsi-11" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>12 0.60541636 <a title="328-lsi-12" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>13 0.58562684 <a title="328-lsi-13" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>14 0.56139666 <a title="328-lsi-14" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>15 0.5557245 <a title="328-lsi-15" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>16 0.53564185 <a title="328-lsi-16" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>17 0.53447741 <a title="328-lsi-17" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>18 0.52710277 <a title="328-lsi-18" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>19 0.51854914 <a title="328-lsi-19" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>20 0.51632303 <a title="328-lsi-20" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.089), (16, 0.025), (26, 0.036), (28, 0.265), (33, 0.261), (67, 0.098), (69, 0.049), (80, 0.018), (87, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86080903 <a title="328-lda-1" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>2 0.85319126 <a title="328-lda-2" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>3 0.84989834 <a title="328-lda-3" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>4 0.84432584 <a title="328-lda-4" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>Author: Austin Abrams, Kylia Miskell, Robert Pless</p><p>Abstract: Shadows encode a powerful geometric cue: if one pixel casts a shadow onto another, then the two pixels are colinear with the lighting direction. Given many images over many lighting directions, this constraint can be leveraged to recover the depth of a scene from a single viewpoint. For outdoor scenes with solar illumination, we term this the episolar constraint, which provides a convex optimization to solve for the sparse depth of a scene from shadow correspondences, a method to reduce the search space when finding shadow correspondences, and a method to geometrically calibrate a camera using shadow constraints. Our method constructs a dense network of nonlocal constraints which complements recent work on outdoor photometric stereo and cloud based cues for 3D. We demonstrate results across a variety of time-lapse sequences from webcams “in . wu st l. edu (b)(c) the wild.”</p><p>5 0.8354423 <a title="328-lda-5" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>Author: Minh Hoai, Andrew Zisserman</p><p>Abstract: The objective of this work is to learn sub-categories. Rather than casting this as a problem of unsupervised clustering, we investigate a weakly supervised approach using both positive and negative samples of the category. We make the following contributions: (i) we introduce a new model for discriminative sub-categorization which determines cluster membership for positive samples whilst simultaneously learning a max-margin classifier to separate each cluster from the negative samples; (ii) we show that this model does not suffer from the degenerate cluster problem that afflicts several competing methods (e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discover interpretable sub-categories in various datasets. The model is evaluated experimentally over various datasets, and itsperformance advantages over k-means and Latent SVM are demonstrated. We also stress test the model and show its resilience in discovering sub-categories as the parameters are varied.</p><p>same-paper 6 0.83227241 <a title="328-lda-6" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>7 0.79656714 <a title="328-lda-7" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>8 0.79237664 <a title="328-lda-8" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>9 0.77423543 <a title="328-lda-9" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>10 0.76287752 <a title="328-lda-10" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>11 0.76149875 <a title="328-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.76004243 <a title="328-lda-12" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>13 0.75866133 <a title="328-lda-13" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>14 0.75859779 <a title="328-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.75728792 <a title="328-lda-15" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>16 0.75575894 <a title="328-lda-16" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>17 0.75371194 <a title="328-lda-17" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>18 0.75327039 <a title="328-lda-18" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>19 0.75188076 <a title="328-lda-19" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>20 0.75169343 <a title="328-lda-20" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
