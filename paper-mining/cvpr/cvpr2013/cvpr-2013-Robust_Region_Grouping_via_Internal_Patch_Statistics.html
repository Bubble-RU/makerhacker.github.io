<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-366" href="#">cvpr2013-366</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</h1>
<br/><p>Source: <a title="cvpr-2013-366-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Liu_Robust_Region_Grouping_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xiaobai Liu, Liang Lin, Alan L. Yuille</p><p>Abstract: In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.</p><p>Reference: <a title="cvpr-2013-366-reference" href="../cvpr2013_reference/cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. [sent-7, score-0.255]
</p><p>2 Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. [sent-8, score-0.566]
</p><p>3 First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. [sent-10, score-0.187]
</p><p>4 We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. [sent-11, score-0.742]
</p><p>5 Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. [sent-12, score-0.341]
</p><p>6 Introduction Image segmentation is to partition input image into several semantically consistent regions. [sent-16, score-0.124]
</p><p>7 Therefore, the quality of segmentation heavily depends on how well the extra images match with the given image [17]. [sent-22, score-0.119]
</p><p>8 (a) two input images overlaid with superpixel oversegmentation results; (b) repeatedly occurred patches (identified with the same color); (c) segmentation results by our algorithm(unsupevised). [sent-77, score-0.378]
</p><p>9 In this work, we introduce a simple yet efficient internal image statistics for image segmentation, and integrate it within a unified low-rank image representation. [sent-79, score-0.258]
</p><p>10 For each scale of the input image, we partition it into a set of non-overlapping superpixels [16], and construct an affinity graph by taking superpixels as graph vertices. [sent-81, score-0.546]
</p><p>11 As conventionally, each superpixel is represented as one appropriate appearance feature, e. [sent-82, score-0.247]
</p><p>12 Given these superpixel features, we assume that intra-class superpixels, namely the superpixels belonging to the same semantic region, are drawn from one identical low-rank feature subspace. [sent-85, score-0.519]
</p><p>13 Our goal is to seek for a low-rank refined superpixel affinity matrix, so that: the intra-class superpixel affinities are dense, whereas the inter-class superpixel affinities are all sparse or zeros. [sent-86, score-1.205]
</p><p>14 The inference of low-rank refined affinity matrices is guided by two more purposes. [sent-88, score-0.315]
</p><p>15 We can intuitively find that a small-size patch usually has multiple copies (identified with the same color) in the same semantic region (shown in the right column). [sent-95, score-0.21]
</p><p>16 The replication prior can be used to measure how likely two subregions 1 are semantically similar. [sent-96, score-0.584]
</p><p>17 Generally, if every patch within one subregion has many copies in another one, namely these two subregions have high replication prior, they will belong to the same semantic region with high probability, and vice versa. [sent-97, score-0.845]
</p><p>18 In later sections, we will further quantitatively justify that the above observation on fairly small-size patches is partially true in real image databases. [sent-98, score-0.136]
</p><p>19 The second purpose lies on the fact that the desired superpixel-pair affinities at different scales should be consistently solved, which leads to the so-called cross-scale consistency constraint. [sent-103, score-0.218]
</p><p>20 We formulate the pursuit of low-  rank refined affinity matrices and the above two purposes as a unified constraint nuclear norm and ? [sent-104, score-0.408]
</p><p>21 Taking the solved low-ranked refined superpixel affinities, one can call the Normalized Cut method [6] to address the unsupervised segmentation problem. [sent-107, score-0.455]
</p><p>22 First, we develop a multi-scale low-rank representation to seek for the affinity matrix at each scale in parallel, while preserving the cross-scale consistency. [sent-109, score-0.206]
</p><p>23 Second, we study a simple yet efficient internal image statistics, and present a practical method for image segmentation. [sent-110, score-0.147]
</p><p>24 The advantages of our approach are demonstrated by extensive experiments with comparisons to the popular segmentation algorithms on two public datasets MSRC [19] and BSD500 [15]. [sent-111, score-0.133]
</p><p>25 All these three algorithms directly utilize the low-  1A subregion indicates a part of the whole image, either a semanticless superpixel or a semantic region (e. [sent-116, score-0.483]
</p><p>26 As aforementioned, extra knowledge or statistics have been studied to address the ill-posed nature of image segmentation [13] [17]. [sent-122, score-0.193]
</p><p>27 In contrast, the proposed replication prior is a kind of internal image statistics, which bears the obvious benefit of low computational demand. [sent-124, score-0.631]
</p><p>28 Moreover, we will experimentally show that, to achieve the equally good quality of segmentation by the low-rank refined replication prior, hundreds of images are required for the external statistics [13]. [sent-125, score-0.735]
</p><p>29 In computer vision literature, internal image statistics has been used in various low-level image tasks, e. [sent-126, score-0.194]
</p><p>30 [4] assume that one semantic region can be well explained by repeatable compositions and utilized this assumption for interactive image segmentation which requires user input. [sent-130, score-0.176]
</p><p>31 Recently, Zontak and Irani [23] further quantitatively evaluate the strength of internal statistics, and demonstrate its advantages in enhancing the quality of image denoising and super-resolution. [sent-131, score-0.12]
</p><p>32 Our work extends these  methods, and introduces a practical method to apply the internal image statistic for image segmentation. [sent-132, score-0.12]
</p><p>33 Each superpixel comprises an ensemble of pixels that are spatially coherent and perceptually similar with respect to certain appearance fea-  111999333200  tures (e. [sent-145, score-0.247]
</p><p>34 We construct an affinity graph by taking the superpixels in Is as graph vertices. [sent-151, score-0.376]
</p><p>35 Thus, the overall quality of segmentation depends on the pairwise superpixel affinity matrix. [sent-153, score-0.531]
</p><p>36 Let xis ∈ Rd denote the d-dimension feature descriptor extracted fo∈r Rthe ith superpixel in Is. [sent-154, score-0.329]
</p><p>37 One common method to compute the superpixel-pair affinity [6] is exp(−? [sent-156, score-0.206]
</p><p>38 ve irnthdeiclaetsess, tbheeca Fruoseb onfi uvsa nrioorums image noises and clutters, the obtained affinity matrix is always corrupted and not discriminative enough to produce high-quality segmentations. [sent-164, score-0.291]
</p><p>39 We will introduce in the rest of this section a novel formulation to infer more powerful superpixel-pair affinities for the input image, and further discuss how to apply this representation to segmentation problems in next section. [sent-165, score-0.174]
</p><p>40 Objective-I: Low-rank Image Representation The major step of our method is to pursue the low-rank refined affinity matrix from the low-level superpixel features. [sent-168, score-0.519]
</p><p>41 Herein, we assume that superpixels belonging to the same semantic region are all drawn from the same low-  rank feature subspace, and all superpixels in the same image (also the same scale) lying on a union of multiple subspaces [14]. [sent-169, score-0.494]
</p><p>42 We aim to represent each superpixel descriptor as a linear combination of other superpixel descriptors, and seek for the lowest rank representation of all superpixels in a joint fashion. [sent-170, score-0.693]
</p><p>43 Let denote the number of superpixels in Is, and Xs = [xs1 , x2s , . [sent-171, score-0.17]
</p><p>44 Each vector xis can be represented as the linear] ]c ∈om Rbination of the column vectors of Xs, denoted as,  ns  xis = Xszis,  (1)  where zis ∈ Rns is the coefficient vector. [sent-175, score-0.25]
</p><p>45 Large zisj generally indica∈tes R that xis and xjs have similar projection in the feature subspaces spanned by the column vectors of Xs, and vice versa. [sent-176, score-0.181]
</p><p>46 Thus, we can use zisj to measure the affinity between superpixels iand j. [sent-177, score-0.42]
</p><p>47 The lowest rank representation of the superpixel affinity matrix Zs can be solved by following program  ×  ? [sent-185, score-0.482]
</p><p>48 Objective-II: Replication Prior The discovered replication prior is from a statistical observation on natural images: local small-size patches (e. [sent-209, score-0.598]
</p><p>49 6 6 pixels) tend to recur frequently within the same se6m ×ant 6ic region, yet lteos sre frequently nwtlityhi wni semantically different regions. [sent-211, score-0.121]
</p><p>50 The size of image patches is fairly small so one superpixel may contain multiple patches. [sent-215, score-0.332]
</p><p>51 Replication prior can be used to measure the semantic consistency of two superpixels. [sent-216, score-0.153]
</p><p>52 We use patch recurrence density to quantify the replication prior. [sent-217, score-0.596]
</p><p>53 Let Λ denote a subregion and q index the patches in Λ. [sent-219, score-0.13]
</p><p>54 We perform an experiment on the Berkeley Segmentation Database [15] to justify the replication prior. [sent-227, score-0.517]
</p><p>55 The intra-class density of patch p is calculated using Eq. [sent-232, score-0.13]
</p><p>56 (4) where Λ is set to be the semantic region in groundtruth that contains the patch p. [sent-233, score-0.182]
</p><p>57 Correspondingly, the inter-class density of patch p is estimated with respect to the semantic region that does not contain patch p. [sent-234, score-0.312]
</p><p>58 We compare the mean intra-class patch densities and mean inter-class patch densities under different patch sizes, including 6  ddeiftfaeilrse. [sent-492, score-0.35]
</p><p>59 See texts for more  more than one semantically different regions for patch p, we select the highest one (or the most ambiguous one) as the inter-class density of patch p . [sent-495, score-0.26]
</p><p>60 Figures 2 plots the density comparisons while looking at different patch-sizes, including 6 6, 10 10, 12 12 and 14 14 pixels, respectively. [sent-497, score-0.129]
</p><p>61 This experiment shows that the discovered replication prior is partially true while using fairly small-size patches. [sent-502, score-0.577]
</p><p>62 We utilize the replication prior to measure how likely two superpixels belong to the same semantic region. [sent-503, score-0.777]
</p><p>63 Let Λis denote the subregion in Is covered by superpixel i, Qs denote a matrix. [sent-504, score-0.324]
</p><p>64 (5)  Large Qisj indicates the associated suerpixel-pair has low replication prior, namely the superpixels iand j belong to different semantic regions with high probability, and vice versa. [sent-508, score-0.796]
</p><p>65 In this way, replication prior is used as a kind of soft constraint to regularize the inference of the lowest rank affinity matrices from the low-level visual features. [sent-519, score-0.789]
</p><p>66 (6) aims to compute the optimal superpixel affinity matrix for each scale separately. [sent-526, score-0.453]
</p><p>67 We can achieve this by projecting the superpixels at the coarse-level to the finelevel and introducing a cross-scale consistency constraint. [sent-530, score-0.213]
</p><p>68 Formally, let Iis indicate the superpixel at the scale s indexed by i. [sent-531, score-0.278]
</p><p>69 We impose a cross-scale constraint for every two neighbor scales (namely the coarse-scale s + 1and the fine-scale s): for every two superpixels at the coarse-level, their affinity should be locally average of the affinities between their respective children at the fine-level. [sent-533, score-0.521]
</p><p>70 2  (10)  Minimizing the above term will enforce the desired superpixel affinity matrices at different scales are consistent. [sent-564, score-0.575]
</p><p>71 Suppose the optimal superpixel affinity matrices are solved from Eq. [sent-650, score-0.496]
</p><p>72 We first project Zs∗ to the pixel-level as follows: if two pixels in Is belong to the same superpixel, we set their affinity to be 1; otherwise, we set their affinity to be the corresponding superpixel-pair affinity. [sent-654, score-0.412]
</p><p>73 We define the affinity between neighboring pixels at scale s using the linear combination of a set of Gaussian kernels:  =  ? [sent-655, score-0.206]
</p><p>74 Experiments In this section, we apply the discovered replication prior and the proposed multi-scale low-rank representation (MsLRR) for image segmentation and evaluate them on publicly available image databases. [sent-674, score-0.623]
</p><p>75 5, and over-segmented into superpixels using the method in [16]. [sent-685, score-0.17]
</p><p>76 The number of superpixels is set to be 80, 120 and 150, respectively. [sent-686, score-0.17]
</p><p>77 We extract for each superpixel three types of feature descriptors, including 12-dimension color histogram in each channel of RGB, 59-dimension Local Binary Pattern and 31-dimension Histogram of Oriented Gradient [19]. [sent-687, score-0.247]
</p><p>78 We use two segmentation metrics, Covering Rate (CR), namely the percentage of per-pixel agreement between the obtained results w. [sent-692, score-0.115]
</p><p>79 It degenerates to seeking for the low-rank refined affinity matrices at multiple scales separately. [sent-701, score-0.364]
</p><p>80 3) MsLRR-III, that sets γ = 0, and only uses the regularization term of replication prior . [sent-703, score-0.511]
</p><p>81 Moreover, we compute the superpixel-pair affinities based on feature descriptors, namely letting Δirj = exp(−|xi xj |2/2σ2) in Eq. [sent-706, score-0.133]
</p><p>82 The threshold ζ for patch density feisxtiedma ttoio bne i 6s ×fix 6ed p itox e bles . [sent-731, score-0.13]
</p><p>83 Table 1-(a) reports the performance comparisons of various unsupervised segmentation algorithms on MSRC [19] and BSD [15] databases. [sent-744, score-0.166]
</p><p>84 These comparisons well justify the effectiveness of MsLRR and the internal replication prior. [sent-749, score-0.692]
</p><p>85 Performance comparisons of unsupervised segmentation algorithms on MSRC [19] and BSD500 [15] (a) Segment single image  MetricsCR (M%S)RCVICR (%B)SDVI  M TeCUaBNTCnEM S uh[ti-21fS[ 8t6][P76 20– . [sent-752, score-0.166]
</p><p>86 It takes about 20 seconds to process one image given superpixel features extracted offline. [sent-772, score-0.247]
</p><p>87 We show several exemplar comparisons of segmentation results on BSD [15] in Figures 3, where  each row shows the original image in column 1 and corresponding segmentation results obtained by MsLRR-IV, MeanShift [7] and MNCut [6] in columns 2, 3 and 4, respectively. [sent-773, score-0.211]
</p><p>88 Exp-II: Internal Replication Prior and External Image Statistics We further evaluate the effectiveness of the discovered replication prior by comparing it with the external image statistics. [sent-777, score-0.596]
</p><p>89 As aforementioned, there are several external statistics based methods, and we choose to implement the most recent one by Liu et al. [sent-778, score-0.125]
</p><p>90 They proposed to  BSD500 [15] extract superpixel co-occurrence frequencies from an extra unlabeled image corpus and utilize this knowledge for image segmentation. [sent-780, score-0.434]
</p><p>91 Our formulation in (11) can integrate this knowledge by re-defining the matrix Qs as Qisj = exp{−cij } where cij indicates the co-occurrence frequency eofx superpixels ie raen dc j. [sent-781, score-0.263]
</p><p>92 The details of calculating superpixel co-occurrence from unlabeled images are referred to the paper [13]. [sent-782, score-0.32]
</p><p>93 The training subsets are used as the unlabeled image corpus for extracting the superpixel co-occurrence, as in [13]. [sent-785, score-0.362]
</p><p>94 All images are oversegmented into superpixels [16], and we use the same superpixel descriptors as in Exp-I. [sent-788, score-0.417]
</p><p>95 From the comparisons in Table 1, one can observe that MsLRR-IV is able to achieve comparable accuracies as MsLRR-V that uses extra unlabeled images. [sent-790, score-0.169]
</p><p>96 Furthermore, in BSD500 database, MsLRR-V using 300 unlabeled images (in Table 1-b) is inferior to the same algorithm using 2300 unlabeled images (in Table 1-c). [sent-792, score-0.146]
</p><p>97 These comparisons partially demonstrate that, external image statistics requires fairly large-scale unlabeled image corpus to achieve the robustness, which is consistent with the claims in previous works [13] [23]. [sent-794, score-0.327]
</p><p>98 In contrast, the proposed replication prior is extracted from the image itself, and thus has less limitations in real applications. [sent-795, score-0.511]
</p><p>99 Summary  In this work, we studied a simple yet efficient internal image statistics and presented a practical method, the multiscale low-rank representation (MsLRR), for image segmentation. [sent-797, score-0.221]
</p><p>100 MsLRR aims to infer the low-rank refined superpixel affinity matrices at different scales of the input image in parallel, and meanwhile, to impose the cross-scale constraint to make the desired affinity matrices consistent. [sent-798, score-0.89]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('replication', 0.466), ('zs', 0.407), ('superpixel', 0.247), ('js', 0.239), ('affinity', 0.206), ('xs', 0.179), ('superpixels', 0.17), ('mslrr', 0.153), ('internal', 0.12), ('es', 0.12), ('mncut', 0.109), ('xsps', 0.109), ('bsd', 0.097), ('affinities', 0.096), ('msrc', 0.093), ('xszs', 0.087), ('ns', 0.086), ('patch', 0.084), ('xis', 0.082), ('segmentation', 0.078), ('subregion', 0.077), ('hs', 0.077), ('statistics', 0.074), ('unlabeled', 0.073), ('ps', 0.071), ('qisj', 0.066), ('zms', 0.066), ('zstqs', 0.066), ('refined', 0.066), ('semantic', 0.065), ('cz', 0.062), ('isb', 0.058), ('ijs', 0.058), ('qs', 0.058), ('ias', 0.056), ('comparisons', 0.055), ('ncut', 0.054), ('nes', 0.054), ('irj', 0.054), ('patches', 0.053), ('external', 0.051), ('justify', 0.051), ('lrr', 0.051), ('scales', 0.049), ('densities', 0.049), ('recur', 0.048), ('ys', 0.048), ('corrupted', 0.047), ('alm', 0.046), ('ilj', 0.046), ('semantically', 0.046), ('density', 0.046), ('prior', 0.045), ('pstqs', 0.044), ('tbes', 0.044), ('wisj', 0.044), ('zisj', 0.044), ('bagon', 0.043), ('matrices', 0.043), ('consistency', 0.043), ('corpus', 0.042), ('extra', 0.041), ('vs', 0.041), ('iaj', 0.039), ('cs', 0.038), ('noises', 0.038), ('unified', 0.037), ('namely', 0.037), ('ucm', 0.036), ('eofx', 0.036), ('subspace', 0.035), ('discovered', 0.034), ('unsupervised', 0.033), ('region', 0.033), ('tr', 0.032), ('fairly', 0.032), ('tpami', 0.031), ('wright', 0.031), ('utilize', 0.031), ('exp', 0.031), ('indexed', 0.031), ('call', 0.031), ('lagrange', 0.03), ('indicates', 0.03), ('desired', 0.03), ('rank', 0.029), ('guangdong', 0.028), ('nonoverlapping', 0.028), ('copies', 0.028), ('malik', 0.028), ('vice', 0.028), ('segment', 0.028), ('looking', 0.028), ('subspaces', 0.027), ('yet', 0.027), ('fowlkes', 0.027), ('nuclear', 0.027), ('cij', 0.027), ('subregions', 0.027), ('sastry', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="366-tfidf-1" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>Author: Xiaobai Liu, Liang Lin, Alan L. Yuille</p><p>Abstract: In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.</p><p>2 0.16462983 <a title="366-tfidf-2" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>Author: Jason Chang, Donglai Wei, John W. Fisher_III</p><p>Abstract: We develop a generative probabilistic model for temporally consistent superpixels in video sequences. In contrast to supervoxel methods, object parts in different frames are tracked by the same temporal superpixel. We explicitly model flow between frames with a bilateral Gaussian process and use this information to propagate superpixels in an online fashion. We consider four novel metrics to quantify performance of a temporal superpixel representation and demonstrate superior performance when compared to supervoxel methods.</p><p>3 0.15143287 <a title="366-tfidf-3" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>Author: Yan Wang, Rongrong Ji, Shih-Fu Chang</p><p>Abstract: Recent years have witnessed a growing interest in understanding the semantics of point clouds in a wide variety of applications. However, point cloud labeling remains an open problem, due to the difficulty in acquiring sufficient 3D point labels towards training effective classifiers. In this paper, we overcome this challenge by utilizing the existing massive 2D semantic labeled datasets from decadelong community efforts, such as ImageNet and LabelMe, and a novel “cross-domain ” label propagation approach. Our proposed method consists of two major novel components, Exemplar SVM based label propagation, which effectively addresses the cross-domain issue, and a graphical model based contextual refinement incorporating 3D constraints. Most importantly, the entire process does not require any training data from the target scenes, also with good scalability towards large scale applications. We evaluate our approach on the well-known Cornell Point Cloud Dataset, achieving much greater efficiency and comparable accuracy even without any 3D training data. Our approach shows further major gains in accuracy when the training data from the target scenes is used, outperforming state-ofthe-art approaches with far better efficiency.</p><p>4 0.15014881 <a title="366-tfidf-4" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>5 0.14517443 <a title="366-tfidf-5" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>Author: Gautam Singh, Jana Kosecka</p><p>Abstract: This paper presents a nonparametric approach to semantic parsing using small patches and simple gradient, color and location features. We learn the relevance of individual feature channels at test time using a locally adaptive distance metric. To further improve the accuracy of the nonparametric approach, we examine the importance of the retrieval set used to compute the nearest neighbours using a novel semantic descriptor to retrieve better candidates. The approach is validated by experiments on several datasets used for semantic parsing demonstrating the superiority of the method compared to the state of art approaches.</p><p>6 0.14180563 <a title="366-tfidf-6" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>7 0.13195392 <a title="366-tfidf-7" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>8 0.11978364 <a title="366-tfidf-8" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>9 0.1163488 <a title="366-tfidf-9" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>10 0.10278228 <a title="366-tfidf-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.09846177 <a title="366-tfidf-11" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>12 0.097223274 <a title="366-tfidf-12" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>13 0.086610414 <a title="366-tfidf-13" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>14 0.079940714 <a title="366-tfidf-14" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>15 0.077885553 <a title="366-tfidf-15" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>16 0.07697574 <a title="366-tfidf-16" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>17 0.076252997 <a title="366-tfidf-17" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>18 0.074058935 <a title="366-tfidf-18" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>19 0.073659085 <a title="366-tfidf-19" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>20 0.073648803 <a title="366-tfidf-20" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, -0.012), (2, 0.026), (3, 0.025), (4, 0.094), (5, 0.021), (6, 0.018), (7, 0.035), (8, -0.117), (9, -0.007), (10, 0.153), (11, -0.065), (12, -0.013), (13, 0.051), (14, -0.014), (15, -0.005), (16, 0.011), (17, -0.105), (18, -0.072), (19, 0.116), (20, 0.107), (21, 0.0), (22, -0.1), (23, -0.055), (24, -0.039), (25, 0.009), (26, -0.106), (27, -0.13), (28, 0.038), (29, -0.036), (30, 0.015), (31, -0.037), (32, 0.027), (33, -0.088), (34, 0.014), (35, 0.033), (36, -0.07), (37, 0.032), (38, 0.052), (39, -0.072), (40, 0.045), (41, -0.016), (42, 0.016), (43, 0.011), (44, -0.008), (45, -0.051), (46, 0.022), (47, 0.007), (48, -0.02), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92944199 <a title="366-lsi-1" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>Author: Xiaobai Liu, Liang Lin, Alan L. Yuille</p><p>Abstract: In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.</p><p>2 0.78121537 <a title="366-lsi-2" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>3 0.77761024 <a title="366-lsi-3" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>Author: Jason Chang, Donglai Wei, John W. Fisher_III</p><p>Abstract: We develop a generative probabilistic model for temporally consistent superpixels in video sequences. In contrast to supervoxel methods, object parts in different frames are tracked by the same temporal superpixel. We explicitly model flow between frames with a bilateral Gaussian process and use this information to propagate superpixels in an online fashion. We consider four novel metrics to quantify performance of a temporal superpixel representation and demonstrate superior performance when compared to supervoxel methods.</p><p>4 0.76486593 <a title="366-lsi-4" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>Author: Jeremie Papon, Alexey Abramov, Markus Schoeler, Florentin Wörgötter</p><p>Abstract: Unsupervised over-segmentation of an image into regions of perceptually similar pixels, known as superpixels, is a widely used preprocessing step in segmentation algorithms. Superpixel methods reduce the number of regions that must be considered later by more computationally expensive algorithms, with a minimal loss of information. Nevertheless, as some information is inevitably lost, it is vital that superpixels not cross object boundaries, as such errors will propagate through later steps. Existing methods make use of projected color or depth information, but do not consider three dimensional geometric relationships between observed data points which can be used to prevent superpixels from crossing regions of empty space. We propose a novel over-segmentation algorithm which uses voxel relationships to produce over-segmentations which are fully consistent with the spatial geometry of the scene in three dimensional, rather than projective, space. Enforcing the constraint that segmented regions must have spatial connectivity prevents label flow across semantic object boundaries which might otherwise be violated. Additionally, as the algorithm works directly in 3D space, observations from several calibrated RGB+D cameras can be segmented jointly. Experiments on a large data set of human annotated RGB+D images demonstrate a significant reduction in occurrence of clusters crossing object boundaries, while maintaining speeds comparable to state-of-the-art 2D methods.</p><p>5 0.74546427 <a title="366-lsi-5" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>Author: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen</p><p>Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.</p><p>6 0.71835589 <a title="366-lsi-6" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>7 0.71388406 <a title="366-lsi-7" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>8 0.68232739 <a title="366-lsi-8" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>9 0.60773295 <a title="366-lsi-9" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>10 0.60279995 <a title="366-lsi-10" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>11 0.59916806 <a title="366-lsi-11" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>12 0.55331159 <a title="366-lsi-12" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>13 0.52328825 <a title="366-lsi-13" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>14 0.51524013 <a title="366-lsi-14" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>15 0.49020439 <a title="366-lsi-15" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>16 0.48942938 <a title="366-lsi-16" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>17 0.48858956 <a title="366-lsi-17" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<p>18 0.4707267 <a title="366-lsi-18" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>19 0.4651407 <a title="366-lsi-19" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>20 0.45801595 <a title="366-lsi-20" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.128), (16, 0.014), (26, 0.053), (33, 0.276), (35, 0.222), (39, 0.013), (67, 0.061), (69, 0.037), (77, 0.022), (80, 0.015), (87, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88906819 <a title="366-lda-1" href="./cvpr-2013-Sparse_Quantization_for_Patch_Description.html">404 cvpr-2013-Sparse Quantization for Patch Description</a></p>
<p>Author: Xavier Boix, Michael Gygli, Gemma Roig, Luc Van_Gool</p><p>Abstract: The representation of local image patches is crucial for the good performance and efficiency of many vision tasks. Patch descriptors have been designed to generalize towards diverse variations, depending on the application, as well as the desired compromise between accuracy and efficiency. We present a novel formulation of patch description, that serves such issues well. Sparse quantization lies at its heart. This allows for efficient encodings, leading to powerful, novel binary descriptors, yet also to the generalization of existing descriptors like SIFTorBRIEF. We demonstrate the capabilities of our formulation for both keypoint matching and image classification. Our binary descriptors achieve state-of-the-art results for two keypoint matching benchmarks, namely those by Brown [6] and Mikolajczyk [18]. For image classification, we propose new descriptors that perform similar to SIFT on Caltech101 [10] and PASCAL VOC07 [9].</p><p>same-paper 2 0.85750401 <a title="366-lda-2" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>Author: Xiaobai Liu, Liang Lin, Alan L. Yuille</p><p>Abstract: In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.</p><p>3 0.82942754 <a title="366-lda-3" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>4 0.82127208 <a title="366-lda-4" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>5 0.81923497 <a title="366-lda-5" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>6 0.81779939 <a title="366-lda-6" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>7 0.8176142 <a title="366-lda-7" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>8 0.81708223 <a title="366-lda-8" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>9 0.81675684 <a title="366-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.81612903 <a title="366-lda-10" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>11 0.81598586 <a title="366-lda-11" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>12 0.8158586 <a title="366-lda-12" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>13 0.81575537 <a title="366-lda-13" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>14 0.81559932 <a title="366-lda-14" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>15 0.81534815 <a title="366-lda-15" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>16 0.81452692 <a title="366-lda-16" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>17 0.8144722 <a title="366-lda-17" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>18 0.81405544 <a title="366-lda-18" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>19 0.8136307 <a title="366-lda-19" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>20 0.81326139 <a title="366-lda-20" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
