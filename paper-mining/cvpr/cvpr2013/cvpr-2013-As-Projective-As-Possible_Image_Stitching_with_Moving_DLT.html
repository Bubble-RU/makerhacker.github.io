<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-47" href="#">cvpr2013-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</h1>
<br/><p>Source: <a title="cvpr-2013-47-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zaragoza_As-Projective-As-Possible_Image_Stitching_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>Reference: <a title="cvpr-2013-47-reference" href="../cvpr2013_reference/cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Brown† David Suter∗ ∗Australian Centre for Visual Technologies, The University of Adelaide †School of Computing, National University of Singapore  Abstract We investigate projective estimation under model inadequacies, i. [sent-2, score-0.342]
</p><p>2 We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. [sent-5, score-1.281]
</p><p>3 Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. [sent-6, score-0.505]
</p><p>4 , warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. [sent-9, score-0.319]
</p><p>5 Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. [sent-10, score-0.342]
</p><p>6 The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting. [sent-11, score-0.138]
</p><p>7 In this paper, we are primarily concerned with model inadequacies in projective estimation. [sent-17, score-0.384]
</p><p>8 More specifically, we consider situations where the enabling assumptions for the projective model are not fully met by the data, thus fundamentally limiting the achievable goodness of fit. [sent-18, score-0.397]
</p><p>9 Image stitching is typically solved by estimating 2D projective warps to bring images into alignment. [sent-22, score-0.871]
</p><p>10 Parametrised by 3 3 homographies, 2D projective warps arejustified ifthe scene is planar or ifthe views differ purely by rotation [17]. [sent-23, score-0.807]
</p><p>11 Thus the projective model cannot adequately characterise the required warp, causing misalignments or ghosting effects. [sent-25, score-0.526]
</p><p>12 Many commercial stitching software like Autostitch and Photosynth (specifically the panorama tool) use projective warps1 , arguably for their simplicity. [sent-28, score-0.636]
</p><p>13 When the requisite imaging conditions are not met, their success relies on deghosting algorithms to remove unwanted artefacts [17]. [sent-29, score-0.1]
</p><p>14 Here, we offer a different strategy: instead of relying on a projective model (which is often inadequate) and then fix the resulting errors, we adjust the model based on the data to improve the fit. [sent-30, score-0.342]
</p><p>15 , warps that aim to be globally projective, yet allow local deviations to account for model inadequacy; Fig. [sent-33, score-0.319]
</p><p>16 Our method significantly reduces alignment errors, yet is able to maintain overall geometric plausibility. [sent-35, score-0.07]
</p><p>17 Note that our aim is not to perform image stitching for arbitrary camera motions (e. [sent-38, score-0.292]
</p><p>18 Rather, our aim is to tweak the projective model to fit the data as accurately as possible. [sent-41, score-0.342]
</p><p>19 It is also not our goal to dispense with deghosting algorithms, which are still useful if there are serious misalignments or moving objects. [sent-42, score-0.175]
</p><p>20 However, we argue that a good initial stitch is very desirable since it imposes a much lower requirement on subsequent deghosting and postprocessing; the result in Fig. [sent-43, score-0.074]
</p><p>21 More fundamentally, we learn the proposed warp based on a novel estimation technique called Moving DLT. [sent-45, score-0.482]
</p><p>22 It is inspired by the Moving Least Squares (MLS) method [2] for image manipulation [14], but our method applies projective regularisation instead of rigid or affine regularisation. [sent-46, score-0.5]
</p><p>23 This is essential to ensure that the warp extrapolates correctly beyond the image overlap (interpolation) region to maintain perceptual realism. [sent-47, score-0.516]
</p><p>24 1(b) and 1(c) contrast warps from  ×  1Both tools require the camera to rotate about a point, or that the photos be taken from the same spot and with the same focal length. [sent-49, score-0.287]
</p><p>25 aspx 222333333977  ’x1FDitecdorwreasrp xondences’xF1iDtecdorwreasrp oxndences’xF1iDt ecdorwreasrp oxndences  (a) Projective warp. [sent-57, score-0.084]
</p><p>26 The two views differ by a rotation and translation, and the data are not corrupted by noise. [sent-63, score-0.112]
</p><p>27 (a) A 1D projective warp, parametrised by a 2 2 homography, is unable to model the local deviations of the data. [sent-64, score-0.43]
</p><p>28 Note that these deviations are caused purely by model inadequacy since there is no noise in the data. [sent-65, score-0.16]
</p><p>29 (b) An as-affine-as-possible warp, estimated based on [14], can interpolate the local deviations better, but fails to impose global projectivity. [sent-66, score-0.097]
</p><p>30 (c) Our as-projective-as-possible warp interpolates the local deviations flexibly and extrapolates correctly following a global projective trend. [sent-68, score-1.023]
</p><p>31 Being able to interpolate to minimise ghosting and extrapolate  flexibly  corectly to maintain  geometric consistency are vital qualities for image stitching. [sent-70, score-0.314]
</p><p>32 Our work is different in that we fit projective functions instead of geometric  surfaces. [sent-72, score-0.342]
</p><p>33 Further, function  extrapolation is a crucial aspect that was not stressed in [6]. [sent-73, score-0.083]
</p><p>34 2 and 3 introduce the proposed warp and its efficient learning for image stitching. [sent-78, score-0.482]
</p><p>35 Related work While the fundamentals of image stitching are well studied (see [17] for an excellent survey), how to produce good  results when the data is noisy or uncooperative is an open problem. [sent-84, score-0.267]
</p><p>36 In our context, we categorise previous works into two groups: (1) methods that reduce ghosting by constructing better alignment functions, and (2) methods that reduce ghosting after alignment using advanced methods in compositing, pixel selection or blending. [sent-85, score-0.36]
</p><p>37 In the second group, seam cutting [1, 3] and Poisson blending [13] are influential. [sent-86, score-0.07]
</p><p>38 Given matching features between the original and target image frames, the novel view is synthesised by warping the original image using an as-similar-as-possible warp [8] that jointly minimises the registration error and preserves the rigidity of the scene. [sent-95, score-0.686]
</p><p>39 The method also pre-warps the original image with a homography, thus effectively yielding a smoothly interpolating projective warp. [sent-96, score-0.381]
</p><p>40 Imposing scene rigidity minimises the dreaded “wobbling” effect in the smoothed video. [sent-97, score-0.117]
</p><p>41 4, in image stitching where there can be large rotational and translational difference between views, their method does not interpolate flexibly enough due to the rigidity constraints. [sent-99, score-0.498]
</p><p>42 A recent work proposed smoothly varying affine warps for image stitching [9]. [sent-104, score-0.649]
</p><p>43 An interesting innovation of [9] is an affine initialisation of the registration function, which is then deformed locally to minimise registration errors while maintaining global affinity. [sent-106, score-0.211]
</p><p>44 Fundamentally, using affine regularisation may be suboptimal, since an affinity does not contain sufficient degrees of freedom to achieve a fully perspective warp [17], e. [sent-107, score-0.64]
</p><p>45 4 and 5 (second row) show, while the method can interpolate flexibly, it produces highly distorted results in the extrapolation region, where there are no data to guide the local deformation and the warp reverts to global affinity; Fig. [sent-111, score-0.628]
</p><p>46 Essentially theirs is a special case of a piece-wise projective warp, which is more flexible than using a single homography. [sent-115, score-0.342]
</p><p>47 As-Projective-As-Possible Warps We first review the estimation of projective transformations customarily used in image stitching, and then describe the proposed as-projective-as-possible warp. [sent-120, score-0.379]
</p><p>48 A projective warp or and homography aims to map x to x? [sent-128, score-0.952]
</p><p>49 The divisions in (2) cause the warp to be non-linear, as Fig. [sent-134, score-0.482]
</p><p>50 do not differ purely by rotation or are not of a planar scene, using a basic projective warp inevitably yields ghosting effects in the alignment. [sent-166, score-1.07]
</p><p>51 To alleviate this problem, our idea is to warp each x∗ using a location dependent homography ˜x? [sent-167, score-0.61]
</p><p>52 of Intuitively, since (8) assigns higher weights to data closer to x∗, the projective warp H∗ better respects the local structure around x∗ . [sent-179, score-0.853]
</p><p>53 Contrast this to (5) which uses a single and global projective warp H for all x∗ . [sent-180, score-0.824]
</p><p>54 Moreover, as x∗ is moved continuously in its domain I, the warp H∗ also varies smoothly. [sent-181, score-0.482]
</p><p>55 This produces an overall warp that adapts flexibly to the data, yet attempts to be as-projectiveas-possible. [sent-182, score-0.627]
</p><p>56 1(c) and 3(c) illustrate such a warp in 1D and 2D. [sent-184, score-0.482]
</p><p>57 , when x∗ is in a data poor or extrapolation region. [sent-195, score-0.083]
</p><p>58 (11) This also serves to regularise the warp, whereby a high γ reduces the warp complexity; in fact as γ → 1 the warp reduces to the global projective warp. [sent-201, score-1.362]
</p><p>59 Conceptually, Moving DLT can be seen as the projective version of MLS [2]. [sent-205, score-0.342]
</p><p>60 In the context of warping points in 2D for image manipulation [14], MLS estimates for each x∗ an affine transformation defined by a matrix F∗ ∈ R2×3  x? [sent-206, score-0.129]
</p><p>61 ible warps, but such warps are ultimately only as-affine-aspossible; see Fig. [sent-224, score-0.262]
</p><p>62 222333334199  ’x1FiDt ecdorwreasrp xondences  ××  Figure 2. [sent-227, score-0.084]
</p><p>63 Results from Moving DLT without regularisation for a 1D projective estimation problem on synthetic data. [sent-228, score-0.419]
</p><p>64 Efficient Learning for Image Stitching Here we describe an efficient algorithm for image stitching based on the proposed warp. [sent-230, score-0.267]
</p><p>65 One might argue against RANSAC since we consider cases where the inliers may deviate from the projective model. [sent-233, score-0.385]
</p><p>66 3(c) illustrates a warp learnt with 100 100 cells for a 1500 2000-pixel image pair. [sent-241, score-0.539]
</p><p>67 Note that, even without parallel computations, learning the warp in Fig. [sent-244, score-0.482]
</p><p>68 3 with 100 100 cells and N = 2100  ×  keypoint matches (A is of size 4200 9) takes less than a minute on a Pentium i7 2. [sent-245, score-0.146]
</p><p>69 Further speedups are possible if we realise that, for most cells, due to the offsetting (11) many of the weights do not differ from the offset γ. [sent-248, score-0.122]
</p><p>70 3(d) histograms across all cells the number of weights that differ from γ (here, γ = 0. [sent-251, score-0.153]
</p><p>71 A vast majority of cells (> 40%) have fewer than 20 weights (out of 2100) that differ from γ. [sent-253, score-0.153]
</p><p>72 Observe that the warp is globally projective for extrapolation, but adapts flexibly in the overlap region for better alignment. [sent-262, score-0.969]
</p><p>73 (d) Histogram of number of weights γ for the cells in (b). [sent-335, score-0.086]
</p><p>74 The input images correspond to views that differ by rotation and translation. [sent-338, score-0.112]
</p><p>75 log2  We compare our as-projective-as-possible (APAP) warp against other warp improvement methods for image stitching, namely, content preserving warps (CPW) [10], dual homography warps (DHW) [4], and smoothly varying affine (SVA) [9]. [sent-352, score-1.76]
</p><p>76 To cogently differentiate the methods, we avoid sophisticated postprocessing like seam cutting and straightening such as in [4], and simply blend the aligned images by intensity averaging such that any misalignments remain obvious. [sent-353, score-0.149]
</p><p>77 We select testing images which correspond to views that differ by more than a pure rotation. [sent-356, score-0.112]
</p><p>78 In addition, following [10], for CPW we pre-warp the source image with the global homography estimated via DLT on the inliers returned by RANSAC. [sent-369, score-0.171]
</p><p>79 4 and 5 depict results on the railtracks and temple image pairs. [sent-374, score-0.073]
</p><p>80 The baseline warp (global homography via DLT on inliers) is clearly unable to satisfactorily align the images since the views do not differ purely by rotation. [sent-376, score-0.791]
</p><p>81 SVA, DHW and Autostitch are marginally better, but significant ghosting remains. [sent-377, score-0.138]
</p><p>82 Further, note the highly distorted warp produced by SVA, especially in the extrapolation regions. [sent-378, score-0.588]
</p><p>83 was not completely successful; observe the misaligned rail tracks and tiles on the ground. [sent-381, score-0.066]
</p><p>84 This reduces the burden on postprocessing; we have confirmed that pyramid blending [17] is sufficient to account for exposure differences and to smoothen the blend. [sent-383, score-0.069]
</p><p>85 While CPW with pre-warping is able to produce good results, the rigidity constraints (a grid like in Fig. [sent-384, score-0.083]
</p><p>86 3(b) is defined and discouraged from deforming) may counterproductively limit the flexibility of the warp (observe the only slightly nonlinear outlines of the warped images3). [sent-385, score-0.551]
</p><p>87 Thus although the rail tracks and tiles are aligned correctly (more  × ×  keypoint matches exist in these relatively texture-rich areas to influence the warp), ghosting occurs in regions near the skyline. [sent-386, score-0.293]
</p><p>88 For DHW, CPW, SVA and APAP (without WSVD updating), we record the total duration for warp estimation (plus any data structure preparation time), pixel warping and blending. [sent-389, score-0.53]
</p><p>89 While 8 mins was reported in [9] for 500 500 images, in our experiments SVA takes 15 mins for temple (1024 768) and 1hour for railtracks (1500 2000). [sent-393, score-0.131]
</p><p>90 We then incrementally warp the other images via APAP onto the panorama. [sent-396, score-0.509]
</p><p>91 Quantitative benchmarking To quantify the alignment accuracy of an estimated warp f : R2 → R2, we compute the root mean squared error  (RMSE) of f on a set of keypoint matches {xi, x? [sent-401, score-0.613]
</p><p>92 mly partitioned the available SIFT keypoint matches into a “training” and “testing” set. [sent-412, score-0.089]
</p><p>93 1, imposing warp rigidity is essential to prevent wobbling in video stabilisation, which is the original aim of [10]. [sent-419, score-0.607]
</p><p>94 This permits the direct application of the various warp estimation methods. [sent-426, score-0.482]
</p><p>95 For SVA this is most likely due to its affine instead of projective regularisation; cf. [sent-433, score-0.423]
</p><p>96 Additionally, for CPW, it appears that enforcing rigidity has perturbed the effects of the pre-warping by a global homography. [sent-436, score-0.083]
</p><p>97 In contrast, APAP reduces gracefully to a global homography as the camera centres coincide, and provides the most accurate alignment as the translation increases. [sent-437, score-0.31]
</p><p>98 The results on image stitching showed encouraging results, where our method was able to accurately align images that differ by more than a pure rotation. [sent-440, score-0.362]
</p><p>99 The experiments also demonstrated that the proposed warp reduces gracefully to a global homography as the camera translation tends to zero, but adapts flexibly to account for model inadequacy as the translation increases. [sent-441, score-0.955]
</p><p>100 Seamless image stitching of scenes with large motions and expoure differences. [sent-469, score-0.267]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('warp', 0.482), ('projective', 0.342), ('stitching', 0.267), ('warps', 0.262), ('dlt', 0.25), ('apap', 0.208), ('cpw', 0.203), ('sva', 0.177), ('wsvd', 0.146), ('ghosting', 0.138), ('dhw', 0.129), ('homography', 0.128), ('flexibly', 0.108), ('autostitch', 0.083), ('photosynth', 0.083), ('rigidity', 0.083), ('extrapolation', 0.083), ('affine', 0.081), ('regularisation', 0.077), ('rmse', 0.076), ('deghosting', 0.074), ('differ', 0.067), ('mls', 0.064), ('inadequacy', 0.062), ('cells', 0.057), ('deviations', 0.057), ('moving', 0.055), ('atw', 0.051), ('warping', 0.048), ('singular', 0.047), ('misalignments', 0.046), ('postprocessing', 0.046), ('keypoint', 0.045), ('xi', 0.045), ('views', 0.045), ('matches', 0.044), ('inliers', 0.043), ('ransac', 0.042), ('alignment', 0.042), ('arghmin', 0.042), ('arghmini', 0.042), ('bestviewdonscren', 0.042), ('bsadcap', 0.042), ('counterproductively', 0.042), ('ecdorwreasrp', 0.042), ('inadequacies', 0.042), ('initialisms', 0.042), ('oxndences', 0.042), ('railtracks', 0.042), ('vdvt', 0.042), ('vecomparison', 0.042), ('wobbling', 0.042), ('xondences', 0.042), ('blending', 0.041), ('purely', 0.041), ('interpolate', 0.04), ('smoothly', 0.039), ('registration', 0.039), ('adapts', 0.037), ('customarily', 0.037), ('rail', 0.037), ('stabilisation', 0.037), ('extrapolates', 0.034), ('minimises', 0.034), ('fundamentally', 0.033), ('centres', 0.032), ('temple', 0.031), ('parametrised', 0.031), ('translation', 0.03), ('mins', 0.029), ('seam', 0.029), ('tiles', 0.029), ('weights', 0.029), ('blend', 0.028), ('minimise', 0.028), ('mismatches', 0.028), ('repetitions', 0.028), ('rit', 0.028), ('reduces', 0.028), ('align', 0.028), ('eigendecomposition', 0.028), ('shum', 0.028), ('warped', 0.027), ('onto', 0.027), ('panorama', 0.027), ('centre', 0.027), ('offset', 0.026), ('artefacts', 0.026), ('intrinsics', 0.026), ('ifthe', 0.025), ('homographies', 0.025), ('camera', 0.025), ('gracefully', 0.025), ('siggraph', 0.025), ('preserving', 0.024), ('errors', 0.024), ('distorted', 0.023), ('vlfeat', 0.023), ('yy', 0.022), ('met', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="47-tfidf-1" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>2 0.21853727 <a title="47-tfidf-2" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>Author: Zihan Zhou, Hailin Jin, Yi Ma</p><p>Abstract: Recently, a new image deformation technique called content-preserving warping (CPW) has been successfully employed to produce the state-of-the-art video stabilization results in many challenging cases. The key insight of CPW is that the true image deformation due to viewpoint change can be well approximated by a carefully constructed warp using a set of sparsely constructed 3D points only. However, since CPW solely relies on the tracked feature points to guide the warping, it works poorly in large textureless regions, such as ground and building interiors. To overcome this limitation, in this paper we present a hybrid approach for novel view synthesis, observing that the textureless regions often correspond to large planar surfaces in the scene. Particularly, given a jittery video, we first segment each frame into piecewise planar regions as well as regions labeled as non-planar using Markov random fields. Then, a new warp is computed by estimating a single homography for regions belong to the same plane, while in- heriting results from CPW in the non-planar regions. We demonstrate how the segmentation information can be efficiently obtained and seamlessly integrated into the stabilization framework. Experimental results on a variety of real video sequences verify the effectiveness of our method.</p><p>3 0.16173846 <a title="47-tfidf-3" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>Author: Christian Richardt, Yael Pritch, Henning Zimmer, Alexander Sorkine-Hornung</p><p>Abstract: We present a solution for generating high-quality stereo panoramas at megapixel resolutions. While previous approaches introduced the basic principles, we show that those techniques do not generalise well to today’s high image resolutions and lead to disturbing visual artefacts. As our first contribution, we describe the necessary correction steps and a compact representation for the input images in order to achieve a highly accurate approximation to the required ray space. Our second contribution is a flow-based upsampling of the available input rays which effectively resolves known aliasing issues like stitching artefacts. The required rays are generated on the fly to perfectly match the desired output resolution, even for small numbers of input images. In addition, the upsampling is real-time and enables direct interactive control over the desired stereoscopic depth effect. In combination, our contributions allow the generation of stereoscopic panoramas at high output resolutions that are virtually free of artefacts such as seams, stereo discontinuities, vertical parallax and other mono-/stereoscopic shape distortions. Our process is robust, and other types of multiperspective panoramas, such as linear panoramas, can also benefit from our contributions. We show various comparisons and high-resolution results.</p><p>4 0.11229822 <a title="47-tfidf-4" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>Author: Erik Ask, Olof Enqvist, Fredrik Kahl</p><p>Abstract: This paper is concerned with model fitting in the presence of noise and outliers. Previously it has been shown that the number of outliers can be minimized with polynomial complexity in the number of measurements. This paper improves on these results in two ways. First, it is shown that for a large class of problems, the statistically more desirable truncated L2-norm can be optimized with the same complexity. Then, with the same methodology, it is shown how to transform multi-model fitting into a purely combinatorial problem—with worst-case complexity that is polynomial in the number of measurements, though exponential in the number of models. We apply our framework to a series of hard registration and stitching problems demonstrating that the approach is not only of theoretical interest. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers for fitting problems with lowdimensional models.</p><p>5 0.099014215 <a title="47-tfidf-5" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>Author: Adrien Bartoli, Toby Collins</p><p>Abstract: It has been shown that a surface deforming isometrically can be reconstructed from a single image and a template 3D shape. Methods from the literature solve this problem efficiently. However, they all assume that the camera model is calibrated, which drastically limits their applicability. We propose (i) a general variational framework that applies to (calibrated and uncalibrated) general camera models and (ii) self-calibrating 3D reconstruction algorithms for the weak-perspective and full-perspective camera models. In the former case, our algorithm returns the normal field and camera ’s scale factor. In the latter case, our algorithm returns the normal field, depth and camera ’s focal length. Our algorithms are the first to achieve deformable 3D reconstruction including camera self-calibration. They apply to much more general setups than existing methods. Experimental results on simulated and real data show that our algorithms give results with the same level of accuracy as existing methods (which use the true focal length) on perspective images, and correctly find the normal field on affine images for which the existing methods fail.</p><p>6 0.079058371 <a title="47-tfidf-6" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>7 0.07576374 <a title="47-tfidf-7" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>8 0.06880556 <a title="47-tfidf-8" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>9 0.06797795 <a title="47-tfidf-9" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>10 0.063226596 <a title="47-tfidf-10" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>11 0.060169805 <a title="47-tfidf-11" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>12 0.056440905 <a title="47-tfidf-12" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>13 0.050321583 <a title="47-tfidf-13" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>14 0.048588533 <a title="47-tfidf-14" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>15 0.044092197 <a title="47-tfidf-15" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>16 0.043937743 <a title="47-tfidf-16" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>17 0.043875676 <a title="47-tfidf-17" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>18 0.043638449 <a title="47-tfidf-18" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>19 0.043538362 <a title="47-tfidf-19" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>20 0.041586243 <a title="47-tfidf-20" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, 0.07), (2, -0.007), (3, 0.02), (4, 0.008), (5, -0.014), (6, -0.008), (7, -0.044), (8, 0.003), (9, 0.0), (10, 0.016), (11, 0.064), (12, 0.012), (13, -0.032), (14, 0.021), (15, -0.075), (16, 0.026), (17, 0.021), (18, 0.003), (19, 0.012), (20, -0.02), (21, -0.022), (22, 0.0), (23, -0.031), (24, 0.039), (25, -0.048), (26, -0.05), (27, 0.026), (28, 0.048), (29, 0.028), (30, -0.004), (31, -0.001), (32, -0.054), (33, 0.0), (34, 0.033), (35, -0.053), (36, -0.102), (37, 0.06), (38, -0.0), (39, 0.04), (40, 0.003), (41, -0.001), (42, -0.056), (43, -0.052), (44, 0.029), (45, -0.084), (46, 0.048), (47, -0.061), (48, -0.003), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91019058 <a title="47-lsi-1" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>2 0.76754421 <a title="47-lsi-2" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>3 0.73312539 <a title="47-lsi-3" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>4 0.65874231 <a title="47-lsi-4" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>5 0.64867288 <a title="47-lsi-5" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>Author: Ming Zeng, Jiaxiang Zheng, Xuan Cheng, Xinguo Liu</p><p>Abstract: This paper presents a method for quasi-rigid objects modeling from a sequence of depth scans captured at different time instances. As quasi-rigid objects, such as human bodies, usually have shape motions during the capture procedure, it is difficult to reconstruct their geometries. We represent the shape motion by a deformation graph, and propose a model-to-partmethod to gradually integrate sampled points of depth scans into the deformation graph. Under an as-rigid-as-possible assumption, the model-to-part method can adjust the deformation graph non-rigidly, so as to avoid error accumulation in alignment, which also implicitly achieves loop-closure. To handle the drift and topological error for the deformation graph, two algorithms are introduced. First, we use a two-stage registration to largely keep the rigid motion part. Second, in the step of graph integration, we topology-adaptively integrate new parts and dynamically control the regularization effect of the deformation graph. We demonstrate the effectiveness and robustness of our method by several depth sequences of quasi-rigid objects, and an application in human shape modeling.</p><p>6 0.64203513 <a title="47-lsi-6" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>7 0.62269396 <a title="47-lsi-7" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>8 0.60608292 <a title="47-lsi-8" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>9 0.56180954 <a title="47-lsi-9" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>10 0.55640221 <a title="47-lsi-10" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>11 0.54935497 <a title="47-lsi-11" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>12 0.53924888 <a title="47-lsi-12" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>13 0.53115821 <a title="47-lsi-13" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>14 0.52990913 <a title="47-lsi-14" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>15 0.52945632 <a title="47-lsi-15" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>16 0.51813251 <a title="47-lsi-16" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>17 0.51203746 <a title="47-lsi-17" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>18 0.50877041 <a title="47-lsi-18" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>19 0.5031836 <a title="47-lsi-19" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>20 0.50286889 <a title="47-lsi-20" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.012), (10, 0.095), (16, 0.039), (26, 0.041), (28, 0.011), (33, 0.194), (61, 0.321), (65, 0.024), (67, 0.041), (69, 0.037), (87, 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75474328 <a title="47-lda-1" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>2 0.68286067 <a title="47-lda-2" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>Author: Hao Jiang, Jianxiong Xiao</p><p>Abstract: We propose a novel linear method to match cuboids in indoor scenes using RGBD images from Kinect. Beyond depth maps, these cuboids reveal important structures of a scene. Instead of directly fitting cuboids to 3D data, we first construct cuboid candidates using superpixel pairs on a RGBD image, and then we optimize the configuration of the cuboids to satisfy the global structure constraints. The optimal configuration has low local matching costs, small object intersection and occlusion, and the cuboids tend to project to a large region in the image; the number of cuboids is optimized simultaneously. We formulate the multiple cuboid matching problem as a mixed integer linear program and solve the optimization efficiently with a branch and bound method. The optimization guarantees the global optimal solution. Our experiments on the Kinect RGBD images of a variety of indoor scenes show that our proposed method is efficient, accurate and robust against object appearance variations, occlusions and strong clutter.</p><p>3 0.6593082 <a title="47-lda-3" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>4 0.62983835 <a title="47-lda-4" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>5 0.62848896 <a title="47-lda-5" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>Author: LiMin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motionlet, a mid-level and spatiotemporal part, for human motion recognition. Motionlet can be seen as a tight cluster in motion and appearance space, corresponding to the moving process of different body parts. We postulate three key properties of motionlet for action recognition: high motion saliency, multiple scale representation, and representative-discriminative ability. Towards this goal, we develop a data-driven approach to learn motionlets from training videos. First, we extract 3D regions with high motion saliency. Then we cluster these regions and preserve the centers as candidate templates for motionlet. Finally, we examine the representative and discriminative power of the candidates, and introduce a greedy method to select effective candidates. With motionlets, we present a mid-level representation for video, called motionlet activation vector. We conduct experiments on three datasets, KTH, HMDB51, and UCF50. The results show that the proposed methods significantly outperform state-of-the-art methods.</p><p>6 0.62067759 <a title="47-lda-6" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>7 0.60694069 <a title="47-lda-7" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>8 0.59279656 <a title="47-lda-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.58853972 <a title="47-lda-9" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>10 0.58848298 <a title="47-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>11 0.58746552 <a title="47-lda-11" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>12 0.58740014 <a title="47-lda-12" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>13 0.58737856 <a title="47-lda-13" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>14 0.58737522 <a title="47-lda-14" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>15 0.58708566 <a title="47-lda-15" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>16 0.58696014 <a title="47-lda-16" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>17 0.58670539 <a title="47-lda-17" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>18 0.58626884 <a title="47-lda-18" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>19 0.5861544 <a title="47-lda-19" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>20 0.58603764 <a title="47-lda-20" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
