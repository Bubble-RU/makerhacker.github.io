<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-18" href="#">cvpr2013-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</h1>
<br/><p>Source: <a title="cvpr-2013-18-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lan_A_Max-Margin_Riffled_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>Reference: <a title="cvpr-2013-18-reference" href="../cvpr2013_reference/cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca a5  Abstract We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. [sent-3, score-0.825]
</p><p>2 The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. [sent-4, score-1.434]
</p><p>3 Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. [sent-6, score-0.682]
</p><p>4 We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications. [sent-7, score-0.842]
</p><p>5 Although more than a dozen of tags are associated with the image, it is arguable only a few ofthem are perceptually important to humans. [sent-16, score-0.64]
</p><p>6 In this paper, our goal is to rank tags according to their importance or relevance to the image content. [sent-17, score-0.849]
</p><p>7 We demonstrate that tag ranking leads towards better image understanding. [sent-18, score-0.667]
</p><p>8 Not all tags associated with an image are equally important. [sent-22, score-0.64]
</p><p>9 The goal of this paper is to rank tags according to their importance or relevance to the image content. [sent-23, score-0.849]
</p><p>10 However, instead of modeling importance prediction as a binary decision problem, we predict image tags with multiple importance levels. [sent-27, score-0.734]
</p><p>11 With the increasing popularity of social photo sharing websites like Flickr, tons of images with user-  specified tags are available. [sent-29, score-0.619]
</p><p>12 However, these massive quantities of tags are collected in extremely uncontrolled settings, thus lots of irrelevant tags can be associated with images. [sent-30, score-1.285]
</p><p>13 This limits the benefits of these tags in potential applications such as visual search and image organization. [sent-31, score-0.619]
</p><p>14 In computer vision, a variety of methods has been developed to explore the correspondences between tags and images, for automatic image annotation [2, 3, 9, 20, 22]. [sent-32, score-0.619]
</p><p>15 However, none of these methods considers the relevance of tags to the image content. [sent-33, score-0.67]
</p><p>16 Our work is inspired by the recent research that explores the ordering of tags associated with an image. [sent-34, score-0.64]
</p><p>17 Hwang and Grauman [12, 13] observe that human taggers usually name prominent tags first, and gradually expand to irrelevant tags. [sent-35, score-0.645]
</p><p>18 [15] develop an unsupervised tag ranking scheme on Flickr images with associated tag lists. [sent-38, score-1.204]
</p><p>19 In contrast to all of the above mentioned work, our method directly learns the ordering of tags from the training data. [sent-40, score-0.619]
</p><p>20 So when given a new image, our model can predict an ordered list of tags for the image, where the tags are ranked according to the importance to the image content. [sent-41, score-1.486]
</p><p>21 Compared to the familiar problem of image ranking, tag ranking outputs a rank list for each image, and the tags are  more closely correlated than images. [sent-43, score-1.509]
</p><p>22 For example, given four tags “bear”, “furry”, “stripe” and “zebra”, knowing “stripe” is preferable to “furry” will indicate that “zebra” is preferable to “bear”. [sent-44, score-0.711]
</p><p>23 These complex and structured relations can be represented by a densely connected graph, where nodes are tags and edges indicate the pairwise interactions between tags. [sent-45, score-0.642]
</p><p>24 As with many challenging learning problems, learning to rank tags involves intractably large state spaces, e. [sent-49, score-0.776]
</p><p>25 The interleaving stage characterizes riffled independence for rankings and distinguishes it from other probabilistic independence assumptions. [sent-55, score-0.658]
</p><p>26 We propose a novel max-margin learning framework for training tag ranking models with riffled independence as-  sumptions. [sent-57, score-1.071]
</p><p>27 To the best of our knowledge, this is the first max-margin learning framework designed for the problem of image tag ranking modeling structured preferences among tags. [sent-60, score-0.825]
</p><p>28 Suppose we have six tags associated with an image: tree (T), furry (F), bear (B), grass (G), stripe (S) and zebra (Z). [sent-64, score-1.032]
</p><p>29 Our goal is to sort the tags into an ordered list, which  (a)(b) Figure 2: Illustration of the hierarchical riffle independent decomposition algorithm [11]. [sent-65, score-0.802]
</p><p>30 “T, F, B, G, S, Z” represents six image tags “tree, furry, bear, grass, stripe and zebra” respectively. [sent-67, score-0.668]
</p><p>31 The idea of riffled independence is to decompose the original set of tags into two subsets and rank them independently: {Z, B, S, iFn}t o(w twhioch s represents “raannkim tahel”m) a inndd {pTe,n Gde}n (lwy:hi {chZ represFe}n t(sw “plant”). [sent-71, score-1.157]
</p><p>32 The algorithm first performs a top-down decomposition that recursively partitions the full set of tags into riffled independent groups (see Fig. [sent-89, score-0.992]
</p><p>33 example with three important riffle independence properties: 1) tags within each subset have strong correlations. [sent-99, score-0.838]
</p><p>34 2) The absolute rank of a tag in one subset gives no information about the relative ranks of tags in another subset. [sent-101, score-1.346]
</p><p>35 For example, knowing bear (B) ranks first among all the six tags does not tell that grass (G) is preferred to tree (T). [sent-102, score-0.875]
</p><p>36 Based on this intuition, one can rank tags in each subset independently. [sent-103, score-0.753]
</p><p>37 The structure consists of three levels, where tags form into larger groups from bottom up. [sent-107, score-0.66]
</p><p>38 The bottom level captures the preferences among tags within each leaf set (blue lines), such as whether “tree” is preferable to “grass”. [sent-108, score-0.867]
</p><p>39 The full rank list predicted from the tags is shown on the left side. [sent-110, score-0.842]
</p><p>40 We model two types of preferences (or interactions) between image tags: preferences among tags (which we call tag preferences) and preferences among groups (which we call group preferences). [sent-118, score-1.604]
</p><p>41 Suppose there are V tags in an image, we write xi for the feature vector for tag i. [sent-131, score-1.152]
</p><p>42 The tags associated with the image are represented as Y = (y1, y2 , . [sent-136, score-0.64]
</p><p>43 o Gdievle eins tag ranks that arrange the annotations into an ordered list. [sent-141, score-0.631]
</p><p>44 , rV), where ri ∈ R denotes the rank for tag iin the current image and R∈ ∈d eRno dteesn othtees se thte eo rfa arnaknk fso. [sent-145, score-0.671]
</p><p>45 In Ea, all pairs tosf tags owteitdhin by yth Ee same elereaf E Eset = are Eco,nEne}c. [sent-152, score-0.619]
</p><p>46 In Eb, all pairs of tags from two differeblnut groups are gco. [sent-155, score-0.66]
</p><p>47 The group labels of the tags associated with an image are represented as H = (h1, h2 , . [sent-162, score-0.663]
</p><p>48 We define the score of labeling image X with tags Y in the order of R as: θ? [sent-168, score-0.619]
</p><p>49 3 represent the summation of weighted differences of the confidence scores between every pair of tags, under the condition that the i-th tag is ranked higher than the j-th tag (ri > rj). [sent-183, score-1.085]
</p><p>50 αyi and βhi are standard linear models for ranking tag yi and group hi respectively. [sent-184, score-0.732]
</p><p>51 Max-Margin Learning Assume we are given a collection of training images Xn, annotations Yn and rankings Rn, we want to train the model parameters θ that tends to correctly predict the tag 333 111000335  ranks R∗ . [sent-195, score-0.681]
</p><p>52 Riffle Independent Tag Rank Prediction  θ  Given the model parameters and tags Y , the inference problem is to find the best rank list R∗ for an image I. [sent-215, score-0.871]
</p><p>53 2) Interleave groups of ranked tags into larger groups recursively in a stagewise fashion to generate the full ranking. [sent-219, score-0.799]
</p><p>54 8 respectively capture the preferences among tags within the same leaf set, and from different groups. [sent-231, score-0.831]
</p><p>55 9 represents the first stage of inference, where tags in each leaf set are ranked independently, and Ra denotes the local rank lists of all leaf sets. [sent-239, score-1.001]
</p><p>56 a,nEd h}e,re w we assume the tags in a leaf set are fully connected. [sent-248, score-0.696]
</p><p>57 2 (b), this stage corresponds to ranking tags “B, F”, “Z, S” and “G, T” independently in the three leaf sets. [sent-253, score-0.887]
</p><p>58 After the first stage, tags within each leaf set are ordered into local rank lists. [sent-257, score-0.866]
</p><p>59 In this stage, groups of tags are recursively interleaved to form larger groups in a stagewise fashion. [sent-259, score-0.767]
</p><p>60 2 (b): the tags in the groups “bear” and “zebra” are interleaved to form the larger group “animal”, then the tags in ‘animal” are interleaved with tags in “plant” to form the full rank list. [sent-261, score-2.097]
</p><p>61 We introduce variables zijst for all edges (i, j) ∈ Eak: zijst = 1if the i-th and j-th tags are assigned w (ii,tjh )r ∈ank Es s and t respectively, and zero otherwise. [sent-264, score-0.805]
</p><p>62 Note that here we do not enforce the mutual exclusivity constraints which ensure tags iand j map to different ranks. [sent-289, score-0.619]
</p><p>63 This is because in terms of image tag ranking, it is common that two tags are equally important to an image according to human’s perception. [sent-290, score-1.135]
</p><p>64 10, where we interleave the groups of ranked tags into larger groups in a stagewise fashion. [sent-292, score-0.801]
</p><p>65 For example, suppose that two ranked groups have k and n − k tags respectively, tahnadt tMw ois r atnhek etodt aglr onuupmsb hearv oef k k ra annkds, n nth? [sent-296, score-0.734]
</p><p>66 In contrast, ranking tags over a fully connected graph takes around 5 min per image under the same settings. [sent-311, score-0.786]
</p><p>67 The most direct application is to use the model to rank the randomly permuted tags associated with each testing image, which we call tag ranking. [sent-314, score-1.29]
</p><p>68 Thus we also use the model to simultaneously predict the tag list and rank the tags for an unannotated image. [sent-316, score-1.401]
</p><p>69 Furthermore, we also demonstrate that the predicted rank list of tags help improve higher-level computer vision tasks, such as image retrieval and tag-based image search. [sent-318, score-0.865]
</p><p>70 We define the four different applications as follows: •  •  •  Tag Ranking: Given an image and its associated tag lTisatg, our goal :is Gtoiv reann ka nth iem tags according tooc ithateeird i tmagportance or relevance to the image content. [sent-319, score-1.207]
</p><p>71 Image Retrieval: Given a novel query image, we first  use our metroideveal t:o G predict an eolr dqeureerdy limista goef, tags ifrosrt it. [sent-322, score-0.701]
</p><p>72 rank for tag iin the query image, and V is the total number of tags associated with the query image. [sent-334, score-1.358]
</p><p>73 As opposed to the presence of tags, relative tags indicate the strength of a tag w. [sent-340, score-1.15]
</p><p>74 Relative tags are more informative and descriptive compared to  the traditional keywords. [sent-346, score-0.619]
</p><p>75 The LabelMe dataset consists of mostly office and street scenes of 3825 images with an average of 23 tags per image. [sent-355, score-0.619]
</p><p>76 Different from SUN Attribute where tags correspond to attributes, the tags in LabelMe are objects. [sent-356, score-1.238]
</p><p>77 We construct the tag list for an image by using the attributes receive more  ×  than zero votes. [sent-364, score-0.657]
</p><p>78 In order to mimic real annotations from 333 111000557  the Internet, we further add three noisy tags to the tag list of each image. [sent-365, score-1.241]
</p><p>79 The noisy tags are randomly sampled from the tags that receive zero votes. [sent-366, score-1.258]
</p><p>80 For LabelMe, we use the 3825 images compiled in [13], where the tag rank list associated with each image are also provided. [sent-367, score-0.76]
</p><p>81 We quantize the tag rank list associated with each image into three levels: Most Relevant, Relevant and Less Relevant. [sent-368, score-0.76]
</p><p>82 We also randomly sample three irrelevant tags and add them to the tag list of each image. [sent-369, score-1.25]
</p><p>83 We use the Normalized Discounted Cumulative Gains (NDCG) [4] to measure the performance of the tag ranking approaches. [sent-378, score-0.667]
</p><p>84 To obtain the tag rank list for an image, we directly sort the output scores of SVM classifiers for each tag. [sent-387, score-0.755]
</p><p>85 To obtain the rank list for an image, we sort the output scores obtained from the individual tag rankers. [sent-389, score-0.755]
</p><p>86 The potential function for an individual tag ranker is: αyi · xi, where xi is the feature vector of the i-th tag and yi is th·e x tag category label. [sent-390, score-1.584]
</p><p>87 At the truncation level of 4 (NDCG@4), we see our method yields around  (a) (b) Figure 4: Comparison of tag ranking results of different methods on SUN Attribute dataset and Labelme dataset respectively. [sent-398, score-0.689]
</p><p>88 But for LabelMe, the tag ranks provided in [13] are obtained in a less controlled setting, by the order in which tags are added to the image. [sent-406, score-1.197]
</p><p>89 The order usually reflects the importance of tags but is subjective to the AMT workers. [sent-407, score-0.664]
</p><p>90 Some visualizations of the tag ranking results for MMRIM and rankSVM are shown in Fig. [sent-408, score-0.667]
</p><p>91 Now we demonstrate that our method is also capable of predicting an ordered list of tags for an unannotated image. [sent-413, score-0.789]
</p><p>92 In this scenario, rather than reordering the given annotations during testing, we assume each image is associated with the whole tag vocabulary, and predict an ordered list of the whole vocabulary for each testing image. [sent-414, score-0.704]
</p><p>93 We expect to rank the most relevant tags to the top and irrelevant tags to the bottom of the rank list. [sent-415, score-1.565]
</p><p>94 333 111000668  5: Examples of tag ranking results using our method and rankSVM on SUN Attribute. [sent-429, score-0.667]
</p><p>95 Figure  The tags are ordered according levels: most  relevant,  relevant  to the  and  less  MethodF1 score SVM0. [sent-433, score-0.688]
</p><p>96 Relative Tag based Image Search A relative tag query consists of M pairs of tags with preferences, e. [sent-468, score-1.184]
</p><p>97 a Wnde we generate trhiees query dsoetu by randomly sampling from the tags in the training set. [sent-476, score-0.653]
</p><p>98 Conclusion We have presented the Max-Margin Riffled Independence Model (MMRIM) that integrates the max-margin criterion and riffled independence partitions within the same framework for image tag ranking. [sent-487, score-0.92]
</p><p>99 Furthermore, our approach models the correlations between different tags leading to improved tag ranking performance. [sent-488, score-1.286]
</p><p>100 Besides tag ranking, we also apply our model to three higher-level computer vision applications: image auto-annotation, image retrieval and relative tag based image search. [sent-489, score-1.07]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tags', 0.619), ('tag', 0.516), ('riffled', 0.278), ('ranking', 0.151), ('preferences', 0.135), ('rank', 0.134), ('independence', 0.126), ('mmrim', 0.104), ('bear', 0.094), ('ndcg', 0.093), ('riffle', 0.093), ('zijst', 0.093), ('zebra', 0.09), ('list', 0.089), ('labelme', 0.087), ('ranksvm', 0.082), ('eak', 0.081), ('furry', 0.079), ('leaf', 0.077), ('ranks', 0.062), ('rankings', 0.061), ('attribute', 0.056), ('grass', 0.053), ('ranked', 0.053), ('plant', 0.053), ('relevance', 0.051), ('rj', 0.05), ('stripe', 0.049), ('interleaving', 0.048), ('zis', 0.048), ('rak', 0.046), ('importance', 0.045), ('ocean', 0.045), ('sun', 0.045), ('animal', 0.042), ('groups', 0.041), ('ordered', 0.036), ('preferable', 0.036), ('queries', 0.034), ('query', 0.034), ('relevant', 0.033), ('ship', 0.033), ('attributes', 0.032), ('eb', 0.03), ('inference', 0.029), ('stagewise', 0.029), ('votes', 0.028), ('items', 0.028), ('predicting', 0.027), ('tree', 0.027), ('mountain', 0.026), ('svm', 0.026), ('irrelevant', 0.026), ('levels', 0.025), ('predict', 0.025), ('hwang', 0.024), ('rim', 0.024), ('hi', 0.023), ('intractably', 0.023), ('limista', 0.023), ('rnj', 0.023), ('zjt', 0.023), ('group', 0.023), ('groupings', 0.023), ('structured', 0.023), ('retrieval', 0.023), ('lists', 0.022), ('baselines', 0.022), ('truncation', 0.022), ('rel', 0.022), ('associated', 0.021), ('decomposition', 0.021), ('ra', 0.021), ('independently', 0.021), ('interleaved', 0.021), ('ri', 0.021), ('knowing', 0.02), ('receive', 0.02), ('stage', 0.019), ('yi', 0.019), ('ea', 0.019), ('interleave', 0.018), ('guestrin', 0.018), ('unannotated', 0.018), ('ilp', 0.017), ('independent', 0.017), ('annotations', 0.017), ('xi', 0.017), ('graph', 0.016), ('recursively', 0.016), ('ihs', 0.016), ('berg', 0.016), ('sort', 0.016), ('perceive', 0.015), ('stripes', 0.015), ('triple', 0.015), ('relative', 0.015), ('baseline', 0.015), ('spain', 0.014), ('penalties', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="18-tfidf-1" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>2 0.48007959 <a title="18-tfidf-2" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>3 0.44097048 <a title="18-tfidf-3" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>4 0.082406022 <a title="18-tfidf-4" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>5 0.071044713 <a title="18-tfidf-5" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>6 0.070496365 <a title="18-tfidf-6" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>7 0.06943658 <a title="18-tfidf-7" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>8 0.069194749 <a title="18-tfidf-8" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>9 0.063493274 <a title="18-tfidf-9" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>10 0.060233437 <a title="18-tfidf-10" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>11 0.05984626 <a title="18-tfidf-11" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>12 0.059007347 <a title="18-tfidf-12" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>13 0.05617018 <a title="18-tfidf-13" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>14 0.052578483 <a title="18-tfidf-14" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>15 0.050243493 <a title="18-tfidf-15" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>16 0.050092507 <a title="18-tfidf-16" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>17 0.049545459 <a title="18-tfidf-17" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>18 0.048952844 <a title="18-tfidf-18" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>19 0.048469398 <a title="18-tfidf-19" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>20 0.046549849 <a title="18-tfidf-20" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.097), (1, -0.069), (2, -0.031), (3, 0.045), (4, 0.054), (5, 0.016), (6, -0.053), (7, 0.044), (8, -0.038), (9, 0.067), (10, 0.002), (11, 0.048), (12, -0.015), (13, 0.036), (14, 0.009), (15, -0.027), (16, 0.043), (17, 0.03), (18, -0.039), (19, -0.067), (20, 0.048), (21, 0.033), (22, -0.028), (23, 0.103), (24, -0.005), (25, -0.01), (26, 0.266), (27, 0.143), (28, 0.612), (29, -0.17), (30, -0.072), (31, -0.039), (32, 0.143), (33, 0.041), (34, -0.107), (35, 0.034), (36, 0.001), (37, -0.016), (38, 0.044), (39, 0.008), (40, 0.027), (41, -0.018), (42, 0.078), (43, -0.061), (44, -0.051), (45, -0.02), (46, 0.023), (47, -0.001), (48, -0.01), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96447623 <a title="18-lsi-1" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>2 0.94955343 <a title="18-lsi-2" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>3 0.67688197 <a title="18-lsi-3" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>4 0.20525926 <a title="18-lsi-4" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>Author: Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</p><p>Abstract: Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently, and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query, which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions without explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discrimina- tive binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improveperformance significantly over several strong baselines, while being an order of magnitude faster than exhaustively searching through all possible conjunctions.</p><p>5 0.20198822 <a title="18-lsi-5" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>6 0.19386113 <a title="18-lsi-6" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>7 0.184635 <a title="18-lsi-7" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>8 0.18243612 <a title="18-lsi-8" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>9 0.18011381 <a title="18-lsi-9" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>10 0.16866125 <a title="18-lsi-10" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>11 0.15832053 <a title="18-lsi-11" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>12 0.15707541 <a title="18-lsi-12" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>13 0.15371965 <a title="18-lsi-13" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>14 0.15351282 <a title="18-lsi-14" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>15 0.15119508 <a title="18-lsi-15" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>16 0.15091808 <a title="18-lsi-16" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>17 0.14943965 <a title="18-lsi-17" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>18 0.14770323 <a title="18-lsi-18" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>19 0.14735532 <a title="18-lsi-19" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>20 0.14723709 <a title="18-lsi-20" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.088), (16, 0.03), (26, 0.036), (33, 0.175), (67, 0.082), (69, 0.022), (77, 0.35), (80, 0.011), (87, 0.073), (99, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73330504 <a title="18-lda-1" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>2 0.71782076 <a title="18-lda-2" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>3 0.71012777 <a title="18-lda-3" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>4 0.65303248 <a title="18-lda-4" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>Author: Tim Matthews, Mark S. Nixon, Mahesan Niranjan</p><p>Abstract: We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pairwise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence ofattributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.</p><p>5 0.64852434 <a title="18-lda-5" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>6 0.59867334 <a title="18-lda-6" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>7 0.5924356 <a title="18-lda-7" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>8 0.55378139 <a title="18-lda-8" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>9 0.54493636 <a title="18-lda-9" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>10 0.53683734 <a title="18-lda-10" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>11 0.53605276 <a title="18-lda-11" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>12 0.53541124 <a title="18-lda-12" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>13 0.5330382 <a title="18-lda-13" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>14 0.53223175 <a title="18-lda-14" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>15 0.53188246 <a title="18-lda-15" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>16 0.53149438 <a title="18-lda-16" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>17 0.53051019 <a title="18-lda-17" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>18 0.52969998 <a title="18-lda-18" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>19 0.52962613 <a title="18-lda-19" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>20 0.52933973 <a title="18-lda-20" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
