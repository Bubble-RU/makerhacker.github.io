<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-322" href="#">cvpr2013-322</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</h1>
<br/><p>Source: <a title="cvpr-2013-322-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shi_PISA_Pixelwise_Image_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>Reference: <a title="cvpr-2013-322-reference" href="../cvpr2013_reference/cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 More often, such fine-grained saliency detection is also desired to have a fast runtime. [sent-2, score-0.792]
</p><p>2 Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. [sent-3, score-1.211]
</p><p>3 We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. [sent-5, score-0.425]
</p><p>4 By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. [sent-6, score-0.466]
</p><p>5 Without requiring reliable region segmentation or post–  relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. [sent-7, score-0.864]
</p><p>6 put image, the general objective is to automatically detect salient objects and assign consistently high saliency values to them, while the background part should take on zero values ideally. [sent-18, score-0.899]
</p><p>7 Though quite challenging, being able to separate salient objects from the background is a very useful tool for many computer vision and graphics applications such as object recognition [22], content-aware image retargeting [23], and image classification [20]. [sent-19, score-0.144]
</p><p>8 Driven by these recent applications, saliency detection has also evolved to  aim at assigning pixel-accurate saliency values to uniformly highlight foreground objects, going far beyond its early goal of mimicing human eye fixation. [sent-20, score-1.589]
</p><p>9 More often, such finegrained saliency detection is also desired to have a fast runtime. [sent-21, score-0.792]
</p><p>10 Without any user intervention, inferring (pixel-accurate) saliency assignment for diversified natural images is a highly ill-posed problem, because of the lack of a rigorous definition of saliency itself. [sent-23, score-1.549]
</p><p>11 To tackle this problem, a myriad of computational models [21, 11, 7, 24, 8, 15, 4] have been proposed using various principles or priors ranging from high-level biological vision [12] to low-level image properties [10, 8]. [sent-24, score-0.147]
</p><p>12 Focusing on bottom-up, low-level saliency computational models in this paper, we can classify most of the previous methods into two basic classes depending on the way the saliency cues are defined: contrast priors and background priors [24]. [sent-25, score-1.898]
</p><p>13 Contrast priors have been widely adopted in many previous methods to model the appearance contrast between foreground objects and the background. [sent-26, score-0.233]
</p><p>14 Various appearance contrast measures can be computed either in a local neighborhood of a pixel or patch [11, 15] or from an entire image context globally [5, 1]. [sent-27, score-0.217]
</p><p>15 Typical limitations of the existing methods based on contrast priors include attenuated object interior e. [sent-28, score-0.236]
</p><p>16 1(e) and ambiguous saliency detection for images with rich structures in foreground or/and background e. [sent-31, score-0.833]
</p><p>17 Complementing the prime role ofcontrast priors  in this research topic, background priors [24] have been proposed recently to exploit two interesting priors about backgrounds – boundary and connectivity priors. [sent-35, score-0.334]
</p><p>18 Saliency map comparisons on (a) four example images detected by (d) our PISA method and (e-h) a few representative contrast  prior based methods modeling only the color contrast [5, 19, 7]. [sent-37, score-0.334]
</p><p>19 Inspired by the insights and lessons from the significant amount of previous work, we target studying this challenging saliency detection problem in a more holistic manner. [sent-43, score-0.792]
</p><p>20 More specifically, this work is primarily motivated by three key principles or priors supported by psycho-  logical evidence and observations of natural images: Complementary appearance contrast in a global context. [sent-45, score-0.264]
</p><p>21 Though the color contrast is a popular saliency cue used dominantly in many methods [5, 19, 14], other influential factors do exist, which make certain pixels or regions outstanding. [sent-46, score-0.957]
</p><p>22 It is known from perceptual research [6] that different local receptive fields are associated with different kinds of visual stimuli, so local analysis regions where saliency cues are extracted should be adapted to match specific image attributes. [sent-49, score-0.877]
</p><p>23 Previous works have used the spatial variance to further (b/c) Raw saliency detection result using the color/structure contrast modulate saliency values computed from a single visual attribute (e. [sent-52, score-1.713]
</p><p>24 This spatial prior can also be generalized to consider the spatial distribution of different saliency cues, including also other useful location priors such as the center prior [15]. [sent-55, score-1.056]
</p><p>25 Another observation is that pixel-accurate saliency maps are often spatially coherent with the discontinuities well aligned to image edges. [sent-56, score-0.805]
</p><p>26 Based on these principles, we propose a generic and fast computational framework called PISA Pixelwise Image –  Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. [sent-57, score-1.211]
</p><p>27 (ii) We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. [sent-59, score-0.425]
</p><p>28 By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. [sent-60, score-0.466]
</p><p>29 (iii) Without requiring reliable region segmentation and then post-relaxation for pixelwise saliency assignment, PISA exploits an efficient edge-aware image representation and filtering technique [16] to produce spatially coherent yet edge-preserving saliency maps. [sent-61, score-1.726]
</p><p>30 It first performs saliency computation for a feature-driven, subsampled image grid, and then uses an adaptive upsampling scheme with the color image as the guidance signal to recover a fullresolution saliency map. [sent-65, score-1.645]
</p><p>31 Compared to segmentation-based  saliency methods [19], our F-PISA method reduces the computational complexity similarly by considering a coarse image grid, while having the advantage of utilizing image structural information for saliency reasoning over [19]. [sent-66, score-1.528]
</p><p>32 1, we propose PISA in this paper as a computational framework for effective and efficient pixel-accurate saliency detection, aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. [sent-70, score-2.035]
</p><p>33 In the framework, a saliency measure representing the structure contrast is proposed in addition to the well exploited color-based measure. [sent-71, score-0.891]
</p><p>34 These two measures complement each other in detecting saliency cues from different perspectives, and are combined together to give the initial saliency value. [sent-72, score-1.618]
</p><p>35 More formally, given an image I, we compute the initial saliency value for each pixel p by aggregating the two contrast measures {Uc(p) , Ug (p)} with spatial priors {Dc(p) , Dg (p) }, giving a general PI(SpA)} }fr waimthe wspoartkia as:  S˜(p)  S˜(p)  +  = Uc(p) · Dc(p) Ug(p) · Dg(p) . [sent-73, score-1.182]
</p><p>36 (1) Four terms are computed for pixel p in (1), which are: Appearance contrast terms {Uc(p) , Ug (p)}. [sent-74, score-0.139]
</p><p>37 They are evaAlupapteeda rbaansecde on tthrea general sc o{nUtrast prior principle th araet rare or infrequent visual features in a global image context  give rise to high salient values. [sent-75, score-0.196]
</p><p>38 Uc(p) denotes the rarity of pixel p with respect to the entire image in the color feature space (Sect. [sent-76, score-0.176]
</p><p>39 Rather than describing the features for pixel p by a single or just a few quantities, we use nonparametric histogram distributions to capture and represent both the color and OM features within an appropriate pixelwise adaptive neighborhood around p. [sent-82, score-0.294]
</p><p>40 They are evaluatedS p baatsieadl on othre t generally valid spatial prior yth aaret s eavliaelnutpixels tend to have a compact spatial distribution or small spatial variance in image domain, while background can distribute quite widely over the entire image. [sent-84, score-0.263]
</p><p>41 Therefore, a pixel p should not be rendered salient, if its visually similar peers have a high spatial variance. [sent-85, score-0.139]
</p><p>42 It is also often useful to integrate the center prior in this saliency reweighting process. [sent-86, score-0.846]
</p><p>43 We use Dc(p) and Dg (p) to denote such an integrative spatial reweighting term imposed on the color and structure contrast measure contrast, respectively (Sect. [sent-87, score-0.345]
</p><p>44 By fusing the two complementary saliency cues in such a pixelwise adaptive manner, the saliency detection effectiveness is significantly boosted. [sent-90, score-1.875]
</p><p>45 Though the initial saliency estimation map is already good for some applications, it is not pixel-accurate aa lnrde asdtiyll geoxohdibi ftosr many spurious nonoiss,e ist or unsmooth saliency values even within a small neighborhood. [sent-91, score-1.549]
</p><p>46 We hence employ an efficient edge-aware image filtering [16] to smooth out to generate a filtered output S,  S˜  S˜  which is spatially coherentS San tdo wgeitnhe trhatee saliency ddis ocuotpnutitnu S-, ities aligned to the guidance color image edges (Sect. [sent-92, score-0.926]
</p><p>47 In fact, the aforementioned four terms and their aggregation as in (1) present only one specific implementation of our PISA framework, other kinds of saliency cues or priors can be integrated as well. [sent-95, score-0.945]
</p><p>48 F-PISA generates saliency maps caatl tlehed detection accuracy close to that achieved by PISA, but it brings an over 18-times speedup over PISA. [sent-99, score-0.792]
</p><p>49 Color-Based Contrast Term Directly computing pixelwise color contrast in a global image context is computationally expensive, as its complexity is O(N2) with N being the number of pixels in I. [sent-103, score-0.309]
</p><p>50 They assume that if the spatial correlation is not accounted for, pixels with the similar appearance should be assigned the same saliency values. [sent-106, score-0.859]
</p><p>51 However, their strategy of defining the contrast on the color value of a single pixel individually is sensitive to noise, and it is not extensible for measuring additional attribute. [sent-107, score-0.221]
</p><p>52 In this work, we compute the color contrast based on nonparametric color distributions extracted from a locally adaptive homogeneous region [16]. [sent-108, score-0.356]
</p><p>53 Moreover, taking segments as the atomic units for saliency evaluation does not lend itself to easy integration of other appearance contrast measures. [sent-111, score-0.865]
</p><p>54 A color histogram hc(p) for pixel p is then built from the pixels q ∈ Ωp covered in the localized homogeneous region. [sent-114, score-0.221]
</p><p>55 Using the Lab color space, we quantize each color channel uniformly into 12 bins, so the color histogram hc(p) is a 36-d descriptor (see Fig. [sent-116, score-0.32]
</p><p>56 Next, we cluster pixels that share similar color histograms together using kmeans. [sent-121, score-0.162]
</p><p>57 We estimate the color-based contrast measure Uc(p) for pixel p as, ? [sent-129, score-0.162]
</p><p>58 j=1  ωj  uses the number of pixels belonging to the cluster φj as  a weight to emphasize the color contrast to bigger clusters. [sent-135, score-0.273]
</p><p>59 hc(φi) is the average color histogram of the cluster φi. [sent-136, score-0.158]
</p><p>60 In addition to the L2 distance between the two histograms, we add the color dissimilarity between the center pixels into the distance measurement. [sent-141, score-0.138]
</p><p>61 Second, we adopt a linearly-varying smoothing scheme [5] to refine the quantization-based saliency measurement. [sent-142, score-0.755]
</p><p>62 The saliency value of each cluster is replaced by the weighted average of the saliency values of visually similar clusters. [sent-143, score-1.561]
</p><p>63 Such a refinement smooths the saliency assignment to each pixel. [sent-145, score-0.794]
</p><p>64 The cluster number Kc ofthe color feature space is adaptively decided with regard to image content. [sent-146, score-0.133]
</p><p>65 Specifically, we choose the most frequently occurring color features by ensuring they cover 95% of histogram distributions of all pixels in the input image I. [sent-147, score-0.136]
</p><p>66 (i) Spatial prior-modulated structure contrast measure Ug · Dg . [sent-161, score-0.136]
</p><p>67 1(second and third rows), using color information only is not adequate to discriminatively describe and detect salient objects or parts of them from the background. [sent-165, score-0.204]
</p><p>68 Even in the event that the color uniqueness measurement gives a good saliency value to fore-  ×  ground objects, other complementary contrast measures can still be helpful in reinforcing the saliency assignment e. [sent-166, score-1.903]
</p><p>69 Based on the PISA framework, we propose a structure-based descriptor to complement the color descriptor here. [sent-170, score-0.144]
</p><p>70 The proposed structure descriptor models the image gradient distribution for pixel p by a histogram hg (p) in a rectangular region Wp. [sent-171, score-0.24]
</p><p>71 hg (p) measures the occurrence frequency of a concatenated vector consisting of the gradient orientation component and magnitude component. [sent-172, score-0.141]
</p><p>72 , Gabor [17] and LBP [9] in the image saliency detection task. [sent-179, score-0.792]
</p><p>73 Similar to the color contrast measure, kmeans is utilized to partition the OM feature space into Kg clusters indexed by {ϕ1 , . [sent-180, score-0.231]
</p><p>74 The structure contrast measure for pixel p yis { equivalent to measuring ϕi teh caot p risa grouped to, as, ? [sent-184, score-0.184]
</p><p>75 j=1 ωj  is the weight stressing the contrast against bigger clus222111 111 686  PISA and F-PISA methods on the three datasets from left to right: ASD [1], SOD [18] and SED1 [2], respectively. [sent-189, score-0.13]
</p><p>76 hg (ϕi) is the average OM histogram of the cluster ϕi. [sent-194, score-0.158]
</p><p>77 For each pixel p, we evaluate the initial spatial prior term D˜c/g (p) based on the cluster φi/ϕi that contains p from two aspects: 1) compactness of salient objects defined by the intra-cluster spatial variance, and 2) preference to the image center. [sent-202, score-0.38]
</p><p>78 (4)  is the number of pixels which are contained in the same color (or OM) cluster φi (or ϕi) with p. [sent-210, score-0.162]
</p><p>79 Since clusters exhibiting higher spatial variance or farther from the image center are quite unlikely to be salient, we compute the final spatial prior term Dc/g (p) for pixel p using a threshold T as, ni  Dc/g(p) =? [sent-218, score-0.274]
</p><p>80 Saliency Coherence Based on (1), an initial saliency estimation map S˜ is generated. [sent-227, score-0.773]
</p><p>81 Though good for certain applications, thisS Sin i -s tial saliency map does not consider the spatial coherence in its evaluation, resulting in spurious noises and non-uniform saliency assignment even for pixels close to each other. [sent-228, score-1.673]
</p><p>82 We therefore employ the efficient CLMF filtering technique [16] here to smooth out S˜ and produce a spatially coherent yet discontinuity-preserving saliency map Sati. [sent-229, score-0.85]
</p><p>83 Instead of processing the full image grid, we perform a gradient-driven subsampling of the input image I, so the saliency computation in (1) is only applied to  ×  this set of selected pixels. [sent-244, score-0.755]
</p><p>84 Tp ahtec htw oon proposed cro inmtraagset saliency measures are then computed for Il, giving a sparse saliency map S˜l. [sent-246, score-1.587]
</p><p>85 To obtain a full-sized saliency map S, we  S(p) =m1i? [sent-247, score-0.773]
</p><p>86 αpqi = which gives higher weights to the support pixels with a shorter spatial distance to pixel p. [sent-252, score-0.133]
</p><p>87 Evaluation on Benchmarks We evaluate the proposed algorithm for saliency detec-  exp(−? [sent-256, score-0.755]
</p><p>88 Extensive study for different saliency measures (CC, SC) in our method. [sent-265, score-0.814]
</p><p>89 We observe the advantage of aggregating the two complementary contrast measures. [sent-267, score-0.244]
</p><p>90 We can observe that the aggregated saliency detection achieves superior performance, as CC and SC capture saliency from different aspects, verified by the visual results in Fig. [sent-319, score-1.547]
</p><p>91 They serve as good evidences to advocate our choice in fusing complementary saliency cues. [sent-324, score-0.876]
</p><p>92 As our approach uses the spatial priors, it has problems when such priors are invalid. [sent-344, score-0.16]
</p><p>93 For example, ifthe center prior does not hold, the background regions located near the image center cannot be effectively suppressed in saliency evaluation (see Fig. [sent-345, score-0.86]
</p><p>94 For any background regions that have been assigned high saliency values from either of the contrast cues after the modulation of the spatial priors, they remain salient in the final saliency map (see Fig. [sent-349, score-1.868]
</p><p>95 Conclusion We have presented a generic framework for pixelwise saliency detection via aggregating two complementary appearance contrast measures (color and structure) with spatial priors. [sent-353, score-1.277]
</p><p>96 For future work, we plan to incorporate high-level knowledge, which could be beneficial to handle more challenging cases and investigate other kinds of saliency cues or priors to be embedded into the PISA framework. [sent-356, score-0.927]
</p><p>97 Does luminance-contrast contribute to a saliency map for overt visual attention? [sent-393, score-0.773]
</p><p>98 Spatial-temporal saliency detection using phase spectrum of quaternion fourier transform. [sent-405, score-0.792]
</p><p>99 Design and perceptual validation of performance measures for salient object segmentation. [sent-468, score-0.199]
</p><p>100 Saliency filters: contrast based filtering for salient region detection. [sent-475, score-0.272]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saliency', 0.755), ('pisa', 0.394), ('ug', 0.132), ('salient', 0.122), ('om', 0.115), ('hc', 0.109), ('pixelwise', 0.107), ('priors', 0.104), ('asd', 0.099), ('uc', 0.096), ('contrast', 0.091), ('complementary', 0.084), ('color', 0.082), ('hg', 0.082), ('kc', 0.075), ('aggregating', 0.069), ('kg', 0.066), ('clmf', 0.06), ('measures', 0.059), ('spatial', 0.056), ('dg', 0.054), ('cluster', 0.051), ('cues', 0.049), ('pixel', 0.048), ('rarity', 0.046), ('qi', 0.045), ('lbp', 0.043), ('sod', 0.041), ('contrasts', 0.041), ('priormodulated', 0.04), ('assignment', 0.039), ('fusing', 0.037), ('gabor', 0.037), ('homogeneous', 0.037), ('detection', 0.037), ('receptive', 0.036), ('reweighting', 0.035), ('rendered', 0.035), ('region', 0.032), ('adaptive', 0.032), ('descriptor', 0.031), ('dc', 0.031), ('liang', 0.03), ('cc', 0.029), ('pixels', 0.029), ('prior', 0.029), ('overcoming', 0.027), ('filtering', 0.027), ('coherent', 0.027), ('sc', 0.027), ('center', 0.027), ('runtime', 0.026), ('motivated', 0.025), ('principles', 0.025), ('histogram', 0.025), ('compact', 0.025), ('public', 0.025), ('attention', 0.024), ('rows', 0.023), ('highlight', 0.023), ('spatially', 0.023), ('limitations', 0.023), ('measure', 0.023), ('comparisons', 0.023), ('koch', 0.022), ('structure', 0.022), ('background', 0.022), ('guidance', 0.021), ('spurious', 0.021), ('clusters', 0.021), ('singapore', 0.021), ('impose', 0.02), ('uniqueness', 0.02), ('faster', 0.02), ('bigger', 0.02), ('treatment', 0.019), ('tog', 0.019), ('variance', 0.019), ('kinds', 0.019), ('datasets', 0.019), ('foreground', 0.019), ('effectiveness', 0.019), ('appearance', 0.019), ('indexed', 0.019), ('lc', 0.018), ('kmeans', 0.018), ('computational', 0.018), ('quantize', 0.018), ('aggregation', 0.018), ('perceptual', 0.018), ('map', 0.018), ('term', 0.018), ('requisite', 0.018), ('ddis', 0.018), ('integrative', 0.018), ('linliang', 0.018), ('ffroormm', 0.018), ('infrequent', 0.018), ('reinforcing', 0.018), ('attenuated', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="322-tfidf-1" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>2 0.62859434 <a title="322-tfidf-2" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>3 0.62818891 <a title="322-tfidf-3" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>4 0.61635298 <a title="322-tfidf-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.61319149 <a title="322-tfidf-5" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>Author: Long Mai, Yuzhen Niu, Feng Liu</p><p>Abstract: A variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First, different methods perform differently in saliency analysis. Second, the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neighboringpixels. To account for the dependence of aggregation on an individual image, our approach selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods.</p><p>6 0.55028826 <a title="322-tfidf-6" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>7 0.44129479 <a title="322-tfidf-7" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>8 0.32562566 <a title="322-tfidf-8" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>9 0.30314684 <a title="322-tfidf-9" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>10 0.26940697 <a title="322-tfidf-10" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>11 0.24120004 <a title="322-tfidf-11" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>12 0.22258011 <a title="322-tfidf-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.15853743 <a title="322-tfidf-13" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>14 0.1365968 <a title="322-tfidf-14" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>15 0.12986267 <a title="322-tfidf-15" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>16 0.078196123 <a title="322-tfidf-16" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>17 0.075368896 <a title="322-tfidf-17" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>18 0.071736425 <a title="322-tfidf-18" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>19 0.071208738 <a title="322-tfidf-19" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>20 0.062686242 <a title="322-tfidf-20" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, -0.229), (2, 0.631), (3, 0.334), (4, -0.156), (5, -0.039), (6, -0.015), (7, -0.087), (8, 0.072), (9, 0.024), (10, -0.027), (11, 0.041), (12, -0.033), (13, -0.001), (14, -0.015), (15, 0.036), (16, 0.003), (17, 0.007), (18, -0.018), (19, -0.025), (20, -0.018), (21, -0.018), (22, -0.018), (23, 0.029), (24, -0.013), (25, 0.014), (26, -0.033), (27, -0.008), (28, -0.008), (29, 0.008), (30, 0.016), (31, 0.013), (32, 0.003), (33, -0.018), (34, -0.022), (35, -0.024), (36, -0.001), (37, 0.002), (38, 0.018), (39, 0.011), (40, 0.002), (41, 0.004), (42, 0.004), (43, -0.033), (44, 0.018), (45, -0.01), (46, -0.002), (47, 0.01), (48, -0.003), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98851472 <a title="322-lsi-1" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>Author: Long Mai, Yuzhen Niu, Feng Liu</p><p>Abstract: A variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First, different methods perform differently in saliency analysis. Second, the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neighboringpixels. To account for the dependence of aggregation on an individual image, our approach selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods.</p><p>same-paper 2 0.96184421 <a title="322-lsi-2" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>3 0.94445086 <a title="322-lsi-3" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>4 0.91374546 <a title="322-lsi-4" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>5 0.90320987 <a title="322-lsi-5" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>6 0.89252341 <a title="322-lsi-6" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>7 0.80554599 <a title="322-lsi-7" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>8 0.78850865 <a title="322-lsi-8" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>9 0.78128201 <a title="322-lsi-9" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>10 0.57474452 <a title="322-lsi-10" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>11 0.51805514 <a title="322-lsi-11" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>12 0.49530753 <a title="322-lsi-12" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>13 0.36488694 <a title="322-lsi-13" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>14 0.35547936 <a title="322-lsi-14" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>15 0.32698134 <a title="322-lsi-15" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>16 0.31505635 <a title="322-lsi-16" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>17 0.25563309 <a title="322-lsi-17" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>18 0.22620344 <a title="322-lsi-18" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>19 0.20973857 <a title="322-lsi-19" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>20 0.20597593 <a title="322-lsi-20" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.134), (10, 0.1), (16, 0.038), (26, 0.058), (28, 0.02), (33, 0.231), (67, 0.166), (69, 0.035), (80, 0.024), (87, 0.077), (93, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89740843 <a title="322-lda-1" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>Author: Magnus Burenius, Josephine Sullivan, Stefan Carlsson</p><p>Abstract: We consider the problem of automatically estimating the 3D pose of humans from images, taken from multiple calibrated views. We show that it is possible and tractable to extend the pictorial structures framework, popular for 2D pose estimation, to 3D. We discuss how to use this framework to impose view, skeleton, joint angle and intersection constraints in 3D. The 3D pictorial structures are evaluated on multiple view data from a professional football game. The evaluation is focused on computational tractability, but we also demonstrate how a simple 2D part detector can be plugged into the framework.</p><p>2 0.893089 <a title="322-lda-2" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>Author: Georgia Gkioxari, Pablo Arbeláez, Lubomir Bourdev, Jitendra Malik</p><p>Abstract: We propose a novel approach for human pose estimation in real-world cluttered scenes, and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose, we build on the notion of poselets [4] and train highly discriminative classifiers to differentiate among arm configurations, which we call armlets. We propose a rich representation which, in addition to standardHOGfeatures, integrates the information of strong contours, skin color and contextual cues in a principled manner. Unlike existing methods, we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset, where critical visual phenomena, such as occlusion, truncation, multiple instances and clutter are the norm. Our approach outperforms Yang and Ramanan [26], the state-of-the-art technique, with an improvement from 29.0% to 37.5% PCP accuracy on the arm keypoint prediction task, on this new pose estimation dataset.</p><p>3 0.89006865 <a title="322-lda-3" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>same-paper 4 0.8879174 <a title="322-lda-4" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>5 0.88676798 <a title="322-lda-5" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>6 0.8856917 <a title="322-lda-6" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>7 0.88332921 <a title="322-lda-7" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>8 0.88082469 <a title="322-lda-8" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>9 0.88045955 <a title="322-lda-9" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>10 0.87998724 <a title="322-lda-10" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>11 0.87594855 <a title="322-lda-11" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>12 0.87355602 <a title="322-lda-12" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>13 0.87133497 <a title="322-lda-13" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>14 0.87124604 <a title="322-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.86908299 <a title="322-lda-15" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>16 0.86900377 <a title="322-lda-16" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>17 0.86531973 <a title="322-lda-17" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>18 0.86493832 <a title="322-lda-18" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>19 0.86086023 <a title="322-lda-19" href="./cvpr-2013-Monocular_Template-Based_3D_Reconstruction_of_Extensible_Surfaces_with_Local_Linear_Elasticity.html">289 cvpr-2013-Monocular Template-Based 3D Reconstruction of Extensible Surfaces with Local Linear Elasticity</a></p>
<p>20 0.85693669 <a title="322-lda-20" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
