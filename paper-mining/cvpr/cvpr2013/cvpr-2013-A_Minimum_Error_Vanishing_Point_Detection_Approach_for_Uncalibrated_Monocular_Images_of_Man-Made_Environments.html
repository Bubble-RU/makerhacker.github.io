<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-19" href="#">cvpr2013-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</h1>
<br/><p>Source: <a title="cvpr-2013-19-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xu_A_Minimum_Error_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>Reference: <a title="cvpr-2013-19-reference" href="../cvpr2013_reference/cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. [sent-6, score-0.381]
</p><p>2 We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. [sent-7, score-0.964]
</p><p>3 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. [sent-9, score-1.338]
</p><p>4 Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. [sent-10, score-0.391]
</p><p>5 Introduction In man-made environments, structural objects such as building facades and road lane marks frequently present sets of parallel lines that intersect at points at infinity in the world, whose projections in an image are called vanishing points (VPs). [sent-14, score-0.389]
</p><p>6 In particular, VPs uniquely determine the orientations of parallel line clusters in the world. [sent-17, score-0.352]
</p><p>7 Ideally, assuming perfect imaging condition and line segment extraction, parallel lines should intersect at the corresponding VPs. [sent-22, score-0.569]
</p><p>8 However, in the realworld, there exists pixel noise, image distortion, discretization error, and line segment extraction error, which make the problem much more challenging. [sent-23, score-0.501]
</p><p>9 The problem becomes even harder when camera parameters or motion cues are unavailable, or the scene becomes complex and does not sat-  isfy the Manhattan world assumption [3] which considers only three mutually parallel line clusters. [sent-24, score-0.41]
</p><p>10 In this paper, we present a novel minimum error vanishing point detection algorithm for uncalibrated monocular images of man-made environments, without the Manhattan world assumption. [sent-25, score-0.611]
</p><p>11 We tackle the VP detection problem by modeling the measurement error in the line segment extraction and minimizing its impact on the ultimate error in VP estimation. [sent-27, score-0.806]
</p><p>12 1 1 13 3 37 7 746 4  First, a novel consistency measure is developed, which evaluates the consistency between a line segment and a hypothesized VP. [sent-29, score-0.996]
</p><p>13 Unlike existing hand-crafted models [4, 6, 13, 17], we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error. [sent-30, score-0.889]
</p><p>14 This new consistency measure is used to improve the assignment of line segments to corresponding hypothesized VPs. [sent-31, score-0.784]
</p><p>15 Second, a novel minimum error vanishing point estimation method is presented. [sent-32, score-0.466]
</p><p>16 Given a cluster of line segments, the extension lines of any line segment pair intersect at a hypothesized VP, establishing a minimal solution. [sent-33, score-1.052]
</p><p>17 The error prop-  agated from line segment endpoints towards the final VP estimation is minimized by optimally weighing all minimal solutions. [sent-35, score-0.753]
</p><p>18 Our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. [sent-36, score-0.391]
</p><p>19 The error analysis quantitatively indicates to what degree each line segment pair should be trusted. [sent-37, score-0.595]
</p><p>20 An early class of the VP detection algorithms, assuming known camera intrinsic parameters, relies on the Hough transform of the line segments on the Gaussian sphere [2] and clusters line segments in a bottom-up manner using their orientation votes. [sent-42, score-0.99]
</p><p>21 These approaches usually do not handle noise and outliers very well, and suffer from the suboptimality, which leads to misclassification of line segments [10]. [sent-43, score-0.425]
</p><p>22 Many recent work assumes the Manhattan world [3], where only three mutually orthogonal vanishing directions are considered: one vertical and two horizontal. [sent-45, score-0.324]
</p><p>23 Tardif [13] proposes to use a variant of RANSAC algorithm, called J-linkage, to generate line segment clusters. [sent-53, score-0.476]
</p><p>24 1, our algorithm is similar to [13] in the overall structure, but, uses our new consistency measure and minimum error VP estimation, which results in substantial improvement in accuracy. [sent-56, score-0.367]
</p><p>25 , [1] propose an algorithm to globally maximize the number of inlier line segments for all VPs. [sent-62, score-0.425]
</p><p>26 Our  approach is different because line segment pairs are optimally weighted to minimize the VP error. [sent-63, score-0.504]
</p><p>27 Our reasoning is that the uniform use of more inlier segments [1] does not necessarily guarantee more accurate VP estimation (imagine a lot of short, noisy line segments versus a few long, certain line segments. [sent-64, score-0.894]
</p><p>28 ) Most aforementioned works require a consistency measure that assesses the degree to which geometric primitives such as edges and line segments are consistent with hypothesized VPs. [sent-65, score-0.803]
</p><p>29 We denote any line segment by its two endpoints, l = [e1, e2] = [u1, v1, u2 , v2] . [sent-82, score-0.476]
</p><p>30 lˆ  segment l, we define as its corresponding extension line in the homogeneous image coordinate system lˆ = [a, b, c] . [sent-89, score-0.476]
</p><p>31 For initialization, first, line segments are extracted from images. [sent-101, score-0.425]
</p><p>32 Let us assume that total N line segments are extracted, which are denoted as {l1, . [sent-102, score-0.425]
</p><p>33 While any line segment wehxtircahc atioren approach can be used},. [sent-106, score-0.476]
</p><p>34 Then, an initial set of VPs are computed by clus-  ×  tering line segments using our novel consistency measure (see Sec. [sent-108, score-0.629]
</p><p>35 In this work, the J-linkage algorithm [13] is used to cluster line segments to respective VP clusters in the following manner: first, M line segment pairs (e. [sent-111, score-0.979]
</p><p>36 In the M-like step, for each cluster of line segments, we compute its corresponding VP using a novel minimum error vanishing point estimation method (see Sec. [sent-119, score-0.775]
</p><p>37 In the E-like step, given a set of hypothesized VPs, we reassign each line segment to a VP such that the consistency measure (in Sec. [sent-122, score-0.835]
</p><p>38 2) between the line segment and the VP  is maximal and greater than a threshold (otherwise, assign to an outlier cluster). [sent-124, score-0.503]
</p><p>39 The resulting assignment forms a new line segment clusters for the M-like step. [sent-125, score-0.515]
</p><p>40 In the following sections, we describe both the justification and algorithmic details for the newly developed consistency measure and minimum error vanishing point estimation in Sec. [sent-127, score-0.67]
</p><p>41 Consistency Measure A consistency measure c(l, v), between a line segment land a hypothesized VP v, is one of the most important algorithmic component required by most VP detection algorithms. [sent-134, score-0.863]
</p><p>42 While there are existing consistency measures [4, 6, 13, 17] which work fairly well in practice, however, most of them are hand-crafted; and do not encode the measurement error during line segment extraction and its impact on the VP estimation in a principled manner. [sent-136, score-0.874]
</p><p>43 To provide the motivation for our new probabilistic consistency measure, we briefly review existing consistency measures and their potential limitations first. [sent-137, score-0.322]
</p><p>44 This formulation is biased against VPs far away from the line segment (along the direction of the line segment). [sent-139, score-0.746]
</p><p>45 2(a), lshould have higher  consistency with v1 than v2 and v3; and c(l, v3) should be zero since the projection of v3 on resides on litself while any finite line segment in the world should not pass through its own VP. [sent-141, score-0.704]
</p><p>46 This would lead to the frequent incorrect assignment of a line segment to a nearby VP; and degrade the detection of VPs at infinity or far away. [sent-143, score-0.543]
</p><p>47 [4] use the angular deviation θ between the line segment and the line connecting the centroid e of the line segment and the VP. [sent-146, score-1.222]
</p><p>48 d from the endpoints of lto the line connecting the VP and the line segment centroid e. [sent-155, score-0.82]
</p><p>49 This formulation favors shorter line segments such as l2 in Fig. [sent-156, score-0.425]
</p><p>50 Another common issue with the aforementioned models is that they do not encode the impact of the length L of the line segments on the VP detection. [sent-158, score-0.514]
</p><p>51 2(b), shortening the line segment without changing e will not change its consistency measure. [sent-160, score-0.637]
</p><p>52 However, intuitively, very short line segments are frequently noisy, hence, their contribution should be constrained compared to the long segments. [sent-161, score-0.425]
</p><p>53 1  Probabilistic Consistency Measure  We propose a new consistency measure which models the measurement error in line segment extraction and its impact on the VP estimation. [sent-165, score-0.891]
</p><p>54 For each line segment, the proposed  model builds a probabilistic consistency distribution over all possible VP locations, and explicitly captures the impact of segment length as well. [sent-166, score-0.707]
</p><p>55 3(a) in which an extracted (solid black) line segment l = [e1, e2] is shown with the measurement uncertainty in its endpoint locations illustrated with ellipses around them. [sent-168, score-0.657]
</p><p>56 Following the endpoint distribution, any possible true line segment l? [sent-169, score-0.523]
</p><p>57 While the location error of e1 and e2 depends on the line segment extraction algorithm and sensor noise, in this work, we use a widely accepted isotropic Gaussian distribution1 [8]. [sent-171, score-0.592]
</p><p>58 }os s⊂i Lle as uthee l isneet of line segments which are collinear w=ith { v. [sent-174, score-0.449]
</p><p>59 Longer line segments and smaller σ produce narrower ridges, meaning more certainty about the location of the hypothesized vanishing point. [sent-187, score-0.894]
</p><p>60 segment is computed as the product of two probabilities of hypothesized end points with respect to two 2D uncertainty ellipses. [sent-188, score-0.412]
</p><p>61 Then any possible endpoints of the true line segment can be presented as e? [sent-196, score-0.55]
</p><p>62 Ferreom f( (1), we can observe that our formulation allows us to encode the measurement error σ explicitly and captures the intuitive impact of line length L, which provides a more sound framework compared to empirical set-ups by other approaches [4, 6, 13, 17]. [sent-222, score-0.477]
</p><p>63 Longer line segments produce narrower ridges, meaning more certainty about the location of the hypothesized VP. [sent-226, score-0.637]
</p><p>64 , more accurate line segment extraction and less image noise, produces higher confidence about the VP location. [sent-229, score-0.501]
</p><p>65 1, we associate each line segment with one of the hypothesized VPs  or outlier class, using the consistency measure in (1). [sent-232, score-0.862]
</p><p>66 , vm} and line segment l, we dhyetpeortmheinseiz ethde VVPPs t Vha t= =l belongs to by:  ζ(l;V) =? [sent-236, score-0.476]
</p><p>67 1  Maximum Likelihood (ML) Estimation  By (2), we form clusters of line segments {Si}. [sent-242, score-0.464]
</p><p>68 ally due to short line segments (see Fig 4) and subset of line segments that are close to parallel to each other. [sent-263, score-0.893]
</p><p>69 2  Minimum Error Estimation  To resolve the aforementioned issues, we propose a novel minimum error VP estimation by optimally weighting minimal solutions produced by line segment pairs. [sent-267, score-0.766]
</p><p>70 Given a line segment pair (lj , lk) ∈ Li2, j k, the intersection of corresponding and lˆ ∈k is a minimal solution as a hypothesized VP. [sent-269, score-0.693]
</p><p>71 It encodes the relative location of line segments in the pair, line segment length, image noise level, segmentation error, etc. [sent-293, score-0.901]
</p><p>72 An illustration of the intuition behind the minimum error vanishing point estimation. [sent-318, score-0.44]
</p><p>73 (7)  From (7), apparently, each line segment pair has a different impact on the covariance of the VP estimation. [sent-329, score-0.615]
</p><p>74 6, even though the pair (l1, l2) have longer line segments, since they are almost parallel, the covariance of the intersection is larger than that from the pair (l1, l3) , which has shorter line segments. [sent-331, score-0.658]
</p><p>75 5(b) visualizes the sorted trace of v˜(j,k) from different pairs of line segments (in the same cluster) shown in Fig. [sent-333, score-0.534]
</p><p>76 This indicates that some line segment pairs are more important than others. [sent-335, score-0.476]
</p><p>77 5(a), the line segment pair with the least trace (i. [sent-337, score-0.567]
</p><p>78 , the most trustable) line segment pair is highlighted with a larger width. [sent-339, score-0.504]
</p><p>79 Uniformly using more line segments does not necessarily guarantee more accuracy. [sent-340, score-0.443]
</p><p>80 Given the line segment cluster S with n line segments, define the set of all line segment pairs, Γ = {(lj , lk) |lj , lk ∈ dS,e j tkhe}. [sent-342, score-1.323]
</p><p>81 − w < 0,  and  (9)  1(Tn−1)n/2w = 1,  where A is an n(n − 1)/2 n(n − 1)/2 matrix with its positive diagonal enle −me 1n)t/s2 A ×( an(, an) −= 1 /tr2(Σ m˜ va(tjr,ki)x x) , iwthhe irtse a is the index of w corresponding to the line segment pair (lj , lk) . [sent-357, score-0.504]
</p><p>82 The off-diagonal elements of A model the correlation between two line segment pairs. [sent-358, score-0.476]
</p><p>83 ) With the minimum error VP estimation in (11) and the consistency measure in (1), we complete our algorithm described in Sec. [sent-382, score-0.393]
</p><p>84 For line segment detection, [16] with sub-pixel accuracy is used. [sent-387, score-0.476]
</p><p>85 )Foreachimage,itshows  clusters of line segments (with different colors), and the computed horizon (solid purple) compared with the ground truth (dashed cyan. [sent-391, score-0.629]
</p><p>86 We set the consistency measure threshold η = which is the consistency measure at one standard deviation away from the mean (see (1)). [sent-393, score-0.408]
</p><p>87 In J-linkage, we choose upto M = 3000 line segment pairs for VP initialization. [sent-394, score-0.476]
</p><p>88 Manually annotated line segments, ground truth VPs and camera intrinsic parameters are provided. [sent-398, score-0.322]
</p><p>89 Many scenes in ECD do not satisfy the Manhattan world assumption as more than two horizontal vanishing directions exist and/or buildings with irregular non-box shapes without clear straight lines are more common. [sent-400, score-0.386]
</p><p>90 Manually annotated line segments and ground truth  √21πσe−21,  VPs are provided but camera intrinsic parameters are missing. [sent-401, score-0.477]
</p><p>91 It can be observed that our algorithm achieves very accurate line segment clustering and horizon detection for both indoor and outdoor scenes. [sent-411, score-0.669]
</p><p>92 Since the horizon should be orthogonal with the line connecting the zenith and the principal point, by assuming the image center as the principal point and the focal length f ∈ [0. [sent-417, score-0.535]
</p><p>93 This procedure is similar to [14], except that we use the inverse of the trace of each VP’s covariance as the weight, instead of empirically choosing the number of associated line segments as the weight in [14]. [sent-423, score-0.55]
</p><p>94 Following [14, 15, 17], we define the horizon error as the maximum distance from the computed horizon to the ground truth horizon in the image, normalized by the image height. [sent-424, score-0.611]
</p><p>95 The x-axis value is the horizon error and the y-axis value is the share of the images that have less horizon error than the corresponding x-value. [sent-427, score-0.512]
</p><p>96 However, our algorithm is still able to cluster small piecewise line segments and compute a fairly accurate horizon. [sent-456, score-0.464]
</p><p>97 Our approach advances the state-of-theart using a new consistency measure and a minimum error VP estimation approach. [sent-467, score-0.393]
</p><p>98 Globally optimal line clustering and vanishing point estimation in Manhattan world. [sent-485, score-0.573]
</p><p>99 Performance evaluation and analysis of vanishing point detection techniques. [sent-533, score-0.305]
</p><p>100 Lsd: A fast line segment detector with a false detection control. [sent-566, score-0.504]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vp', 0.618), ('vps', 0.309), ('line', 0.27), ('vanishing', 0.257), ('segment', 0.206), ('manhattan', 0.178), ('horizon', 0.165), ('consistency', 0.161), ('segments', 0.155), ('hypothesized', 0.155), ('yud', 0.133), ('ecd', 0.133), ('error', 0.091), ('lj', 0.088), ('endpoints', 0.074), ('minimum', 0.072), ('world', 0.067), ('eurasian', 0.066), ('trace', 0.063), ('lk', 0.062), ('covariance', 0.062), ('cities', 0.057), ('uncertainty', 0.051), ('impact', 0.049), ('endpoint', 0.047), ('measurement', 0.046), ('measure', 0.043), ('parallel', 0.043), ('uncalibrated', 0.041), ('uv', 0.041), ('york', 0.04), ('urban', 0.04), ('clusters', 0.039), ('cluster', 0.039), ('infinity', 0.039), ('latest', 0.038), ('tardif', 0.037), ('monocular', 0.035), ('minimal', 0.034), ('narrower', 0.033), ('reconciled', 0.033), ('reconciling', 0.033), ('ml', 0.033), ('focal', 0.032), ('tilt', 0.031), ('camera', 0.03), ('tan', 0.029), ('detection', 0.028), ('jacobian', 0.028), ('optimally', 0.028), ('pair', 0.028), ('bazin', 0.027), ('kosecka', 0.027), ('tretyak', 0.027), ('zenith', 0.027), ('anthony', 0.027), ('kitware', 0.027), ('outlier', 0.027), ('ransac', 0.027), ('sorted', 0.027), ('competing', 0.027), ('estimation', 0.026), ('denis', 0.026), ('hoogs', 0.026), ('lines', 0.025), ('intersect', 0.025), ('maximum', 0.025), ('extraction', 0.025), ('ridges', 0.025), ('twofold', 0.025), ('preference', 0.024), ('weighing', 0.024), ('certainty', 0.024), ('collinear', 0.024), ('vu', 0.023), ('robot', 0.023), ('intrinsic', 0.022), ('sphere', 0.021), ('length', 0.021), ('public', 0.021), ('si', 0.02), ('convention', 0.02), ('oh', 0.02), ('weighting', 0.02), ('vm', 0.02), ('point', 0.02), ('visualizes', 0.019), ('remote', 0.019), ('agency', 0.019), ('satisfy', 0.019), ('aforementioned', 0.019), ('ellipses', 0.019), ('necessarily', 0.018), ('parsing', 0.018), ('navigation', 0.018), ('horizontal', 0.018), ('environments', 0.018), ('pan', 0.018), ('distortion', 0.018), ('illustrated', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="19-tfidf-1" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>2 0.4582476 <a title="19-tfidf-2" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>3 0.28017759 <a title="19-tfidf-3" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>4 0.13591987 <a title="19-tfidf-4" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>Author: Srikumar Ramalingam, Jaishanker K. Pillai, Arpit Jain, Yuichi Taguchi</p><p>Abstract: Junctions are strong cues for understanding the geometry of a scene. In this paper, we consider the problem of detecting junctions and using them for recovering the spatial layout of an indoor scene. Junction detection has always been challenging due to missing and spurious lines. We work in a constrained Manhattan world setting where the junctions are formed by only line segments along the three principal orthogonal directions. Junctions can be classified into several categories based on the number and orientations of the incident line segments. We provide a simple and efficient voting scheme to detect and classify these junctions in real images. Indoor scenes are typically modeled as cuboids and we formulate the problem of the cuboid layout estimation as an inference problem in a conditional random field. Our formulation allows the incorporation of junction features and the training is done using structured prediction techniques. We outperform other single view geometry estimation methods on standard datasets.</p><p>5 0.092232838 <a title="19-tfidf-5" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>Author: S. Hussain Raza, Matthias Grundmann, Irfan Essa</p><p>Abstract: We present a novel algorithm for estimating the broad 3D geometric structure of outdoor video scenes. Leveraging spatio-temporal video segmentation, we decompose a dynamic scene captured by a video into geometric classes, based on predictions made by region-classifiers that are trained on appearance and motion features. By examining the homogeneity of the prediction, we combine predictions across multiple segmentation hierarchy levels alleviating the need to determine the granularity a priori. We built a novel, extensive dataset on geometric context of video to evaluate our method, consisting of over 100 groundtruth annotated outdoor videos with over 20,000 frames. To further scale beyond this dataset, we propose a semisupervised learning framework to expand the pool of labeled data with high confidence predictions obtained from unlabeled data. Our system produces an accurate prediction of geometric context of video achieving 96% accuracy across main geometric classes.</p><p>6 0.088821642 <a title="19-tfidf-6" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>7 0.086449817 <a title="19-tfidf-7" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>8 0.085814476 <a title="19-tfidf-8" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>9 0.080877066 <a title="19-tfidf-9" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>10 0.079515085 <a title="19-tfidf-10" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>11 0.078507558 <a title="19-tfidf-11" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>12 0.065274946 <a title="19-tfidf-12" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>13 0.062406506 <a title="19-tfidf-13" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>14 0.059049033 <a title="19-tfidf-14" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>15 0.058969397 <a title="19-tfidf-15" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>16 0.058388185 <a title="19-tfidf-16" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>17 0.058184713 <a title="19-tfidf-17" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>18 0.05817344 <a title="19-tfidf-18" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>19 0.055898402 <a title="19-tfidf-19" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>20 0.054884795 <a title="19-tfidf-20" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, 0.06), (2, 0.014), (3, -0.006), (4, 0.041), (5, -0.018), (6, -0.001), (7, 0.016), (8, -0.037), (9, 0.022), (10, 0.061), (11, 0.03), (12, 0.033), (13, -0.047), (14, -0.069), (15, -0.013), (16, 0.082), (17, 0.125), (18, -0.055), (19, 0.009), (20, 0.001), (21, 0.061), (22, -0.01), (23, -0.007), (24, 0.111), (25, -0.064), (26, -0.087), (27, -0.088), (28, -0.003), (29, 0.07), (30, -0.079), (31, 0.105), (32, -0.123), (33, 0.308), (34, -0.153), (35, 0.193), (36, -0.124), (37, 0.001), (38, 0.11), (39, 0.147), (40, 0.16), (41, -0.18), (42, 0.066), (43, 0.107), (44, -0.051), (45, -0.054), (46, -0.081), (47, 0.143), (48, -0.057), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96306288 <a title="19-lsi-1" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>2 0.9240669 <a title="19-lsi-2" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>3 0.7168718 <a title="19-lsi-3" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>4 0.68196249 <a title="19-lsi-4" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>Author: Srikumar Ramalingam, Jaishanker K. Pillai, Arpit Jain, Yuichi Taguchi</p><p>Abstract: Junctions are strong cues for understanding the geometry of a scene. In this paper, we consider the problem of detecting junctions and using them for recovering the spatial layout of an indoor scene. Junction detection has always been challenging due to missing and spurious lines. We work in a constrained Manhattan world setting where the junctions are formed by only line segments along the three principal orthogonal directions. Junctions can be classified into several categories based on the number and orientations of the incident line segments. We provide a simple and efficient voting scheme to detect and classify these junctions in real images. Indoor scenes are typically modeled as cuboids and we formulate the problem of the cuboid layout estimation as an inference problem in a conditional random field. Our formulation allows the incorporation of junction features and the training is done using structured prediction techniques. We outperform other single view geometry estimation methods on standard datasets.</p><p>5 0.61502236 <a title="19-lsi-5" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>6 0.46673882 <a title="19-lsi-6" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>7 0.46443167 <a title="19-lsi-7" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>8 0.4411808 <a title="19-lsi-8" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>9 0.43954262 <a title="19-lsi-9" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>10 0.35660106 <a title="19-lsi-10" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>11 0.34872356 <a title="19-lsi-11" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>12 0.34868717 <a title="19-lsi-12" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>13 0.33598256 <a title="19-lsi-13" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>14 0.322041 <a title="19-lsi-14" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>15 0.32178247 <a title="19-lsi-15" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>16 0.29762176 <a title="19-lsi-16" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>17 0.29401511 <a title="19-lsi-17" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>18 0.29235011 <a title="19-lsi-18" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>19 0.29067278 <a title="19-lsi-19" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>20 0.27572834 <a title="19-lsi-20" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.107), (16, 0.027), (26, 0.06), (28, 0.031), (33, 0.212), (45, 0.235), (67, 0.04), (69, 0.065), (87, 0.114)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83796591 <a title="19-lda-1" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>2 0.82808256 <a title="19-lda-2" href="./cvpr-2013-Bayesian_Grammar_Learning_for_Inverse_Procedural_Modeling.html">57 cvpr-2013-Bayesian Grammar Learning for Inverse Procedural Modeling</a></p>
<p>Author: Andelo Martinovic, Luc Van_Gool</p><p>Abstract: Within the fields of urban reconstruction and city modeling, shape grammars have emerged as a powerful tool for both synthesizing novel designs and reconstructing buildings. Traditionally, a human expert was required to write grammars for specific building styles, which limited the scope of method applicability. We present an approach to automatically learn two-dimensional attributed stochastic context-free grammars (2D-ASCFGs) from a set of labeled buildingfacades. To this end, we use Bayesian Model Merging, a technique originally developed in the field of natural language processing, which we extend to the domain of two-dimensional languages. Given a set of labeled positive examples, we induce a grammar which can be sampled to create novel instances of the same building style. In addition, we demonstrate that our learned grammar can be used for parsing existing facade imagery. Experiments conducted on the dataset of Haussmannian buildings in Paris show that our parsing with learned grammars not only outperforms bottom-up classifiers but is also on par with approaches that use a manually designed style grammar.</p><p>3 0.77256644 <a title="19-lda-3" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>Author: Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</p><p>Abstract: Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently, and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query, which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions without explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discrimina- tive binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improveperformance significantly over several strong baselines, while being an order of magnitude faster than exhaustively searching through all possible conjunctions.</p><p>4 0.76118988 <a title="19-lda-4" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>Author: Michel Antunes, João P. Barreto</p><p>Abstract: This article presents a new global approach for detecting vanishing points and groups of mutually orthogonal vanishing directions using lines detected in images of man-made environments. These two multi-model fitting problems are respectively cast as Uncapacited Facility Location (UFL) and Hierarchical Facility Location (HFL) instances that are efficiently solved using a message passing inference algorithm. We also propose new functions for measuring the consistency between an edge and aputative vanishingpoint, and for computing the vanishing point defined by a subset of edges. Extensive experiments in both synthetic and real images show that our algorithms outperform the state-ofthe-art methods while keeping computation tractable. In addition, we show for the first time results in simultaneously detecting multiple Manhattan-world configurations that can either share one vanishing direction (Atlanta world) or be completely independent.</p><p>5 0.76001853 <a title="19-lda-5" href="./cvpr-2013-Is_There_a_Procedural_Logic_to_Architecture%3F.html">228 cvpr-2013-Is There a Procedural Logic to Architecture?</a></p>
<p>Author: Julien Weissenberg, Hayko Riemenschneider, Mukta Prasad, Luc Van_Gool</p><p>Abstract: Urban models are key to navigation, architecture and entertainment. Apart from visualizing fa ¸cades, a number of tedious tasks remain largely manual (e.g. compression, generating new fac ¸ade designs and structurally comparing fa c¸ades for classification, retrieval and clustering). We propose a novel procedural modelling method to automatically learn a grammar from a set of fa c¸ades, generate new fa ¸cade instances and compare fa ¸cades. To deal with the difficulty of grammatical inference, we reformulate the problem. Instead of inferring a compromising, onesize-fits-all, single grammar for all tasks, we infer a model whose successive refinements are production rules tailored for each task. We demonstrate our automatic rule inference on datasets of two different architectural styles. Our method supercedes manual expert work and cuts the time required to build a procedural model of a fa ¸cade from several days to a few milliseconds.</p><p>6 0.74236256 <a title="19-lda-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.73888296 <a title="19-lda-7" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>8 0.73789948 <a title="19-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.73728621 <a title="19-lda-9" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>10 0.7367295 <a title="19-lda-10" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>11 0.73540086 <a title="19-lda-11" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>12 0.73409718 <a title="19-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.73336929 <a title="19-lda-13" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>14 0.73307353 <a title="19-lda-14" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>15 0.73150259 <a title="19-lda-15" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>16 0.72876042 <a title="19-lda-16" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>17 0.7285639 <a title="19-lda-17" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>18 0.7284835 <a title="19-lda-18" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>19 0.72812837 <a title="19-lda-19" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>20 0.72783637 <a title="19-lda-20" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
