<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-28" href="#">cvpr2013-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</h1>
<br/><p>Source: <a title="cvpr-2013-28-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>Reference: <a title="cvpr-2013-28-reference" href="../cvpr2013_reference/cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu an l  Abstract The problem of describing images through natural language has gained importance in the computer vision community. [sent-4, score-0.235]
</p><p>2 Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. [sent-5, score-0.699]
</p><p>3 In contrast, describing videos with natural language is a less studied problem. [sent-6, score-0.341]
</p><p>4 In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. [sent-7, score-0.636]
</p><p>5 We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. [sent-8, score-1.35]
</p><p>6 We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level. [sent-9, score-0.692]
</p><p>7 Introduction The problem of generating natural language descriptions of images and videos has been steadily gaining prominence in the computer vision community. [sent-11, score-0.694]
</p><p>8 A number of papers have been proposed to leverage latent topic models on lowlevel features [4, 6, 7, 22, 32], for example. [sent-12, score-0.231]
</p><p>9 Figure 1: A framework of our hybrid system showing a video being processed through our pipeline and described by a few natural language sentences. [sent-18, score-0.355]
</p><p>10 To date, the most common approach to such lingual description of images has been to model the joint distribution over lowlevel image features and language, typically nouns. [sent-22, score-0.608]
</p><p>11 [4] and subsequent extensions [6, 7, 11, 22, 32] jointly model image features (predominantly SIFT and HOG derivatives) and language words as mixed memberships over latent topics with considerable success. [sent-24, score-0.405]
</p><p>12 [18] and TagProp [12], rely on large annotated sets to generate descriptions from similar samples. [sent-26, score-0.315]
</p><p>13 These methods have demonstrated a capability of lingual description on images at varying levels, but they have two main limitations. [sent-27, score-0.608]
</p><p>14 222666333422  Alternatively, a second class of approaches to lingual description of images directly seeks a set of high-level concepts, typically objects but possibly others such as scene categories. [sent-32, score-0.608]
</p><p>15 Despite being able to guarantee the semantic veracity of the generated lingual description, these methods have found limited use due to the overall complexity of object detection in-the-wild and its constituent limitations (i. [sent-34, score-0.56]
</p><p>16 Namely, our model leverages the power of low-level joint distributions over video features and language by treating  them as a set of lingual proposals which are subsequently filtered by a set of mid-level concept detectors. [sent-38, score-1.0]
</p><p>17 We use multimodal latent topic models to find a proposal distribution over some training vocabulary of textual words [4, 7], then select the most probable keywords as potential subjects, objects and verbs through a natural language dependency grammar and part-of-speech tagging. [sent-42, score-0.828]
</p><p>18 Second, in a top down fashion, we detect and stitch together a set of concepts, such as “artificial rock wall” and “person climbing wall” similar to [26], which are then converted to lingual descriptions through a tripartite graph template. [sent-43, score-1.109]
</p><p>19 Third, for high level semantic verification, we relate the predicted caption keywords with the detected concepts to produce a ranked set ofwell formed natural language sentences. [sent-44, score-0.701]
</p><p>20 Our semantic verification step is independent of any computer vision framework and works by measuring the number of inversions between two ranked lists of predicted keywords and detected concepts both being conditional on their respective learned topic multinomials. [sent-45, score-0.698]
</p><p>21 Images Recent work in [9, 16, 34] is  mainly focused on generating fluent descriptions of a single image—images not videos. [sent-48, score-0.353]
</p><p>22 Most related work in vision has focused only on the activity classification side: example methods using topic models for activities are the hidden topic Markov model [33] and frame-by-frame Markov topic models [13], but these methods do not model language and visual topics jointly. [sent-50, score-0.847]
</p><p>23 A recent activity classification paper of relevance is the Action Bank method [25], which ties high-level actions to constituent low-level action detections, but it does not include any language generation framework. [sent-51, score-0.27]
</p><p>24 , and generate language description by template filling; [19] additionally uses externally mined language data to help rank the best subject-verbobject triplet. [sent-57, score-0.615]
</p><p>25 We, in contrast, focus on descriptions of general videos (e. [sent-60, score-0.421]
</p><p>26 , from YouTube) directly through bottom-up visual fea-  ture translations to text and top-down concept detections. [sent-62, score-0.208]
</p><p>27 We leverage both detailed object annotations and human lingual descriptions. [sent-63, score-0.544]
</p><p>28 First, we use an asymmetric Dirichlet prior, α for the document level topic proportions θd following [3 1]  unlike the symmetric one in [4]. [sent-66, score-0.306]
</p><p>29 2, D is the number of documents, each consisting of a video and a lingual description (the text is only available during training). [sent-68, score-0.74]
</p><p>30 The number ofdiscrete visual words and lingual words per video document d are N and M. [sent-69, score-0.802]
</p><p>31 The parameters for corpus level topic multinomials over visual words are ρ1:K. [sent-70, score-0.429]
</p><p>32 The param222666333533  eters for corpus level topic multinomials over textual words are β1:K—only the training instances of these parameters are used for keyword prediction. [sent-71, score-0.524]
</p><p>33 The indicator variables for choosing a topic are {zd,n} and {yd,m}; wd,m is the text cwhoorods iant position m ien {vzideo} “ adnodcu {myent”} d; wwith vocabulary size V . [sent-72, score-0.267]
</p><p>34 The free multinomial parameters  of the variation|aαl topic . [sent-85, score-0.223]
</p><p>35 =2 4 1 851470  Table 1: Average word prediction 1-gram recall for different topic models with 200 topics when the full corpus is used. [sent-140, score-0.317]
</p><p>36 The figures are obtained by topic modeling on the entire corpus of multimedia documents (video with corresponding lingual description). [sent-147, score-0.749]
</p><p>37 Middle Level: Concepts to Language The middle level is a top-down approach that detects concepts sparsely throughout the video, matches them over time, which we call stitching, and relates them to a tripartite template graph for generating language output. [sent-153, score-0.733]
</p><p>38 1  Concept Detectors  Instead of using publicly available object detectors from datasets like the PASCAL VOC [8], or training independent object detectors for objects such as microphone, we build the concept object detectors like microphone with upper body, group of people etc. [sent-156, score-0.408]
</p><p>39 A concept detector captures richer semantic information (from object, action and scene level) than object detectors, and usually reduces the visual complexity compared to individual objects, which requires less training examples for an accurate detector. [sent-158, score-0.246]
</p><p>40 These con-  cept detectors are closely related to Sadeghi and Farhadi’s visual phrases [26] but do not use any decoding process and 222666333644  person with microphone  person climbing wall Figure 4: Examples of DPM based concept detectors. [sent-159, score-0.427]
</p><p>41 The specific concepts we choose are based on the most frequently occurring object-groupings in the human descriptions from the training videos. [sent-163, score-0.475]
</p><p>42 We use the VATIC tool [29] to annotate the trajectories of concept detectors in training videos, which are also used in Sec. [sent-164, score-0.225]
</p><p>43 Our approach is called sparse object stitching; we sparsely obtain the concept detections in a video and then sequentially group frames based on commonly detected concepts. [sent-174, score-0.292]
</p><p>44 For a given video, we run the set of concept detectors L on T sparsely distributed frames (e. [sent-175, score-0.225]
</p><p>45 The algorithm tries to segment the video into a set of concept shots S = {S1, S2, . [sent-179, score-0.288]
</p><p>46 For each such concept shot, we match it to a tripartite template graph and translate it to lan-  guage, as we describe next. [sent-203, score-0.352]
</p><p>47 Figure 5: Lingual descriptions from tripartite template graphs consisting of concepts as vertices. [sent-204, score-0.632]
</p><p>48 3  Tripartite Template Graph  We use a tripartite graph G = (Vs, Vt, Vo, E)—Vs for Whuem aunse subjects, tVet g rfaoprh hto Gols =, a (nVd Vo for objects—that takes the concept detections from each Sj and generates template-based language description. [sent-207, score-0.577]
</p><p>49 The set of paths P = {(Eτ,μ, Eμ,ν) |τ ∈ Vs, μ ∈ Vt, ν ∈ Vo} is defined as Pall =va {li(dE paths from)| τV ∈s tVo V,μo through V∈t V, an}d i se daechfi nfeodrm ass a possible language output. [sent-212, score-0.313]
</p><p>50 ”c Language Output: Given the top confident concept detections Lc ⊂ L in one concept shot Sj, we activate the set otefc paths LPc ⊂⊂ L P in. [sent-224, score-0.456]
</p><p>51 Ane en caotuncraelp language sentence is output ofofr paths containing a common subject using tchee template ? [sent-225, score-0.486]
</p><p>52 This node acts as a “backspace” production rule in the final lingual output thereby connecting the subject to an object effectively through a single edge. [sent-230, score-0.513]
</p><p>53 Histogram ∩coVunt=s are iuns ewdh ficorh ranking atnhe s concept nodes for the lingual output. [sent-233, score-0.677]
</p><p>54 The edges represent the action phrases or function words that stitch the concepts together cohesively. [sent-236, score-0.29]
</p><p>55 High Level: Semantic Verification The high level system joins the two earlier sets of lingual descriptions (from the low and middle levels) to enhance the set of sentences given from the middle level and at the same time to filter the sentences from the low level. [sent-247, score-1.534]
</p><p>56 In contrast, we rank over semantically verified low level sentences, giving higher weight to shorter sentences and a fixed preference to middle level sentences. [sent-251, score-0.459]
</p><p>57 We use the dependency grammar and part-of-speech (POS) models in the Stanford NLP Suite∗ to create annotated dictionaries based on word morphologies; the human descriptions provide the input. [sent-252, score-0.379]
</p><p>58 The predicted keywords from the low level topic models are labeled through these dictionaries. [sent-253, score-0.446]
</p><p>59 To obtain the final lingual description of a test video, the output from the middle level is used first. [sent-259, score-0.751]
</p><p>60 For semantic verification, we train MMLDA on a vocabulary of training descriptions and training concept annotations available using VATIC. [sent-261, score-0.561]
</p><p>61 Then we compute the number of topic rank inversions for two ranked lists of the top P predictions and top C detections from a test video as:  Lkeywords=? [sent-262, score-0.442]
</p><p>62 shtml  I√f the number of inversions is less than a threshold (≤  ( b≤y  I√fP th e+ Cnu)m thbeenr othfe keywords are semantically rveesrhifoieldd the detected concept list. [sent-278, score-0.371]
</p><p>63 Finally, we retrieve nearest neighbor sentences from the training descriptions by a ranking function. [sent-279, score-0.525]
</p><p>64 Each sentence s is ranked as: rs = bh(w1xs1 + w2xs2 ) where b is a boolean variable indicating that a sentence must have at least two of the labeled predictions, which are verified by the class of words to which the concept models belong. [sent-280, score-0.63]
</p><p>65 The variable indicating the total number of matches divided by the number of words in the sentence is xs1—this penalizes longer and irrelevant sentences. [sent-282, score-0.241]
</p><p>66 The sum of the weights of the predicted words from the topic model in the sentence is xs2—the latent topical strength is reflected here. [sent-283, score-0.507]
</p><p>67 The weights for sentence length penalty and topic strength respectively are w1 and w2 (set to be equal in our implementation). [sent-285, score-0.35]
</p><p>68 Datasets and Features TRECVID MED12 dataset: The first dataset we use for generating lingual descriptions of real life videos is part of TRECVID Multimedia Event Detection (MED12) [20]. [sent-289, score-0.972]
</p><p>69 The training set has 25 event categories each containing about 200 videos of positive and related instances of the event descriptions. [sent-290, score-0.266]
</p><p>70 1) we use the positive videos and descriptions in the 25 training events and predict the words for the positive videos for the first five events in the Dev-T collection. [sent-293, score-0.678]
</p><p>71 The descriptions in the training set consist of short and very high level descriptions of the corresponding videos ranging from 2 to 42 words and averaging 10 words with stopwords. [sent-294, score-0.969]
</p><p>72 A separate dataset released as part of the Multimedia Event Recounting (MER) task contains six test videos per event where the five events are selected from the 25 events for MED12. [sent-296, score-0.258]
</p><p>73 In-house “YouCook” dataset on cooking videos: We have also collected a new dataset for this video description task, which we call YouCook. [sent-299, score-0.264]
</p><p>74 We use MTurk to obtain multiple human descriptions for each video. [sent-305, score-0.346]
</p><p>75 Participants in MTurk are instructed to watch a cooking video as many times as required to lingually describe the video in at least three sentences totaling a minimum of 15 words. [sent-307, score-0.467]
</p><p>76 The average number of words per summary is 67, the average number of words per sentence is 10 with stopwords and the average number of descriptions per video is eight. [sent-309, score-0.723]
</p><p>77 For a given description task, the event type is assumed known (specified manually or by some prior event detection output); we hence learn separate topic models for each event that vary based on the language vocabulary. [sent-323, score-0.758]
</p><p>78 Quantitative Evaluation We use the ROUGE [17] tool to evaluate the level of relevant content generated in our system output video descriptions. [sent-329, score-0.198]
</p><p>79 Quantitative evaluation itself is a challenge—in the UIUC PASCAL sentence dataset [23], five sentences are used per image. [sent-334, score-0.372]
</p><p>80 On the other hand we only allow at most five sentences per video per level low or middle up to a maximum of ten. [sent-335, score-0.441]
</p><p>81 In Tables 2 and 3, “Low” is the sentence output from our low level topic models and NLP tools, “Middle” is the output from the middle level concepts, “High” is the semantically verified final output. [sent-338, score-0.599]
</p><p>82 All system descriptions are sentences, except the baseline [7], which is keywords. [sent-341, score-0.315]
</p><p>83 From Table 2, it is clear that lingual descriptions from both the low and middle levels of our system cover more relevant information, albeit, at the cost of introducing additional words. [sent-342, score-0.931]
</p><p>84 The “Rock climbing” event has very short descriptions as human descriptions and the “Cleaning an appliance” event is a very hard event both for DPM as well as MMLDA since multiple related concepts indicative of appliances in context appear in prediction and detection. [sent-346, score-1.03]
</p><p>85 From Table 2 we see the efficacy of the short lingual descriptions from the middle level in terms of precision while the final output of our system significantly outperforms relevant content coverage of the lingual descriptions from the other individual levels with regards to recall. [sent-347, score-1.834]
</p><p>86 The length of all system summaries is truncated at 67 words based on the average human description length. [sent-351, score-0.205]
</p><p>87 The sentences from the low level are chosen based on the top 15 predictions only. [sent-352, score-0.32]
</p><p>88 Our lingual descriptions built on top of concept labels and just a few keywords significantly outperform labeling with even four times as large a set of keywords. [sent-379, score-1.14]
</p><p>89 This can also tune language models to context since creating a sentence out of the predicted nouns and verbs does not increase recall based on unigrams. [sent-380, score-0.493]
</p><p>90 The first one or two italicized sentences in each row are the result of the middle level output. [sent-385, score-0.353]
</p><p>91 The “health care reform” in the second row is a noise phrase that actually cannot be verified though our middle level but remains in the description due to our conservative ranking formula. [sent-386, score-0.269]
</p><p>92 The human descriptions in the last two rows are shown for the purpose of illustrating their variance and yet their relevancy. [sent-388, score-0.346]
</p><p>93 The last cooking video has a low R1 score of 21% due to imprecise predictions and detections. [sent-389, score-0.204]
</p><p>94 Conclusion  In this paper we combine the best aspects of top-down and bottom-up methods of producing lingual descriptions of videos in-the-wild that exploit the rich semantic space of both text and visual features. [sent-391, score-1.025]
</p><p>95 Our contribution is unique in that the class of concept detectors semantically verify low level predictions from the bottom up and leverage both sentence generation and selection that together outperforms output from the independent modules. [sent-392, score-0.497]
</p><p>96 emphasize scalability in the semantic space to increase the generality of plausible lingual descriptions. [sent-394, score-0.56]
</p><p>97 Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes. [sent-457, score-0.231]
</p><p>98 Translating related words to videos and back through latent topics. [sent-465, score-0.228]
</p><p>99 Every picture tells a story: generating sentences from images. [sent-486, score-0.248]
</p><p>100 Only the top 5 sentences from our system are shown. [sent-585, score-0.21]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lingual', 0.513), ('descriptions', 0.315), ('language', 0.235), ('sentences', 0.21), ('topic', 0.188), ('concept', 0.164), ('sentence', 0.162), ('youcook', 0.158), ('keywords', 0.148), ('tripartite', 0.138), ('concepts', 0.129), ('videos', 0.106), ('bacon', 0.099), ('description', 0.095), ('rock', 0.094), ('vo', 0.094), ('video', 0.088), ('rouge', 0.087), ('cooking', 0.081), ('event', 0.08), ('words', 0.079), ('mmlda', 0.079), ('level', 0.075), ('trecvid', 0.074), ('middle', 0.068), ('nlp', 0.065), ('microphone', 0.061), ('verbs', 0.061), ('detectors', 0.061), ('verification', 0.06), ('inversions', 0.059), ('vs', 0.058), ('keyword', 0.056), ('vt', 0.055), ('eggs', 0.052), ('blei', 0.051), ('stitching', 0.051), ('template', 0.05), ('climbing', 0.049), ('shot', 0.049), ('naacl', 0.049), ('topics', 0.048), ('corpus', 0.048), ('semantic', 0.047), ('phrases', 0.047), ('sk', 0.047), ('hlt', 0.046), ('tch', 0.046), ('wall', 0.045), ('text', 0.044), ('das', 0.044), ('latent', 0.043), ('pan', 0.043), ('document', 0.043), ('detections', 0.04), ('cheese', 0.039), ('chenliang', 0.039), ('climb', 0.039), ('cokingvde', 0.039), ('cooked', 0.039), ('cooks', 0.039), ('malkarnenkar', 0.039), ('multinomials', 0.039), ('pradipto', 0.039), ('utensils', 0.039), ('textual', 0.039), ('man', 0.039), ('paths', 0.039), ('generating', 0.038), ('pos', 0.037), ('sadeghi', 0.037), ('shots', 0.036), ('subjects', 0.036), ('events', 0.036), ('free', 0.035), ('predictions', 0.035), ('predicted', 0.035), ('bleu', 0.035), ('crafts', 0.035), ('renovating', 0.035), ('vocabulary', 0.035), ('relevant', 0.035), ('action', 0.035), ('dpm', 0.033), ('word', 0.033), ('barbu', 0.032), ('appliance', 0.032), ('bowls', 0.032), ('fal', 0.032), ('tlo', 0.032), ('ranked', 0.032), ('annotation', 0.032), ('young', 0.032), ('hybrid', 0.032), ('human', 0.031), ('verified', 0.031), ('tagprop', 0.031), ('makadia', 0.031), ('morphologies', 0.031), ('synopsis', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="28-tfidf-1" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>2 0.21735467 <a title="28-tfidf-2" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>Author: Sanja Fidler, Abhishek Sharma, Raquel Urtasun</p><p>Abstract: We are interested in holistic scene understanding where images are accompanied with text in the form of complex sentential descriptions. We propose a holistic conditional random field model for semantic parsing which reasons jointly about which objects are present in the scene, their spatial extent as well as semantic segmentation, and employs text as well as image information as input. We automatically parse the sentences and extract objects and their relationships, and incorporate them into the model, both via potentials as well as by re-ranking candidate detections. We demonstrate the effectiveness of our approach in the challenging UIUC sentences dataset and show segmentation improvements of 12.5% over the visual only model and detection improvements of 5% AP over deformable part-based models [8].</p><p>3 0.14876369 <a title="28-tfidf-3" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>4 0.14229769 <a title="28-tfidf-4" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>5 0.14188796 <a title="28-tfidf-5" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>Author: Quannan Li, Jiajun Wu, Zhuowen Tu</p><p>Abstract: Obtaining effective mid-level representations has become an increasingly important task in computer vision. In this paper, we propose a fully automatic algorithm which harvests visual concepts from a large number of Internet images (more than a quarter of a million) using text-based queries. Existing approaches to visual concept learning from Internet images either rely on strong supervision with detailed manual annotations or learn image-level classifiers only. Here, we take the advantage of having massive wellorganized Google and Bing image data; visual concepts (around 14, 000) are automatically exploited from images using word-based queries. Using the learned visual concepts, we show state-of-the-art performances on a variety of benchmark datasets, which demonstrate the effectiveness of the learned mid-level representations: being able to generalize well to general natural images. Our method shows significant improvement over the competing systems in image classification, including those with strong supervision.</p><p>6 0.1244192 <a title="28-tfidf-6" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>7 0.11703335 <a title="28-tfidf-7" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>8 0.11396708 <a title="28-tfidf-8" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>9 0.10887972 <a title="28-tfidf-9" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>10 0.10825852 <a title="28-tfidf-10" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>11 0.10257076 <a title="28-tfidf-11" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>12 0.094932646 <a title="28-tfidf-12" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>13 0.094867572 <a title="28-tfidf-13" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>14 0.084728181 <a title="28-tfidf-14" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>15 0.080699913 <a title="28-tfidf-15" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>16 0.079556175 <a title="28-tfidf-16" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>17 0.078891754 <a title="28-tfidf-17" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>18 0.077713154 <a title="28-tfidf-18" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>19 0.076635845 <a title="28-tfidf-19" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>20 0.076524615 <a title="28-tfidf-20" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, -0.086), (2, -0.003), (3, -0.075), (4, -0.021), (5, 0.034), (6, -0.072), (7, 0.025), (8, -0.014), (9, 0.043), (10, 0.026), (11, -0.033), (12, 0.035), (13, -0.01), (14, -0.014), (15, -0.026), (16, 0.079), (17, 0.069), (18, -0.007), (19, -0.111), (20, -0.009), (21, 0.011), (22, 0.066), (23, -0.057), (24, -0.033), (25, -0.035), (26, 0.021), (27, 0.038), (28, 0.007), (29, -0.078), (30, -0.017), (31, -0.005), (32, -0.04), (33, 0.014), (34, 0.008), (35, 0.081), (36, -0.082), (37, 0.177), (38, -0.074), (39, -0.036), (40, 0.036), (41, 0.0), (42, -0.134), (43, -0.042), (44, -0.094), (45, 0.124), (46, -0.103), (47, -0.028), (48, 0.125), (49, -0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92975432 <a title="28-lsi-1" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>2 0.78062463 <a title="28-lsi-2" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>3 0.71002752 <a title="28-lsi-3" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>4 0.70752102 <a title="28-lsi-4" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>5 0.65439487 <a title="28-lsi-5" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>Author: Sanja Fidler, Abhishek Sharma, Raquel Urtasun</p><p>Abstract: We are interested in holistic scene understanding where images are accompanied with text in the form of complex sentential descriptions. We propose a holistic conditional random field model for semantic parsing which reasons jointly about which objects are present in the scene, their spatial extent as well as semantic segmentation, and employs text as well as image information as input. We automatically parse the sentences and extract objects and their relationships, and incorporate them into the model, both via potentials as well as by re-ranking candidate detections. We demonstrate the effectiveness of our approach in the challenging UIUC sentences dataset and show segmentation improvements of 12.5% over the visual only model and detection improvements of 5% AP over deformable part-based models [8].</p><p>6 0.62276328 <a title="28-lsi-6" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>7 0.6041978 <a title="28-lsi-7" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>8 0.58917207 <a title="28-lsi-8" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>9 0.5861876 <a title="28-lsi-9" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>10 0.57352817 <a title="28-lsi-10" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>11 0.57255995 <a title="28-lsi-11" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>12 0.56701112 <a title="28-lsi-12" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>13 0.54860759 <a title="28-lsi-13" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>14 0.54719484 <a title="28-lsi-14" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>15 0.54453135 <a title="28-lsi-15" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>16 0.52473009 <a title="28-lsi-16" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>17 0.51434356 <a title="28-lsi-17" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>18 0.51179123 <a title="28-lsi-18" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>19 0.50696957 <a title="28-lsi-19" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>20 0.50383008 <a title="28-lsi-20" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.1), (16, 0.016), (26, 0.046), (28, 0.013), (33, 0.226), (67, 0.086), (69, 0.053), (74, 0.237), (76, 0.017), (77, 0.023), (80, 0.025), (87, 0.048), (99, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82065743 <a title="28-lda-1" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>2 0.78719056 <a title="28-lda-2" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>Author: Bo Zheng, Yibiao Zhao, Joey C. Yu, Katsushi Ikeuchi, Song-Chun Zhu</p><p>Abstract: In this paper, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.</p><p>3 0.77978456 <a title="28-lda-3" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>Author: Victor Fragoso, Matthew Turk</p><p>Abstract: We present SWIGS, a Swift and efficient Guided Sampling method for robust model estimation from image feature correspondences. Our method leverages the accuracy of our new confidence measure (MR-Rayleigh), which assigns a correctness-confidence to a putative correspondence in an online fashion. MR-Rayleigh is inspired by Meta-Recognition (MR), an algorithm that aims to predict when a classifier’s outcome is correct. We demonstrate that by using a Rayleigh distribution, the prediction accuracy of MR can be improved considerably. Our experiments show that MR-Rayleigh tends to predict better than the often-used Lowe ’s ratio, Brown’s ratio, and the standard MR under a range of imaging conditions. Furthermore, our homography estimation experiment demonstrates that SWIGS performs similarly or better than other guided sampling methods while requiring fewer iterations, leading to fast and accurate model estimates.</p><p>4 0.76047814 <a title="28-lda-4" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>Author: Kazuki Maeno, Hajime Nagahara, Atsushi Shimada, Rin-Ichiro Taniguchi</p><p>Abstract: Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance ofsuch objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light?eld image as an input and model the distortion of the light ?eld caused by the refractive property of a transparent object. We propose a new feature, called the light ?eld distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.</p><p>5 0.74384129 <a title="28-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.74173427 <a title="28-lda-6" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>7 0.73920834 <a title="28-lda-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.73892903 <a title="28-lda-8" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>9 0.73856992 <a title="28-lda-9" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>10 0.73855346 <a title="28-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.73850274 <a title="28-lda-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.73826891 <a title="28-lda-12" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>13 0.73780727 <a title="28-lda-13" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>14 0.73775625 <a title="28-lda-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.73763919 <a title="28-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.73686731 <a title="28-lda-16" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>17 0.73663342 <a title="28-lda-17" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>18 0.7366178 <a title="28-lda-18" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>19 0.73620057 <a title="28-lda-19" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>20 0.7355994 <a title="28-lda-20" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
