<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-442" href="#">cvpr2013-442</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</h1>
<br/><p>Source: <a title="cvpr-2013-442-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Long_Transfer_Sparse_Coding_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Mingsheng Long, Guiguang Ding, Jianmin Wang, Jiaguang Sun, Yuchen Guo, Philip S. Yu</p><p>Abstract: Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. It has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-ofthe-art methods on three types of computer vision datasets.</p><p>Reference: <a title="cvpr-2013-442-reference" href="../cvpr2013_reference/cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  {dinggg ,  Abstract Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. [sent-3, score-0.257]
</p><p>2 However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. [sent-5, score-0.365]
</p><p>3 In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. [sent-6, score-0.247]
</p><p>4 Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. [sent-7, score-0.9]
</p><p>5 As a powerful tool for finding succinct representations of stimuli and capturing high-level semantics in visual data, sparse coding can represent images using only a few active coefficients. [sent-11, score-0.458]
</p><p>6 This makes the sparse representations easy to interpret and manipulate, and facilitates efficient content-based image indexing and retrieval. [sent-12, score-0.203]
</p><p>7 Sparse coding is receiving increasing ∗Corresponding author: Jianmin Wang. [sent-13, score-0.209]
</p><p>8 One major computational problem of sparse coding is to improve the quality of the sparse representation while maximally preserving the signal fidelity. [sent-20, score-0.511]
</p><p>9 [6] introduced a Laplacian term of coefficients in sparse coding, which was extended  to an efficient algorithm in Cai et al. [sent-25, score-0.172]
</p><p>10 However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations. [sent-33, score-0.336]
</p><p>11 In this case, the dictionary learned from the labeled images cannot effectively encode the unlabeled images with high fidelity, and also the unlabeled images may reside far away from the labeled images under the new representation. [sent-34, score-0.655]
</p><p>12 This distribution difference will greatly challenge the robustness of existing sparse coding algorithms for cross-distribution image classification problems. [sent-35, score-0.413]
</p><p>13 Recently, the literature has witnessed an increasing focus on transfer learning [15] problems where the labeled training data and unlabeled test data are sampled from different probability distributions. [sent-36, score-0.519]
</p><p>14 In this case, standard classifiers such as SVM and logistic regression trained on the labeled data may fail to make correct predictions on the unlabeled data [13, 14, 16, 17]. [sent-38, score-0.323]
</p><p>15 [13, 14] proposed to extract a “good” feature representation through which the probability distributions of labeled and unlabeled data are drawn close. [sent-40, score-0.391]
</p><p>16 444000557  Inspired by recent progress in sparse coding and transfer learning, we propose a novel Transfer Sparse Coding (TSC) algorithm to construct robust sparse representations for classifying cross-distribution images accurately. [sent-42, score-0.721]
</p><p>17 We aim to minimize the distribution divergence between labeled and unlabeled images using a nonparametric distance measure. [sent-43, score-0.39]
</p><p>18 Specifically, we incorporate this criterion into the objective function of sparse coding to make the new representations of the labeled and unlabeled images close to each other. [sent-44, score-0.737]
</p><p>19 In this way, the induced representations are made robust for cross-distribution image classification problems. [sent-45, score-0.115]
</p><p>20 Moreover, to enrich the new representations with more discriminating power, we also incorporate the graph Laplacian term of coefficients [24] in our objective function. [sent-46, score-0.236]
</p><p>21 Related Work In this section, we discuss prior works that are most related to ours, including sparse coding and transfer learning. [sent-49, score-0.474]
</p><p>22 Recently, sparse coding has been a hot research focus in computer vision. [sent-50, score-0.348]
</p><p>23 For adapting the dictionary to achieve sparse representation, Aharon et al. [sent-54, score-0.224]
</p><p>24 Our work aims to discover a shared dictionary which can encode both labeled and unlabeled data sampled from different probability distributions. [sent-56, score-0.44]
</p><p>25 To improve the quality of sparse representations, researchers have modified the sparse constraint by adding nonnegative constraint [10], graph regularization [6, 24], weighted ℓ2-norm constraint [20], etc. [sent-57, score-0.4]
</p><p>26 Our approach aims to construct robust sparse representations for cross-distribution image classification problems, which is a different learning goal from the previous works. [sent-58, score-0.276]
</p><p>27 In the machine learning literature, transfer learning [15], which aims to transfer knowledge between the labeled and unlabeled data sampled from different distributions, has also attracted extensive research interest. [sent-59, score-0.589]
</p><p>28 proposed a Transfer Component Analysis (TCA) method to reduce the Maximum Mean Discrepancy (MMD) [7] between the labeled and unlabeled data, and simultaneously minimize the reconstruction error of the input data using PCA. [sent-61, score-0.304]
</p><p>29 Different from their method, our work focuses on learning robust image representations by building an adaptive model based on sparse coding. [sent-62, score-0.225]
</p><p>30 [16, 17] have explored sparse coding to extract features for knowledge transfer. [sent-64, score-0.348]
</p><p>31 However, their method adopts a kernel density estimation (KDE) technique to estimate the PDFs of distributions and then minimizes the Jensen-Shannon divergence between them. [sent-65, score-0.138]
</p><p>32 Moreover, our work additionally incorporates the graph Laplacian term of coefficients [24] in the objective function, which can discover more discriminating representations for classification tasks. [sent-67, score-0.221]
</p><p>33 , b푘] ∈ ℝ푚× be the dictionary matrix where each column b푖] represents a basis vector in the dictionary, and let S = [s1, . [sent-77, score-0.134]
</p><p>34 , s푛] ∈ ℝ푘×푛 be the coding matrix where each column s푖 is a sparse representation for a data point x푖. [sent-80, score-0.392]
</p><p>35 The goal of sparse coding is to learn a dictionary (over-complete if > 푚) and corresponding sparse codes such that input data can be well approximated [16]. [sent-81, score-0.648]
</p><p>36 ,푘  (1) where 휆 is a tunable regularization parameter to trade off the sparsity of coding and the approximation of input data. [sent-87, score-0.342]
</p><p>37 , 푘∑  (2)  where 훾 is a graph regularization parameter to trade off the weight between sparse coding and geometric preservation. [sent-112, score-0.528]
</p><p>38 Transfer Sparse Coding In this section, we present the Transfer Sparse Coding (TSC) algorithm for robust image representation, which extends GraphSC by taking into account the minimization of distribution divergence between labeled and unlabeled data. [sent-114, score-0.412]
</p><p>39 Problem Definition  풟푙  Given labeled data = {(x1, 푦1) , . [sent-117, score-0.114]
</p><p>40 Assume t]h ∈at ℝthe labeled and unlabeled data are sampled from different probability distributions in an 푚-dimensional feature space. [sent-127, score-0.424]
</p><p>41 Problem 1(Transfer Sparse Coding) Given labeled data and unlabeled data 풟푢 under different distributions, our g풟oaal nisd t uon lleaabrenl a d dicattiaon 풟ary B and a sparse coding S which performs robustly across the labeled and unlabeled data. [sent-129, score-0.956]
</p><p>42 In this way, a supervised classifier trained on the labeled data can generalize better on the unlabeled data. [sent-131, score-0.322]
</p><p>43 Objective Function To make sparse coding robust to different probability distributions, one may expect that the basis vectors can capture the commonality underlying both the labeled and unlabeled data, rather than only the individual property in the labeled data. [sent-134, score-0.792]
</p><p>44 However, even in the extracted 푘-dimensional sparse representation, the distribution difference between labeled and unlabeled data will still be significantly large. [sent-135, score-0.479]
</p><p>45 To realize this idea, a natural strategy is to make the probability distributions of labeled and unlabeled data close to each other in the sparse representation. [sent-137, score-0.53]
</p><p>46 That is, by representing all data points X with the learned coding matrix S, the probability distributions of the sparse codes for the labeled and unlabeled data should be close enough. [sent-138, score-0.84]
</p><p>47 XMSLBgraipdnchipMcoLtudioMatnp dDgla rtmcy ai ma tnra imtxriaxtr  computes the distance between the sample means of the labeled and unlabeled data in the 푘-dimensional coefficients:  ? [sent-143, score-0.304]
</p><p>48 trix and is computed as follows  푀푖푗= ⎧⎨1 푛−/푙푛 1푢푙2푢 , xo t푖 h,ex r푗 w∈i se 풟 푙푢  (4)  By regularizing E⎩quation (2) with Equation (3), dictionary matrix B is refined and the probability distributions of labeled and unlabeled data are drawn close under the new representation S. [sent-157, score-0.501]
</p><p>49 , 푘 (5) where 휇 > 0 is the MMD regularization parameter trading off the weight between GraphSC and distribution matching. [sent-163, score-0.142]
</p><p>50 To compare the effectiveness between MMD regularization and graph regularization (GraphSC), we refer to the special case of TSC with 훾 = 0 as TSCMMD and test it empirically. [sent-164, score-0.159]
</p><p>51 By minimizing MMD, TSC can match distributions between labeled and unlabeled data based on sparse coding. [sent-168, score-0.512]
</p><p>52 Following [9, 11, 24], we divide the optimization of TSC into two iterative steps: 1) learning transfer sparse codes S with dictionary B fixed, i. [sent-169, score-0.407]
</p><p>53 , an ℓ1-regularized least squares problem; and 2) learning dictionary B with transfer sparse codes S fixed, i. [sent-171, score-0.437]
</p><p>54 Learning Transfer Sparse Codes We solve optimization problem (5) for transfer sparse codes S. [sent-176, score-0.322]
</p><p>55 By fixing dictionary B, problem (5) becomes  mSin∥X − BS∥2퐹+ tr(S(휇M + 훾L)ST) + 휆∑푖푛=1  ∣s푖∣ (7)  Unfortunately, problem (7) is nondifferentiable when s푖 takes values of 0, which makes standard unconstrained optimization techniques infeasible. [sent-177, score-0.143]
</p><p>56 Several recent approaches 444000779  Algorithm 1: Learning Transfer Sparse Codes Input: Data matrix X, dictionary B, MMD matrix M, graph Laplacian matrix L, MMD/graph/sparsity regularization parameters 휇, 훾, 휆. [sent-178, score-0.247]
</p><p>57 Output: Current optimal coding matrix S∗ = [s∗1 s푛∗] . [sent-179, score-0.234]
</p><p>58 In nonsmooth optimization  methods for solving nondifferentiable problems, a necessary condition for a parameter vector to be a local minimum is that the zero-vector is an element of the subdifferential— the set containing all subgradients at the parameter vector [5]. [sent-186, score-0.143]
</p><p>59 , 휃푘] while updating each s푖, and systematically searches for the optimal active set and coefficients signs which minimize objective function (9). [sent-206, score-0.128]
</p><p>60 Learning Dictionary Learning the dictionary B with the coding S fixed is reduced to the following ℓ2-constrained optimization problem  mBin∥X − BS∥2퐹,  푠. [sent-213, score-0.313]
</p><p>61 To speed up experiments, we construct one dataset USPS vs MNIST by randomly sampling 1,800 images in USPS to form the training data, and randomly sampling 2,000 images in MNIST to form the test data. [sent-230, score-0.113]
</p><p>62 We construct one dataset PIE1 vs PIE2 by selecting  all 2,856 images in PIE1 to form the training data, and all 3,329 images in PIE2 to form the test data. [sent-252, score-0.113]
</p><p>63 We construct one dataset MSRC vs VOC by selecting all 1,269 images in MSRC to form the training data, and all 1,530 images in VOC2007 to form the test data. [sent-258, score-0.113]
</p><p>64 2  Implementation Details  Following [24, 14], SC, GraphSC, TSCMMD, and TSC are performed on both labeled and unlabeled data as an unsupervised dimensionality reduction procedure, then a super-  vised LR classifier is trained on labeled data to classify unlabeled data. [sent-287, score-0.627]
</p><p>65 Under our experimental setup, it is impossible to automatically tune the optimal parameters for the target classifier using cross validation, since the labeled and unlabeled data are sampled from different distributions. [sent-289, score-0.337]
</p><p>66 c The TSC approach has three model parameters: MMD regularization parameter 휇, graph regularization parameter 훾, and sparsity regularization parameter 휆. [sent-308, score-0.312]
</p><p>67 html  DatasetUSPS vs MNIST PIE1 vs PIE2 MSRC vs VOC  TPSLGaRCSbr Al[epM9h3]S. [sent-321, score-0.242]
</p><p>68 ) E isx tpheer liambeeln ptraeld Ricteesdu bltys t The classification accuracy of TSC and the five baseline methods on the three cross-distribution image datasets USPS vs MNIST, PIE1 vs PIE2, and MSRC vs VOC is illustrated in Table 3. [sent-336, score-0.269]
</p><p>69 This verifies that TSC can construct robust sparse representations for classifying cross-distribution images accurately. [sent-347, score-0.275]
</p><p>70 This validates that minimizing the distribution divergence is very important to make the induced representations robust for cross-distribution image classification. [sent-349, score-0.218]
</p><p>71 In particular, TSCMMD has significantly outperformed GraphSC, which indicates that minimizing the distribution divergence is more important than preserving the geometric structure when labeled and unlabeled images are sampled from different distributions. [sent-350, score-0.444]
</p><p>72 By incorporating the graph Laplacian term of coefficients into TSC, we aim to enrich the sparse representations with more discriminating power to benefit the classification problems. [sent-352, score-0.383]
</p><p>73 , LR, treat input data from different distributions as if they were sampled from the same distribution. [sent-359, score-0.121]
</p><p>74 In real applications, this strict assumption is usually violated, since labeled training data and unlabeled test data are usually collected in different time periods, or under different conditions. [sent-360, score-0.342]
</p><p>75 In this case, the optimal decision hyperplane trained from the labeled data cannot discriminate the unlabeled data effectively, leading to poor classification performance, as is shown in Table 3. [sent-361, score-0.352]
</p><p>76 A possible reason for preferring PCA is that it can extract a low-dimensional subspace, where the distribution divergence may be reduced to some extent. [sent-366, score-0.153]
</p><p>77 The reason for preferring SC and GraphSC is that the sparse representations can capture more succinct high-level semantics for image understanding. [sent-372, score-0.259]
</p><p>78 By taking into account the graph Laplacian regularizer, GraphSC can further outperform SC, which verifies that the geometric structure can indeed enrich the sparse representations with more discriminating power. [sent-373, score-0.37]
</p><p>79 However, since the labeled data and unlabeled data are sampled from different distributions as in our adopted datasets, SC and GraphSC may further enlarge the distribution divergence due to the sparse representation. [sent-374, score-0.687]
</p><p>80 By extracting sparse representations and matching different distributions simultaneously, TSC and TSCMMD can greatly enhance the robustness of sparse coding, shown in Table 3. [sent-376, score-0.411]
</p><p>81 We visualize in Figure 2 the values of matrices obtained by running TSC on USPS vs MNIST with 휇 = 0 and 휇 = 105, and then computing W in Equation (2) on sparse representation S. [sent-380, score-0.211]
</p><p>82 Note that, the first 90 images are from the labeled training data while the last 100 images  are from the unlabeled test data. [sent-382, score-0.323]
</p><p>83 Most existing sparse coding methods, such as SC and GraphSC, have not explicitly minimized the distribution difference, resulting in unsatisfactory performance for cross-distribution problems. [sent-386, score-0.384]
</p><p>84 This naturally leads to better generalization capability, that is, with sparse representation S, a supervised classifier trained on the labeled training data is expected to perform much better on the unlabeled test data. [sent-389, score-0.48]
</p><p>85 Parameter Sensitivity We conduct empirical analysis on parameter sensitivity using all datasets, which validates that TSC can achieve optimal performance under a wide range of parameter values. [sent-392, score-0.11]
</p><p>86 An extreme case is 휇 → ∞, where only distribution matching is guaranteed, ibsu t휇 b →oth ∞ sparse coding daisndtr geometric preservation afontre tehde,  input images are discarded. [sent-395, score-0.474]
</p><p>87 Another extreme case is 휇 → 0, iwnpheurte i only sparse coding da. [sent-396, score-0.379]
</p><p>88 In both extreme cases, TSC cannot extract robust sparse representations for cross-distribution image classification. [sent-398, score-0.256]
</p><p>89 Wuese run T inSC F iwguitrhe varying dva claunes c hofo graph regularization parameter 훾. [sent-403, score-0.116]
</p><p>90 Theoretically, 훾 controls the weight of graph regularization, and larger values of 훾 will make the geometric preservation more important in TSC. [sent-404, score-0.138]
</p><p>91 iTsh 훾en → →TS 0C, wwhilel degenerate etot TicSC prMeMseDr-, which cannot enrich the new representations with discriminating power. [sent-408, score-0.148]
</p><p>92 Theoretically, 휆 controls the complexity  of coding matrix S, and can prevent TSC from over-fitting the input data or degenerating to trivial solutions during the iterative procedure. [sent-417, score-0.274]
</p><p>93 An important advantage of TSC is the robustness to the distribution difference between the labeled and unlabeled images, which can substantially improve cross-distribution image classification problems. [sent-433, score-0.35]
</p><p>94 Extensive experimental results on several benchmark datasets show that TSC can achieve superior performance against state-of-the-art sparse coding methods. [sent-434, score-0.348]
</p><p>95 K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. [sent-440, score-0.139]
</p><p>96 Spectral regression: A unified approach for sparse subspace learning. [sent-451, score-0.139]
</p><p>97 Local features are not lonely laplacian sparse coding for image classification. [sent-473, score-0.4]
</p><p>98 Knowledge transfer with low-quality data: A feature extraction issue. [sent-554, score-0.126]
</p><p>99 Knowledge transfer with low-quality data: A feature extraction issue. [sent-560, score-0.126]
</p><p>100 Linear spatial pyramid matching using sparse coding for image classification. [sent-601, score-0.348]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tsc', 0.731), ('graphsc', 0.325), ('coding', 0.209), ('unlabeled', 0.19), ('mmd', 0.18), ('sparse', 0.139), ('tscmmd', 0.13), ('transfer', 0.126), ('usps', 0.104), ('labeled', 0.095), ('mnist', 0.091), ('dictionary', 0.085), ('msrc', 0.079), ('vs', 0.072), ('lr', 0.071), ('divergence', 0.069), ('distributions', 0.069), ('bs', 0.069), ('quanz', 0.065), ('representations', 0.064), ('nondifferentiable', 0.058), ('codes', 0.057), ('sc', 0.055), ('signs', 0.055), ('regularization', 0.053), ('laplacian', 0.052), ('proceedings', 0.044), ('enrich', 0.044), ('pan', 0.044), ('pca', 0.042), ('discriminating', 0.04), ('preservation', 0.038), ('distribution', 0.036), ('nonnegative', 0.035), ('conference', 0.035), ('graph', 0.034), ('sampled', 0.033), ('regularized', 0.033), ('coefficients', 0.033), ('cai', 0.033), ('qp', 0.033), ('huan', 0.032), ('sparsity', 0.032), ('extreme', 0.031), ('squares', 0.03), ('parameter', 0.029), ('classification', 0.029), ('preferring', 0.029), ('jianmin', 0.029), ('verifies', 0.028), ('validates', 0.027), ('subgradients', 0.027), ('succinct', 0.027), ('html', 0.026), ('sensitivity', 0.025), ('eca', 0.025), ('kwok', 0.025), ('gretton', 0.025), ('matrix', 0.025), ('ts', 0.025), ('optimality', 0.025), ('signal', 0.024), ('neural', 0.024), ('pdfs', 0.024), ('basis', 0.024), ('weight', 0.024), ('baseline', 0.024), ('sign', 0.023), ('transactions', 0.023), ('robust', 0.022), ('theoretically', 0.022), ('face', 0.022), ('rescale', 0.022), ('philip', 0.022), ('kde', 0.022), ('construct', 0.022), ('advances', 0.021), ('controls', 0.021), ('objective', 0.021), ('geometric', 0.021), ('tsang', 0.02), ('tr', 0.02), ('periods', 0.019), ('reduced', 0.019), ('test', 0.019), ('aharon', 0.019), ('trade', 0.019), ('active', 0.019), ('data', 0.019), ('criterion', 0.019), ('dimensionality', 0.019), ('fidelity', 0.018), ('codebook', 0.018), ('software', 0.018), ('probability', 0.018), ('adopted', 0.018), ('vlfeat', 0.018), ('supervised', 0.018), ('processing', 0.018), ('discrepancy', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="442-tfidf-1" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>Author: Mingsheng Long, Guiguang Ding, Jianmin Wang, Jiaguang Sun, Yuchen Guo, Philip S. Yu</p><p>Abstract: Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. It has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-ofthe-art methods on three types of computer vision datasets.</p><p>2 0.147562 <a title="442-tfidf-2" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>3 0.13603584 <a title="442-tfidf-3" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>4 0.12439728 <a title="442-tfidf-4" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>5 0.12215028 <a title="442-tfidf-5" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>6 0.11924907 <a title="442-tfidf-6" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>7 0.11339588 <a title="442-tfidf-7" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>8 0.10171732 <a title="442-tfidf-8" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>9 0.10006879 <a title="442-tfidf-9" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>10 0.096138276 <a title="442-tfidf-10" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>11 0.095820226 <a title="442-tfidf-11" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>12 0.09541636 <a title="442-tfidf-12" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>13 0.094901569 <a title="442-tfidf-13" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>14 0.094529726 <a title="442-tfidf-14" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>15 0.089973986 <a title="442-tfidf-15" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>16 0.082398199 <a title="442-tfidf-16" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>17 0.082339115 <a title="442-tfidf-17" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>18 0.080685861 <a title="442-tfidf-18" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>19 0.079090349 <a title="442-tfidf-19" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>20 0.07867384 <a title="442-tfidf-20" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, -0.087), (2, -0.118), (3, 0.121), (4, -0.0), (5, -0.022), (6, -0.012), (7, -0.009), (8, -0.038), (9, 0.007), (10, 0.013), (11, -0.015), (12, -0.002), (13, -0.013), (14, -0.037), (15, -0.002), (16, -0.037), (17, -0.058), (18, -0.006), (19, 0.01), (20, -0.05), (21, -0.056), (22, -0.042), (23, -0.055), (24, 0.025), (25, 0.08), (26, 0.004), (27, 0.037), (28, -0.024), (29, -0.022), (30, -0.048), (31, 0.03), (32, -0.051), (33, 0.012), (34, -0.006), (35, 0.051), (36, -0.042), (37, -0.014), (38, 0.093), (39, -0.107), (40, -0.128), (41, -0.053), (42, 0.067), (43, 0.05), (44, 0.021), (45, -0.052), (46, -0.046), (47, -0.083), (48, -0.051), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93251109 <a title="442-lsi-1" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>Author: Mingsheng Long, Guiguang Ding, Jianmin Wang, Jiaguang Sun, Yuchen Guo, Philip S. Yu</p><p>Abstract: Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. It has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-ofthe-art methods on three types of computer vision datasets.</p><p>2 0.76393145 <a title="442-lsi-2" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>Author: Bin Zhao, Eric P. Xing</p><p>Abstract: Many vision tasks require a multi-class classifier to discriminate multiple categories, on the order of hundreds or thousands. In this paper, we propose sparse output coding, a principled way for large-scale multi-class classification, by turning high-cardinality multi-class categorization into a bit-by-bit decoding problem. Specifically, sparse output coding is composed of two steps: efficient coding matrix learning with scalability to thousands of classes, and probabilistic decoding. Empirical results on object recognition and scene classification demonstrate the effectiveness ofour proposed approach.</p><p>3 0.74744946 <a title="442-lsi-3" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>4 0.68992352 <a title="442-lsi-4" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>5 0.66238475 <a title="442-lsi-5" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>6 0.62907141 <a title="442-lsi-6" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>7 0.61798829 <a title="442-lsi-7" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>8 0.59956592 <a title="442-lsi-8" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>9 0.59413981 <a title="442-lsi-9" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>10 0.59233147 <a title="442-lsi-10" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>11 0.58113122 <a title="442-lsi-11" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>12 0.58108133 <a title="442-lsi-12" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>13 0.57822311 <a title="442-lsi-13" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>14 0.57804757 <a title="442-lsi-14" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>15 0.57023031 <a title="442-lsi-15" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>16 0.5632531 <a title="442-lsi-16" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>17 0.5553124 <a title="442-lsi-17" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>18 0.55005497 <a title="442-lsi-18" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>19 0.53467649 <a title="442-lsi-19" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>20 0.52961588 <a title="442-lsi-20" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.19), (10, 0.124), (16, 0.033), (26, 0.033), (28, 0.026), (33, 0.281), (59, 0.01), (67, 0.06), (69, 0.037), (76, 0.018), (87, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89921981 <a title="442-lda-1" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>Author: Carl Olsson, Johannes Ulén, Yuri Boykov</p><p>Abstract: It is commonly believed that higher order smoothness should be modeled using higher order interactions. For example, 2nd order derivatives for deformable (active) contours are represented by triple cliques. Similarly, the 2nd order regularization methods in stereo predominantly use MRF models with scalar (1D) disparity labels and triple clique interactions. In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels, e.g. tangent planes. This general paradigm has been criticized due to perceived computational complexity of optimization in higher-dimensional label space. Contrary to popular beliefs, we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization problems with (nearly) submodular pairwise interactions. Our theoretical and experimental re- sults demonstrate advantages over state-of-the-art methods for 2nd order smoothness stereo. 1</p><p>same-paper 2 0.87097651 <a title="442-lda-2" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>Author: Mingsheng Long, Guiguang Ding, Jianmin Wang, Jiaguang Sun, Yuchen Guo, Philip S. Yu</p><p>Abstract: Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. It has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-ofthe-art methods on three types of computer vision datasets.</p><p>3 0.86882365 <a title="442-lda-3" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>4 0.86421257 <a title="442-lda-4" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>Author: Jianguo Li, Yimin Zhang</p><p>Abstract: This paper presents a novel learning framework for training boosting cascade based object detector from large scale dataset. The framework is derived from the wellknown Viola-Jones (VJ) framework but distinguished by three key differences. First, the proposed framework adopts multi-dimensional SURF features instead of single dimensional Haar features to describe local patches. In this way, the number of used local patches can be reduced from hundreds of thousands to several hundreds. Second, it adopts logistic regression as weak classifier for each local patch instead of decision trees in the VJ framework. Third, we adopt AUC as a single criterion for the convergence test during cascade training rather than the two trade-off criteria (false-positive-rate and hit-rate) in the VJ framework. The benefit is that the false-positive-rate can be adaptive among different cascade stages, and thus yields much faster convergence speed of SURF cascade. Combining these points together, the proposed approach has three good properties. First, the boosting cascade can be trained very efficiently. Experiments show that the proposed approach can train object detectors from billions of negative samples within one hour even on personal computers. Second, the built detector is comparable to the stateof-the-art algorithm not only on the accuracy but also on the processing speed. Third, the built detector is small in model-size due to short cascade stages.</p><p>5 0.85740405 <a title="442-lda-5" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>6 0.85519624 <a title="442-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.85513943 <a title="442-lda-7" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>8 0.85506964 <a title="442-lda-8" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>9 0.85450137 <a title="442-lda-9" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>10 0.85449654 <a title="442-lda-10" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>11 0.85443592 <a title="442-lda-11" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>12 0.85431725 <a title="442-lda-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.85419226 <a title="442-lda-13" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>14 0.85354793 <a title="442-lda-14" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>15 0.85326535 <a title="442-lda-15" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>16 0.85286045 <a title="442-lda-16" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>17 0.85272598 <a title="442-lda-17" href="./cvpr-2013-Tensor-Based_Human_Body_Modeling.html">426 cvpr-2013-Tensor-Based Human Body Modeling</a></p>
<p>18 0.85237646 <a title="442-lda-18" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>19 0.85222846 <a title="442-lda-19" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>20 0.8520593 <a title="442-lda-20" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
