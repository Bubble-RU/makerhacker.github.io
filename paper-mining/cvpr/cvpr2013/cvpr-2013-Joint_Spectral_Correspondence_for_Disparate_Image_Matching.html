<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-234" href="#">cvpr2013-234</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</h1>
<br/><p>Source: <a title="cvpr-2013-234-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bansal_Joint_Spectral_Correspondence_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Mayank Bansal, Kostas Daniilidis</p><p>Abstract: We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night), age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixellevel descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art.</p><p>Reference: <a title="cvpr-2013-234-reference" href="../cvpr2013_reference/cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. [sent-3, score-0.418]
</p><p>2 We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. [sent-7, score-0.805]
</p><p>3 Introduction In this paper, we focus on matching images with disparate appearance. [sent-10, score-0.282]
</p><p>4 Such images might be taken during day and night or in different times in history, and they differ at the local pixel level in the sense that neither intensity nor gradient distributions are locally comparable. [sent-11, score-0.284]
</p><p>5 Instead, we propose to use the joint image graph spectrum to detect and match persistent features which robustly encode the appearance similarity we perceive when we look at such images. [sent-13, score-0.846]
</p><p>6 Numerous SIFT features are detected in these images and they show good repeatability (blue bars in the plot) as well. [sent-17, score-0.462]
</p><p>7 The spectrum of the joint image graphs is computed. [sent-24, score-0.226]
</p><p>8 The first row shows a day-time query-image (blue box) which is matched pair-wise against the pre-dusk, dusk and night images respectively from left to right. [sent-25, score-0.214]
</p><p>9 The plot compares the repeatability (bars) and average-precision (AP)(polyline) of the SIFT detector (blue) with the spectral method (red). [sent-28, score-0.476]
</p><p>10 J2(2)  J1(2)  grades as the illumination difference between the matched image pairs is increased as is visible from the blue polyline in the plot. [sent-29, score-0.257]
</p><p>11 In contrast, the spectral features we propose in this paper are comparable in their repeatability (red bars) and they behave significantly better in the Average Precision (red polyline) even for the most challenging pair: night vs. [sent-30, score-0.598]
</p><p>12 Spectral methods on the image graph laplacian have been used extensively in the literature for applications like clustering, segmentation [1, 11] etc. [sent-32, score-0.169]
</p><p>13 The extracted eigenfunctions are either discretized to obtain the desired number of clusters or segments in the image or they are used  directly as the spectral space coordinates of the pixels in an 222888000200  embedded space representation. [sent-33, score-0.209]
</p><p>14 We show how such a representation captures persistent regions in the image pair even when the appearance difference between them is substantial (day-night, historic-new etc. [sent-36, score-0.586]
</p><p>15 Moreover, we propose a new definition of the joint image graph: all pixels of both images are nodes and the corresponding edge weights depend only on the difference of the local image structures and not on the proximity between the pixels. [sent-38, score-0.231]
</p><p>16 Although a partitioning of such a graph might cluster together distant regions, these regions even though disconnected in the image space are persistent across images. [sent-39, score-0.575]
</p><p>17 We show that such persistent features are both repeatable across images  and similar in terms of SIFT descriptors computed in the eigen space itself, in a variety of cross domain setups. [sent-41, score-0.64]
</p><p>18 We show experimental results of our approach on the challenging dataset from [3] which contains image pairs exhibiting dramatic illumination, age and rendering style differences. [sent-42, score-0.283]
</p><p>19 Our results clearly indicate the substantial matching improvement possible by looking at features derived from a joint image spectrum rather than relying on features detected individually in the two images to match in their descriptors. [sent-43, score-0.672]
</p><p>20 However, we believe that the global information encoded in the joint image graph and its eigen-functions is the new cue that enables a better performance than approaches relying only on local neighborhoods. [sent-45, score-0.26]
</p><p>21 Shechtman and Irani [7] proposed an approach for matching disparate images using patterns of local self-similarity encoded into a shape-context like descriptor. [sent-48, score-0.282]
</p><p>22 However, for the kind of disparate images we consider, the local self-similarity pattern itself can be significantly different between corresponding points in the image pair. [sent-49, score-0.22]
</p><p>23 Using a linear classifier, they learn the relative importance of different features (specifically, components of the global image HoG descriptor in the paper) for a given query image and then use the weight vector to define a matching score. [sent-52, score-0.252]
</p><p>24 In contrast, our focus is on extracting local features that are persistent between a pair of images instead of deriving a global image descriptor that can be used for retrieval. [sent-53, score-0.64]
</p><p>25 Recently, Hauagge and Snavely [3] have focused on the task of matching such images by defining “local-symmetry” features which capture various symmetries like bilateral, rotational etc. [sent-54, score-0.225]
</p><p>26 The spectral analysis of the joint matrix between two images appeared first in [11] where the affinity matrices of object model patches and the input image are combined with a non-diagonal matrix associating object patches and image pixels. [sent-58, score-0.492]
</p><p>27 [9] proposed an approach to determine co-salient regions between two images using a spectral technique on the joint image graph constructed from the images. [sent-60, score-0.452]
</p><p>28 Their joint image graph was constructed with all the pixels in the two images by defining separate affinity functions for intra and inter image terms. [sent-61, score-0.614]
</p><p>29 The intra image affinity was defined using the intervening contour cue while the inter image term was based entirely on the  initial set of feature correspondences between the images. [sent-62, score-0.406]
</p><p>30 They show examples of image pairs with illumination differences where their joint segmentation approach achieves better co-clustering than what is possible by using intra-image constraints alone. [sent-68, score-0.24]
</p><p>31 Fortheimagepair nthefirstcolumn,thesuc es ive  columns show the second-through-fifth eigen-function pairs obtained using a pixel-color based joint image graph. [sent-76, score-0.224]
</p><p>32 Thus, we do not need any prior segmentation and we are not prone to errors due to misdetection of contours, particularly since contour detection would be very challenging for the kind of disparate images we focus on. [sent-79, score-0.284]
</p><p>33 Contours of the soft version of the eigenvectors of a single image affinity matrix computed following the Normalized Cuts criterion have also been used in [1] to include global relationships into the probabilistic boundary feature vector. [sent-80, score-0.254]
</p><p>34 It is evident that finding features that are repetitive between the two pictures is a daunting task; in fact, the problem of finding descriptors that can account for the appearance differences at geometrically matching locations is itself quite challenging. [sent-85, score-0.284]
</p><p>35 The eigen analysis is performed on each image graph independently. [sent-90, score-0.17]
</p><p>36 Most of the dominant contours in the scene are very low energy and the intensity at which corresponding contours would get detected varies between different regions in the two images. [sent-94, score-0.283]
</p><p>37 Therefore, we propose a spectral approach that detects these persistent image features using the eigen-spectrum of the joint image graph computed from appropriate local gradients in the two images. [sent-95, score-0.776]
</p><p>38 Before going into the details of the way the graph is constructed, let us focus on the images in the second to fifth columns of Fig. [sent-96, score-0.191]
</p><p>39 In each column, the top and bottom images correspond to one of the eigen-functions of the joint graph reshaped back to the size of the images. [sent-98, score-0.35]
</p><p>40 images in a single column) the distribution and shapes of these eigen extrema correspond well between the two images and the image regions where this correspondence is strong is in agreement with the actually corresponding image regions. [sent-102, score-0.402]
</p><p>41 Thus, by computing features that encode these extrema (both in their shape and the eigenenergy profile), we can more robustly match these images without relying on descriptors computed directly from the images. [sent-103, score-0.47]
</p><p>42 222888000422  First,  we will review basic fundamentals of the image graph construction and its spectrum, followed by a look at the actual features we use to build the joint image graph. [sent-105, score-0.287]
</p><p>43 Then, we will characterize the eigen-function extrema as persistent regions and discuss algorithms to detect and match these extrema. [sent-106, score-0.672]
</p><p>44 Image Graph  The spectral analysis of the content of an image is carried out on a weighted image graph G(V, E, W) which contains all the image-pixels as vertices in the vertex-set V of cardinality n. [sent-109, score-0.274]
</p><p>45 We can collect these weights into an n n affinity matrix WWe = ca (wij )i,j=1,. [sent-112, score-0.223]
</p><p>46 o fH aonw iemveagr,e i bna tsheids paper, we will study the individual eigen-vectors directly to ascertain useful persistent regions in the image. [sent-127, score-0.479]
</p><p>47 Then the joint image graph fGor(V, i mEa, gWes) Iis daenfdine Id such that V = V1 ∪ V2, E = E1∪E2 ∪ V1 V2 where V1 V2 is the set of edges connecting every pair oVf vertices in ×(VV1 , V2). [sent-130, score-0.345]
</p><p>48 T×he n eigenspectra for the joint graph can be computed exactly as before by defining the normalized laplacian L¯ and carrying out its eigen-value decomposition. [sent-134, score-0.378]
</p><p>49 Image Features and the Joint Spectrum  Consider first an experiment where we perform spectral analysis of the joint image graph G(V, E, W) with the matrix W defined directly in terms of the pixel color values in the two images, i. [sent-137, score-0.384]
</p><p>50 3 shows the second through fifth eigen-function pairs (reshaped back into a matrix) for the same image pair as in Fig. [sent-142, score-0.215]
</p><p>51 It is clear that we do not see much correspondence between the eigenfunctions in this case this motivates the need for features stronger than just the individual pixel colors. [sent-144, score-0.197]
</p><p>52 Note that unlike most image-domain spectral approaches in the literature, we do not use a spatial affinity term to reduce the influence of spatially separated pixels. [sent-165, score-0.285]
</p><p>53 With a spatial proximity term in the affinity matrix, we run the risk of artificially limiting the spatial extent of an eigen-function extrema and thus rendering the derived features less distinctive. [sent-167, score-0.485]
</p><p>54 Given the joint graph affinity matrix W from eqns-(1), (2) and (3), it is straightforward to compute the eigenspectra. [sent-168, score-0.411]
</p><p>55 But before we do that, let us see if we can determine any correspondence information between image regions by extracting the spectra from each image graph separately. [sent-169, score-0.237]
</p><p>56 Even though the eigen-functions correctly represent the grouping of gradient information as is expected from our gradient features, one cannot infer useful correspondence information between image regions from the corresponding pair of eigen-functions directly. [sent-172, score-0.216]
</p><p>57 The first n1 entries ofuk are reshaped to the dimensions ofI1 by assigning its component evsahluaepse dtoto tthhee sampling nlosocaftIions where the features were extracted from and then interpolating the values in between. [sent-180, score-0.165]
</p><p>58 Characterization of persistent regions As discussed before, the extrema of the eigen-function pairs . [sent-184, score-0.666]
</p><p>59 , represent persistent features that can serve well as means of finding correspondences across these difficult pairs of images. [sent-187, score-0.582]
</p><p>60 We want to characterize these extrema in terms of their location, their region of support as well as the variation ofthe eigen-energy in the vicinity of each extrema. [sent-188, score-0.207]
</p><p>61 Since the extrema can commonly exhibit elongated ridge-like shapes, an isotropic blob-detector would not work well. [sent-189, score-0.199]
</p><p>62 The intensity-based MSER detector is typically used to find affine-covariant regions in an image by looking for water-shed areas that remain stable as an image intensity threshold is varied. [sent-192, score-0.254]
</p><p>63 Each detected region is a set of connected pixels to which an ellipse is typically fit to represent the support region. [sent-193, score-0.243]
</p><p>64 Then, we run intensity-based MSER along with ellipse fitting to detect stable affine regions. [sent-195, score-0.209]
</p><p>65 Each detected MSER ellipse is affine corrected to a circular region and a SIFT descriptor is computed for a region five times the ellipse size by computing gradients on the eigen-function. [sent-199, score-0.506]
</p><p>66 Compute affinity matrix W using(x xeq)n ats- a(1 sp), a(2ti)a la snadm (3p)li. [sent-203, score-0.19]
</p><p>67 We will use the term JSPEC to refer to this feature which combines MSER ellipse keypoint with the eigen-space SIFT descriptor. [sent-218, score-0.193]
</p><p>68 Eigen-function feature matching The centroids of the MSER ellipses along with their associated SIFT descriptors can be treated as image features in a traditional sense. [sent-221, score-0.29]
</p><p>69 Therefore, we adopt a simple approach to matching these features by using the nearestneighbor criterion coupled with the ratio-test [4]. [sent-222, score-0.223]
</p><p>70 However, we match the descriptors from each pair of eigen-functions independently i. [sent-223, score-0.267]
</p><p>71 for each descriptor in the nearest and second-nearest descriptors are searched only in and the association to the nearest descriptor is accepted only if its euclidean descriptor distance is less than τ times the distance to the second-nearest descriptor. [sent-225, score-0.374]
</p><p>72 To enforce a stronger match criterion, we perform matching from to and from to and keep the matches  (J(1k),J2(k))  J1(k),  J2(k)  J1(k)  J2(k)  J2(k)  J1(k)  which are mutually consistent. [sent-226, score-0.257]
</p><p>73 This gives us a set of correspondences Ck from the eigen-function pair It sshpoounldde nbcee sno Cted that unlike traditional SIFT feature matching, our constraint on being able to match between individual eigen-function pairs results in a much stronger match criterion. [sent-227, score-0.411]
</p><p>74 This dataset contains 46 pairs of images exhibiting dramatic illumination, age and rendering style differences. [sent-234, score-0.313]
</p><p>75 Some image pairs are pre-registered with a homography to focus on appearance differences, while others exhibit both geometric and photometric variation. [sent-235, score-0.211]
</p><p>76 Hauagge and Snavely [3] evaluated their local symmetry features first, in terms of the detector repeatability and second, in terms of descriptor mean-average-precision performance. [sent-237, score-0.506]
</p><p>77 Detector repeatability To evaluate the repeatability of the eigen-space MSER features for a given image pair, we consider all the detections before the SIFT matching step. [sent-255, score-0.762]
</p><p>78 We collect all the features from across all eigen-functions into two sets of keypoints K1 and K2 for images I1 and I2 respectively. [sent-256, score-0.219]
</p><p>79 Each keypoint has a cenfotrorid im aangde an ellipse Iassociated with it. [sent-257, score-0.193]
</p><p>80 Therefore, we can directly apply the repeatability metric from [6] which we briefly review next. [sent-258, score-0.283]
</p><p>81 Each keypoint k1 ∈ K1 is warped into I2’s coordinate frame using the  ground-truth homography IH12 and its (warped) support region is compared with the support region of each keypoint k2 ∈ K2 to obtain an overlap score. [sent-259, score-0.38]
</p><p>82 Hauagge and Snavely [3] computed the repeatability scores of their features by considering subsets of top-100 and top-200 detections ordered by either feature scale or score. [sent-267, score-0.386]
</p><p>83 Our MSER detector does not output a detection score and so we only present repeatability numbers based on ordering by scale. [sent-269, score-0.347]
</p><p>84 We observe that our JSPEC features achieve slightly better repeatability than what SYM-G achieved using the top scoring 200 detections. [sent-271, score-0.349]
</p><p>85 Then, we match these descriptors using the standard ratio test [4] on the top two nearest neighbor distances. [sent-279, score-0.176]
</p><p>86 For a given choice of the ratio threshold, we get a set of candidate correspondences which are evaluated with the ellipse overlap criterion of [6] using the ground-truth homography H12 to compute a point on the precision-recall curve. [sent-280, score-0.339]
</p><p>87 This is meant to test how well thien descriptor matches appearance of perfect geometrically matching locations. [sent-288, score-0.266]
</p><p>88 Even though we do not use the grid-detector, a comparison of the JSPEC PR-curves with other curves in the “Grid” row clearly indicate that SIFT features computed on the eigen-functions match better across the extreme day-night appearance changes. [sent-289, score-0.208]
</p><p>89 The graffiti image pair (fifth column) shows that we perform similar to the SYMD descriptor on SIFT features but, as expected, worse than the SIFT detector-descriptor pair. [sent-290, score-0.307]
</p><p>90 Precision-Recall curves comparing performance of the spectral approach (JSPEC) with the features evaluated in [3]. [sent-293, score-0.195]
</p><p>91 Each column shows plots for the image pair in the top row. [sent-294, score-0.167]
</p><p>92 Also note that we have not applied either the bi-directional matching criterion or the “match only within each eigen-function pair” criterion to obtain these precision-recall curves for a fair comparison with other methods (which also do not apply the bidirectional constraint). [sent-298, score-0.221]
</p><p>93 The matches overlaid on the images are the final matches obtained after bi-directional SIFT matching on the JSPEC features at a ratio-threshold of 0. [sent-309, score-0.322]
</p><p>94 7 shows three more different kinds of examples with the correspondences detected in each of the four eigen-function pairs collected together and overlaid in the first column. [sent-317, score-0.195]
</p><p>95 Conclusion Image matching across different illumination conditions and capture times has been addressed in the past by comparing descriptors of local neighborhoods or employing discriminative learning of local patches. [sent-323, score-0.275]
</p><p>96 duced global image information into the matching process by computing the spectrum of the graph of all pixels in both images associated only by the similarity of their neighborhoods. [sent-326, score-0.358]
</p><p>97 Significantly, the eigen-functions of this joint graph exhibit persistent regions across disparate images which can be captured with the MSER characteristic point detector and represented with the SIFT descriptor in the resulting stable regions. [sent-327, score-1.122]
</p><p>98 Such characteristic points exhibit surprisingly high repeatability and local similarity. [sent-328, score-0.323]
</p><p>99 In our ongoing work, we study how such persistent features can be used for testing geometric consistency, a task impossible when no correspondences can be established in the raw image domain. [sent-329, score-0.476]
</p><p>100 Establishing the two view geom-  etry using these persistent photography:  features  would also allow re-  establishing the same view for a photo today  given a reference photo from the past. [sent-330, score-0.458]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('persistent', 0.36), ('jspec', 0.352), ('mser', 0.297), ('repeatability', 0.283), ('sift', 0.166), ('extrema', 0.159), ('disparate', 0.159), ('affinity', 0.156), ('spectral', 0.129), ('ellipse', 0.123), ('night', 0.12), ('graph', 0.112), ('joint', 0.109), ('reshaped', 0.099), ('hauagge', 0.099), ('day', 0.098), ('polyline', 0.096), ('descriptors', 0.095), ('matching', 0.093), ('descriptor', 0.093), ('pair', 0.091), ('spectrum', 0.088), ('match', 0.081), ('pairs', 0.075), ('intra', 0.074), ('regions', 0.072), ('keypoint', 0.07), ('contours', 0.069), ('features', 0.066), ('homography', 0.066), ('criterion', 0.064), ('dusk', 0.064), ('eigenspectra', 0.064), ('historic', 0.064), ('monument', 0.064), ('detector', 0.064), ('contour', 0.064), ('shrivastava', 0.062), ('inter', 0.062), ('keypoints', 0.059), ('eigen', 0.058), ('proximity', 0.057), ('laplacian', 0.057), ('graffiti', 0.057), ('symd', 0.057), ('illumination', 0.056), ('correspondence', 0.053), ('stable', 0.052), ('matches', 0.05), ('uk', 0.05), ('fi', 0.05), ('dramatic', 0.05), ('correspondences', 0.05), ('fifth', 0.049), ('region', 0.048), ('maxima', 0.048), ('rendering', 0.047), ('ascertain', 0.047), ('bars', 0.046), ('snavely', 0.046), ('eigenfunctions', 0.045), ('glasner', 0.045), ('plots', 0.044), ('extremal', 0.044), ('wij', 0.043), ('warped', 0.042), ('toshev', 0.041), ('arl', 0.041), ('style', 0.041), ('exhibit', 0.04), ('ive', 0.04), ('relying', 0.039), ('detections', 0.037), ('detected', 0.037), ('intensity', 0.036), ('defining', 0.036), ('overlap', 0.036), ('ellipses', 0.036), ('exhibiting', 0.036), ('pixels', 0.035), ('discrepancy', 0.034), ('affine', 0.034), ('age', 0.034), ('matrix', 0.034), ('vertices', 0.033), ('matas', 0.033), ('substantial', 0.033), ('collect', 0.033), ('overlaid', 0.033), ('stronger', 0.033), ('column', 0.032), ('establishing', 0.032), ('kind', 0.031), ('across', 0.031), ('looking', 0.03), ('images', 0.03), ('appearance', 0.03), ('blue', 0.03), ('graphs', 0.029), ('smallest', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="234-tfidf-1" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>Author: Mayank Bansal, Kostas Daniilidis</p><p>Abstract: We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night), age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixellevel descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art.</p><p>2 0.14785817 <a title="234-tfidf-2" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>Author: Samuele Salti, Alessandro Lanza, Luigi Di_Stefano</p><p>Abstract: The paper conjectures and demonstrates that repeatable keypoints based on salient symmetries at different scales can be detected by a novel analysis grounded on the wave equation rather than the heat equation underlying traditional Gaussian scale–space theory. While the image structures found by most state-of-the-art detectors, such as blobs and corners, occur typically on planar highly textured surfaces, salient symmetries are widespread in diverse kinds of images, including those related to untextured objects, which are hardly dealt with by current feature-based recognition pipelines. We provide experimental results on standard datasets and also contribute with a new dataset focused on untextured objects. Based on the positive experimental results, we hope to foster further research on the promising topic ofscale invariant analysis through the wave equation.</p><p>3 0.14328717 <a title="234-tfidf-3" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>4 0.11219945 <a title="234-tfidf-4" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<p>Author: Eduard Trulls, Iasonas Kokkinos, Alberto Sanfeliu, Francesc Moreno-Noguer</p><p>Abstract: In this work we exploit segmentation to construct appearance descriptors that can robustly deal with occlusion and background changes. For this, we downplay measurements coming from areas that are unlikely to belong to the same region as the descriptor’s center, as suggested by soft segmentation masks. Our treatment is applicable to any image point, i.e. dense, and its computational overhead is in the order of a few seconds. We integrate this idea with Dense SIFT, and also with Dense Scale and Rotation Invariant Descriptors (SID), delivering descriptors that are densely computable, invariant to scaling and rotation, and robust to background changes. We apply our approach to standard benchmarks on large displacement motion estimation using SIFT-flow and widebaseline stereo, systematically demonstrating that the introduction of segmentation yields clear improvements.</p><p>5 0.10929426 <a title="234-tfidf-5" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>Author: Shmuel Asafi, Daniel Cohen-Or</p><p>Abstract: In this paper, we introduce a new approach to constrained clustering which treats the constraints as features. Our method augments the original feature space with additional dimensions, each of which derived from a given Cannot-link constraints. The specified Cannot-link pair gets extreme coordinates values, and the rest of the points get coordinate values that express their spatial influence from the specified constrained pair. After augmenting all the new features, a standard unconstrained clustering algorithm can be performed, like k-means or spectral clustering. We demonstrate the efficacy of our method for active semi-supervised learning applied to image segmentation and compare it to alternative methods. We also evaluate the performance of our method on the four most commonly evaluated datasets from the UCI machine learning repository.</p><p>6 0.10159196 <a title="234-tfidf-6" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>7 0.098518968 <a title="234-tfidf-7" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>8 0.094225086 <a title="234-tfidf-8" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>9 0.093697153 <a title="234-tfidf-9" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>10 0.091287158 <a title="234-tfidf-10" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>11 0.09110333 <a title="234-tfidf-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.08920458 <a title="234-tfidf-12" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>13 0.086558312 <a title="234-tfidf-13" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>14 0.085473776 <a title="234-tfidf-14" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>15 0.081083104 <a title="234-tfidf-15" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>16 0.080482543 <a title="234-tfidf-16" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>17 0.079771876 <a title="234-tfidf-17" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>18 0.078383476 <a title="234-tfidf-18" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>19 0.07721445 <a title="234-tfidf-19" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>20 0.077157132 <a title="234-tfidf-20" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, 0.006), (2, 0.038), (3, 0.018), (4, 0.047), (5, -0.022), (6, -0.023), (7, -0.037), (8, -0.059), (9, -0.047), (10, 0.021), (11, 0.0), (12, 0.02), (13, -0.025), (14, 0.07), (15, -0.092), (16, 0.012), (17, -0.073), (18, 0.085), (19, 0.01), (20, 0.093), (21, 0.056), (22, 0.01), (23, 0.009), (24, 0.057), (25, -0.032), (26, 0.079), (27, -0.029), (28, 0.008), (29, 0.021), (30, 0.09), (31, 0.011), (32, 0.052), (33, -0.031), (34, 0.063), (35, 0.043), (36, 0.07), (37, 0.126), (38, 0.047), (39, 0.022), (40, 0.001), (41, 0.031), (42, -0.024), (43, 0.036), (44, 0.037), (45, -0.118), (46, 0.063), (47, 0.04), (48, 0.066), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94009054 <a title="234-lsi-1" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>Author: Mayank Bansal, Kostas Daniilidis</p><p>Abstract: We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night), age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixellevel descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art.</p><p>2 0.76860338 <a title="234-lsi-2" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>Author: Samuele Salti, Alessandro Lanza, Luigi Di_Stefano</p><p>Abstract: The paper conjectures and demonstrates that repeatable keypoints based on salient symmetries at different scales can be detected by a novel analysis grounded on the wave equation rather than the heat equation underlying traditional Gaussian scale–space theory. While the image structures found by most state-of-the-art detectors, such as blobs and corners, occur typically on planar highly textured surfaces, salient symmetries are widespread in diverse kinds of images, including those related to untextured objects, which are hardly dealt with by current feature-based recognition pipelines. We provide experimental results on standard datasets and also contribute with a new dataset focused on untextured objects. Based on the positive experimental results, we hope to foster further research on the promising topic ofscale invariant analysis through the wave equation.</p><p>3 0.69811964 <a title="234-lsi-3" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>Author: Elhanan Elboer, Michael Werman, Yacov Hel-Or</p><p>Abstract: The graph Laplacian operator, which originated in spectral graph theory, is commonly used for learning applications such as spectral clustering and embedding. In this paper we explore the Laplacian distance, a distance function related to the graph Laplacian, and use it for visual search. We show that previous techniques such as Matching by Tone Mapping (MTM) are particular cases of the Laplacian distance. Generalizing the Laplacian distance results in distance measures which are tolerant to various visual distortions. A novel algorithm based on linear decomposition makes it possible to compute these generalized distances efficiently. The proposed approach is demonstrated for tone mapping invariant, outlier robust and multimodal template matching.</p><p>4 0.6891194 <a title="234-lsi-4" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>5 0.6876393 <a title="234-lsi-5" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>Author: Feng Zhou, Fernando De_la_Torre</p><p>Abstract: Graph matching (GM) is a fundamental problem in computer science, and it has been successfully applied to many problems in computer vision. Although widely used, existing GM algorithms cannot incorporate global consistence among nodes, which is a natural constraint in computer vision problems. This paper proposes deformable graph matching (DGM), an extension of GM for matching graphs subject to global rigid and non-rigid geometric constraints. The key idea of this work is a new factorization of the pair-wise affinity matrix. This factorization decouples the affinity matrix into the local structure of each graph and the pair-wise affinity edges. Besides the ability to incorporate global geometric transformations, this factorization offers three more benefits. First, there is no need to compute the costly (in space and time) pair-wise affinity matrix. Second, it provides a unified view of many GM methods and extends the standard iterative closest point algorithm. Third, it allows to use the path-following optimization algorithm that leads to improved optimization strategies and matching performance. Experimental results on synthetic and real databases illustrate how DGM outperforms state-of-the-art algorithms for GM. The code is available at http : / / human s en s ing . c s . cmu .edu / fgm.</p><p>6 0.68208283 <a title="234-lsi-6" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<p>7 0.67829841 <a title="234-lsi-7" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>8 0.67686522 <a title="234-lsi-8" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>9 0.67469996 <a title="234-lsi-9" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>10 0.66055721 <a title="234-lsi-10" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>11 0.64989245 <a title="234-lsi-11" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>12 0.62955129 <a title="234-lsi-12" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>13 0.62925583 <a title="234-lsi-13" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<p>14 0.62923735 <a title="234-lsi-14" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>15 0.62386733 <a title="234-lsi-15" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>16 0.61997688 <a title="234-lsi-16" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>17 0.61058217 <a title="234-lsi-17" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>18 0.60963911 <a title="234-lsi-18" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>19 0.60227942 <a title="234-lsi-19" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>20 0.6001302 <a title="234-lsi-20" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.096), (16, 0.036), (26, 0.045), (28, 0.217), (33, 0.297), (39, 0.012), (67, 0.059), (69, 0.042), (87, 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92949182 <a title="234-lda-1" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>2 0.92522585 <a title="234-lda-2" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>Author: Austin Abrams, Kylia Miskell, Robert Pless</p><p>Abstract: Shadows encode a powerful geometric cue: if one pixel casts a shadow onto another, then the two pixels are colinear with the lighting direction. Given many images over many lighting directions, this constraint can be leveraged to recover the depth of a scene from a single viewpoint. For outdoor scenes with solar illumination, we term this the episolar constraint, which provides a convex optimization to solve for the sparse depth of a scene from shadow correspondences, a method to reduce the search space when finding shadow correspondences, and a method to geometrically calibrate a camera using shadow constraints. Our method constructs a dense network of nonlocal constraints which complements recent work on outdoor photometric stereo and cloud based cues for 3D. We demonstrate results across a variety of time-lapse sequences from webcams “in . wu st l. edu (b)(c) the wild.”</p><p>3 0.921471 <a title="234-lda-3" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>4 0.91322345 <a title="234-lda-4" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>5 0.90592062 <a title="234-lda-5" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>Author: Minh Hoai, Andrew Zisserman</p><p>Abstract: The objective of this work is to learn sub-categories. Rather than casting this as a problem of unsupervised clustering, we investigate a weakly supervised approach using both positive and negative samples of the category. We make the following contributions: (i) we introduce a new model for discriminative sub-categorization which determines cluster membership for positive samples whilst simultaneously learning a max-margin classifier to separate each cluster from the negative samples; (ii) we show that this model does not suffer from the degenerate cluster problem that afflicts several competing methods (e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discover interpretable sub-categories in various datasets. The model is evaluated experimentally over various datasets, and itsperformance advantages over k-means and Latent SVM are demonstrated. We also stress test the model and show its resilience in discovering sub-categories as the parameters are varied.</p><p>6 0.89977622 <a title="234-lda-6" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>7 0.88432509 <a title="234-lda-7" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>same-paper 8 0.87922764 <a title="234-lda-8" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>9 0.85406494 <a title="234-lda-9" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>10 0.84889495 <a title="234-lda-10" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>11 0.84672153 <a title="234-lda-11" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>12 0.84628838 <a title="234-lda-12" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>13 0.84548348 <a title="234-lda-13" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>14 0.84445512 <a title="234-lda-14" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>15 0.84418786 <a title="234-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.84348691 <a title="234-lda-16" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>17 0.8429625 <a title="234-lda-17" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>18 0.84253258 <a title="234-lda-18" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>19 0.84206361 <a title="234-lda-19" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>20 0.84194976 <a title="234-lda-20" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
