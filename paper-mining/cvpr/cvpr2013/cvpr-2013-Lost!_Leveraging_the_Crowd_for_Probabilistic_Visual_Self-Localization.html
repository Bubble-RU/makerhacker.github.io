<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-274" href="#">cvpr2013-274</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</h1>
<br/><p>Source: <a title="cvpr-2013-274-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Brubaker_Lost_Leveraging_the_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>Reference: <a title="cvpr-2013-274-reference" href="../cvpr2013_reference/cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu c  Abstract In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. [sent-7, score-0.635]
</p><p>2 Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e. [sent-9, score-0.591]
</p><p>3 By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads. [sent-12, score-1.096]
</p><p>4 In this paper we are interested in building affordable and ro-  bust solutions to self-localization for the autonomous driving scenario. [sent-16, score-0.237]
</p><p>5 Notably, the GPS signal is not always available, and its localization can become imprecise (e. [sent-19, score-0.124]
</p><p>6 They assume that image or depth features from anywhere around the globe can be stored in a database, and cast the localization problem as a retrieval task. [sent-24, score-0.162]
</p><p>7 However, it remains unclear if main-  a vehicle with an average accuracy of 3. [sent-29, score-0.146]
</p><p>8 1m within a map of ∼ 2, v1e5h0ikcmle wofi rhoa adn using only cvuirsuaacly odometry wmitehainsu raem meanpts o afn ∼d freely available maps. [sent-30, score-0.59]
</p><p>9 In this case, localization took less than 21 seconds. [sent-31, score-0.124]
</p><p>10 Furthermore, these solutions are far from affordable as every corner of the world needs to be visited and updated constantly. [sent-34, score-0.13]
</p><p>11 The OSM maps are detailed and freely available, making this an inexpensive solution. [sent-39, score-0.104]
</p><p>12 Towards this goal, we derive a probabilistic map localization approach that uses visual odometry estimates and OSM data as the only inputs. [sent-41, score-0.715]
</p><p>13 We demonstrate the effectiveness of our approach on a variety of challenging scenarios making use of the recently released KITTI visual odometry benchmark [8]. [sent-42, score-0.487]
</p><p>14 As our experiments show, we are able to localize ourselves after only a few seconds of driving with an accuracy of 3 meters on a 18km2 map containing 2, 150km of drivable roads. [sent-43, score-0.394]
</p><p>15 Here we use OSM maps and visual odometry estimates as the only inputs for localizing within the map. [sent-47, score-0.526]
</p><p>16 Related Work Early approaches for map localization [5, 7, 10, 20] make use of Monte Carlo methods and the Markov assumption to maintain a sample-based posterior representation of the agent’s pose. [sent-49, score-0.271]
</p><p>17 In contrast, the maps used by our localization approach require only a few gigabytes for storing the whole planet earth1 . [sent-54, score-0.163]
</p><p>18 Relative motion estimates can be obtained using visual odometry [19], which refers to generating motion estimates from visual input alone. [sent-55, score-0.523]
</p><p>19 In contrast, the proposed approach enables geographic localization and relies only on freely available map information (i. [sent-61, score-0.307]
</p><p>20 Visual Localization We propose to use one or two roof-mounted cameras to self-localize a driving vehicle. [sent-69, score-0.11]
</p><p>21 The only other information we have is a map of the environment in which the vehicle is driving. [sent-70, score-0.22]
</p><p>22 This map contains streets as line segments as well as intersection points. [sent-71, score-0.3]
</p><p>23 We exploit visual odometry in order to obtain the trajectory of the vehicle. [sent-72, score-0.521]
</p><p>24 As this trajectory is too noisy for direct shape matching, here we propose a probabilistic approach to self-localization that employs visual odometry measurements in order to determine the instantaneous position and orientation ofthe vehicle in a given map. [sent-73, score-0.75]
</p><p>25 Towards this goal, we first define a graph-based representation of the map as well as a probabilistic model of  how a vehicle can traverse the graph. [sent-74, score-0.25]
</p><p>26 , street types, traffic lights, postboxes, trees, shops, power lines) and most importantly can be freely downloaded and used under the Open Database License. [sent-86, score-0.262]
</p><p>27 We extracted all crossings and drivable roads (represented as piece-wise linear segments) connecting them. [sent-87, score-0.205]
</p><p>28 For each street we additionally extract its type (i. [sent-88, score-0.197]
</p><p>29 By splitting each bi-directional street into two one-way streets and ’smoothing’ intersections using circular arcs, we obtain a lane-based –  –  map representation, on which we define the vehicle state. [sent-91, score-0.626]
</p><p>30 Lane-based Map Representation We assume that the map data is represented by a directed graph where nodes represent street segments and edges define the connectivity of the roads. [sent-94, score-0.317]
</p><p>31 As mentioned above, we convert all street segments to oneway streets. [sent-96, score-0.243]
</p><p>32 Street Segment: (right) Each street segment has a start and end position p0 and p1, a length ? [sent-104, score-0.304]
</p><p>33 The parameters of the street segment geometry are described in Fig. [sent-110, score-0.251]
</p><p>34 We define the position and orientation of a vehicle in the map in terms of the street segment u that the vehicle is on, the distance from the origin of that street segment d and the offset of the local street heading θ. [sent-112, score-1.282]
</p><p>35 The global heading of the vehicle is then θ + β + αd and its position is ? [sent-113, score-0.363]
</p><p>36 State-Space Model We define the state of the model at time t to be xt = (ut, st) where st = (dt, θt, and are the distance and angle at the previous time defined relative to the current street ut. [sent-122, score-0.521]
</p><p>37 Visual odometry observations at time t, yt, measure the linear and angular displacement from time t − 1 to time t. [sent-123, score-0.451]
</p><p>38 The curvat=ure ( 1of, −th1e, street, αu, is necessary αbec,a1u,s−e 1th)e global heading of the vehicle depends on both d and θ. [sent-125, score-0.31]
</p><p>39 We factorize the state transition distribution p(xt |xt−1 ) = p(ut |xt−1)p(st |ut, xt−1) in terms of the street tran|xsition probability p(ut |x|tu−1), and the state transition model p(st |ut, xt−1). [sent-126, score-0.407]
</p><p>40 Th|ex state transition model is  assumed to be Gau|uss-Linear, taking the form p(st|ut, xt−1)  = N(st|Aut,ut−1st−1 + but,ut−1  ,  Σuxt) (2) with Σuxt the covariance matrix for a given ut which is learned from data as discussed in Section 4. [sent-127, score-0.579]
</p><p>41 Because the components of st are relative to the current street, ut, when ut ut−1 the state transition model must be adjusted so that? [sent-133, score-0.749]
</p><p>42 ut−1 subtracted, and needs to be updated so that relative to ut has the same global heading as θt−1 relative to ut−1 . [sent-136, score-0.638]
</p><p>43 ut−1) is the angle between the end of ut an−d (thβe beginning of ut−1 . [sent-142, score-0.474]
</p><p>44 The street transition probability p(ut |xt−1) defines the probability of transitioning onto the stree|tx ut given the previous state xt−1 . [sent-143, score-0.807]
</p><p>45 We use the Gaussian transition dynamics to define the probability of changing street segments, i. [sent-144, score-0.27]
</p><p>46 As short segments cannot bejumped over in a single time step, we introduce “leapfrog” edges which allow the vehicle to move from ut−1 to any ut to which there exists a path in the graph. [sent-151, score-0.666]
</p><p>47 As the speed of the vehicle is assumed to be limited, we need to add edges only up to a certain distance. [sent-153, score-0.146]
</p><p>48 We can write the posterior using  the product rule as p(xt |y1:t) = p(st |ut, y1:t)p(ut |y1:t), where p(ut |y1:t) is a disc|yrete distributi|oun over street|sy and p(st |ut, y1:t|)y is a continuous distribution over the position and o|urientation on a given street. [sent-158, score-0.126]
</p><p>49 Return: Posterior at t,P P{Put , Mtu}  Return:  Posterior  at  tP,  {P,M}  where Nut is the number of components for the mixture as-  ××  {πu(it),μ(uit), Σ(uit)}iN=u1t  sociated with ut and Mtut = are the parameters of the amndixt Mure for= ut. [sent-171, score-0.582]
</p><p>50 Substituting in |thye mixture model form of p(st−1 |ut−1 , y1:t−1), and the model transition dynamics the integrand in the above equation becomes ? [sent-177, score-0.152]
</p><p>51 Map Size: Driving time (left) and distance travelled (right) before localization as a function of the map size. [sent-203, score-0.198]
</p><p>52 This algorithm can also be parallelized by assigning subsets of streets to different threads, a fact which we exploit to achieve real-  time performance. [sent-205, score-0.18]
</p><p>53 First, for each pair of connected streets, the modes that transition from ut−1 to ut are all likely similar. [sent-215, score-0.575]
</p><p>54 Thus, we truncate the distribution for streets whose probability p(ut |y1:t) is below a threshold and discard their modes. [sent-218, score-0.211]
</p><p>55 To prevent this from happening, we run a mixture model simplification procedure when the number of modes on a street segment exceeds a threshold. [sent-222, score-0.455]
</p><p>56 84d1603m◦s tero  odometry, “G” GPS-based odometry and “O” is the oracle error, i. [sent-240, score-0.487]
</p><p>57 Experimental Evaluation To evaluate our approach in realistic situations, we performed experiments on the recently released KITTI benchmark for visual odometry [8]. [sent-246, score-0.487]
</p><p>58 The visual odometry input to our system is computed using LIBVISO2 [9], a freely available library for monocular and stereo visual odometry. [sent-250, score-0.684]
</p><p>59 Slower rates were found to suffer from excessive accumulated odometry error. [sent-252, score-0.451]
</p><p>60 On average, they cover an area of 2km2 and contain 47km of drivable roads. [sent-254, score-0.149]
</p><p>61 1 for example, which  covers 18km2 and contains 2,150km of drivable roads. [sent-256, score-0.149]
</p><p>62 = 10−2 which is applied when the number of mixture components for a segment is greater than one per 10m segment length. [sent-258, score-0.216]
</p><p>63 Here, “M” and “S” indicate results using monocular and stereo visual odometry respectively. [sent-261, score-0.583]
</p><p>64 In addition, we computed odometry measurements from the GPS trajectories (entry “G” in the table) and ran our algorithm using the learned parameters from the stereo data. [sent-262, score-0.494]
</p><p>65 In particular, the street state evolution noise covariance Σux, the angular AR(1) parameter γu and the observation noise Σuy were estimated using maximum likelihood. [sent-269, score-0.285]
</p><p>66 We learn different parameters for highways and city/rural roads as the visual odometry performs  significantly worse at higher speeds. [sent-270, score-0.543]
</p><p>67 The accuracy of position and heading estimates is not well defined until the posterior has converged to a single mode. [sent-271, score-0.29]
</p><p>68 We define a sequence to be localized when for at least five seconds there is a single mode in the posterior and the distance to the ground truth position from that mode is less than 20 meters. [sent-274, score-0.209]
</p><p>69 Once the criteria for localization is met, all subsequent frames are considered localized. [sent-275, score-0.124]
</p><p>70 Errors in global position and heading of the MAP state for localized frames were computed using the GPS data as ground truth. [sent-276, score-0.289]
</p><p>71 Overall, we are able to estimate the position and heading to 3. [sent-278, score-0.217]
</p><p>72 Using monocular odometry as input performs worse, but is still accurate to 18. [sent-285, score-0.504]
</p><p>73 This sequence contains highway driving only,  where high speeds and sparse visual features make monocular visual odometry very challenging, leading to an accumulated error in the monocular odometry of more than 500m. [sent-289, score-1.308]
</p><p>74 In contrast, while the stereo visual odometry has somewhat higher than typical errors on this sequence, our method is still able to localize successfully as shown in Fig. [sent-290, score-0.591]
</p><p>75 Localization Accuracy with Noise: Position and heading error with different noise levels. [sent-293, score-0.222]
</p><p>76 Sequence 04 is a short sequence on a straight road segment and, in the absence of any turns, cannot be localized beyond somewhere on the long road segment. [sent-297, score-0.215]
</p><p>77 Simplification Threshold: We study the impact of vary-  ing the mixture model simplification threshold. [sent-300, score-0.207]
</p><p>78 4 depicts computation time per frame and localized position error averaged over sequences as a function of the threshold, ranging from 10−5 to 0. [sent-302, score-0.167]
</p><p>79 As expected, computation time decreases and error increases with more simplification (i. [sent-305, score-0.127]
</p><p>80 Map Size: To investigate the impact of region size on localization performance, we assign uniform probability to portions ofthe map in a square region centered at the ground truth initial position and give zero initial probability to map locations outside the region. [sent-310, score-0.356]
</p><p>81 We varied the size of the square from 100m up to the typical map size of 2km, constituting an average of 300m to 47km of drivable road. [sent-311, score-0.223]
</p><p>82 We evaluated the time to localization for all non-ambiguous sequences (i. [sent-312, score-0.168]
</p><p>83 Somewhat surprisingly, after the region becomes sufficiently large, the impact on localization becomes negligible. [sent-317, score-0.155]
</p><p>84 This is due to the inherent uniqueness of most sufficiently long paths, even in very large regions with many streets as the one shown in Fig. [sent-318, score-0.18]
</p><p>85 While localization in a large and truly perfect Manhattan world with equiangular intersections and equilength streets would be nearly impossible based purely on odometry, such  a world is not often realized as even Manhattan itself has non-perpendicular roads such as Broadway! [sent-320, score-0.42]
</p><p>86 Noise: To study the impact of noise on the localization accuracy, we synthesized odometry measurements by adding  Figure7. [sent-321, score-0.634]
</p><p>87 Sequence 04 consists of a short, straight driving sequence and 06 traverses a symmetric part of the map, resulting in two equally likely modes. [sent-323, score-0.182]
</p><p>88 For each sequence five different samples of noisy odometry were created with signal-to-noise ratios (SNR) ranging from 0. [sent-325, score-0.494]
</p><p>89 6 depicts error in position and heading after localization. [sent-328, score-0.247]
</p><p>90 To test the ability of our method to scale to large maps we ran the sequences using stereo odometry and a map covering the entire urban district of Karlsruhe, Germany. [sent-332, score-0.651]
</p><p>91 This map was approximately 18km2 and had over 2,150km of drivable road. [sent-333, score-0.223]
</p><p>92 Conclusions In this paper we have proposed an affordable approach to self-localization which employs (one or two) cameras mounted on the vehicle as well as crowd sourcing in the form of free online maps. [sent-339, score-0.247]
</p><p>93 Furthermore, we have validated our approach on the KITTI visual odometry benchmark and shown that we are able to localize our vehicle with a precision of 3 m after only 20 seconds of driving. [sent-341, score-0.694]
</p><p>94 In particular, OpenStreetMaps contains many other salient pieces of information to aid in localization such as speed limits, street names, numbers of lanes, and more; we plan to exploit this information  in the future. [sent-343, score-0.321]
</p><p>95 Thelft  most column shows the full map region for each sequence, followed by zoomed in sections of the map showing the posterior distribution over time. [sent-350, score-0.221]
</p><p>96 Monte carlo localization: Efficient position estimation for mobile robots. [sent-397, score-0.141]
</p><p>97 0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges. [sent-474, score-0.192]
</p><p>98 Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks. [sent-521, score-0.193]
</p><p>99 Performing the above for each component and each pair of nodes produces a set of mixture model components for each u, the weights of which are proportional to Put. [sent-557, score-0.108]
</p><p>100 After normalizing the mixtures for each street, normalizing across streets allows for the computation of Put, the probability of being on a given street. [sent-558, score-0.18]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ut', 0.474), ('odometry', 0.451), ('street', 0.197), ('streets', 0.18), ('gps', 0.17), ('heading', 0.164), ('xt', 0.151), ('drivable', 0.149), ('vehicle', 0.146), ('st', 0.141), ('openstreetmap', 0.127), ('osm', 0.127), ('localization', 0.124), ('driving', 0.11), ('simplification', 0.097), ('uit', 0.094), ('mixture', 0.079), ('pred', 0.075), ('map', 0.074), ('transition', 0.073), ('posterior', 0.073), ('yt', 0.072), ('affordable', 0.07), ('upd', 0.07), ('freely', 0.065), ('mtu', 0.064), ('uxt', 0.064), ('localize', 0.061), ('autonomous', 0.057), ('roads', 0.056), ('kitti', 0.055), ('carlo', 0.054), ('segment', 0.054), ('position', 0.053), ('monocular', 0.053), ('burgard', 0.052), ('snr', 0.052), ('cummins', 0.052), ('put', 0.05), ('dst', 0.049), ('geiger', 0.048), ('segments', 0.046), ('highway', 0.045), ('sequences', 0.044), ('geographic', 0.044), ('sequence', 0.043), ('stereo', 0.043), ('cpredn', 0.042), ('icirs', 0.042), ('marcus', 0.042), ('mtui', 0.042), ('integral', 0.042), ('ijrr', 0.042), ('monte', 0.042), ('icra', 0.041), ('fox', 0.041), ('localized', 0.04), ('maps', 0.039), ('road', 0.039), ('manhattan', 0.038), ('globe', 0.038), ('dellaert', 0.038), ('newman', 0.038), ('montemerlo', 0.038), ('kaess', 0.038), ('leapfrog', 0.038), ('nut', 0.038), ('oracle', 0.036), ('visual', 0.036), ('slam', 0.035), ('dt', 0.035), ('cdf', 0.035), ('oront', 0.035), ('filtering', 0.035), ('trajectory', 0.034), ('analytically', 0.034), ('mobile', 0.034), ('place', 0.034), ('mapping', 0.033), ('brubaker', 0.033), ('arc', 0.032), ('state', 0.032), ('threshold', 0.031), ('kit', 0.031), ('tti', 0.031), ('billion', 0.031), ('transitioning', 0.031), ('crowd', 0.031), ('impact', 0.031), ('supplemental', 0.031), ('drift', 0.03), ('world', 0.03), ('probabilistic', 0.03), ('visited', 0.03), ('error', 0.03), ('traverses', 0.029), ('components', 0.029), ('circular', 0.029), ('noise', 0.028), ('modes', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="274-tfidf-1" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>2 0.11530626 <a title="274-tfidf-2" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><p>3 0.098152652 <a title="274-tfidf-3" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>4 0.093082137 <a title="274-tfidf-4" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>Author: Julien P.C. Valentin, Sunando Sengupta, Jonathan Warrell, Ali Shahrokni, Philip H.S. Torr</p><p>Abstract: Semantic reconstruction of a scene is important for a variety of applications such as 3D modelling, object recognition and autonomous robotic navigation. However, most object labelling methods work in the image domain and fail to capture the information present in 3D space. In this work we propose a principled way to generate object labelling in 3D. Our method builds a triangulated meshed representation of the scene from multiple depth estimates. We then define a CRF over this mesh, which is able to capture the consistency of geometric properties of the objects present in the scene. In this framework, we are able to generate object hypotheses by combining information from multiple sources: geometric properties (from the 3D mesh), and appearance properties (from images). We demonstrate the robustness of our framework in both indoor and outdoor scenes. For indoor scenes we created an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. For outdoor scenes, we created ground truth object labellings for the KITTI odometry dataset (stereo image sequence). We observe a signifi- cant speed-up in the inference stage by performing labelling on the mesh, and additionally achieve higher accuracies.</p><p>5 0.083099313 <a title="274-tfidf-5" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>6 0.081961632 <a title="274-tfidf-6" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>7 0.07584893 <a title="274-tfidf-7" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>8 0.074234702 <a title="274-tfidf-8" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>9 0.073667631 <a title="274-tfidf-9" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>10 0.068426646 <a title="274-tfidf-10" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>11 0.066811353 <a title="274-tfidf-11" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>12 0.066557467 <a title="274-tfidf-12" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>13 0.066475578 <a title="274-tfidf-13" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>14 0.063579671 <a title="274-tfidf-14" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>15 0.062587887 <a title="274-tfidf-15" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>16 0.061669689 <a title="274-tfidf-16" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>17 0.05993085 <a title="274-tfidf-17" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>18 0.058927063 <a title="274-tfidf-18" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>19 0.057746504 <a title="274-tfidf-19" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>20 0.056476608 <a title="274-tfidf-20" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.146), (1, 0.037), (2, 0.005), (3, -0.021), (4, 0.024), (5, -0.02), (6, 0.026), (7, -0.013), (8, 0.003), (9, 0.051), (10, 0.01), (11, 0.002), (12, -0.0), (13, 0.031), (14, -0.023), (15, -0.028), (16, -0.016), (17, 0.036), (18, 0.021), (19, -0.03), (20, 0.024), (21, 0.023), (22, 0.017), (23, 0.052), (24, -0.028), (25, -0.021), (26, -0.012), (27, -0.023), (28, -0.002), (29, 0.051), (30, 0.008), (31, 0.013), (32, -0.078), (33, 0.019), (34, -0.108), (35, -0.013), (36, -0.007), (37, -0.053), (38, -0.039), (39, -0.001), (40, -0.009), (41, -0.033), (42, -0.002), (43, -0.041), (44, -0.115), (45, 0.013), (46, 0.052), (47, -0.009), (48, -0.068), (49, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93034571 <a title="274-lsi-1" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>2 0.6425482 <a title="274-lsi-2" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>Author: Dengfeng Chai, Wolfgang Förstner, Florent Lafarge</p><p>Abstract: The automatic extraction of line-networks from images is a well-known computer vision issue. Appearance and shape considerations have been deeply explored in the literature to improve accuracy in presence of occlusions, shadows, and a wide variety of irrelevant objects. However most existing works have ignored the structural aspect of the problem. We present an original method which provides structurally-coherent solutions. Contrary to the pixelbased and object-based methods, our result is a graph in which each node represents either a connection or an ending in the line-network. Based on stochastic geometry, we develop a new family of point processes consisting in sampling junction-points in the input image by using a Monte Carlo mechanism. The quality of a configuration is measured by a probability density which takes into account both image consistency and shape priors. Our experiments on a variety of problems illustrate the potential of our approach in terms of accuracy, flexibility and efficiency.</p><p>3 0.61440992 <a title="274-lsi-3" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>4 0.58619815 <a title="274-lsi-4" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>Author: Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi</p><p>Abstract: Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. In this paper, we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i.e., analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling.</p><p>5 0.56255728 <a title="274-lsi-5" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>Author: Jan D. Wegner, Javier A. Montoya-Zegarra, Konrad Schindler</p><p>Abstract: The aim of this work is to extract the road network from aerial images. What makes the problem challenging is the complex structure of the prior: roads form a connected network of smooth, thin segments which meet at junctions and crossings. This type of a-priori knowledge is more difficult to turn into a tractable model than standard smoothness or co-occurrence assumptions. We develop a novel CRF formulation for road labeling, in which the prior is represented by higher-order cliques that connect sets of superpixels along straight line segments. These long-range cliques have asymmetric PN-potentials, which express a preference to assign all rather than just some of their constituent superpixels to the road class. Thus, the road likelihood is amplified for thin chains of superpixels, while the CRF is still amenable to optimization with graph cuts. Since the number of such cliques of arbitrary length is huge, we furthermorepropose a sampling scheme which concentrates on those cliques which are most relevant for the optimization. In experiments on two different databases the model significantly improves both the per-pixel accuracy and the topological correctness of the extracted roads, and outper- forms both a simple smoothness prior and heuristic rulebased road completion.</p><p>6 0.54758471 <a title="274-lsi-6" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>7 0.54557598 <a title="274-lsi-7" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>8 0.53812546 <a title="274-lsi-8" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>9 0.53695977 <a title="274-lsi-9" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>10 0.53277957 <a title="274-lsi-10" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>11 0.52614617 <a title="274-lsi-11" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>12 0.52433735 <a title="274-lsi-12" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>13 0.50814432 <a title="274-lsi-13" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>14 0.50645053 <a title="274-lsi-14" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>15 0.49940178 <a title="274-lsi-15" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>16 0.49751136 <a title="274-lsi-16" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>17 0.49690273 <a title="274-lsi-17" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>18 0.49149671 <a title="274-lsi-18" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>19 0.48395035 <a title="274-lsi-19" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>20 0.48375508 <a title="274-lsi-20" href="./cvpr-2013-Discrete_MRF_Inference_of_Marginal_Densities_for_Non-uniformly_Discretized_Variable_Space.html">128 cvpr-2013-Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.085), (16, 0.013), (26, 0.05), (33, 0.194), (67, 0.042), (69, 0.031), (87, 0.493)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87954706 <a title="274-lda-1" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>2 0.86827719 <a title="274-lda-2" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>Author: Christian Häne, Christopher Zach, Andrea Cohen, Roland Angst, Marc Pollefeys</p><p>Abstract: Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being ’too noisy’. Unfortunately, these priors generally yield overly smooth reconstructions and/or segmentations in certain regions whereas they fail in other areas to constrain the solution sufficiently. In this paper we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other’s task. As a consequence, we propose a rigorous mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. Image segmentations provide geometric cues about which surface orientations are more likely to appear at a certain location in space whereas a dense 3D reconstruction yields a suitable regularization for the segmentation problem by lifting the labeling from 2D images to 3D space. We show how appearance-based cues and 3D surface orientation priors can be learned from training data and subsequently used for class-specific regularization. Experimental results on several real data sets highlight the advantages of our joint formulation.</p><p>3 0.86232108 <a title="274-lda-3" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>Author: Eno Töppe, Claudia Nieuwenhuis, Daniel Cremers</p><p>Abstract: We introduce the concept of relative volume constraints in order to account for insufficient information in the reconstruction of 3D objects from a single image. The key idea is to formulate a variational reconstruction approach with shape priors in form of relative depth profiles or volume ratios relating object parts. Such shape priors can easily be derived either from a user sketch or from the object’s shading profile in the image. They can handle textured or shadowed object regions by propagating information. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware. In contrast to existing single view reconstruction algorithms, the proposed algorithm provides substantially more flexibility to recover shape details such as self-occlusions, dents and holes, which are not visible in the object silhouette.</p><p>4 0.83183026 <a title="274-lda-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.81689185 <a title="274-lda-5" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>Author: Yi-Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa, P. Jonathon Phillips</p><p>Abstract: We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification, where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.</p><p>6 0.80523074 <a title="274-lda-6" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>7 0.80077744 <a title="274-lda-7" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>8 0.79457963 <a title="274-lda-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.76515365 <a title="274-lda-9" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>10 0.73407733 <a title="274-lda-10" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>11 0.72456789 <a title="274-lda-11" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>12 0.68178725 <a title="274-lda-12" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>13 0.6689626 <a title="274-lda-13" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>14 0.65694547 <a title="274-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.64900786 <a title="274-lda-15" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>16 0.64623326 <a title="274-lda-16" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>17 0.64280611 <a title="274-lda-17" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>18 0.63990927 <a title="274-lda-18" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>19 0.63877666 <a title="274-lda-19" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>20 0.63845146 <a title="274-lda-20" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
