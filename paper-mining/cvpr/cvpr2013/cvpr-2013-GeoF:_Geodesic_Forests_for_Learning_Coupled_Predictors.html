<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-186" href="#">cvpr2013-186</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</h1>
<br/><p>Source: <a title="cvpr-2013-186-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kontschieder_GeoF_Geodesic_Forests_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>Reference: <a title="cvpr-2013-186-reference" href="../cvpr2013_reference/cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Kohli  University of Technology, Austria Abstract  Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. [sent-3, score-0.706]
</p><p>2 This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. [sent-4, score-0.232]
</p><p>3 This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. [sent-6, score-0.921]
</p><p>4 Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. [sent-7, score-0.598]
</p><p>5 A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. [sent-9, score-0.505]
</p><p>6 This analysis yields a new objective for training decision forests  that encourages more accurate structured prediction. [sent-10, score-0.539]
</p><p>7 GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF. [sent-12, score-0.493]
</p><p>8 In fact, conventional decision forests ignore the structure in output spaces and make predictions for each output variable independently. [sent-22, score-0.586]
</p><p>9 This assumption prevents them from enforcing dependencies between variables, and for semantic segmentation tasks, translates into pixel labellings that do not follow object boundaries and are inconsistent with context. [sent-23, score-0.405]
</p><p>10 In the forest approach in [13], spatial smoothness is achieved by combining structured class-labels that are learned by incorporating joint statistics in a small neighborhood. [sent-28, score-0.431]
</p><p>11 Our framework overcomes the above-mentioned problem by incorporating learned spatial context directly within the forest itself. [sent-31, score-0.426]
</p><p>12 Long-range corre-  lations between pixel labels are captured via new soft connectivity features which can be computed efficiently using generalized geodesic distance transforms. [sent-33, score-0.663]
</p><p>13 Another contribution is to analyse the relationship between a typical CRFlike energy and the forest training objective. [sent-34, score-0.505]
</p><p>14 This analysis leads to a new objective for training decision forests that produces more accurate semantic segmentation. [sent-35, score-0.602]
</p><p>15 Quantitative results demonstrate the superiority of our model both in terms of accuracy and efficiency, with respect to state-ofthe-art forest models and grid-based pairwise CRFs. [sent-37, score-0.435]
</p><p>16 The recent work on autocontext [24, 26], stacking [18, 28], deep learning [14, 15] and entanglement [17] has shown how a sequence of classifiers using the output of the previous classifier as input to the next can both effectively capture spatial context (e. [sent-40, score-0.221]
</p><p>17 In [9], the relationship between anytime classification and intermediate predictions within decision trees is shown. [sent-43, score-0.32]
</p><p>18 Our geodesic forest model (GeoF) can be seen as a gen-  eralization of semantic texton forests [24], auto-context [24, 26], and entanglement forests [17]. [sent-45, score-1.625]
</p><p>19 In fact, GeoF builds upon these models by using: (i) new, long-range soft connectivity features, and (ii) a new field-inspired objective for forest training. [sent-46, score-0.647]
</p><p>20 ΩW e⊂ ca Nst th→e seamnadnt aic 2 segmentation otans kis as nthotate do fp associating eaastch th pixel p with its corresponding discrete class label c ∈ C. [sent-50, score-0.194]
</p><p>21 vWarei use cd etodenote predictions obtained at depth d in the tree. [sent-61, score-0.162]
</p><p>22 (2)  exten-  seeks parameters θj which aim to maximize both the class purity and spatial compactnes of pixel clusters in child nodes. [sent-70, score-0.174]
</p><p>23 Decision forests [3, 5, 8] further assume that the poste? [sent-75, score-0.313]
</p><p>24 Typically, a decision tree is trained greedily, where for each split node j the parameters θj associated with a low energy (e. [sent-88, score-0.275]
</p><p>25 Figure 1 illustrates this point and suggests that ideally we would like training to maximize class purity as well as encouraging spatial compactness of the resulting pixel clusters. [sent-91, score-0.221]
</p><p>26 Coupling forest predictions to reveal hidden correla-  tions. [sent-92, score-0.503]
</p><p>27 In this paper, we overcome this problem and encourage forests 666666  to produce spatially compact/coherent pixel labellings. [sent-94, score-0.378]
</p><p>28 In what follows, we will show how a learned model of spatial context can be encoded within a decision forest directly. [sent-95, score-0.534]
</p><p>29 One of the key theoretical insights of our work is the observation that although forests make predictions for each variable independently, these predictions are related due to correlations at the feature level. [sent-97, score-0.527]
</p><p>30 For instance, in the semantic image segmentation task consider the class predictions of two pixels p and q. [sent-98, score-0.334]
</p><p>31 Therefore, output-variable dependencies can be encoded in the features that the forest operates on. [sent-102, score-0.46]
</p><p>32 Long-range, soft connectivity features The need for long-range connectivity features. [sent-106, score-0.354]
</p><p>33 In [16, 23, 27] the authors have shown how simple pixel comparison features can be effective in classification tasks when used within a decision forest. [sent-107, score-0.173]
</p><p>34 Since the shortest path connecting them has a high geodesic length (it cuts through high image gradients, see definition in (4)), this provides a hint that the two points may not be part of the same object/class. [sent-115, score-0.456]
</p><p>35 (a) Given a pixel pair (a reference and a probe pixel) popular features only look at the intensities at the two pixel positions, and ignore what happens in between. [sent-117, score-0.277]
</p><p>36 (b) In contrast, the length of the shortest path connecting the pixel pair carries richer information. [sent-118, score-0.175]
</p><p>37 The geodesic length of the shortest path connecting two points provides hints about the points belonging (or not) to the same object class (e. [sent-119, score-0.535]
</p><p>38 They are based on the use of generalized geodesic distances, as introduced in [7] and summarized next. [sent-123, score-0.383]
</p><p>39 Given a grey-valued image J, and a real-valued object “soft mask” (that encodes pixel likelihood) M(p) : Ω ∈ → [0, 1] the generalized geodesic ldihisotaondc)e M MQ( ips )d e:fi Ωne ∈d as fo→llow [0s,:1  Nd  Q(p;M,∇J)  = p m? [sent-125, score-0.448]
</p><p>40 ))  (4)  with the geodesic distance between two points p and q:  δ(p,q) =Γ∈inPfp,q? [sent-128, score-0.346]
</p><p>41 Soft connectivity between a pixel and a class region. [sent-134, score-0.283]
</p><p>42 They efficiently capture long-range connectivity (of a pixel to a class region). [sent-158, score-0.283]
</p><p>43 We can use those probabilities to construct the soft masks M needed for the generalized geodesic distance transform, and the corresponding filtered probabilities will be g(c = torso) and g(c = left leg). [sent-160, score-0.561]
</p><p>44 Contrast sensitivity is modulated by the geodesic strength parameter γ ≥ 0 in (5). [sent-162, score-0.346]
</p><p>45 Entangled geodesic forests Here we are interested in extremely efficient semantic segmentation. [sent-166, score-0.757]
</p><p>46 Thus, we build upon decision forests [3, 5, 8], because of their speed and flexibility. [sent-167, score-0.421]
</p><p>47 4 in the spirit of entangled forests [17] we train all trees: (i) in parallel, (ii) in breadthfirst order, and (iii) in sections. [sent-172, score-0.672]
</p><p>48 In fact, the class posteriors p(c|v) of the previous section may be tuhseed c as input freioatrusre ps( |tov )th oef tnheext p r[1ev7]io. [sent-177, score-0.168]
</p><p>49 Given a class posterior psi (c|v) computed at the ith section (with i> 0), its geodesically )sm coomotphueted dve artsti ohne i is defined as gsi(c|v(p)) =  W1psi(c|v(p)) e−Q(p;psi(c|σv2(Ω)),∇J)2  (6)  Figure 4. [sent-184, score-0.181]
</p><p>50 The trees are entangled because intermediate predictions of their top section are used (together with raw intensity features) as features for training of the lower sections. [sent-187, score-0.625]
</p><p>51 Feature responses for a reference pixel r are defined as a function of tree depth d, and as sum, differences or absolute differences between two pixel probe values in different feature channels3, i. [sent-198, score-0.365]
</p><p>52 the intermediate class posteriors computed in the( cs|e(cpti)o),n i s. [sent-204, score-0.201]
</p><p>53 The entangled feature channels (k = 1, 2) are available only for section s1 and greater, and are computed very efficiently as table look-ups. [sent-210, score-0.359]
</p><p>54 Field-inspired forest training objective This section describes our second contribution: the use of a new objective for the forest training procedure. [sent-213, score-0.958]
</p><p>55 Most algorithms for training classification forests are greedy and find the optimal parameters for a split node j as θj = argminθ E(Sj , θ) (Fig. [sent-216, score-0.393]
</p><p>56 ∈Cn(c,Sij) logn(|cS,jiS|ji)  (7)  with n(c, S) denoting the number of training pixels of class c i tnh hth ne( training nsuotbinsget tSh (please erre foefr t rtoa Fig. [sent-225, score-0.173]
</p><p>57 f training each tree split node by using an MRF energy E = ERF, which is typically defined as  ERF(Sj,θ) =i∈? [sent-229, score-0.214]
</p><p>58 Thus, conventional entropy-based tree training corresponds exactly to minimizing an MRF-like energy which uses the log-loss as unary and no pairwise term4. [sent-238, score-0.323]
</p><p>59 This is particularly important in the context of semantic segmentation, where often the pixels in the background class are much more numerous than those in other classes. [sent-248, score-0.207]
</p><p>60 Results and Comparisons We validate our semantic segmentation approach on four, very diverse labelled image datasets. [sent-267, score-0.218]
</p><p>61 We have the following 9 classes: background (BG), heart (HR), liver (LI), spleen (SP), left/right lung (LL/RL), left/right kidney (LK/RK) and aorta (AO). [sent-276, score-0.486]
</p><p>62 In the latter, as energy model, we used a log-loss as unary term and a contrast-sensitive Potts model as pairwise term. [sent-289, score-0.146]
</p><p>63 Additionally, we also implemented an auto-context [26] version of classification forests where: A first forest is trained using raw intensity features; Then, a second forest is trained using both raw intensities and the probabilities from the first forest as features. [sent-290, score-1.671]
</p><p>64 Both entangled geodesic features and un-entangled class posteriors are 666999  Figure5. [sent-291, score-0.873]
</p><p>65 Entangling the p feature channels only helps spatial  Enabling the long-range geodesic feature channels g helps  The spurious hand region is gone. [sent-302, score-0.346]
</p><p>66 (f, i, l) Results from forest with geodesic entanglement  and field-inspired  energy term. [sent-303, score-0.926]
</p><p>67 The combination of entangled geodesic features and log-loss training produces coherent segmentations without the need for field-based post-processing. [sent-314, score-0.752]
</p><p>68 For all forest based algorithms we fix T = 10 and D = 20, except for the CamVid dataset where we use a maximum depth D = 17 since the number of training samples is considerably smaller. [sent-316, score-0.498]
</p><p>69 However, decision forests are well-suited for GPU implementations [22]. [sent-318, score-0.421]
</p><p>70 The baseline forest (0 1) yields a mean Jaccard score of only 38. [sent-320, score-0.396]
</p><p>71 2%, still lower than what our implemented auto-context forest (0 3) and our proposed geodesic forests achieve (0 7-16). [sent-324, score-1.055]
</p><p>72 Both the use of entangled geodesic features and the field-inspired  energy help achieve the highest accuracy in this dataset. [sent-325, score-0.767]
</p><p>73 Entangled geodesic forests using either of the two energy models (14 ,16) work better than the conventional forest (0 1). [sent-329, score-1.175]
</p><p>74 Accuracy as a function of tree depth D, for different forest variants, evaluated on the LFW face dataset. [sent-332, score-0.523]
</p><p>75 Our auto-context geodesic forest (0 8) does well, but the second forest does not seem to yield much additional improvement. [sent-334, score-1.138]
</p><p>76 In terms of runtime, the standard forest + CRF (0 2) takes ∼ 0. [sent-335, score-0.396]
</p><p>77 3% (0 2) we find again that providing entangled geodesic features improves on all our compared methods. [sent-345, score-0.705]
</p><p>78 The autocontext forest performs well here too, even without these additional features. [sent-346, score-0.465]
</p><p>79 However, the best results are achieved 777000  with one or two sections ofentanglement in geodesic forests (12 16). [sent-347, score-0.659]
</p><p>80 2s per frame (w1h2i,le geodesic efo CrResFts a p(1p r2o) ancehed (0 ∼2 0 ta. [sent-349, score-0.346]
</p><p>81 e best results are achieved by our auto-context geodesic forests (0 7, 0 8) which yield strong improvements over the baseline (+ 6. [sent-354, score-0.659]
</p><p>82 0 3, 0 7 , 0 8) results in higher runtimes as two forests need to be evaluated (resulting in ∼ 1. [sent-359, score-0.313]
</p><p>83 r3am9se/f rwamhilee) entangled geodesic fhor (e0st2s) are emsu c∼h 1fa. [sent-363, score-0.705]
</p><p>84 3% which we are able to considerably outperform with all our geodesic forest variants. [sent-371, score-0.742]
</p><p>85 The best performing geodesic forest (16) improves over the recent work in [13] (+2. [sent-372, score-0.742]
</p><p>86 tried training forests by adding pairwise terms or other global smoothness terms in the energy (10), but without consistently improving the accuracy further. [sent-383, score-0.461]
</p><p>87 7a we see that at depth 10 (after one level of entanglement) when the reference pixel is in the liver, the two probes tend to be selected (during training) to also be in the liver. [sent-391, score-0.23]
</p><p>88 7b the probes tend to be selected frequently also in the heart and right lung regions. [sent-395, score-0.174]
</p><p>89 Conclusion This paper has presented a new forest-based model for structured-output learning, applied to the task of semantic Class of reference pixels = liver (LI) Depht  10  Depht 13  Depht  17  (a)ABRLSHGOKLPR IBGHRLISP RLK AO0 0. [sent-402, score-0.247]
</p><p>90 8765432  Class of reference pixels = left kidney (LK)  Figure 7. [sent-413, score-0.185]
</p><p>91 In this dataset (CT) classes are: background (BG), heart (HR),  liver (LI), spleen (SP), l. [sent-418, score-0.229]
</p><p>92 in b’ when trying to identify the left kidney it helps to use probes either in the spleen region (just above the left kidney) or in the left kidney itself (encouraging local smoothness). [sent-426, score-0.423]
</p><p>93 Our model encourages spatial smoothness and long-range, semantic context within the forest itself, via the use of new, soft connectivity features which build upon entangled, generalized geodesic distances. [sent-428, score-1.122]
</p><p>94 In addition, the paper shows how training forests by minimizing a new random field-inspired energy yields higher accuracy than entropy based approaches. [sent-429, score-0.422]
</p><p>95 27 6  for our geodesic forest algorithm as compared to existing techniques (e. [sent-490, score-0.742]
</p><p>96 random classification forest, and forest + CRF), for four different labelled image databases. [sent-492, score-0.466]
</p><p>97 As time goes by anytime semantic segmentation with iterative context forests. [sent-498, score-0.211]
</p><p>98 Structured class-labels in random forests for semantic image labelling. [sent-528, score-0.411]
</p><p>99 Entangled decision forests and their application for semantic segmentation of CT images. [sent-561, score-0.569]
</p><p>100 Hough forest random field for object recognition and segmentation. [sent-586, score-0.396]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('forest', 0.396), ('entangled', 0.359), ('geodesic', 0.346), ('forests', 0.313), ('geof', 0.207), ('kidney', 0.143), ('connectivity', 0.139), ('fkd', 0.138), ('entanglement', 0.122), ('decision', 0.108), ('liver', 0.107), ('predictions', 0.107), ('semantic', 0.098), ('posteriors', 0.089), ('cp', 0.084), ('class', 0.079), ('soft', 0.076), ('sji', 0.076), ('tree', 0.072), ('zk', 0.072), ('labelled', 0.07), ('sj', 0.07), ('autocontext', 0.069), ('depht', 0.069), ('geodesically', 0.069), ('rlk', 0.069), ('spleen', 0.069), ('crf', 0.068), ('probes', 0.068), ('probe', 0.066), ('erf', 0.065), ('pixel', 0.065), ('dependencies', 0.064), ('energy', 0.062), ('aorta', 0.061), ('labellings', 0.059), ('conventional', 0.058), ('vid', 0.057), ('depth', 0.055), ('criminisi', 0.054), ('shotton', 0.054), ('lung', 0.053), ('heart', 0.053), ('ct', 0.052), ('kontschieder', 0.051), ('camvid', 0.051), ('probabilities', 0.051), ('segmentation', 0.05), ('eit', 0.049), ('bands', 0.047), ('unaries', 0.047), ('training', 0.047), ('gsi', 0.046), ('kinbg', 0.046), ('logwczn', 0.046), ('labelling', 0.045), ('unary', 0.045), ('jaccard', 0.044), ('reference', 0.042), ('springer', 0.041), ('lfw', 0.041), ('logn', 0.041), ('shortest', 0.041), ('raw', 0.04), ('intensities', 0.039), ('trees', 0.039), ('pairwise', 0.039), ('translates', 0.038), ('bul', 0.038), ('eyebrow', 0.038), ('lungs', 0.038), ('medical', 0.038), ('texton', 0.037), ('connecting', 0.037), ('leg', 0.037), ('nowozin', 0.037), ('generalized', 0.037), ('objective', 0.036), ('rota', 0.036), ('structured', 0.035), ('variables', 0.034), ('cq', 0.034), ('intermediate', 0.033), ('ji', 0.033), ('torso', 0.033), ('node', 0.033), ('maxp', 0.033), ('anytime', 0.033), ('psi', 0.033), ('path', 0.032), ('jancsary', 0.031), ('prevents', 0.031), ('kohli', 0.031), ('sharp', 0.031), ('purity', 0.03), ('munoz', 0.03), ('smoothing', 0.03), ('context', 0.03), ('kinect', 0.03), ('ii', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="186-tfidf-1" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>2 0.36465213 <a title="186-tfidf-2" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>3 0.18273744 <a title="186-tfidf-3" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>Author: Ming-Yu Liu, Oncel Tuzel, Yuichi Taguchi</p><p>Abstract: We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. Specifically, it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. Though this is closely related to the all-pairshortest-path problem which has O(n2 log n) complexity, we develop a novel approximation algorithm whose complexity grows linearly with the image size and achieve realtime performance. We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. In addition, we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps, an important sensor fusion application.</p><p>4 0.18180275 <a title="186-tfidf-4" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>5 0.15356521 <a title="186-tfidf-5" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>Author: Matthias Dantone, Juergen Gall, Christian Leistner, Luc Van_Gool</p><p>Abstract: In this work, we address the problem of estimating 2d human pose from still images. Recent methods that rely on discriminatively trained deformable parts organized in a tree model have shown to be very successful in solving this task. Within such a pictorial structure framework, we address the problem of obtaining good part templates by proposing novel, non-linear joint regressors. In particular, we employ two-layered random forests as joint regressors. The first layer acts as a discriminative, independent body part classifier. The second layer takes the estimated class distributions of the first one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This results in a pose estimation framework that takes dependencies between body parts already for joint localization into account and is thus able to circumvent typical ambiguities of tree structures, such as for legs and arms. In the experiments, we demonstrate that our body parts dependent joint regressors achieve a higher joint localization accuracy than tree-based state-of-the-art methods.</p><p>6 0.12764403 <a title="186-tfidf-6" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>7 0.12367433 <a title="186-tfidf-7" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>8 0.11875032 <a title="186-tfidf-8" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>9 0.11031045 <a title="186-tfidf-9" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>10 0.10912701 <a title="186-tfidf-10" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>11 0.10911895 <a title="186-tfidf-11" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>12 0.10554844 <a title="186-tfidf-12" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>13 0.10552908 <a title="186-tfidf-13" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>14 0.10346145 <a title="186-tfidf-14" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>15 0.10168421 <a title="186-tfidf-15" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>16 0.098494992 <a title="186-tfidf-16" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>17 0.093666211 <a title="186-tfidf-17" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>18 0.093265839 <a title="186-tfidf-18" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>19 0.092082404 <a title="186-tfidf-19" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>20 0.08968243 <a title="186-tfidf-20" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.202), (1, 0.029), (2, 0.022), (3, -0.007), (4, 0.077), (5, 0.019), (6, 0.023), (7, 0.102), (8, -0.04), (9, -0.068), (10, 0.028), (11, 0.042), (12, -0.047), (13, 0.121), (14, -0.043), (15, 0.032), (16, -0.089), (17, -0.035), (18, 0.048), (19, -0.082), (20, 0.009), (21, 0.009), (22, -0.07), (23, 0.032), (24, -0.071), (25, 0.08), (26, -0.008), (27, 0.087), (28, 0.02), (29, 0.033), (30, -0.197), (31, 0.004), (32, -0.065), (33, -0.059), (34, 0.0), (35, -0.022), (36, -0.062), (37, -0.126), (38, 0.002), (39, 0.223), (40, -0.061), (41, 0.068), (42, -0.105), (43, 0.011), (44, 0.1), (45, 0.099), (46, -0.062), (47, 0.111), (48, 0.06), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94311792 <a title="186-lsi-1" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>2 0.82976258 <a title="186-lsi-2" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>3 0.68919492 <a title="186-lsi-3" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>4 0.65339905 <a title="186-lsi-4" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>5 0.59797531 <a title="186-lsi-5" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>Author: Roman Shapovalov, Dmitry Vetrov, Pushmeet Kohli</p><p>Abstract: This paper addresses the problem of semantic segmentation of 3D point clouds. We extend the inference machines framework of Ross et al. by adding spatial factors that model mid-range and long-range dependencies inherent in the data. The new model is able to account for semantic spatial context. During training, our method automatically isolates and retains factors modelling spatial dependencies between variables that are relevant for achieving higher prediction accuracy. We evaluate the proposed method by using it to predict 1 7-category semantic segmentations on sets of stitched Kinect scans. Experimental results show that the spatial dependencies learned by our method significantly improve the accuracy of segmentation. They also show that our method outperforms the existing segmentation technique of Koppula et al.</p><p>6 0.54849988 <a title="186-lsi-6" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>7 0.53232807 <a title="186-lsi-7" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>8 0.51780999 <a title="186-lsi-8" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>9 0.51778769 <a title="186-lsi-9" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>10 0.48113629 <a title="186-lsi-10" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>11 0.4650785 <a title="186-lsi-11" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>12 0.4607273 <a title="186-lsi-12" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>13 0.45330128 <a title="186-lsi-13" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>14 0.45014393 <a title="186-lsi-14" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>15 0.44777095 <a title="186-lsi-15" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>16 0.44074747 <a title="186-lsi-16" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>17 0.43941054 <a title="186-lsi-17" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>18 0.43873999 <a title="186-lsi-18" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>19 0.43839145 <a title="186-lsi-19" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>20 0.43200919 <a title="186-lsi-20" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.528), (16, 0.018), (26, 0.035), (33, 0.195), (67, 0.054), (69, 0.024), (87, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92814523 <a title="186-lda-1" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>2 0.91547179 <a title="186-lda-2" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>3 0.91241091 <a title="186-lda-3" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>4 0.90596908 <a title="186-lda-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.89992416 <a title="186-lda-5" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>6 0.88607424 <a title="186-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>same-paper 7 0.8660028 <a title="186-lda-7" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>8 0.86478382 <a title="186-lda-8" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>9 0.82859367 <a title="186-lda-9" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>10 0.82825577 <a title="186-lda-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.80504644 <a title="186-lda-11" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>12 0.77482396 <a title="186-lda-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.76454794 <a title="186-lda-13" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>14 0.75377226 <a title="186-lda-14" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>15 0.75114095 <a title="186-lda-15" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>16 0.73758376 <a title="186-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.7332958 <a title="186-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.72467101 <a title="186-lda-18" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>19 0.72104055 <a title="186-lda-19" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>20 0.71922827 <a title="186-lda-20" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
