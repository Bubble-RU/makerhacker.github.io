<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-77" href="#">cvpr2013-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-77-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Capturing_Complex_Spatio-temporal_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>Reference: <a title="cvpr-2013-77-reference" href="../cvpr2013_reference/cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. [sent-2, score-1.965]
</p><p>2 One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. [sent-3, score-0.41]
</p><p>3 Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. [sent-4, score-0.634]
</p><p>4 To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. [sent-5, score-2.185]
</p><p>5 We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. [sent-6, score-2.546]
</p><p>6 Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods. [sent-7, score-1.786]
</p><p>7 Introduction Facial expressions are the outcome of a set of muscle motions over a time interval. [sent-9, score-0.339]
</p><p>8 These movements interact in different patterns and convey different expressions. [sent-10, score-0.059]
</p><p>9 Understanding such complex facial activity not only requires us to study each individual facial muscle motion, but also how they interact with each other in both the space and time domain. [sent-11, score-1.716]
</p><p>10 Spatially, facial muscle motions can co-occur or can be mutually exclusive at each time slice. [sent-12, score-0.846]
</p><p>11 Temporally, the movement of one facial muscle can activate, overlap or follow another muscle. [sent-13, score-0.852]
</p><p>12 These spatio-temporal relations capture significant information about facial expressions yet have not been thoroughly studied, partially due to the limitations of the current models. [sent-14, score-1.185]
</p><p>13 cn ing works that perform facial expression recognition on the manually labeled peak frame, we model a facial expression as a complex activity that spans over a time interval and consists of a group of primitive facial events happening sequentially or in parallel. [sent-17, score-3.201]
</p><p>14 More importantly, modeling facial expression as such a complex activity allows us to further study and capture a larger variety of complex spatial and temporal interactions among the primitive events. [sent-18, score-1.85]
</p><p>15 In this work, we aim to overcome the limitations of current models and thoroughly explore and exploit more complex spatio-temporal relations in the facial activities for expression recognition. [sent-19, score-1.326]
</p><p>16 Understanding a complex activity and capturing the underlying temporal relations is challenging and most of the existing methods do not handle this adroitly. [sent-20, score-0.604]
</p><p>17 Modeling and recognizing a complex activity is naturally solved by building a structure that is able to semantically capture the spatiotemporal relationships among primitive events. [sent-21, score-0.602]
</p><p>18 Among various visual recognition methodologies, such as graphical, syntactic and description-based approaches, time-sliced graphical models, i. [sent-22, score-0.121]
</p><p>19 hidden Markov models (HMMs) and dynamic Bayesian networks (DBNs), have become the most popular tool for modeling and understanding complex activities [2, 11, 13, 5]. [sent-24, score-0.3]
</p><p>20 Syntactic and description-based approaches have also gained attention and used mainly for action units detection in recent years [10]. [sent-25, score-0.063]
</p><p>21 While these approaches have been applied to capture the dynamics of facial expressions, they face one or more of the following issues when modeling and understanding complex visual activities that involve interactions between different entities over durations of time. [sent-26, score-1.074]
</p><p>22 First, time-sliced (based on time points) graphical models (e. [sent-27, score-0.068]
</p><p>23 HMM, DBN, or their variants) typically represent an activity as a sequence of instantaneously occurring events, which is generally unrealistic for facial expression. [sent-29, score-0.819]
</p><p>24 For example, the eye movement and nose movement may both last for a period of time and they may overlap. [sent-30, score-0.136]
</p><p>25 More333444222200  over, time-sliced dynamic models can only offer three time-  point relations (precedes, follows, equals), and so they are not expressive enough to capture many of the temporal relations between events that happen over the duration of an activity. [sent-31, score-0.919]
</p><p>26 Secondly, time-sliced graphical models typically assume first order Markov property and stationary transition. [sent-32, score-0.118]
</p><p>27 Hence, they can only capture local stationary dynamics and cannot represent global temporal relations. [sent-33, score-0.388]
</p><p>28 Finally, syntactic and description-based models lack the expressive power to capture and propagate the uncertainties associated with event detection and with their temporal dependencies in a principled manner. [sent-34, score-0.505]
</p><p>29 To address these issues and comprehensively model facial expression, we propose a unified probabilistic framework that combines the probabilistic semantics of Bayesian networks (BNs) with the temporal semantics of interval algebra (IA). [sent-35, score-1.159]
</p><p>30 In particular, ITBN is time-interval based in contrast to time-sliced models, which allows us to model the relations among both sequential and overlapping temporal events. [sent-37, score-0.517]
</p><p>31 In this paper we take a holistic approach to modeling the facial activities. [sent-38, score-0.696]
</p><p>32 We will first identify all of the related primitive facial events, which provide us the basis to define a  larger variety of temporal relations. [sent-39, score-1.197]
</p><p>33 We then apply ITBN to capture their spatio-temporal interactions for expression recognition. [sent-40, score-0.316]
</p><p>34 The remainder of this paper is organized as follows. [sent-41, score-0.028]
</p><p>35 Section 3 introduces the definition and implementation of ITBN. [sent-43, score-0.022]
</p><p>36 We discuss how we identify the primitive facial events and how we model the facial expressions with ITBN in Section 4. [sent-44, score-1.926]
</p><p>37 Related Works Recognizing facial expressions generally involves bottom-level feature extraction and top-level classifier design. [sent-48, score-0.835]
</p><p>38 Features for expression classification can be grouped into appearance features such as Gabor [8] and LBP [9], and geometric features that are extracted from the location of the salient facial points. [sent-49, score-0.871]
</p><p>39 While appearance features capture the local or global appearance information of the facial components, studying the movement of the facial feature points provides us a more explicit manner to analyze the dynamics. [sent-50, score-1.455]
</p><p>40 Classifiers for facial expression recognition include static models and dynamic models. [sent-51, score-0.993]
</p><p>41 Static models recognize facial expressions based on the apex frame of an image sequence and have achieved successful performance. [sent-52, score-0.934]
</p><p>42 However, peak frames usually require manual labeling and the static approach completely disregards the dynamic interactions among the facial muscles that are very important for  discriminating facial activates. [sent-53, score-1.756]
</p><p>43 In contrast dynamic models rely on the whole image sequence and study their temporal dynamics for facial expression recognition. [sent-54, score-1.26]
</p><p>44 In this paper we focus on expression recognition works that are based on the facial feature points and image sequences. [sent-55, score-0.871]
</p><p>45 A more comprehensive literature review of facial expression recognition ban be found in [14]. [sent-56, score-0.905]
</p><p>46 Dynamic models that have been widely applied for facial expression recognition include the hidden Markov model (HMM) and its variants [2, 11, 13], the dynamic Bayesian network (DBN) [5], and latent conditional random fields (LCRF) [4]. [sent-57, score-1.056]
</p><p>47 In [2] a multilevel HMM is introduced to automatically segment and recognize human facial expressions from image sequences based on the local deformations of the facial feature points tracked with a piecewise Bezier volume deformation tracker. [sent-59, score-1.602]
</p><p>48 In [11], a nonparametric discriminant HMM is applied to recognize the expression and the facial features are tracked with Active Shape Model. [sent-60, score-0.945]
</p><p>49 A different approach is used in [13], where an HMM was used together with support vector machines and AdaBoost to simultaneously recognize action units and facial expressions by modeling the dynamics among the action units. [sent-61, score-1.14]
</p><p>50 Similarly, DBN also captures local temporal interactions and an example can be found in [5]. [sent-62, score-0.251]
</p><p>51 Besides these generative models, discriminative approaches such as LCRF have also been applied for expression analysis. [sent-63, score-0.202]
</p><p>52 For instance, in [4], features from 68 landmark points of video  sequences were fed into an LCRF to perform expression recognition. [sent-64, score-0.202]
</p><p>53 However, all of these models are time-slice based and as a result can only capture a small portion of the temporal relations. [sent-65, score-0.28]
</p><p>54 Moreover, these relations are assumed to be stationary and time-independent. [sent-66, score-0.233]
</p><p>55 To overcome these restrictions our proposed method models a complex activity as sequential or overlapping primitive events, and each event spans over a time interval. [sent-68, score-0.688]
</p><p>56 This allows us to capture a wider variety of complex temporal relations which can further enhance the performance of facial activity recognition. [sent-69, score-1.339]
</p><p>57 Interval Algebra Bayesian Network Different spatial and temporal configurations of primitive facial events lead to different expressions. [sent-71, score-1.287]
</p><p>58 Unlike the related works, ITBN looks at facial activity from a global view and is able to model a larger variety of spatio-temporal relations. [sent-72, score-0.833]
</p><p>59 To formally introduce the definition of ITBN, we will first define the primitive events that constitute a complex activity and several related concepts. [sent-73, score-0.662]
</p><p>60 A primitive event is also called a temporal entity and we do not differentiate 333444222311  between these two terms in the remainder of this paper. [sent-74, score-0.621]
</p><p>61 We then introduce how we model the temporal relations among primitive events. [sent-75, score-0.701]
</p><p>62 Finally, we will formally introduce ITBN and its implementation. [sent-76, score-0.023]
</p><p>63 Definition 1(Temporal Entity) A temporal entity is characterized by a pair ? [sent-77, score-0.284]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facial', 0.669), ('primitive', 0.271), ('itbn', 0.266), ('expression', 0.202), ('temporal', 0.196), ('relations', 0.183), ('expressions', 0.166), ('muscles', 0.152), ('events', 0.151), ('hmm', 0.13), ('activity', 0.127), ('muscle', 0.125), ('lcrf', 0.114), ('entity', 0.088), ('dbn', 0.084), ('dynamics', 0.083), ('syntactic', 0.078), ('thoroughly', 0.075), ('interval', 0.074), ('complex', 0.068), ('dynamic', 0.065), ('capture', 0.059), ('movement', 0.058), ('expressive', 0.057), ('bayesian', 0.056), ('sequential', 0.055), ('interactions', 0.055), ('among', 0.051), ('network', 0.05), ('stationary', 0.05), ('recognize', 0.048), ('algebra', 0.047), ('spans', 0.043), ('graphical', 0.043), ('activities', 0.042), ('event', 0.038), ('interact', 0.038), ('variety', 0.037), ('bezier', 0.034), ('ban', 0.034), ('bns', 0.034), ('ebt', 0.034), ('precedes', 0.034), ('rensselaer', 0.034), ('rpi', 0.034), ('tbhye', 0.034), ('semantics', 0.034), ('action', 0.033), ('limitations', 0.033), ('overlapping', 0.032), ('peak', 0.032), ('static', 0.032), ('ia', 0.031), ('dbns', 0.031), ('disregards', 0.031), ('units', 0.03), ('temporally', 0.03), ('capturing', 0.03), ('methodologies', 0.029), ('principled', 0.029), ('overcome', 0.029), ('motions', 0.028), ('remainder', 0.028), ('hmms', 0.028), ('understanding', 0.027), ('modeling', 0.027), ('markov', 0.027), ('probabilistic', 0.027), ('comprehensively', 0.027), ('polytechnic', 0.027), ('tracked', 0.026), ('recognizing', 0.026), ('activate', 0.026), ('contributing', 0.026), ('apex', 0.026), ('qiang', 0.025), ('ust', 0.025), ('models', 0.025), ('happening', 0.024), ('multilevel', 0.024), ('networks', 0.024), ('basis', 0.024), ('exclusive', 0.024), ('durations', 0.024), ('unrealistic', 0.023), ('uncertainties', 0.023), ('concluded', 0.023), ('formally', 0.023), ('variants', 0.023), ('definition', 0.022), ('iq', 0.022), ('feasibility', 0.022), ('hidden', 0.022), ('relational', 0.021), ('convey', 0.021), ('adaboost', 0.021), ('nose', 0.02), ('study', 0.02), ('entities', 0.02), ('outcome', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="77-tfidf-1" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>2 0.56890893 <a title="77-tfidf-2" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>3 0.23468696 <a title="77-tfidf-3" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>4 0.18473162 <a title="77-tfidf-4" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>5 0.14514218 <a title="77-tfidf-5" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>6 0.12905176 <a title="77-tfidf-6" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>7 0.12193254 <a title="77-tfidf-7" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>8 0.11317917 <a title="77-tfidf-8" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>9 0.10612326 <a title="77-tfidf-9" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>10 0.1036958 <a title="77-tfidf-10" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>11 0.099283889 <a title="77-tfidf-11" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>12 0.098678492 <a title="77-tfidf-12" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>13 0.097215891 <a title="77-tfidf-13" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>14 0.096430384 <a title="77-tfidf-14" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>15 0.096122593 <a title="77-tfidf-15" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>16 0.09329199 <a title="77-tfidf-16" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>17 0.093148984 <a title="77-tfidf-17" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>18 0.092555486 <a title="77-tfidf-18" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>19 0.092187479 <a title="77-tfidf-19" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>20 0.088953212 <a title="77-tfidf-20" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.128), (1, -0.058), (2, -0.047), (3, -0.081), (4, -0.073), (5, -0.013), (6, -0.077), (7, -0.084), (8, 0.181), (9, -0.118), (10, 0.166), (11, -0.069), (12, 0.052), (13, 0.081), (14, 0.062), (15, 0.179), (16, 0.037), (17, 0.202), (18, 0.084), (19, -0.005), (20, -0.075), (21, -0.036), (22, 0.049), (23, -0.04), (24, -0.002), (25, 0.1), (26, 0.06), (27, -0.125), (28, 0.082), (29, 0.148), (30, -0.044), (31, -0.147), (32, -0.125), (33, -0.105), (34, -0.206), (35, -0.114), (36, 0.051), (37, 0.017), (38, -0.006), (39, -0.041), (40, -0.007), (41, -0.037), (42, -0.044), (43, 0.242), (44, 0.026), (45, -0.016), (46, 0.025), (47, -0.005), (48, 0.166), (49, -0.156)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98791927 <a title="77-lsi-1" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>2 0.7781539 <a title="77-lsi-2" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>3 0.63033485 <a title="77-lsi-3" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>4 0.59331208 <a title="77-lsi-4" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>5 0.49339867 <a title="77-lsi-5" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>Author: Robert Anderson, Björn Stenger, Vincent Wan, Roberto Cipolla</p><p>Abstract: This paper presents a complete system for expressive visual text-to-speech (VTTS), which is capable of producing expressive output, in the form of a ‘talking head’, given an input text and a set of continuous expression weights. The face is modeled using an active appearance model (AAM), and several extensions are proposed which make it more applicable to the task of VTTS. The model allows for normalization with respect to both pose and blink state which significantly reduces artifacts in the resulting synthesized sequences. We demonstrate quantitative improvements in terms of reconstruction error over a million frames, as well as in large-scale user studies, comparing the output of different systems.</p><p>6 0.48417154 <a title="77-lsi-6" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>7 0.46156889 <a title="77-lsi-7" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>8 0.42925841 <a title="77-lsi-8" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>9 0.42833903 <a title="77-lsi-9" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>10 0.42672044 <a title="77-lsi-10" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>11 0.35265902 <a title="77-lsi-11" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>12 0.34831646 <a title="77-lsi-12" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>13 0.34150097 <a title="77-lsi-13" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>14 0.32832336 <a title="77-lsi-14" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>15 0.32211059 <a title="77-lsi-15" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>16 0.28242028 <a title="77-lsi-16" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>17 0.27724257 <a title="77-lsi-17" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>18 0.27581853 <a title="77-lsi-18" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>19 0.26790464 <a title="77-lsi-19" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>20 0.26664948 <a title="77-lsi-20" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.136), (16, 0.011), (26, 0.089), (33, 0.246), (67, 0.065), (69, 0.081), (85, 0.241), (87, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84834343 <a title="77-lda-1" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>2 0.79271412 <a title="77-lda-2" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>Author: Qiang Chen, Zheng Song, Rogerio Feris, Ankur Datta, Liangliang Cao, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In recent years, efficiency of large-scale object detection has arisen as an important topic due to the exponential growth in the size of benchmark object detection datasets. Most current object detection methods focus on improving accuracy of large-scale object detection with efficiency being an afterthought. In this paper, we present the Efficient Maximum Appearance Search (EMAS) model which is an order of magnitude faster than the existing state-of-the-art large-scale object detection approaches, while maintaining comparable accuracy. Our EMAS model consists of representing an image as an ensemble of densely sampled feature points with the proposed Pointwise Fisher Vector encoding method, so that the learnt discriminative scoring function can be applied locally. Consequently, the object detection problem is transformed into searching an image sub-area for maximum local appearance probability, thereby making EMAS an order of magnitude faster than the traditional detection methods. In addition, the proposed model is also suitable for incorporating global context at a negligible extra computational cost. EMAS can also incorporate fusion of multiple features, which greatly improves its performance in detecting multiple object categories. Our experiments show that the proposed algorithm can perform detection of 1000 object classes in less than one minute per image on the Image Net ILSVRC2012 dataset and for 107 object classes in less than 5 seconds per image for the SUN09 dataset using a single CPU.</p><p>3 0.77688956 <a title="77-lda-3" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>Author: Bojan Pepikj, Michael Stark, Peter Gehler, Bernt Schiele</p><p>Abstract: Despite the success of recent object class recognition systems, the long-standing problem of partial occlusion remains a major challenge, and a principled solution is yet to be found. In this paper we leave the beaten path of methods that treat occlusion as just another source of noise instead, we include the occluder itself into the modelling, by mining distinctive, reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular, we evaluate and compare models that range from standard object class detectors to hierarchical, part-based representations of occluder/occludee pairs. In an extensive evaluation we derive insights that can aid further developments in tackling the occlusion challenge. –</p><p>4 0.77035505 <a title="77-lda-4" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>5 0.76653361 <a title="77-lda-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.76594591 <a title="77-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.7629928 <a title="77-lda-7" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>8 0.76294738 <a title="77-lda-8" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>9 0.76256973 <a title="77-lda-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.76217413 <a title="77-lda-10" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>11 0.76133209 <a title="77-lda-11" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>12 0.75775909 <a title="77-lda-12" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>13 0.75738341 <a title="77-lda-13" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>14 0.75675094 <a title="77-lda-14" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>15 0.75626874 <a title="77-lda-15" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>16 0.75626493 <a title="77-lda-16" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>17 0.75616193 <a title="77-lda-17" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>18 0.75478011 <a title="77-lda-18" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>19 0.75302738 <a title="77-lda-19" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>20 0.75295556 <a title="77-lda-20" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
