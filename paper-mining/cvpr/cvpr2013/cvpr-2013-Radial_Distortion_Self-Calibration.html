<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>344 cvpr-2013-Radial Distortion Self-Calibration</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-344" href="#">cvpr2013-344</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>344 cvpr-2013-Radial Distortion Self-Calibration</h1>
<br/><p>Source: <a title="cvpr-2013-344-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Brito_Radial_Distortion_Self-Calibration_2013_CVPR_paper.pdf">pdf</a></p><p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>Reference: <a title="cvpr-2013-344-reference" href="../cvpr2013_reference/cvpr-2013-Radial_Distortion_Self-Calibration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch Abstract In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. [sent-5, score-0.782]
</p><p>2 Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. [sent-6, score-1.252]
</p><p>3 By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. [sent-7, score-1.289]
</p><p>4 The non-iterative  nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. [sent-9, score-0.694]
</p><p>5 To compensate for such distortion present in many real lens systems, ∗This work was done while this author ment of Computer Science, ETH Z ¨urich  was  employed by the Depart-  Roland Angst∗ Stanford University ETH Z u¨rich,  Switzerland  rangst @ st anford . [sent-16, score-0.652]
</p><p>6 ) are (degenerated) circles in general, but those that go through the center of radial distortion are straight lines (l2 and l2? [sent-22, score-1.419]
</p><p>7 a) When the radial fundamental matrix is obtained with respect to two other images (potentially with different or no distortion) the intersection of the straight epipolar lines reveals the CoD. [sent-24, score-1.052]
</p><p>8 b) In case the same camera is used to take a pair of images the CoD has to lie on the straight epipolar curves in both images. [sent-25, score-0.583]
</p><p>9 most importantly radial distortion, several pre-calibration techniques based on calibration objects have been proposed [4, 24, 16, 14, 25, 22] that allow rectification of distorted images prior to further processing. [sent-26, score-0.707]
</p><p>10 Only few work has addressed autocalibration of radial distortion and virtually all of this focuses on the distortion strength and assumes the distortion  center being known. [sent-28, score-2.351]
</p><p>11 In this contribution we drop this assumption and show how to estimate the center of distortion (CoD) from image data of completely unknown scenes without a calibration object, given a perspective (single center of projection) camera with strictly radial distortion. [sent-29, score-1.422]
</p><p>12 When projecting the viewing ray of another camera to some point in space into the view of such a camera, one does not obtain an epipolar line, but more generally an epipolar curve. [sent-30, score-0.626]
</p><p>13 However - under any radial distortion model - the epipolar curve that contains the CoD must be a straight line, since the distortion happens 1 1 13 3 36 6 6 8 6  in direction of the line1 . [sent-31, score-2.136]
</p><p>14 Given a second straight epipolar curve from a different epipolar geometry (e. [sent-33, score-0.733]
</p><p>15 While these considerations apply to all radial distortion models, we chose to demonstrate the auto-calibration technique using the division distortion model proposed by Fitzgibbon [10], however using the lifted formulation by Brito et al. [sent-37, score-1.759]
</p><p>16 The main reason for this is that the set of all epipolar curves is just the row space of the radial fundamental matrix and the search for the straight line becomes a linear problem. [sent-39, score-1.063]
</p><p>17 [3] to the rational function distortion by Claus et al. [sent-46, score-0.701]
</p><p>18 Previous Work Lens distortion has been studied in photogrammetry and computer vision for a long time (cf. [sent-49, score-0.601]
</p><p>19 Here, radial distortion is one of the dominant distortions [24] and can severely hurt image registration, reconstruction or measurement in images when not being considered. [sent-51, score-1.054]
</p><p>20 However, given significant distortion most of these methods fail or require pre-rectification with known distortion parameters. [sent-59, score-1.202]
</p><p>21 Only few authors have considered to include distortion into the self-calibration problem [10, 19, 2, 23, 8, 13, 3]. [sent-60, score-0.601]
</p><p>22 Among these, Thirtala and Pollefeys [23] assume the CoD to be known and reason about the shape of the distortion in 1Any line through the CoD is a fixed line when radially distorting undistorting an image, although not a line of fixed points. [sent-61, score-0.754]
</p><p>23 Also Fitzgibbon [10] assumed the CoD to be known and then introduced an undistortion model rather than a distortion model allowing to work directly with distorted coordinates. [sent-63, score-0.922]
</p><p>24 After the division undistortion model of [10] also Claus and Fitzgibbon [8] generalize the model, now to a rational function undistortion where up to 17 distortion parameters exist, allowing to represent more general distortion functions, not just radial distortion. [sent-68, score-2.059]
</p><p>25 Using a similar lifting framework as in [1], our work extends Geyer and Daniilidis’ work [11] from catadioptric to perspective cameras with radial (barrel or pincushion) distortion. [sent-77, score-0.598]
</p><p>26 Essentially, their idea is that when observing a planar calibration pattern with a camera with radial distortion, then the CoD will act like a focus of expansion or contraction and consequently behave similarly as an epipole. [sent-80, score-0.636]
</p><p>27 Our solution is based on the observation that lines that go through the CoD are fixed lines under distortion and undistortion. [sent-86, score-0.785]
</p><p>28 Consequently, we look for straight lines in the set of all possible epipolar curves and argue that the CoD must be on such a line. [sent-87, score-0.607]
</p><p>29 In order to obtain a parametric representation of the set of all possible epipolar curves, we use the generalization of Fitzgibbon’s division distortion model by Brito et al. [sent-88, score-0.943]
</p><p>30 On top, we also estimate the distortion strength, which - in sum - is why we call this method radial distortion self-calibration. [sent-91, score-1.636]
</p><p>31 Relation between Undistortion Models The traditionally used second-order distortion model in computer vision (motivated in [4]) with CoD (dx , dy)T ∈ R2 describes the radial distortion as  ? [sent-93, score-1.636]
</p><p>32 ively, ∈w Rhereas λ ∈ R is the distortion coefficient and r˜2 = ? [sent-104, score-0.601]
</p><p>33 ctually describes the distorted point in explicit form: given the undistorted point (xu, yu)T and the distortion parameters λ and (dx , dy)T, the distorted point can be computed easily by evaluating the right-hand side of Eq. [sent-115, score-1.194]
</p><p>34 For auto-calibration, or for direct estimation based on distorted measurements, an undistortion model rather than a distortion model is required. [sent-117, score-0.922]
</p><p>35 2  The division undistortion model In [10], Fitzgibbon argues that an undistortion model can be equally powerful as a distortion model and compares several (un-)distortion functions. [sent-121, score-0.924]
</p><p>36 This is the model also used in the radial fundamental matrix with lifted coordinates [2] and later by minimal solvers with radial distortion like [17]. [sent-123, score-1.669]
</p><p>37 The undistorted distance to the CoD is plotted as a function of the distorted distance d for different λ according to the division distortion model (Eq. [sent-125, score-1.043]
</p><p>38 This leads to interesting consequences: Although every point can be undistorted according to this model, only those close to the distortion center can be distorted. [sent-128, score-0.859]
</p><p>39 However, since non-monotonic distortion curves do not make much sense, the one closer to the distortion center is the interesting one, since this is on the (useful) monotonic part of the curve and the other one can be considered an artifact by the model. [sent-132, score-1.378]
</p><p>40 represents the squared distance to the radial distortion center, this center has to be known when working with these models. [sent-133, score-1.128]
</p><p>41 All of these approaches can cover the same family of radial distortion functions and thus the same family of lenses. [sent-134, score-1.083]
</p><p>42 Having different coefficients for the x2 and y2 terms also means that is no longer a radially symmetric distortion model. [sent-141, score-0.646]
</p><p>43 On top, the parameters can be recovered only up to a homography and in the end, although epipolar lines are straightened, there is significant distortion left in their example images. [sent-143, score-0.977]
</p><p>44 Division undistortion with unknown center In [3] it has been shown that the lifting and the radial fundamental matrix can be reformulated such that one can work with (r? [sent-144, score-0.849]
</p><p>45 Rather, it is absorbed in the lifting matrix, or ultimately, in the radial fundamental matrix. [sent-146, score-0.583]
</p><p>46 In the Claus and Fitzgibbon framework this would mean that the second column of A is zero (no mixed terms), the first and third column are equal and that all entries depend only on the distortion center (dx , dy) and λ. [sent-147, score-0.726]
</p><p>47 CoD From  Straight  Epipolar Lines  We now consider a pair of images A and B, at least one of them (say image A) having radial distortion (but not necessarily being described parametrically by any of the above  Figure 3. [sent-155, score-1.035]
</p><p>48 Visualization of the radial distortion due to the division distortion model: The CoD is visualized in red. [sent-156, score-1.751]
</p><p>49 The distorted epipolar line becomes an epipolar circle (orange). [sent-158, score-0.764]
</p><p>50 Recall that always all epipolar curves go through the epipole and now we look at the one that also includes the distortion center. [sent-163, score-1.068]
</p><p>51 Distortion happens in radial direction from the center to the epipole and thus within the line, but does not change the line as a whole. [sent-164, score-0.719]
</p><p>52 Radial Fundamental Matrix We now choose the distortion model of Brito et al. [sent-168, score-0.601]
</p><p>53 and compute the radial fundamental matrix [3] between our image A and some other image (depending on the other image this could be the single-sided or two-sided radial fundamental matrix). [sent-169, score-1.071]
</p><p>54 As argued before the distortion center must be on that line. [sent-183, score-0.716]
</p><p>55 1  Degenerate Cases  In case image A does not have distortion, then λ is zero and consequently the last column of the radial fundamental will be zero. [sent-186, score-0.571]
</p><p>56 In that case all epipolar curves are straight and the distortion center is not defined. [sent-187, score-1.175]
</p><p>57 All epipolar curves are also straight if the epipole coincides with the distortion center. [sent-188, score-1.208]
</p><p>58 CoD from three (different) images As argued before, from a pair of images A and B, we can constrain the position of the distortion center to a line in the image plane. [sent-195, score-0.73]
</p><p>59 To obtain the full coordinates of the distortion center we can intersect the line with another line. [sent-196, score-0.795]
</p><p>60 Consequently, the straight epipolar curve will be different and by intersecting the two straight epipolar curves we obtain the distortion center. [sent-198, score-1.552]
</p><p>61 For practical reasons, the lines should intersect ideally at a right angle, so the epipoles should be at different directions when viewed from the distortion center. [sent-200, score-0.793]
</p><p>62 CoD from two images of the same camera  In case we have a video or multiple images taken by the same camera we can even extract the distortion center from a pair of two images. [sent-209, score-0.854]
</p><p>63 In each of the images we obtain the constraint that the distortion center must be on the straight epipolar line. [sent-210, score-1.139]
</p><p>64 However, in case we know that it is the same distortion center in both images, we can just intersect these two lines. [sent-211, score-0.735]
</p><p>65 Note that this is a similar setting as studied by Fitzgibbon [10], however we also estimate the distortion center. [sent-215, score-0.601]
</p><p>66 As before, straight lines in the image without distortion are mapped to (circular) curves in the distorted view. [sent-223, score-1.098]
</p><p>67 When looking for those curves that are straight lines, we obtain a 1D family of lines that all intersect at the distortion center. [sent-225, score-0.965]
</p><p>68 In projective space this 1D family of lines spans a 2D subspace, and since the distortion center must lie on all of the lines, it is the orthogonal complement of the 2D subspace. [sent-226, score-0.845]
</p><p>69 Since distortion happens in radial direction, we can compute the distances d1, d2 of both distorted epipoles to the distortion center and study the distortion as a 1D problem, where u is the undistorted distance:  u =1 +d1 λd12=1 +d λ2d22. [sent-238, score-2.775]
</p><p>70 (10)  Here u is the distance of the undistorted epipole to the distortion center. [sent-239, score-0.876]
</p><p>71 Higher order distortion parameters As long as we have a truly radially symmetric distortion model, we can always play the same trick with the intersection of the straight epipolar curve to determine the distortion center. [sent-243, score-2.296]
</p><p>72 Therefore the above mentioned algorithms for extracting the distortion center remain valid even for higher order radial distortion models and one can also think of a higher order radial fundamental. [sent-244, score-2.163]
</p><p>73 For all of the exper-  iments we estimate the (two-sided) radial fundamental matrix using direct linear transformation on normalized feature coordinates (similar to [12]) using all inliers. [sent-247, score-0.57]
</p><p>74 Synthetic Data First, we verify whether it is actually possible to estimate the distortion center with synthetic data (see Fig. [sent-248, score-0.694]
</p><p>75 As can be seen, in an ideal setting with no noise we can always extract the distortion center, which proves our general idea of finding straight epipolar curves. [sent-251, score-1.024]
</p><p>76 It is clear that in case the effects from noise come close to one of the other phenomena then the radial fundamental estimate (and thus the distortion center estimation) will be highly fragile. [sent-254, score-1.219]
</p><p>77 Synthetic experiments for stereo pairs of the same camera with moderate distortion (|λ| = 0. [sent-258, score-0.681]
</p><p>78 Then the two-sided radial fundamental is estimated from 100 correspondences and the distortion center is extracted. [sent-264, score-1.24]
</p><p>79 First we calibrate the distortion center using a chessboard by the method from Hartley and Kang [13] . [sent-269, score-0.694]
</p><p>80 From each pair we extract the distortion center and distortion coefficient as plotted in Fig. [sent-273, score-1.313]
</p><p>81 Although we can see that the distortion centers cluster in a believable position, there is still a substantial offset to the position reported by the calibration-target-based method of Hartley and Kang. [sent-275, score-0.623]
</p><p>82 We compute the radial distortion center from pairs of views, where one is from the start of the sequence and the other from the end of the sequence. [sent-280, score-1.128]
</p><p>83 The centers cluster around (1108;360) close to the center of the image (also visualized) while we obtained a distortion center using the chessboard-based method of [13] at (983;530). [sent-281, score-0.809]
</p><p>84 We do not show the distortion centers when estimated without normalization here, because they are not useful and often very far away from the image, however we do plot the estimated λ showing that normalization is important. [sent-282, score-0.645]
</p><p>85 that the camera’s actual distortion curve does not fit to the division distortion model. [sent-284, score-1.304]
</p><p>86 For the center image the estimated radial distortion center is visualized by showing the two straight epipolar lines. [sent-287, score-1.682]
</p><p>87 As compared to a chessboard-based calibration by the method of [13] the distortion center is 2. [sent-288, score-0.769]
</p><p>88 We then compute two radial fundamental matrices to the center camera and obtain a distortion center that is 2. [sent-295, score-1.392]
</p><p>89 Conclusion Based on the observation that straight lines through the distortion center are fixed lines under any radial distortion model, we have derived constraints on the radial distortion center from epipolar geometry. [sent-301, score-3.446]
</p><p>90 Essentially, by intersecting two lines that must include the distortion center, its coordinates are revealed. [sent-302, score-0.752]
</p><p>91 For high distortion, it is important to choose the appropriate distortion model, maybe with more than one parameter or directly aim at a parameter-free representation as in [13] (which then comes at the cost of requiring more views and many more correspondences). [sent-304, score-0.62]
</p><p>92 Unknown radial distortion centers in multiple view geometry problems. [sent-328, score-1.077]
</p><p>93 A plumbline constraint for the rational function lens distortion model. [sent-354, score-0.779]
</p><p>94 Parameter-free radial distortion correction with center of distortion estimation. [sent-387, score-1.729]
</p><p>95 A non-iterative method for correcting lens distortion from nine-point correspondences. [sent-418, score-0.652]
</p><p>96 Multi-view geometry of 1d radial cameras and its application to omnidirectional camera calibration. [sent-450, score-0.611]
</p><p>97 the distortion parameters are the same in both views. [sent-463, score-0.601]
</p><p>98 Their radial fundamental matrix is a 6-by-6  matrix, since the distorted points are lifted to R6. [sent-464, score-0.79]
</p><p>99 FW ∈hi Rle equation (23) in [8] suggests a non-linear optimization in order to extract the lifting matrix A from a given radial fundamental we argue that this can actually be done in closed-form. [sent-467, score-0.625]
</p><p>100 In the setting used in [8], the radial distortion matrix A is only defined up to an arbitrary projective mapping of the image plane. [sent-474, score-1.056]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('distortion', 0.601), ('radial', 0.434), ('cod', 0.295), ('epipolar', 0.265), ('distorted', 0.198), ('straight', 0.158), ('undistorted', 0.149), ('epipole', 0.126), ('undistortion', 0.123), ('rational', 0.1), ('claus', 0.096), ('center', 0.093), ('fundamental', 0.091), ('brito', 0.084), ('lines', 0.083), ('camera', 0.08), ('fitzgibbon', 0.08), ('division', 0.077), ('calibration', 0.075), ('epipoles', 0.068), ('curves', 0.058), ('lifting', 0.058), ('rowspace', 0.054), ('lens', 0.051), ('cameras', 0.049), ('photogrammetric', 0.048), ('lifted', 0.046), ('radially', 0.045), ('hartley', 0.044), ('intersect', 0.041), ('yd', 0.04), ('catadioptric', 0.04), ('visualized', 0.038), ('barreto', 0.036), ('line', 0.036), ('dy', 0.032), ('consequently', 0.032), ('circles', 0.032), ('angst', 0.032), ('radius', 0.032), ('xd', 0.03), ('happens', 0.03), ('unknown', 0.029), ('homography', 0.028), ('omnidirectional', 0.028), ('geyer', 0.028), ('barrel', 0.027), ('clarke', 0.027), ('plumbline', 0.027), ('yxuu', 0.027), ('curve', 0.025), ('coordinates', 0.024), ('family', 0.024), ('conceptually', 0.023), ('must', 0.022), ('switzerland', 0.022), ('devernay', 0.022), ('urich', 0.022), ('daniilidis', 0.022), ('lie', 0.022), ('proceedings', 0.022), ('centers', 0.022), ('yx', 0.022), ('intersecting', 0.022), ('pollefeys', 0.022), ('away', 0.022), ('correspondences', 0.021), ('portugal', 0.021), ('autocalibration', 0.021), ('micusik', 0.021), ('pictures', 0.021), ('matrix', 0.021), ('argue', 0.021), ('dx', 0.02), ('geometry', 0.02), ('ser', 0.02), ('degenerate', 0.02), ('views', 0.019), ('nice', 0.019), ('obey', 0.019), ('marc', 0.019), ('roland', 0.019), ('kevin', 0.019), ('pu', 0.019), ('pajdla', 0.019), ('distortions', 0.019), ('entries', 0.018), ('later', 0.018), ('go', 0.018), ('plotted', 0.018), ('conference', 0.017), ('perspective', 0.017), ('entirely', 0.016), ('point', 0.016), ('photo', 0.016), ('pattern', 0.015), ('pinhole', 0.015), ('horizontally', 0.015), ('register', 0.015), ('zero', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="344-tfidf-1" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>2 0.19840235 <a title="344-tfidf-2" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>3 0.19092524 <a title="344-tfidf-3" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>4 0.16423962 <a title="344-tfidf-4" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>Author: Koichiro Yamaguchi, David McAllester, Raquel Urtasun</p><p>Abstract: We consider the problem of computing optical flow in monocular video taken from a moving vehicle. In this setting, the vast majority of image flow is due to the vehicle ’s ego-motion. We propose to take advantage of this fact and estimate flow along the epipolar lines of the egomotion. Towards this goal, we derive a slanted-plane MRF model which explicitly reasons about the ordering of planes and their physical validity at junctions. Furthermore, we present a bottom-up grouping algorithm which produces over-segmentations that respect flow boundaries. We demonstrate the effectiveness of our approach in the challenging KITTI flow benchmark [11] achieving half the error of the best competing general flow algorithm and one third of the error of the best epipolar flow algorithm.</p><p>5 0.16402377 <a title="344-tfidf-5" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>6 0.14974424 <a title="344-tfidf-6" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>7 0.13945036 <a title="344-tfidf-7" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>8 0.11668807 <a title="344-tfidf-8" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>9 0.098901019 <a title="344-tfidf-9" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>10 0.094568074 <a title="344-tfidf-10" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>11 0.091886468 <a title="344-tfidf-11" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>12 0.085560933 <a title="344-tfidf-12" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>13 0.083173923 <a title="344-tfidf-13" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>14 0.082274146 <a title="344-tfidf-14" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>15 0.080260143 <a title="344-tfidf-15" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>16 0.076574206 <a title="344-tfidf-16" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>17 0.070761822 <a title="344-tfidf-17" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>18 0.067604251 <a title="344-tfidf-18" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>19 0.066230454 <a title="344-tfidf-19" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>20 0.063984692 <a title="344-tfidf-20" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.106), (2, -0.003), (3, 0.023), (4, 0.001), (5, -0.04), (6, -0.031), (7, -0.053), (8, -0.012), (9, 0.033), (10, -0.023), (11, 0.109), (12, 0.132), (13, -0.068), (14, -0.15), (15, 0.028), (16, 0.061), (17, 0.063), (18, -0.017), (19, 0.057), (20, 0.057), (21, 0.015), (22, -0.031), (23, -0.059), (24, 0.022), (25, 0.024), (26, -0.038), (27, -0.013), (28, -0.008), (29, -0.022), (30, 0.01), (31, 0.023), (32, 0.003), (33, 0.087), (34, -0.098), (35, 0.043), (36, -0.053), (37, -0.058), (38, -0.012), (39, 0.117), (40, 0.039), (41, -0.038), (42, 0.008), (43, -0.036), (44, 0.04), (45, -0.096), (46, -0.029), (47, -0.139), (48, 0.091), (49, 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9671095 <a title="344-lsi-1" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>2 0.81271052 <a title="344-lsi-2" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>3 0.74669361 <a title="344-lsi-3" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>4 0.72125083 <a title="344-lsi-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.69300151 <a title="344-lsi-5" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>6 0.66325122 <a title="344-lsi-6" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>7 0.64412451 <a title="344-lsi-7" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>8 0.61889941 <a title="344-lsi-8" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>9 0.58819991 <a title="344-lsi-9" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>10 0.54535371 <a title="344-lsi-10" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>11 0.54167461 <a title="344-lsi-11" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>12 0.49099979 <a title="344-lsi-12" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>13 0.45666891 <a title="344-lsi-13" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>14 0.4559131 <a title="344-lsi-14" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>15 0.45113641 <a title="344-lsi-15" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>16 0.41706076 <a title="344-lsi-16" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>17 0.41177928 <a title="344-lsi-17" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>18 0.40548289 <a title="344-lsi-18" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>19 0.40492293 <a title="344-lsi-19" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>20 0.40203431 <a title="344-lsi-20" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.139), (16, 0.021), (26, 0.04), (33, 0.268), (66, 0.258), (67, 0.029), (69, 0.035), (87, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91733241 <a title="344-lda-1" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>Author: Zhe Wu, Ping Tan</p><p>Abstract: Under unknown directional lighting, the uncalibrated Lambertian photometric stereo algorithm recovers the shape of a smooth surface up to the generalized bas-relief (GBR) ambiguity. We resolve this ambiguity from the halfvector symmetry, which is observed in many isotropic materials. Under this symmetry, a 2D BRDF slice with low-rank structure can be obtained from an image, if the surface normals and light directions are correctly recovered. In general, this structure is destroyed by the GBR ambiguity. As a result, we can resolve the ambiguity by restoring this structure. We develop a simple algorithm of auto-calibration from separable homogeneous specular reflection of real images. Compared with previous methods, this method takes a holistic approach to exploiting reflectance symmetry and produces superior results.</p><p>2 0.89223337 <a title="344-lda-2" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>Author: Ilya Reshetouski, Alkhazur Manakov, Ayush Bandhari, Ramesh Raskar, Hans-Peter Seidel, Ivo Ihrke</p><p>Abstract: We investigate the problem of identifying the position of a viewer inside a room of planar mirrors with unknown geometry in conjunction with the room’s shape parameters. We consider the observations to consist of angularly resolved depth measurements of a single scene point that is being observed via many multi-bounce interactions with the specular room geometry. Applications of this problem statement include areas such as calibration, acoustic echo cancelation and time-of-flight imaging. We theoretically analyze the problem and derive sufficient conditions for a combination of convex room geometry, observer, and scene point to be reconstructable. The resulting constructive algorithm is exponential in nature and, therefore, not directly applicable to practical scenarios. To counter the situation, we propose theoretically devised geometric constraints that enable an efficient pruning of the solution space and develop a heuristic randomized search algorithm that uses these constraints to obtain an effective solution. We demonstrate the effectiveness of our algorithm on extensive simulations as well as in a challenging real-world calibration scenario.</p><p>same-paper 3 0.85104126 <a title="344-lda-3" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>4 0.84816575 <a title="344-lda-4" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>Author: Zhenglong Zhou, Zhe Wu, Ping Tan</p><p>Abstract: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple, which consists of only a digital camera and a handheld light source. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distributionfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments, the captured shapes are accurate to 0.3 millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9%.</p><p>5 0.8371042 <a title="344-lda-5" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>6 0.8053633 <a title="344-lda-6" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>7 0.80325782 <a title="344-lda-7" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>8 0.79648417 <a title="344-lda-8" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>9 0.7952081 <a title="344-lda-9" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>10 0.7887814 <a title="344-lda-10" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>11 0.78664362 <a title="344-lda-11" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>12 0.78644454 <a title="344-lda-12" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>13 0.78114218 <a title="344-lda-13" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>14 0.779966 <a title="344-lda-14" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>15 0.77902561 <a title="344-lda-15" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>16 0.77173716 <a title="344-lda-16" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>17 0.77095658 <a title="344-lda-17" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>18 0.77001792 <a title="344-lda-18" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>19 0.76994658 <a title="344-lda-19" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>20 0.76867115 <a title="344-lda-20" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
