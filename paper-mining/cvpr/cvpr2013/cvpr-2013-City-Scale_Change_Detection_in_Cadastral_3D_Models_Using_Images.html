<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-81" href="#">cvpr2013-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</h1>
<br/><p>Source: <a title="cvpr-2013-81-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Taneja_City-Scale_Change_Detection_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>Reference: <a title="cvpr-2013-81-reference" href="../cvpr2013_reference/cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ch le  Abstract In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. [sent-10, score-1.022]
</p><p>2 We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. [sent-11, score-0.6]
</p><p>3 These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. [sent-13, score-0.598]
</p><p>4 In fact, most city administrations already maintain such information for cadastral applications such as city planning, real estate evaluation and so on. [sent-19, score-1.065]
</p><p>5 As these changes occur, any previous reconstructions do not comply with the current state of the city and need to be updated accordingly. [sent-21, score-0.478]
</p><p>6 Changes detected on the cadastral 3D model of a city using panoramic images. [sent-24, score-1.03]
</p><p>7 The detected changes are marked in blue, while the locations of the input images are represented as green points. [sent-25, score-0.245]
</p><p>8 high resolution cameras on the scale of a city is not feasible on a frequent basis. [sent-28, score-0.351]
</p><p>9 Recent works like [15] proposed to efficiently perform this update task by first localizing in the environment the areas where geometric changes have occurred, and then by running the high quality data collection selectively only on those locations where significant changes have been de-  tected. [sent-29, score-0.327]
</p><p>10 Their work showed convincing results on multiple urban scenarios detecting changes from images. [sent-30, score-0.189]
</p><p>11 However, the evaluated locations were all spatially constrained, and while some suggestions were presented to make the approach scalable to large environments, it needs to be adapted significantly to address the different challenges involved in a city scale application of change detection. [sent-31, score-0.695]
</p><p>12 Namely, •  Inaccuracies in the cadastral 3D model: CadastIrnaal cincuforramciaetison in, m tahient caaindeads by city Dad mmiondiseltr:ations, is typically encoded as 3D mesh models representing the main constructions in the city. [sent-32, score-0.746]
</p><p>13 A large scale change detection algorithm therefore, needs to differentiate between real changes in the geometry and changes induced by inaccuracies in these cadastral 3D models. [sent-35, score-1.214]
</p><p>14 In the envisioned scenario of a city scale change detection application and model update, images depicting the current state of the city are captured as panoramic images,  from cars driving around the city. [sent-36, score-1.457]
</p><p>15 However, the data recorded with such devices is typically noisy, and while the position and orientation inaccuracies may be tolerable for applications such as street navigation, they are definitely not for the purpose of change detection. [sent-39, score-0.496]
</p><p>16 Therefore a building well visible in one image, will be only partially visible in a nearby image. [sent-41, score-0.239]
</p><p>17 A large scale change detection algorithm needs to be able to cope with such sparse imagery. [sent-42, score-0.268]
</p><p>18 In this paper, we propose a method to detect changes in the geometry of a city. [sent-43, score-0.21]
</p><p>19 While our formulation builds on the work of [15], we explicitly address the challenges involved in a large scale application of change detection. [sent-44, score-0.333]
</p><p>20 In  particular, we use cadastral 3D models provided by the city administration and panoramic images captured all over the city. [sent-45, score-1.198]
</p><p>21 For our experiments we used the Google StreetView images which, besides being publicly available, are also a good example of panoramic images captured with a driving vehicle on the scale of a city. [sent-46, score-0.601]
</p><p>22 Related Work There has been a lot of work in the field of change detection mostly focusing on comparing images of a scene captured at an earlier time instant with images captured later [12]. [sent-48, score-0.439]
</p><p>23 These changes may or may not correspond to changes in the geometry. [sent-54, score-0.25]
</p><p>24 Viceversa, if this projection reveals inconsistencies then the geometry represented in the  images is different from the original one. [sent-57, score-0.222]
</p><p>25 In this paper, we extend their approach to account for the challenges involved in a city scale application of a change detection algorithm, as mentioned in the introduction. [sent-58, score-0.696]
</p><p>26 Change Detection Given a cadastral 3D model of a city and a set of panoramic images depicting its current state, the goal of the proposed algorithm is to detect geometric changes that may have occurred between the time the 3D model was built and the time the new images were captured. [sent-60, score-1.345]
</p><p>27 For the reader’s convenience, we briefly recall, in this section, the major concepts presented in [15], namely, the inconsistency map and the used probabilistic framework. [sent-62, score-0.251]
</p><p>28 For each pair of images Is and It observing a location in the environment, the geometry of the environment is used to project the source image Is into the point of view of the target image It. [sent-63, score-0.196]
</p><p>29 The resulting image projection, denoted as It←s, is then compared with the original target image It to obtain a pixel-wise map of inconsistencies between the geometry and the images Is and It. [sent-64, score-0.299]
</p><p>30 This map is referred to as the inconsistency map, and is denoted with the symbol Mt←s. [sent-65, score-0.251]
</p><p>31 n In principle, if the two images Is and It are consistent with the geometry, then the resulting inconsistency map  Mt←s is zero everywhere. [sent-67, score-0.29]
</p><p>32 In order to localize the occurred changes in the city, the entire city is discretized into a grid of uniformly sized voxels, precisely of size 1m3 each. [sent-69, score-0.555]
</p><p>33 The goal of the change detection algorithm is to estimate a binary labeling L = {li}i for each voxel iin this grid, indicating tlaheb presence, or th}e absence, of a change inside that voxel (with li = 1and li = 0, respectively). [sent-70, score-0.568]
</p><p>34 l By using tihveen Bayes’ p ruutle i,m mthagise corresponds to  P (li|I) =P (IP|li ()IP) (li)  (1)  where the generative model P (I|li) is computed on the bawsish oerfe et thhee inconsistency maps. [sent-72, score-0.216]
</p><p>35 Since changes corresponding to vehicles, pedestrians and vegetation are not relevant for the purpose of updating a 3D model, a classifier is used to recognize those classes of objects in the images [4]. [sent-79, score-0.237]
</p><p>36 This approach on its own is however not sufficient to deal with the challenges involved in a large scale application of change detection, such as geometric inaccuracies, geolocation information inaccuracies, and wide baseline imagery. [sent-81, score-0.406]
</p><p>37 Inaccuracies in the geo-location information In a scenario where a car is driving around capturing panoramic images in a city, the geo-location information, providing the position and orientation where each of these images were taken, is typically captured using sensors like GPSs and IMUs. [sent-85, score-0.568]
</p><p>38 The data recorded by these sensors is in general noisy, with errors being on the order of ±5 meters gine tnherea alloc naotiisoyn, awnitdh ± er5r degrees gin o tnhe th hoeri oerndteatrio onf. [sent-86, score-0.193]
</p><p>39 [ 14], cannot be applied in this case due to the typical absence of texture information in the cadastral 3D models. [sent-91, score-0.427]
</p><p>40 False changes due to missing details on the building facade disappear in the latter. [sent-114, score-0.31]
</p><p>41 For each panoramic image, an object class segmentation is performed in order to estimate the building outlines in these images [7]. [sent-116, score-0.636]
</p><p>42 Let St denote the building outlines estimated on the image It. [sent-117, score-0.313]
</p><p>43 Let ξt represent the current estimate for the pose of image It (the geolocation information), and let B(ξt) denote the corresponding building outlines obtained by rendering the cadastral 3D model at pose ξt. [sent-119, score-0.779]
</p><p>44 Ideally, at the correct pose estimate, the building outlines B(ξt) align perfectly with the actual outlines St. [sent-120, score-0.539]
</p><p>45 Our registration approach In general, minimizing for Equation 4 results in an accurate registration of the input images with respect to the cadastral 3D model. [sent-129, score-0.664]
</p><p>46 However, while the individual errors in the registration might be small, these errors quickly accumulate during the reprojection process. [sent-130, score-0.216]
</p><p>47 Since the proposed change detection algorithm bases its inference on the reprojected images It←s, even small errors in the registration are not tolerable, since they will generate false evidence of a change in the inconsistency maps Mt←s. [sent-131, score-0.916]
</p><p>48 Minimizing for Equation 4 is therefore insufficient for our purpose, and a registration technique accounting also for the relative alignment between neighboring images, needs to be designed. [sent-132, score-0.218]
</p><p>49 We then perform |thIe pose estimation over a window of n = 5 consecutive panoramic  images. [sent-134, score-0.284]
</p><p>50 This makes the pose estimation more robust to outliers, such as changes in the geometry and/or segmentation errors in the images. [sent-159, score-0.254]
</p><p>51 Dealing with geometric inaccuracies Cadastral 3D models typically show low level of detail. [sent-164, score-0.257]
</p><p>52 Consequently, the projections of each of these structures from one image into another can result in high inconsistency values in the Mt←s maps. [sent-166, score-0.216]
</p><p>53 To account for these geometric inaccuracies, we draw multiple hypotheses on the real extent of the missing or the inaccurately represented structures, by shifting the building walls on the ground plane. [sent-168, score-0.245]
</p><p>54 For each of these hypotheses, the corresponding inconsistency map is computed. [sent-169, score-0.251]
</p><p>55 In principle, the inconsistency map Mt←s resulting from a geometry which perfectly represents the actual building corresponds to the pixel-wise minimum of the individual inconsistency maps produced by each hypothesis. [sent-170, score-0.736]
</p><p>56 Then the inconsistency map resulting from a perfectly represented geometry is  v,  v  Mt←s = min |I? [sent-173, score-0.368]
</p><p>57 Figure 2 shows the effects of the usage of this approach on the generated Mt←s maps in a scenario where the balconies and the extended roofofa building facade were missing from the 3D model. [sent-180, score-0.268]
</p><p>58 It is visible, in the bottom image, that false inconsistencies disappear when multiple hypotheses are evaluated for the location of these elements. [sent-181, score-0.241]
</p><p>59 Dealing with sparse imagery While the multiple hypotheses approach introduced in the previous section allows us to account for small inaccuracies in the cadastral 3D model, another issue needs to be considered when projecting images captured very far apart. [sent-184, score-0.875]
</p><p>60 In these cases in fact, high perspective distortions and image sub-samplings corrupt the reprojected image It←s by generating blurring artifacts (Figure 3(c)), and consequently decreasing the accuracy of the detector by generating more false positives (Figure 3(d)). [sent-185, score-0.201]
</p><p>61 This however would also reduce the amount of information at our disposal for the change detection inference. [sent-188, score-0.197]
</p><p>62 Since we already have a  limited amount of data observing the same location, due to the sparse imagery, we need to use all possible images inside a certain radius even if that means considering images captured more than 30 meters apart. [sent-189, score-0.303]
</p><p>63 Therefore, to better compare the reprojected image It←s with the target image It, we simulate in It the same blurring  artifacts as in It←s, by applying to each pixel of It a spatial filter shaped accordingly to the ellipse projecting into p. [sent-207, score-0.225]
</p><p>64 It is visible that accounting for these distortions/blurring artifacts significantly improves the Mt←s image by eliminating the false inconsistencies caused by the large baseline between the images. [sent-215, score-0.357]
</p><p>65 Additional cue: building outlines consistency To improve the performance of our detection algorithm, we introduce an additional cue to the original generative model P (I|li) of Equation 2, accounting for building outmlinoedse consistency. [sent-218, score-0.547]
</p><p>66 In principle, in case of no change, not only should the inconsistency Mt←s maps be zero, but the corresponding building outlines seen in the images should be consistent with those in the geometry as well. [sent-219, score-0.686]
</p><p>67 Formally, let Ct be the image representing the pixelwise inconsistencies between the building outlines estimated from the image It and the outlines of the 3D model visible from the point of view of It, i. [sent-220, score-0.665]
</p><p>68 t  While the first series of products indicate the independence between the image formation process of the different inconsistency maps Mt←s, the second series of products underlines the independence between the image formation process of the building outlines seen from the different images It. [sent-230, score-0.601]
</p><p>69 Further, assuming that the conditional probability of Ct given a voxel label li is only influenced by the pixels in the footprint of voxel i on Ct, we introduce an additional random variable ηti representing the fraction of incorrec? [sent-231, score-0.19]
</p><p>70 l i == 10  (10)  ×  As observed earlier, inaccuracies in the geometry might lead to false inconsistencies in Ct. [sent-238, score-0.468]
</p><p>71 In total, 3420 panoramic images were used to detect changes in this environment. [sent-246, score-0.448]
</p><p>72 The geo-location data for each panoramic image was obtained also from the Google StreetView service. [sent-252, score-0.284]
</p><p>73 Since this data is in general too inaccurate for the purpose of change  detection, showing errors with a standard deviation of 3. [sent-253, score-0.197]
</p><p>74 Precisely, Equation 5 was optimized using an initial swarm noise of 7 meters in translation and 6 degrees in rotation. [sent-256, score-0.267]
</p><p>75 The cadastral 3D model was instead obtained from the city administration, and its claimed accuracy was 0. [sent-257, score-0.746]
</p><p>76 Similarly, the parameters σc and σs, modeling the color and building outline consistency respectively (see Equation 3 and Equation 10), were estimated on another set of 75 images where each pixel was manually labeled as change or no change. [sent-264, score-0.34]
</p><p>77 Figure 5 and Figure 1 show the changes detected by our approach on two small regions of the processed cadastral 3D model. [sent-267, score-0.552]
</p><p>78 The green dots denote the locations of the input panoramas, while the blue dots represent voxels labeled as change. [sent-268, score-0.252]
</p><p>79 Each of those images shows the cadastral 3D model (red) overlaid on one of the input panoramic images captured at that location. [sent-270, score-0.871]
</p><p>80 It is visible that a high density of the blue voxels in the map corresponds to a change revealed by the input images. [sent-271, score-0.328]
</p><p>81 Locations (G) and (H) instead show two examples of false changes that were detected due to trees (mislabeled as building by the classifier), and due to strong reflections, respectively. [sent-301, score-0.306]
</p><p>82 Quantitative evaluation and comparison with prior work We generated ground truth data by manually labeling each panoramic image as corresponding to a change or not. [sent-304, score-0.437]
</p><p>83 The labeling was performed on the basis that, an image represents a change if an actual change in the geometry was visible from approximately 25 meters distance. [sent-306, score-0.564]
</p><p>84 We compared this ground truth with the results obtained using our change detection algorithm. [sent-308, score-0.197]
</p><p>85 Precisely, using the same labeling methodology as for the ground truth, an image was labeled as corresponding to a change if a sufficient number of voxels were detected as change in a radius of 25 meters from the image location. [sent-309, score-0.528]
</p><p>86 4, making it more robust to inaccuracies in the geometry and to wide baseline imagery. [sent-321, score-0.308]
</p><p>87 Conclusions In this paper, we proposed a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. [sent-323, score-1.022]
</p><p>88 We extended the work of [15] to account for all the challenges involved in a large scale application of change detection. [sent-324, score-0.333]
</p><p>89 In particular, we showed how to deal with the geometric inaccuracies typically present in a cadastral 3D model, by evaluating different hypotheses on the correct geometry of the buildings contained in it. [sent-325, score-0.879]
</p><p>90 We showed how to deal with errors in the geo-location data of the input images, by proposing a registration technique aimed at minimizing the absolute alignment error of each image with respect to the 3D model, as well as the relative alignment error with respect to its neighboring images. [sent-326, score-0.239]
</p><p>91 To further improve the detection accuracy, we proposed to use building outlines as an additional cue for our change detection inference. [sent-328, score-0.554]
</p><p>92 The performance of our algorithm was evaluated on the scale of a city (6 square kilometers area) using 3420 images downloaded from Google StreetView. [sent-329, score-0.502]
</p><p>93 These images, besides being publicly available, are also a good example of panoramic images captured with a driving vehicle on the scale of a city. [sent-330, score-0.562]
</p><p>94 Using 3d line segments for robust and efficient change detection from multiple noisy images. [sent-354, score-0.197]
</p><p>95 Constructing 3-d city models by merging aerial and ground views. [sent-368, score-0.349]
</p><p>96 Monitoring changes of 3d building elements from unordered photo collections. [sent-374, score-0.244]
</p><p>97 Towards geographical referencing of monocular slam reconstruction using 3d city models: Application to real-time accurate vision-based localization. [sent-395, score-0.319]
</p><p>98 Image based detection of geometric changes in urban environments. [sent-436, score-0.267]
</p><p>99 Registration of spherical panoramic images with cadastral 3d models. [sent-442, score-0.75]
</p><p>100 (Bottom) Images  corresponding to the green markers in the map overlaid with the cadastral model. [sent-468, score-0.56]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cadastral', 0.427), ('city', 0.319), ('panoramic', 0.284), ('mt', 0.242), ('inaccuracies', 0.223), ('inconsistency', 0.216), ('outlines', 0.194), ('change', 0.153), ('changes', 0.125), ('building', 0.119), ('swarm', 0.118), ('meters', 0.113), ('registration', 0.099), ('inconsistencies', 0.098), ('driving', 0.088), ('ct', 0.086), ('geometry', 0.085), ('captured', 0.082), ('kilometers', 0.08), ('roofs', 0.08), ('voxels', 0.08), ('reprojected', 0.073), ('accounting', 0.071), ('artifacts', 0.066), ('ballan', 0.066), ('streetview', 0.066), ('taneja', 0.066), ('challenges', 0.066), ('urban', 0.064), ('equation', 0.064), ('precisely', 0.063), ('false', 0.062), ('buildings', 0.062), ('visible', 0.06), ('markers', 0.06), ('li', 0.058), ('imagery', 0.056), ('google', 0.054), ('aparna', 0.053), ('tolerable', 0.053), ('viceversa', 0.053), ('zurich', 0.053), ('voxel', 0.051), ('involved', 0.051), ('roc', 0.048), ('hypotheses', 0.048), ('occurred', 0.048), ('alignment', 0.048), ('administration', 0.047), ('balconies', 0.047), ('protruding', 0.047), ('ellipse', 0.044), ('errors', 0.044), ('detection', 0.044), ('luca', 0.044), ('inaccurately', 0.044), ('vegetation', 0.044), ('locations', 0.043), ('target', 0.042), ('cornelis', 0.041), ('old', 0.04), ('formally', 0.04), ('geolocation', 0.039), ('eth', 0.039), ('cope', 0.039), ('images', 0.039), ('evolutionary', 0.038), ('marc', 0.038), ('green', 0.038), ('inf', 0.038), ('vehicle', 0.037), ('cities', 0.037), ('particle', 0.036), ('degrees', 0.036), ('scenario', 0.036), ('blending', 0.035), ('definitely', 0.035), ('map', 0.035), ('geometric', 0.034), ('reconstructions', 0.034), ('facade', 0.033), ('disappear', 0.033), ('maps', 0.033), ('street', 0.032), ('scale', 0.032), ('st', 0.032), ('downloaded', 0.032), ('perfectly', 0.032), ('dots', 0.031), ('planning', 0.031), ('application', 0.031), ('curve', 0.031), ('aerial', 0.03), ('influenced', 0.03), ('depicting', 0.03), ('observing', 0.03), ('navigation', 0.029), ('updating', 0.029), ('labeled', 0.029), ('reprojection', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="81-tfidf-1" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>2 0.20637949 <a title="81-tfidf-2" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>3 0.12543267 <a title="81-tfidf-3" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>4 0.099975847 <a title="81-tfidf-4" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>Author: Christian Häne, Christopher Zach, Andrea Cohen, Roland Angst, Marc Pollefeys</p><p>Abstract: Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being ’too noisy’. Unfortunately, these priors generally yield overly smooth reconstructions and/or segmentations in certain regions whereas they fail in other areas to constrain the solution sufficiently. In this paper we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other’s task. As a consequence, we propose a rigorous mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. Image segmentations provide geometric cues about which surface orientations are more likely to appear at a certain location in space whereas a dense 3D reconstruction yields a suitable regularization for the segmentation problem by lifting the labeling from 2D images to 3D space. We show how appearance-based cues and 3D surface orientation priors can be learned from training data and subsequently used for class-specific regularization. Experimental results on several real data sets highlight the advantages of our joint formulation.</p><p>5 0.075827695 <a title="81-tfidf-5" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>6 0.073834509 <a title="81-tfidf-6" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>7 0.073667631 <a title="81-tfidf-7" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>8 0.069958381 <a title="81-tfidf-8" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>9 0.068650283 <a title="81-tfidf-9" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>10 0.066273309 <a title="81-tfidf-10" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>11 0.065997995 <a title="81-tfidf-11" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>12 0.065838762 <a title="81-tfidf-12" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>13 0.065806031 <a title="81-tfidf-13" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>14 0.064438514 <a title="81-tfidf-14" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>15 0.063387528 <a title="81-tfidf-15" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>16 0.062867492 <a title="81-tfidf-16" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>17 0.062618226 <a title="81-tfidf-17" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>18 0.061821651 <a title="81-tfidf-18" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>19 0.060709469 <a title="81-tfidf-19" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>20 0.059967287 <a title="81-tfidf-20" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, 0.061), (2, 0.004), (3, -0.002), (4, 0.028), (5, -0.018), (6, 0.004), (7, 0.003), (8, 0.014), (9, 0.011), (10, -0.007), (11, 0.012), (12, 0.014), (13, -0.025), (14, -0.006), (15, -0.121), (16, 0.005), (17, 0.054), (18, 0.041), (19, 0.003), (20, 0.024), (21, 0.001), (22, -0.028), (23, 0.004), (24, -0.028), (25, -0.04), (26, 0.028), (27, -0.077), (28, 0.033), (29, 0.024), (30, -0.022), (31, 0.006), (32, -0.039), (33, 0.041), (34, 0.037), (35, -0.116), (36, -0.014), (37, 0.016), (38, -0.068), (39, -0.004), (40, 0.011), (41, -0.006), (42, -0.02), (43, -0.094), (44, -0.066), (45, -0.003), (46, -0.047), (47, -0.084), (48, -0.146), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9225114 <a title="81-lsi-1" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>2 0.68161637 <a title="81-lsi-2" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>3 0.64076161 <a title="81-lsi-3" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>4 0.61674923 <a title="81-lsi-4" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>Author: Shihui Ying, Guorong Wu, Qian Wang, Dinggang Shen</p><p>Abstract: Recently, groupwise registration has been investigated for simultaneous alignment of all images without selecting any individual image as the template, thus avoiding the potential bias in image registration. However, none of current groupwise registration method fully utilizes the image distribution to guide the registration. Thus, the registration performance usually suffers from large inter-subject variations across individual images. To solve this issue, we propose a novel groupwise registration algorithm for large population dataset, guided by the image distribution on the manifold. Specifically, we first use a graph to model the distribution of all image data sitting on the image manifold, with each node representing an image and each edge representing the geodesic pathway between two nodes (or images). Then, the procedure of warping all images to theirpopulation center turns to the dynamic shrinking ofthe graph nodes along their graph edges until all graph nodes become close to each other. Thus, the topology ofimage distribution on the image manifold is always preserved during the groupwise registration. More importantly, by modeling , the distribution of all images via a graph, we can potentially reduce registration error since every time each image is warped only according to its nearby images with similar structures in the graph. We have evaluated our proposed groupwise registration method on both synthetic and real datasets, with comparison to the two state-of-the-art groupwise registration methods. All experimental results show that our proposed method achieves the best performance in terms of registration accuracy and robustness.</p><p>5 0.61551827 <a title="81-lsi-5" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>6 0.6076346 <a title="81-lsi-6" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>7 0.59756583 <a title="81-lsi-7" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>8 0.59628123 <a title="81-lsi-8" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>9 0.57960457 <a title="81-lsi-9" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>10 0.55685323 <a title="81-lsi-10" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>11 0.55318552 <a title="81-lsi-11" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>12 0.55278641 <a title="81-lsi-12" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>13 0.55163616 <a title="81-lsi-13" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>14 0.54282475 <a title="81-lsi-14" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>15 0.53208375 <a title="81-lsi-15" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>16 0.5315026 <a title="81-lsi-16" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>17 0.52948612 <a title="81-lsi-17" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>18 0.52124763 <a title="81-lsi-18" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>19 0.51723909 <a title="81-lsi-19" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>20 0.50576568 <a title="81-lsi-20" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.011), (10, 0.118), (16, 0.025), (26, 0.042), (33, 0.303), (41, 0.209), (67, 0.049), (69, 0.037), (87, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88518298 <a title="81-lda-1" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>2 0.88437057 <a title="81-lda-2" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>Author: Paul Wohlhart, Martin Köstinger, Michael Donoser, Peter M. Roth, Horst Bischof</p><p>Abstract: The development of complex, powerful classifiers and their constant improvement have contributed much to the progress in many fields of computer vision. However, the trend towards large scale datasets revived the interest in simpler classifiers to reduce runtime. Simple nearest neighbor classifiers have several beneficial properties, such as low complexity and inherent multi-class handling, however, they have a runtime linear in the size of the database. Recent related work represents data samples by assigning them to a set of prototypes that partition the input feature space and afterwards applies linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper, we go a step beyond these approaches and purely focus on 1-nearest prototype classification, where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable, parameter free, avoids noise overfitting and, since during testing only comparisons to the derived prototypes are required, highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods, while even getting close to the results of more complex classifiers.</p><p>3 0.87445855 <a title="81-lda-3" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>Author: Weihong Deng, Jiani Hu, Jun Guo</p><p>Abstract: The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. The experiments results on AR, FERET and FRGC databases validate that, if the proposed prototype plus variation representation model is applied, sparse coding plays a crucial role in face recognition, and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.</p><p>4 0.85754633 <a title="81-lda-4" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>5 0.85544038 <a title="81-lda-5" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>6 0.85451597 <a title="81-lda-6" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>7 0.85394275 <a title="81-lda-7" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>8 0.85254329 <a title="81-lda-8" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>9 0.85224807 <a title="81-lda-9" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>10 0.85148209 <a title="81-lda-10" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>11 0.85057515 <a title="81-lda-11" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>12 0.85014182 <a title="81-lda-12" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>13 0.84969449 <a title="81-lda-13" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>14 0.84951448 <a title="81-lda-14" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>15 0.84894168 <a title="81-lda-15" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>16 0.84893626 <a title="81-lda-16" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>17 0.84880823 <a title="81-lda-17" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>18 0.84833199 <a title="81-lda-18" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>19 0.84787816 <a title="81-lda-19" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>20 0.8477757 <a title="81-lda-20" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
