<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-153" href="#">cvpr2013-153</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</h1>
<br/><p>Source: <a title="cvpr-2013-153-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sharma_Expanded_Parts_Model_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>Reference: <a title="cvpr-2013-153-reference" href="../cvpr2013_reference/cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract We propose a new model for recognizing human attributes (e. [sent-3, score-0.384]
</p><p>2 wearing a suit, sitting, short hair) and actions (e. [sent-5, score-0.348]
</p><p>3 The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). [sent-8, score-0.811]
</p><p>4 To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. [sent-12, score-0.683]
</p><p>5 We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. [sent-13, score-0.456]
</p><p>6 Introduction  The focus of this paper is a semantic description of humans in still images using attributes and actions. [sent-16, score-0.263]
</p><p>7 Given the daily growing amount of human centric data (e. [sent-17, score-0.221]
</p><p>8 Most recent work on human attributes or action recognition either rely on, accurate or approximate, estimation of human pose e. [sent-20, score-0.679]
</p><p>9 Interestingly, several recent methods propose to model interactions between humans and the object(s) associated with the actions [8, 10, 15, 24, 34, 35]. [sent-26, score-0.345]
</p><p>10 While modelling interactions between humans and contextual objects is an interesting problem, we explore here the broader problem of modelling appearance of humans for  ? [sent-27, score-0.259]
</p><p>11 actions like running, walking) and/or the pose is not immediately relevant (e. [sent-69, score-0.288]
</p><p>12 As human attributes and actions are often localized in space e. [sent-81, score-0.567]
</p><p>13 shoulder regions for ‘wearing a tank top’, we aim to explain the images partially with the most discriminative regions, i. [sent-83, score-0.264]
</p><p>14 In our model the parts compete to explain each image individually, which is in contrast with traditional part based discriminative mod-  els where all parts are used for every image. [sent-87, score-0.664]
</p><p>15 Our learning algorithm allows us 666555002  to mine out important parts for the task, and learn their discriminative templates from a large pool of candidate parts obtained by dense random sampling of the training images. [sent-89, score-1.028]
</p><p>16 We validate our method on three publicly available datasets of human attributes and actions, and show promising qualitative and state-of-the-art quantitative results. [sent-90, score-0.382]
</p><p>17 Generic image classification algorithms, which have been quite successful in human action recognition [11], generally learn a discriminative model for each class. [sent-94, score-0.422]
</p><p>18 A discriminative class model w is, then, learnt using a margin maximizing classifier [16]. [sent-96, score-0.34]
</p><p>19 A new image is scored based on its match with the learnt class model, quantified as the dot product wTx between the image vector x and the class model w. [sent-97, score-0.239]
</p><p>20 Although SPM has never been viewed as a template learning method, methods using histogram of oriented gradients (HOG) [6] features have been  presented as such and the recent literature is full of visualizations of templates (class models) learnt with HOG-like features e. [sent-99, score-0.63]
</p><p>21 Both of these methods have been applied to the task of human analysis [7] and we build on them and formulate our model in a discriminative template learning framework. [sent-102, score-0.341]
</p><p>22 We differ in that we learn a collection of templates instead of a single template. [sent-103, score-0.371]
</p><p>23 The recently proposed Exemplar SVM (ESVM) [21] learns discriminative templates for each object instance of the training set independently and then combines their calibrated outputs on test images as a post-processing step. [sent-104, score-0.52]
</p><p>24 In contrast, we work at a part level and use all templates together during both training and testing. [sent-105, score-0.429]
</p><p>25 Similar to our approach it involves sampling image regions and, then, vector quantizes the region descriptors, whereas we propose a mechanism to select discriminative regions and build discriminative partbased models from them. [sent-107, score-0.284]
</p><p>26 In contrast humans involved in actions can have huge appearance variations due to both cosmetic changes (e. [sent-112, score-0.288]
</p><p>27 Probably  because of the high complexity of such a task, DPMs do not perform well for human action recognition [7]. [sent-116, score-0.28]
</p><p>28 pose estimation [9, 33, 39], where a relatively large number of components and parts are used. [sent-122, score-0.273]
</p><p>29 While the components account for global changes in aspect/viewpoint, the parts account for the local variations of the articulations. [sent-123, score-0.213]
</p><p>30 Here, we propose a richer, but less structured, expanded parts model. [sent-125, score-0.301]
</p><p>31 2 (left), in the mixture of components model the training images are usually assigned to only one component and thus contribute to training only one of the templates (and similarly in testing). [sent-127, score-0.428]
</p><p>32 Note that the clustering and averaging within such a model are a form of regularization/complexity control enforced by the system, which involves manual setting the number of parts and components. [sent-129, score-0.213]
</p><p>33 In the proposed expanded parts model (i) we neither enforce nor forbid averaging a priori and (ii) we allow the model to have a large number of ‘parts’ (up to the order of the number of training images) if found necessary despite  sufficient regularization (Fig. [sent-130, score-0.351]
</p><p>34 While in part-based deformable models the parts initialization is either based on heuristics (e. [sent-132, score-0.213]
</p><p>35 initialization with regions with high average energy [13]) or available annotations [9], our method systematically explores parts at all possible locations, scales and atomicities and selects the ones best suited for the task. [sent-134, score-0.213]
</p><p>36 Our model belongs to a family of models which use parts but do not assume that all possible variations and articulations can be captured by a few averaged, spatially constrained, templates of parts. [sent-136, score-0.611]
</p><p>37 Our model has similarities with poselets [3, 4, 20] which are compound parts consisting of multiple anatomical parts, highly clustered in 3D configuration space e. [sent-137, score-0.302]
</p><p>38 6 (top right) shows some of our parts for the ‘female’ class which show some resemblances with poselets, though are not as clean. [sent-144, score-0.254]
</p><p>39 While poselets learn discriminative templates, meth1See the results of different versions of the DPM software http://people. [sent-145, score-0.231]
</p><p>40 are generally generative while here we aim to mine out good patches and learn corresponding discriminative templates with the direct aim of achieving good classification. [sent-155, score-0.613]
</p><p>41 Moreover, such models have not been previously applied to human attribute and/or action recognition. [sent-156, score-0.333]
</p><p>42 We define (the parameters of) our model to be a collection of discriminative templates with an associated scale space location and the image scoring as a process of partially ‘reconstructing’ the important (task specific) regions in the images from these discriminative templates. [sent-167, score-0.88]
</p><p>43 Shape preference seems to work, and perhaps to help, for human pose estimation [9, 33, 39] but seems to be a probable reason for the disappointing performance of DPMs on human action recognition [7]. [sent-169, score-0.544]
</p><p>44 Regularized loss minimization  ××  Our model is defined as a collection of discriminative templates with associated locations i. [sent-178, score-0.513]
</p><p>45 e Mr Mof = parts, ,dl |isw wth ∈e size ,olf ∈Bo RF cod}eb wohoekr,e w i iss t thhee cn ounmcbateern oatfio pnar tos,f dN i part templates (each of dimension d) and lis a matrix of their scale-space positions, with each row specifying a bounding box i. [sent-182, score-0.379]
</p><p>46 αpf(Ii, lp) 10: endw f ←or 11: end for 12: part s image map ← note image parts (M, I) 13: M ← prune parts (M←, part sma igmea pgaer mtsa (pM) 14: iMf it e←r = pr 5u ndeo r ←s ( Mr/,5 end if 15: enidf fi toerr  2. [sent-204, score-0.577]
</p><p>47 Scoring function Our scoring function is inspired by the method of image scoring with learnt discriminative templates and that by image reconstruction. [sent-206, score-1.009]
</p><p>48 We want to score the image with the part templates which are capable of reconstructing it well while penalizing high overlap. [sent-207, score-0.459]
</p><p>49 The discriminative information for human actions and attributes is often localized in space i. [sent-208, score-0.709]
</p><p>50 for ‘riding horse’ only the rider and the horse is discriminative and not the background and for ‘wearing shorts’ only the lower part of the image is important. [sent-210, score-0.29]
</p><p>51 Hence, we aim to reconstruct the image partially (in space) with the most important parts only (e. [sent-211, score-0.286]
</p><p>52 , αN] are the binary coefficients which specify if a model part is used to score the image or not, Ov (α, l) calculates overlap between the parts selected to score the image. [sent-232, score-0.389]
</p><p>53 0 norm constraint on α enforces the use of exactly k parts for scoring while the second constraint encourages coverage in reconstruction by limiting high overlaps. [sent-234, score-0.404]
</p><p>54 {lp} and tihnieti paoliszieti our mainoidnegl parts as wp =at [2xp, −iti1o]nTs, iw. [sent-252, score-0.323]
</p><p>55 At any given instant we greedily select the best scoring part (and assign corresponding αp = 1) which does not overlap appreciably with the currently selected parts which were generated from the same training image as that of the part under consideration. [sent-270, score-0.556]
</p><p>56 We observed, on initial experiments on validation sets, that if a false positive was similar to one train image, then it would take numerous overlapping parts which were generated from that image. [sent-271, score-0.286]
</p><p>57 we do not use the parts which were generated from the same training image. [sent-276, score-0.263]
</p><p>58 We keep a record of which part is being used to reconstruct which images while training and then simply prune the parts which have not been used to reconstruct even a single image. [sent-427, score-0.441]
</p><p>59 Such parts only contribute to the | |w| |22 teevremn aan sdin ngolte t iom tahgee l. [sent-428, score-0.213]
</p><p>60 Pruning in this way removes nearduplicate and non discriminative parts (Fig. [sent-430, score-0.355]
</p><p>61 3) which were considered because of the random sampling and allows us to mine out the discriminative parts. [sent-431, score-0.224]
</p><p>62 For fixing the number of parts we did preliminary experiments on the validation set of Willow actions database [7]. [sent-434, score-0.544]
</p><p>63 With this we concluded that we need a sufficiently high number of parts and fixed k = 100 for all experiments. [sent-436, score-0.213]
</p><p>64 Since we initialize our parts with the BoFs of patches from training images, we can use the initial patches to visualize the reconstructions. [sent-451, score-0.385]
</p><p>65 0s 1765430 This is clearly a loose association as the part templates wp  ×  are modified in the the learning process, but we found it to give reasonable visualizations (Fig. [sent-455, score-0.53]
</p><p>66 Experimental results We evaluate our method on three challenging publicly available databases: (i) Willow 7 human actions [7], (ii) 27 human attributes (HAT) [25], and (iii) Stanford 40 human actions [37]. [sent-458, score-1.067]
</p><p>67 To include immediate context we expand the human bounding boxes by 50% in both width and height. [sent-478, score-0.262]
</p><p>68 the method focuses on the relevant parts, such as torso and arms for ‘bent arms’, shorts and tee-shirts for ‘wearing bermuda shorts’, and even computer (left bottom) for ‘using computer’ . [sent-510, score-0.298]
</p><p>69 Interestingly, we observe that for both riding horse and riding bike classes, the person gets ignored but the hairs and helmet have been partially reconstructed. [sent-511, score-0.401]
</p><p>70 This seems to stress the discriminative nature of the learnt models. [sent-512, score-0.333]
</p><p>71 Willow actions database Willow actions4 [7] is a challenging database for action classification on unconstrained consumer images downloaded from the internet. [sent-516, score-0.5]
</p><p>72 The task is to predict the action being performed given the human bounding box. [sent-521, score-0.28]
</p><p>73 Database of human attributes (HAT) HAT5 is a database for learning semantic human attributes. [sent-532, score-0.539]
</p><p>74 It contains 9344 unconstrained human images obtained by applying a human detector [13] on images downloaded from the internet. [sent-533, score-0.272]
</p><p>75 It has annotations for 27 attributes based on sex, pose (e. [sent-534, score-0.263]
</p><p>76 The models are learnt with the train and validation sets and the performance is reported on the test set. [sent-542, score-0.23]
</p><p>77 The per attribute performance (AP) of the proposed methods (red/dark) and the baseline SPM [16] (blue/light) on the database of Human attributes (HAT) [25]. [sent-551, score-0.378]
</p><p>78 Among the different human attributes those based on pose (e. [sent-559, score-0.399]
</p><p>79 5 indicates that recognizing human attributes is far from solved. [sent-566, score-0.384]
</p><p>80 Stanford 40 actions Stanford 40 actions6 [36] is a database of human actions with 40 diverse daily human actions e. [sent-569, score-1.054]
</p><p>81 To derive their bases they use pre-trained systems for 81 objects, 45 attributes and 150 poselets, using large amount (comparable to the size of the database) of external data. [sent-597, score-0.242]
</p><p>82 Since they use human based attributes also, arguably, our method can be used to improve their generic classifiers and improve performance further i. [sent-598, score-0.339]
</p><p>83 2 norm of the learnt part templates, along with top scoring patches for selected parts, with norms across the spectrum for three classes. [sent-605, score-0.498]
</p><p>84 The first image in any row is the patch with which the part was initialized and the remaining one are its top scoring patches. [sent-606, score-0.279]
</p><p>85 The top scoring patches give an idea of what kind of appearances the learnt templates wp captures. [sent-607, score-0.847]
</p><p>86 We observe that, across datasets, while most of the parts seem interpretable, like face, head, arms, horse saddle, legs etc. [sent-608, score-0.348]
</p><p>87 , there are a few parts which seem to correspond to random background (e. [sent-609, score-0.251]
</p><p>88 This is in line with a recent study [40], in “mixture of template’ like formulations, there are clean interpretable templates along with noisy templates which correspond to background. [sent-612, score-0.701]
</p><p>89 Determining a clear relation between the statistics of templates and their contribution to the overall performance is an interesting question, which we leave as future work. [sent-619, score-0.328]
</p><p>90 The model learns a collection of discriminative templates which can appear at specific scalespace positions. [sent-630, score-0.513]
</p><p>91 The algorithm is capable of exploring a large number of candidate parts and mining out the discriminative parts best suited for the cur-  rent binary classification. [sent-633, score-0.568]
</p><p>92 We validated our method on three challenging publicly available datasets for human attributes and actions. [sent-634, score-0.339]
</p><p>93 We analysed the learnt parts with statistics of their discriminative templates and plan to pursue this direction further to gain additional insight. [sent-636, score-0.84]
</p><p>94 Poselets: Body part detectors trained using 3D human pose annotations. [sent-660, score-0.247]
</p><p>95 Recognizing human actions in still images: A study ofbag-of-features and part-based representations. [sent-683, score-0.364]
</p><p>96 the part and the remaining images are its top scoring patches (see Sec. [sent-734, score-0.303]
</p><p>97 Recognizing human actions from still images with latent poses. [sent-859, score-0.364]
</p><p>98 Grouplet: A structured image representation for recognizing human and object interactions. [sent-870, score-0.217]
</p><p>99 Modeling mutual context of object and human pose in human-object interaction activities. [sent-875, score-0.257]
</p><p>100 Action recognition by learning bases of action attributes and parts. [sent-886, score-0.386]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('templates', 0.328), ('actions', 0.228), ('parts', 0.213), ('bof', 0.211), ('attributes', 0.203), ('scoring', 0.191), ('epm', 0.175), ('shorts', 0.164), ('learnt', 0.157), ('action', 0.144), ('discriminative', 0.142), ('willow', 0.139), ('human', 0.136), ('spm', 0.126), ('wearing', 0.12), ('riding', 0.117), ('wp', 0.11), ('horse', 0.097), ('stanford', 0.089), ('poselets', 0.089), ('expanded', 0.088), ('mine', 0.082), ('hat', 0.076), ('lp', 0.076), ('articulations', 0.07), ('arms', 0.068), ('female', 0.067), ('bermuda', 0.066), ('caen', 0.066), ('wtpxb', 0.066), ('immediate', 0.065), ('database', 0.064), ('template', 0.063), ('context', 0.061), ('patches', 0.061), ('pose', 0.06), ('ii', 0.06), ('humans', 0.06), ('yis', 0.059), ('bent', 0.058), ('crouching', 0.058), ('baseline', 0.058), ('interactions', 0.057), ('elderly', 0.054), ('wtx', 0.054), ('attribute', 0.053), ('yao', 0.052), ('sitting', 0.051), ('part', 0.051), ('centric', 0.051), ('standing', 0.05), ('training', 0.05), ('hair', 0.05), ('prune', 0.049), ('recognizing', 0.045), ('explain', 0.045), ('interpretable', 0.045), ('score', 0.045), ('shoulder', 0.043), ('delaitre', 0.043), ('qualitative', 0.043), ('collection', 0.043), ('raised', 0.042), ('convincing', 0.042), ('bank', 0.042), ('slower', 0.041), ('class', 0.041), ('climbing', 0.041), ('visualizations', 0.041), ('stochastic', 0.041), ('full', 0.041), ('modelling', 0.041), ('ov', 0.04), ('clothes', 0.04), ('bases', 0.039), ('validation', 0.039), ('reconstruct', 0.039), ('sharma', 0.039), ('dpms', 0.039), ('trivially', 0.038), ('khosla', 0.038), ('norms', 0.038), ('seem', 0.038), ('weakly', 0.037), ('boiman', 0.037), ('bourdev', 0.037), ('patch', 0.037), ('vlfeat', 0.036), ('bike', 0.036), ('structured', 0.036), ('calculates', 0.035), ('young', 0.035), ('pf', 0.035), ('pruning', 0.035), ('reconstructing', 0.035), ('train', 0.034), ('partially', 0.034), ('seems', 0.034), ('daily', 0.034), ('hands', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="153-tfidf-1" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>2 0.24611302 <a title="153-tfidf-2" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>3 0.23761067 <a title="153-tfidf-3" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>Author: Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</p><p>Abstract: How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What defines these spatiotemporal patches is their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore, these patches can be used as a discriminative vocabulary for action classification where they demonstrate stateof-the-art performance on UCF50 and Olympics datasets.</p><p>4 0.22831483 <a title="153-tfidf-4" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>Author: Matthias Dantone, Juergen Gall, Christian Leistner, Luc Van_Gool</p><p>Abstract: In this work, we address the problem of estimating 2d human pose from still images. Recent methods that rely on discriminatively trained deformable parts organized in a tree model have shown to be very successful in solving this task. Within such a pictorial structure framework, we address the problem of obtaining good part templates by proposing novel, non-linear joint regressors. In particular, we employ two-layered random forests as joint regressors. The first layer acts as a discriminative, independent body part classifier. The second layer takes the estimated class distributions of the first one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This results in a pose estimation framework that takes dependencies between body parts already for joint localization into account and is thus able to circumvent typical ambiguities of tree structures, such as for legs and arms. In the experiments, we demonstrate that our body parts dependent joint regressors achieve a higher joint localization accuracy than tree-based state-of-the-art methods.</p><p>5 0.22636454 <a title="153-tfidf-5" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>6 0.21410008 <a title="153-tfidf-6" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>7 0.19479358 <a title="153-tfidf-7" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>8 0.18717572 <a title="153-tfidf-8" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>9 0.18647154 <a title="153-tfidf-9" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>10 0.18196906 <a title="153-tfidf-10" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>11 0.18159749 <a title="153-tfidf-11" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>12 0.17537202 <a title="153-tfidf-12" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>13 0.16853687 <a title="153-tfidf-13" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>14 0.1547467 <a title="153-tfidf-14" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>15 0.15296832 <a title="153-tfidf-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.15285683 <a title="153-tfidf-16" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>17 0.1510362 <a title="153-tfidf-17" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>18 0.14932442 <a title="153-tfidf-18" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>19 0.14792281 <a title="153-tfidf-19" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>20 0.14718428 <a title="153-tfidf-20" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.307), (1, -0.177), (2, -0.014), (3, -0.172), (4, -0.066), (5, 0.072), (6, -0.101), (7, 0.143), (8, 0.05), (9, -0.037), (10, -0.146), (11, 0.098), (12, -0.074), (13, -0.028), (14, -0.002), (15, 0.003), (16, 0.031), (17, -0.056), (18, 0.025), (19, 0.108), (20, 0.044), (21, 0.036), (22, 0.094), (23, -0.011), (24, 0.035), (25, 0.037), (26, -0.014), (27, -0.001), (28, 0.001), (29, -0.03), (30, 0.043), (31, -0.023), (32, 0.029), (33, 0.004), (34, 0.038), (35, -0.021), (36, -0.028), (37, -0.013), (38, -0.002), (39, 0.005), (40, 0.06), (41, -0.069), (42, 0.017), (43, 0.006), (44, 0.031), (45, -0.044), (46, -0.022), (47, -0.098), (48, -0.017), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9566161 <a title="153-lsi-1" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>2 0.74361879 <a title="153-lsi-2" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>3 0.73294818 <a title="153-lsi-3" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>Author: Mayank Juneja, Andrea Vedaldi, C.V. Jawahar, Andrew Zisserman</p><p>Abstract: The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previouspart-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of- the-art classification performance on this data.</p><p>4 0.71070457 <a title="153-lsi-4" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>5 0.70573068 <a title="153-lsi-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.6997515 <a title="153-lsi-6" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>7 0.69693631 <a title="153-lsi-7" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>8 0.68666023 <a title="153-lsi-8" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>9 0.67701542 <a title="153-lsi-9" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>10 0.6728133 <a title="153-lsi-10" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>11 0.6488592 <a title="153-lsi-11" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>12 0.64799732 <a title="153-lsi-12" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>13 0.64796543 <a title="153-lsi-13" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>14 0.64679706 <a title="153-lsi-14" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>15 0.63511777 <a title="153-lsi-15" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>16 0.63246989 <a title="153-lsi-16" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>17 0.63185984 <a title="153-lsi-17" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>18 0.61789989 <a title="153-lsi-18" href="./cvpr-2013-Fine-Grained_Crowdsourcing_for_Fine-Grained_Recognition.html">174 cvpr-2013-Fine-Grained Crowdsourcing for Fine-Grained Recognition</a></p>
<p>19 0.61225843 <a title="153-lsi-19" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>20 0.60803157 <a title="153-lsi-20" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.104), (16, 0.023), (26, 0.043), (33, 0.309), (67, 0.078), (69, 0.05), (80, 0.238), (87, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92681015 <a title="153-lda-1" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>2 0.90686882 <a title="153-lda-2" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>3 0.89303625 <a title="153-lda-3" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: In this paper we consider the challenging problem of articulated human pose estimation in still images. We observe that despite high variability of the body articulations, human motions and activities often simultaneously constrain the positions of multiple body parts. Modelling such higher order part dependencies seemingly comes at a cost of more expensive inference, which resulted in their limited use in state-of-the-art methods. In this paper we propose a model that incorporates higher order part dependencies while remaining efficient. We achieve this by defining a conditional model in which all body parts are connected a-priori, but which becomes a tractable tree-structured pictorial structures model once the image observations are available. In order to derive a set of conditioning variables we rely on the poselet-based features that have been shown to be effective for people detection but have so far found limited application for articulated human pose estimation. We demon- strate the effectiveness of our approach on three publicly available pose estimation benchmarks improving or being on-par with state of the art in each case.</p><p>same-paper 4 0.88652033 <a title="153-lda-4" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>5 0.87988853 <a title="153-lda-5" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>Author: Bing Li, Weihua Xiong, Weiming Hu, Houwen Peng</p><p>Abstract: Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods.</p><p>6 0.87422401 <a title="153-lda-6" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>7 0.85377181 <a title="153-lda-7" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>8 0.85155702 <a title="153-lda-8" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>9 0.83733535 <a title="153-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.8345238 <a title="153-lda-10" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>11 0.83446223 <a title="153-lda-11" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>12 0.8331877 <a title="153-lda-12" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>13 0.83284056 <a title="153-lda-13" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>14 0.83152223 <a title="153-lda-14" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>15 0.82907456 <a title="153-lda-15" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>16 0.82786077 <a title="153-lda-16" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>17 0.82664007 <a title="153-lda-17" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>18 0.82626987 <a title="153-lda-18" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>19 0.82395488 <a title="153-lda-19" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>20 0.82356799 <a title="153-lda-20" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
