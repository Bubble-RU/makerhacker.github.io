<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>135 cvpr-2013-Discriminative Subspace Clustering</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-135" href="#">cvpr2013-135</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>135 cvpr-2013-Discriminative Subspace Clustering</h1>
<br/><p>Source: <a title="cvpr-2013-135-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zografos_Discriminative_Subspace_Clustering_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>Reference: <a title="cvpr-2013-135-reference" href="../cvpr2013_reference/cvpr-2013-Discriminative_Subspace_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). [sent-3, score-0.918]
</p><p>2 We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. [sent-4, score-0.646]
</p><p>3 A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. [sent-5, score-0.445]
</p><p>4 We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. [sent-6, score-0.04]
</p><p>5 Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity. [sent-7, score-0.085]
</p><p>6 Since in most natural data the total variance is contained in a small number of principal axes, even if the measured data is high-dimensional, its intrinsic dimensionality is usually much lower. [sent-10, score-0.078]
</p><p>7 Furthermore, it is reasonable to model data which comes from different classes as lying in a union of linear subspaces, rather than in a single subspace. [sent-11, score-0.122]
</p><p>8 Therefore, the problem of high-dimensional data segmentation, simplifies to one of lower-dimensional subspace clustering. [sent-12, score-0.461]
</p><p>9 That is, recovering the appropriate linear subspaces and the membership of each data point to a particu∗zografos@isy. [sent-13, score-0.375]
</p><p>10 For this reason, in the last few years a large number of scientific publications in computer vision and machine learning literature have emerged proposing a wide range of sophisticated solutions to the subspace clustering problem. [sent-21, score-0.987]
</p><p>11 The sparsity information is then used as a point clustering affinity. [sent-23, score-0.356]
</p><p>12 More recent examples are the LRR method by [6], that tries to recover a low-rank representation of the data points, and its improvement LLRR [7], which is able to handle the effects of unobserved (“hidden”) data by solving a convex minimization problem. [sent-24, score-0.201]
</p><p>13 Finally, we have the two algebraic methods SSQP [8] and LSR [9]. [sent-25, score-0.101]
</p><p>14 The former works on the premise that every data point is a regularized linear combination of few other points, and a quadratic programming approach is used to recover such configurations. [sent-26, score-0.302]
</p><p>15 LSR is a fast method which takes advantage of the data sample correlation and groups points that have high correlation together. [sent-27, score-0.199]
</p><p>16 We present a novel method for the solution of the subspace clustering problem, which follows the machine learning principle of Discriminative Clustering, that is, solving an unsupervised learning problem by means of a classifier or put more simply “clustering by classification”. [sent-28, score-0.854]
</p><p>17 Our method is called Discriminative Subspace Clustering (DiSC) and is fundamentally different from the generative, often algebraic methods that one encounters in subspace clustering literature. [sent-29, score-1.016]
</p><p>18 Furthermore, obtaining a generative model can often be difficult in many application domains. [sent-32, score-0.121]
</p><p>19 Conversely in discriminative algorithms, performance can be affected by incomplete and noisy training data. [sent-33, score-0.256]
</p><p>20 However as we will show, this potential problem can be minimized by the information leveraging abilities of ensemble clustering. [sent-34, score-0.283]
</p><p>21 DiSC exploits three very simple observations: First and foremost is that two subspaces in general configurations (i. [sent-35, score-0.361]
</p><p>22 non-coincidental intersection) can be optimally separated by a quadratic classifier; Second is the locality principle. [sent-37, score-0.246]
</p><p>23 Namely that very often a point lies in close proximity to a small number of points from the same subspace; And third, that by combining together the results of multiple, diversely-trained classifiers (ensemble), we can obtain an overall improved result. [sent-38, score-0.23]
</p><p>24 The second provides a set of weak (incomplete and noisy) labels, and the third generates a final, robust result from an ensemble of diverse clusterings. [sent-40, score-0.203]
</p><p>25 Our contribution with this paper is the combination of these three basic observations inside a workable discriminative clustering framework and the production a novel method for the accurate and robust solution of the subspace  clustering problem. [sent-41, score-1.381]
</p><p>26 We have tested our method on a number of real and synthetic datasets and against state-of-theart approaches from literature. [sent-42, score-0.04]
</p><p>27 The experiments show that DiSC compares very favorably against the other methods. [sent-43, score-0.047]
</p><p>28 In addition, it is stable to parameter settings and has low complexity as a function of the number of subspaces, the number of points and the number of ambient dimensions. [sent-44, score-0.132]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disc', 0.41), ('subspace', 0.41), ('clustering', 0.323), ('subspaces', 0.258), ('zografos', 0.207), ('lsr', 0.183), ('mester', 0.183), ('ensemble', 0.141), ('ping', 0.133), ('generative', 0.121), ('algebraic', 0.101), ('incomplete', 0.093), ('locality', 0.093), ('discriminative', 0.092), ('scc', 0.092), ('encounters', 0.092), ('rudolf', 0.092), ('workable', 0.092), ('performer', 0.085), ('link', 0.082), ('lrr', 0.08), ('premise', 0.08), ('union', 0.076), ('sweden', 0.073), ('abilities', 0.073), ('simplex', 0.073), ('quadratic', 0.071), ('ellis', 0.071), ('publications', 0.071), ('points', 0.069), ('foremost', 0.068), ('ssc', 0.068), ('unobserved', 0.066), ('ambient', 0.063), ('lar', 0.06), ('handwritten', 0.059), ('production', 0.059), ('emerged', 0.059), ('restrictive', 0.055), ('character', 0.055), ('fundamentally', 0.055), ('membership', 0.054), ('neighborhood', 0.054), ('germany', 0.052), ('conversely', 0.051), ('simplifies', 0.051), ('recover', 0.05), ('correlation', 0.05), ('separates', 0.049), ('proposing', 0.047), ('cosine', 0.047), ('dimensionality', 0.047), ('favorably', 0.047), ('classifier', 0.046), ('lying', 0.046), ('curvature', 0.046), ('proximity', 0.046), ('tries', 0.045), ('notable', 0.044), ('observations', 0.044), ('together', 0.044), ('axes', 0.044), ('scientific', 0.043), ('optimally', 0.043), ('compression', 0.043), ('looks', 0.043), ('electrical', 0.041), ('partitions', 0.04), ('tested', 0.04), ('solving', 0.04), ('noisy', 0.04), ('author', 0.039), ('separated', 0.039), ('suited', 0.039), ('classifiers', 0.038), ('basic', 0.038), ('affinity', 0.036), ('leveraging', 0.036), ('thereby', 0.036), ('regularized', 0.035), ('sc', 0.035), ('called', 0.035), ('exploits', 0.035), ('unlabeled', 0.035), ('put', 0.035), ('lab', 0.034), ('sophisticated', 0.034), ('segmentation', 0.034), ('solves', 0.033), ('point', 0.033), ('minimized', 0.033), ('formed', 0.033), ('former', 0.033), ('drawn', 0.031), ('generates', 0.031), ('affected', 0.031), ('contained', 0.031), ('diverse', 0.031), ('recovering', 0.03), ('advantage', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="135-tfidf-1" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>2 0.25692981 <a title="135-tfidf-2" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>Author: Shaokang Chen, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell</p><p>Abstract: Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once, with fixed clusters used for matching with other image sets. However, this may result in the two closest clusters to represent different characteristics of an object, due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem, we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace, we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error, we approach the nearest point on a Grassmann manifold. Experiments on Honda, ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques, such as Affine Hull based Image Set Distance (AHISD), Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).</p><p>3 0.25666735 <a title="135-tfidf-3" href="./cvpr-2013-Scalable_Sparse_Subspace_Clustering.html">379 cvpr-2013-Scalable Sparse Subspace Clustering</a></p>
<p>Author: Xi Peng, Lei Zhang, Zhang Yi</p><p>Abstract: In this paper, we address two problems in Sparse Subspace Clustering algorithm (SSC), i.e., scalability issue and out-of-sample problem. SSC constructs a sparse similarity graph for spectral clustering by using ?1-minimization based coefficients, has achieved state-of-the-art results for image clustering and motion segmentation. However, the time complexity of SSC is proportion to the cubic of problem size such that it is inefficient to apply SSC into large scale setting. Moreover, SSC does not handle with out-ofsample data that are not used to construct the similarity graph. For each new datum, SSC needs recalculating the cluster membership of the whole data set, which makes SSC is not competitive in fast online clustering. To address the problems, this paper proposes out-of-sample extension of SSC, named as Scalable Sparse Subspace Clustering (SSSC), which makes SSC feasible to cluster large scale data sets. The solution of SSSC adopts a ”sampling, clustering, coding, and classifying” strategy. Extensive experimental results on several popular data sets demonstrate the effectiveness and efficiency of our method comparing with the state-of-the-art algorithms.</p><p>4 0.23116939 <a title="135-tfidf-4" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>Author: Bo Wang, Zhuowen Tu</p><p>Abstract: With the increasing availability of high dimensional data and demand in sophisticated data analysis algorithms, manifold learning becomes a critical technique to perform dimensionality reduction, unraveling the intrinsic data structure. The real-world data however often come with noises and outliers; seldom, all the data live in a single linear subspace. Inspired by the recent advances in sparse subspace learning and diffusion-based approaches, we propose a new manifold denoising algorithm in which data neighborhoods are adaptively inferred via sparse subspace reconstruction; we then derive a new formulation to perform denoising to the original data. Experiments carried out on both toy and real applications demonstrate the effectiveness of our method; it is insensitive to parameter tuning and we show significant improvement over the competing algorithms.</p><p>5 0.20785192 <a title="135-tfidf-5" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: Estimating geographic location from images is a challenging problem that is receiving recent attention. In contrast to many existing methods that primarily model discriminative information corresponding to different locations, we propose joint learning of information that images across locations share and vary upon. Starting with generative and discriminative subspaces pertaining to domains, which are obtained by a hierarchical grouping of images from adjacent locations, we present a top-down approach that first models cross-domain information transfer by utilizing the geometry ofthese subspaces, and then encodes the model results onto individual images to infer their location. We report competitive results for location recognition and clustering on two public datasets, im2GPS and San Francisco, and empirically validate the utility of various design choices involved in the approach.</p><p>6 0.17606746 <a title="135-tfidf-6" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>7 0.17472953 <a title="135-tfidf-7" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>8 0.14994742 <a title="135-tfidf-8" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>9 0.14653353 <a title="135-tfidf-9" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>10 0.11749488 <a title="135-tfidf-10" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>11 0.11517198 <a title="135-tfidf-11" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>12 0.11179166 <a title="135-tfidf-12" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>13 0.10760769 <a title="135-tfidf-13" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>14 0.093662716 <a title="135-tfidf-14" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>15 0.082672775 <a title="135-tfidf-15" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>16 0.080899708 <a title="135-tfidf-16" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>17 0.080196179 <a title="135-tfidf-17" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>18 0.079060003 <a title="135-tfidf-18" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>19 0.073653616 <a title="135-tfidf-19" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>20 0.065758184 <a title="135-tfidf-20" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, -0.016), (2, -0.063), (3, 0.053), (4, 0.039), (5, -0.018), (6, -0.04), (7, -0.119), (8, -0.033), (9, -0.106), (10, 0.039), (11, -0.041), (12, -0.111), (13, -0.123), (14, -0.067), (15, 0.054), (16, -0.113), (17, -0.063), (18, -0.165), (19, -0.039), (20, 0.195), (21, -0.011), (22, 0.063), (23, -0.035), (24, -0.008), (25, -0.131), (26, 0.026), (27, -0.197), (28, 0.101), (29, -0.039), (30, 0.011), (31, 0.267), (32, 0.12), (33, -0.098), (34, -0.031), (35, 0.092), (36, -0.055), (37, -0.044), (38, -0.092), (39, 0.034), (40, -0.09), (41, 0.016), (42, -0.09), (43, 0.052), (44, 0.02), (45, 0.019), (46, 0.001), (47, -0.066), (48, -0.017), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.974015 <a title="135-lsi-1" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>2 0.86364681 <a title="135-lsi-2" href="./cvpr-2013-Scalable_Sparse_Subspace_Clustering.html">379 cvpr-2013-Scalable Sparse Subspace Clustering</a></p>
<p>Author: Xi Peng, Lei Zhang, Zhang Yi</p><p>Abstract: In this paper, we address two problems in Sparse Subspace Clustering algorithm (SSC), i.e., scalability issue and out-of-sample problem. SSC constructs a sparse similarity graph for spectral clustering by using ?1-minimization based coefficients, has achieved state-of-the-art results for image clustering and motion segmentation. However, the time complexity of SSC is proportion to the cubic of problem size such that it is inefficient to apply SSC into large scale setting. Moreover, SSC does not handle with out-ofsample data that are not used to construct the similarity graph. For each new datum, SSC needs recalculating the cluster membership of the whole data set, which makes SSC is not competitive in fast online clustering. To address the problems, this paper proposes out-of-sample extension of SSC, named as Scalable Sparse Subspace Clustering (SSSC), which makes SSC feasible to cluster large scale data sets. The solution of SSSC adopts a ”sampling, clustering, coding, and classifying” strategy. Extensive experimental results on several popular data sets demonstrate the effectiveness and efficiency of our method comparing with the state-of-the-art algorithms.</p><p>3 0.81134439 <a title="135-lsi-3" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>Author: Shaokang Chen, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell</p><p>Abstract: Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once, with fixed clusters used for matching with other image sets. However, this may result in the two closest clusters to represent different characteristics of an object, due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem, we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace, we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error, we approach the nearest point on a Grassmann manifold. Experiments on Honda, ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques, such as Affine Hull based Image Set Distance (AHISD), Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).</p><p>4 0.75512582 <a title="135-lsi-4" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>Author: Bo Wang, Zhuowen Tu</p><p>Abstract: With the increasing availability of high dimensional data and demand in sophisticated data analysis algorithms, manifold learning becomes a critical technique to perform dimensionality reduction, unraveling the intrinsic data structure. The real-world data however often come with noises and outliers; seldom, all the data live in a single linear subspace. Inspired by the recent advances in sparse subspace learning and diffusion-based approaches, we propose a new manifold denoising algorithm in which data neighborhoods are adaptively inferred via sparse subspace reconstruction; we then derive a new formulation to perform denoising to the original data. Experiments carried out on both toy and real applications demonstrate the effectiveness of our method; it is insensitive to parameter tuning and we show significant improvement over the competing algorithms.</p><p>5 0.71316892 <a title="135-lsi-5" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>Author: Raffay Hamid, Dennis Decoste, Chih-Jen Lin</p><p>Abstract: We present a robust and efficient technique for matching dense sets of points undergoing non-rigid spatial transformations. Our main intuition is that the subset of points that can be matched with high confidence should be used to guide the matching procedure for the rest. We propose a novel algorithm that incorporates these high-confidence matches as a spatial prior to learn a discriminative subspace that simultaneously encodes both the feature similarity as well as their spatial arrangement. Conventional subspace learning usually requires spectral decomposition of the pair-wise distance matrix across the point-sets, which can become inefficient even for moderately sized problems. To this end, we propose the use of random projections for approximate subspace learning, which can provide significant time improvements at the cost of minimal precision loss. This efficiency gain allows us to iteratively find and remove high-confidence matches from the point sets, resulting in high recall. To show the effectiveness of our approach, we present a systematic set of experiments and results for the problem of dense non-rigid image-feature matching.</p><p>6 0.69916147 <a title="135-lsi-6" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>7 0.64733696 <a title="135-lsi-7" href="./cvpr-2013-Graph-Laplacian_PCA%3A_Closed-Form_Solution_and_Robustness.html">191 cvpr-2013-Graph-Laplacian PCA: Closed-Form Solution and Robustness</a></p>
<p>8 0.62936777 <a title="135-lsi-8" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>9 0.59534681 <a title="135-lsi-9" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>10 0.56492108 <a title="135-lsi-10" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>11 0.45630282 <a title="135-lsi-11" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>12 0.442435 <a title="135-lsi-12" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>13 0.40633637 <a title="135-lsi-13" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>14 0.37967458 <a title="135-lsi-14" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>15 0.37857157 <a title="135-lsi-15" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<p>16 0.37263182 <a title="135-lsi-16" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>17 0.36962736 <a title="135-lsi-17" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>18 0.35998449 <a title="135-lsi-18" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>19 0.34704304 <a title="135-lsi-19" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>20 0.33100867 <a title="135-lsi-20" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.069), (26, 0.014), (33, 0.231), (67, 0.021), (69, 0.544), (87, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82115382 <a title="135-lda-1" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>Author: Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: 3D volumetric reasoning is important for truly understanding a scene. Humans are able to both segment each object in an image, and perceive a rich 3D interpretation of the scene, e.g., the space an object occupies, which objects support other objects, and which objects would, if moved, cause other objects to fall. We propose a new approach for parsing RGB-D images using 3D block units for volumetric reasoning. The algorithm fits image segments with 3D blocks, and iteratively evaluates the scene based on block interaction properties. We produce a 3D representation of the scene based on jointly optimizing over segmentations, block fitting, supporting relations, and object stability. Our algorithm incorporates the intuition that a good 3D representation of the scene is the one that fits the data well, and is a stable, self-supporting (i.e., one that does not topple) arrangement of objects. We experiment on several datasets including controlled and real indoor scenarios. Results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation.</p><p>same-paper 2 0.79178429 <a title="135-lda-2" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>3 0.78728801 <a title="135-lda-3" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>4 0.78589439 <a title="135-lda-4" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>5 0.75900459 <a title="135-lda-5" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>6 0.74991584 <a title="135-lda-6" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>7 0.74104899 <a title="135-lda-7" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>8 0.69766957 <a title="135-lda-8" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>9 0.63819402 <a title="135-lda-9" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>10 0.59190875 <a title="135-lda-10" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>11 0.57053429 <a title="135-lda-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.56047696 <a title="135-lda-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.5588119 <a title="135-lda-13" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>14 0.55879927 <a title="135-lda-14" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>15 0.55338383 <a title="135-lda-15" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>16 0.55069691 <a title="135-lda-16" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>17 0.55065632 <a title="135-lda-17" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>18 0.55056673 <a title="135-lda-18" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>19 0.54767585 <a title="135-lda-19" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>20 0.54453671 <a title="135-lda-20" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
