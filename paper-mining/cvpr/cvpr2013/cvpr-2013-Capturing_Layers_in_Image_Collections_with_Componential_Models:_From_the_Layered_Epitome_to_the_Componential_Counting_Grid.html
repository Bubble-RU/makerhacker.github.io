<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-78" href="#">cvpr2013-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</h1>
<br/><p>Source: <a title="cvpr-2013-78-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Perina_Capturing_Layers_in_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>Reference: <a title="cvpr-2013-78-reference" href="../cvpr2013_reference/cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Capturing layers in image collections with componential models: from the layered epitome to the componential counting grid  Alessandro Perina Microsoft Research, Redmond, USA alperina @mi cro s o ft . [sent-1, score-1.731]
</p><p>2 com Abstract Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. [sent-2, score-0.222]
</p><p>3 This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. [sent-3, score-0.491]
</p><p>4 Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. [sent-4, score-0.71]
</p><p>5 In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. [sent-5, score-0.55]
</p><p>6 In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. [sent-6, score-0.162]
</p><p>7 We tested the models on scene and place classification where their com-  ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras. [sent-7, score-0.187]
</p><p>8 Introduction The most basic Counting Grid (CG) model [5] represents each input image as a point k in a large grid of feature (SIFT, color, high level feature) counts. [sent-9, score-0.247]
</p><p>9 This latent point is a corner of a window of grid points which are all uniformly combined to form feature counts that match the (normalized) feature counts in the image. [sent-10, score-0.57]
</p><p>10 Thus, the CG model strikes an unusual compromise between modeling the spatial layout of features and simply representing image features as a bag of words where feature layout is completely sacrificed. [sent-11, score-0.421]
</p><p>11 The spatial layout is indeed forgone in the representation of any single image, as the model is simply concerned with modeling the feature histogram. [sent-12, score-0.154]
</p><p>12 However the spatial layout is present in the counting grid itself, which, by being trained on a large number of individual image histograms, recovers some spatial layout characteristics of the Nebojsa Jojic Microsoft Research, Redmond, USA j o j i @mi cro s o ft . [sent-13, score-0.759]
</p><p>13 a) Images from 4 classes of the SenseCam dataset [6] (Office, Atrium, Corridor, Lounge) b-c) Visualization of the top words in each counting grid location. [sent-15, score-0.529]
</p><p>14 Componential Counting Grids map each image in multiple locations, in this figure we only show a window in correspondence  of the most likely location. [sent-18, score-0.146]
</p><p>15 For example, in a collection of images of a scene taken by a camera with a field of view that is insufficient to cover the entire scene, each image will capture different scene parts. [sent-20, score-0.082]
</p><p>16 Ignoring the spatial layout in the image frees the model from having to align individual image locations, allowing for geometric defor445909880  Table 1. [sent-23, score-0.132]
</p><p>17 N> 1x W 1× × 1N 1 yN>1 x1×S× × 1 N1y mations, while the grid itself reconstructs some of the 2D spatial layout needed for modeling feature count correlations. [sent-28, score-0.354]
</p><p>18 1, arranging counts on a topology that allows feature sharing through windowing can have representational advantages beyond this surprising possibility of panoramic scene reconstruc-  tion from bags of features. [sent-30, score-0.228]
</p><p>19 In this paper we introduce the Componential Counting Grids (CCG), a family of models which extend the basic Counting Grid model so that each input image is represented by multiple latent locations in CG, rather than just one. [sent-32, score-0.152]
</p><p>20 Through admixing locations, CCG models become multi -part or -object models but, like their CG predecessor, they recreate only as much of spatial layout in the counting grid as necessary for capturing count correlations. [sent-33, score-0.715]
</p><p>21 In each of these corners, substantial research effort has been invested to refine and apply these basic approaches, but it turns out that the CCG models at neither end of the spectrum tend to perform best in our experiments. [sent-36, score-0.091]
</p><p>22 Componential Counting Grids and Topic models [7] The original counting grid model shares its focus on modeling image feature counts (rather than feature layouts) with another category of generative models, the “topic models”,  such as latent Dirichlet allocation (LDA) [7, 1]. [sent-39, score-0.764]
</p><p>23 The CG model is essentially a mixture model, assuming only one source for all features in the bag, while the LDA model is an admixture model that allows mixing of multiple topics to explain a single bag. [sent-41, score-0.187]
</p><p>24 By using large windows to collate many grid distributions from a large grid, CG model can be a very large mixture of sources without overtraining, as these sources are highly correlated: Small shifts in the grid change the window distribution only slightly. [sent-42, score-0.704]
</p><p>25 LDA model does not have this benefit, and thus has to deal with a smaller number of topics to avoid overtraining. [sent-43, score-0.116]
</p><p>26 Topic mixing cannot quite appropriately represent feature correlations due to translational camera motion. [sent-44, score-0.099]
</p><p>27 But, the equivalent of LDA topics are windows in a counting grid, which allows the model to have a very large number of topics that are highly related, as shift in the grid only slightly refines any topic. [sent-46, score-0.795]
</p><p>28 [17] worked on sources positioned in a plane at real-valued locations, with the idea that sources within a radius would be combined to produce topics in an LDA-like model. [sent-49, score-0.188]
</p><p>29 They used an expensive sampling algorithm that aimed at moving the sources in the plane and determining the circular window size. [sent-50, score-0.182]
</p><p>30 The grid  ××  placement of sources of CCG yields much more efficient algorithms and denser packing. [sent-51, score-0.236]
</p><p>31 In addition, as illustrated below, CCG model can be run with various tessellations efficiently making it especially useful in vision applications. [sent-52, score-0.12]
</p><p>32 In computer vision, instead of forming a single bag of words out of one image, separate bags are typically extracted from a uniform S = Sx Sy rectangular tessellation of the image [6, 8, 10]. [sent-54, score-0.442]
</p><p>33 All sections are mapped to the same grid, but, the corresponding window is tessellated in the same way as the image, and the feature histograms from corresponding rectangular segments are supposed to match. [sent-56, score-0.247]
</p><p>34 Even with as coarse tessellations as 2 2, training CG on image patches can result sine panoramic r×ec2o,n tsratriuncitniogn C sGim oinla irm taog teh pata cofh ethse c epitome model which entirely preserves the spatial layout. [sent-57, score-0.351]
</p><p>35 1 When the Tessellation is equal to the image size S = Nx Ny, every bag is composed by a single feature, and we o×btNain the Layered Epitome. [sent-58, score-0.127]
</p><p>36 Like regular Epitomes [2] and flexible sprites [18] preserves the spatial layout of features. [sent-59, score-0.275]
</p><p>37 However, differently from [2], tCCGs break each image into layers and maps them separately in the epitome space, and differently from [18], it does not assume a pre-defined number and an ordering between layers. [sent-60, score-0.178]
</p><p>38 2, though, we show a variety of Componential  models one can obtain by varying the tessellation and the window size for the mapping. [sent-62, score-0.41]
</p><p>39 The window size need not,  and usually in our experiments  does not match the size of  the input image, except for the . [sent-63, score-0.146]
</p><p>40 2  vantages for this illustration,  Due to visualization ad-  all models were trained us-  ××  ing discretized colors rather than SIFT features, and they all have roughly the same capacity – the number of independent topics that can be created in the allotted space without overlapping the windows. [sent-69, score-0.219]
</p><p>41 This means that counting grids created with smaller windows have to be proportionally smaller, but for better visualization we enlarged all grids to the same size. [sent-70, score-0.773]
</p><p>42 Window overlaps create smooth interpolations among topics that compensate for camera motion. [sent-71, score-0.136]
</p><p>43 When 1 1 windows are used, there is no sharing toiof grid dhiestnri 1bu ×tion 1s w among topics, aednd, thheer em iosd neol r sehdauricnegs to LDA shown in the corner with its histograms for its topics. [sent-72, score-0.289]
</p><p>44 As there is no sharing, the spatial arrangement of four topics onto the 2 2 grid has no meaning or value. [sent-73, score-0.342]
</p><p>45 ered epitomes or flexible sprites are another extreme where both the window size and the tessellation match the resolution of input images3, but the CCG models with as coarse a tessellation as 8 8 already look indistinguishable from epitome/flexible sprite arelsruelatds. [sent-75, score-0.956]
</p><p>46 While LDA color model will obviously confuse the white elements of the background with these foreground objects, the model with full tessellation has to learn multiple versions ofeach person to capture the scale changes due to their motion at an angle with the motion of the camera. [sent-78, score-0.237]
</p><p>47 The inter-  ×  mediate tessellations and window size provide more interesting tradeoffs. [sent-79, score-0.266]
</p><p>48 When the model is forced to simplify further, through appropriate choice of window and tessellation size, the two persons dressed in white are generalized into a single object (though it may occur twice in one image). [sent-81, score-0.432]
</p><p>49 While this illustration reinforces the naturally good fit of CCG models to images of scenes with multiple moving objects taken by a camera with a moving field of view, the applicability ofthe CCG models hardly stops there. [sent-82, score-0.092]
</p><p>50 1illustrates the value of computing a grid of features in a very different context, where one large grid is computed from all images from 4 of the 32 class wearable camera dataset [6]. [sent-84, score-0.475]
</p><p>51 Each image was represented by a single bag of features (1 1tessellation) and the counting grid is computed using (318 15 t0e wssienlldaotwions. [sent-85, score-0.622]
</p><p>52 The model tends to break up each bag into more topics, and instead of reflecting a panoramic reconstruction, the grid now models smaller scene parts, such as vertical  and horizontal edges found in windows and building walls that the subject sees in his office and elsewhere. [sent-88, score-0.552]
</p><p>53 The choice of edges placed close together shows that the model makes sure that a window into the grid captures an appropriate feature mix found in some of the images in the training set. [sent-89, score-0.389]
</p><p>54 In multiple places in the grid we see that when the window is moved the orientation of the edges changes slightly and in concert. [sent-90, score-0.346]
</p><p>55 Thus, in this case the CG real-estate and window overlapping strategy was often used to model rotation, rather translation. [sent-91, score-0.17]
</p><p>56 From  Counting  Grids  to Componential  Models The basic 2-D Counting Grid πi,z is a set of normalized counts of words/features indexed by z on the 2-dimensional discrete grid indexed by i = (ix , iy) where each id ∈ [1. [sent-97, score-0.282]
</p><p>57 Counting Grids assume that each bags follow a feature distribution found somewhere in the counting grid; In particular, using windows of dimensions W = (Wx , Wy), a bag can be generated by first averaging all counts in the window  Figure 3. [sent-107, score-0.78]
</p><p>58 Wi starting at 2-dimensional grid location iand extending in each direction d by W? [sent-112, score-0.234]
</p><p>59 In other words, the position of the window iin the grid is a latent variable given which we can write the probability of the bag as  p({w}|i) =? [sent-118, score-0.518]
</p><p>60 Relaxing the terminology, E and W are referred to as, respectively, the counting grid and the window size. [sent-126, score-0.641]
</p><p>61 The ratio of the two volumes, κ, is called the capacity of the model in terms of an equivalent number of topics, as this is how many non-overlapping windows can be fit onto the grid. [sent-127, score-0.12]
</p><p>62 Finally, with Wi we indicate the particular window placed at location i. [sent-128, score-0.201]
</p><p>63 Componential Counting Grids As seen in the previous section, Counting Grids generate words from a feature distribution in a window W, placed at location i in the grid. [sent-129, score-0.257]
</p><p>64 As we move the window on the grid, some new features appear while others are dropped. [sent-131, score-0.146]
</p><p>65 On the other hand in standard componential models, [7], each feature can be generated 555000113  by a different “process” or “topic. [sent-136, score-0.497]
</p><p>66 , sands often comes with sea), and by breaking the bag into topics can potentially segment the image into parts. [sent-139, score-0.243]
</p><p>67 In the CCG generative model each bag is generated by mixing several windows in the grid following the location distribution θ. [sent-143, score-0.516]
</p><p>68 More precisely, each word wn can be generated from a different window, placed at location ln, but the choice of the window follows the same prior distributions θl for all words. [sent-144, score-0.367]
</p><p>69 Within the window at location ln the word comes from a particular grid location kn, and from that grid  distribution the word is assumed to have been generated. [sent-145, score-0.856]
</p><p>70 ,kn  where p(wn = z|kn, π) = πkn (z) is a multinomial over the word indices,= = p z(k|kn|ln) = UkWn−ln is a distribution over the Counting Grid, equal to (Wx·1Wy) in the upper left window of size W and 0 elsewhere (see Fig. [sent-156, score-0.215]
</p><p>71 By introducing the posterior distributions q, and approximating the true posterior as qt (k, l,θ) = qt (θ) · ? [sent-162, score-0.358]
</p><p>72 n  = k)[wn=z]  (4)  (5)  where [wn = z] is an indicator function, equal to 1 when wn is equal to z. [sent-191, score-0.097]
</p><p>73 Tessellated Componential Counting Grids  The proce-  ××  dure described in the previous section does not require information about the spatial layout of features in the bag and can be in principle applied to any kind of data. [sent-194, score-0.259]
</p><p>74 When inferring the mapping of each “section” bag, the window Wk is tessellated into section WkS in the same way images are tessellated and the histogram comparisons are done accordingly. [sent-196, score-0.304]
</p><p>75 Moreover UkWn−ln becomes where Ws is a window of the same lsize and it is sho−wln in Fig. [sent-197, score-0.146]
</p><p>76 In a similar way nonuniform and most descriptive image layout patterns can be used [20]. [sent-199, score-0.106]
</p><p>77 UkWn−sln,  Layered Epitomes In the limit, when S = Nx Ny each bag contains a single feature and the model be×coNmes the Layered (or componential) Epitome. [sent-200, score-0.149]
</p><p>78 , wn = zi) and Ui highlights now the single pixel i. [sent-203, score-0.097]
</p><p>79 In both the layered epitome ean [d·] te iss tsheella itneddi case fθu anncdti π are updated as aiyne Eq. [sent-213, score-0.264]
</p><p>80 555000224  Latent Dirichlet Allocation Layered Epitomes / Componential Counting Grids Counting Grids  Grid Size large grids  small grids  ×  Figure 4. [sent-216, score-0.41]
</p><p>81 We considered squared grids of various complexities E = [2, 3, . [sent-225, score-0.205]
</p><p>82 We tried single bag models (1 1tessellation), tessellated models 2 2, 4 4 and the layered epitome (nN),x t ×s Ny). [sent-236, score-0.524]
</p><p>83 The capacity κ is roughly equivalent to the number of LDA topics as it represents the number of independent windows that can be fit in the grid; we compared the results using this parallelism [4, 6]. [sent-241, score-0.236]
</p><p>84 485]% ture, by laying out the features into a 2D window and stitch-  ×  ing overlapping windows. [sent-252, score-0.17]
</p><p>85 This fits both the panoramic and componential qualities of the data acquired by a wearable camera. [sent-253, score-0.604]
</p><p>86 Moderate tessellations (up to 4 4) significantly helped, except aftoer very slamtioalnl grid/window 4si)z essig, nwifhicearne tlhye h melpoedde,l reduces itself to a very low resolution layered epitome, or for high κs, where it probably overtrains. [sent-254, score-0.253]
</p><p>87 Layered epitomes did not perform well (≤ 40%) as the training data is limited adindd images are t wooe ldli v(≤ers 4e0 %for) panoramic stitching. [sent-255, score-0.24]
</p><p>88 The trick was to use low resolution epitome with each low res image location represented by a histogram of features. [sent-261, score-0.165]
</p><p>89 We also reported the result of the layered epitome. [sent-274, score-0.133]
</p><p>90 The variation in spatial layout of the objects here was sufficient to render tessellations beyond 1 1unnecessary: They ideno tn toot improve cslelaslastiifoicnasti boeny ornedsul 1ts × (b 1u ut ninnecrceeasssea yin: the window size is needed). [sent-301, score-0.398]
</p><p>91 Discussion  The componential models introduced here can be seen as a generalization of both LDA and template-based models  such as flexible sprites [18] or epitomes [3, 2]. [sent-303, score-0.838]
</p><p>92 As opposed to the basic CG model, it allows for source (object, part) admixing in a single bag of words. [sent-304, score-0.186]
</p><p>93 Keeping the capacity  κ  fixed, the increase in window size  ×  incurs the proportional increase in the computational cost, but provides for smoother reconstruction in the spatial layout. [sent-308, score-0.224]
</p><p>94 The tessellation S guides the rough positioning of the features from different image quadrants and moderate tessellations never hurt. [sent-310, score-0.379]
</p><p>95 In our experiments we invariably find that the basic LDA and epitome-like models, which are at opposite corners of the model organization by tessellation and window size, underperform the CCG models from somewhere in the middle of the triangle illustrated on the toy data in Fig. [sent-311, score-0.476]
</p><p>96 Componential models (CCG, LDA) break the image and perform well, while CGs fails as they classify the scene in which the event take place. [sent-317, score-0.128]
</p><p>97 ,Se hnursteC thaem r images ahendy Torralba sequences are collected with a wearable camera and in principle the spatial layout can be at least piecewise reconstructed. [sent-319, score-0.225]
</p><p>98 Here all methods perform well and the tessellations significantly helped. [sent-320, score-0.12]
</p><p>99 Torralba sequences was the only dataset where layered epitomes were found  ×  to perform well. [sent-321, score-0.317]
</p><p>100 Jojic Image Analysis by Counting on a grid CVPR 2011 [5] N. [sent-343, score-0.2]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('componential', 0.475), ('ccg', 0.323), ('counting', 0.295), ('tessellation', 0.237), ('cg', 0.223), ('grids', 0.205), ('grid', 0.2), ('qt', 0.17), ('epitomes', 0.166), ('window', 0.146), ('ln', 0.14), ('layered', 0.133), ('epitome', 0.131), ('bag', 0.127), ('lda', 0.123), ('tessellations', 0.12), ('kn', 0.117), ('topics', 0.116), ('layout', 0.106), ('sprites', 0.105), ('wn', 0.097), ('ukwn', 0.085), ('tessellated', 0.079), ('cgs', 0.075), ('panoramic', 0.074), ('windows', 0.068), ('ccgs', 0.068), ('sensecam', 0.068), ('counts', 0.057), ('wearable', 0.055), ('capacity', 0.052), ('word', 0.051), ('generative', 0.05), ('allocation', 0.046), ('latent', 0.045), ('bags', 0.044), ('dirichlet', 0.044), ('ki', 0.043), ('epitomic', 0.039), ('spectrum', 0.039), ('flexible', 0.038), ('mixing', 0.037), ('sources', 0.036), ('helped', 0.035), ('cr', 0.034), ('words', 0.034), ('admixing', 0.034), ('admixture', 0.034), ('logukwn', 0.034), ('wln', 0.034), ('location', 0.034), ('scene', 0.031), ('dressed', 0.03), ('family', 0.03), ('ba', 0.029), ('sports', 0.028), ('topic', 0.027), ('models', 0.027), ('spatial', 0.026), ('bo', 0.025), ('break', 0.025), ('locations', 0.025), ('basic', 0.025), ('spring', 0.024), ('overlapping', 0.024), ('frey', 0.023), ('event', 0.023), ('accuracies', 0.023), ('moderate', 0.022), ('nx', 0.022), ('sb', 0.022), ('textual', 0.022), ('redmond', 0.022), ('po', 0.022), ('feature', 0.022), ('classify', 0.022), ('layers', 0.022), ('rc', 0.021), ('place', 0.021), ('somewhere', 0.021), ('placed', 0.021), ('corner', 0.021), ('ny', 0.021), ('camera', 0.02), ('sea', 0.02), ('correlations', 0.02), ('corners', 0.02), ('sx', 0.019), ('wx', 0.019), ('forced', 0.019), ('wi', 0.019), ('uiuc', 0.019), ('sa', 0.018), ('illustration', 0.018), ('multinomial', 0.018), ('distributions', 0.018), ('sequences', 0.018), ('torralba', 0.018), ('lattice', 0.018), ('sy', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="78-tfidf-1" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>2 0.12314557 <a title="78-tfidf-2" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>Author: Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah</p><p>Abstract: We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark con- trast to datasets usedfor existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.</p><p>3 0.11543737 <a title="78-tfidf-3" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>4 0.091241129 <a title="78-tfidf-4" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>5 0.082240917 <a title="78-tfidf-5" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>Author: Yun Jiang, Hema Koppula, Ashutosh Saxena</p><p>Abstract: For scene understanding, one popular approach has been to model the object-object relationships. In this paper, we hypothesize that such relationships are only an artifact of certain hidden factors, such as humans. For example, the objects, monitor and keyboard, are strongly spatially correlated only because a human types on the keyboard while watching the monitor. Our goal is to learn this hidden human context (i.e., the human-object relationships), and also use it as a cue for labeling the scenes. We present Infinite Factored Topic Model (IFTM), where we consider a scene as being generated from two types of topics: human configurations and human-object relationships. This enables our algorithm to hallucinate the possible configurations of the humans in the scene parsimoniously. Given only a dataset of scenes containing objects but not humans, we show that our algorithm can recover the human object relationships. We then test our algorithm on the task ofattribute and object labeling in 3D scenes and show consistent improvements over the state-of-the-art.</p><p>6 0.081799977 <a title="78-tfidf-6" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>7 0.080624998 <a title="78-tfidf-7" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>8 0.079020321 <a title="78-tfidf-8" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>9 0.07883101 <a title="78-tfidf-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.078763746 <a title="78-tfidf-10" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>11 0.071239129 <a title="78-tfidf-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.06638626 <a title="78-tfidf-12" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>13 0.062490299 <a title="78-tfidf-13" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>14 0.058628928 <a title="78-tfidf-14" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>15 0.054397989 <a title="78-tfidf-15" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>16 0.054381963 <a title="78-tfidf-16" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>17 0.052201297 <a title="78-tfidf-17" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>18 0.051911034 <a title="78-tfidf-18" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>19 0.051686138 <a title="78-tfidf-19" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>20 0.049921483 <a title="78-tfidf-20" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.008), (2, 0.004), (3, -0.022), (4, 0.016), (5, 0.02), (6, 0.001), (7, 0.019), (8, -0.013), (9, -0.001), (10, -0.01), (11, 0.001), (12, 0.035), (13, -0.036), (14, 0.034), (15, -0.006), (16, 0.011), (17, 0.052), (18, 0.033), (19, -0.065), (20, 0.038), (21, 0.042), (22, 0.016), (23, -0.054), (24, -0.012), (25, -0.024), (26, 0.011), (27, -0.038), (28, -0.002), (29, -0.072), (30, -0.008), (31, 0.024), (32, 0.01), (33, 0.112), (34, 0.028), (35, -0.072), (36, 0.005), (37, 0.016), (38, 0.042), (39, -0.058), (40, -0.036), (41, 0.017), (42, -0.156), (43, 0.016), (44, -0.039), (45, -0.008), (46, 0.01), (47, -0.032), (48, -0.016), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90462458 <a title="78-lsi-1" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>2 0.74145627 <a title="78-lsi-2" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>Author: Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah</p><p>Abstract: We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark con- trast to datasets usedfor existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.</p><p>3 0.73609197 <a title="78-lsi-3" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>4 0.71643406 <a title="78-lsi-4" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>5 0.60782599 <a title="78-lsi-5" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>Author: Carlos Arteta, Victor Lempitsky, J. Alison Noble, Andrew Zisserman</p><p>Abstract: The objective of this work is to detect all instances of a class (such as cells or people) in an image. The instances may be partially overlapping and clustered, and hence quite challenging for traditional detectors, which aim at localizing individual instances. Our approach is to propose a set of candidate regions, and then select regions based on optimizing a global classification score, subject to the constraint that the selected regions are non-overlapping. Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. For example, our detector can pick a region containing two or three object instances, while assigning such region an appropriate label. We show that this formulation can be learned within the structured output SVM framework, and that the inference in such model can be accomplished using dynamic programming on a tree structured region graph. Furthermore, the learning only requires weak annotations – a dot on each instance. The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on quite disparate data sets: fluorescence microscopy images and UCSD pedestrians.</p><p>6 0.56006664 <a title="78-lsi-6" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>7 0.55598521 <a title="78-lsi-7" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>8 0.53929996 <a title="78-lsi-8" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>9 0.53123379 <a title="78-lsi-9" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>10 0.52173096 <a title="78-lsi-10" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>11 0.50425118 <a title="78-lsi-11" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>12 0.49847397 <a title="78-lsi-12" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>13 0.48901483 <a title="78-lsi-13" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>14 0.48185822 <a title="78-lsi-14" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>15 0.4692674 <a title="78-lsi-15" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>16 0.4602564 <a title="78-lsi-16" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>17 0.45694849 <a title="78-lsi-17" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>18 0.45487767 <a title="78-lsi-18" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>19 0.45317811 <a title="78-lsi-19" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>20 0.45256001 <a title="78-lsi-20" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.124), (16, 0.018), (26, 0.05), (28, 0.013), (33, 0.249), (38, 0.199), (67, 0.038), (69, 0.063), (76, 0.021), (87, 0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91682273 <a title="78-lda-1" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>Author: Engin Türetken, Fethallah Benmansour, Bjoern Andres, Hanspeter Pfister, Pascal Fua</p><p>Abstract: We propose a novel approach to automated delineation of linear structures that form complex and potentially loopy networks. This is in contrast to earlier approaches that usually assume a tree topology for the networks. At the heart of our method is an Integer Programming formulation that allows us to find the global optimum of an objective function designed to allow cycles but penalize spurious junctions and early terminations. We demonstrate that it outperforms state-of-the-art techniques on a wide range of datasets.</p><p>same-paper 2 0.87717932 <a title="78-lda-2" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>3 0.86458045 <a title="78-lda-3" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>Author: Jianzhou Yan, Stephen Lin, Sing Bing Kang, Xiaoou Tang</p><p>Abstract: Image cropping is a common operation used to improve the visual quality of photographs. In this paper, we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content, and enhancement of overall composition. Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. In contrast to the many methods that exist for general assessment of image quality, ours specifically examines differences between the original and cropped photo in solving for the crop parameters. To this end, several novel image features are proposed to model the changes in image content and composition when a crop is applied. Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images.</p><p>4 0.83906221 <a title="78-lda-4" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>Author: Lena Gorelick, Frank R. Schmidt, Yuri Boykov</p><p>Abstract: Trust region is a well-known general iterative approach to optimization which offers many advantages over standard gradient descent techniques. In particular, it allows more accurate nonlinear approximation models. In each iteration this approach computes a global optimum of a suitable approximation model within a fixed radius around the current solution, a.k.a. trust region. In general, this approach can be used only when some efficient constrained optimization algorithm is available for the selected nonlinear (more accurate) approximation model. In this paper we propose a Fast Trust Region (FTR) approach for optimization of segmentation energies with nonlinear regional terms, which are known to be challenging for existing algorithms. These energies include, but are not limited to, KL divergence and Bhattacharyya distance between the observed and the target appearance distributions, volume constraint on segment size, and shape prior constraint in a form of 퐿2 distance from target shape moments. Our method is 1-2 orders of magnitude faster than the existing state-of-the-art methods while converging to comparable or better solutions.</p><p>5 0.83021593 <a title="78-lda-5" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied, we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers, we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components, across other videos in the system, to perform subsequent video analysis. We demonstrate the efficacy of our approach for activity classification and clustering by reporting competitive results on standard datasets such as, HMDB, UCF-50, Olympic Sports and KTH.</p><p>6 0.82503492 <a title="78-lda-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.82452297 <a title="78-lda-7" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>8 0.82416278 <a title="78-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.82323313 <a title="78-lda-9" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>10 0.82076079 <a title="78-lda-10" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>11 0.81866354 <a title="78-lda-11" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>12 0.81847972 <a title="78-lda-12" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>13 0.81813794 <a title="78-lda-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.81799984 <a title="78-lda-14" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>15 0.81781626 <a title="78-lda-15" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>16 0.81726778 <a title="78-lda-16" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>17 0.8165307 <a title="78-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.81622773 <a title="78-lda-18" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>19 0.81615955 <a title="78-lda-19" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>20 0.8160795 <a title="78-lda-20" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
