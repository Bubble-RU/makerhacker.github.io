<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-63" href="#">cvpr2013-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</h1>
<br/><p>Source: <a title="cvpr-2013-63-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_Binary_Code_Ranking_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>Reference: <a title="cvpr-2013-63-reference" href="../cvpr2013_reference/cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu 0 ct an  Abstract Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. [sent-11, score-0.643]
</p><p>2 In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. [sent-12, score-0.759]
</p><p>3 The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. [sent-13, score-0.386]
</p><p>4 In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. [sent-14, score-0.821]
</p><p>5 By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. [sent-15, score-0.944]
</p><p>6 We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. [sent-16, score-0.479]
</p><p>7 Recently, binary hashing [20, 21, 16, 14, 12, 3, 6, 11] is becoming increasingly popular for efficient approximated nearest neighbor (ANN) search due to its good query and storage efficiency. [sent-21, score-0.838]
</p><p>8 The goal of binary hashing is to learn binary representations for data such that the neighborhood structure in the original data space can be preserved after embedded into Hamming space. [sent-22, score-0.611]
</p><p>9 Given a dataset, binary hashing generates binary code for each data point and approximates the distance or similarity of two points by the Hamming distance between their binary codes, which means most hashing methods rank the returned results based on their Hamming distances to query. [sent-23, score-1.452]
</p><p>10 However, since the Hamming distance is discrete and bounded by the code length, in practice, there will be a lot of data points sharing the same Hamming distance to the query and the ranking of these data points is ambiguous, which poses a critical issue for similarity search, e. [sent-25, score-0.587]
</p><p>11 As a result, most existing binary hashing methods lack in providing a good ranking of results. [sent-28, score-0.629]
</p><p>12 In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to improve the ranking performance of binary hashing methods. [sent-29, score-0.853]
</p><p>13 By assigning different bit-level weights to different hash bits, it is possible to rank two binary codes sharing the same Hamming distance to a query at a finer-grained binary code level, and gives binary hashing methods the ability to distinguish between the relative importance of different bits. [sent-30, score-1.613]
</p><p>14 We also give an algorithm to learn a set of dynamic bit-level weights of hash bits for a given query. [sent-31, score-0.636]
</p><p>15 By taking account of the information provided by the hash functions and dataset, we learn a set of dataadaptive and query-sensitive bit-level weights to reveal the relative importance of different hash bits. [sent-32, score-0.939]
</p><p>16 Recently, hashing based methods [5, 3, 24] has been widely used for efficient similarity search in a large variety of applications due to its efficiency in terms of query speed and storage space. [sent-43, score-0.643]
</p><p>17 The goal of binary hashing is to map each dataset point to a compact binary code such that similar data points in the original data space can be mapped to similar binary codes in Hamming space. [sent-44, score-0.962]
</p><p>18 However, long code results in low recall since the collision probability of similar points mapped to similar binary codes decreases exponentially as the code length increases. [sent-47, score-0.456]
</p><p>19 As a result, LSH-related methods usually construct multi-tables to ensure a reasonable probability that a query will collide with its near neighbors in at least one of the tables, which leads to a long query time and increases the memory occupation. [sent-48, score-0.414]
</p><p>20 Semantic Hashing [17] adopts a deep generative model based on restricted Boltzmann machine to learn the hash functions that map similar points to similar binary codes. [sent-50, score-0.562]
</p><p>21 Spectral Hashing (SPH) [21] uses spectral graph partitioning strategy for hash function learning and uses the simple analytical  eigenfunction solution of 1-D Laplacians as the hash function. [sent-51, score-0.889]
</p><p>22 In most existing binary hashing methods, including those methods discussed above, the returned results of a given query are simply ranked based on their Hamming distance to the query. [sent-56, score-0.845]
</p><p>23 The calculation of Hamming distance is efficient, however, since this distance metric gives each hash bit the same weight, it unable to distinguish between the relative importance of different bits and causes ambiguity for ranking. [sent-57, score-0.888]
</p><p>24 One way to alleviate this ambiguity is assigning different bit-level weights to different hash bits. [sent-58, score-0.475]
</p><p>25 In [20], each bit of the binary code is assigned with a bit-level weight, while in [4], the aim is to weight the overall Hamming distance of local features for image matching. [sent-60, score-0.426]
</p><p>26 propose a query-adaptive Hamming distance for image retrieve which assigns dynamic weights to hash bits, such that each bit is treated differently and dynamically. [sent-63, score-0.708]
</p><p>27 In [22], the authors propose a query-sensitive hash code ranking algorithm (QsRank) for PCA-based hashing methods. [sent-67, score-1.036]
</p><p>28 Given a query, QsRank assigns two weights to each hash bit and defines a score function to measure the confidence of the neighbors of a query mapped to a binary code. [sent-68, score-0.991]
</p><p>29 First, QsRank is developed only for PCAbased binary hashing while WhRank can be applied to most  existing binary hashing methods. [sent-72, score-1.023]
</p><p>30 In most binary hashing algorithms, the distance between two points is simply measured by the Hamming distance between their binary codes. [sent-78, score-0.741]
</p><p>31 With the bit-level weights, the returned binary codes can be ranked by the weighted Hamming distance at 111555888755  a finer-grained binary code level rather than at the original integer Hamming distance level. [sent-89, score-0.635]
</p><p>32 The bit-level weight associated with hash bit k is denoted as ωk. [sent-90, score-0.619]
</p><p>33 Note that, our algorithm is not to propose a new binary hashing method, but to give a ranking algorithm to improve the search accuracy of most existing binary hashing methods. [sent-92, score-1.184]
</p><p>34 { xThe} paradigm of binary hashing is to first use a set of linear or non-linear hash functions F = {fk : Rd → R}kK=1 to map x ∈ X to F(x) ∈ RK,  fK(x))T  and then binarize F(x) = (f1(x), ··· , by comparing each fk (x) with a thresh(oxld) ,T··k ·to , get a K-bit binary code H(x) ∈ {0, 1}K. [sent-95, score-1.262]
</p><p>35 Hence, the binary hash function is hcokd d(xe )H =(x s)gn ∈(f {k0 (,x1)} Tk). [sent-96, score-0.544]
</p><p>36 Each(x xd)im −en Tsion of H(x) is called a hash bit, and for a query q and its neighbor p, if the k-th bit of H(q) and H(p) is different, we call there is a bit-flipping on hash bit k. [sent-98, score-1.434]
</p><p>37 Data-Adaptive Weight We introduce a term discriminating power to denote the ability of a hash function hk (x) mapping similar data points to the same bit (0/1). [sent-102, score-0.895]
</p><p>38 A hash function hk (x) is called discriminative if the probability of similar data points mapped to the same bit by hk (x) is not small (> 0. [sent-103, score-1.169]
</p><p>39 The more discriminative hk (x), the more discriminative hash bit k. [sent-105, score-0.852]
</p><p>40 Obviously, the discriminating power of a hash function is  dependent on the algorithm generates it and the dataset used for training. [sent-106, score-0.509]
</p><p>41 PCAH [20], SPH [21], ITQ [3] and AGH [12], the discriminating power of different hash function is intrinsically different. [sent-109, score-0.487]
</p><p>42 For a hash function with a stronger discriminating power, it’s less likely for this hash function to generate different bits for two neighbor points. [sent-110, score-1.134]
</p><p>43 In other word, for a query q and two data points p(1) , p(2) sharing the same Hamming distance (1) to q, where H(p(1) ) and H(p(2) ) are different with H(q) on hash bits k1 and k2 respectively. [sent-111, score-0.868]
</p><p>44 If hash bit k1 is more discriminative than k2, then p(1) is considered to be less similar with q than p(2) , since the bit-flipping on hash bit k1 gives a higher confidence that p(1) is not a neighbor of q than that on k2. [sent-112, score-1.269]
</p><p>45 To make DHw (H(q) , H(p(1) )) larger than DHw (H(q) , H(p(2) )), ωk1 should be larger than ωk2 , which means the more discriminative a hash bit k is, the larger the associated weight ωk is. [sent-113, score-0.632]
</p><p>46 hash function hk (x)) is related to the probability of similar points mapped to the same bit by hk (x), given a hash function hk (x), we can use the distribution of hk (p) − hk (q), where p ∈ N(q), to reveal how discriminative( hpa)s h− b hit k is. [sent-116, score-2.332]
</p><p>47 Histograms of the differences between the unbinarized hash values of a query and its neighbors, generated by ITQ [3] and SPH [21]. [sent-118, score-0.676]
</p><p>48 However, since hk (p) and hk (q) are binarized, too much useful information is lost. [sent-120, score-0.466]
</p><p>49 An alternative is to use the distribution of the difference between their unbinarized hash values, i. [sent-121, score-0.524]
</p><p>50 If sk (p, q)( pis) −difstributed in a small interval centered around 0, then the probability of hk (p) = hk (q) is high, yielding a high discriminative hash bit k. [sent-124, score-1.123]
</p><p>51 As can be seen from this figure, the distributions are all Bell-shaped, as all these binary hashing methods try to minimize the distances between similar points after hashing. [sent-127, score-0.536]
</p><p>52 As a result, ωk is a function of the distribution of sk, which is parameterized by the its mean μk and standard deviation σk: −  ωk  = g(μk , σk)  (1)  Note that, hash bit k is more discriminative if σk is smaller, therefore, ωk = g(μk, σk) should be monotonically nonincreasing w. [sent-128, score-0.649]
</p><p>53 2, as shown, the probability of bit-flipping on hash bit k increases with the standard deviation σk. [sent-133, score-0.619]
</p><p>54 Query-Sensitive Weight Meanwhile, for a specified data point q, the probability of its neighbor p mapped to a bit different from Hk (q) by hash function hk (x) is also dependent on q itself. [sent-136, score-0.997]
</p><p>55 Intuitively, if |fk (q) Tk | is small, then after adding a random nitioviseely n,˜ i tfo | q, iqt’)s more likely tlhl,at th fekn n( aqf t+e rn˜ a)d ldi ensg on tahned opposite side of Tk as compared to fk (q), which means the probability of hk (p) differs from hk (q) for p ∈ N(q) is high. [sent-137, score-0.628]
</p><p>56 A query q is mapped to “101”, the binary codes of p1, p2 and p3 are “001”, “1 11”, “1 10” respectively. [sent-140, score-0.417]
</p><p>57 However, it’s more suitable to rank the hash code “1 10” and “1 11” before “001” because q is far from the threshold of hash function h1, it’s less likely for a near neighbor of q lies on the opposite side of h1. [sent-142, score-1.037]
</p><p>58 The probability of a query q’s neighbor, p, mapped to a bit different from h(q) by hash function h(x) = sgn(f(x) T). [sent-153, score-0.85]
</p><p>59 In this section, we give a simple method to calculate the data-adaptive and query-sensitive bit-level weight ωk (q) of each hash bit k for a given query q, and we will show that ωk (q) satisfies the abovementioned constraints theoretically. [sent-169, score-0.828]
</p><p>60 Therefore, given a query q and a binary code h, a function parameterized by Pr(h|H(q)) is used as a probabilisttiico interpretation odf DHw (rH(h(q|H) , (hq)). [sent-171, score-0.365]
</p><p>61 As a result, given a query q, the weighted Hamming distance between H(q) and a binary code h is defined as follows: DHw(H(q), h)  ≈  −log Pr(h|H(q))  (3)  0ph012 h1 10 1qp2 13h103Pr(Δhk(q)≠0Tkf (q)Prs(kΔ=hfk (qp)=-f0k( )q(x) Figure 3. [sent-183, score-0.463]
</p><p>62 The Right illustrates the probability of a neighbor of q mapped to a different bit by hash function fk (x), Tk is the binary threshold. [sent-185, score-0.986]
</p><p>63 Assume all the hash bits are independent [22], we have: Pr(h|H(q))  =  ? [sent-186, score-0.592]
</p><p>64 k  =  =  where Pr(hk hk (q)) (denoted by Pr(Δhk (q) 0)) is the probability ohf hash bit k of h flipped as compared with that of H(q), and Pr(hk = hk (q)) (denoted by Pr(Δhk (q) = 0)) is the probability of hash bit k of h not flipped . [sent-190, score-1.734]
</p><p>65 Apparently, these two probabilities are dependent on the specified query q and the hash function hk (x). [sent-191, score-0.869]
</p><p>66 k∈S  where S is the set of hash bits in h differ from H(q), and  λk(q) = logPPrr((ΔΔhhkk((qq))? [sent-198, score-0.592]
</p><p>67 −2, we can use the distribution of s(q + q) = fk (q + n˜ ) fk (q) with density function pdfk (s) to estimate( qPr +(Δ n˜h)k − −(q) f 0). [sent-211, score-0.36]
</p><p>68 Given a query q, first the unbinarized hash value fk (q) of each hash bit k is calculated. [sent-229, score-1.405]
</p><p>69 Note that, since we make no assumption about the hashing method used in the bit-level weights learning, our algorithm, WhRank, can be applied to different kinds of hashing methods. [sent-243, score-0.847]
</p><p>70 (5), given a query q and a binary code h, DHw (H(q) , h) can be calculated efficiently as: ωT (q) (H(q) ⊗ h), where ⊗ means the xor of two binary cod(eqs a(Hnd( qω)( q⊗) =), w(ωh1e(rqe) ⊗, ω m2(eqa)n,s s· ·t · , ωxoKr( oqf) )twTo. [sent-246, score-0.473]
</p><p>71 b Winhairley the weighted distances can now ) b,e· c·a ,lωculated by innerproduct operation, it is actually possible to avoid this computational cost by computing the traditional Hamming distance first, and then ranking the returned binary codes based on their weighted-Hamming distances to H(q). [sent-247, score-0.532]
</p><p>72 Therefore, the ranking of the returned binary codes can be obtained with minor additional cost. [sent-248, score-0.392]
</p><p>73 To learn the μk and σk of a hash function hk (x), we construct a training set consists of s query points, each of which has m neighbors. [sent-249, score-0.847]
</p><p>74 The complexity of calculating the unbinarized hash values of each query and its neighbors is almost O(s(m + 1)d), and the complexity of calculating μk  and σk is bounded by O(3sm). [sent-250, score-0.722]
</p><p>75 3, our methods can be applied to different kinds of binary hashing methods. [sent-263, score-0.519]
</p><p>76 In our experiments, some representative hashing methods, Locality Sensitive Hashing (LSH) [1], PCA Hashing (PCAH) [20], Iterative Quantization (ITQ) [3], Spectral Hashing (SPH) [21] and Anchor Graph Hashing (AGH) [12], are chosen to evaluate the effectiveness of WhRank. [sent-264, score-0.395]
</p><p>77 Note that, the hash functions of LSH, PCAH and ITQ are linear, while those of SPH and AGH are nonlinear. [sent-267, score-0.436]
</p><p>78 2 show that, WhRank is applicable to both linear and nonlinear hashing methods. [sent-269, score-0.395]
</p><p>79 Since QsRank is developed only for PCA-based hashing methods, the comparisons are carried out on PCAH and ITQ. [sent-271, score-0.424]
</p><p>80 Given a query, by ranking with traditional Hamming distance and our weighted Hamming distance, the returned top N nearest neighbors and the rankings are both different. [sent-272, score-0.383]
</p><p>81 For MINST70K, a returned point is considered as a true neighbor of a query if they share the same digit label. [sent-281, score-0.359]
</p><p>82 For ANN-SIFT1M, we use the same criterion as in [19]: a returned point is considered to be a true neighbor if it lies in the top 1% points closest to the query in terms of Euclidean distance in the original space. [sent-282, score-0.402]
</p><p>83 Experimental Results To demonstrate the efficacy of applying our weighted Hamming distance for ranking, given a query, the returned results of each baseline hashing method are ranked by their traditional Hamming distance and the weighted Hamming distance to the query respectively. [sent-285, score-1.014]
</p><p>84 The dataset is first embedded into Hamming space using each baseline hashing method. [sent-290, score-0.417]
</p><p>85 It is easy to find out that, by ranking with our weighted Hamming distance (WhRank), all baseline hashing methods achieve a better search performance. [sent-299, score-0.676]
</p><p>86 On average, we get a 5% higher precision for each hashing method. [sent-300, score-0.427]
</p><p>87 Once again, we can easily find out that the performance of each baseline hashing method is improved when combined with WhRank. [sent-307, score-0.436]
</p><p>88 5(c), even with a relatively short binary code (32 bits), the retrieval accuracy of each baseline method combined with WhRank is almost the same as, sometimes better than, that of the baseline method itself with a binary code of larger size (64 bits, 96 bits). [sent-310, score-0.452]
</p><p>89 The experimental results demonstrate that applying WhRank to existing hashing methods yielding a more accurate similarity search result. [sent-321, score-0.448]
</p><p>90 Since QsRank is developed only for PCA-based hashing method, The comparisons are carried out on PCAH [20] and ITQ [3]. [sent-323, score-0.424]
</p><p>91 -neighbor search, in our experiments on MINST70K, given a query q and N, the 111555889199  (a) 48 bits  (b) 64 bits  (c) 96 bits  Figure 5. [sent-325, score-0.646]
</p><p>92 One remarkable advantage of WhRank over QsRank is that, the ranking model of WhRank is more general, thus WhRank is also applicable to other non-PCA-based hashing methods, e. [sent-340, score-0.521]
</p><p>93 -neighbor search, while QsRank is not very effective for nearest neighbor search since the distance between a query and its nearest neighbor is often unknown in practice. [sent-344, score-0.479]
</p><p>94 Conclusion Most existing binary hashing methods rank the returned results of a query simply with the traditional Hamming distance, which poses a critical issue for similarity search where ranking is important, since there can be many results sharing the same Hamming distance to the query. [sent-346, score-1.048]
</p><p>95 When applied to existing hashing methods, different bit-level weights are assigned to different hash bits, and the returned results can be ranked at a finer-grained binary code level rather than at the original integer Hamming distance level. [sent-348, score-1.209]
</p><p>96 The search performances of all evaluated hashing methods are improved when combined with WhRank. [sent-351, score-0.449]
</p><p>97 First, WhRank can be applied to various kinds ofhashing methods while QsRank is only developed for PCA-based hashing methods. [sent-354, score-0.428]
</p><p>98 -neighbor  search, it’s not very effective for nearest neighbor search since the distance of a query to its nearest neighbor is unknown in practice. [sent-356, score-0.479]
</p><p>99 Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. [sent-370, score-0.5]
</p><p>100 Supervised  [12] [13] [14]  [15]  [16]  [17] [18] [19]  [20] [21] [22]  [23]  hashing with kernels. [sent-442, score-0.395]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hash', 0.436), ('hashing', 0.395), ('whrank', 0.384), ('hamming', 0.299), ('qsrank', 0.26), ('hk', 0.233), ('query', 0.178), ('bit', 0.157), ('bits', 0.156), ('dhw', 0.143), ('fk', 0.136), ('ranking', 0.126), ('binary', 0.108), ('pcah', 0.106), ('pr', 0.097), ('sph', 0.097), ('itq', 0.088), ('tk', 0.086), ('returned', 0.08), ('code', 0.079), ('codes', 0.078), ('neighbor', 0.07), ('agh', 0.064), ('lsh', 0.064), ('pdfk', 0.062), ('unbinarized', 0.062), ('distance', 0.056), ('mapped', 0.053), ('efficacy', 0.047), ('weighted', 0.042), ('discriminating', 0.036), ('search', 0.035), ('nearest', 0.035), ('neighbors', 0.032), ('precision', 0.032), ('retrieve', 0.032), ('digit', 0.031), ('laplace', 0.03), ('ranked', 0.028), ('weights', 0.027), ('weight', 0.026), ('erf', 0.026), ('distribution', 0.026), ('probability', 0.026), ('evaluations', 0.025), ('quantization', 0.025), ('dataadaptive', 0.025), ('fkk', 0.025), ('sk', 0.025), ('sharing', 0.024), ('pages', 0.023), ('annosearch', 0.022), ('china', 0.022), ('baseline', 0.022), ('lengths', 0.022), ('dependent', 0.022), ('nsfc', 0.02), ('hq', 0.019), ('combined', 0.019), ('weighting', 0.019), ('anchor', 0.018), ('similarity', 0.018), ('points', 0.018), ('storage', 0.017), ('monotonically', 0.017), ('locality', 0.017), ('developed', 0.017), ('give', 0.017), ('spectral', 0.017), ('eq', 0.017), ('beijing', 0.016), ('texas', 0.016), ('moreover', 0.016), ('rank', 0.016), ('kinds', 0.016), ('flipped', 0.015), ('compact', 0.015), ('sometimes', 0.015), ('ratio', 0.015), ('student', 0.015), ('apparently', 0.015), ('power', 0.015), ('distances', 0.015), ('calculation', 0.015), ('recall', 0.015), ('reveal', 0.015), ('bounded', 0.014), ('kulis', 0.014), ('assumption', 0.014), ('satisfies', 0.014), ('egou', 0.014), ('douze', 0.013), ('jiang', 0.013), ('hh', 0.013), ('academy', 0.013), ('discriminative', 0.013), ('ambiguity', 0.012), ('traditional', 0.012), ('smaller', 0.012), ('carried', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="63-tfidf-1" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>2 0.48925164 <a title="63-tfidf-2" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>Author: Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van_den_Hengel, Zhenmin Tang</p><p>Abstract: Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original highdimensional data. The complexity of these models, and the problems with out-of-sample data, havepreviously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE [29] outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths.</p><p>3 0.48371351 <a title="63-tfidf-3" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>4 0.4037874 <a title="63-tfidf-4" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>Author: Yue Lin, Rong Jin, Deng Cai, Shuicheng Yan, Xuelong Li</p><p>Abstract: Recent studies have shown that hashing methods are effective for high dimensional nearest neighbor search. A common problem shared by many existing hashing methods is that in order to achieve a satisfied performance, a large number of hash tables (i.e., long codewords) are required. To address this challenge, in this paper we propose a novel approach called Compressed Hashing by exploring the techniques of sparse coding and compressed sensing. In particular, we introduce a sparse coding scheme, based on the approximation theory of integral operator, that generate sparse representation for high dimensional vectors. We then project sparse codes into a low dimensional space by effectively exploring the Restricted Isometry Property (RIP), a key property in compressed sensing theory. Both of the theoretical analysis and the empirical studies on two large data sets show that the proposed approach is more effective than the state-of-the-art hashing algorithms.</p><p>5 0.35312709 <a title="63-tfidf-5" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>Author: Thomas Dean, Mark A. Ruzon, Mark Segal, Jonathon Shlens, Sudheendra Vijayanarasimhan, Jay Yagnik</p><p>Abstract: Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object’s appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times— four orders of magnitude— when compared withperforming the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.</p><p>6 0.26031339 <a title="63-tfidf-6" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>7 0.18925411 <a title="63-tfidf-7" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>8 0.17111211 <a title="63-tfidf-8" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>9 0.16492991 <a title="63-tfidf-9" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>10 0.15789159 <a title="63-tfidf-10" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>11 0.11192923 <a title="63-tfidf-11" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>12 0.10971431 <a title="63-tfidf-12" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>13 0.10468608 <a title="63-tfidf-13" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>14 0.086736754 <a title="63-tfidf-14" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>15 0.082297929 <a title="63-tfidf-15" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>16 0.064735256 <a title="63-tfidf-16" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>17 0.063092887 <a title="63-tfidf-17" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>18 0.059787646 <a title="63-tfidf-18" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>19 0.05834759 <a title="63-tfidf-19" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>20 0.05639419 <a title="63-tfidf-20" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, -0.05), (2, -0.037), (3, 0.052), (4, 0.122), (5, 0.026), (6, -0.091), (7, -0.256), (8, -0.315), (9, -0.09), (10, -0.341), (11, 0.015), (12, 0.084), (13, 0.278), (14, 0.033), (15, 0.152), (16, 0.079), (17, 0.061), (18, -0.09), (19, 0.082), (20, -0.194), (21, -0.005), (22, -0.083), (23, 0.029), (24, -0.006), (25, -0.121), (26, 0.052), (27, -0.049), (28, 0.007), (29, 0.043), (30, -0.05), (31, -0.082), (32, -0.022), (33, 0.001), (34, 0.091), (35, 0.012), (36, -0.015), (37, 0.007), (38, -0.058), (39, -0.027), (40, -0.009), (41, -0.072), (42, -0.034), (43, -0.008), (44, -0.046), (45, 0.006), (46, 0.059), (47, 0.014), (48, 0.003), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96672487 <a title="63-lsi-1" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>2 0.86116076 <a title="63-lsi-2" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>3 0.85812736 <a title="63-lsi-3" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>Author: Yue Lin, Rong Jin, Deng Cai, Shuicheng Yan, Xuelong Li</p><p>Abstract: Recent studies have shown that hashing methods are effective for high dimensional nearest neighbor search. A common problem shared by many existing hashing methods is that in order to achieve a satisfied performance, a large number of hash tables (i.e., long codewords) are required. To address this challenge, in this paper we propose a novel approach called Compressed Hashing by exploring the techniques of sparse coding and compressed sensing. In particular, we introduce a sparse coding scheme, based on the approximation theory of integral operator, that generate sparse representation for high dimensional vectors. We then project sparse codes into a low dimensional space by effectively exploring the Restricted Isometry Property (RIP), a key property in compressed sensing theory. Both of the theoretical analysis and the empirical studies on two large data sets show that the proposed approach is more effective than the state-of-the-art hashing algorithms.</p><p>4 0.82656986 <a title="63-lsi-4" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>Author: Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van_den_Hengel, Zhenmin Tang</p><p>Abstract: Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original highdimensional data. The complexity of these models, and the problems with out-of-sample data, havepreviously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE [29] outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths.</p><p>5 0.53076041 <a title="63-lsi-5" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>6 0.49980116 <a title="63-lsi-6" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>7 0.47029543 <a title="63-lsi-7" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>8 0.47003454 <a title="63-lsi-8" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>9 0.46809909 <a title="63-lsi-9" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>10 0.4545958 <a title="63-lsi-10" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>11 0.30136243 <a title="63-lsi-11" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>12 0.30037433 <a title="63-lsi-12" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>13 0.23067079 <a title="63-lsi-13" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>14 0.21313721 <a title="63-lsi-14" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>15 0.18954696 <a title="63-lsi-15" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>16 0.189022 <a title="63-lsi-16" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>17 0.18192333 <a title="63-lsi-17" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>18 0.18018471 <a title="63-lsi-18" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>19 0.17463696 <a title="63-lsi-19" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>20 0.16532113 <a title="63-lsi-20" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.113), (16, 0.038), (26, 0.023), (28, 0.013), (33, 0.19), (36, 0.111), (52, 0.181), (67, 0.101), (69, 0.049), (70, 0.038), (87, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81246907 <a title="63-lda-1" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>2 0.77542895 <a title="63-lda-2" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>3 0.77464139 <a title="63-lda-3" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan</p><p>Abstract: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the andnodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC2007and UIUCPeople, and it achieves state-of-the-arts performance.</p><p>4 0.7720716 <a title="63-lda-4" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>5 0.76621395 <a title="63-lda-5" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>6 0.75716132 <a title="63-lda-6" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>7 0.74550647 <a title="63-lda-7" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>8 0.74163437 <a title="63-lda-8" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>9 0.73838717 <a title="63-lda-9" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>10 0.73730499 <a title="63-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>11 0.73728156 <a title="63-lda-11" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>12 0.73698533 <a title="63-lda-12" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>13 0.73583907 <a title="63-lda-13" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>14 0.73449051 <a title="63-lda-14" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>15 0.73379409 <a title="63-lda-15" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>16 0.73322403 <a title="63-lda-16" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>17 0.73287398 <a title="63-lda-17" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>18 0.73166633 <a title="63-lda-18" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>19 0.73129618 <a title="63-lda-19" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>20 0.73076957 <a title="63-lda-20" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
