<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-67" href="#">cvpr2013-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</h1>
<br/><p>Source: <a title="cvpr-2013-67-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Juneja_Blocks_That_Shout_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Mayank Juneja, Andrea Vedaldi, C.V. Jawahar, Andrew Zisserman</p><p>Abstract: The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previouspart-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of- the-art classification performance on this data.</p><p>Reference: <a title="cvpr-2013-67-reference" href="../cvpr2013_reference/cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. [sent-10, score-1.15]
</p><p>2 We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. [sent-12, score-0.498]
</p><p>3 We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. [sent-14, score-0.735]
</p><p>4 We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previouspart-learning methods such as Singh et al. [sent-16, score-0.34]
</p><p>5 Yet, the automatic discovery of good parts is still a difficult problem. [sent-21, score-0.333]
</p><p>6 In DPM, for example, part occurrences are initially assumed to be in a fixed location relative to the ground truth object bounding boxes, and then are refined as latent variables during learning [9]. [sent-22, score-0.367]
</p><p>7 In this paper, a simple, efficient, and effective method for discovering parts automatically and with very little supervision is proposed. [sent-25, score-0.273]
</p><p>8 Its power is demonstrated in the context of scene recognition where, unlike in object recognition, object bounding boxes are not available, making part alignment very challenging. [sent-26, score-0.306]
</p><p>9 1 shows examples of the learned  parts detected on the test set. [sent-29, score-0.377]
</p><p>10 The first is to find and align part instances in the training data while a model of the part is not yet available. [sent-31, score-0.322]
</p><p>11 While this procedure requires training a sequence of detectors, the LDA technique of [13] is used to avoid mining for hard negative examples, eliminating the main bottleneck in detector learning [9, 32], and enabling a very efficient part-learning algorithm. [sent-35, score-0.299]
</p><p>12 The second issue is to select distinctive parts among the ones that are generated by the part mining process. [sent-36, score-0.672]
</p><p>13 This criterion selects parts that are informative for a small proportion of classes. [sent-39, score-0.34]
</p><p>14 Differently to other measures such as average precision, the resulting parts can then be shared by more than one object category. [sent-40, score-0.273]
</p><p>15 This is particularly important because parts should be regarded as mid-level primitives that do not necessarily have to respond to a single object class. [sent-41, score-0.273]
</p><p>16 Example of occurrences of distinctive parts learned by our method from weakly supervised image data. [sent-43, score-0.836]
</p><p>17 These part occurrences are detected on the test data. [sent-44, score-0.374]
</p><p>18 The result of our procedure is the automatic discovery of distinctive part detectors. [sent-46, score-0.403]
</p><p>19 In models such as DPMs, parts are devoid of a specific semantic content and are used to represent deformations of a two dimensional template. [sent-51, score-0.273]
</p><p>20 For example, in Poselets [5] object parts correspond to recognizable clusters in appearance and configuration, in Li et al. [sent-53, score-0.273]
</p><p>21 [15] scene parts correspond to object categories, and in Raptis et al. [sent-54, score-0.378]
</p><p>22 [25] action parts capture spatio-temporal components of human activities. [sent-55, score-0.273]
</p><p>23 The learning of parts is usually integrated into the learning of a complete object or scene model [3, 9]. [sent-56, score-0.45]
</p><p>24 [28] explores learning parts in both an unsupervised and weakly supervised manner, where  the weakly supervised case (as here) only uses the class label of the image. [sent-60, score-0.631]
</p><p>25 Their weakly supervised procedure is applied to the MIT Scene 67 dataset, obtaining state-of-the-art scene classification performance. [sent-61, score-0.335]
</p><p>26 As will be seen though, our part-learning method is: (i) simpler, (ii) more efficient, and (iii) able to learn parts that are significantly better at scene classification. [sent-62, score-0.378]
</p><p>27 [19] propose a reconfigurable version of a spatial bag of visual words (BoW) model that associates different BoW descriptors to different image segments, corresponding to different types of “stuff”. [sent-66, score-0.308]
</p><p>28 The standard DPM model is applied to the task of scene categorization by Pandey and Lazebnik [18], but the problem of part initialization and learning is not addressed, and the quality of the parts that can be obtained in this manner remains unclear. [sent-68, score-0.602]
</p><p>29 Blocks that shout: learning distinctive parts In characterizing images of particular scene classes, e. [sent-75, score-0.581]
</p><p>30 In practice, however, a distinctive part is useful only if it can be detected automatically, preferably by an efficient and simple algorithm. [sent-80, score-0.343]
</p><p>31 Moreover, distinctive parts may include other structures that have a weaker or more abstract semantic, such as the corners of a room or a corridor, particular shapes (rounded, square), and so on. [sent-81, score-0.44]
</p><p>32 Designing a good vocabulary of parts is therefore best left to learning. [sent-82, score-0.273]
</p><p>33 Learning a distinctive part means identifying a localized detectable entity that is informative for the task at hand (in our example discriminating different scene types). [sent-83, score-0.472]
</p><p>34 This is very challenging because (i) one does not know if a part occurs in any given training image or not, and (ii) when the part occurs, one does not know its location. [sent-84, score-0.322]
</p><p>35 While methods such as multiple instance learning have often been proposed to try to identify parts automatically, in practice they require careful initialization to work well. [sent-85, score-0.342]
</p><p>36 An example block learnt automatically for the laundromat class. [sent-117, score-0.328]
</p><p>37 This block is a characteristic part for this class. [sent-118, score-0.344]
</p><p>38 All such blocks are treated initially as potentially different parts. [sent-120, score-0.268]
</p><p>39 2) each block is used as a seed to build a model for a part while gradually searching for more and more part occurrences in the training data. [sent-123, score-0.844]
</p><p>40 This paced expansion addresses the issue of detecting and localizing part exemplars. [sent-124, score-0.313]
</p><p>41 3) finds the most distinctive parts in the pool of candidate parts generated by seeding and expansion by looking at their predictive power in terms of entropy-rank. [sent-127, score-1.014]
</p><p>42 The procedure is weakly supervised in that positives are only sought in the seeding and expansion stages within images of a single class. [sent-128, score-0.483]
</p><p>43 Once these distinctive parts are obtained, they can be used for a variety of tasks. [sent-129, score-0.44]
</p><p>44 Seeding: proposing an initial set of parts Initially, no part model is available and, without further information, any sub-window in any training image is equally likely to contain a distinctive part. [sent-137, score-0.629]
</p><p>45 In principle, one could simply try to learn a part model by starting from all possible image sub-windows and identify good parts aposteriori, during the selection stage (Sect. [sent-138, score-0.439]
</p><p>46 Unfortunately, most of these parts will in fact not be distinctive (e. [sent-141, score-0.44]
</p><p>47 Each part is described by a block cofe l8s × o f8 8 8H ×OG 8 pceixl esl,s . [sent-155, score-0.307]
</p><p>48 AO part sllese,d a nisd i nhietinacleiz eodcc fuopri eesac ahn superpixel by centering the 64 64 pixel block at the center of mass of tcheen superpixel. [sent-157, score-0.307]
</p><p>49 Figure 3 shows an example of the superpixels computed from a training image, and the seed blocks obtained using this procedure. [sent-159, score-0.537]
</p><p>50 Expansion: learning part detectors Learning a part detector requires a set of part exemplars, and these need to be identified in the training data. [sent-162, score-0.621]
</p><p>51 A possible approach is to sample at random a set of part occurrences, but this is extremely unlikely to hit multiple occurrences of the same part. [sent-163, score-0.331]
</p><p>52 In practice, part initialization can be obtained by means of some heuristic, such as clustering patches, or taking parts at a fixed location assuming that images are at least partially aligned. [sent-164, score-0.406]
</p><p>53 However, the detector of a part is, by definition, the most general and reliable tool for the identification of that part occurrences. [sent-165, score-0.331]
</p><p>54 There is a special case in which a part detector can be learned without worrying about exemplar alignment: a  training set consisting exactly of one part instance. [sent-166, score-0.533]
</p><p>55 In practice, at each round of learning the current part model is used to rank blocks from images of the selected class and the highest scoring blocks are considered as further part occurrences. [sent-169, score-0.974]
</p><p>56 Figure 4 shows an example seed part on the left, and the additional part occurrences that are added to the training set during successive iterations of expansion. [sent-174, score-0.67]
</p><p>57 The super-pixels (b) suggest characteristic regions of the image, and blocks are formed for these. [sent-177, score-0.305]
</p><p>58 The downside of this mining process is that the part detector must be learned multiple times. [sent-187, score-0.358]
</p><p>59 In practice, the parameter vector w of a part classifier is learned simply as w = Σ−1 ( x¯ − μ0) where ¯x is the mean of the HOG features of the( positive part samples, μ0 is the mean of the HOG blocks in the dataset, and Σ the corresponding covariance matrix. [sent-190, score-0.635]
</p><p>60 Selection: identifying distinctive parts Our notion of a discriminative block is that it should occur in many of the images of the class from which it is learnt, but not in many images from other classes. [sent-196, score-0.803]
</p><p>61 How-  ever, it is not reasonable to assume that parts (represented by blocks) are so discriminative that they only occur in the class from which they are learnt. [sent-197, score-0.415]
</p><p>62 For example, the door of a washing machine will occur in the laundromat class, but can also occur in the kitchen or garage class. [sent-198, score-0.321]
</p><p>63 However, one would not expect these parts to appear in many other of the indoor classes. [sent-200, score-0.316]
</p><p>64 The block classifiers were learnt on training images for a particular class, and they are tested as detectors on validation images of all classes. [sent-203, score-0.464]
</p><p>65 On the right the additional example blocks added to the positive training set for retraining the part detector are shown in the order that they are added. [sent-207, score-0.522]
</p><p>66 Note that mining uses blocks selected from a certain scene category, but no other supervision is used. [sent-208, score-0.472]
</p><p>67 learned from a class are not required to be detected only from images of that class; instead, the milder constraint that the distribution of classes in which the block is detected should have low entropy is imposed. [sent-209, score-0.532]
</p><p>68 In this manner, distinctive but shareable mid-level parts can be selected. [sent-210, score-0.527]
</p><p>69 For the laundromat example above, we would expect the washing machine door to be detected in only a handful of the classes, so the entropy would be low. [sent-211, score-0.361]
</p><p>70 In contrast the block for a wall would be detected across many classes, so its distribution would be nearer uniform across classes, and hence the entropy higher. [sent-212, score-0.385]
</p><p>71 To operationalize this requirement, each block is evaluated in a sliding-window manner on each validation image. [sent-213, score-0.276]
</p><p>72 Then, five block occurrences are extracted from each image by max-pooling in five image regions, corresponding to the  spatial subdivisions used in the encoding of Sect. [sent-214, score-0.595]
</p><p>73 Classifier (a) has low entropy at top ranks, which shows that it is picking the blocks from a few classes. [sent-219, score-0.392]
</p><p>74 block occurrence (zi, yi) detected in this manner receives a detection score z and a class label y equal to the label of the image. [sent-222, score-0.376]
</p><p>75 The blocks are sorted on their score z, and the top r ranking blocks selected. [sent-223, score-0.536]
</p><p>76 We introduce Entropy-Rank Curves (ER curves) to measure the entropy of a block classifier at different ranks. [sent-229, score-0.298]
</p><p>77 Note, entropy for all part classifiers converges to a constant value (which depends on the class prior) as the rank increases. [sent-232, score-0.371]
</p><p>78 In fact, there is no guarantee that the part mining procedure will not return the same or similar parts multiple times. [sent-236, score-0.548]
</p><p>79 Bag of parts  In order to compute an image-level descriptor from the parts learned in Sect. [sent-272, score-0.645]
</p><p>80 Note that the method of selecting the parts (Sect. [sent-276, score-0.273]
</p><p>81 In order to use non-linear additive kernels instead of the linear one, the χ2 explicit feature map of [33] is used (the bag of parts and bag of words histograms are l1 normalized). [sent-301, score-0.665]
</p><p>82 Experiments and results The part-learning algorithm is evaluated on the task of scene classification on the MIT 67 indoor scene dataset of Quattoni and Torralba [24]. [sent-308, score-0.303]
</p><p>83 For each of these seed blocks, a classifier is learned by following the expansion procedure of Sect. [sent-360, score-0.369]
</p><p>84 We sample about 620,000 HOG blocks randomly from the training set, and compute the mean (μ0) and covariance (Σ) ofthis set. [sent-363, score-0.364]
</p><p>85 Once the parts have been learned as described in Sect. [sent-366, score-0.334]
</p><p>86 3, the bag of parts representation is extracted from each training image as described in Sect. [sent-370, score-0.504]
</p><p>87 Finally, 67 one-vs-rest SVMs are learned from the training images, and the resulting scene classifiers are evaluated on the test data. [sent-373, score-0.288]
</p><p>88 As one can expect, the classification accuracy increases as more parts are added to the representation (Table 2), but the peak is at around 50 parts per category. [sent-374, score-0.596]
</p><p>89 The probable reason is a lack of training material (after all the parts and classifiers are learned on the same 999992222288666  methods (previous publications and this paper). [sent-375, score-0.501]
</p><p>90 So the parts found by our algorithm are much more informative, improving the accuracy by 8% using only a quarter of the number of detectors. [sent-389, score-0.273]
</p><p>91 In the final experiment, the BoP (using 50 parts per class) and BoW/IFV representa-  ? [sent-396, score-0.273]
</p><p>92 Figure 6 shows qualitative results obtained by the combined bag of parts and IFV method. [sent-446, score-0.481]
</p><p>93 Summary We have presented a novel method to learn distinctive parts of objects or scenes automatically, from image-level category labels. [sent-448, score-0.44]
</p><p>94 The key problem of simultaneously learning a part model and detecting its occurrences in the training data was solved by paced learning of Exemplar SVMs, growing a model from just one occurrence of the part. [sent-449, score-0.58]
</p><p>95 The distinctiveness of parts was measured by the new concept of entropy-rank, capturing the idea that parts are at the same time predictive of certain object categories but shareable between different categories. [sent-450, score-0.734]
</p><p>96 The learned parts have been shown to perform very well on the task of scene classification, where they improved a very solid bag of words or Fisher Vector baseline that in itself establishes the new state-of-the-art on the MIT Scene 67 benchmark. [sent-451, score-0.656]
</p><p>97 This mid-level representation is useful for other tasks, for example to initialize the region models of [19] or the part models of [18], and yields more understandable and diagnosable models than the original bag of visual words method. [sent-453, score-0.35]
</p><p>98 (a), (c) Seed blocks and the learnt HOG templates, and (b), (d) detections on the validation set images. [sent-458, score-0.371]
</p><p>99 Beyond bag of features: Spatial pyramid matching for recognizing natural scene categories. [sent-554, score-0.28]
</p><p>100 Discovering discriminative  [26] [27]  [28] [29]  [30]  [31]  [32] [33] [34]  [35] [36] [37]  action parts from mid-level video representations. [sent-640, score-0.323]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ifv', 0.294), ('parts', 0.273), ('blocks', 0.268), ('occurrences', 0.198), ('bow', 0.183), ('bag', 0.175), ('block', 0.174), ('distinctive', 0.167), ('seeding', 0.152), ('seed', 0.15), ('rootsift', 0.134), ('part', 0.133), ('entropy', 0.124), ('encodings', 0.116), ('expansion', 0.115), ('mit', 0.111), ('fisher', 0.107), ('scene', 0.105), ('mining', 0.099), ('laundromat', 0.098), ('shout', 0.098), ('encoding', 0.094), ('round', 0.088), ('weakly', 0.087), ('shareable', 0.087), ('hog', 0.085), ('exemplar', 0.085), ('corridor', 0.069), ('distinctiveness', 0.067), ('informative', 0.067), ('classifiers', 0.066), ('paced', 0.065), ('subdivisions', 0.065), ('upstream', 0.065), ('detector', 0.065), ('detectors', 0.065), ('superpixels', 0.063), ('lda', 0.063), ('learned', 0.061), ('discovery', 0.06), ('quattoni', 0.058), ('washing', 0.058), ('occurrence', 0.056), ('training', 0.056), ('learnt', 0.056), ('manner', 0.055), ('supervised', 0.05), ('discriminative', 0.05), ('classification', 0.05), ('er', 0.049), ('descriptors', 0.048), ('parizi', 0.048), ('class', 0.048), ('notion', 0.047), ('validation', 0.047), ('grids', 0.045), ('pegasos', 0.045), ('publications', 0.045), ('wall', 0.044), ('poselets', 0.044), ('vedaldi', 0.044), ('occur', 0.044), ('pandey', 0.043), ('reconfigurable', 0.043), ('indoor', 0.043), ('procedure', 0.043), ('detected', 0.043), ('rescaling', 0.042), ('raptis', 0.042), ('words', 0.042), ('singh', 0.041), ('dpm', 0.041), ('sadeghi', 0.041), ('amit', 0.041), ('covariance', 0.04), ('kitchen', 0.039), ('dpms', 0.039), ('classes', 0.039), ('door', 0.038), ('objectness', 0.038), ('descriptor', 0.038), ('retained', 0.037), ('cats', 0.037), ('characteristic', 0.037), ('pooling', 0.037), ('learning', 0.036), ('sought', 0.036), ('curves', 0.035), ('boxes', 0.035), ('exemplars', 0.034), ('auc', 0.034), ('predictive', 0.034), ('seeds', 0.033), ('combined', 0.033), ('alignment', 0.033), ('svms', 0.033), ('identify', 0.033), ('quantization', 0.033), ('voc', 0.032), ('five', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="67-tfidf-1" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>Author: Mayank Juneja, Andrea Vedaldi, C.V. Jawahar, Andrew Zisserman</p><p>Abstract: The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previouspart-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of- the-art classification performance on this data.</p><p>2 0.27978507 <a title="67-tfidf-2" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>3 0.25878045 <a title="67-tfidf-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.18647154 <a title="67-tfidf-4" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>5 0.14740208 <a title="67-tfidf-5" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>Author: Fang Wang, Yi Li</p><p>Abstract: Simple tree models for articulated objects prevails in the last decade. However, it is also believed that these simple tree models are not capable of capturing large variations in many scenarios, such as human pose estimation. This paper attempts to address three questions: 1) are simple tree models sufficient? more specifically, 2) how to use tree models effectively in human pose estimation? and 3) how shall we use combined parts together with single parts efficiently? Assuming we have a set of single parts and combined parts, and the goal is to estimate a joint distribution of their locations. We surprisingly find that no latent variables are introduced in the Leeds Sport Dataset (LSP) during learning latent trees for deformable model, which aims at approximating the joint distributions of body part locations using minimal tree structure. This suggests one can straightforwardly use a mixed representation of single and combined parts to approximate their joint distribution in a simple tree model. As such, one only needs to build Visual Categories of the combined parts, and then perform inference on the learned latent tree. Our method outperformed the state of the art on the LSP, both in the scenarios when the training images are from the same dataset and from the PARSE dataset. Experiments on animal images from the VOC challenge further support our findings.</p><p>6 0.14618805 <a title="67-tfidf-6" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>7 0.14218049 <a title="67-tfidf-7" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>8 0.14190623 <a title="67-tfidf-8" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>9 0.13804938 <a title="67-tfidf-9" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>10 0.13003126 <a title="67-tfidf-10" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>11 0.12609008 <a title="67-tfidf-11" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>12 0.12475965 <a title="67-tfidf-12" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>13 0.12228693 <a title="67-tfidf-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.12110605 <a title="67-tfidf-14" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>15 0.12094085 <a title="67-tfidf-15" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>16 0.12017397 <a title="67-tfidf-16" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>17 0.11574899 <a title="67-tfidf-17" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>18 0.1133854 <a title="67-tfidf-18" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>19 0.11283195 <a title="67-tfidf-19" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>20 0.1124791 <a title="67-tfidf-20" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.285), (1, -0.133), (2, 0.001), (3, -0.051), (4, 0.067), (5, 0.041), (6, 0.034), (7, 0.115), (8, -0.035), (9, -0.071), (10, -0.095), (11, -0.026), (12, 0.066), (13, -0.044), (14, 0.033), (15, -0.071), (16, 0.075), (17, -0.023), (18, -0.023), (19, -0.017), (20, 0.083), (21, -0.008), (22, 0.174), (23, -0.018), (24, 0.012), (25, 0.128), (26, -0.013), (27, 0.014), (28, -0.064), (29, -0.036), (30, 0.005), (31, 0.044), (32, -0.003), (33, 0.014), (34, 0.02), (35, -0.036), (36, 0.013), (37, -0.07), (38, 0.008), (39, -0.055), (40, 0.019), (41, -0.085), (42, -0.077), (43, 0.01), (44, 0.004), (45, -0.067), (46, -0.042), (47, -0.124), (48, -0.067), (49, -0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96567863 <a title="67-lsi-1" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>Author: Mayank Juneja, Andrea Vedaldi, C.V. Jawahar, Andrew Zisserman</p><p>Abstract: The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previouspart-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of- the-art classification performance on this data.</p><p>2 0.83835 <a title="67-lsi-2" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>3 0.82827455 <a title="67-lsi-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.78501242 <a title="67-lsi-4" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>Author: Qiang Chen, Zheng Song, Rogerio Feris, Ankur Datta, Liangliang Cao, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In recent years, efficiency of large-scale object detection has arisen as an important topic due to the exponential growth in the size of benchmark object detection datasets. Most current object detection methods focus on improving accuracy of large-scale object detection with efficiency being an afterthought. In this paper, we present the Efficient Maximum Appearance Search (EMAS) model which is an order of magnitude faster than the existing state-of-the-art large-scale object detection approaches, while maintaining comparable accuracy. Our EMAS model consists of representing an image as an ensemble of densely sampled feature points with the proposed Pointwise Fisher Vector encoding method, so that the learnt discriminative scoring function can be applied locally. Consequently, the object detection problem is transformed into searching an image sub-area for maximum local appearance probability, thereby making EMAS an order of magnitude faster than the traditional detection methods. In addition, the proposed model is also suitable for incorporating global context at a negligible extra computational cost. EMAS can also incorporate fusion of multiple features, which greatly improves its performance in detecting multiple object categories. Our experiments show that the proposed algorithm can perform detection of 1000 object classes in less than one minute per image on the Image Net ILSVRC2012 dataset and for 107 object classes in less than 5 seconds per image for the SUN09 dataset using a single CPU.</p><p>5 0.75750673 <a title="67-lsi-5" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>6 0.74847627 <a title="67-lsi-6" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>7 0.73680973 <a title="67-lsi-7" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>8 0.72019678 <a title="67-lsi-8" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>9 0.70411742 <a title="67-lsi-9" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>10 0.68491012 <a title="67-lsi-10" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>11 0.68300813 <a title="67-lsi-11" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>12 0.68040562 <a title="67-lsi-12" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>13 0.66274345 <a title="67-lsi-13" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>14 0.66259098 <a title="67-lsi-14" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>15 0.65721023 <a title="67-lsi-15" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>16 0.64704591 <a title="67-lsi-16" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>17 0.64555162 <a title="67-lsi-17" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>18 0.64120626 <a title="67-lsi-18" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>19 0.63223398 <a title="67-lsi-19" href="./cvpr-2013-Fine-Grained_Crowdsourcing_for_Fine-Grained_Recognition.html">174 cvpr-2013-Fine-Grained Crowdsourcing for Fine-Grained Recognition</a></p>
<p>20 0.62893647 <a title="67-lsi-20" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.142), (16, 0.023), (26, 0.064), (28, 0.017), (33, 0.275), (48, 0.141), (67, 0.085), (69, 0.086), (72, 0.011), (80, 0.01), (87, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91120344 <a title="67-lda-1" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>Author: Mayank Juneja, Andrea Vedaldi, C.V. Jawahar, Andrew Zisserman</p><p>Abstract: The automatic discovery of distinctive parts for an object or scene class is challenging since it requires simultaneously to learn the part appearance and also to identify the part occurrences in images. In this paper, we propose a simple, efficient, and effective method to do so. We address this problem by learning parts incrementally, starting from a single part occurrence with an Exemplar SVM. In this manner, additional part instances are discovered and aligned reliably before being considered as training examples. We also propose entropy-rank curves as a means of evaluating the distinctiveness of parts shareable between categories and use them to select useful parts out of a set of candidates. We apply the new representation to the task of scene categorisation on the MIT Scene 67 benchmark. We show that our method can learn parts which are significantly more informative and for a fraction of the cost, compared to previouspart-learning methods such as Singh et al. [28]. We also show that a well constructed bag of words or Fisher vector model can substantially outperform the previous state-of- the-art classification performance on this data.</p><p>2 0.91031933 <a title="67-lda-2" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>3 0.90533781 <a title="67-lda-3" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>4 0.90509719 <a title="67-lda-4" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>5 0.9020893 <a title="67-lda-5" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>Author: Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun</p><p>Abstract: In this paper we are interested in how semantic segmentation can help object detection. Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional HOG filters as well as a set of novel segmentation features. Thus our model “blends ” between the detector and segmentation models. Since our features can be computed very efficiently given the segments, we maintain the same complexity as the original DPM [14]. We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector [12] on all classes, achieving 13% higher average AP. When employing the parts, we outperform the original DPM [14] in 19 out of 20 classes, achieving an improvement of 8% AP. Furthermore, we outperform the previous state-of-the-art on VOC’10 test by 4%.</p><p>6 0.90109849 <a title="67-lda-6" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>7 0.90071666 <a title="67-lda-7" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>8 0.89966434 <a title="67-lda-8" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>9 0.89963299 <a title="67-lda-9" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>10 0.89915484 <a title="67-lda-10" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>11 0.89913094 <a title="67-lda-11" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>12 0.89845484 <a title="67-lda-12" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>13 0.89781749 <a title="67-lda-13" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>14 0.89706886 <a title="67-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.89582509 <a title="67-lda-15" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>16 0.89564782 <a title="67-lda-16" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>17 0.89526635 <a title="67-lda-17" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>18 0.89462364 <a title="67-lda-18" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>19 0.89434594 <a title="67-lda-19" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>20 0.89421743 <a title="67-lda-20" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
