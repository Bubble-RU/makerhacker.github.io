<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-461" href="#">cvpr2013-461</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</h1>
<br/><p>Source: <a title="cvpr-2013-461-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Weakly_Supervised_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>Reference: <a title="cvpr-2013-461-reference" href="../cvpr2013_reference/cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. [sent-9, score-1.241]
</p><p>2 We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. [sent-12, score-0.395]
</p><p>3 The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. [sent-14, score-0.717]
</p><p>4 We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. [sent-15, score-0.676]
</p><p>5 For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. [sent-17, score-0.48]
</p><p>6 Then update the HST and attribute association based on the in-  ferred parse trees. [sent-18, score-0.692]
</p><p>7 We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts. [sent-19, score-0.79]
</p><p>8 Thus the interest in studying the scene attributes [6, 7] has been growing. [sent-24, score-0.41]
</p><p>9 A typical recent work is by Patterson and Hays [7] which identified 102 scene attributes through human perception experiments and trained 102 independent classifiers. [sent-25, score-0.384]
</p><p>10 In this paper, we propose a weakly supervised method to study the scene configuration and attribute localization. [sent-27, score-0.605]
</p><p>11 1, our approach begins with a collection of images with attributes in text (Fig. [sent-29, score-0.317]
</p><p>12 The training images are labeled with the presence of several attributes, with the precise localization of the attributes left unknown. [sent-31, score-0.354]
</p><p>13 Through a learning-by-parsing strategy, we can learn the HST/AOT model and a scene part dictionary, in which each scene part corresponds to a meaningful region in the scenes such as sky, building, road, field. [sent-42, score-0.291]
</p><p>14 (b) Iterative learning process including the learning of scene configuration and attribute association, and the joint inference (text in square brackets denotes the inferred attributes). [sent-51, score-0.646]
</p><p>15 The nouns are assigned to the learned scene part dictionary according to an association matrix as shown in the bottom left of Fig. [sent-54, score-0.434]
</p><p>16 The association matrix measures the probability of a noun and a scene parts appearing simultaneously in the training set and it can be achieve by a non-maximum suppression. [sent-56, score-0.737]
</p><p>17 Each noun has a mixture of appearance models corresponding to the adjectives, e. [sent-57, score-0.439]
</p><p>18 Given an image, we jointly infer the optimal parse tree and localize the semantic attributes to the scene parts by dynamic programming (right panel in Fig. [sent-62, score-0.725]
</p><p>19 Then based on the inferred parse trees, we re-estimate the HST/AOT model and attribute association matrix. [sent-64, score-0.726]
</p><p>20 Thus, we integrate the parsing and attribute localization under an uniform framework. [sent-65, score-0.506]
</p><p>21 We evaluate the proposed method by showing: (i) The semantic attributes are properly associated with the local scene parts. [sent-66, score-0.384]
</p><p>22 (ii) Compared with traditional classification algorithms, our method achieves better attribute recognition performance. [sent-67, score-0.375]
</p><p>23 (iii) We improve the precision of attribute localization against a baseline sliding window method [10]. [sent-68, score-0.569]
</p><p>24 (iv) The most related work is the Hierarchical Space Tiling (HST) [17] which introduced a scene hierarchy by the And-Or Tree (AOT) and proposed a structure learning method to learn a scene part dictionary and compact HST model. [sent-75, score-0.357]
</p><p>25 We extend [17] to take raw images with text as input and associate scene attributes to the learned scene part dictionary. [sent-77, score-0.577]
</p><p>26 Scene attributes Beyond recognizing an individual scene category, visual attributes are demonstrated as valuable semantic cues in various problems such as generating descriptions of unfamiliar objects [6]. [sent-78, score-0.727]
</p><p>27 Patterson and Hays [7] proposed an attribute based scene representation containing 102 binary attributes to describe the intra-class scene variations (e. [sent-79, score-0.889]
</p><p>28 These attributes were learned and inferred at the image level without localization. [sent-87, score-0.288]
</p><p>29 In contrast, we jointly parse the im-  ages into spatial configurations and localize the attributes, which allows us to provide more accurate and detailed descriptions. [sent-88, score-0.26]
</p><p>30 Attributes localization In learning the relationships between the attributes and specific image regions, we relate to the recent work on object detection and localization. [sent-89, score-0.378]
</p><p>31 scene configurations, and (ii) HST-att which models the appearance types of the scene attributes and the correlations between the scene parts and attributes. [sent-104, score-0.711]
</p><p>32 The terminal nodes VT, form a scene part dictionary Δ = VT. [sent-112, score-0.501]
</p><p>33 hA c enlul imsb seeer no fa sth aen aattoommiicc elements compose the higher-level terminal nodes at different scales, locations and shapes. [sent-114, score-0.347]
</p><p>34 Beyond HST-geo, we combine the scene attributes to represent both the geometry and semantics of the scenes. [sent-118, score-0.384]
</p><p>35 Scene attributes come from the text descriptions of training  images which contain several noun+adjective phrases. [sent-119, score-0.406]
</p><p>36 The nouns correspond to the objects in the scenes and the adjectives correspond to the appearance. [sent-120, score-0.244]
</p><p>37 And each terminal node in HST-geo can link to a noun and further an adjective attribute by an association matrix. [sent-123, score-1.5]
</p><p>38 The HST is naturally recursive, starting from a root which is an Or-node, generating the alternating levels of And-nodes and Or-nodes, and stopping at the terminal nodes with a specific appearance type (noun+adjective). [sent-127, score-0.353]
</p><p>39 The And-Or structure defines a space of possible parse trees and embodies probabilistic context free grammar (PCFG) [15]. [sent-128, score-0.253]
</p><p>40 By selecting the branches at Or-nodes, a parse tree pt is derived, e. [sent-129, score-0.388]
</p><p>41 2 represents two parse trees as instances of the HST. [sent-132, score-0.218]
</p><p>42 When parse trees collapse to the image lattice, they produce configurations. [sent-133, score-0.218]
</p><p>43 In the learning process, we maximize the likelihood subject to a model complexity and prune out the branches with zero or low probability to obtain a compact HST and the scene part dictionary. [sent-135, score-0.211]
</p><p>44 VT is a set of terminal nodes forming the scene part dictionary Δ = VT. [sent-141, score-0.501]
</p><p>45 sTeh ae learning requires us ttioo enst siemta Cte = =the { branching probabilities Θ and scene part dictionary Δ by maximizing a log-likelihood. [sent-225, score-0.282]
</p><p>46 ∈VpTt  where VpOtR, VpTt denote the Or-nodes and terminal nodes in the pt, and is the parameter to balance the two terms (λ = 0. [sent-241, score-0.322]
</p><p>47 Cvk denotes the segmented patch covered by the terminal node v. [sent-243, score-0.282]
</p><p>48 ,  λ  EOR(vi|v) = −lnθ(v → vi) The energy for a terminal node is defined as  ET(Ckv|v) = −ln|C1vk|i∈? [sent-247, score-0.282]
</p><p>49 n Idn nl tkvh ies kth-teh d loamyeinr,a lnt label of the terminal node v. [sent-252, score-0.282]
</p><p>50 The first term measures the homogeneity of the terminal nodes in terms of segmentation labels and the second term penalizes large k. [sent-253, score-0.349]
</p><p>51 2, we adopt an iteratively learning-by-parsing strategy including: (i) inferring the optimal parse tree pt by dynamic programming (optimize Eq. [sent-255, score-0.354]
</p><p>52 Then we collect the terminal nodes from all the parse trees to form a scene part dictionary Δ. [sent-259, score-0.719]
</p><p>53 Hence, the terminal nodes are allowed to be locally adjustable to fit the scene region boundaries. [sent-262, score-0.452]
</p><p>54 Learning for the HST-att The text descriptions usually contain noun+adjective phrases: The nouns indicate objects/regions inside a scene (e. [sent-267, score-0.397]
</p><p>55 i Ls etht eA noun Aattr,iAbute }se dt annodte eA thaedj tistthrieb adjective hatetrriebu Ate siest . [sent-275, score-0.703]
</p><p>56 t We explore the relationship between a noun a ∈ An and a scene part v ∈ th Δe r by an nasshsoipc biaetitowne emna atrni xo:u  Φ : An  Δ? [sent-276, score-0.538]
</p><p>57 ∈An  Φ  where the entries of the rows in are the noun attributes and the columns are the scene parts, and we normalize each  ×  columns to be one. [sent-281, score-0.792]
</p><p>58 1, each training image has an optimal parse tree pt. [sent-284, score-0.226]
</p><p>59 Because the attributes are annotated at the image level rather than the precise image regions, we initialize Φ by counting all the combinations of the nouns and the terminal nodes in pt: ? [sent-285, score-0.691]
</p><p>60 1  ·[v ∈ ptm] · φm(a,v)  (7)  where Anm ⊆ An is the noun attribute set for an image, and φm (a, v) d⊆eno Ates its association probability initialized by φm (a, v) = 1. [sent-289, score-0.946]
</p><p>61 3 is the suppression parameter; (ii) suppress the association between the selected node with other noun attributes: φm (a, v∗) = s φm (a, v∗) ; a ∈ Anm\a∗ , I ∈  I˜,  a∗ v∗:  v∗  ? [sent-296, score-0.632]
</p><p>62 4 (left) shows the association of noun attributes and scene parts, where the horizontal axis denotes the nodes in HST-geo and the vertical axis denotes the normalized association probability. [sent-365, score-1.162]
</p><p>63 For example, “sky” has highly probability with the nodes covering the top area of an image and “horse” has highly probability with the nodes covering the middle area of an image. [sent-366, score-0.226]
</p><p>64 To qualitatively evaluate the  association, for each noun attribute, we average the image patches assigned to it. [sent-367, score-0.408]
</p><p>65 4 (right), although learning in a weakly supervised way, our association shows the similar spatial priors of the object categories with [4] (see Fig. [sent-369, score-0.232]
</p><p>66 5 shows the image patches assigned to each noun are then split into multiple clusters according to the given adjectives. [sent-372, score-0.408]
</p><p>67 And we train a binary SVM classifier for each noun+adjective attribute based on those image patches using color histogram feature and SIFT bag-of-words feature. [sent-373, score-0.375]
</p><p>68 Joint inference and learning Take the learned HST-geo and association matrix Φ as an initialization, we infer pt+ ={pt,A} to simultaneously achieve the optimal scene configuration pt an sidm auttltraibnueoteu assignment A={An,Aadj }, then re-estimate HST-geo and Φ. [sent-375, score-0.479]
</p><p>69 8) 3 Jointly infer pt+ with attribute localization (optimize Eq. [sent-383, score-0.5]
</p><p>70 The second term measures the noun attribute association: En(an |v) = −ln Φ(an, v) (10) The third term is designed to model the co-occurrence of a noun and an adjective attribute Ea(aadj|an) = −lnp(aadj|an) where p(aadj|an)  =  ? [sent-401, score-1.861]
</p><p>71 oun and an adjective and can be counted from the given text phrases. [sent-404, score-0.358]
</p><p>72 9, the dynamic programming algorithm can be employed to infer the optimal parse tree with the attributes (pt+ )∗ = arg minpt+ E(pt+ , I; Θ, Δ, Φ) . [sent-411, score-0.536]
</p><p>73 Moreover, some attribute types in [7], such as functions and affordances (e. [sent-561, score-0.375]
</p><p>74 [6] proposed the CORE dataset including 2,800 images with segmentations and attribute annotations for vehicles and animals. [sent-570, score-0.403]
</p><p>75 1 Finally, we got the attribute set An={sky, flower, mountain, ibis, horse. [sent-581, score-0.4]
</p><p>76 }in,s A 17 noun uaet-, tcrliobuudteys, raoncdk y3,0 s noun+adjective awtthriibchut ceo pairs sin 1 7to ntaol. [sent-590, score-0.408]
</p><p>77 For the testing set, we also ask peo-  ×  ple to localize the attributes through bounding box Bgdth as ground truth for evaluating the part localization accuracy, as it is shown at the bottom right panel of Fig. [sent-594, score-0.408]
</p><p>78 Attribute Recognition Baselines We first compare our method in attribute recognition, which evaluates the accuracy of an attribute presence in images. [sent-598, score-0.75]
</p><p>79 (iii) HST-geo: To evaluate the contribution of attribute association, we also compare our method with HST-geo [17]. [sent-604, score-0.375]
</p><p>80 Specifically, for a given image, we first parse it from its multi-layer segmentation and classify each terminal nodes in the parse tree by the classifiers trained in (i). [sent-605, score-0.725]
</p><p>81 7 shows the average precision (AP) for classifying each attribute and the mean average precision (MAP) for the entire attribute set is reported in Table. [sent-607, score-0.812]
</p><p>82 BoW+SPM shows lower performance because the lack of color feature which is a strong cue in scene attribute recognition. [sent-609, score-0.505]
</p><p>83 Attribute Localization Baselines For attribute localization, we benchmark our method against a fully supervised sliding window method (SW-FS) [10]. [sent-614, score-0.466]
</p><p>84 SW-FS trains an attribute classifier using ground truth bounding boxes as positive examples and random rectangles from each negative image for negative data. [sent-615, score-0.403]
</p><p>85 By treating localization as localized detection, the SW-FS applies attribute classifiers subsequently to sub-images at 1http://www. [sent-616, score-0.475]
</p><p>86 The detected sub-windows is ordered by the classification score and taken as indications for the presence of an attribute in this region by nonmaximum suppression with 0. [sent-624, score-0.44]
</p><p>87 In addition, we also compare with HST-geo for evaluating the attribute association. [sent-626, score-0.375]
</p><p>88 Without the geometric constraint, (i) Certain attributes will be confused by appearance (e. [sent-629, score-0.285]
</p><p>89 8(a) shows the attributed parse trees and configurations generated from the joint inference and Fig. [sent-636, score-0.298]
</p><p>90 We quantitatively evaluate the attribute localization performance by following the procedure adopted in [18]. [sent-638, score-0.475]
</p><p>91 The average precisions (AP) for each attribute are shown in Fig. [sent-650, score-0.375]
</p><p>92 3 shows a surprising improvement of attribute localization of our method. [sent-653, score-0.475]
</p><p>93 Discussion and future work  This paper presents a weakly supervised method for learning the scene configurations with attribute localizations. [sent-655, score-0.65]
</p><p>94 (i) We quantize the space of scene configurations by an Hierarchical Space Tiling (HST) and utilize a learningby-parsing strategy to do parameter estimation; (ii) We discover the relationship between the scene parts and attributes Table 2. [sent-656, score-0.603]
</p><p>95 The attribute localization performance  (nouns and adjectives) by an association matrix; (iii) We joint infer the scene configuration and attribute localization by dynamic programming. [sent-665, score-1.304]
</p><p>96 The attributes used in this paper are related to local object and regions, but there are also global attributes (style of the whole parse tree) such as aesthetics, which we are studying in ongoing work by extending our model to an attribute grammar. [sent-667, score-1.086]
</p><p>97 Automatic attribute discovery and characterization from noisy web images. [sent-685, score-0.375]
</p><p>98 Nonparametric scene parsing: label transfer via dense scene alignment. [sent-691, score-0.26]
</p><p>99 Sun attribute database: discovering, annotating, and recognizing scene attributes. [sent-709, score-0.505]
</p><p>100 (c) More attribute  localization results from our method. [sent-827, score-0.475]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('noun', 0.408), ('attribute', 0.375), ('adjective', 0.295), ('hst', 0.295), ('attributes', 0.254), ('terminal', 0.232), ('aadj', 0.19), ('parse', 0.177), ('association', 0.14), ('scene', 0.13), ('pt', 0.128), ('nouns', 0.115), ('sky', 0.114), ('localization', 0.1), ('adjectives', 0.098), ('nodes', 0.09), ('descriptions', 0.089), ('anm', 0.084), ('tiling', 0.078), ('bgdth', 0.063), ('ckernel', 0.063), ('cvk', 0.063), ('vptt', 0.063), ('ck', 0.063), ('text', 0.063), ('ii', 0.058), ('cloudy', 0.056), ('vand', 0.056), ('configurations', 0.053), ('branching', 0.053), ('node', 0.05), ('tree', 0.049), ('dictionary', 0.049), ('eor', 0.047), ('outdoor', 0.044), ('iii', 0.043), ('kulkarni', 0.043), ('axlogp', 0.042), ('canyon', 0.042), ('overcast', 0.042), ('tangram', 0.042), ('vpot', 0.042), ('vpttm', 0.042), ('trees', 0.041), ('weakly', 0.04), ('sliding', 0.04), ('ucla', 0.04), ('mil', 0.038), ('axm', 0.037), ('iv', 0.037), ('bow', 0.036), ('parts', 0.036), ('grammar', 0.035), ('spm', 0.035), ('vor', 0.035), ('suppression', 0.034), ('inferred', 0.034), ('branches', 0.034), ('vi', 0.033), ('ocean', 0.033), ('lnp', 0.033), ('configuration', 0.032), ('ln', 0.032), ('scenes', 0.031), ('lik', 0.031), ('aot', 0.031), ('nonmaximum', 0.031), ('precision', 0.031), ('parsing', 0.031), ('arg', 0.031), ('appearance', 0.031), ('localize', 0.03), ('hierarchical', 0.03), ('patterson', 0.03), ('captioned', 0.03), ('hays', 0.03), ('ea', 0.029), ('ordonez', 0.029), ('acting', 0.029), ('segmentations', 0.028), ('rectangles', 0.028), ('reconfigurable', 0.028), ('supervised', 0.028), ('homogeneity', 0.027), ('joint', 0.027), ('probabilities', 0.026), ('layouts', 0.026), ('excessive', 0.026), ('blue', 0.026), ('studying', 0.026), ('vt', 0.025), ('infer', 0.025), ('got', 0.025), ('compose', 0.025), ('optimize', 0.024), ('learning', 0.024), ('panel', 0.024), ('hierarchy', 0.024), ('probability', 0.023), ('window', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="461-tfidf-1" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>2 0.33102846 <a title="461-tfidf-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.27170837 <a title="461-tfidf-3" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>4 0.23938777 <a title="461-tfidf-4" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>5 0.21590045 <a title="461-tfidf-5" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>Author: Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy</p><p>Abstract: A number of computer vision problems such as human age estimation, crowd density estimation and body/face pose (view angle) estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalarvalued output. Such a learning problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated. Encouraged by the recent success in using attributes for solving classification problems with sparse training data, this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. More precisely, low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation (a label) that captures how the scalar output value (e.g. age, people count) changes continuously and cumulatively. Extensive experiments show that our cumulative attribute framework gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models, especially when the labelled training data is sparse with imbalanced sampling.</p><p>6 0.20929083 <a title="461-tfidf-6" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>7 0.2042508 <a title="461-tfidf-7" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>8 0.20353281 <a title="461-tfidf-8" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>9 0.20132656 <a title="461-tfidf-9" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>10 0.18562493 <a title="461-tfidf-10" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>11 0.17524554 <a title="461-tfidf-11" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>12 0.15950923 <a title="461-tfidf-12" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>13 0.1548934 <a title="461-tfidf-13" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>14 0.15055676 <a title="461-tfidf-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.14555568 <a title="461-tfidf-15" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>16 0.13235041 <a title="461-tfidf-16" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>17 0.13080314 <a title="461-tfidf-17" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>18 0.1272772 <a title="461-tfidf-18" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>19 0.11762629 <a title="461-tfidf-19" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>20 0.11039451 <a title="461-tfidf-20" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.137), (2, -0.038), (3, -0.043), (4, 0.136), (5, 0.116), (6, -0.244), (7, 0.143), (8, 0.084), (9, 0.243), (10, -0.046), (11, 0.142), (12, -0.072), (13, 0.014), (14, 0.088), (15, 0.04), (16, -0.027), (17, 0.023), (18, -0.028), (19, 0.06), (20, 0.024), (21, 0.066), (22, 0.041), (23, 0.014), (24, 0.043), (25, -0.002), (26, 0.043), (27, -0.038), (28, -0.061), (29, 0.085), (30, -0.073), (31, 0.042), (32, 0.07), (33, 0.003), (34, -0.035), (35, -0.041), (36, -0.028), (37, -0.003), (38, 0.004), (39, -0.055), (40, 0.024), (41, 0.05), (42, -0.014), (43, -0.091), (44, -0.046), (45, 0.018), (46, -0.046), (47, 0.022), (48, 0.069), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9510631 <a title="461-lsi-1" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>2 0.83904755 <a title="461-lsi-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.82910162 <a title="461-lsi-3" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>4 0.82262111 <a title="461-lsi-4" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>5 0.81597394 <a title="461-lsi-5" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>6 0.80619293 <a title="461-lsi-6" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>7 0.76651883 <a title="461-lsi-7" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>8 0.73512888 <a title="461-lsi-8" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>9 0.66298193 <a title="461-lsi-9" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>10 0.65976697 <a title="461-lsi-10" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>11 0.6487233 <a title="461-lsi-11" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>12 0.64570111 <a title="461-lsi-12" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>13 0.57384622 <a title="461-lsi-13" href="./cvpr-2013-Bayesian_Grammar_Learning_for_Inverse_Procedural_Modeling.html">57 cvpr-2013-Bayesian Grammar Learning for Inverse Procedural Modeling</a></p>
<p>14 0.55816239 <a title="461-lsi-14" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>15 0.54226255 <a title="461-lsi-15" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>16 0.47565293 <a title="461-lsi-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.46272194 <a title="461-lsi-17" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>18 0.45328778 <a title="461-lsi-18" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>19 0.44568667 <a title="461-lsi-19" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>20 0.44546357 <a title="461-lsi-20" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.088), (16, 0.021), (21, 0.235), (26, 0.038), (28, 0.01), (33, 0.287), (39, 0.043), (63, 0.011), (67, 0.049), (69, 0.037), (77, 0.011), (80, 0.011), (87, 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89048105 <a title="461-lda-1" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>2 0.88367009 <a title="461-lda-2" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>same-paper 3 0.85301304 <a title="461-lda-3" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>4 0.8440755 <a title="461-lda-4" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>Author: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar</p><p>Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In thispaper wepresent an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.</p><p>5 0.8382405 <a title="461-lda-5" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>6 0.81684691 <a title="461-lda-6" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>7 0.80339533 <a title="461-lda-7" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>8 0.8016575 <a title="461-lda-8" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>9 0.79705107 <a title="461-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.79648209 <a title="461-lda-10" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>11 0.79621285 <a title="461-lda-11" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>12 0.79602462 <a title="461-lda-12" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>13 0.79456961 <a title="461-lda-13" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>14 0.79440176 <a title="461-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.79405564 <a title="461-lda-15" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>16 0.793917 <a title="461-lda-16" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>17 0.79372644 <a title="461-lda-17" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>18 0.79368579 <a title="461-lda-18" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>19 0.79364842 <a title="461-lda-19" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>20 0.79334259 <a title="461-lda-20" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
