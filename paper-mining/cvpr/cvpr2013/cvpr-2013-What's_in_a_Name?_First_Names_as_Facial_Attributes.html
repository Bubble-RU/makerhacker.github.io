<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>463 cvpr-2013-What's in a Name? First Names as Facial Attributes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-463" href="#">cvpr2013-463</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>463 cvpr-2013-What's in a Name? First Names as Facial Attributes</h1>
<br/><p>Source: <a title="cvpr-2013-463-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Whats_in_a_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>Reference: <a title="cvpr-2013-463-reference" href="../cvpr2013_reference/cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. [sent-11, score-1.102]
</p><p>2 We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. [sent-12, score-0.701]
</p><p>3 We use the scores from each pairwise name classifier as a set of facial attributes. [sent-15, score-0.678]
</p><p>4 Our name attributes predict the correct first names of test faces at rates far greater than chance. [sent-17, score-1.32]
</p><p>5 The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet. [sent-18, score-1.169]
</p><p>6 , ethnicity, socio-economic background, popularity of names, names of relatives and friends). [sent-26, score-0.682]
</p><p>7 Consequently, first names are not distributed at random among the people in a society. [sent-27, score-0.685]
</p><p>8 Taking these examples further, specific first names vary in prevalence even within a race. [sent-29, score-0.653]
</p><p>9 For example, though both of the following names are primarily Caucasian, the name “Anthony” has an Italian origin, and the name “Sean” has an Irish origin. [sent-30, score-1.607]
</p><p>10 We might ex-  (a) Alejandra  (b) Heather (c) Ethan Figure 1: Face examples of 2 female and 1 male names and their average faces computed from 280 aligned faces. [sent-31, score-0.857]
</p><p>11 , inferring the gender, age and ethnicity by guessing likely names of a face). [sent-44, score-0.967]
</p><p>12 Our contributions are the following: First, we present the first treatment of first names as a facial attribute. [sent-48, score-0.781]
</p><p>13 Second, we show that our model is surprisingly accurate, guessing the correct first name at a rate greater than 4× the expected rtahend coomrr assignment e(an adt a greater tehaatner r2 t×ha inf gender  iesx pasescutemde rda ntod kmno awssnig) nfrmoemn a pool roeaf 1e0r0 th achno 2ic×es i. [sent-50, score-0.88]
</p><p>14 f gTenhdirde,r we show applications using names as attributes for state-ofthe-art facial gender and age classification that require no manually labeled training images. [sent-51, score-1.487]
</p><p>15 In our work, first names are treated as attributes, and the representation implicitlyjointly models the age, gender, race, and other (possibly unnamed) appearance attributes associated with the people having that first name (Figure 1). [sent-59, score-1.275]
</p><p>16 In [10], names from captions are matched to the faces in the image based on attributes of age and gender (derived from facial analysis from images, and from records of name popularity over time). [sent-62, score-2.026]
</p><p>17 In this paper, we extend attributes far beyond the simple modeling of faces using gender and age attributes, to an appearance model of what distinguishes first names from one another. [sent-63, score-1.411]
</p><p>18 Our work is the first attempt of modeling the relation between facial appearance and first names from a computer vision perspective. [sent-64, score-0.8]
</p><p>19 At a first glance, it might seem odd to expect that learning appearance models for different first names would be a fruitful strategy for facial appearance modeling. [sent-66, score-0.819]
</p><p>20 First, it shows that names matter and affect the lives of the people to whom they are assigned [7, 14, 16, 23, 26, 25]. [sent-68, score-0.685]
</p><p>21 Second, people themselves employ stereotypical models for names that even affect their perception of attractiveness and appearance [12, 21]. [sent-69, score-0.734]
</p><p>22 Building on the findings from social psychology studies, in this work, we also demonstrate the power of first name attributes via a  series of facial analysis experiments. [sent-70, score-0.82]
</p><p>23 Further, [7] shows that first names associated with lower socio-economic status (e. [sent-74, score-0.653]
</p><p>24 , names with an apostrophe, with a high “Scrabble score”, or having other attributes) result in both lower standardized test scores and lower teacher expectations, even after using sibling pairs to control for race and socio-economic status. [sent-76, score-0.681]
</p><p>25 People disproportionately choose spouses with names similar to their own [14]. [sent-81, score-0.653]
</p><p>26 People have careers and states of residence that are similar in sound to their names at disproportionate rates [26]. [sent-83, score-0.691]
</p><p>27 This line of work is extended to towns of residence and street names in [25]. [sent-85, score-0.675]
</p><p>28 Those photos assigned desirable names (at the time, Kathy, Christine, or Jennifer) were rated as more attractive than those assigned less desirable names (Ethel, Harriet, or Gertrude) even though the photographs were ranked as equally attractive when no names were assigned. [sent-88, score-2.033]
</p><p>29 In another relevant study [21], subjects first used facial manipulation software to produce stereotypical face images for 15 common male names (e. [sent-89, score-0.985]
</p><p>30 Additional subjects are able to identify the prototype names for each face at rates far above random guesses (10. [sent-92, score-0.759]
</p><p>31 Name100: A First Name Face Dataset To model the relation between names and appearance, we assembled a large dataset by sampling images and tags from Flickr. [sent-98, score-0.653]
</p><p>32 The dataset contains 100 popular first names based on the statistics from the US Social Security Administration (SSA) [2], with 800 faces tagged for each name. [sent-99, score-0.735]
</p><p>33 The 100 names were selected as follows: First, we ranked the names from the SSA database in order of the total number  of times each name was used between 1940 and the present. [sent-100, score-1.812]
</p><p>34 Then, the top names for males and females were found. [sent-101, score-0.703]
</p><p>35 In turn, first names were used as a Flickr query, and names for which enough (≥ 800) image examples were found were kept ihn etnheo udgahtas (≥et. [sent-102, score-1.306]
</p><p>36 8T0h0e) completed mdatpalseest winecrelud feosu n4d8 mwaerlee names, 48 female names, and 4 neutral (a name held by both males and females) names to model the real-world distribution of names. [sent-103, score-1.214]
</p><p>37 Second, we filter out images that are tagged with any of 4717 celebrity names that could bias the sampling. [sent-109, score-0.669]
</p><p>38 For each pair offirst names, we then build a Support Vector Machine (SVM) [4] classifier to discriminate between that pair of names (more details on classifier construction are in Section 5). [sent-121, score-0.703]
</p><p>39 Using  N×(2N−1)  these pairwise name classifiers, a test face can then be described by a vector of dimensions, each being an SVM output score indicating whether the name of the face is more likely to be the first or the second in the name pair. [sent-127, score-1.673]
</p><p>40 The pairwise name attributes establish the link between a face and the names that best fit its appearance, which naturally leads to many interesting applications as we describe in Section 6. [sent-129, score-1.362]
</p><p>41 We show that our system accomplishes the obvious task, guessing the first name of a person, at rates far superior to random chance, even after accounting for the effects of age and gender. [sent-130, score-0.803]
</p><p>42 We then describe an application of gender classification based on our pairwise name attributes, which achieves state-of-the art performance. [sent-131, score-0.874]
</p><p>43 Further, we demonstrate that the pairwise name attributes are very effective on the task of age classification. [sent-132, score-0.863]
</p><p>44 The pairwise name classifiers outputs confidence scores which we call pairwise name attribute vector, which can be used for many applications as we will show Section 6. [sent-140, score-1.146]
</p><p>45 Second, because our system is driven by first names as attributes, we avoid semantic issues related to attribute tagging (e. [sent-142, score-0.703]
</p><p>46 Although, for now, we explore the popular first names from the United States, extending the system to other cultures is as easy as performing additional image downloads with additional name queries as search terms. [sent-146, score-1.13]
</p><p>47 However, performing classification in such a high dimensional feature space is susceptible to overfitting, especially on our challenging classification task of assigning first names to faces. [sent-150, score-0.681]
</p><p>48 On average, the pairwise name classifiers perform quite well at distinguishing between first names as shown in Table 1. [sent-180, score-1.235]
</p><p>49 As expected, it is easier to classify between names that differ in gender. [sent-181, score-0.653]
</p><p>50 We first  show that the name models are surprisingly accurate on the task of first name prediction, then raise novel applications that utilize names for gender and age classification. [sent-187, score-2.186]
</p><p>51 Even when our predictions are wrong, reasonable names are predicted (e. [sent-191, score-0.694]
</p><p>52 The bottom four rows show the most and least accurate pairwise name classifiers when classifying between two mostly male or two mostly female names. [sent-210, score-0.706]
</p><p>53 First Name Prediction First name predictions name attributes  as follows:  are derived from the pairwise Each first name is associated  with N − 1 pairwise name classifiers. [sent-217, score-2.122]
</p><p>54 The total name margin hfo Nr a particular name ise produced by marginalizing over each associated pairwise name classifier. [sent-218, score-1.493]
</p><p>55 By sorting the first names according to the total name margins, a rank-ordered list of first names is produced. [sent-219, score-1.797]
</p><p>56 Table 2 shows the performance of  our model for guessing first names as a function of the number of names. [sent-243, score-0.703]
</p><p>57 It is because names are not randomly dteirsttr hibaunte rda across people, acnadu many ceosrr aerleat inoonts r exist between given names and various facial features (e. [sent-246, score-1.452]
</p><p>58 names  given  ran-  even nameless attributes [24]). [sent-254, score-0.747]
</p><p>59 To more thoroughly investigate the relationship between names and faces, we examine a baseline of estimating gender and age for the task of name prediction. [sent-255, score-1.709]
</p><p>60 We train gender and age classifiers using the Group Image Dataset [11], a dataset which contains a total of 5,080 images with 28,23 1 faces manually labeled with ground truth gender and coarse age categories (age categories include 0-2, 3-7, 8-12, 1319, 20-36, 37-65, 66+). [sent-257, score-1.267]
</p><p>61 We construct the gender and age classifiers in the exact same manner as we train the name  models, by first extracting max-pooled LLC codes on the face pyramid, then passing the features to MFSVM classifiers and finally marginalizing the outputs from the classifiers. [sent-258, score-1.266]
</p><p>62 Having trained the gender and age classifiers, we use them to predict the gender and age of the faces in our Name100 dataset. [sent-259, score-1.224]
</p><p>63 The gender and age predictions associated with a testing face are not independent of first name, hence considering these features offer a better performance than random guess. [sent-260, score-0.693]
</p><p>64 First names are predicted from gender and age estimates as follows: Considering estimated gender, if a test face is classified as a male, then we make a random guess among the male names. [sent-261, score-1.468]
</p><p>65 Since each name has a birth year probability distribution over time (see Figure 6), the first name is predicted as the name that has the maximum birth probability within the range of predicted birth years. [sent-263, score-1.934]
</p><p>66 We can also combine gender and age, by incorporating the estimated age information to make first name guess only within the subset of names selected by the estimated gender. [sent-264, score-1.74]
</p><p>67 Table 3 compares our name models trained using 640 images/name to the baseline performances achieved by considering estimated age and gender as described above. [sent-265, score-1.056]
</p><p>68 , gender  and age) to learn the relation between names and their facial appearances. [sent-271, score-1.116]
</p><p>69 In other words, our name models capture visual cues beyond just age and gender. [sent-272, score-0.721]
</p><p>70 We additionally evaluated the human performance on guessing first names via Amazon Mechanical Turk. [sent-273, score-0.703]
</p><p>71 0c1 A8l675u9P32dinge-  der and age effects on first name prediction. [sent-276, score-0.721]
</p><p>72 By directly modeling names and faces, we achieve much better performance even when gender and age effects are taken into account. [sent-277, score-1.232]
</p><p>73 As it is unrealistic to ask human to select 1name out of the 100 names, we show a face with 10 possible names, where the names include the correct name and 9 other random names of the same gender in random order. [sent-279, score-2.222]
</p><p>74 Gender Recognition From Names  Using our first name attributes, we are able to construct a state-of-the-art gender classifier by exploiting the fact that many first names have a strong association with gender. [sent-286, score-1.49]
</p><p>75 Our gender classifier works as follows: First, we produce the pairwise name attribute vector for each test face. [sent-288, score-0.935]
</p><p>76 Next, we order the first names by their total name margins as described in Section 6. [sent-289, score-1.13]
</p><p>77 Finally, we classify the gender of the test face as male or female depending on the gender associated with the majority of top 5 names in the ordered list of 100 first names. [sent-291, score-1.579]
</p><p>78 A neutral name is counted as either a male or a female name based on the gender ratio of that name, which is computed with SSA database [2] statistics. [sent-292, score-1.427]
</p><p>79 It is important to again note that our gen-  der classifier uses name models trained with names freely available on the web, and does not require any manually labeled gender training examples. [sent-299, score-1.526]
</p><p>80 n 4 i%t %ionac uracy  Table 4: Without any gender training labels, we perform gender recognition using our name models and achieve state-of-the-art performance. [sent-309, score-1.166]
</p><p>81 We use the statistics from the SSA database to plot the birth year probabilities of several names in Figure 6, where it can be seen that the birth probabilities of names have large fluctuations over the years. [sent-313, score-1.674]
</p><p>82 Thus, once we are able to describe a test face with our first name models, then we can utilize the birth probability of names to predict the age of the face. [sent-315, score-1.607]
</p><p>83 The advantage of such an age-from-names approach is obvious: as with our gender classifier, we again do not require any age ground truth labels to produce a reasonable age classification. [sent-316, score-0.839]
</p><p>84 Our age-from-names classification works by first generating a ranked list of 100 names for a test face (again following Section 6. [sent-317, score-0.814]
</p><p>85 1), using the 4950 pairwise name models trained for first name prediction. [sent-318, score-1.002]
</p><p>86 We also compute the birth year probabilities from 1921 to 2010 for these 100 names, using the SSA baby name database. [sent-319, score-0.716]
</p><p>87 Certainly, the names ranked at the top of the list should be given higher weights for the task of age classification. [sent-320, score-0.94]
</p><p>88 Denoting the birth probability of the i-th ranked name in year j as pi (j), then the birth probability of the ranked 100 names are combined using weighted product:  pcombined(j)  =  ? [sent-322, score-1.528]
</p><p>89 0% accuracy, which is surprisingly good given the fact that we are simply utilizing the age information hidden inside the names and use no other manually labeled information. [sent-342, score-0.897]
</p><p>90 By analyzing the confusion between our first name classifiers and then embedding the first names into a two-dimensional space, we see that visually similar names are placed near one another. [sent-352, score-1.847]
</p><p>91 Katie  visual similarity of the faces having the first names in our dataset. [sent-353, score-0.719]
</p><p>92 Some pairs of first names are easier to distinguish than others. [sent-354, score-0.672]
</p><p>93 , pairs of names that perspective parents were deciding between) should have face populations that appear to be similar, and should be close in our embedding. [sent-357, score-0.768]
</p><p>94 Notice that the horizontal dimension relates to gender (males on the right) and age corresponds to the vertical axis (younger names are near the top). [sent-361, score-1.232]
</p><p>95 Again, we emphasize that this name embedding is produced solely as a by-product of our pairwise name classifiers, and is completely based on the visual similarity between faces having the given names. [sent-363, score-1.089]
</p><p>96 Conclusion In this paper, we consider a new problem of facial processing by modeling the relation between first names and faces. [sent-365, score-0.781]
</p><p>97 We build models for common names and treat first names as attributes for describing the facial appearance. [sent-366, score-1.544]
</p><p>98 We show the surprising result that first names can be correctly inferred at rates far exceeding random chance. [sent-368, score-0.669]
</p><p>99 We have also described several applications of our name attributes, including first name prediction, gender recognition and age classification. [sent-369, score-1.533]
</p><p>100 Our first name attributes representation is powerful for performing various facial analysis tasks, and has the advantage of using name labels that are freely available from the internet. [sent-370, score-1.209]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('names', 0.653), ('name', 0.477), ('gender', 0.335), ('age', 0.244), ('birth', 0.129), ('facial', 0.128), ('attributes', 0.094), ('face', 0.09), ('mfsvm', 0.09), ('male', 0.084), ('year', 0.082), ('social', 0.073), ('faces', 0.066), ('ethan', 0.056), ('llc', 0.055), ('female', 0.054), ('guessing', 0.05), ('pairwise', 0.048), ('psychology', 0.048), ('egotism', 0.045), ('pelham', 0.045), ('classifiers', 0.043), ('ssa', 0.041), ('attribute', 0.036), ('alejandra', 0.034), ('brad', 0.034), ('caucasian', 0.034), ('christina', 0.034), ('heather', 0.034), ('stephanie', 0.034), ('zoe', 0.034), ('people', 0.032), ('guess', 0.031), ('prediction', 0.031), ('pyramid', 0.03), ('younger', 0.03), ('stereotypical', 0.03), ('males', 0.03), ('personality', 0.03), ('ranked', 0.029), ('popularity', 0.029), ('person', 0.028), ('flickr', 0.028), ('classifier', 0.025), ('parents', 0.025), ('svm', 0.025), ('predictions', 0.024), ('david', 0.023), ('carvallo', 0.022), ('natalia', 0.022), ('pcombined', 0.022), ('residence', 0.022), ('chance', 0.022), ('embedding', 0.021), ('gallagher', 0.021), ('administration', 0.02), ('females', 0.02), ('sue', 0.02), ('ethnicity', 0.02), ('zt', 0.02), ('security', 0.02), ('codes', 0.02), ('distinguish', 0.019), ('appearance', 0.019), ('training', 0.019), ('category', 0.019), ('rda', 0.018), ('anthony', 0.018), ('boy', 0.018), ('mike', 0.018), ('born', 0.017), ('confidence', 0.017), ('freely', 0.017), ('predicted', 0.017), ('hair', 0.017), ('jones', 0.017), ('rated', 0.017), ('mds', 0.017), ('tagged', 0.016), ('obvious', 0.016), ('labels', 0.016), ('describing', 0.016), ('rates', 0.016), ('service', 0.015), ('implicit', 0.015), ('query', 0.015), ('workers', 0.014), ('attractive', 0.014), ('probabilities', 0.014), ('classification', 0.014), ('distinguishing', 0.014), ('marginalizing', 0.014), ('expectations', 0.014), ('baby', 0.014), ('race', 0.014), ('test', 0.014), ('list', 0.014), ('tagging', 0.014), ('life', 0.014), ('unrealistic', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="463-tfidf-1" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>2 0.16649844 <a title="463-tfidf-2" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>Author: Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy</p><p>Abstract: A number of computer vision problems such as human age estimation, crowd density estimation and body/face pose (view angle) estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalarvalued output. Such a learning problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated. Encouraged by the recent success in using attributes for solving classification problems with sparse training data, this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. More precisely, low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation (a label) that captures how the scalar output value (e.g. age, people count) changes continuously and cumulatively. Extensive experiments show that our cumulative attribute framework gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models, especially when the labelled training data is sparse with imbalanced sampling.</p><p>3 0.161246 <a title="463-tfidf-3" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>4 0.12613393 <a title="463-tfidf-4" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>5 0.12214994 <a title="463-tfidf-5" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>6 0.11415735 <a title="463-tfidf-6" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>7 0.10546386 <a title="463-tfidf-7" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>8 0.10334308 <a title="463-tfidf-8" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>9 0.096895874 <a title="463-tfidf-9" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>10 0.092187479 <a title="463-tfidf-10" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>11 0.084575646 <a title="463-tfidf-11" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>12 0.081711866 <a title="463-tfidf-12" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>13 0.076970018 <a title="463-tfidf-13" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>14 0.074195184 <a title="463-tfidf-14" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>15 0.073744141 <a title="463-tfidf-15" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>16 0.072413981 <a title="463-tfidf-16" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>17 0.069842041 <a title="463-tfidf-17" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>18 0.068973668 <a title="463-tfidf-18" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>19 0.067824684 <a title="463-tfidf-19" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>20 0.06659732 <a title="463-tfidf-20" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.109), (1, -0.076), (2, -0.04), (3, -0.01), (4, 0.072), (5, 0.039), (6, -0.122), (7, -0.026), (8, 0.156), (9, -0.013), (10, 0.032), (11, 0.01), (12, 0.026), (13, 0.049), (14, 0.047), (15, 0.05), (16, 0.006), (17, 0.022), (18, 0.016), (19, 0.009), (20, -0.035), (21, 0.018), (22, -0.024), (23, 0.042), (24, 0.02), (25, 0.006), (26, 0.01), (27, -0.016), (28, 0.01), (29, -0.031), (30, -0.002), (31, 0.003), (32, 0.008), (33, -0.003), (34, -0.019), (35, -0.058), (36, 0.005), (37, 0.057), (38, 0.057), (39, -0.003), (40, 0.004), (41, 0.009), (42, -0.046), (43, 0.085), (44, 0.028), (45, -0.024), (46, -0.002), (47, -0.001), (48, 0.029), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94215918 <a title="463-lsi-1" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>2 0.68059289 <a title="463-lsi-2" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>3 0.65485585 <a title="463-lsi-3" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>4 0.62491918 <a title="463-lsi-4" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>Author: Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy</p><p>Abstract: A number of computer vision problems such as human age estimation, crowd density estimation and body/face pose (view angle) estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalarvalued output. Such a learning problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated. Encouraged by the recent success in using attributes for solving classification problems with sparse training data, this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. More precisely, low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation (a label) that captures how the scalar output value (e.g. age, people count) changes continuously and cumulatively. Extensive experiments show that our cumulative attribute framework gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models, especially when the labelled training data is sparse with imbalanced sampling.</p><p>5 0.59222835 <a title="463-lsi-5" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>6 0.58841193 <a title="463-lsi-6" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>7 0.56977737 <a title="463-lsi-7" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>8 0.56444508 <a title="463-lsi-8" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>9 0.56307006 <a title="463-lsi-9" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>10 0.55749965 <a title="463-lsi-10" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>11 0.55017889 <a title="463-lsi-11" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>12 0.54652518 <a title="463-lsi-12" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>13 0.53332275 <a title="463-lsi-13" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>14 0.52998245 <a title="463-lsi-14" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>15 0.52704537 <a title="463-lsi-15" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>16 0.51824349 <a title="463-lsi-16" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>17 0.5180065 <a title="463-lsi-17" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>18 0.51709753 <a title="463-lsi-18" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>19 0.51635134 <a title="463-lsi-19" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>20 0.50744826 <a title="463-lsi-20" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.075), (16, 0.017), (19, 0.336), (26, 0.041), (33, 0.199), (67, 0.071), (69, 0.041), (72, 0.017), (77, 0.011), (87, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76931834 <a title="463-lda-1" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>2 0.71404469 <a title="463-lda-2" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>Author: Patrick Lucey, Alina Bialkowski, Peter Carr, Stuart Morgan, Iain Matthews, Yaser Sheikh</p><p>Abstract: In this paper, we describe a method to represent and discover adversarial group behavior in a continuous domain. In comparison to other types of behavior, adversarial behavior is heavily structured as the location of a player (or agent) is dependent both on their teammates and adversaries, in addition to the tactics or strategies of the team. We present a method which can exploit this relationship through the use of a spatiotemporal basis model. As players constantly change roles during a match, we show that employing a “role-based” representation instead of one based on player “identity” can best exploit the playing structure. As vision-based systems currently do not provide perfect detection/tracking (e.g. missed or false detections), we show that our compact representation can effectively “denoise ” erroneous detections as well as enabling temporal analysis, which was previously prohibitive due to the dimensionality of the signal. To evaluate our approach, we used a fully instrumented field-hockey pitch with 8 fixed highdefinition (HD) cameras and evaluated our approach on approximately 200,000 frames of data from a state-of-the- art real-time player detector and compare it to manually labelled data.</p><p>3 0.68998402 <a title="463-lda-3" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>Author: Yun Jiang, Hema Koppula, Ashutosh Saxena</p><p>Abstract: For scene understanding, one popular approach has been to model the object-object relationships. In this paper, we hypothesize that such relationships are only an artifact of certain hidden factors, such as humans. For example, the objects, monitor and keyboard, are strongly spatially correlated only because a human types on the keyboard while watching the monitor. Our goal is to learn this hidden human context (i.e., the human-object relationships), and also use it as a cue for labeling the scenes. We present Infinite Factored Topic Model (IFTM), where we consider a scene as being generated from two types of topics: human configurations and human-object relationships. This enables our algorithm to hallucinate the possible configurations of the humans in the scene parsimoniously. Given only a dataset of scenes containing objects but not humans, we show that our algorithm can recover the human object relationships. We then test our algorithm on the task ofattribute and object labeling in 3D scenes and show consistent improvements over the state-of-the-art.</p><p>4 0.62651861 <a title="463-lda-4" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>5 0.62068915 <a title="463-lda-5" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>Author: Dong Liu, Kuan-Ting Lai, Guangnan Ye, Ming-Syan Chen, Shih-Fu Chang</p><p>Abstract: Late fusion addresses the problem of combining the prediction scores of multiple classifiers, in which each score is predicted by a classifier trained with a specific feature. However, the existing methods generally use a fixed fusion weight for all the scores of a classifier, and thus fail to optimally determine the fusion weight for the individual samples. In this paper, we propose a sample-specific late fusion method to address this issue. Specifically, we cast the problem into an information propagation process which propagates the fusion weights learned on the labeled samples to individual unlabeled samples, while enforcing that positive samples have higher fusion scores than negative samples. In this process, we identify the optimal fusion weights for each sample and push positive samples to top positions in the fusion score rank list. We formulate our problem as a L∞ norm constrained optimization problem and apply the Alternating Direction Method of Multipliers for the optimization. Extensive experiment results on various visual categorization tasks show that the proposed method consis- tently and significantly beats the state-of-the-art late fusion methods. To the best knowledge, this is the first method supporting sample-specific fusion weight learning.</p><p>6 0.6195398 <a title="463-lda-6" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>7 0.60533106 <a title="463-lda-7" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>8 0.58671117 <a title="463-lda-8" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>9 0.58335018 <a title="463-lda-9" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>10 0.57732403 <a title="463-lda-10" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>11 0.57406068 <a title="463-lda-11" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>12 0.57219404 <a title="463-lda-12" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>13 0.57118297 <a title="463-lda-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.57025665 <a title="463-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.57020831 <a title="463-lda-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.57007623 <a title="463-lda-16" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>17 0.56979603 <a title="463-lda-17" href="./cvpr-2013-Bilinear_Programming_for_Human_Activity_Recognition_with_Unknown_MRF_Graphs.html">62 cvpr-2013-Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</a></p>
<p>18 0.56965369 <a title="463-lda-18" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>19 0.56911361 <a title="463-lda-19" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>20 0.56843805 <a title="463-lda-20" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
