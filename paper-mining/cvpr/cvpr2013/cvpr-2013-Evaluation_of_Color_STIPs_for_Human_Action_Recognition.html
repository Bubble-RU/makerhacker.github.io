<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-149" href="#">cvpr2013-149</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-149-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Everts_Evaluation_of_Color_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><p>Reference: <a title="cvpr-2013-149-reference" href="../cvpr2013_reference/cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 van Gemert and Theo Gevers Intelligent Systems Lab Amsterdam University of Amsterdam  Abstract This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). [sent-2, score-0.166]
</p><p>2 Existing STIP-based action recognition approaches operate on intensity representations of the image data. [sent-3, score-0.354]
</p><p>3 Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. [sent-4, score-0.323]
</p><p>4 Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. [sent-5, score-0.262]
</p><p>5 Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. [sent-7, score-0.496]
</p><p>6 Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. [sent-9, score-0.187]
</p><p>7 Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition. [sent-10, score-0.248]
</p><p>8 For this reason, research has shifted from recognizing simple human actions under controlled conditions to more complex activities and events ‘in the wild’ [9]. [sent-15, score-0.167]
</p><p>9 Therefore, in this paper, we focus on low-level representations for recognizing human actions in video. [sent-22, score-0.195]
</p><p>10 Low-level action recognition approaches are typically based on spatio-temporal interest points (STIPs) where image sequences are represented by descriptors extracted locally around STIP detections. [sent-23, score-0.308]
</p><p>11 These spatio-temporal feature detectors and descriptors typically use intensity-only representations of the video data and are therefore sensitive to disturbing illumination conditions such as shadows and highlights. [sent-24, score-0.405]
</p><p>12 In a variety of image matching and object recognition tasks, color descriptors outperform intensity descriptors [2, 19] in the spatial (non-temporal) domain. [sent-26, score-0.417]
</p><p>13 By using color, our approach can extract more temporal variations, since pure chromatic temporal transitions such as e. [sent-28, score-0.3]
</p><p>14 In this paper, we propose to incorporate chromatic representations in the spatio-temporal domain. [sent-34, score-0.289]
</p><p>15 This comprises a reformulation of STIP detectors and descriptors for multichannel video representations. [sent-35, score-0.271]
</p><p>16 For this, videos are repre-  sented in a variety of color spaces exhibiting different levels of photometric invariance. [sent-36, score-0.325]
</p><p>17 By this enhanced modeling of appearance, we aim to increase the quality (robustness and discriminative power) of STIP detectors and descriptors 222888445088  for recognizing human activities in video. [sent-37, score-0.338]
</p><p>18 This is validated through a set of repeatability and recognition experiments on challenging video benchmarks. [sent-38, score-0.209]
</p><p>19 Related Work In the spatial domain, multi-channel photometric invariant feature detectors [16, 20, 21] increase repeatability, entropy, and image categorization over intensity-based detections. [sent-43, score-0.321]
</p><p>20 Based on this, we formulate a family of increasingly invariant photometric representations which are incorporated in multi-channel formulations of spatio-temporal feature detectors and descriptors. [sent-45, score-0.476]
</p><p>21 In contrast to other color-STIPS [15], we improve over standard baselines, use a well-founded representation model and we evaluate detectors and descriptors separately. [sent-46, score-0.188]
</p><p>22 Gabor STIPs are therefore essentially different from Harris STIPs and we develop multichannel formulations for both detectors to study differential as well as raw spatio-temporal image data. [sent-53, score-0.199]
</p><p>23 However, it is shown in [10] that motionbased descriptors are not scalable with respect to the number of action categories. [sent-57, score-0.279]
</p><p>24 2  Spatio-temporal Descriptors  Among the local spatio-temporal descriptors available in literature, the HOG3D descriptor [6] is well-suited for large scale video representation and multi-channel extensions. [sent-62, score-0.284]
</p><p>25 Also, good results in a STIP-based bag-of-features recognition framework using the HOG3D descriptor have been achieved, especially in combination with the Gabor STIP detector [23]. [sent-66, score-0.203]
</p><p>26 Therefore, we derive several multi-channel variants of the HOG3D descriptor and evaluate their performance for realistic human action recognition. [sent-67, score-0.37]
</p><p>27 Another recently proposed video descriptor for human action recognition in web videos is Gist3D [14]. [sent-68, score-0.432]
</p><p>28 The works mentioned above comprise low/medium level approaches to action recognition. [sent-71, score-0.158]
</p><p>29 We contribute by considering  a variety of photometric representations for STIP detection and description for enhancing low-level approaches to action recognition. [sent-77, score-0.452]
</p><p>30 Photometric Representations We model image formation by the dichromatic reflection model [13], f = e(mbcb + mici), (1) where a RGB vector f = (R, G, B)T is the sum ofthe body reflectance color cb with the interface reflection color ci. [sent-79, score-0.236]
</p><p>31 Invariance against highlights (shifts in the signal) can be achieved by representations that cancel out the additive interface reflection term mici. [sent-83, score-0.204]
</p><p>32 Superimposed Harris and Gabor responses for Intensity, Chromatic, Normalized chromatic and Hue on three images of a rotating object on which a strong highlight is present. [sent-87, score-0.236]
</p><p>33 Note the dampened response to the highlight in the invariant channels. [sent-89, score-0.158]
</p><p>34 channels, resulting in intensity O3 and chromatic components O1, O2. [sent-100, score-0.294]
</p><p>35 Based on these formulations, several photometric properties can be derived. [sent-101, score-0.213]
</p><p>36 (1) is canceled out in the formulations of O1 and O2, making the chromatic opponent components invariant to signal shifts such as those caused by (white) highlights. [sent-105, score-0.433]
</p><p>37 The chromatic components are normalized by the intensity O3, canceling out the light source intensity term fro? [sent-107, score-0.38]
</p><p>38 Invariance against both scalings and shifts in the signal is achieved by considering the ratio of chromatic components: OO21. [sent-117, score-0.301]
</p><p>39 We refer to these photometric image representations as I(intensity), C(hromatic), N(ormalized chromatic) and H(ue). [sent-119, score-0.294]
</p><p>40 Therefore, the intensity-normalized representations N and H have a higher level of photometric invariance than C, in which the light source intensity is preserved. [sent-130, score-0.427]
</p><p>41 The lack of discriminative power associated with the chromatic representations C, N and H typically renders them unsuitable for matching and recognition tasks. [sent-132, score-0.375]
</p><p>42 Combinations of intensity and chromatic channels result in IC, IN and IH. [sent-133, score-0.396]
</p><p>43 Note that the three-channel representation IC comprises the original opponent channels [O1, O2 , O3] . [sent-134, score-0.182]
</p><p>44 [21] which prevents opposing color gradient directions to cancel each other out. [sent-144, score-0.236]
</p><p>45 Incorporating increasingly invariant photometric representations has a dramatic effect on the Harris energy. [sent-157, score-0.37]
</p><p>46 Intensity normalization of the chromatic components (N) then causes this response to be dampened, while emphasizing colorful transitions on the object surface. [sent-161, score-0.3]
</p><p>47 As illustrated in figure (1), the IGabor energy is mainly clustered around an incidental highlight, whereas the response-triggering local photometric events become increasingly rare and colorful along with the level of photometric invariance level of the representation. [sent-171, score-0.615]
</p><p>48 Multi-Channel STIP Description The HOG3D [6] descriptor is formulated as a discretized approximation of the full range of continuous directions of the 3D gradient in the video volume. [sent-173, score-0.258]
</p><p>49 Descriptor dimensionality may be reduced by allocating opposing gradient directions to the same orientation bin. [sent-183, score-0.176]
</p><p>50 The descriptor algorithm proceeds by centering a cuboid at the STIP location, which is tessellated into a spatio-temporal grid. [sent-184, score-0.168]
</p><p>51 The dimensionality of an integrated direction-based descriptor is considered default (1D, 360 in this paper), based on which we derive the dimensionality of the other descriptor variants. [sent-188, score-0.308]
</p><p>52 Chromaticity is incorporated in the HOG3D descriptor by considering the representations from section (2) in a multi-channel formulation of the gradient vector g in eq. [sent-190, score-0.26]
</p><p>53 (6)  We also evaluate a single gradient variant where we prevent the effect ofopposing color gradient directions by using tensor mathematics. [sent-197, score-0.255]
</p><p>54 In tensors, opposing directions reinforce each other by summing the gradient orientations as opposed to their directions [21],  g? [sent-198, score-0.196]
</p><p>55 Together with the tensorbased approach, we call this descriptor integration as opposed to concatenation. [sent-206, score-0.164]
</p><p>56 Note that the differences between integration and concatenation of channels do not apply to single-channel descriptors. [sent-208, score-0.169]
</p><p>57 The descriptor variants and their associated dimensionalities are summarized in table (2). [sent-209, score-0.182]
</p><p>58 Experiments We evaluate the multi-channel STIP detectors and descriptors through a set of repeatability and action recognition datasets. [sent-211, score-0.521]
</p><p>59 [4] while reimplementing the HOG3D descriptor of Kl¨ aser et al. [sent-215, score-0.16]
</p><p>60 In this paper, we refrain from such an optimization scheme in order to maintain focus on the integration of chromatic channels. [sent-236, score-0.275]
</p><p>61 We consider the four variants of the multichannel HOG3D descriptor as summarized in table (2). [sent-243, score-0.231]
</p><p>62 The variants are denoted by flagging the descriptor names. [sent-244, score-0.182]
</p><p>63 The first flag denotes whether the descriptor channels are integrated (or otherwise concatenated), whereas the second flag denotes the usage of gradient orientations (as opposed to directions). [sent-245, score-0.36]
</p><p>64 We refrain from gradient approximation based on integral video representations of the partial derivatives as in [6], because this affects the very information that we wish to study. [sent-251, score-0.197]
</p><p>65 In summary, apart from the photometric representations, our HOG3D implementation differs slightly from the original [6] by 1) exact gradient computation, 2) descriptor normalization and 3) spatio-temporal pooling. [sent-255, score-0.417]
</p><p>66 Based on the multi-channel STIP detectors and descriptors, we perform action recognition in a standard bag-of-features learning framework. [sent-257, score-0.254]
</p><p>67 Datasets We measure STIP repeatability on videos from the FeEval dataset [17]. [sent-267, score-0.198]
</p><p>68 Every video is artificially distorted by applying different types of photometric and geometric transformations. [sent-269, score-0.247]
</p><p>69 For an in-depth evaluation of detector and descriptor settings, we use the UCF sports dataset [11]. [sent-274, score-0.225]
</p><p>70 The dataset exhibits 10 sports action categories in 150 videos, all of which are horizontally flipped to increase the dataset size. [sent-275, score-0.209]
</p><p>71 The best performing experimental settings are applied to UCF11 [9] which has 11 human actions in 1200 videos, and its superset UCF50 [10] with 50 human action classes in about 6700 videos. [sent-278, score-0.269]
</p><p>72 STIP repeatability for multi-channel Harris and Gabor detectors based on the considered photometric representations. [sent-296, score-0.426]
</p><p>73 Recognition performance on the UCF sports dataset per photometric representation for varying amounts of Harris (a) and Gabor (b) STIPs. [sent-308, score-0.289]
</p><p>74 Influence of the photometric  representations  on descriptor variants (c). [sent-309, score-0.476]
</p><p>75 A repeatability score is  obtained by considering the detections in the challenge sequence, and computing the relative overlap of the cuboid around the detected STIP location with the corresponding location in the original sequence. [sent-314, score-0.185]
</p><p>76 Nonlinear differential spatio-temporal signal changes are more distinctive than temporal fluctuations only. [sent-318, score-0.169]
</p><p>77 The behavior of the detector in different photometric representations are in line with figure (1). [sent-319, score-0.339]
</p><p>78 As the representation becomes increasingly invariant, repeatability progressively decreases. [sent-320, score-0.181]
</p><p>79 Also, combining the invariants with intensity does not increase repeatability with respect to using intensity only (save marginal improvements for the IC representation). [sent-321, score-0.381]
</p><p>80 Moreover, the IH representation attains much lower repeatability scores than I. [sent-322, score-0.17]
</p><p>81 From here on, the pure chromatic representations are discarded from the experimental batch due to the associated lack of discriminative power and we focus only on I, IC,  IN, IH. [sent-326, score-0.346]
</p><p>82 Color STIP Detector Discriminability For evaluating action recognition performance on the UCF sports dataset, we consider the photometric variants of both the Harris and Gabor detector. [sent-329, score-0.504]
</p><p>83 Direction-based intensity HOG3D (I·,0) descriptors are extracted around multichannel STIP detections (i. [sent-330, score-0.256]
</p><p>84 This is expected because H is associated to the highest level of photometric invariance. [sent-345, score-0.213]
</p><p>85 The primary characterization of Harris STIPs in terms of distinctiveness and sparseness is mainly due to nonlinear fluctuations in the spatio-temporal intensity signal. [sent-352, score-0.161]
</p><p>86 Adding chromatic components to the formulation of the energy function does not drastically alter this characterization. [sent-353, score-0.232]
</p><p>87 While I itself by contains the most important information regarding spatiotemporal signal fluctuations, invariants may prevent the detector to fire on disturbing factors such as highlights and shadows. [sent-355, score-0.256]
</p><p>88 Color STIP Descriptor Discriminability For the following action recognition experiments on the UCF sports dataset, descriptors are extracted around Gabor STIPs as these have shown superior recognition per222888555533  formance over Harris STIPs in figure (2a,b). [sent-361, score-0.388]
</p><p>89 General conclusions about photometric invariance relate to the discriminative power of the descriptors. [sent-365, score-0.317]
</p><p>90 We observe a general preference for direction-based descriptors over orientation-based descriptors (table 2). [sent-368, score-0.242]
</p><p>91 It may even be the case that the (implicit) preservation of opposing gradient directions between channels is informative. [sent-374, score-0.253]
</p><p>92 Furthermore, IC-based descriptors favor channel integration over concatenation, which is not the case for IN- and IH- based descriptors. [sent-375, score-0.194]
</p><p>93 In fact, one would expect concatenation-based descriptors to perform better in general due the enhanced expressiveness associated to multiple channels and increased dimensionality. [sent-376, score-0.281]
</p><p>94 We have not observed a relationship between descriptor dimensionality and codebook size. [sent-395, score-0.207]
</p><p>95 In contrast to these low/medium level action recognition approaches, the high level Action Bank approach of [12] reaches an accuracy of 95% on UCF sports. [sent-396, score-0.187]
</p><p>96 6% for densely sampled I-HOG3D descriptors in [23], which on average yields over 600 descriptors per frame. [sent-398, score-0.242]
</p><p>97 Thus, we conclude that a certain amount of invariance against local photometric events is beneficial for STIP detection, whereas the descriptor should be extracted from the most discriminative representation. [sent-407, score-0.467]
</p><p>98 Color STIP action recognition results on UCF1 1 and UCF50 datasets. [sent-499, score-0.187]
</p><p>99 Conclusion We have reformulated existing STIP detectors and descriptors to incorporate multiple photometric channels, resulting in Color STIPs. [sent-516, score-0.401]
</p><p>100 An evaluation on color invariant based local spatiotemporal features for action recognition. [sent-611, score-0.285]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stips', 0.477), ('stip', 0.471), ('harris', 0.293), ('gabor', 0.22), ('photometric', 0.213), ('chromatic', 0.208), ('action', 0.158), ('repeatability', 0.146), ('descriptor', 0.129), ('descriptors', 0.121), ('channels', 0.102), ('ucf', 0.088), ('intensity', 0.086), ('representations', 0.081), ('opponent', 0.08), ('mbh', 0.074), ('disturbing', 0.07), ('ic', 0.068), ('detectors', 0.067), ('color', 0.06), ('reflection', 0.058), ('opposing', 0.056), ('dampened', 0.054), ('feeval', 0.054), ('stipbased', 0.054), ('bank', 0.054), ('variants', 0.053), ('codebook', 0.053), ('videos', 0.052), ('sports', 0.051), ('actions', 0.051), ('tensor', 0.05), ('gradient', 0.05), ('chromaticity', 0.049), ('multichannel', 0.049), ('invariance', 0.047), ('temporal', 0.046), ('detector', 0.045), ('directions', 0.045), ('polyhedron', 0.044), ('differential', 0.044), ('gevers', 0.041), ('invariant', 0.041), ('signal', 0.041), ('highlights', 0.04), ('formulations', 0.039), ('cuboid', 0.039), ('fluctuations', 0.038), ('channel', 0.038), ('discriminability', 0.037), ('distinctiveness', 0.037), ('disturbances', 0.036), ('everts', 0.036), ('gaabboorr', 0.036), ('ggaabboorr', 0.036), ('mosift', 0.036), ('mvap', 0.036), ('ottinger', 0.036), ('triggers', 0.036), ('integration', 0.035), ('response', 0.035), ('increasingly', 0.035), ('video', 0.034), ('invariants', 0.034), ('enhanced', 0.033), ('recognizing', 0.033), ('ici', 0.032), ('refrain', 0.032), ('colorful', 0.032), ('iinc', 0.032), ('shadows', 0.032), ('concatenation', 0.032), ('aser', 0.031), ('hue', 0.031), ('power', 0.03), ('human', 0.03), ('recognition', 0.029), ('marginal', 0.029), ('lowlevel', 0.028), ('highlight', 0.028), ('amsterdam', 0.028), ('scalings', 0.028), ('discriminative', 0.027), ('doll', 0.027), ('activities', 0.027), ('flag', 0.027), ('spatiotemporal', 0.026), ('events', 0.026), ('combinations', 0.026), ('cancel', 0.025), ('favored', 0.025), ('normalization', 0.025), ('whereas', 0.025), ('amounts', 0.025), ('dimensionality', 0.025), ('gemert', 0.025), ('expressiveness', 0.025), ('energy', 0.024), ('shifts', 0.024), ('attains', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="149-tfidf-1" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><p>2 0.24929547 <a title="149-tfidf-2" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>3 0.2222161 <a title="149-tfidf-3" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>Author: Simon Hadfield, Richard Bowden</p><p>Abstract: Action recognition in unconstrained situations is a difficult task, suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent emergence of 3D data, both in broadcast content, and commercial depth sensors, provides the possibility to overcome this issue. This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use ofthe 3D data, andfive new interestpoint detection strategies are alsoproposed, that extend to the 3D data. Our evaluation compares all 4 feature descriptors, using 7 different types of interest point, over a variety of threshold levels, for the Hollywood3D dataset. We make the dataset including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community.</p><p>4 0.17815885 <a title="149-tfidf-4" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>Author: Feng Shi, Emil Petriu, Robert Laganière</p><p>Abstract: Local spatio-temporal features and bag-of-features representations have become popular for action recognition. A recent trend is to use dense sampling for better performance. While many methods claimed to use dense feature sets, most of them are just denser than approaches based on sparse interest point detectors. In this paper, we explore sampling with high density on action recognition. We also investigate the impact of random sampling over dense grid for computational efficiency. We present a real-time action recognition system which integrates fast random sampling method with local spatio-temporal features extracted from a Local Part Model. A new method based on histogram intersection kernel is proposed to combine multiple channels of different descriptors. Our technique shows high accuracy on the simple KTH dataset, and achieves state-of-the-art on two very challenging real-world datasets, namely, 93% on KTH, 83.3% on UCF50 and 47.6% on HMDB51.</p><p>5 0.1744633 <a title="149-tfidf-5" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>6 0.16438946 <a title="149-tfidf-6" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>7 0.1516898 <a title="149-tfidf-7" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>8 0.14286329 <a title="149-tfidf-8" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>9 0.13346158 <a title="149-tfidf-9" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>10 0.12584303 <a title="149-tfidf-10" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>11 0.12311335 <a title="149-tfidf-11" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>12 0.11579537 <a title="149-tfidf-12" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>13 0.11486252 <a title="149-tfidf-13" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>14 0.10889945 <a title="149-tfidf-14" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>15 0.10572854 <a title="149-tfidf-15" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>16 0.10403369 <a title="149-tfidf-16" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>17 0.10271335 <a title="149-tfidf-17" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>18 0.099429414 <a title="149-tfidf-18" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>19 0.098518968 <a title="149-tfidf-19" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>20 0.097759001 <a title="149-tfidf-20" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, -0.024), (2, -0.008), (3, -0.094), (4, -0.206), (5, -0.036), (6, -0.097), (7, 0.017), (8, -0.058), (9, -0.044), (10, -0.028), (11, -0.102), (12, 0.031), (13, -0.007), (14, 0.045), (15, -0.006), (16, 0.035), (17, -0.048), (18, 0.11), (19, 0.039), (20, 0.079), (21, -0.02), (22, 0.003), (23, 0.065), (24, 0.035), (25, 0.032), (26, 0.042), (27, 0.073), (28, -0.044), (29, -0.031), (30, 0.008), (31, 0.036), (32, 0.034), (33, -0.098), (34, 0.002), (35, -0.018), (36, -0.025), (37, 0.133), (38, 0.05), (39, 0.031), (40, 0.05), (41, 0.017), (42, -0.043), (43, 0.007), (44, 0.013), (45, -0.057), (46, 0.016), (47, 0.088), (48, 0.006), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95251924 <a title="149-lsi-1" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><p>2 0.72445005 <a title="149-lsi-2" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>Author: Feng Shi, Emil Petriu, Robert Laganière</p><p>Abstract: Local spatio-temporal features and bag-of-features representations have become popular for action recognition. A recent trend is to use dense sampling for better performance. While many methods claimed to use dense feature sets, most of them are just denser than approaches based on sparse interest point detectors. In this paper, we explore sampling with high density on action recognition. We also investigate the impact of random sampling over dense grid for computational efficiency. We present a real-time action recognition system which integrates fast random sampling method with local spatio-temporal features extracted from a Local Part Model. A new method based on histogram intersection kernel is proposed to combine multiple channels of different descriptors. Our technique shows high accuracy on the simple KTH dataset, and achieves state-of-the-art on two very challenging real-world datasets, namely, 93% on KTH, 83.3% on UCF50 and 47.6% on HMDB51.</p><p>3 0.70580113 <a title="149-lsi-3" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>Author: Simon Hadfield, Richard Bowden</p><p>Abstract: Action recognition in unconstrained situations is a difficult task, suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent emergence of 3D data, both in broadcast content, and commercial depth sensors, provides the possibility to overcome this issue. This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use ofthe 3D data, andfive new interestpoint detection strategies are alsoproposed, that extend to the 3D data. Our evaluation compares all 4 feature descriptors, using 7 different types of interest point, over a variety of threshold levels, for the Hollywood3D dataset. We make the dataset including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community.</p><p>4 0.70184135 <a title="149-lsi-4" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>Author: Chunfeng Yuan, Xi Li, Weiming Hu, Haibin Ling, Stephen Maybank</p><p>Abstract: Spatio-temporal interest points serve as an elementary building block in many modern action recognition algorithms, and most of them exploit the local spatio-temporal volume features using a Bag of Visual Words (BOVW) representation. Such representation, however, ignorespotentially valuable information about the global spatio-temporal distribution of interest points. In this paper, we propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the ℛ transform which is defined as an extended 3D discrete Rℛa tdroann transform, followed by applying a tewdo 3-dDir decitsicorneatel two-dimensional principal component analysis. Such ℛ feature captures the geometrical information of the Sinuctehre ℛst points and keeps invariant to geometry transformation and robust to noise. In addition, we propose a new fusion strategy to combine the ℛ feature with the BOVW representation for further improving recognition accuracy. Wpree suetnilitzaea context-aware fusion method to capture both the pairwise similarities and higher-order contextual interactions of the videos. Experimental results on several publicly available datasets demonstrate the effectiveness of the proposed approach for action recognition.</p><p>5 0.68401146 <a title="149-lsi-5" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>Author: LiMin Wang, Yu Qiao, Xiaoou Tang</p><p>Abstract: This paper proposes motionlet, a mid-level and spatiotemporal part, for human motion recognition. Motionlet can be seen as a tight cluster in motion and appearance space, corresponding to the moving process of different body parts. We postulate three key properties of motionlet for action recognition: high motion saliency, multiple scale representation, and representative-discriminative ability. Towards this goal, we develop a data-driven approach to learn motionlets from training videos. First, we extract 3D regions with high motion saliency. Then we cluster these regions and preserve the centers as candidate templates for motionlet. Finally, we examine the representative and discriminative power of the candidates, and introduce a greedy method to select effective candidates. With motionlets, we present a mid-level representation for video, called motionlet activation vector. We conduct experiments on three datasets, KTH, HMDB51, and UCF50. The results show that the proposed methods significantly outperform state-of-the-art methods.</p><p>6 0.67223686 <a title="149-lsi-6" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>7 0.65531713 <a title="149-lsi-7" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>8 0.65405571 <a title="149-lsi-8" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>9 0.61541146 <a title="149-lsi-9" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>10 0.59900206 <a title="149-lsi-10" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>11 0.59070915 <a title="149-lsi-11" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>12 0.58955818 <a title="149-lsi-12" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>13 0.57881641 <a title="149-lsi-13" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>14 0.5608359 <a title="149-lsi-14" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>15 0.55353034 <a title="149-lsi-15" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>16 0.53298855 <a title="149-lsi-16" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<p>17 0.53147233 <a title="149-lsi-17" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>18 0.51552546 <a title="149-lsi-18" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>19 0.51390713 <a title="149-lsi-19" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>20 0.49888292 <a title="149-lsi-20" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.121), (16, 0.072), (26, 0.026), (28, 0.024), (33, 0.251), (48, 0.015), (67, 0.081), (69, 0.033), (72, 0.211), (80, 0.016), (87, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87114245 <a title="149-lda-1" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><p>2 0.86103255 <a title="149-lda-2" href="./cvpr-2013-Bilinear_Programming_for_Human_Activity_Recognition_with_Unknown_MRF_Graphs.html">62 cvpr-2013-Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</a></p>
<p>Author: Zhenhua Wang, Qinfeng Shi, Chunhua Shen, Anton van_den_Hengel</p><p>Abstract: Markov Random Fields (MRFs) have been successfully applied to human activity modelling, largely due to their ability to model complex dependencies and deal with local uncertainty. However, the underlying graph structure is often manually specified, or automatically constructed by heuristics. We show, instead, that learning an MRF graph and performing MAP inference can be achieved simultaneously by solving a bilinear program. Equipped with the bilinear program based MAP inference for an unknown graph, we show how to estimate parameters efficiently and effectively with a latent structural SVM. We apply our techniques to predict sport moves (such as serve, volley in tennis) and human activity in TV episodes (such as kiss, hug and Hi-Five). Experimental results show the proposed method outperforms the state-of-the-art.</p><p>3 0.8504262 <a title="149-lda-3" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>Author: Armand Joulin, Sing Bing Kang</p><p>Abstract: An anaglyph is a single image created by selecting complementary colors from a stereo color pair; the user can perceive depth by viewing it through color-filtered glasses. We propose a technique to reconstruct the original color stereo pair given such an anaglyph. We modified SIFT-Flow and use it to initially match the different color channels across the two views. Our technique then iteratively refines the matches, selects the good matches (which defines the “anchor” colors), and propagates the anchor colors. We use a diffusion-based technique for the color propagation, and added a step to suppress unwanted colors. Results on a variety of inputs demonstrate the robustness of our technique. We also extended our method to anaglyph videos by using optic flow between time frames.</p><p>4 0.84554231 <a title="149-lda-4" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>5 0.84051752 <a title="149-lda-5" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>6 0.83971816 <a title="149-lda-6" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>7 0.81031877 <a title="149-lda-7" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>8 0.8039549 <a title="149-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.80145741 <a title="149-lda-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.79817384 <a title="149-lda-10" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>11 0.79739112 <a title="149-lda-11" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>12 0.79674071 <a title="149-lda-12" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>13 0.79663384 <a title="149-lda-13" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>14 0.79622954 <a title="149-lda-14" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>15 0.79602104 <a title="149-lda-15" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>16 0.79570514 <a title="149-lda-16" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>17 0.79509979 <a title="149-lda-17" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>18 0.79501736 <a title="149-lda-18" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>19 0.79500431 <a title="149-lda-19" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>20 0.79468006 <a title="149-lda-20" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
