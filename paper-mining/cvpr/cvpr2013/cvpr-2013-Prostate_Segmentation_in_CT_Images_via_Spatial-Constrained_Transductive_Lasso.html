<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-342" href="#">cvpr2013-342</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</h1>
<br/><p>Source: <a title="cvpr-2013-342-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shi_Prostate_Segmentation_in_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yinghuan Shi, Shu Liao, Yaozong Gao, Daoqiang Zhang, Yang Gao, Dinggang Shen</p><p>Abstract: Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. In this paper, a novel semi-automated prostate segmentation method is presented. Specifically, to segment the prostate in the current treatment image, the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. Then, the prostate is segmented automatically by theproposed two steps: (i) Thefirst step of prostate-likelihood estimation to predict the prostate likelihood for each voxel in the current treatment image, aiming to generate the 3-D prostate-likelihood map by the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) The second step of multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from the planning and previous treatment images. The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset, consisting of 24 patients with 330 images. Moreover, it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification of the first and last slices of the prostate.</p><p>Reference: <a title="cvpr-2013-342-reference" href="../cvpr2013_reference/cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. [sent-3, score-0.866]
</p><p>2 In this paper, a novel semi-automated prostate segmentation method is presented. [sent-4, score-0.829]
</p><p>3 Specifically, to segment the prostate in the current treatment image, the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. [sent-5, score-2.097]
</p><p>4 The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset, consisting of 24 patients with 330 images. [sent-7, score-1.68]
</p><p>5 Moreover, it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification  of the first and last slices of the prostate. [sent-8, score-0.429]
</p><p>6 Introduction According to the data [1] reported from National Cancer Institute, prostate cancer will possibly cause 241740 new cases for U. [sent-10, score-0.819]
</p><p>7 Recently, CT image guided radiotherapy for prostate cancer treatment has attracted lots of research interest, due to its ability in better guiding the delivery of radiation to prostate cancer [15]. [sent-14, score-1.989]
</p><p>8 During the CT image guided radiotherapy, a sequence of CT scans will be acquired from a patient in the planning and treatment days. [sent-15, score-0.629]
</p><p>9 A CT scan acquired in the planning day is called the planning image, and the scans acquired in the subsequent treatment days are called the treatment images. [sent-16, score-0.963]
</p><p>10 Since the locations of prostate might vary in CT scans, the core problem is to accurately determine the location of prostate in the images acquired from different treatment days, which is usually done by the physician with slice-byslice manual segmentation. [sent-17, score-2.047]
</p><p>11 However, manual segmentation can spend a lot of time for each treatment image, i. [sent-18, score-0.435]
</p><p>12 Most importantly, the segmentation results are not consistent across different treatment days due to interand intra- operator variability. [sent-21, score-0.364]
</p><p>13 The major challenging issues to accurately segment prostate in the CT images include: (i) the boundary between prostate and background is usually unclear due to the low contrast in the CT images, e. [sent-22, score-1.516]
</p><p>14 1(a) where the  prostate region is highlighted by the physician using green contour. [sent-25, score-0.938]
</p><p>15 (ii) The locations of the prostate regions scanned at different treatment days are usually different due to the irregular and unpredictable prostate motion, e. [sent-26, score-1.862]
</p><p>16 1(b) where the red and blue contours denote the manual segmentations of the two bone-aligned CT images scanned from two different treatment days for the same patient. [sent-29, score-0.388]
</p><p>17 We can observe the large prostate motion even after bone-based alignment of two scans, indicating the possible large motion of prostate relative to the bones. [sent-30, score-1.546]
</p><p>18 (a) Low contrast in CT image; (b) Large prostate motion relative to the bones, even after bone-based alignment for the two CT images. [sent-32, score-0.773]
</p><p>19 Recently, several prostate segmentation methods for CT image guided radiotherapy have been developed, with the common goal of segmenting the prostate in the current treatment image by borrowing the knowledge learned from the planning and previous treatment images. [sent-33, score-2.391]
</p><p>20 In deformable-model-based methods [6] [11], the prostate shapes learned from the planning and previous treatment images are first used to initialize the deformable model, and then specific optimization strategies are developed to guide prostate segmentation. [sent-35, score-1.954]
</p><p>21 In learning-based methods [14][21], prostate segmentation is first formulated as a prostate-likelihood estimation problem using visual features (e. [sent-37, score-0.829]
</p><p>22 Note that, besides segmentation on CT images, other prostate segmentation methods are also proposed for segmentation of prostate from other imaging modalities such as MR [12][13] and ultrasound [25] images. [sent-42, score-1.729]
</p><p>23 a 2-D slice from CT image  prefer choosing different features. [sent-44, score-0.095]
</p><p>24 In this paper, we propose a novel prostate segmentation method for CT image guided radiotherapy. [sent-46, score-0.866]
</p><p>25 Previous learning-based methods [14][21] first collect the voxels from certain slices, and then conduct both the feature selection and the subsequent prostate-likelihood estimation for all voxels in those selected slices jointly. [sent-47, score-0.631]
</p><p>26 However, different local regions may prefer choosing different features to better discriminate between their own prostate and nonprostate voxels, as indicated by a typical example in Fig. [sent-48, score-0.789]
</p><p>27 In this example, we extracted features for three different local regions, and then apply Lasso (a supervised feature selection technique as introduced in [22]) for the respective feature selections. [sent-50, score-0.107]
</p><p>28 In this paper, we design a novel local learning strategy: partition each 2D slice into several non-overlapping local blocks, and then select the respective local features to predict the prostatelikelihood for each local block. [sent-53, score-0.175]
</p><p>29 a  local feature  segmentation on the current treatment image, the physician only needs to spend a few seconds to specify just the first and last slices of prostate in the CT image. [sent-60, score-1.462]
</p><p>30 By spending this little manual time, the segmentation results can be significantly improved, compared with the fully automatic methods [14][15]. [sent-61, score-0.146]
</p><p>31 The contributions of our proposed method can be summarized into the following two folds: • A novel semi-automatic prostate segmentation method in CT images is proposed. [sent-62, score-0.829]
</p><p>32 For current treatment image, the  information obtained from the planning and previous treatment images of the same patient and also the manual specification of the first and last slices of prostate helps guide the accurate segmentation. [sent-63, score-1.806]
</p><p>33 In the prostate-likelihood estimation step: First, all previous and current treatment images are rigidly aligned to the planning image of the same patient based on the pelvic bone structures, for removing the whole-body patient motion that is irrelevant to prostate segmentation. [sent-70, score-1.623]
</p><p>34 Then, we extract the ROI regions according to the prostate center in the planning image. [sent-71, score-0.936]
</p><p>35 Second, for the current treatment image, physician is required to specify the first and last slices of the prostate in the CT images. [sent-72, score-1.339]
</p><p>36 The proposed SCOTO is applied for joint feature selection for all blocks, and SVR is further adopted to predict the 2D prostate-likelihood map for all the voxels in the current slice. [sent-75, score-0.349]
</p><p>37 Finally, the predicted 2-D prostate-likelihood map of each individual slice will be merged into a 3-D prostatelikelihood map according to the order of their original slices. [sent-76, score-0.171]
</p><p>38 The planning image and its corresponding manual segmentation result are denoted as Ip and Gp, respectively. [sent-84, score-0.31]
</p><p>39 The nth treatment image, which is the current treatment image, is denoted as In. [sent-85, score-0.549]
</p><p>40 The previous treatment images and their corresponding manual segmentation results are denoted as I1, . [sent-86, score-0.42]
</p><p>41 Also, the final 3-D prostate-likelihood map and its segmentation result for the current treatment image In by adopting the proposed method are denoted as Mn and Sn, respectively. [sent-93, score-0.362]
</p><p>42 , In) rigidly to the planning image (Ip) based on their pelvic bone structures. [sent-99, score-0.266]
</p><p>43 For each patient, we first calculate the mass center of the prostate in the planning image Ip, and then extract a large enough 3-D region centered at the calculated mass center. [sent-102, score-0.94]
</p><p>44 When asking physician for manual interaction, we only ask for manual specification of the first and last slices of the prostate along the z-axis. [sent-104, score-1.225]
</p><p>45 In the experiments, we will also show that the segmentation results can be largely improved by asking physician to spend such a little interaction time, which is also clinically feasible. [sent-107, score-0.322]
</p><p>46 Patch-Based Feature Representation: Three differen222222222977  t kinds of features from 2-D slice are extracted, which include 9 histogram of oriented gradient (HoG) [8], 30 local binary pattern (LBP) [20] and 14 multi-resolution Haar wavelet [18]1 . [sent-108, score-0.108]
</p><p>47 The feature vector ofthe current voxel consists ofthe features (9+30+14 = 53  ×  features) extracted from all voxels in the small patch. [sent-112, score-0.345]
</p><p>48 Since the confusing voxels are frequently lying on the boundary of the prostate region, it is reasonable to sample relatively more voxels around the boundary. [sent-115, score-1.23]
</p><p>49 That is, the boundary voxels will have higher probability to be sampled, as illustrated in Fig. [sent-116, score-0.236]
</p><p>50 Thetypicalexamplestoilustraethesamplingofthetrain g  voxels, with the red points denoting the prostate voxels and the blue points denoting the background voxels. [sent-119, score-0.994]
</p><p>51 SCOTO: Problem Formulation For each 2-D slice, our goal is to estimate the prostatelikelihood for each voxel in the current slice. [sent-123, score-0.165]
</p><p>52 Since our feature representation for each voxel is a high dimensional vec-  ×  tor (R1325), the feature selection is significant to avoid the “curse of dimensionality”. [sent-124, score-0.142]
</p><p>53 For each slice, we first partition the slice into non-overlapping Nx Ny blocks as shown in Fig. [sent-125, score-0.119]
</p><p>54 Then for the ith block, we use li ∈ R and ui ∈ R to denote the number of training voxels2 and testing voxels, respectively. [sent-127, score-0.128]
</p><p>55 yi ∈ Rli+ui and Fi ∈ ×d denotes the ground-truth label and feature  R(li+ui)  1HOG: calculated within 3 3 cell blocks with 9 histogram bins similar in [8]. [sent-129, score-0.119]
</p><p>56 2The training voxels come from the sampled voxels, whose locations are in the current block within the slices [sc −1, sc + 1] of training images, where sc is the current slice index of testing voxels in z-axis. [sent-132, score-0.767]
</p><p>57 The reason is that training voxels in adjacent slices have similar distribution in feature space, which guarantees enough voxels are sampled, especially on the base and apex slices of prostate. [sent-133, score-0.697]
</p><p>58 Without loss of generality, all the training voxels are listed before the testing voxels in both yi and Fi. [sent-135, score-0.537]
</p><p>59 It is noteworthy that the labels of testing voxels in yi are set to 0. [sent-139, score-0.325]
</p><p>60 Also in yi, the labels of training voxels are set to 1if they belong to the prostate, and set to 0 if they belong to the background. [sent-140, score-0.236]
</p><p>61 Ji ∈ R(li+ui)×(li+ui), which is used to indicate the training voxels since the testing voxels have no contribution on the first term, is a diagonal matrix defined as  Ji= diag? [sent-164, score-0.49]
</p><p>62 For each individual block, we apply SVR, which is a conventional regression method, to predict the prostatelikelihood for all the voxels in each block. [sent-291, score-0.313]
</p><p>63 Specifically, SVR model is first trained by the training voxels in Fi? [sent-292, score-0.236]
</p><p>64 as well as available labels in yi, and then preformed over the ui testing voxels on the ith block for prediction of prostate. [sent-293, score-0.382]
</p><p>65 It is noteworthy that we will first obtain 2-D prostatelikelihood maps slice by slice, and then merge all the results to get the final 3-D prostate-likelihood map, which is denoted as Mn. [sent-295, score-0.205]
</p><p>66 Multi-Atlases based Label Fusion  ××  To make full use of all the shape information from the planning and previous treatment images for segmentation, we adopt the multi-atlases based label fusion with the following steps: First, previous binary segmentation results G1,. [sent-297, score-0.586]
</p><p>67 Dataset Description and Experimental Setup The proposed method is evaluated on a prostate 3-D CTimage dataset consisting of 24 patients with 330 images, and each patient has at least 9 images obtained from 1planning day and several treatment days. [sent-305, score-1.253]
</p><p>68 All the images of the patients are manually segmented by experienced physician, which are used as ground-truth for evaluation in the experiments. [sent-309, score-0.131]
</p><p>69 , the planning image and the first two treatment images) are used as training images, from which the train-  ing voxels are sampled, and segmentation ground-truths are available. [sent-312, score-0.729]
</p><p>70 The TPF indicates that the percentage of corrected predicted prostate voxels in the manually segmented prostate regions. [sent-315, score-1.788]
</p><p>71 The centroid distance means the Euclidean distance between the central locations of the manual segmentation result and predicted result. [sent-316, score-0.178]
</p><p>72 Since prostate CT-images are 3-D, the CD along 3 directions, including the lateral (x-axis), anterior-posterior (y-axis), and superior-inferior (z-axis) directions, need to be calculated. [sent-317, score-0.776]
</p><p>73 Too large block size will ignore the variations of appearance along the prostate boundary, while too small block size will increase the computational burden. [sent-325, score-0.838]
</p><p>74 It is noteworthy that the same multi-atlases based label fusion is adopted for all the methods. [sent-331, score-0.125]
</p><p>75 1 lists the segmentation accuracies obtained by different feature selection schemes, and the best results are marked by the bold fonts. [sent-333, score-0.153]
</p><p>76 For [11] [15][21], all the 24 patients are evaluated, which is the same with ours, so we name the 24 patients CT dataset as “CT dataset 1”. [sent-359, score-0.186]
</p><p>77 Also, two different subsets of the 24 patients are selected in [14] and [15], which are named as “CT dataset 2” and “CT dataset 3”, respectively. [sent-360, score-0.093]
</p><p>78 8 several typical segmented examples as well as prostate-likelihood map for the image 14 of patient 3, the image 10 of patient 11, the image 5 of patient 16, the image 6 of patient 21 and the image 8 of patient 24, respectively. [sent-378, score-0.64]
</p><p>79 8, the red curves denote the manual segmentation results by the physician, and the yellow curves denote the segmentation results by the proposed methods. [sent-380, score-0.217]
</p><p>80 We found that the predicted prostate boundaries are very close to the boundaries delineated by the physician. [sent-381, score-0.774]
</p><p>81 Also the proposed method can accurately separate the prostate regions and background even in the base and apex slices as shown in Figs. [sent-382, score-0.883]
</p><p>82 Patients with Large Prostate Motion In our work, it is found that the patients 3, 10 and 15 have larger prostate motions according to the standard devi-  (a)Typicalresultsofthe14thimageofpatient3,withDiceratio f0. [sent-386, score-0.851]
</p><p>83 Red curves indicate manual segmentation results by physician and the yellow curves indicate the segmentation results by our proposed method. [sent-398, score-0.397]
</p><p>84 ation of prostate centers in the planning and treatment images, which can be found by referring to Fig. [sent-399, score-1.18]
</p><p>85 By applying the proposed method to patients 3, 10 and 15, the obtained median Dice ratio are 0. [sent-401, score-0.144]
</p><p>86 These results show the effectiveness of the proposed method, especially with the initial physician’s manual interaction when the large irregular motion occurs in the prostate regions. [sent-413, score-0.867]
</p><p>87 The  standard devia-  tion of prostate centers for each  Figure 10. [sent-421, score-0.758]
</p><p>88 Conclusion We have proposed a novel semi-automatic learning method for prostate segmentation in CT images during the image-guided radiotherapy. [sent-425, score-0.829]
</p><p>89 Then, the multi-atlases based label fusion method will combine the segmentation results of the planning and previous treatment images for final segmentation. [sent-427, score-0.57]
</p><p>90 A real CT-prostate dataset is used for evaluation, which consists of 24 patients and 330 images, all with the manual delineation results by the experienced physician. [sent-428, score-0.186]
</p><p>91 , higher Dice ratio and TPF, and lower centroid distances) compared with the state-of-the-art methods, but also demonstrates its capability in dealing with large irregular prostate motions. [sent-431, score-0.813]
</p><p>92 Segmenting the prostate and rectum in CT imagery using anatomical constraints. [sent-475, score-0.758]
</p><p>93 3D meshless prostate segmentation and registration in image guided radiotherapy. [sent-484, score-0.866]
</p><p>94 Automatic segmentation of intra-treatment CT images for adaptive radiation therapy of the prostate. [sent-498, score-0.089]
</p><p>95 Segmenting CT prostate images using population and patient-specific statistics for radiotherapy. [sent-512, score-0.758]
</p><p>96 A coupled global registration and segmentation framework with application to magnetic resonance prostate imagery. [sent-519, score-0.829]
</p><p>97 Label fusion in atlas-based segmentation using a selective and iterative method for performance level estimation (SIMPLE). [sent-523, score-0.114]
</p><p>98 Learning image context for segmentation of prostate in CT-guided radiotherapy. [sent-531, score-0.829]
</p><p>99 A feature based learning framework for accurate prostate localization in CT images. [sent-536, score-0.779]
</p><p>100 Precise segmentation of multiple organs in ct volumes using learning-based approach and information theory. [sent-548, score-0.211]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prostate', 0.758), ('treatment', 0.258), ('voxels', 0.236), ('scoto', 0.218), ('physician', 0.18), ('planning', 0.164), ('ct', 0.14), ('patient', 0.124), ('tpf', 0.115), ('patients', 0.093), ('slices', 0.093), ('lasso', 0.088), ('dice', 0.085), ('slice', 0.078), ('prostatelikelihood', 0.077), ('manual', 0.075), ('ui', 0.073), ('segmentation', 0.071), ('cancer', 0.061), ('voxel', 0.055), ('noteworthy', 0.05), ('fi', 0.047), ('roi', 0.046), ('selection', 0.045), ('fusion', 0.043), ('svr', 0.041), ('blocks', 0.041), ('block', 0.04), ('lifi', 0.038), ('pelvic', 0.038), ('radiotherapy', 0.038), ('tlassob', 0.038), ('miccai', 0.038), ('guided', 0.037), ('days', 0.035), ('rigidly', 0.034), ('haar', 0.034), ('current', 0.033), ('spend', 0.031), ('median', 0.031), ('bone', 0.03), ('wavelet', 0.03), ('transductive', 0.029), ('scans', 0.028), ('ji', 0.028), ('specification', 0.027), ('listed', 0.026), ('cd', 0.026), ('foskey', 0.026), ('jiangsu', 0.026), ('jrssb', 0.026), ('lassob', 0.026), ('lassos', 0.026), ('tlasso', 0.026), ('tlassos', 0.026), ('tmi', 0.025), ('qi', 0.023), ('clinically', 0.023), ('lbp', 0.022), ('li', 0.022), ('yi', 0.021), ('feature', 0.021), ('respective', 0.02), ('segmented', 0.02), ('ratio', 0.02), ('ei', 0.02), ('ik', 0.02), ('day', 0.02), ('scanned', 0.02), ('percentile', 0.019), ('irregular', 0.019), ('testing', 0.018), ('radiation', 0.018), ('label', 0.018), ('acquired', 0.018), ('calculated', 0.018), ('lateral', 0.018), ('apex', 0.018), ('experienced', 0.018), ('asking', 0.017), ('prefer', 0.017), ('specify', 0.017), ('liao', 0.016), ('predicted', 0.016), ('bold', 0.016), ('centroid', 0.016), ('manifold', 0.016), ('previous', 0.016), ('medical', 0.016), ('ip', 0.015), ('descent', 0.015), ('aligned', 0.015), ('china', 0.015), ('motion', 0.015), ('national', 0.015), ('tpami', 0.015), ('ith', 0.015), ('regions', 0.014), ('adopted', 0.014), ('irrelevant', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="342-tfidf-1" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>Author: Yinghuan Shi, Shu Liao, Yaozong Gao, Daoqiang Zhang, Yang Gao, Dinggang Shen</p><p>Abstract: Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. In this paper, a novel semi-automated prostate segmentation method is presented. Specifically, to segment the prostate in the current treatment image, the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. Then, the prostate is segmented automatically by theproposed two steps: (i) Thefirst step of prostate-likelihood estimation to predict the prostate likelihood for each voxel in the current treatment image, aiming to generate the 3-D prostate-likelihood map by the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) The second step of multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from the planning and previous treatment images. The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset, consisting of 24 patients with 330 images. Moreover, it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification of the first and last slices of the prostate.</p><p>2 0.61180723 <a title="342-tfidf-2" href="./cvpr-2013-Efficient_3D_Endfiring_TRUS_Prostate_Segmentation_with_Globally_Optimized_Rotational_Symmetry.html">139 cvpr-2013-Efficient 3D Endfiring TRUS Prostate Segmentation with Globally Optimized Rotational Symmetry</a></p>
<p>Author: Jing Yuan, Wu Qiu, Eranga Ukwatta, Martin Rajchl, Xue-Cheng Tai, Aaron Fenster</p><p>Abstract: Segmenting 3D endfiring transrectal ultrasound (TRUS) prostate images efficiently and accurately is of utmost importance for the planning and guiding 3D TRUS guided prostate biopsy. Poor image quality and imaging artifacts of 3D TRUS images often introduce a challenging task in computation to directly extract the 3D prostate surface. In this work, we propose a novel global optimization approach to delineate 3D prostate boundaries using its rotational resliced images around a specified axis, which properly enforces the inherent rotational symmetry of prostate shapes to jointly adjust a series of 2D slicewise segmentations in the global 3D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard, we propose a novel coupled continuous max-flow model, which not only provides a powerful mathematical tool to analyze the proposed optimization problem but also amounts to a new and efficient duality-basedalgorithm. Ex- tensive experiments demonstrate that the proposed method significantly outperforms the state-of-art methods in terms ofefficiency, accuracy, reliability and less user-interactions, and reduces the execution time by a factor of 100.</p><p>3 0.090951815 <a title="342-tfidf-3" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>Author: Erik Ask, Olof Enqvist, Fredrik Kahl</p><p>Abstract: This paper is concerned with model fitting in the presence of noise and outliers. Previously it has been shown that the number of outliers can be minimized with polynomial complexity in the number of measurements. This paper improves on these results in two ways. First, it is shown that for a large class of problems, the statistically more desirable truncated L2-norm can be optimized with the same complexity. Then, with the same methodology, it is shown how to transform multi-model fitting into a purely combinatorial problem—with worst-case complexity that is polynomial in the number of measurements, though exponential in the number of models. We apply our framework to a series of hard registration and stitching problems demonstrating that the approach is not only of theoretical interest. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers for fitting problems with lowdimensional models.</p><p>4 0.084217191 <a title="342-tfidf-4" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>Author: Bo Zheng, Yibiao Zhao, Joey C. Yu, Katsushi Ikeuchi, Song-Chun Zhu</p><p>Abstract: In this paper, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.</p><p>5 0.077241883 <a title="342-tfidf-5" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>Author: Jeremie Papon, Alexey Abramov, Markus Schoeler, Florentin Wörgötter</p><p>Abstract: Unsupervised over-segmentation of an image into regions of perceptually similar pixels, known as superpixels, is a widely used preprocessing step in segmentation algorithms. Superpixel methods reduce the number of regions that must be considered later by more computationally expensive algorithms, with a minimal loss of information. Nevertheless, as some information is inevitably lost, it is vital that superpixels not cross object boundaries, as such errors will propagate through later steps. Existing methods make use of projected color or depth information, but do not consider three dimensional geometric relationships between observed data points which can be used to prevent superpixels from crossing regions of empty space. We propose a novel over-segmentation algorithm which uses voxel relationships to produce over-segmentations which are fully consistent with the spatial geometry of the scene in three dimensional, rather than projective, space. Enforcing the constraint that segmented regions must have spatial connectivity prevents label flow across semantic object boundaries which might otherwise be violated. Additionally, as the algorithm works directly in 3D space, observations from several calibrated RGB+D cameras can be segmented jointly. Experiments on a large data set of human annotated RGB+D images demonstrate a significant reduction in occurrence of clusters crossing object boundaries, while maintaining speeds comparable to state-of-the-art 2D methods.</p><p>6 0.06610883 <a title="342-tfidf-6" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>7 0.061029814 <a title="342-tfidf-7" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>8 0.053479102 <a title="342-tfidf-8" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>9 0.051905245 <a title="342-tfidf-9" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>10 0.050199594 <a title="342-tfidf-10" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>11 0.041907109 <a title="342-tfidf-11" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>12 0.041689649 <a title="342-tfidf-12" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>13 0.03972825 <a title="342-tfidf-13" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>14 0.0378033 <a title="342-tfidf-14" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>15 0.037074991 <a title="342-tfidf-15" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>16 0.035843898 <a title="342-tfidf-16" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>17 0.035264738 <a title="342-tfidf-17" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>18 0.034769449 <a title="342-tfidf-18" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>19 0.034558229 <a title="342-tfidf-19" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>20 0.034146164 <a title="342-tfidf-20" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (1, 0.026), (2, 0.001), (3, 0.017), (4, 0.027), (5, -0.012), (6, 0.002), (7, -0.013), (8, -0.033), (9, 0.001), (10, 0.047), (11, -0.008), (12, -0.027), (13, -0.031), (14, 0.02), (15, -0.026), (16, 0.03), (17, -0.009), (18, 0.025), (19, 0.063), (20, -0.053), (21, 0.053), (22, -0.057), (23, -0.011), (24, 0.074), (25, 0.036), (26, -0.086), (27, -0.091), (28, 0.135), (29, -0.027), (30, -0.053), (31, 0.135), (32, -0.166), (33, 0.044), (34, 0.123), (35, -0.264), (36, 0.351), (37, 0.062), (38, -0.158), (39, 0.026), (40, -0.164), (41, -0.304), (42, 0.132), (43, -0.169), (44, 0.039), (45, 0.018), (46, -0.255), (47, 0.137), (48, 0.004), (49, -0.153)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93615478 <a title="342-lsi-1" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>Author: Yinghuan Shi, Shu Liao, Yaozong Gao, Daoqiang Zhang, Yang Gao, Dinggang Shen</p><p>Abstract: Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. In this paper, a novel semi-automated prostate segmentation method is presented. Specifically, to segment the prostate in the current treatment image, the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. Then, the prostate is segmented automatically by theproposed two steps: (i) Thefirst step of prostate-likelihood estimation to predict the prostate likelihood for each voxel in the current treatment image, aiming to generate the 3-D prostate-likelihood map by the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) The second step of multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from the planning and previous treatment images. The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset, consisting of 24 patients with 330 images. Moreover, it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification of the first and last slices of the prostate.</p><p>2 0.90641373 <a title="342-lsi-2" href="./cvpr-2013-Efficient_3D_Endfiring_TRUS_Prostate_Segmentation_with_Globally_Optimized_Rotational_Symmetry.html">139 cvpr-2013-Efficient 3D Endfiring TRUS Prostate Segmentation with Globally Optimized Rotational Symmetry</a></p>
<p>Author: Jing Yuan, Wu Qiu, Eranga Ukwatta, Martin Rajchl, Xue-Cheng Tai, Aaron Fenster</p><p>Abstract: Segmenting 3D endfiring transrectal ultrasound (TRUS) prostate images efficiently and accurately is of utmost importance for the planning and guiding 3D TRUS guided prostate biopsy. Poor image quality and imaging artifacts of 3D TRUS images often introduce a challenging task in computation to directly extract the 3D prostate surface. In this work, we propose a novel global optimization approach to delineate 3D prostate boundaries using its rotational resliced images around a specified axis, which properly enforces the inherent rotational symmetry of prostate shapes to jointly adjust a series of 2D slicewise segmentations in the global 3D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard, we propose a novel coupled continuous max-flow model, which not only provides a powerful mathematical tool to analyze the proposed optimization problem but also amounts to a new and efficient duality-basedalgorithm. Ex- tensive experiments demonstrate that the proposed method significantly outperforms the state-of-art methods in terms ofefficiency, accuracy, reliability and less user-interactions, and reduces the execution time by a factor of 100.</p><p>3 0.38184771 <a title="342-lsi-3" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>Author: Byung-Woo Hong, Zhaojin Lu, Ganesh Sundaramoorthi</p><p>Abstract: In this work, we address the multi-label Mumford-Shah problem, i.e., the problem of jointly estimating a partitioning of the domain of the image, and functions defined within regions of the partition. We create algorithms that are efficient, robust to undesirable local minima, and are easy-toimplement. Our algorithms are formulated by slightly modifying the underlying statistical model from which the multilabel Mumford-Shah functional is derived. The advantage of this statistical model is that the underlying variables: the labels and thefunctions are less coupled than in the original formulation, and the labels can be computed from the functions with more global updates. The resulting algorithms can be tuned to the desired level of locality of the solution: from fully global updates to more local updates. We demonstrate our algorithm on two applications: joint multi-label segmentation and denoising, and joint multi-label motion segmentation and flow estimation. We compare to the stateof-the-art in multi-label Mumford-Shah problems and show that we achieve more promising results.</p><p>4 0.31708503 <a title="342-lsi-4" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>Author: Erik Ask, Olof Enqvist, Fredrik Kahl</p><p>Abstract: This paper is concerned with model fitting in the presence of noise and outliers. Previously it has been shown that the number of outliers can be minimized with polynomial complexity in the number of measurements. This paper improves on these results in two ways. First, it is shown that for a large class of problems, the statistically more desirable truncated L2-norm can be optimized with the same complexity. Then, with the same methodology, it is shown how to transform multi-model fitting into a purely combinatorial problem—with worst-case complexity that is polynomial in the number of measurements, though exponential in the number of models. We apply our framework to a series of hard registration and stitching problems demonstrating that the approach is not only of theoretical interest. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers for fitting problems with lowdimensional models.</p><p>5 0.26265299 <a title="342-lsi-5" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>6 0.25459695 <a title="342-lsi-6" href="./cvpr-2013-Auxiliary_Cuts_for_General_Classes_of_Higher_Order_Functionals.html">51 cvpr-2013-Auxiliary Cuts for General Classes of Higher Order Functionals</a></p>
<p>7 0.254419 <a title="342-lsi-7" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>8 0.24938476 <a title="342-lsi-8" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>9 0.24063072 <a title="342-lsi-9" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>10 0.23558725 <a title="342-lsi-10" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>11 0.22175083 <a title="342-lsi-11" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>12 0.22149543 <a title="342-lsi-12" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>13 0.21925478 <a title="342-lsi-13" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>14 0.21883576 <a title="342-lsi-14" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>15 0.21233936 <a title="342-lsi-15" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>16 0.20320572 <a title="342-lsi-16" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>17 0.19945505 <a title="342-lsi-17" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>18 0.19410403 <a title="342-lsi-18" href="./cvpr-2013-Blind_Deconvolution_of_Widefield_Fluorescence_Microscopic_Data_by_Regularization_of_the_Optical_Transfer_Function_%28OTF%29.html">65 cvpr-2013-Blind Deconvolution of Widefield Fluorescence Microscopic Data by Regularization of the Optical Transfer Function (OTF)</a></p>
<p>19 0.18883781 <a title="342-lsi-19" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>20 0.18747485 <a title="342-lsi-20" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.29), (10, 0.157), (16, 0.038), (26, 0.047), (33, 0.182), (57, 0.01), (65, 0.027), (67, 0.04), (69, 0.036), (87, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78714681 <a title="342-lda-1" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>same-paper 2 0.71656775 <a title="342-lda-2" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>Author: Yinghuan Shi, Shu Liao, Yaozong Gao, Daoqiang Zhang, Yang Gao, Dinggang Shen</p><p>Abstract: Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. In this paper, a novel semi-automated prostate segmentation method is presented. Specifically, to segment the prostate in the current treatment image, the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. Then, the prostate is segmented automatically by theproposed two steps: (i) Thefirst step of prostate-likelihood estimation to predict the prostate likelihood for each voxel in the current treatment image, aiming to generate the 3-D prostate-likelihood map by the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) The second step of multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from the planning and previous treatment images. The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset, consisting of 24 patients with 330 images. Moreover, it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification of the first and last slices of the prostate.</p><p>3 0.69953144 <a title="342-lda-3" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>Author: Ning Zhu, Albert C.S. Chung</p><p>Abstract: In this paper, we propose a graph-based method for 3D vessel tree structure segmentation based on a new tubularity Markov tree model ( TMT), which works as both new energy function and graph construction method. With the help of power-watershed implementation [7], a global optimal segmentation can be obtained with low computational cost. Different with other graph-based vessel segmentation methods, the proposed method does not depend on any skeleton and ROI extraction method. The classical issues of the graph-based methods, such as shrinking bias and sensitivity to seed point location, can be solved with the proposed method thanks to vessel data fidelity obtained with TMT. The proposed method is compared with some classical graph-based image segmentation methods and two up-to-date 3D vessel segmentation methods, and is demonstrated to be more accurate than these methods for 3D vessel tree segmentation. Although the segmentation is done without ROI extraction, the computational cost for the proposed method is low (within 20 seconds for 256*256*144 image).</p><p>4 0.64611775 <a title="342-lda-4" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>5 0.64587003 <a title="342-lda-5" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>Author: Dan Levi, Shai Silberstein, Aharon Bar-Hillel</p><p>Abstract: In this work we present a new part-based object detection algorithm with hundreds of parts performing realtime detection. Part-based models are currently state-ofthe-art for object detection due to their ability to represent large appearance variations. However, due to their high computational demands such methods are limited to several parts only and are too slow for practical real-time implementation. Our algorithm is an accelerated version of the “Feature Synthesis ” (FS) method [1], which uses multiple object parts for detection and is among state-of-theart methods on human detection benchmarks, but also suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses several strategies for reducing the number of locations searched for each part. The first strategy uses a novel algorithm for approximate nearest neighbor search which we developed, termed “KDFerns ”, to compare each image location to only a subset of the model parts. Candidate part locations for a specific part are further reduced using spatial inhibition, and using an object-level “coarse-to-fine ” strategy. In our empirical evaluation on pedestrian detection benchmarks, AFS main- × tains almost fully the accuracy performance of the original FS, while running more than 4 faster than existing partbased methods which use only several parts. AFS is to our best knowledge the first part-based object detection method achieving real-time running performance: nearly 10 frames per-second on 640 480 images on a regular CPU.</p><p>6 0.64559799 <a title="342-lda-6" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>7 0.64534956 <a title="342-lda-7" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>8 0.64401299 <a title="342-lda-8" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>9 0.64348423 <a title="342-lda-9" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>10 0.64214993 <a title="342-lda-10" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>11 0.64100492 <a title="342-lda-11" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>12 0.64091879 <a title="342-lda-12" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>13 0.63935024 <a title="342-lda-13" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>14 0.63758397 <a title="342-lda-14" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>15 0.6374554 <a title="342-lda-15" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>16 0.63657933 <a title="342-lda-16" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>17 0.63593948 <a title="342-lda-17" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>18 0.63558429 <a title="342-lda-18" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>19 0.63518929 <a title="342-lda-19" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>20 0.63423097 <a title="342-lda-20" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
