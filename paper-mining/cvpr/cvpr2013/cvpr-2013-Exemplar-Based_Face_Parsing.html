<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>152 cvpr-2013-Exemplar-Based Face Parsing</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-152" href="#">cvpr2013-152</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>152 cvpr-2013-Exemplar-Based Face Parsing</h1>
<br/><p>Source: <a title="cvpr-2013-152-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Smith_Exemplar-Based_Face_Parsing_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>Reference: <a title="cvpr-2013-152-reference" href="../cvpr2013_reference/cvpr-2013-Exemplar-Based_Face_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu/~lizhang/projects/face-parsing/  Abstract In this work, we propose an exemplar-based face image segmentation algorithm. [sent-5, score-0.341]
</p><p>2 Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. [sent-7, score-0.768]
</p><p>3 Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. [sent-8, score-1.076]
</p><p>4 Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. [sent-9, score-0.715]
</p><p>5 We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. [sent-10, score-0.624]
</p><p>6 We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. [sent-11, score-0.664]
</p><p>7 Introduction In face image analysis, one common task is to parse an in-  put face image into facial parts, e. [sent-14, score-0.677]
</p><p>8 Most previous methods accomplish this task by marking a few landmarks [1, 22] or a few contours [4, 18] on the input face image. [sent-17, score-0.425]
</p><p>9 In this paper, we seek to mark each pixel on the face with its semantic part label; that is, our algorithm parses a face image into its constituent facial parts. [sent-18, score-0.724]
</p><p>10 •  Other than eye corners and mouth corners, most landmarks are not well-defined. [sent-20, score-0.459]
</p><p>11 For example, it is unclear how many landmarks should be defined on the chinline, or how noses should be represented: should there be a line segment along the nose ridge, or a contour around the nostrils? [sent-21, score-0.327]
</p><p>12 Our exemplar-based algorithm parses a face image into its  constituent facial parts using a soft segmentation. [sent-25, score-0.519]
</p><p>13 •  •  Contour-based representations are not general enough to model several facial parts useful for robust face analysis. [sent-26, score-0.426]
</p><p>14 For example, teeth are important cues for analyzing open-mouth expressions; ears are important cues for analyzing profile faces; strands of hair are often confused by algorithms as occluders. [sent-27, score-0.36]
</p><p>15 For example, the precise location of the tip of an eyebrow or the contour of a nose ridge are difficult to determine, even for human labelers. [sent-30, score-0.42]
</p><p>16 Such uncertainty leads to errors in human labeled face data that are used in both the training and evaluation of algorithms. [sent-31, score-0.328]
</p><p>17 Segment-based representations alleviate the aforementioned limitations: segments can represent any facial part, be they hair or teeth, and soft segmentation can model uncertain transitions between parts. [sent-32, score-0.535]
</p><p>18 Although semantic segmentation for general scenes has received tremendous attention in recent years [7, 8, 12, 19], there has been relatively little attention given specifically to face part segmentation, with the exception of [15, 21]. [sent-33, score-0.341]
</p><p>19 Since facial parts have special geometric configurations compared to general indoor and outdoor scenes, we propose an exemplar-based face image segmentation algorithm, taking inspiration from previous work in image parsing for general scenes. [sent-34, score-0.65]
</p><p>20 Specifically, our approach assumes a database of face images, each of which is associated with a hand-labeled segmentation map and a set of sparse keypoint descriptors. [sent-35, score-0.466]
</p><p>21 [1] to select m top exemplar images from the database as input. [sent-38, score-0.453]
</p><p>22 Our algorithm then computes a nonrigid warp for each top exemplar; each nonrigid warp aligns the exemplar image to the test image by matching the set of sparse precomputed exemplar keypoints to the test image. [sent-39, score-1.4]
</p><p>23 Finally, we propagate labels from the exemplar images to the test image in a pixelwise manner, using trained weights that modulate and combine label maps differently for each part type. [sent-40, score-0.715]
</p><p>24 We evaluate our method on two challenging datasets [6, 9] and compare with two face parsing algorithms [15, 21] and a general scene parsing algorithm [12]. [sent-41, score-0.624]
</p><p>25 We also compare our segmentation results with contour-based face alignment results: that is, we first run the alignment algorithms [4, 18, 22] to extract contour points and then derive segments from the contours. [sent-42, score-0.664]
</p><p>26 , skin and eyebrow regions, we recover label probabilities at each pixel. [sent-51, score-0.542]
</p><p>27 A learning algorithm for finding optimal parameters for calibrating exemplar label types. [sent-54, score-0.546]
</p><p>28 To correct these biases, we train a set of label weights that adjust the relative importance of each label type. [sent-58, score-0.37]
</p><p>29 Hair mattes are also included for future work in hair segmentation. [sent-63, score-0.338]
</p><p>30 Second, because they produce only a binary classification, the component-specific segmentors do not generalize well to more complicated label interactions, such as those that exist between the inner mouth region, the lips, and the skin around the lips, for example. [sent-75, score-0.696]
</p><p>31 Warrell and Prince argued that the scene parsing approach is advantageous because it is general enough to handle unconstrained face images, where the shape and appearance of features vary widely and relatively rare semantic label classes exist, such as moustaches and hats. [sent-78, score-0.613]
</p><p>32 As part of their contribution, they introduced priors to loosely model the topological structure of face images (so that mouth labels do not appear in the forehead, for example). [sent-79, score-0.603]
</p><p>33 However, the labels they generate are often coarse and inaccurate, especially for small face components like eyes and eyebrows. [sent-80, score-0.375]
</p><p>34 We show in this work that our approach produces accurate, fine-scale label estimates in unconstrained face images. [sent-81, score-0.444]
</p><p>35 This savings allows us to use a large set of top exemplar images for label transfer (in [12] they use m ≤ 9 top exemplar images; we use m =ns 1fe0r0 ()i,n nw [h1i2c]h t hise important i n9 our approach for two reasons. [sent-92, score-0.986]
</p><p>36 To this end, we propose a training algorithm for estimat333444888533  ing a set of weights that convert label maps from exemplars to label probabilities on the test image. [sent-96, score-0.622]
</p><p>37 We remark that soft segmentation is useful for future work on hair segmentation, among other applications. [sent-97, score-0.367]
</p><p>38 Each pi encodes label uncertainty at the pixel level, which reflects the natural indistinctness of some facial features (e. [sent-108, score-0.346]
</p><p>39 OEuarch d exemplar Mj choams four parts: an image, a l aarbsel { map, a very sparse set of facial landmark points, and a sparse set of SIFT [14] keypoint descriptors. [sent-118, score-0.65]
</p><p>40 We use 12 landmark points: 2 mouth corners, 4 eye corners, 2 points on the eyebrows (each centered on the top edge), 2 points on the mouth (one on the top edge ofthe upper lip and one on the bottom edge ofthe bottom lip), 1point between the nostrils, and 1chin point. [sent-124, score-0.966]
</p><p>41 Runtime Pre-processing Given a test image, we first use a face detector (i. [sent-130, score-0.333]
</p><p>42 The test image is then rescaled so that the face has an IOD of approximately 55 pixels, which is the size of the exemplar faces. [sent-133, score-0.721]
</p><p>43 To search for a subset of m top exemplar faces in the database, we use Belhumeur et al. [sent-135, score-0.482]
</p><p>44 The output of the pre-processing is a set of m exemplars, each of which is associated with a similarity transformation that aligns the exemplar to the face in the test image. [sent-137, score-0.721]
</p><p>45 Step 1: Nonrigid exemplar alignment For each keypoint in each of the top m exemplars, search within a small window in the test image to find the best match; record the matching score and the location offset of the best match for each keypoint. [sent-138, score-0.685]
</p><p>46 Warp the label map of each top exemplar nonrigidly using a displacement field interpolated from the location offsets. [sent-139, score-0.616]
</p><p>47 Step 2: Exemplar label map aggregation Aggregate warped label maps using weights derived from the keypoint matching scores in Step 1. [sent-140, score-0.485]
</p><p>48 The weights are spatially varying among exemplar pixel locations and favor exemplar pixels near keypoints that are matched well with the test image. [sent-141, score-0.969]
</p><p>49 Step 3: Pixel-wise label selection Produce a label probability vector at each pixel by first attenuating each channel in the aggregated label map and then normalizing it. [sent-142, score-0.565]
</p><p>50 Step 1: Nonrigid Exemplar Alignment Due to local deformation, a similarity transformation is not sufficient to align an exemplar with the testing image. [sent-147, score-0.388]
</p><p>51 The goal of Step 1is to refine the registration using a nonrigid warp between each top exemplar label map and the test image. [sent-148, score-0.825]
</p><p>52 Therefore, for efficiency reasons, we instead rely on about 150 SIFT keypoints to compute the nonrigid warp between each exemplar and the test image. [sent-152, score-0.733]
</p><p>53 333444888644  Our algorithm computes a nonrigid warp for each exemplar label map by interpolating the displacements {Δxf}fF=1, pwlaherr lae Fel ims tphe b ynu inmtebrepro olaft nSIgF tThe keypoints einn ttsh e{ exemplar. [sent-158, score-0.844]
</p><p>54 Step 2: Exemplar Aggregation For each exemplar label map, we interpolate the matching scores r(Δxf) in Eq. [sent-162, score-0.546]
</p><p>55 Now, )ea ∈ch [ nonrigidly warped exemplar label map is associated with a perpixel matching score map. [sent-165, score-0.619]
</p><p>56 Near smaller regions, like the eyes, eyebrows, and lips, we observe that, if the aggregated label probabilities are incorrect, they tend to be incorrect in the direction of the larger surrounding regions, namely the face skin and background. [sent-172, score-0.626]
</p><p>57 Assuming noise prevents perfect correspondences, “skin” label correspondences will occur more frequently than “eyebrow” label correspondence simply because there are many more skin labels than eyebrow labels. [sent-174, score-0.729]
</p><p>58 A common symptom of this label bias is that estimated eyebrow regions (and other small regions of the face) tend to be too small. [sent-175, score-0.356]
</p><p>59 We compensate for this bias by re-weighting each component of the aggregated label probability vector, and then renormalizing each pixel’s label probability vector afterward. [sent-176, score-0.398]
</p><p>60 Given a tuning set with ground truth label probabilities, we find label component weights α = [α1 , α2 , . [sent-177, score-0.439]
</p><p>61 After the label component weights have been found, we adjust each label probability vector. [sent-223, score-0.397]
</p><p>62 Results and Discussion We have evaluated our method on two different datasets, and we show that it clearly improves upon a recent general scene parsing approach and existing face parsing approaches. [sent-227, score-0.624]
</p><p>63 Additionally, we adapt a recent landmark localization method and two face alignment algorithms to produce segmentation results, and show that our method is more accurate. [sent-228, score-0.484]
</p><p>64 Following their procedure to evaluate accuracy, we generated ground truth by annotating each face with contour points around each segment. [sent-235, score-0.431]
</p><p>65 Our second (primary) dataset is Helen [9], which is composed of 2330 face images with densely-sampled, manuallyannotated contours around the eyes, eyebrows, nose, outer lips, inner lips, and jawline. [sent-236, score-0.388]
</p><p>66 Our exemplar set was used for all experiments, including experiments on LFW images. [sent-239, score-0.388]
</p><p>67 Our Helen tuning and test sets were formed by taking the first 330 images in the dataset; they include no subjects from the exemplar set. [sent-240, score-0.475]
</p><p>68 For face skin, we used the jawline contour as the lower boundary; for the upper boundary, we separated the forehead from the hair by manually annotating forehead and hair scribbles and running an automatic matting algorithm [11] on each image. [sent-276, score-1.107]
</p><p>69 Although we do not focus on hair segmentation in this work, we also recovered “ground truth” hair regions using this  approach. [sent-277, score-0.587]
</p><p>70 The hair mattes from [11] are usually accurate, but mistakes are inevitable. [sent-278, score-0.338]
</p><p>71 Therefore, to ensure fair accuracy measurements, we manually annotated the face skin in all test images. [sent-279, score-0.453]
</p><p>72 4 (we matched the LFW segment representation by grouping the Helen mouth components, and treating face skin as background). [sent-289, score-0.72]
</p><p>73 [18], and Gu & Kanade [4] are face alignment methods. [sent-313, score-0.358]
</p><p>74 (4) finds label weights that maximize the recall rates of eye, eyebrow, nose, and mouth pixels, which are relatively few and sensitive to errors, by sacrificing the recall rate of background pixels, which are numerous and insensitive to errors. [sent-322, score-0.521]
</p><p>75 i1t02h814036s127evraloth  face parsing and alignment methods [4, 18, 21, 22] in Table 1. [sent-329, score-0.527]
</p><p>76 The inside of the mouth is not given as ground truth for the LFW images, and so we show only the entire mouth segment. [sent-333, score-0.583]
</p><p>77 case, the “overall” measure is computed over eye, eyebrow, nose, inner mouth, upper lip, and lower lip segments; face skin is excluded in the overall measure, as it cannot be computed for Zhu & Ramanan, Saragih et al. [sent-339, score-0.591]
</p><p>78 The difference is minimal and is primarily due to our algorithm incorrectly “hallucinates” skin in hair regions, while Liu et al. [sent-343, score-0.413]
</p><p>79 Regardless, we see that our approach improves upon the segments generated by recent face alignment algorithms. [sent-357, score-0.421]
</p><p>80 By comparing the fifth and sixth rows of Table 2, we observe that the local search and nonrigid exemplar alignment from Step 1 of our algorithm modestly improves the quantitative accuracy of our results. [sent-371, score-0.56]
</p><p>81 We see a noticeable improvement from row six to row seven in Table 2, especially in the inner mouth region, due to the label weights. [sent-375, score-0.466]
</p><p>82 In our view, the mouth is the most challenging region of the face to segment. [sent-376, score-0.563]
</p><p>83 The shape and appearance of lips vary widely between subjects, mouths deform significantly, and the overall appearance of the mouth region changes depending on whether the inside of the mouth is visible or not. [sent-377, score-0.651]
</p><p>84 Unusual mouth expressions, like the one shown in the rightmost column of Figure 4, are not represented well in the exemplar images, which results in poor label transfer from the top exemplars to the test image. [sent-378, score-1.067]
</p><p>85 Our improvement over other algorithms demonstrates the advantages of using segments to parse face parts. [sent-380, score-0.349]
</p><p>86 For example, the inside of the mouth is not well modeled using a classical contour based representation. [sent-381, score-0.393]
</p><p>87 However, we can recover contours by treating contour points in the exemplars in almost the same way that we treat segment labels. [sent-411, score-0.427]
</p><p>88 That is, in Step 1, we warp the contour point from each exemplar in the same way that we warp the exemplar label maps. [sent-412, score-1.262]
</p><p>89 Specifically, each contour point is found by computing the weighted average location of the warped exemplar contour points; each weight j is given by the match scores in Rj closest to the contour point. [sent-415, score-0.765]
</p><p>90 Hair Segmentation Several approaches for hair segmentation start by estimating a set of hair / not hair seed pixels in the image, and then refine the hair region using a matting algorithm ([16] is one example). [sent-417, score-1.16]
</p><p>91 We can also generate seeds by counting the votes from hair / not hair labels from the top exemplars, and thresholding the counts. [sent-418, score-0.643]
</p><p>92 Figure 6 shows seeds generating using this approach, and hair mattes computed from these seeds using [11]. [sent-419, score-0.428]
</p><p>93 Face Image Reconstruction and Synthesis Examplarbased face image reconstruction/synthesis is applicable for various face image editing tasks, including grayscale image  colorization [10] and automatic face image retouching [5]. [sent-420, score-0.89]
</p><p>94 We can create a synthetic version of the input face by propagating color and intensity information from the exemplar images to the input image; this can be easily accomplished by replacing the label vectors with the color (or intensity) channels of the exemplar images. [sent-421, score-1.22]
</p><p>95 We can recover accurate hair mattes in many cases (first two columns), but the procedure often fails on difficult cases (third column). [sent-427, score-0.37]
</p><p>96 Wecansythesiz theinputfacebyreplacingthe xmplar  label vectors with the color channels from the exemplar images. [sent-430, score-0.546]
</p><p>97 Conclusion and Future Work In this paper, we have proposed an automatic face parsing technique that recovers a soft segment-based representation of the face, which naturally encodes the segment class uncertainty in the image. [sent-434, score-0.58]
</p><p>98 Second, we proposed a learning algorithm for finding optimal label calibration weights, which remove biases between label types. [sent-436, score-0.35]
</p><p>99 Third, we offer a new face segmentation dataset built as an extension of the recent Helen face dataset [9], which offers ground truth pixel-wise labels for face parts in high quality images. [sent-437, score-1.017]
</p><p>100 Labeled faces in the wild: A database for studying face recognition in unconstrained environments. [sent-491, score-0.366]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exemplar', 0.388), ('helen', 0.307), ('face', 0.286), ('mouth', 0.277), ('hair', 0.266), ('eyebrow', 0.198), ('exemplars', 0.171), ('parsing', 0.169), ('label', 0.158), ('skin', 0.12), ('lfw', 0.118), ('contour', 0.116), ('segmentors', 0.11), ('warp', 0.106), ('nose', 0.106), ('facial', 0.105), ('nonrigid', 0.1), ('lip', 0.097), ('lips', 0.097), ('keypoints', 0.092), ('eyebrows', 0.09), ('keypoint', 0.086), ('xf', 0.078), ('runtime', 0.077), ('saragih', 0.075), ('alignment', 0.072), ('eye', 0.072), ('mattes', 0.072), ('contours', 0.071), ('landmark', 0.071), ('landmarks', 0.068), ('confusion', 0.067), ('ck', 0.065), ('warrell', 0.065), ('segments', 0.063), ('teeth', 0.062), ('iod', 0.058), ('sift', 0.057), ('segmentation', 0.055), ('weights', 0.054), ('belhumeur', 0.053), ('forehead', 0.051), ('liu', 0.051), ('favorably', 0.05), ('eyes', 0.049), ('luo', 0.047), ('test', 0.047), ('parses', 0.047), ('prince', 0.047), ('soft', 0.046), ('seeds', 0.045), ('gu', 0.044), ('gdesc', 0.044), ('gspatial', 0.044), ('hallucinates', 0.044), ('labelfaces', 0.044), ('nonrigidly', 0.044), ('nostrils', 0.044), ('kanade', 0.043), ('corners', 0.042), ('uncertainty', 0.042), ('pi', 0.041), ('matting', 0.041), ('faces', 0.041), ('tuning', 0.04), ('labels', 0.04), ('database', 0.039), ('exemplifies', 0.039), ('zhu', 0.039), ('window', 0.039), ('segment', 0.037), ('attenuating', 0.036), ('aggregate', 0.036), ('parts', 0.035), ('probabilities', 0.034), ('biases', 0.034), ('colorization', 0.032), ('ears', 0.032), ('nonparametric', 0.032), ('recover', 0.032), ('maximize', 0.032), ('inner', 0.031), ('warping', 0.031), ('compares', 0.03), ('upper', 0.03), ('occur', 0.029), ('segmentations', 0.029), ('truth', 0.029), ('warped', 0.029), ('modulate', 0.028), ('brandt', 0.028), ('aggregated', 0.028), ('offset', 0.027), ('bel', 0.027), ('et', 0.027), ('probability', 0.027), ('top', 0.026), ('adobe', 0.026), ('correspondence', 0.026), ('judge', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="152-tfidf-1" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>2 0.41417533 <a title="152-tfidf-2" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>3 0.24165717 <a title="152-tfidf-3" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>4 0.2235458 <a title="152-tfidf-4" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>5 0.19637004 <a title="152-tfidf-5" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>6 0.19100611 <a title="152-tfidf-6" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>7 0.18977971 <a title="152-tfidf-7" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>8 0.18870531 <a title="152-tfidf-8" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>9 0.18559062 <a title="152-tfidf-9" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>10 0.18120486 <a title="152-tfidf-10" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>11 0.17102529 <a title="152-tfidf-11" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>12 0.15527481 <a title="152-tfidf-12" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>13 0.15248023 <a title="152-tfidf-13" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>14 0.15046297 <a title="152-tfidf-14" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>15 0.14425994 <a title="152-tfidf-15" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>16 0.14225021 <a title="152-tfidf-16" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>17 0.14161825 <a title="152-tfidf-17" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>18 0.14141086 <a title="152-tfidf-18" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>19 0.13230874 <a title="152-tfidf-19" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>20 0.12759896 <a title="152-tfidf-20" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.227), (1, -0.055), (2, -0.032), (3, 0.004), (4, 0.131), (5, -0.005), (6, -0.036), (7, -0.064), (8, 0.259), (9, -0.227), (10, 0.185), (11, -0.02), (12, 0.14), (13, 0.137), (14, 0.054), (15, 0.002), (16, 0.086), (17, -0.058), (18, 0.013), (19, 0.094), (20, -0.027), (21, 0.0), (22, 0.067), (23, 0.021), (24, 0.079), (25, 0.021), (26, 0.137), (27, 0.002), (28, 0.006), (29, 0.092), (30, -0.023), (31, -0.009), (32, -0.015), (33, 0.074), (34, 0.018), (35, -0.109), (36, 0.013), (37, -0.084), (38, -0.021), (39, 0.023), (40, 0.066), (41, 0.002), (42, -0.036), (43, -0.014), (44, 0.087), (45, -0.121), (46, 0.007), (47, -0.068), (48, -0.086), (49, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95325238 <a title="152-lsi-1" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>2 0.83290893 <a title="152-lsi-2" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>3 0.814852 <a title="152-lsi-3" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>4 0.66471237 <a title="152-lsi-4" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>5 0.65767235 <a title="152-lsi-5" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>6 0.65357548 <a title="152-lsi-6" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>7 0.64779544 <a title="152-lsi-7" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>8 0.63700622 <a title="152-lsi-8" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>9 0.63598233 <a title="152-lsi-9" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>10 0.63296032 <a title="152-lsi-10" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>11 0.62257195 <a title="152-lsi-11" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>12 0.60566634 <a title="152-lsi-12" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>13 0.56382805 <a title="152-lsi-13" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>14 0.55827916 <a title="152-lsi-14" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>15 0.55292892 <a title="152-lsi-15" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>16 0.53132564 <a title="152-lsi-16" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>17 0.52638328 <a title="152-lsi-17" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>18 0.52375144 <a title="152-lsi-18" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>19 0.51060939 <a title="152-lsi-19" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>20 0.50470835 <a title="152-lsi-20" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.102), (16, 0.023), (19, 0.012), (26, 0.386), (33, 0.197), (67, 0.086), (69, 0.031), (87, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8472203 <a title="152-lda-1" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>Author: Adrien Bartoli, Toby Collins</p><p>Abstract: It has been shown that a surface deforming isometrically can be reconstructed from a single image and a template 3D shape. Methods from the literature solve this problem efficiently. However, they all assume that the camera model is calibrated, which drastically limits their applicability. We propose (i) a general variational framework that applies to (calibrated and uncalibrated) general camera models and (ii) self-calibrating 3D reconstruction algorithms for the weak-perspective and full-perspective camera models. In the former case, our algorithm returns the normal field and camera ’s scale factor. In the latter case, our algorithm returns the normal field, depth and camera ’s focal length. Our algorithms are the first to achieve deformable 3D reconstruction including camera self-calibration. They apply to much more general setups than existing methods. Experimental results on simulated and real data show that our algorithms give results with the same level of accuracy as existing methods (which use the true focal length) on perspective images, and correctly find the normal field on affine images for which the existing methods fail.</p><p>2 0.84114206 <a title="152-lda-2" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>Author: Liang Li, Wei Feng, Liang Wan, Jiawan Zhang</p><p>Abstract: This paper addresses a challenging problem of regularizing arbitrary superpixels into an optimal grid structure, which may significantly extend current low-level vision algorithms by allowing them to use superpixels (SPs) conveniently as using pixels. For this purpose, we aim at constructing maximum cohesive SP-grid, which is composed of real nodes, i.e. SPs, and dummy nodes that are meaningless in the image with only position-taking function in the grid. For a given formation of image SPs and proper number of dummy nodes, we first dynamically align them into a grid based on the centroid localities of SPs. We then define the SP-grid coherence as the sum of edge weights, with SP locality and appearance encoded, along all direct paths connecting any two nearest neighboring real nodes in the grid. We finally maximize the SP-grid coherence via cascade dynamic programming. Our approach can take the regional objectness as an optional constraint to produce more semantically reliable SP-grids. Experiments on object localization show that our approach outperforms state-of-the-art methods in terms of both detection accuracy and speed. We also find that with the same searching strategy and features, object localization at SP-level is about 100-500 times faster than pixel-level, with usually better detection accuracy.</p><p>3 0.81743199 <a title="152-lda-3" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>Author: Jordi Pont-Tuset, Ferran Marques</p><p>Abstract: This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p><p>4 0.80894107 <a title="152-lda-4" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>same-paper 5 0.78785646 <a title="152-lda-5" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>6 0.76317763 <a title="152-lda-6" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>7 0.70175344 <a title="152-lda-7" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>8 0.69914353 <a title="152-lda-8" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>9 0.66646826 <a title="152-lda-9" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>10 0.64845079 <a title="152-lda-10" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>11 0.6481365 <a title="152-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.64006048 <a title="152-lda-12" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>13 0.63926834 <a title="152-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.63197112 <a title="152-lda-14" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>15 0.62802953 <a title="152-lda-15" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>16 0.62789047 <a title="152-lda-16" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>17 0.62554336 <a title="152-lda-17" href="./cvpr-2013-Hyperbolic_Harmonic_Mapping_for_Constrained_Brain_Surface_Registration.html">208 cvpr-2013-Hyperbolic Harmonic Mapping for Constrained Brain Surface Registration</a></p>
<p>18 0.62423462 <a title="152-lda-18" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>19 0.62417978 <a title="152-lda-19" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>20 0.62330931 <a title="152-lda-20" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
