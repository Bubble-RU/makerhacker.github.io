<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-302" href="#">cvpr2013-302</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-302-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yuan_Multi-task_Sparse_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chunfeng Yuan, Weiming Hu, Guodong Tian, Shuang Yang, Haoran Wang</p><p>Abstract: In this paper, we formulate human action recognition as a novel Multi-Task Sparse Learning(MTSL) framework which aims to construct a test sample with multiple features from as few bases as possible. Learning the sparse representation under each feature modality is considered as a single task in MTSL. Since the tasks are generated from multiple features associated with the same visual input, they are not independent but inter-related. We introduce a Beta process(BP) prior to the hierarchical MTSL model, which efficiently learns a compact dictionary and infers the sparse structure shared across all the tasks. The MTSL model enforces the robustness in coefficient estimation compared with performing each task independently. Besides, the sparseness is achieved via the Beta process formulation rather than the computationally expensive l1 norm penalty. In terms of non-informative gamma hyper-priors, the sparsity level is totally decided by the data. Finally, the learning problem is solved by Gibbs sampling inference which estimates the full posterior on the model parameters. Experimental results on the KTH and UCF sports datasets demonstrate the effectiveness of the proposed MTSL approach for action recognition.</p><p>Reference: <a title="cvpr-2013-302-reference" href="../cvpr2013_reference/cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 c@omnlp  Abstract In this paper, we formulate human action recognition as a novel Multi-Task Sparse Learning(MTSL) framework which aims to construct a test sample with multiple features from as few bases as possible. [sent-5, score-0.467]
</p><p>2 Learning the sparse representation under each feature modality is considered as a single task in MTSL. [sent-6, score-0.411]
</p><p>3 We introduce a Beta process(BP) prior to the hierarchical MTSL model, which efficiently learns a compact dictionary and infers the sparse structure shared across all the tasks. [sent-8, score-0.543]
</p><p>4 In terms of non-informative gamma hyper-priors, the sparsity level is totally decided by the data. [sent-11, score-0.324]
</p><p>5 Finally, the learning problem is solved by Gibbs sampling inference which estimates the full posterior on the model parameters. [sent-12, score-0.186]
</p><p>6 Experimental results on the KTH and UCF sports datasets  demonstrate the effectiveness of the proposed MTSL approach for action recognition. [sent-13, score-0.477]
</p><p>7 Introduction Recognition of human actions [1] in videos is an important but challenging task in computer vision. [sent-15, score-0.22]
</p><p>8 It has many potential applications, such as smart surveillance, humancomputer interface, video indexing and browsing, automatic analysis of sports events, and virtual reality. [sent-16, score-0.219]
</p><p>9 The fusion of multiple features is effective for recognizing actions as the single feature based representation is not  Figure 1. [sent-18, score-0.235]
</p><p>10 A hierarchical Bayesian model representation of the multi-task sparse learning method. [sent-19, score-0.289]
</p><p>11 [12] represent the video sequence as several part motion descriptors and then encode these descriptors by sparse representation for classification. [sent-26, score-0.367]
</p><p>12 They employ and solve the sparse representation on each part motion descriptor independently, but actually each part is interrelated. [sent-27, score-0.304]
</p><p>13 It capitalizes on shared information between related tasks to improve the performance of individual tasks, and it has been successfully applied to vision problems such as image classification [10], image annotation [3], and object tracking [4]. [sent-29, score-0.181]
</p><p>14 They extend the l1 framework for learning single sparse model to a setting where the goal is to learn a set of jointly sparse models. [sent-31, score-0.425]
</p><p>15 In this paper, motivated by the success of multi-task learning, we propose a novel Multi-Task Sparse Learning(MTSL) model combined with Beta Process Prior for 444222113  human action recognition. [sent-32, score-0.383]
</p><p>16 The proposed MTSL model jointly formulates multiple modalities of features, each feature modality is viewed as a task in MTSL, and the sparsity structures are shared across all the tasks. [sent-33, score-0.385]
</p><p>17 In terms of the Beta process formulation, the inferred sparsity level is intrinsic to the data, while solving the l1 norm usually needs assume a reconstructed residual error or the sparsity level. [sent-35, score-0.412]
</p><p>18 Via setting the common prior for all the task, the sparse structure shared across all the tasks is efficiently inferred which enforces the robustness in coefficient estimation compared with dealing with each task independently. [sent-36, score-0.473]
</p><p>19 The bottom layer of this hierarchical model is composed of individual models with task-specific parame-  ters; On the layer above, tasks are connected together via the common prior placed on the tasks; the top layer above is the hyper-prior, invoked on parameters of the prior at the level below. [sent-38, score-0.289]
</p><p>20 In this paper, two kinds of action features are used for efficiently describing actions in videos. [sent-39, score-0.45]
</p><p>21 The cooccurrence feature exploits the spatio-temporal geometric distribution information about local features in 3D space which is totally ignored in the histogram feature. [sent-42, score-0.217]
</p><p>22 The main contributions of this paper are: • We propose a multi-task sparse learning method for actWioenp recognition, lwtih-taicshk can efficiently gcmometbhionde fmorualtci-ple features to improve the recognition performance. [sent-44, score-0.3]
</p><p>23 •  •  The proposed MTSL is a robust sparse coding method tThhaet pmroinpeoss ecdor MreTlaStLion iss among dspifafersreen cto tdainskgs m teot hoobdtain the shared sparsity pattern which is ignored when learning each task individually. [sent-45, score-0.47]
</p><p>24 I Bna tyheiss way, tioher dictionary is learned using the Beta process construction [8], and  the sparsity level is decided by the data and one doesn’t need to assume the stopping criteria. [sent-47, score-0.542]
</p><p>25 Related Work In the past few years, sparse representation has been successfully applied to computer vision applications, such as object classification [10], tracking [11], face recognition [5], and action recognition [6]. [sent-50, score-0.659]
</p><p>26 In [6], the local spatiotemporal descriptor is represented as some linear combination of few dictionary elements via sparse representation instead of the traditional bag-of-features methods that involve clustering and vector quantization. [sent-51, score-0.658]
</p><p>27 Then one action class or individual training sample is computed by summing the sparse coefficients of all the local descriptors contained. [sent-52, score-0.683]
</p><p>28 The sparse representation of a local feature is richer and more accurate than quantizing it to a single word via nearest neighbor in the bag-of-features method. [sent-53, score-0.328]
</p><p>29 However, the manner of applying sparse representation on low-level features will be time consuming due to large members of lowlevel feature and high computational complexity of solving sparse representation. [sent-54, score-0.567]
</p><p>30 [7] use the log-covariance matrix to represent each video, and employ sparse representation on the log-covariance matrices. [sent-56, score-0.276]
</p><p>31 Sparse representation is applied on the high level video feature, and the label of a test sample is decided on the reconstruction residual error of the training samples from every class. [sent-57, score-0.309]
</p><p>32 In most sparse representation methods, sparsity is measured by the l1-norm with a Lagrangian constant to balance the reconstruction error and the sparsity constraint. [sent-58, score-0.536]
</p><p>33 However, in many applications one may not know the residual error or sparsity level. [sent-61, score-0.197]
</p><p>34 In this paper, motivated by the success of nonparametric Bayesian formulation in CS, we propose a novel Multi-Task Sparse Learning(MTSL) model combined with Beta process prior for human action recognition. [sent-67, score-0.473]
</p><p>35 Section 2 introduces the multiple features used for action representation. [sent-71, score-0.359]
</p><p>36 Section 4 reports experimental results on several human action datasets. [sent-73, score-0.358]
</p><p>37 Multi-feature representations for video context modeling We employ two features to represent each video sequence: histogram feature, and co-occurrence feature. [sent-76, score-0.249]
</p><p>38 They achieve state-of-the-art results for action recognition when combined with the bag-of-features representation. [sent-78, score-0.377]
</p><p>39 Co-occurrence matrices exploit the spatio-temporal proximity distribution about local features in 3D space to characterize geometric context of action class. [sent-80, score-0.384]
</p><p>40 The histogram feature for each video sequence is namely the statical histogram of SIFT descriptors extracted from it. [sent-84, score-0.25]
</p><p>41 Multi-Task Sparse Recognition  Learning  for Action  The proposed multi-task sparse modeling method makes use of multiple modalities of features for action representation. [sent-95, score-0.594]
</p><p>42 We generate J tasks from J different modalities of features associated with the same input video sequences. [sent-96, score-0.251]
</p><p>43 Learning the sparse representation of the video in one feature space is viewed as an individual task. [sent-97, score-0.394]
</p><p>44 Each vector 풚j employs a dictionary matrix Dj ∈ Rmj×K and is represented as = Dj휽j + 흐j, j = 1, . [sent-105, score-0.276]
</p><p>45 , J (2) where 휽j is the set of sparse transform coefficients and 흐j is a residual error vector associated with task j. [sent-108, score-0.413]
</p><p>46 Thus, the sparse coefficients 휽j for different tasks j = 1,. [sent-120, score-0.331]
</p><p>47 Therefore, we impose that 휽j ∈ RK is sparse and 휽j, j = 1, . [sent-124, score-0.19]
</p><p>48 s a binary vse eclteomr defining which members of the dictionary 풅j,k are used to represent the sample 풚j, and 흎j ∼ N(0, α−1IK) is a weight vector with the precision α. [sent-131, score-0.363]
</p><p>49 흎j is in∼tr Nod(u0c,αed because the reconstruction coefficients of the dictionary are not always binary. [sent-132, score-0.37]
</p><p>50 We employ a Beta process to formulate the dictionary Dj and the K-dimensional binary vector 풛j. [sent-133, score-0.334]
</p><p>51 For our problem, the candidate members 풅j,k of our dictionary Dj correspond to the atoms 풙k, and the kth component of 풛j is drawn zj,k ∼ Bernoulli(πk). [sent-136, score-0.436]
</p><p>52 Therefore, the multi-task sparse representation rmnooduelll (isπ expressed as: α0  ∼  N(Dj휽j, α0−1Imj), j = 1, . [sent-137, score-0.269]
</p><p>53 With these parametric definitions, a graphical model of multi-task sparse representation is illustrated in Figure 1. [sent-146, score-0.244]
</p><p>54 The constraint of joint sparsity across different tasks is valuable since different tasks may favor different sparse reconstruction coefficients, yet  444222335  the joint sparsity may enforce the robustness in coefficient estimation. [sent-148, score-0.723]
</p><p>55 Except the dictionary D, all the other variables are initialized randomly. [sent-158, score-0.314]
</p><p>56 Let Dj denote the dictionary associated with the jth task. [sent-159, score-0.366]
</p><p>57 K-SVD is a method to learn an over-complete dictionary for sparse representation. [sent-161, score-0.466]
</p><p>58 For every task and every action class, we obtain an initial dictionary Dj,c of a large size Kc by K-SVD from the training samples Xj,c, where Xj,c represents the training feature matrix associated with the cth class and the jth task. [sent-162, score-0.843]
</p><p>59 (5)  This model yields a Gibbs sampling scheme for posterior sampling given observations Y. [sent-181, score-0.199]
</p><p>60 Classification Rule  Ideally, if a test sample belongs to action class c, the nonzero components in the estimated coefficient vector휽ˆjwill be associated with multiple columns of Dˆj,c from individual action class c. [sent-210, score-0.792]
</p><p>61 Here, Dˆj,c is a more compact and discriminative dictionary learned by the above MTSL model from the initial Dj,c. [sent-211, score-0.276]
</p><p>62 Therefore, we classify 풚j based on how well 풚j is reproduced by the coefficients associated with the learned dictionary of each individual class. [sent-212, score-0.439]
</p><p>63 (13) Using the coefficients 휽ˆcj associated with the action class c, the jth modality 풚j of a test sample is reproduced as 풚ˆj = Dˆj,c휽ˆjc. [sent-227, score-0.61]
</p><p>64 Sample frames from video sequences of the KTH dataset(top), and the UCF sports dataset (bottom). [sent-237, score-0.219]
</p><p>65 Output: label c∗  휽ˆj  Algorithm 1 summaries the details of the optimization and classification procedure of our multi-task sparse learning model for classification. [sent-263, score-0.269]
</p><p>66 In the dictionary learning phase, we first obtain an initial dictionary via K-SVD for every task and every action class. [sent-264, score-1.012]
</p><p>67 Then the proposed MTSL  method is used to obtain a compact and discriminative dictionary from the initial dictionary by Gibbs sampling. [sent-265, score-0.552]
</p><p>68 In the classification phase, the sparse representation of a test sample is achieved by the MTSL model based on the learned dictionary. [sent-266, score-0.322]
</p><p>69 Experiments We tested our approach on three human action datasets: the KTH [17] and UCF sports [18]. [sent-270, score-0.512]
</p><p>70 For the Gamma prior on the sparse common prior α ,we set c0 = d0 = 0 as a default choice which avoids subjective choice and leads to computational simplifications. [sent-274, score-0.262]
</p><p>71 The KTH video database contains six types of human actions (walking, jogging, running, boxing, hand waving and hand clapping) performed by 25 subjects in four different scenarios. [sent-276, score-0.191]
</p><p>72 The UCF sports dataset consists of 150 action videos including 10 sport actions, diving, golf swinging, kicking, weightlifting, horseback riding, running, skating, swinging bench, swinging from side angle and walking. [sent-281, score-0.683]
</p><p>73 The UCF sports database was tested in a leave-one-out manner, cycling each example in as a test video one at a time, following [18] [24] [22]. [sent-283, score-0.245]
</p><p>74 In the first group, the proposed MTSL method handled one single feature by setting the number of tasks J = 1, and it is symbolized by ”Our-ST” in Tables 1, and 2. [sent-287, score-0.303]
</p><p>75 Sparse representation classification based on the l1reguSlpaarirszeatrioepnr,e symbolized by c”aLt1io-SnRbCas”e dino nththee tlables. [sent-289, score-0.288]
</p><p>76 The comparison accuracy (%) performance of single features and feature combination methods on the UCF sports dataset. [sent-304, score-0.282]
</p><p>77 3637 (a) Single features  ActionsCF-SVMST-SRCOur-MTSL  The bases of dictionary are composed by training samples. [sent-311, score-0.312]
</p><p>78 •  The l1 regularization based sparse representation classTifhieca ltion. [sent-312, score-0.244]
</p><p>79 However, the dictionary is learned by the proposed Beta process formulation as summarized in Algorithm 1. [sent-313, score-0.33]
</p><p>80 This method is symbolized by ”L1SRCDL” in the tables. [sent-314, score-0.174]
</p><p>81 c eMnoartienogve arl,l our sparse representation classification under J = 1 was performed on the resulting long feature vector. [sent-325, score-0.332]
</p><p>82 Feature combination based on independent single task sparse representation. [sent-327, score-0.29]
</p><p>83 For each feature, we still employed the same non-parameter Bayesian to solve the single task sparse representation. [sent-329, score-0.279]
</p><p>84 The final classi-  fication is based on the accumulation of all the single task sparse representation. [sent-330, score-0.292]
</p><p>85 Accuracies from our proposed method with other methods for single features and feature combination on the KTH dataset are listed in Table 1. [sent-333, score-0.175]
</p><p>86 It is observed that the l1 regularized sparse representation method ”L1-SRC” performs worse than SVM on both features, but ”L1-SRC” combined with our dictionary learning method, namely ”L1SRC-DL”, high improves the performance and achieves better results than SVM. [sent-335, score-0.59]
</p><p>87 It proves that our dictionary learning method can obtain a more discriminate dictionary to improve the performance. [sent-336, score-0.597]
</p><p>88 The feature combination results are listed in Table 1(b), from which we can see that all feature combination methods improve the recognition accuracy and our approach achieves the best results. [sent-337, score-0.26]
</p><p>89 It is also better than ”STSRC”, which demonstrates that multi-task sparse represen444222668  tation by considering joint sparsity improves the performance compared with the single task sparse representation. [sent-340, score-0.599]
</p><p>90 In Table 2(a), our method and the l1 sparse representation classification combined with dictionary learning all outperform the SVM classifier method. [sent-342, score-0.624]
</p><p>91 It sufficiently demonstrates the effectiveness of our classification method and dictionary learning method. [sent-343, score-0.355]
</p><p>92 67% accuracy, and best results on eight actions of total ten ones on the UCF sports dataset. [sent-345, score-0.245]
</p><p>93 Conclusion In this paper we have presented a new multi-task sparse learning algorithm with a non-parametric Beyesian hierarchy for visual feature combination. [sent-358, score-0.289]
</p><p>94 (i) By Beta process formulation, the vector of reconstruction coefficients is sparse; this imposition of sparseness is distinct from the widely used l1 regularized sparseness, in which many coefficients are small but not exactly zero. [sent-360, score-0.24]
</p><p>95 (ii) In this framework, the data from all tasks are used to jointly infer the priors; given the priors, individual task is learned independently; the estimation of a task  is affected by both its own training data and the other tasks via the common priors. [sent-361, score-0.335]
</p><p>96 (iii) In terms of the non-informative gamma hyper-priors, the sparsity level is totally decided on the data; while in the l1regularized sparse representation the stopping criteria is defined by assuming the reconstructed residual error or the sparsity level. [sent-362, score-0.82]
</p><p>97 Experimental results on the KTH and UCF sports datasets have demonstrated that the proposed multi-task sparse learning method is an effective and efficient way to fuse the complementary features for improving the overall classification performance. [sent-363, score-0.459]
</p><p>98 Shah, ”A 3-dimensional sift descriptor and its application to action recognition,” In Proc. [sent-476, score-0.386]
</p><p>99 Action MACH: a spatiotemporal maximum average correlation height filter for action recognition. [sent-507, score-0.365]
</p><p>100 Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. [sent-559, score-0.388]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mtsl', 0.492), ('action', 0.323), ('beta', 0.297), ('dictionary', 0.276), ('dj', 0.238), ('ucf', 0.212), ('sparse', 0.19), ('symbolized', 0.174), ('sports', 0.154), ('sparsity', 0.132), ('kth', 0.117), ('actions', 0.091), ('gamma', 0.088), ('rmj', 0.087), ('swinging', 0.087), ('posterior', 0.083), ('tasks', 0.075), ('gibbs', 0.072), ('coefficients', 0.066), ('residual', 0.065), ('video', 0.065), ('task', 0.062), ('jth', 0.06), ('sampling', 0.058), ('ffeeaatuturrees', 0.058), ('stopping', 0.055), ('sparseness', 0.054), ('representation', 0.054), ('feature', 0.054), ('decided', 0.053), ('bernoulli', 0.053), ('pursuits', 0.051), ('totally', 0.051), ('histogram', 0.051), ('modality', 0.051), ('bayesian', 0.05), ('compressive', 0.048), ('oij', 0.048), ('listed', 0.047), ('modalities', 0.045), ('learning', 0.045), ('sample', 0.044), ('phase', 0.043), ('kk', 0.043), ('members', 0.043), ('spatiotemporal', 0.042), ('shared', 0.041), ('coefficient', 0.041), ('fication', 0.04), ('cth', 0.038), ('combination', 0.038), ('variables', 0.038), ('fh', 0.037), ('reproduced', 0.036), ('kc', 0.036), ('features', 0.036), ('prior', 0.036), ('laptev', 0.035), ('human', 0.035), ('sift', 0.035), ('aser', 0.034), ('cs', 0.034), ('classification', 0.034), ('density', 0.034), ('videos', 0.032), ('employ', 0.032), ('mj', 0.031), ('individual', 0.031), ('associated', 0.03), ('via', 0.03), ('recognition', 0.029), ('tran', 0.029), ('norm', 0.029), ('descriptors', 0.029), ('reconstruction', 0.028), ('cc', 0.028), ('descriptor', 0.028), ('inferred', 0.028), ('formulation', 0.028), ('layer', 0.027), ('priors', 0.027), ('fm', 0.027), ('employed', 0.027), ('guo', 0.027), ('process', 0.026), ('yuan', 0.026), ('cfyuan', 0.026), ('chunfeng', 0.026), ('cycling', 0.026), ('paisley', 0.026), ('shuang', 0.026), ('clapping', 0.026), ('dino', 0.026), ('nvec', 0.026), ('weightlifting', 0.026), ('expressed', 0.025), ('geometric', 0.025), ('joint', 0.025), ('combined', 0.025), ('svm', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="302-tfidf-1" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>Author: Chunfeng Yuan, Weiming Hu, Guodong Tian, Shuang Yang, Haoran Wang</p><p>Abstract: In this paper, we formulate human action recognition as a novel Multi-Task Sparse Learning(MTSL) framework which aims to construct a test sample with multiple features from as few bases as possible. Learning the sparse representation under each feature modality is considered as a single task in MTSL. Since the tasks are generated from multiple features associated with the same visual input, they are not independent but inter-related. We introduce a Beta process(BP) prior to the hierarchical MTSL model, which efficiently learns a compact dictionary and infers the sparse structure shared across all the tasks. The MTSL model enforces the robustness in coefficient estimation compared with performing each task independently. Besides, the sparseness is achieved via the Beta process formulation rather than the computationally expensive l1 norm penalty. In terms of non-informative gamma hyper-priors, the sparsity level is totally decided by the data. Finally, the learning problem is solved by Gibbs sampling inference which estimates the full posterior on the model parameters. Experimental results on the KTH and UCF sports datasets demonstrate the effectiveness of the proposed MTSL approach for action recognition.</p><p>2 0.32130119 <a title="302-tfidf-2" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>3 0.25726771 <a title="302-tfidf-3" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>4 0.24215207 <a title="302-tfidf-4" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>Author: Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</p><p>Abstract: How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What defines these spatiotemporal patches is their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore, these patches can be used as a discriminative vocabulary for action classification where they demonstrate stateof-the-art performance on UCF50 and Olympics datasets.</p><p>5 0.241675 <a title="302-tfidf-5" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>Author: Feng Shi, Emil Petriu, Robert Laganière</p><p>Abstract: Local spatio-temporal features and bag-of-features representations have become popular for action recognition. A recent trend is to use dense sampling for better performance. While many methods claimed to use dense feature sets, most of them are just denser than approaches based on sparse interest point detectors. In this paper, we explore sampling with high density on action recognition. We also investigate the impact of random sampling over dense grid for computational efficiency. We present a real-time action recognition system which integrates fast random sampling method with local spatio-temporal features extracted from a Local Part Model. A new method based on histogram intersection kernel is proposed to combine multiple channels of different descriptors. Our technique shows high accuracy on the simple KTH dataset, and achieves state-of-the-art on two very challenging real-world datasets, namely, 93% on KTH, 83.3% on UCF50 and 47.6% on HMDB51.</p><p>6 0.23712184 <a title="302-tfidf-6" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>7 0.23058996 <a title="302-tfidf-7" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>8 0.23052131 <a title="302-tfidf-8" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>9 0.23043799 <a title="302-tfidf-9" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>10 0.22237794 <a title="302-tfidf-10" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>11 0.21830429 <a title="302-tfidf-11" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>12 0.21497399 <a title="302-tfidf-12" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>13 0.20711973 <a title="302-tfidf-13" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>14 0.20576626 <a title="302-tfidf-14" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>15 0.20487985 <a title="302-tfidf-15" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>16 0.19069777 <a title="302-tfidf-16" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>17 0.18450172 <a title="302-tfidf-17" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>18 0.17189175 <a title="302-tfidf-18" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>19 0.17021464 <a title="302-tfidf-19" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>20 0.16671591 <a title="302-tfidf-20" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, -0.188), (2, -0.203), (3, 0.058), (4, -0.346), (5, -0.099), (6, -0.0), (7, 0.101), (8, -0.086), (9, -0.003), (10, 0.005), (11, -0.008), (12, -0.029), (13, 0.056), (14, -0.05), (15, -0.002), (16, 0.004), (17, 0.002), (18, 0.072), (19, 0.142), (20, 0.066), (21, -0.014), (22, -0.007), (23, 0.094), (24, 0.043), (25, -0.066), (26, -0.019), (27, 0.002), (28, -0.033), (29, -0.017), (30, 0.015), (31, 0.048), (32, -0.042), (33, 0.02), (34, 0.004), (35, 0.029), (36, -0.024), (37, -0.027), (38, 0.005), (39, -0.025), (40, 0.01), (41, -0.027), (42, -0.044), (43, -0.022), (44, 0.016), (45, 0.041), (46, -0.005), (47, 0.043), (48, -0.022), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93333405 <a title="302-lsi-1" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>Author: Chunfeng Yuan, Weiming Hu, Guodong Tian, Shuang Yang, Haoran Wang</p><p>Abstract: In this paper, we formulate human action recognition as a novel Multi-Task Sparse Learning(MTSL) framework which aims to construct a test sample with multiple features from as few bases as possible. Learning the sparse representation under each feature modality is considered as a single task in MTSL. Since the tasks are generated from multiple features associated with the same visual input, they are not independent but inter-related. We introduce a Beta process(BP) prior to the hierarchical MTSL model, which efficiently learns a compact dictionary and infers the sparse structure shared across all the tasks. The MTSL model enforces the robustness in coefficient estimation compared with performing each task independently. Besides, the sparseness is achieved via the Beta process formulation rather than the computationally expensive l1 norm penalty. In terms of non-informative gamma hyper-priors, the sparsity level is totally decided by the data. Finally, the learning problem is solved by Gibbs sampling inference which estimates the full posterior on the model parameters. Experimental results on the KTH and UCF sports datasets demonstrate the effectiveness of the proposed MTSL approach for action recognition.</p><p>2 0.719684 <a title="302-lsi-2" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>3 0.69988567 <a title="302-lsi-3" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>Author: Michalis Raptis, Leonid Sigal</p><p>Abstract: In this paper, we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework, where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and minefor hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive with the state of the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.</p><p>4 0.68323934 <a title="302-lsi-4" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>Author: Chunfeng Yuan, Xi Li, Weiming Hu, Haibin Ling, Stephen Maybank</p><p>Abstract: Spatio-temporal interest points serve as an elementary building block in many modern action recognition algorithms, and most of them exploit the local spatio-temporal volume features using a Bag of Visual Words (BOVW) representation. Such representation, however, ignorespotentially valuable information about the global spatio-temporal distribution of interest points. In this paper, we propose a new global feature to capture the detailed geometrical distribution of interest points. It is calculated by using the ℛ transform which is defined as an extended 3D discrete Rℛa tdroann transform, followed by applying a tewdo 3-dDir decitsicorneatel two-dimensional principal component analysis. Such ℛ feature captures the geometrical information of the Sinuctehre ℛst points and keeps invariant to geometry transformation and robust to noise. In addition, we propose a new fusion strategy to combine the ℛ feature with the BOVW representation for further improving recognition accuracy. Wpree suetnilitzaea context-aware fusion method to capture both the pairwise similarities and higher-order contextual interactions of the videos. Experimental results on several publicly available datasets demonstrate the effectiveness of the proposed approach for action recognition.</p><p>5 0.67449027 <a title="302-lsi-5" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>6 0.67110348 <a title="302-lsi-6" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>7 0.66741908 <a title="302-lsi-7" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>8 0.6655612 <a title="302-lsi-8" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>9 0.65634793 <a title="302-lsi-9" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>10 0.65321636 <a title="302-lsi-10" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>11 0.65273935 <a title="302-lsi-11" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>12 0.64885944 <a title="302-lsi-12" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>13 0.64047581 <a title="302-lsi-13" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>14 0.63784587 <a title="302-lsi-14" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>15 0.63054037 <a title="302-lsi-15" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>16 0.61503726 <a title="302-lsi-16" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>17 0.60086733 <a title="302-lsi-17" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>18 0.60016304 <a title="302-lsi-18" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>19 0.5786745 <a title="302-lsi-19" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>20 0.57727349 <a title="302-lsi-20" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.194), (16, 0.016), (21, 0.155), (26, 0.041), (33, 0.313), (67, 0.044), (69, 0.044), (87, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96080506 <a title="302-lda-1" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>Author: Mao Ye, Cha Zhang, Ruigang Yang</p><p>Abstract: With the wide-spread of consumer 3D-TV technology, stereoscopic videoconferencing systems are emerging. However, the special glasses participants wear to see 3D can create distracting images. This paper presents a computational framework to reduce undesirable artifacts in the eye regions caused by these 3D glasses. More specifically, we add polarized filters to the stereo camera so that partial images of reflection can be captured. A novel Bayesian model is then developed to describe the imaging process of the eye regions including darkening and reflection, and infer the eye regions based on Classification ExpectationMaximization (EM). The recovered eye regions under the glasses are brighter and with little reflections, leading to a more nature videoconferencing experience. Qualitative evaluations and user studies are conducted to demonstrate the substantial improvement our approach can achieve.</p><p>2 0.9363513 <a title="302-lda-2" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>3 0.93619138 <a title="302-lda-3" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>Author: Jun Hu, Orazio Gallo, Kari Pulli, Xiaobai Sun</p><p>Abstract: We present a novel method for aligning images in an HDR (high-dynamic-range) image stack to produce a new exposure stack where all the images are aligned and appear as if they were taken simultaneously, even in the case of highly dynamic scenes. Our method produces plausible results even where the image used as a reference is either too dark or bright to allow for an accurate registration.</p><p>4 0.93132079 <a title="302-lda-4" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>Author: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar</p><p>Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In thispaper wepresent an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.</p><p>5 0.91719496 <a title="302-lda-5" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>Author: Shuo Wang, Jungseock Joo, Yizhou Wang, Song-Chun Zhu</p><p>Abstract: In this paper, we propose a weakly supervised method for simultaneously learning scene parts and attributes from a collection ofimages associated with attributes in text, where the precise localization of the each attribute left unknown. Our method includes three aspects. (i) Compositional scene configuration. We learn the spatial layouts of the scene by Hierarchical Space Tiling (HST) representation, which can generate an excessive number of scene configurations through the hierarchical composition of a relatively small number of parts. (ii) Attribute association. The scene attributes contain nouns and adjectives corresponding to the objects and their appearance descriptions respectively. We assign the nouns to the nodes (parts) in HST using nonmaximum suppression of their correlation, then train an appearance model for each noun+adjective attribute pair. (iii) Joint inference and learning. For an image, we compute the most probable parse tree with the attributes as an instantiation of the HST by dynamic programming. Then update the HST and attribute association based on the in- ferred parse trees. We evaluate the proposed method by (i) showing the improvement of attribute recognition accuracy; and (ii) comparing the average precision of localizing attributes to the scene parts.</p><p>6 0.91663343 <a title="302-lda-6" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>7 0.9162876 <a title="302-lda-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.91628343 <a title="302-lda-8" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>9 0.91593289 <a title="302-lda-9" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>10 0.91510081 <a title="302-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>same-paper 11 0.91459173 <a title="302-lda-11" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>12 0.91434979 <a title="302-lda-12" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>13 0.91384125 <a title="302-lda-13" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>14 0.91372955 <a title="302-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.91336405 <a title="302-lda-15" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>16 0.91309959 <a title="302-lda-16" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>17 0.91300011 <a title="302-lda-17" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>18 0.91271216 <a title="302-lda-18" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>19 0.91229767 <a title="302-lda-19" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>20 0.91111428 <a title="302-lda-20" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
