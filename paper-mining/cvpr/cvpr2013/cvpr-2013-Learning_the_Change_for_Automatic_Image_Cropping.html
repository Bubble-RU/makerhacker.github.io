<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 cvpr-2013-Learning the Change for Automatic Image Cropping</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-263" href="#">cvpr2013-263</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>263 cvpr-2013-Learning the Change for Automatic Image Cropping</h1>
<br/><p>Source: <a title="cvpr-2013-263-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yan_Learning_the_Change_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jianzhou Yan, Stephen Lin, Sing Bing Kang, Xiaoou Tang</p><p>Abstract: Image cropping is a common operation used to improve the visual quality of photographs. In this paper, we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content, and enhancement of overall composition. Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. In contrast to the many methods that exist for general assessment of image quality, ours specifically examines differences between the original and cropped photo in solving for the crop parameters. To this end, several novel image features are proposed to model the changes in image content and composition when a crop is applied. Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images.</p><p>Reference: <a title="cvpr-2013-263-reference" href="../cvpr2013_reference/cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Learning the Change for Automatic Image Cropping  Jianzhou  Yan1∗  1The Chinese  Stephen  Lin2  University of Hong Kong  Abstract Image cropping is a common operation used to improve the visual quality of photographs. [sent-1, score-0.553]
</p><p>2 In this paper, we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content, and enhancement of overall composition. [sent-2, score-0.721]
</p><p>3 Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. [sent-3, score-0.862]
</p><p>4 In contrast to the many methods that exist for general assessment of image quality, ours specifically examines differences between the original and cropped photo in solving for the crop parameters. [sent-4, score-0.895]
</p><p>5 To this end, several novel image features are proposed to model the changes in image content and composition when a crop is applied. [sent-5, score-0.727]
</p><p>6 Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images. [sent-6, score-0.509]
</p><p>7 Though photos can be appreciably enhanced in this way, cropping is often a tedious and time-consuming task, especially when done for a large set of images. [sent-11, score-0.568]
</p><p>8 Moreover, highquality cropping can be difficult to achieve without some amount of experience or artistic skill. [sent-12, score-0.509]
</p><p>9 For these reasons, much attention has been focused on developing automatic cropping algorithms. [sent-13, score-0.578]
</p><p>10 Previous work The various techniques that have been proposed for image cropping follow one of two general directions. [sent-16, score-0.509]
</p><p>11 (c)(g) Our computed crop regions, bounded by red frames, which avoid distracting regions and aim for good composition. [sent-46, score-0.667]
</p><p>12 Several of these methods search for the region with the highest attention score and then place the crop window around it. [sent-51, score-0.751]
</p><p>13 [26] use face detection to find regions of interest, and then crop the image in a manner that aligns the faces according to one of 14 predefined templates. [sent-54, score-0.587]
</p><p>14 [6] detect a subject region based on human faces, skin color and/or high saliency map values, and place a bounding box around it. [sent-56, score-0.349]
</p><p>15 [18] use eye tracking to help determine the main attention region, then set the crop boundaries such that the region center lies at a certain position in the final image. [sent-58, score-0.696]
</p><p>16 Aside from region-based processing, other attentionbased methods search for a crop window that would re-  ceive the greatest attention. [sent-59, score-0.641]
</p><p>17 [21] determined crops based on the summed saliency values ofcandidate windows. [sent-61, score-0.392]
</p><p>18 Stentiford [19] cropped the photo by finding the window with the highest average attention score among its pixels. [sent-62, score-0.383]
</p><p>19 While the attention-based approach to image cropping helps to remove unnecessary content from an image, it gives little consideration to overall image composition, and thus may lead to a result that is not visually pleasing. [sent-66, score-0.555]
</p><p>20 The other major direction of cropping methods is an aesthetics-based approach that emphasizes the general at9 9 976 671 9 9  tractiveness of the cropped image. [sent-67, score-0.674]
</p><p>21 These methods have much in common with the large amount of work on photo quality assessment [14] [10] [13] [22], which evaluate the aesthetic quality of an image according to low-level image features and certain rules of photographic composition, such as the well-known rule of thirds. [sent-69, score-0.361]
</p><p>22 [17] trained an SVM to label subject regions of a photo as high or low quality, then find the cropping candidate with the highest  quality score. [sent-71, score-0.75]
</p><p>23 [25] learned local aesthetic features based on position relationships among regions, and used this to measure the quality of cropping candidates. [sent-74, score-0.699]
</p><p>24 Aesthetics-based methods do not directly weigh the influence of the starting composition on the ending composition, or which of the original image regions are most suitable for a crop boundary to cut through. [sent-79, score-0.836]
</p><p>25 They also do not explicitly identify the distracting regions in the input image, or model the lengths to which a photographer will go to remove them at the cost of sacrificing compositional quality. [sent-80, score-0.484]
</p><p>26 In this work, we present a technique that directly accounts for these factors in determining the crop boundaries of an input image. [sent-82, score-0.658]
</p><p>27 Together with some standard aesthetic properties, the influence of these features on crop solutions is learned from training sets composed of 1000 image pairs, before and after cropping by three expert photographers. [sent-84, score-1.312]
</p><p>28 Through analysis of the manual cropping results, the image areas that were cut away, and compositional relationships between the original and cropped images, our method is able to generate effective crops that are shown to surpass representative attentionbased and aesthetics-based techniques. [sent-85, score-1.241]
</p><p>29 Crop-out and cut-through values are used to identify promising crop candidates, and then composition scores are additionally considered to obtain the final crop. [sent-87, score-0.738]
</p><p>30 Change-based Cropping In this section, we introduce our method for changebased image cropping, which involves training set construction, feature extraction, and crop optimization. [sent-93, score-0.569]
</p><p>31 Training set construction Our technique learns the impact of various change-based cropping features on cropping results. [sent-97, score-1.09]
</p><p>32 Each image is manually cropped by three expert photographers (graduate students in art whose primary medium is photography) to form three training sets. [sent-100, score-0.419]
</p><p>33 For each crop we record its four parameters: the horizontal and vertical coordinates of the upper left corner (x1, y1) and lower right corner (x2, y2) of the crop window. [sent-101, score-1.066]
</p><p>34 For 300 of the images, one of the photographers also provided up to three reasons for choosing the crop window. [sent-103, score-0.644]
</p><p>35 Our cropping dataset will be made publicly available upon publication of this work. [sent-105, score-0.509]
</p><p>36 They are particularly aimed at modeling major considerations of photographers as they crop a picture. [sent-130, score-0.671]
</p><p>37 Among these are measures of how likely an image region will be cropped away or cut through by the crop boundaries. [sent-131, score-0.841]
</p><p>38 In addition, they account for compositional changes from the original to the cropped image. [sent-132, score-0.359]
</p><p>39 The most significant of these regions is  the foreground, which is the focus of an image and the area around which a crop is produced. [sent-137, score-0.587]
</p><p>40 To obtain the foreground region, we augment the foreground detection method of [5] by incorporating a human face detector [23] into the saliency map computation. [sent-138, score-0.356]
</p><p>41 Several of the proposed cropping features will later be defined with respect to this foreground. [sent-139, score-0.54]
</p><p>42 2  Exclusion features  The first class of features that are extracted in our algorithm are referred to as exclusion features, as they model what types of regions are within original images but are often excluded from final crops. [sent-146, score-0.417]
</p><p>43 The cut-through value represents the chance that a crop boundary will pass through a region with certain properties. [sent-149, score-0.661]
</p><p>44 In determining whether a region should be cropped out or cut through, it is not the color of the region itself that matters, but rather its difference from the foreground and background (e. [sent-151, score-0.538]
</p><p>45 weight between region i and j, DHi,j and DCi,j are the texture and color distances between region iand j,and Mi and Mj are the areas of region iand j,respectively. [sent-165, score-0.333]
</p><p>46 Sharpness Likewise, the sharpness of a region may influence region exclusion, since cuts through blurred regions may be less distracting. [sent-174, score-0.278]
</p><p>47 Others We additionally include a few basic attributes of regions that may have an effect on whether they are cropped out or cut through. [sent-176, score-0.29]
</p><p>48 For each of these regions, we also determine its crop-out and cut-through values by examining the crop provided by the expert photographer. [sent-185, score-0.697]
</p><p>49 The cut-through value is set to 1if a crop boundary passes through the region, and is otherwise set to 0. [sent-187, score-0.567]
</p><p>50 In our work, the following common compositional features of cropped images are utilized: a. [sent-193, score-0.361]
</p><p>51 Distance of saliency map centroid and detected foreground region center from nearest rule-of-thirds point. [sent-194, score-0.336]
</p><p>52 Middle row: mediocre crop windows that may result from not considering certain exclusion features. [sent-207, score-0.787]
</p><p>53 Bottom row: better crop windows that could be obtained by accounting for certain exclusion features. [sent-208, score-0.787]
</p><p>54 (a) The highlighted region has a large color distance, texture distance, and isolation from the foreground, and thus may be preferable to crop out. [sent-209, score-0.725]
</p><p>55 (b) A crop boundary through the highlighted region with low shape complexity is less desirable than a boundary that passes through a more complex region. [sent-210, score-0.719]
</p><p>56 In addition to measuring these compositional features, we account for their changes in going from the original image to the expertly cropped image, in order to infer how the photographer tends to modify the composition of a given photograph. [sent-220, score-0.692]
</p><p>57 To obtain these change-based features, each of the aforementioned compositional features are extracted for the original and cropped images, and their differences are computed. [sent-221, score-0.412]
</p><p>58 In the learning procedure for compositional features, the expert crops from our training set are treated as positive examples. [sent-223, score-0.531]
</p><p>59 With the positive and negative examples, we use SVM regression to predict the probability of a given crop to be a positive example, and use this value as the composition score. [sent-225, score-0.67]
</p><p>60 Crop Selection The cropping parameter space is large, and each possible cropping solution requires calculation of its composition features. [sent-228, score-1.155]
</p><p>61 In the solution space, we note that many candidates are easy to eliminate, since crop boundaries should not pass through regions with high cut-through values, and regions with large crop-out values should generally be excluded. [sent-235, score-0.727]
</p><p>62 This observation is consistent with comments provided by the expert photographer, which indicate that exclusion features are typically considered prior to composition features when deciding a crop. [sent-236, score-0.619]
</p><p>63 Such candidates to eliminate are readily identified, because it does not require computation of compositional features, and exclusion features of image regions need only to be computed once for an image. [sent-237, score-0.544]
</p><p>64 We therefore utilize exclusion features to identify a relatively small set of candidates (500 in our implementation),  3344  and then determine the final crop from this set using both exclusion and compositional features. [sent-238, score-1.323]
</p><p>65 The exclusion energy function used for selecting candidates is based on crop-out, cut-through, and saliency values: Eexclusion  = Ecropout+λ1Ecutthrough+λ2Esaliency  (4)  with the terms formulated as  Ecropout = ? [sent-239, score-0.463]
</p><p>66 ndidate selection energy is evaluated on an ex-  haustive set of crop windows with parameters sampled at 30 pixel intervals on 1000x1000 images. [sent-258, score-0.554]
</p><p>67 The crops corresponding to the 500 lowest energies are taken as candidates for the final crop selection. [sent-259, score-0.796]
</p><p>68 (8) The crop that minimizes Efinal is selected as the final crop. [sent-261, score-0.533]
</p><p>69 The first of these comparison techniques is an extension of [19] that searches for the crop window with the highest average saliency. [sent-270, score-0.611]
</p><p>70 It too is an extension of an existing technique, namely, a modification of [17] that identifies a crop box with the highest aesthetics score. [sent-274, score-0.677]
</p><p>71 We additionally compare our method to a version of it without the change-based composition features, and a version without the exclusion energy, in order to examine the significance of these two changebased components. [sent-277, score-0.449]
</p><p>72 One is the overlap ratio, area(Wp ∩ Wm)/area(Wp ∪ Wm), where Wp is the expert photographer’s crop window∪, and Wm is the generated crop box of a given method. [sent-297, score-1.227]
</p><p>73 The other metric is the boundary displacement error, | |Bp oBthme e| |r/ m4,e wtrihcic ish measures tahrey d diisstpalnacceesm eofn generated crop box b|/o4u,n wdahriiecsh, Bmmea,s furroems tthheos dei ostaf tnhcee photographer, Bp. [sent-298, score-0.585]
</p><p>74 With only Photographer 1’s cropping data used to train our system, we performed tests using each of the expert photographers’ crops as ground truth. [sent-300, score-0.875]
</p><p>75 The table shows that our method clearly outperforms the attention-based and aesthetics-based cropping techniques. [sent-301, score-0.509]
</p><p>76 This consistency suggests some commonality in the way that experts crop images, and that the image crops of various professionals could readily be combined to form a large, concordant training set. [sent-303, score-0.756]
</p><p>77 The results in the table also demonstrate the  importance of both the exclusion and change-based composition features in the performance of our method. [sent-304, score-0.422]
</p><p>78 Aesthetics-based methods often are relatively better, but may change an image differently than a human would, place crop boundaries through regions that are better included or removed as a whole, or maintain distracting content. [sent-307, score-0.758]
</p><p>79 It can also be observed that neglecting exclusion or compositional change features may lead to results less satisfying than those that account for both. [sent-308, score-0.45]
</p><p>80 They were instructed to double-click the crop they like best. [sent-314, score-0.533]
</p><p>81 For the first 60 images, the crop choices are generated using the attention-based and aesthetics-based methods used in the cross-validation, as well as our own method trained from the data of Photographer 1. [sent-317, score-0.533]
</p><p>82 Among the next 120 images, 60 of them are used to compare our method’s crops to those manually generated by two people who are non-experts in photography, while the other 60 images are used for comparing to crops by two expert photographers. [sent-320, score-0.589]
</p><p>83 The remaining photographs are used to compare our method to its variants described in the cross-validation, namely its versions without change-based composition features and without the exclusion energy. [sent-322, score-0.422]
</p><p>84 For a fair number of them, the differences between two or more of the crops are somewhat subtle and require close examination, so we limit the number of these comparisons to avoid user fatigue. [sent-324, score-0.318]
</p><p>85 The results of this user study are shown in Figure 7, which exhibits the number of times a given method’s crop was selected as the best choice. [sent-325, score-0.601]
</p><p>86 We note that this preference is even stronger among the expert photographers, who have a more discerning eye for crop quality. [sent-327, score-0.721]
</p><p>87 Section 4: comparisons to variants of our method without compositional change features or exclusion energy. [sent-379, score-0.474]
</p><p>88 The third set of comparisons indicate that the exclusion and change-based composition features both play an important role in our technique. [sent-383, score-0.446]
</p><p>89 Discussion Our experiments provided us with some basic observations on the differences in cropping technique among the various approaches. [sent-386, score-0.573]
</p><p>90 By contrast, the aestheticsbased method may crop radically with the goal of maximizing its aesthetic score within the crop window, even if this means cropping out parts of the foreground. [sent-389, score-1.671]
</p><p>91 Moreover, we observed that the optimal aesthetics-based crop window in many instances is not especially pleasing, which leads us to believe that there is much progress still to be made on computational aesthetics evaluation. [sent-391, score-0.653]
</p><p>92 We feel that image cropping is a problem less complex than general aesthetics evaluation and that it is better addressed by directly accounting for its particular motivations, i. [sent-392, score-0.593]
</p><p>93 In both cases, they appear to identify the foreground and place the crop box around it without much consideration of image composition. [sent-396, score-0.713]
</p><p>94 However, a major advantage of human croppers over any automatic technique is that regardless of their cropping skill, they are able to clearly identify the foreground in the photograph, which is of great importance in obtaining good cropping results. [sent-397, score-1.277]
</p><p>95 An incorrect foreground detection result, which is generally caused by poor saliency map estimation, will lead to low cropping quality, such as shown in Figure 8. [sent-398, score-0.774]
</p><p>96 (e-g) Attention-based, aesthetics-based, and our cropping result using the incorrect foreground/saliency map. [sent-429, score-0.532]
</p><p>97 Conclusion We presented a technique for automatic image cropping that directly accounts for changes that result from removing unwanted areas. [sent-435, score-0.632]
</p><p>98 Though our method utilizes compositional properties in evaluating crops, it is relatively efficient because of its use of exclusion features to identify a small set of crop candidates. [sent-437, score-1.029]
</p><p>99 As our work relies on existing techniques for foreground detection and saliency map construction, shortcomings in these methods can degrade the quality of our crops. [sent-438, score-0.286]
</p><p>100 Automatic image cropping for mobile devices with built-in camera. [sent-540, score-0.509]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('crop', 0.533), ('cropping', 0.509), ('exclusion', 0.254), ('crops', 0.223), ('cropped', 0.165), ('compositional', 0.165), ('photographer', 0.16), ('saliency', 0.148), ('expert', 0.143), ('composition', 0.137), ('photographers', 0.111), ('photo', 0.098), ('aesthetic', 0.096), ('foreground', 0.094), ('region', 0.094), ('aesthetics', 0.084), ('distracting', 0.08), ('attentionbased', 0.072), ('croppers', 0.072), ('photos', 0.059), ('regions', 0.054), ('isolation', 0.052), ('alue', 0.051), ('cut', 0.049), ('user', 0.049), ('assessment', 0.048), ('quality', 0.044), ('attention', 0.044), ('candidates', 0.04), ('accounts', 0.038), ('unwanted', 0.037), ('changebased', 0.036), ('ecropout', 0.036), ('eexclusion', 0.036), ('efinal', 0.036), ('expertly', 0.036), ('jianzhou', 0.036), ('nishiyama', 0.036), ('rcjropoutv', 0.036), ('santella', 0.036), ('sharpness', 0.036), ('window', 0.036), ('boundary', 0.034), ('ciocca', 0.032), ('suh', 0.032), ('features', 0.031), ('luo', 0.031), ('wp', 0.03), ('marchesotti', 0.03), ('areas', 0.029), ('mj', 0.029), ('original', 0.029), ('wm', 0.029), ('auto', 0.028), ('foregrounds', 0.028), ('considerations', 0.027), ('skewness', 0.027), ('preference', 0.026), ('content', 0.026), ('rj', 0.026), ('identify', 0.025), ('automatic', 0.025), ('boundaries', 0.025), ('comparisons', 0.024), ('subject', 0.024), ('highlighted', 0.024), ('euler', 0.024), ('cheng', 0.024), ('technique', 0.023), ('pages', 0.023), ('incorrect', 0.023), ('removed', 0.023), ('place', 0.023), ('deciding', 0.023), ('icme', 0.023), ('multimedia', 0.022), ('color', 0.022), ('additionally', 0.022), ('photograph', 0.022), ('differences', 0.022), ('extension', 0.021), ('energy', 0.021), ('values', 0.021), ('utilizes', 0.021), ('highest', 0.021), ('utilize', 0.021), ('consideration', 0.02), ('human', 0.02), ('determining', 0.02), ('moments', 0.02), ('photography', 0.02), ('enhancement', 0.019), ('study', 0.019), ('factors', 0.019), ('among', 0.019), ('cr', 0.018), ('box', 0.018), ('construction', 0.018), ('excluded', 0.018), ('digital', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="263-tfidf-1" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>Author: Jianzhou Yan, Stephen Lin, Sing Bing Kang, Xiaoou Tang</p><p>Abstract: Image cropping is a common operation used to improve the visual quality of photographs. In this paper, we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content, and enhancement of overall composition. Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. In contrast to the many methods that exist for general assessment of image quality, ours specifically examines differences between the original and cropped photo in solving for the crop parameters. To this end, several novel image features are proposed to model the changes in image content and composition when a crop is applied. Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images.</p><p>2 0.16599907 <a title="263-tfidf-2" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>3 0.15771429 <a title="263-tfidf-3" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>4 0.14581348 <a title="263-tfidf-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.14179152 <a title="263-tfidf-5" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>6 0.1365968 <a title="263-tfidf-6" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>7 0.12615505 <a title="263-tfidf-7" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>8 0.12506483 <a title="263-tfidf-8" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>9 0.11601181 <a title="263-tfidf-9" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>10 0.092065856 <a title="263-tfidf-10" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>11 0.085662186 <a title="263-tfidf-11" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>12 0.079414375 <a title="263-tfidf-12" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>13 0.075306304 <a title="263-tfidf-13" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>14 0.067421764 <a title="263-tfidf-14" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>15 0.066393867 <a title="263-tfidf-15" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>16 0.065776996 <a title="263-tfidf-16" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>17 0.062771477 <a title="263-tfidf-17" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>18 0.060785592 <a title="263-tfidf-18" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>19 0.060209498 <a title="263-tfidf-19" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>20 0.055760562 <a title="263-tfidf-20" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, -0.055), (2, 0.158), (3, 0.072), (4, -0.012), (5, -0.004), (6, 0.011), (7, -0.027), (8, 0.035), (9, 0.018), (10, 0.005), (11, 0.002), (12, -0.007), (13, -0.002), (14, 0.002), (15, 0.007), (16, 0.008), (17, 0.012), (18, 0.018), (19, -0.009), (20, -0.027), (21, 0.053), (22, -0.038), (23, 0.002), (24, 0.005), (25, -0.014), (26, 0.079), (27, 0.016), (28, -0.049), (29, -0.029), (30, 0.014), (31, 0.02), (32, -0.017), (33, 0.023), (34, 0.027), (35, -0.026), (36, -0.004), (37, -0.006), (38, -0.027), (39, 0.082), (40, -0.014), (41, 0.013), (42, 0.018), (43, 0.011), (44, -0.027), (45, 0.031), (46, 0.019), (47, -0.014), (48, 0.007), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89262038 <a title="263-lsi-1" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>Author: Jianzhou Yan, Stephen Lin, Sing Bing Kang, Xiaoou Tang</p><p>Abstract: Image cropping is a common operation used to improve the visual quality of photographs. In this paper, we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content, and enhancement of overall composition. Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. In contrast to the many methods that exist for general assessment of image quality, ours specifically examines differences between the original and cropped photo in solving for the crop parameters. To this end, several novel image features are proposed to model the changes in image content and composition when a crop is applied. Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images.</p><p>2 0.8072266 <a title="263-lsi-2" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>3 0.80043411 <a title="263-lsi-3" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>4 0.787938 <a title="263-lsi-4" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>5 0.78646302 <a title="263-lsi-5" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>6 0.76666313 <a title="263-lsi-6" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>7 0.73835206 <a title="263-lsi-7" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>8 0.71672153 <a title="263-lsi-8" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>9 0.70003349 <a title="263-lsi-9" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>10 0.68378019 <a title="263-lsi-10" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>11 0.64840043 <a title="263-lsi-11" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>12 0.5940212 <a title="263-lsi-12" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>13 0.57855433 <a title="263-lsi-13" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>14 0.55296355 <a title="263-lsi-14" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>15 0.53416926 <a title="263-lsi-15" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>16 0.52429205 <a title="263-lsi-16" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>17 0.517537 <a title="263-lsi-17" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>18 0.49621558 <a title="263-lsi-18" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>19 0.4954057 <a title="263-lsi-19" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>20 0.49016491 <a title="263-lsi-20" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.1), (16, 0.029), (22, 0.011), (26, 0.048), (33, 0.234), (38, 0.278), (67, 0.064), (69, 0.03), (80, 0.028), (87, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82784015 <a title="263-lda-1" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>Author: Engin Türetken, Fethallah Benmansour, Bjoern Andres, Hanspeter Pfister, Pascal Fua</p><p>Abstract: We propose a novel approach to automated delineation of linear structures that form complex and potentially loopy networks. This is in contrast to earlier approaches that usually assume a tree topology for the networks. At the heart of our method is an Integer Programming formulation that allows us to find the global optimum of an objective function designed to allow cycles but penalize spurious junctions and early terminations. We demonstrate that it outperforms state-of-the-art techniques on a wide range of datasets.</p><p>same-paper 2 0.77989405 <a title="263-lda-2" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>Author: Jianzhou Yan, Stephen Lin, Sing Bing Kang, Xiaoou Tang</p><p>Abstract: Image cropping is a common operation used to improve the visual quality of photographs. In this paper, we present an automatic cropping technique that accounts for the two primary considerations of people when they crop: removal of distracting content, and enhancement of overall composition. Our approach utilizes a large training set consisting of photos before and after cropping by expert photographers to learn how to evaluate these two factors in a crop. In contrast to the many methods that exist for general assessment of image quality, ours specifically examines differences between the original and cropped photo in solving for the crop parameters. To this end, several novel image features are proposed to model the changes in image content and composition when a crop is applied. Our experiments demonstrate improvements of our method over recent cropping algorithms on a broad range of images.</p><p>3 0.76984674 <a title="263-lda-3" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>4 0.7396425 <a title="263-lda-4" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>Author: Lena Gorelick, Frank R. Schmidt, Yuri Boykov</p><p>Abstract: Trust region is a well-known general iterative approach to optimization which offers many advantages over standard gradient descent techniques. In particular, it allows more accurate nonlinear approximation models. In each iteration this approach computes a global optimum of a suitable approximation model within a fixed radius around the current solution, a.k.a. trust region. In general, this approach can be used only when some efficient constrained optimization algorithm is available for the selected nonlinear (more accurate) approximation model. In this paper we propose a Fast Trust Region (FTR) approach for optimization of segmentation energies with nonlinear regional terms, which are known to be challenging for existing algorithms. These energies include, but are not limited to, KL divergence and Bhattacharyya distance between the observed and the target appearance distributions, volume constraint on segment size, and shape prior constraint in a form of 퐿2 distance from target shape moments. Our method is 1-2 orders of magnitude faster than the existing state-of-the-art methods while converging to comparable or better solutions.</p><p>5 0.7317996 <a title="263-lda-5" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied, we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers, we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components, across other videos in the system, to perform subsequent video analysis. We demonstrate the efficacy of our approach for activity classification and clustering by reporting competitive results on standard datasets such as, HMDB, UCF-50, Olympic Sports and KTH.</p><p>6 0.73030704 <a title="263-lda-6" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>7 0.69958323 <a title="263-lda-7" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>8 0.69940966 <a title="263-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.69934344 <a title="263-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.69673544 <a title="263-lda-10" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>11 0.69631237 <a title="263-lda-11" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>12 0.69610226 <a title="263-lda-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.69550586 <a title="263-lda-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.69526255 <a title="263-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.69521403 <a title="263-lda-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.69516605 <a title="263-lda-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.695158 <a title="263-lda-17" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>18 0.69496512 <a title="263-lda-18" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>19 0.69448382 <a title="263-lda-19" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>20 0.69443053 <a title="263-lda-20" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
