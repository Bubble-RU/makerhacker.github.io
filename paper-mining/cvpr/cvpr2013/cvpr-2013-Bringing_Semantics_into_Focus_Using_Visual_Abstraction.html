<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-73" href="#">cvpr2013-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</h1>
<br/><p>Source: <a title="cvpr-2013-73-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zitnick_Bringing_Semantics_into_2013_CVPR_paper.pdf">pdf</a></p><p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>Reference: <a title="cvpr-2013-73-reference" href="../cvpr2013_reference/cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com Abstract Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. [sent-3, score-0.313]
</p><p>2 The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. [sent-4, score-0.384]
</p><p>3 In this paper, we propose studying semantic information in abstract images created from collections of clip art. [sent-6, score-0.651]
</p><p>4 Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. [sent-9, score-0.343]
</p><p>5 Finding analogous sets of semantically similar real images would be nearly impossible. [sent-10, score-0.389]
</p><p>6 We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. [sent-11, score-0.646]
</p><p>7 We thoroughly analyze this dataset to discover semantically  important features, the relations of words to visual features and methods for measuring semantic similarity. [sent-12, score-0.756]
</p><p>8 Introduction A fundamental goal of computer vision is to discover the semantically meaningful information contained within an image. [sent-14, score-0.348]
</p><p>9 Similarly humans may deem two images as semantically similar, even though the arrangement or even the presence of objects may vary dramatically. [sent-17, score-0.401]
</p><p>10 Discovering the subset of image specific information that is semantically meaningful remains a challenging area of research. [sent-18, score-0.348]
</p><p>11 Numerous works have explored related areas, including predicting the salient locations in an image [17, 26], ranking the relative importance of visible objects [1, 5, 16, 31] and semantically interpreting images [7, 18, 24, 38]. [sent-19, score-0.5]
</p><p>12 Unlike traditional approaches that use real images, we hypothesize that the same information can be learned from abstract images rendered from a collection of clip art, as shown in Figure 1. [sent-30, score-0.473]
</p><p>13 Even with a limited set of clip art, the variety and complexity of semantic information that can be conveyed 333000000977  Figure 2. [sent-31, score-0.564]
</p><p>14 An illustration of the clip art used to create the children (left) and the other available objects (right. [sent-32, score-0.735]
</p><p>15 For instance, clip art can correspond to different attributes of an object, such as a person’s pose, facial expression or clothing. [sent-34, score-0.715]
</p><p>16 Using abstract images, even complex relation information can be easily computed given the relative placement of the clip art, such as “Is the person holding an object? [sent-40, score-0.563]
</p><p>17 We accomplish this by first asking human subjects to generate novel scenes and corresponding written descriptions. [sent-43, score-0.4]
</p><p>18 Next, multiple human subjects are asked to generate scenes depicting the same written description without any knowledge of the original scene’s appearance. [sent-44, score-0.578]
</p><p>19 The result is a set of different scenes with similar semantic meaning, as shown in Figure 1. [sent-45, score-0.348]
</p><p>20 Collecting analogous sets of semantically similar real images would be prohibitively difficult. [sent-46, score-0.389]
</p><p>21 y Wfoer estnuvdiysiionng this to be useful for studying a wide variety of tasks, such as generating semantic descriptions of images, or textbased image search. [sent-48, score-0.345]
</p><p>22 • We measure the mutual information between visual featWuree sm aenasdu trhee t hseem maunttuica lc ilnafsosrems attoi odnis bcoetvwere wnh vicishu avli sfueaa-l features are most semantically meaningful. [sent-50, score-0.534]
</p><p>23 Our semantic classes are defined using sets of semantically similar scenes depicting the same written description. [sent-51, score-0.859]
</p><p>24 We show the relative importance of various features, such as the high importance of a person’s facial expression or the occurrence of a dog, and the relatively low importance of some spatial relations. [sent-52, score-0.516]
</p><p>25 t Wpese compute semantically similar nearest neighbors using a metric learning approach [35]. [sent-58, score-0.307]
</p><p>26 Through our various experiments, we study what aspects of the scenes are semantically important. [sent-59, score-0.534]
</p><p>27 We hypothesize that by analyzing the set of semantically important features in abstract images, we may better understand what information needs to be gathered for semantic understanding in all types of visual data, including real images. [sent-60, score-0.784]
</p><p>28 For instance, methods generating novel sentences rely on the automatic detection of objects [9] and attributes [2, 6, 25], and use language statistics [38] or spatial relationships [18] for verb prediction. [sent-65, score-0.33]
</p><p>29 Works in learning semantic attributes [2, 6, 25] are becoming popular for enabling humans and machines to communicate using natural language. [sent-67, score-0.328]
</p><p>30 The use of semantic concepts such as scenes and objects has also been shown to be effective for video retrieval [20]. [sent-68, score-0.409]
</p><p>31 However, our dataset has the  unique property of having sets of semantically similar images, i. [sent-70, score-0.343]
</p><p>32 The works on attributes described above includes the use of adjectives as well as nouns relating to parts of objects. [sent-78, score-0.357]
</p><p>33 With this in mind, we choose to create abstract scenes of children playing outside. [sent-92, score-0.385]
</p><p>34 Our goal is to create a set of scenes that are semantically similar. [sent-96, score-0.539]
</p><p>35 First, we ask subjects on Amazon’s Mechanical Turk (AMT) to create scenes from a collection ofclip art. [sent-98, score-0.368]
</p><p>36 Next, a new set of subjects are asked to  describe the scenes using a one or two sentence description. [sent-99, score-0.469]
</p><p>37 Finally, semantically similar scenes are generated by asking multiple subjects to create scenes depicting the same written description. [sent-100, score-1.036]
</p><p>38 Initial scene creation: Our scenes are created from a collection of 80 pieces of clip art created by an artist, as shown in Figure 2. [sent-102, score-0.852]
</p><p>39 Clip art depicting a boy and girl are created from seven different poses and five different facial expressions, resulting in 35 possible combinations for each, Figure 2(left). [sent-103, score-0.952]
</p><p>40 56 pieces of clip art represent the other objects in the scene, including trees, toys, hats, animals, etc. [sent-104, score-0.604]
</p><p>41 The subjects were given five pieces of clip art for both the boy and girl assembled randomly from the different facial expressions and poses. [sent-105, score-1.374]
</p><p>42 The subjects were instructed to “create an illustration for a children’s story book by creating a realistic scene from the clip art below”. [sent-111, score-0.713]
</p><p>43 At least six pieces of clip art were required to be used, and each clip art could only be used once. [sent-112, score-1.034]
</p><p>44 At most one boy and one girl could be added to the scene. [sent-113, score-0.579]
</p><p>45 Each piece of clip art could be scaled using three fixed sizes and flipped horizontally. [sent-114, score-0.491]
</p><p>46 The depth ordering was automatically computed using the type of clip art, e. [sent-115, score-0.394]
</p><p>47 a hat should appear on top of the girl, and using the clip art scale. [sent-117, score-0.491]
</p><p>48 A simple interface was created that showed a single scene, and the subjects were asked to describe the scene using one or two sentences. [sent-122, score-0.347]
</p><p>49 For those subjects who wished to use proper names in their descriptions, we provided the names “Mike” and “Jenny” for the boy and girl. [sent-123, score-0.461]
</p><p>50 Generating semantically similar scenes: Finally, we generated sets of semantically similar scenes. [sent-126, score-0.65]
</p><p>51 For this task, we asked subjects to generate scenes depicting the written descriptions. [sent-127, score-0.536]
</p><p>52 By having multiple subjects generate scenes for each description, we can create sets of semantically similar scenes. [sent-128, score-0.711]
</p><p>53 First, the subjects were given a written description of a scene and asked to create a scene depicting it. [sent-131, score-0.6]
</p><p>54 Second, the clip art was randomly chosen as above, except we enforced any clip art that was used in the original scene was also included. [sent-132, score-1.037]
</p><p>55 As a result, on average about 25% of the clip art was from the original scene used to create the written description. [sent-133, score-0.689]
</p><p>56 It is important to note that it is critical to ensure that objects that are in the written description are available to the subjects generating the new scenes. [sent-134, score-0.369]
</p><p>57 However this does introduce a bias, since subjects will always have the option of choosing the clip art present  in the original scene even if it is not described in the scene description. [sent-135, score-0.737]
</p><p>58 Thus it is critical that a significant portion of the clip art remains randomly chosen. [sent-136, score-0.491]
</p><p>59 That is, we have 1,002 sets of 10 scenes that are known to be semantically similar. [sent-140, score-0.503]
</p><p>60 Semantic importance of visual features In this section, we examine the relative semantic importance of various scene properties or features. [sent-144, score-0.536]
</p><p>61 For instance, the study of abstract scenes may help research in semantic scene understanding in real images by suggesting to researchers which properties are important to reliably detect. [sent-146, score-0.565]
</p><p>62 To study the semantic importance of features, we need a quantitative measure of semantic importance. [sent-147, score-0.502]
</p><p>63 In this paper, we use the mutual information shared between a specified feature and a set of classes representing semantically similar scenes. [sent-148, score-0.432]
</p><p>64 In our dataset, we have 1002 sets of semantically similar scenes, resulting in 1002 classes. [sent-149, score-0.343]
</p><p>65 For instance, if the MI between a feature and the classes is small, it indicates that the feature provides minimal information for determining whether scenes are semantically similar. [sent-151, score-0.508]
</p><p>66 Next, we describe various sets of features and analyze their semantic importance using Equations (1) and (2). [sent-166, score-0.334]
</p><p>67 Occurrence: We begin by analyzing the simple features corresponding to the occurrence of the various objects that may exist in the scene. [sent-167, score-0.3]
</p><p>68 In our dataset, there exist 58 object instances, since we group all of the variations of the boy together in one instance, and similarly for girl. [sent-170, score-0.364]
</p><p>69 For instance, objects such as the bear, dog, girl or boy are more semantically meaningful than background objects such as trees or hats. [sent-175, score-1.008]
</p><p>70 3%) occur frequently but are less semantically important, whereas bears (11. [sent-180, score-0.34]
</p><p>71 Interestingly, the individual occurrence of boy and girl have higher scores than the category people. [sent-183, score-0.777]
</p><p>72 Person attributes: Since the occurrence of the boy and  girl are semantically meaningful, it is likely their attributes are also semantically relevant. [sent-186, score-1.449]
</p><p>73 The boy and girl clip art have five different facial expressions and seven different poses. [sent-187, score-1.186]
</p><p>74 We compute the CMI of the person attributes conditioned upon the boy or girl being present. [sent-189, score-0.84]
</p><p>75 Interestingly, features that include combinations of the boy, girl and animals provide significant additional information. [sent-196, score-0.353]
</p><p>76 Other features such as girl and balloons actually have high MI but low CMI, since balloons almost always occur with the girl in our dataset. [sent-197, score-0.69]
</p><p>77 The mutual information measuring the dependence between classes of semantically  similar scenes and the (left) occurrence of  obejcts, (top) co-occurrence, relative depth and position, (middle) person attributes and (bottom) the position relative to the head and hand, and absolute position. [sent-202, score-1.292]
</p><p>78 ) The pie chart shows the sum of  the mutual information or conditional mutual information scores for all features. [sent-204, score-0.333]
</p><p>79 The probability of occurrence of each piece of clip art  occurring is shown to the left. [sent-205, score-0.64]
</p><p>80 Intuitively, the position of the boy and girl provide the most additional information, whereas the location of toys and hats matters less. [sent-208, score-0.716]
</p><p>81 For instance, a boy holding a hamburger implies eating, where a hamburger sitting on a table does not. [sent-212, score-0.459]
</p><p>82 As shown in Figure 4, the relative positions of the boy and girl provide the most information. [sent-215, score-0.652]
</p><p>83 CMI scores were conditioned on both the object and the boy or girl. [sent-224, score-0.46]
</p><p>84 The average results for the boy and girl are shown in Figure 4. [sent-225, score-0.579]
</p><p>85 The depth ordering of the objects also provides important semantic information. [sent-229, score-0.308]
</p><p>86 The absolute depth features are conditioned on the object appearing while the  relative depth features are conditioned on the corresponding pair co-occurring. [sent-234, score-0.523]
</p><p>87 Measuring the semantic similarity of images The semantic similarity of images is dependent on the various characteristics of an image, such as the object present, their attributes and relations. [sent-248, score-0.483]
</p><p>88 In this section, we explore the use of visual features for measuring semantic similarity. [sent-249, score-0.33]
</p><p>89 For ground truth, we assume a set of 10 scenes generated using the same sentence are members of the same semantically similar class, Section 3. [sent-250, score-0.568]
</p><p>90 This is not surprising since semantically im-  portant information is commonly quite subtle, and scenes with very different object arrangements might be semantically similar. [sent-262, score-0.815]
</p><p>91 For instance the combination of occurrence and person attributes provides a very effective set of features. [sent-264, score-0.366]
</p><p>92 In fact, occurrence with person attributes has nearly identical results to using the top 200 features overall. [sent-265, score-0.375]
</p><p>93 For in-  stance, occurrence features are informative of nouns, while relative position features are predictive of more verbs, adverbs and prepositions. [sent-278, score-0.467]
</p><p>94 Notice how the relative positions and orientations of the clip art can dramatically alter the words with highest score. [sent-280, score-0.627]
</p><p>95 Discussion The potential of using abstract images to study the highlevel semantic understanding of visual data is especially promising. [sent-282, score-0.355]
</p><p>96 Abstract images allow for the creation of huge datasets of semantically similar scenes that would be impossible with real images. [sent-283, score-0.513]
</p><p>97 High-level semantic visual features can be learned or designed that better predict not only nouns, but other more complex phenomena represented by verbs, adverbs and prepositions. [sent-286, score-0.403]
</p><p>98 Finally, we hypothesize that the study of high-level semantic information using abstract scenes will provide insights into methods for semantically understanding real im-  ages. [sent-288, score-0.909]
</p><p>99 To simulate detections in real images, artificial noise may be added to the visual features to study the effect of noise on inferring semantic information. [sent-291, score-0.403]
</p><p>100 Finally by removing the dependence on varying sets of noisy automatic detectors, abstract scenes allow for more direct comparison between competing methods for extraction of semantic information from visual information. [sent-292, score-0.514]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clip', 0.335), ('boy', 0.325), ('cmi', 0.307), ('semantically', 0.307), ('girl', 0.254), ('semantic', 0.188), ('scenes', 0.16), ('art', 0.156), ('occurrence', 0.149), ('subjects', 0.136), ('children', 0.111), ('attributes', 0.107), ('nouns', 0.101), ('sentence', 0.101), ('depicting', 0.097), ('prepositions', 0.088), ('conditioned', 0.086), ('mutual', 0.084), ('adjectives', 0.077), ('verbs', 0.077), ('adverbs', 0.074), ('relative', 0.073), ('facial', 0.073), ('relating', 0.072), ('asked', 0.072), ('create', 0.072), ('written', 0.071), ('mi', 0.071), ('sentences', 0.07), ('person', 0.068), ('study', 0.067), ('words', 0.063), ('objects', 0.061), ('toys', 0.061), ('depth', 0.059), ('importance', 0.059), ('generating', 0.059), ('descriptions', 0.058), ('absolute', 0.058), ('relations', 0.056), ('scene', 0.055), ('pieces', 0.052), ('visual', 0.051), ('features', 0.051), ('hypothesize', 0.051), ('balloons', 0.049), ('heider', 0.049), ('scores', 0.049), ('understanding', 0.049), ('animals', 0.048), ('gaze', 0.048), ('created', 0.047), ('holding', 0.046), ('real', 0.046), ('biederman', 0.044), ('hamburger', 0.044), ('jenny', 0.044), ('expression', 0.044), ('expressions', 0.043), ('playing', 0.042), ('instance', 0.042), ('berg', 0.042), ('description', 0.042), ('information', 0.041), ('convey', 0.041), ('spm', 0.041), ('hats', 0.041), ('measuring', 0.04), ('studying', 0.04), ('thousand', 0.039), ('psychology', 0.039), ('phenomena', 0.039), ('exist', 0.039), ('dependence', 0.038), ('interface', 0.037), ('food', 0.037), ('sets', 0.036), ('amazon', 0.036), ('rashtchian', 0.035), ('meanings', 0.035), ('tagged', 0.035), ('worn', 0.035), ('position', 0.035), ('gist', 0.034), ('galleguillos', 0.034), ('rabinovich', 0.034), ('kulkarni', 0.034), ('pie', 0.034), ('informative', 0.034), ('numerous', 0.034), ('meaning', 0.033), ('occur', 0.033), ('oliva', 0.033), ('language', 0.033), ('asking', 0.033), ('humans', 0.033), ('parikh', 0.032), ('sadeghi', 0.031), ('story', 0.031), ('xp', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="73-tfidf-1" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>2 0.15090367 <a title="73-tfidf-2" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>Author: Tim Matthews, Mark S. Nixon, Mahesan Niranjan</p><p>Abstract: We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pairwise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence ofattributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.</p><p>3 0.14929472 <a title="73-tfidf-3" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>Author: Sanja Fidler, Abhishek Sharma, Raquel Urtasun</p><p>Abstract: We are interested in holistic scene understanding where images are accompanied with text in the form of complex sentential descriptions. We propose a holistic conditional random field model for semantic parsing which reasons jointly about which objects are present in the scene, their spatial extent as well as semantic segmentation, and employs text as well as image information as input. We automatically parse the sentences and extract objects and their relationships, and incorporate them into the model, both via potentials as well as by re-ranking candidate detections. We demonstrate the effectiveness of our approach in the challenging UIUC sentences dataset and show segmentation improvements of 12.5% over the visual only model and detection improvements of 5% AP over deformable part-based models [8].</p><p>4 0.1488438 <a title="73-tfidf-4" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>Author: Heesoo Myeong, Kyoung Mu Lee</p><p>Abstract: We propose a novel nonparametric approach for semantic segmentation using high-order semantic relations. Conventional context models mainly focus on learning pairwise relationships between objects. Pairwise relations, however, are not enough to represent high-level contextual knowledge within images. In this paper, we propose semantic relation transfer, a method to transfer high-order semantic relations of objects from annotated images to unlabeled images analogous to label transfer techniques where label information are transferred. Wefirst define semantic tensors representing high-order relations of objects. Semantic relation transfer problem is then formulated as semi-supervised learning using a quadratic objective function of the semantic tensors. By exploiting low-rank property of the semantic tensors and employing Kronecker sum similarity, an efficient approximation algorithm is developed. Based on the predicted high-order semantic relations, we reason semantic segmentation and evaluate the performance on several challenging datasets.</p><p>5 0.14308287 <a title="73-tfidf-5" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>Author: Roozbeh Mottaghi, Sanja Fidler, Jian Yao, Raquel Urtasun, Devi Parikh</p><p>Abstract: Recent trends in semantic image segmentation have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning. In this work, we are interested in understanding the roles of these different tasks in aiding semantic segmentation. Towards this goal, we “plug-in ” human subjects for each of the various components in a state-of-the-art conditional random field model (CRF) on the MSRC dataset. Comparisons among various hybrid human-machine CRFs give us indications of how much “head room ” there is to improve segmentation by focusing research efforts on each of the tasks. One of the interesting findings from our slew of studies was that human classification of isolated super-pixels, while being worse than current machine classifiers, provides a significant boost in performance when plugged into the CRF! Fascinated by this finding, we conducted in depth analysis of the human generated potentials. This inspired a new machine potential which significantly improves state-of-the-art performance on the MRSC dataset.</p><p>6 0.14229769 <a title="73-tfidf-6" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>7 0.13704009 <a title="73-tfidf-7" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>8 0.1356526 <a title="73-tfidf-8" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>9 0.12431314 <a title="73-tfidf-9" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>10 0.11039451 <a title="73-tfidf-10" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>11 0.10881326 <a title="73-tfidf-11" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>12 0.10237218 <a title="73-tfidf-12" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>13 0.10140507 <a title="73-tfidf-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.097703658 <a title="73-tfidf-14" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>15 0.096122593 <a title="73-tfidf-15" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>16 0.089733556 <a title="73-tfidf-16" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>17 0.088910215 <a title="73-tfidf-17" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>18 0.083683826 <a title="73-tfidf-18" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>19 0.081957348 <a title="73-tfidf-19" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>20 0.0819498 <a title="73-tfidf-20" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, -0.06), (2, 0.021), (3, -0.047), (4, 0.053), (5, 0.04), (6, -0.127), (7, 0.086), (8, 0.046), (9, 0.048), (10, 0.005), (11, 0.012), (12, 0.003), (13, 0.059), (14, 0.03), (15, 0.004), (16, 0.019), (17, 0.084), (18, -0.037), (19, -0.024), (20, 0.034), (21, -0.027), (22, 0.046), (23, 0.024), (24, -0.048), (25, -0.017), (26, 0.034), (27, 0.001), (28, -0.006), (29, -0.083), (30, -0.088), (31, -0.066), (32, -0.064), (33, -0.0), (34, -0.053), (35, 0.022), (36, -0.038), (37, 0.153), (38, -0.109), (39, 0.014), (40, -0.067), (41, -0.04), (42, -0.122), (43, 0.129), (44, 0.028), (45, 0.006), (46, -0.029), (47, -0.071), (48, 0.063), (49, -0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93248492 <a title="73-lsi-1" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>2 0.68193007 <a title="73-lsi-2" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>Author: Sanja Fidler, Abhishek Sharma, Raquel Urtasun</p><p>Abstract: We are interested in holistic scene understanding where images are accompanied with text in the form of complex sentential descriptions. We propose a holistic conditional random field model for semantic parsing which reasons jointly about which objects are present in the scene, their spatial extent as well as semantic segmentation, and employs text as well as image information as input. We automatically parse the sentences and extract objects and their relationships, and incorporate them into the model, both via potentials as well as by re-ranking candidate detections. We demonstrate the effectiveness of our approach in the challenging UIUC sentences dataset and show segmentation improvements of 12.5% over the visual only model and detection improvements of 5% AP over deformable part-based models [8].</p><p>3 0.68064141 <a title="73-lsi-3" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>4 0.6776371 <a title="73-lsi-4" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>Author: Tim Matthews, Mark S. Nixon, Mahesan Niranjan</p><p>Abstract: We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pairwise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence ofattributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.</p><p>5 0.63874954 <a title="73-lsi-5" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>Author: Yun Jiang, Hema Koppula, Ashutosh Saxena</p><p>Abstract: For scene understanding, one popular approach has been to model the object-object relationships. In this paper, we hypothesize that such relationships are only an artifact of certain hidden factors, such as humans. For example, the objects, monitor and keyboard, are strongly spatially correlated only because a human types on the keyboard while watching the monitor. Our goal is to learn this hidden human context (i.e., the human-object relationships), and also use it as a cue for labeling the scenes. We present Infinite Factored Topic Model (IFTM), where we consider a scene as being generated from two types of topics: human configurations and human-object relationships. This enables our algorithm to hallucinate the possible configurations of the humans in the scene parsimoniously. Given only a dataset of scenes containing objects but not humans, we show that our algorithm can recover the human object relationships. We then test our algorithm on the task ofattribute and object labeling in 3D scenes and show consistent improvements over the state-of-the-art.</p><p>6 0.63479269 <a title="73-lsi-6" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>7 0.63049066 <a title="73-lsi-7" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>8 0.60641849 <a title="73-lsi-8" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>9 0.60615575 <a title="73-lsi-9" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>10 0.58413935 <a title="73-lsi-10" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>11 0.56421685 <a title="73-lsi-11" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>12 0.56274921 <a title="73-lsi-12" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>13 0.52967995 <a title="73-lsi-13" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>14 0.52870011 <a title="73-lsi-14" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>15 0.52084744 <a title="73-lsi-15" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>16 0.50815582 <a title="73-lsi-16" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>17 0.50701755 <a title="73-lsi-17" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>18 0.49812928 <a title="73-lsi-18" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>19 0.49458539 <a title="73-lsi-19" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>20 0.48638114 <a title="73-lsi-20" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.104), (16, 0.018), (26, 0.064), (33, 0.248), (59, 0.011), (67, 0.088), (69, 0.051), (72, 0.019), (77, 0.014), (80, 0.015), (87, 0.075), (99, 0.229)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85609889 <a title="73-lda-1" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>Author: Yuichi Takeda, Shinsaku Hiura, Kosuke Sato</p><p>Abstract: In this paper we propose a novel depth measurement method by fusing depth from defocus (DFD) and stereo. One of the problems of passive stereo method is the difficulty of finding correct correspondence between images when an object has a repetitive pattern or edges parallel to the epipolar line. On the other hand, the accuracy of DFD method is inherently limited by the effective diameter of the lens. Therefore, we propose the fusion of stereo method and DFD by giving different focus distances for left and right cameras of a stereo camera with coded apertures. Two types of depth cues, defocus and disparity, are naturally integrated by the magnification and phase shift of a single point spread function (PSF) per camera. In this paper we give the proof of the proportional relationship between the diameter of defocus and disparity which makes the calibration easy. We also show the outstanding performance of our method which has both advantages of two depth cues through simulation and actual experiments.</p><p>2 0.84103131 <a title="73-lda-2" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<p>Author: Won Hwa Kim, Moo K. Chung, Vikas Singh</p><p>Abstract: The analysis of 3-D shape meshes is a fundamental problem in computer vision, graphics, and medical imaging. Frequently, the needs of the application require that our analysis take a multi-resolution view of the shape ’s local and global topology, and that the solution is consistent across multiple scales. Unfortunately, the preferred mathematical construct which offers this behavior in classical image/signal processing, Wavelets, is no longer applicable in this general setting (data with non-uniform topology). In particular, the traditional definition does not allow writing out an expansion for graphs that do not correspond to the uniformly sampled lattice (e.g., images). In this paper, we adapt recent results in harmonic analysis, to derive NonEuclidean Wavelets based algorithms for a range of shape analysis problems in vision and medical imaging. We show how descriptors derived from the dual domain representation offer native multi-resolution behavior for characterizing local/global topology around vertices. With only minor modifications, the framework yields a method for extracting interest/key points from shapes, a surprisingly simple algorithm for 3-D shape segmentation (competitive with state of the art), and a method for surface alignment (without landmarks). We give an extensive set of comparison results on a large shape segmentation benchmark and derive a uniqueness theorem for the surface alignment problem.</p><p>same-paper 3 0.82970357 <a title="73-lda-3" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>4 0.81570357 <a title="73-lda-4" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>Author: Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu, Cunzhao Shi</p><p>Abstract: In this paper, we propose a novel method for cross-view action recognition via a continuous virtual path which connects the source view and the target view. Each point on this virtual path is a virtual view which is obtained by a linear transformation of the action descriptor. All the virtual views are concatenated into an infinite-dimensional feature to characterize continuous changes from the source to the target view. However, these infinite-dimensional features cannot be used directly. Thus, we propose a virtual view kernel to compute the value of similarity between two infinite-dimensional features, which can be readily used to construct any kernelized classifiers. In addition, there are a lot of unlabeled samples from the target view, which can be utilized to improve the performance of classifiers. Thus, we present a constraint strategy to explore the information contained in the unlabeled samples. The rationality behind the constraint is that any action video belongs to only one class. Our method is verified on the IXMAS dataset, and the experimental results demonstrate that our method achieves better performance than the state-of-the-art methods.</p><p>5 0.80509478 <a title="73-lda-5" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>6 0.79730254 <a title="73-lda-6" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>7 0.78796184 <a title="73-lda-7" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>8 0.78636968 <a title="73-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.78213632 <a title="73-lda-9" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>10 0.78193402 <a title="73-lda-10" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>11 0.7805171 <a title="73-lda-11" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>12 0.77954382 <a title="73-lda-12" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>13 0.77854747 <a title="73-lda-13" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>14 0.77844858 <a title="73-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.77832711 <a title="73-lda-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.77793872 <a title="73-lda-16" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>17 0.77731878 <a title="73-lda-17" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>18 0.77616531 <a title="73-lda-18" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>19 0.77610701 <a title="73-lda-19" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>20 0.77545118 <a title="73-lda-20" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
