<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 cvpr-2013-Fast Convolutional Sparse Coding</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-164" href="#">cvpr2013-164</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>164 cvpr-2013-Fast Convolutional Sparse Coding</h1>
<br/><p>Source: <a title="cvpr-2013-164-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bristow_Fast_Convolutional_Sparse_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Hilton Bristow, Anders Eriksson, Simon Lucey</p><p>Abstract: Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence.</p><p>Reference: <a title="cvpr-2013-164-reference" href="../cvpr2013_reference/cvpr-2013-Fast_Convolutional_Sparse_Coding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. [sent-7, score-0.462]
</p><p>2 For many natural signals however, sparse coding is applied to sub-elements (i. [sent-9, score-0.437]
</p><p>3 Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. [sent-12, score-0.619]
</p><p>4 In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence. [sent-13, score-0.163]
</p><p>5 nN=1||xn− Dzn||22+ β||zn||1  subject to  | |dk | |22 ≤ 1 for k = 1. [sent-20, score-0.05]
</p><p>6 K,  (1)  where β controls the L1 penalty, and the inequality constraint on the columns of D prevent the dictionary from absorbing all of the system’s energy. [sent-23, score-0.084]
</p><p>7 A selection of filters learned from an unaligned set of lions. [sent-26, score-0.164]
</p><p>8 The spatially invariant algorithm produces expression of generic Gabor-like filters as well as specialized domain specific filters, such as the highlighted “eye” . [sent-27, score-0.185]
</p><p>9 Sparse coding has a fundamental drawback however, as it assumes the ensemble of input vectors are tin adsesupmenedsen tht eof e one abnleot ohfe irn, pi. [sent-28, score-0.231]
</p><p>10 This independence assumption, when applied to nat-  {xn}nN=1  ural images, leads to many basis elements that are translated versions of each other. [sent-31, score-0.046]
</p><p>11 Convolutional sparse coding attempts to remedy this shortcoming by modelling shift invariance directly within the objective,  argmdi,zn subject to  21||x −? [sent-32, score-0.45]
</p><p>12 (2)  Now zk takes the role of a sparse feature map which, when convolved with a filter dk and added over all k, should approximate the input signal x: a full signal 333898991  ( e. [sent-38, score-0.72]
</p><p>13 image or audio sequence) rather than independent patches as in the objective of Equation (1) . [sent-40, score-0.13]
</p><p>14 Like traditional sparse coding the estimated sparse basis {dk}kK=1 twioilnl able s pofa a efi cxoeddi nspga tthiael e support. [sent-41, score-0.541]
</p><p>15 Hspoawrseeve bra, uisn {lidke} traditional sparse coding the input signal x and the sparse feature maps {zk}kK=1 are of a different and usually mfeautcuhr ela mrgearp sdim {zen}sionality. [sent-42, score-0.587]
</p><p>16 Note also that we assume there is only a single signal x in our formulation in Equation (2) ; it is trivial in our proposed formulation to handle multiple signals each of varying length. [sent-43, score-0.166]
</p><p>17 We persist with the single signal assumption throughout the derivation of our approach for the sake of clarity and brevity. [sent-44, score-0.092]
</p><p>18 A 2D convolution operation is represented as the ∗ operatcoonr. [sent-52, score-0.124]
</p><p>19 We have chosen to use a Fourier representation in this paper due to its particularly useful ability to represent convolutions as a Hadamard product in the Fourier domain. [sent-55, score-0.042]
</p><p>20 represents tthhee H faacdtam thaatrd product, =anˆ zd diag() ish an operator tehnatts transforms a D dimensional vector into a D D ditmreannssifoonrmals diagonal menatsriioxn. [sent-58, score-0.119]
</p><p>21 a Commutativity Dho ×lds D Dw dithithis property such that role of filter or signal ˆa can be interchanged. [sent-59, score-0.147]
</p><p>22 Any transpose operator T on a complex vector or matrix in this paper additionally takes the complex conjugate in a similar fashion to the Hermitian adjoint [17] . [sent-60, score-0.095]
</p><p>23 With respect to Equation (2) the objective of central interest in this paper – we will often omit filter indices (e. [sent-61, score-0.145]
</p><p>24 dk refers to the kth filter and zk refers to the kth filter response) when referring  z  z  to the variables being optimized. [sent-63, score-0.493]
</p><p>25 z  z  Prior Art: The idea that sparse part-based representations form the computational foundations of visual systems has been supported by Olshausen and Field [15, 16] and many other studies [8, 20, 21] . [sent-75, score-0.165]
</p><p>26 Neurons in the inferotemporal cortex respond to moderately complex features which are invariant to the position of stimulus within the visual field. [sent-76, score-0.109]
</p><p>27 The resulting features were complex patterns rather than the Gabor-like features obtained by sparse coding. [sent-78, score-0.132]
</p><p>28 The idea of truly shift-invariant or “convolutional” sparse coding was first proposed by Lewicki and Sejnowski for discrete 1D time-varying signals [12] , and later generalized to images by Mrup et al. [sent-80, score-0.437]
</p><p>29 ’s work in convolutional sparse coding was motivated by the study of deconvolutional networks [26, 27] , which are closely related to the seminal works of Lecun on convolutional networks [9, 10] . [sent-83, score-1.027]
</p><p>30 proposed to solve the objective in Equation (2) through an alternation strategy where one solves  a sequence of convex subproblems until convergence. [sent-85, score-0.209]
</p><p>31 The approach alternates between solving the subproblem d given a fixed z, and the subproblem z given a fixed d. [sent-86, score-0.586]
</p><p>32 A drawback to this strategy however, is the computational overhead associated with both subproblems. [sent-87, score-0.048]
</p><p>33 The introduction of convolution necessitates the use of gradient solvers for each subproblem, with linear convergence properties dramatically affecting the convergence properties of the overall algorithm. [sent-88, score-0.415]
</p><p>34 Zeiler further introduced an auxiliary variable, t, to separate the convolution from the L1 regularization (allowing for an explicit and efficient solution to t using soft thresholding) . [sent-89, score-0.197]
</p><p>35 Instead of enforcing the equality constraint z = t explicitly, the authors add a quadratic term 2μ | |z − t | |22 to penalize violations. [sent-90, score-0.108]
</p><p>36 This quadratic penalty can |b|e reinterpreted as a trust region constraint | |z −t | |22 ≤ ? [sent-91, score-0.131]
</p><p>37 In order to satisfy tshtrea equality constraint, μ m ? [sent-94, score-0.066]
</p><p>38 The optimal value of subproblem d requires solution of a QCQP. [sent-97, score-0.293]
</p><p>39 To avoid this added computational burden, Zeiler normalizes the solution to an unconstrained minimization, and while this tends to work in practice, it is an approximation not guaranteed to converge to the global minima of the original constrained objective (see Figure 2) . [sent-98, score-0.159]
</p><p>40 Similar to Zeiler’s method is FISTA, from the family  of proximal gradient methods [1] . [sent-99, score-0.038]
</p><p>41 It is a well known iterative method capable of solving L1 regularized least squares problems with quadratic convergence properties. [sent-100, score-0.132]
</p><p>42 m Aeuthgomde notfe dm uLlatgipralinergse ( mAeDthModMs) we use – – hs uavche similar quadratic convergence properties under more modest conditions [2, 25] and through their capacity 333999002  to compose functions, present fast, scalable and distributed solvers. [sent-103, score-0.17]
</p><p>43 We argue that an algorithmic speedup can be obtained by applying an ADMM approach to the objective as a whole rather than the L1 subproblem alone. [sent-105, score-0.383]
</p><p>44 • We demonstrate that the convolution subproblWeme can oben tsoralvteed heffaitci tehnetly c aonnvdo eluxtpiloicnit lsyu bipnr tohbeFourier domain; outperforming conventional gradient solvers that use spatial convolution. [sent-106, score-0.159]
</p><p>45 By incorporating this approach within an ADMM optimization strategy the inequality constraints on  •  •  the norm of the dictionary elements can be satisfied exactly by scaling the solution to an isotropic problem through the introduction of an additional auxillary variable. [sent-107, score-0.167]
</p><p>46 We propose a quad-decomposition of the objectWivee pinrotop ssueb apr qoublaedm-dse ctohmatp are convex aen odb can be solved efficiently and explicitly without the need for gradient or sparse solvers. [sent-108, score-0.132]
</p><p>47 As a result, we demonstrate an improvement in the computational efficiency of convolutional sparse coding over canonical methods (i. [sent-109, score-0.635]
</p><p>48 Finally, we present a convolutional sparse coding alillbyr,ar wy,e w phreicshe can p cloungv-oalnudt-ipolnaayl d sipreacrtsely cinodtoexisting image and audio coding applications: hiltonbristow . [sent-113, score-0.906]
</p><p>49 Problem Reformulation Our proposed approach to solving convolutional sparse coding involves the introduction of two auxiliary variables t and s as well as the posing of the convolutional portion of the objective in the Fourier domain,  argd m,si,zn,t subject to  21D|| ˆx −k? [sent-116, score-1.154]
</p><p>50 (3)  Φ is a D M submatrix of the Fourier matrix F = [ΦΦ, i Φs ⊥a] Dth ×at Mcor sruebsmpoantdrsix xto o a shmea Fllo supriaetria ml support o=f the filter where M ? [sent-129, score-0.055]
</p><p>51 Fourier convolution is not etxhaect fillyte req wuhivearelen Mt t ? [sent-131, score-0.124]
</p><p>52 Ignoring these for the moment (see Boundary Effects on mitigating these differences) the objective in Equation (3) is equivalent to the original objective in Equation (2) . [sent-134, score-0.18]
</p><p>53 In our proposed Fourier formulation is a D dimensional vector like ˆx and zk, whereas in the original spatial formulation in Equation (2) dk ∈ RM is of a significantly smaller dimensionality to M∈ ? [sent-135, score-0.142]
</p><p>54 en Dforc ceo rtrheesmaller spatial constraint through the auxiliary variable s, which now becomes separable (in terms of variables) to the convolutional component of the objective. [sent-138, score-0.345]
</p><p>55 original approach, we also separate the L1 penalty term from the convolutional component of the objective using the auxiliary variable t. [sent-140, score-0.524]
</p><p>56 dˆk  ’s  Augmented Lagrangian: In this paper we propose to handle the introduced equality constraints through an augmented Lagrangian approach [3] . [sent-141, score-0.146]
</p><p>57 The augmented Lagrangian of our proposed objective can be formed as,  L(d, s, z, t, λs, λt) =  21D|| ˆx −k? [sent-142, score-0.17]
</p><p>58 zˆ k||22+ β||t||1 + λsT(s − [ΦT ⊗ IK]dˆ) + λtT(z − t)  +μ2s||s − [ΦT⊗ IK]dˆ||22 +μ2t||z − t||22 subject to  | |sk | |22 ≤ 1 for k = 1. [sent-144, score-0.05]
</p><p>59 (4)  where λp and μp denote the Lagrange multiplier and penalty weighting for the two auxiliary variables p ∈ {pes,n ta}l yres wpeeigchtitvienlgy. [sent-148, score-0.288]
</p><p>60 A full description of ADMMs is outside the scope of this paper (readers are encouraged to inspect [3] for a full treatment and review) , but they can be loosely interpreted as applying a GaussSeidel optimization strategy to the augmented Lagragian objective. [sent-151, score-0.128]
</p><p>61 Such a strategy is advantageous as it often leads to extremely efficient subproblem decompo-  ×  sitions. [sent-152, score-0.341]
</p><p>62 We detail each of the subproblems following: Subproblem z: z∗ = arg mzin L(z;d,s,t,λs, λt) (5) =  F−1{argm zˆin21|| xˆ −Dˆ zˆ||22+  ˆλtT(ˆ z −tˆ) +μ2t|| zˆ −tˆ||22} (6) = F−1{(DˆTDˆ + μtI)−1(DˆT xˆ + μtˆt −λˆt)} (7) where Dˆ = [diag(ˆd1) , . [sent-159, score-0.114]
</p><p>63 Although the size of the matrix Dˆ is KD KD, it is sparse banded, and an efficient Dvar iisab KleD Dre ×or KdeDrin,g i tex i ssts sp (asersee Figure d3), such that the optimal z∗ can be found as the solution to D independent K K linear systems. [sent-163, score-0.132]
</p><p>64 Subproblem t: t∗  =  arg mtin L(t; d, s, z, λs, λt)  (8)  ×  argmtinμ2t||z − t||22+ λtT(z − t) + β||t||1  = (9) Unlike subproblem z, the solution to t cannot be efficiently computed in the Fourier domain, since the L1 norm is not rotation invariant. [sent-164, score-0.369]
</p><p>65 Since the objective in Equation (9) does not contain any rotations of the data, each element of t = [t1, . [sent-166, score-0.09]
</p><p>66 , tD]T can be solved independently,  z  λˆt  = argmtinβ|t| + λ(z − t) +2μ(z − t)2  t∗ (10) where the optimal solution for each t can be found efficiently using the shrinkage function, t∗  = sgn? [sent-169, score-0.04]
</p><p>67 Subproblem d: d∗ = arg msin L(d;s, z,t,λs, λt) =  (12)  F−1{argmdˆin21|| xˆ −Zˆdˆ||22+ λˆsT(ˆd − sˆ ) +μ2s||dˆ − sˆ ||22}  =  . [sent-173, score-0.093]
</p><p>68 In a similar fashion to subproblem z, even though the size of the matrix is KD KD, a similar variable reordering exists suchZ t ish Kat Dfin×dKinDg ,t ahe s iomptiliamra val dria∗ inlev orelvoersd sroinlugti eoxn-  Zˆ  to D independent K Subproblem s: s∗  = =  ×  K linear systems. [sent-178, score-0.366]
</p><p>69 arg msin L(s;d,z,t,λs, λt)  (15)  argmsinμ2s||dˆ − [ΦT⊗ IK]s||22+ ˆλsT(dˆ − [ΦT ⊗ IK]s) subject to | |sk | |22 ≤ 1 for k = 1. [sent-179, score-0.143]
</p><p>70 (16)  In its general form, solving Equation (16) efficiently is problematic as it is a quadratically constrained quadratic programming (QCQP) problem. [sent-183, score-0.128]
</p><p>71 Fortunately due to the kronecker product with the identity matrix IK it can be broken down into K independent problems, sk∗  =  argmsiknμ2s||dˆk− ΦTsk||22+λˆsTk(dˆk− ΦTsk) subject to ||sk||22 ≤ 1 . [sent-184, score-0.05]
</p><p>72 | ˜sk| ˜s|2k−,1 s˜k, oitfh | e ˜rswk|i 2 se≥ 1 where,  (18)  sk = (μsΦΦT)−1(Φdˆk  + Φλˆsk)  . [sent-186, score-0.189]
</p><p>73 As a result one never needs to actually construct the sub-matrix Φ in order to estimate sk . [sent-194, score-0.189]
</p><p>74 Lagrange Multiplier Update:  λ(ti+1) λ(si+1)  ← ←  λt(i) + μt(z(i+1) − t(i+1)) λs(i) + μs(d(i+1) − s(i+1))  (21) (22)  Penalty Update: Superlinear convergence of the ADMM may be achieved if μ(i) → ∞ [18] . [sent-195, score-0.09]
</p><p>75 We use their “Fruit” dataset consisting of 10 images, apply local contrast normalization and select random 50 50 subimages  Number of filters  Number of input training images  Figure 4. [sent-205, score-0.114]
</p><p>76 Time to convergence as a function of (left) the number of filters learned with fixed number of input images,  and (right) the number input images with fixed number of filters. [sent-206, score-0.204]
</p><p>77 Comparison of objective when learning 64 filters from the experiments of Figure 4. [sent-208, score-0.204]
</p><p>78 Our objective starts at a much larger value than Zeiler et al. [sent-209, score-0.09]
</p><p>79 ’s, due to added Lagrange multipliers and penalty terms, but quickly converges to a good solution. [sent-210, score-0.199]
</p><p>80 We perform two experiments, first holding the number of training examples fixed whilst varying the number of filters learned, then visa versa. [sent-212, score-0.114]
</p><p>81 A representative contains Gabor-like  set of 450 filters learned on a laptop using a collection of whitened  components, as well as more expressive  the spatially-invariant  centre-surround  natural images. [sent-216, score-0.114]
</p><p>82 and cross-like components  The set  which appear since  learning strategy produces less spatially shifted versions of the filters. [sent-217, score-0.083]
</p><p>83 ’s method (red line) with respect to the number of filters being learned. [sent-219, score-0.114]
</p><p>84 This is largely due to two contributing factors: (i) the direct methods we use to solve each subproblem have super-linear convergence properties, whereas the conjugate gradient method employed by Zeiler et al. [sent-220, score-0.432]
</p><p>85 is limited to linear convergence, and (ii) convolution in the Fourier domain involves a simple Hadamard product whereas convolutions must be explicitly recomputed for each iteration of conjugate gradients. [sent-221, score-0.248]
</p><p>86 Our method starts at a much larger objective value, due to the additional Lagrange multiplier and penalty terms. [sent-227, score-0.238]
</p><p>87 The objective quickly decreases however, as these terms vanish and the equality constraints are satisfied. [sent-228, score-0.156]
</p><p>88 The overall curve of our objective is typical of ADMMs: steep convergence to begin with, followed by flat-lining and minimal convergence beyond that point. [sent-229, score-0.27]
</p><p>89 Applying sparse coding algorithms to the original Olshausen and Field dataset [15] has become a standard “sanity check” , to ensure that the method is capable of producing Gabor-like oriented edge filters. [sent-230, score-0.363]
</p><p>90 While our convolutional algorithm produces some Gabor-like responses, it also has a greater expression of non-Gabor filters which are tailored more towards the semantics of the dataset. [sent-234, score-0.386]
</p><p>91 Figure 1 shows a compelling example of this artifact, with one of the filters clearly synthesizing an “eye” feature from a set of unaligned lions. [sent-235, score-0.164]
</p><p>92 Conclusions  We presented a method for solving convolutional sparse coding problems in a fast manner through quaddecomposition of the original objective into subproblems that have an efficient parameterization in the Fourier domain. [sent-237, score-0.796]
</p><p>93 These components working in union allow us to solve the rotation invariant L1 subproblem for each index independently using soft thresholding, and transform the quadratically constrained filter update equation to an unconstrained isotropic system. [sent-238, score-0.573]
</p><p>94 As filter support size increases, the appeal of Fourier convolution becomes more apparent, and where boundary effects are problematic the Fourier transform can be seamlessly replaced by the Discrete Cosine Transform. [sent-239, score-0.213]
</p><p>95 Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. [sent-260, score-0.092]
</p><p>96 Properties of basis functions generated by shift invariant sparse representations of natural images. [sent-288, score-0.253]
</p><p>97 Shift invariant sparse coding of image and music data. [sent-329, score-0.401]
</p><p>98 Emergence of simplecell receptive field properties by learning a sparse code for natural images. [sent-334, score-0.17]
</p><p>99 Recognition of objects and their component parts: responses of single units in the temporal cortex of the macaque. [sent-369, score-0.071]
</p><p>100 Linear spatial pyramid matching using sparse coding for image classification. [sent-386, score-0.363]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zeiler', 0.326), ('subproblem', 0.293), ('convolutional', 0.272), ('coding', 0.231), ('zk', 0.207), ('fourier', 0.201), ('sk', 0.189), ('dk', 0.142), ('lagrange', 0.134), ('sparse', 0.132), ('convolution', 0.124), ('filters', 0.114), ('admms', 0.11), ('olshausen', 0.1), ('diag', 0.097), ('signal', 0.092), ('objective', 0.09), ('convergence', 0.09), ('penalty', 0.089), ('admm', 0.081), ('augmented', 0.08), ('signals', 0.074), ('kd', 0.074), ('auxiliary', 0.073), ('alms', 0.073), ('argmtin', 0.073), ('hashimoto', 0.073), ('lewicki', 0.073), ('wersing', 0.073), ('multipliers', 0.073), ('ish', 0.073), ('subproblems', 0.071), ('cortex', 0.071), ('equality', 0.066), ('anders', 0.065), ('eggert', 0.065), ('fista', 0.065), ('equation', 0.064), ('australia', 0.06), ('tsk', 0.06), ('multiplier', 0.059), ('ik', 0.058), ('tt', 0.056), ('filter', 0.055), ('alternating', 0.054), ('monotone', 0.054), ('hilton', 0.052), ('quadratically', 0.052), ('lagrangian', 0.051), ('unaligned', 0.05), ('bue', 0.05), ('msin', 0.05), ('hadamard', 0.05), ('deconvolutional', 0.05), ('subject', 0.05), ('conjugate', 0.049), ('inequality', 0.048), ('strategy', 0.048), ('adelaide', 0.047), ('basis', 0.046), ('operator', 0.046), ('vectorized', 0.044), ('arg', 0.043), ('quadratic', 0.042), ('convolutions', 0.042), ('shrinkage', 0.04), ('del', 0.04), ('audio', 0.04), ('lecun', 0.038), ('invariant', 0.038), ('properties', 0.038), ('simon', 0.038), ('proximal', 0.038), ('converges', 0.037), ('shift', 0.037), ('unconstrained', 0.036), ('dictionary', 0.036), ('bilinear', 0.035), ('shifted', 0.035), ('award', 0.035), ('networks', 0.035), ('isotropic', 0.035), ('solvers', 0.035), ('problematic', 0.034), ('nn', 0.034), ('variables', 0.034), ('domain', 0.033), ('minima', 0.033), ('td', 0.033), ('foundations', 0.033), ('xne', 0.033), ('qcqp', 0.033), ('ofd', 0.033), ('romberg', 0.033), ('yres', 0.033), ('aslmlearll', 0.033), ('mtin', 0.033), ('oppenheim', 0.033), ('ottoe', 0.033), ('vthecet', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="164-tfidf-1" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>Author: Hilton Bristow, Anders Eriksson, Simon Lucey</p><p>Abstract: Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence.</p><p>2 0.202177 <a title="164-tfidf-2" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>3 0.19011118 <a title="164-tfidf-3" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>Author: Roberto Rigamonti, Amos Sironi, Vincent Lepetit, Pascal Fua</p><p>Abstract: Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-theart methods on the linear structure extraction task, in terms ofboth accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.</p><p>4 0.16835059 <a title="164-tfidf-4" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>Author: Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann Lecun</p><p>Abstract: Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.</p><p>5 0.14552425 <a title="164-tfidf-5" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>6 0.11740869 <a title="164-tfidf-6" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>7 0.10807315 <a title="164-tfidf-7" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>8 0.10372517 <a title="164-tfidf-8" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>9 0.10207013 <a title="164-tfidf-9" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>10 0.096097209 <a title="164-tfidf-10" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>11 0.095820226 <a title="164-tfidf-11" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>12 0.095731243 <a title="164-tfidf-12" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>13 0.094620183 <a title="164-tfidf-13" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>14 0.093567133 <a title="164-tfidf-14" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>15 0.093057536 <a title="164-tfidf-15" href="./cvpr-2013-Continuous_Inference_in_Graphical_Models_with_Polynomial_Energies.html">95 cvpr-2013-Continuous Inference in Graphical Models with Polynomial Energies</a></p>
<p>16 0.092599943 <a title="164-tfidf-16" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>17 0.089568548 <a title="164-tfidf-17" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>18 0.086866915 <a title="164-tfidf-18" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>19 0.084510237 <a title="164-tfidf-19" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>20 0.081759259 <a title="164-tfidf-20" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, -0.021), (2, -0.101), (3, 0.125), (4, 0.001), (5, 0.0), (6, 0.022), (7, -0.031), (8, -0.055), (9, -0.024), (10, -0.005), (11, -0.025), (12, -0.015), (13, -0.076), (14, 0.02), (15, 0.071), (16, -0.035), (17, 0.036), (18, 0.142), (19, 0.045), (20, -0.047), (21, -0.033), (22, -0.023), (23, -0.087), (24, 0.01), (25, 0.141), (26, -0.04), (27, -0.024), (28, 0.001), (29, -0.064), (30, -0.015), (31, -0.044), (32, -0.036), (33, -0.098), (34, -0.117), (35, 0.083), (36, 0.011), (37, 0.099), (38, 0.094), (39, 0.016), (40, -0.082), (41, -0.068), (42, 0.021), (43, -0.021), (44, -0.102), (45, -0.05), (46, 0.073), (47, -0.034), (48, -0.019), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95823985 <a title="164-lsi-1" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>Author: Hilton Bristow, Anders Eriksson, Simon Lucey</p><p>Abstract: Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence.</p><p>2 0.81327093 <a title="164-lsi-2" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>Author: Roberto Rigamonti, Amos Sironi, Vincent Lepetit, Pascal Fua</p><p>Abstract: Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-theart methods on the linear structure extraction task, in terms ofboth accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.</p><p>3 0.71354097 <a title="164-lsi-3" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>Author: Laurent Sifre, Stéphane Mallat</p><p>Abstract: An affine invariant representation is constructed with a cascade of invariants, which preserves information for classification. A joint translation and rotation invariant representation of image patches is calculated with a scattering transform. It is implemented with a deep convolution network, which computes successive wavelet transforms and modulus non-linearities. Invariants to scaling, shearing and small deformations are calculated with linear operators in the scattering domain. State-of-the-art classification results are obtained over texture databases with uncontrolled viewing conditions.</p><p>4 0.7092616 <a title="164-lsi-4" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>5 0.64730388 <a title="164-lsi-5" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Complex real-world signals, such as images, contain discriminative structures that differ in many aspects including scale, invariance, and data channel. While progress in deep learning shows the importance of learning features through multiple layers, it is equally important to learn features through multiple paths. We propose Multipath Hierarchical Matching Pursuit (M-HMP), a novel feature learning architecture that combines a collection of hierarchical sparse features for image classification to capture multiple aspects of discriminative structures. Our building blocks are MI-KSVD, a codebook learning algorithm that balances the reconstruction error and the mutual incoherence of the codebook, and batch orthogonal matching pursuit (OMP); we apply them recursively at varying layers and scales. The result is a highly discriminative image representation that leads to large improvements to the state-of-the-art on many standard benchmarks, e.g., Caltech-101, Caltech-256, MITScenes, Oxford-IIIT Pet and Caltech-UCSD Bird-200.</p><p>6 0.62812847 <a title="164-lsi-6" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>7 0.62783837 <a title="164-lsi-7" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>8 0.61558437 <a title="164-lsi-8" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>9 0.60649604 <a title="164-lsi-9" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>10 0.59953195 <a title="164-lsi-10" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>11 0.59763032 <a title="164-lsi-11" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>12 0.5890556 <a title="164-lsi-12" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>13 0.57020748 <a title="164-lsi-13" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>14 0.55632043 <a title="164-lsi-14" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>15 0.54514211 <a title="164-lsi-15" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>16 0.54234618 <a title="164-lsi-16" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>17 0.53727204 <a title="164-lsi-17" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<p>18 0.52541 <a title="164-lsi-18" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>19 0.5171988 <a title="164-lsi-19" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>20 0.50819218 <a title="164-lsi-20" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.142), (16, 0.028), (26, 0.032), (27, 0.247), (28, 0.016), (33, 0.255), (57, 0.022), (67, 0.054), (69, 0.065), (80, 0.011), (87, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84628445 <a title="164-lda-1" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>same-paper 2 0.83195901 <a title="164-lda-2" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>Author: Hilton Bristow, Anders Eriksson, Simon Lucey</p><p>Abstract: Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence.</p><p>3 0.81568617 <a title="164-lda-3" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.</p><p>4 0.79636258 <a title="164-lda-4" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>Author: Anelia Angelova, Shenghuo Zhu</p><p>Abstract: We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also ‘zoom in ’ on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios [4, 21]. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging largescale flower dataset, containing 578 species of flowers and 250,000 images.</p><p>5 0.77153552 <a title="164-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.77055609 <a title="164-lda-6" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>7 0.77042216 <a title="164-lda-7" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>8 0.76879555 <a title="164-lda-8" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>9 0.76860362 <a title="164-lda-9" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>10 0.76794982 <a title="164-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.7678653 <a title="164-lda-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.7675513 <a title="164-lda-12" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>13 0.76734066 <a title="164-lda-13" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>14 0.76692641 <a title="164-lda-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.76662219 <a title="164-lda-15" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>16 0.76628208 <a title="164-lda-16" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>17 0.76588738 <a title="164-lda-17" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>18 0.76575798 <a title="164-lda-18" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>19 0.76555121 <a title="164-lda-19" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>20 0.76513916 <a title="164-lda-20" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
