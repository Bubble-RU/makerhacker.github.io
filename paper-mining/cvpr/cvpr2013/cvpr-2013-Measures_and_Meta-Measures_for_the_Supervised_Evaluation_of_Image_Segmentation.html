<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-281" href="#">cvpr2013-281</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</h1>
<br/><p>Source: <a title="cvpr-2013-281-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Pont-Tuset_Measures_and_Meta-Measures_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jordi Pont-Tuset, Ferran Marques</p><p>Abstract: This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p><p>Reference: <a title="cvpr-2013-281-reference" href="../cvpr2013_reference/cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper tackles the supervised evaluation of image segmentation algorithms. [sent-3, score-0.213]
</p><p>2 First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. [sent-4, score-0.383]
</p><p>3 To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. [sent-5, score-0.336]
</p><p>4 As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. [sent-7, score-0.198]
</p><p>5 We make the datasets and code of all the measures publicly available. [sent-8, score-0.201]
</p><p>6 Semantic segmentation is the final objective, where detection and segmentation meet, but it is still far from being solved [8]. [sent-11, score-0.31]
</p><p>7 In this scenario, bottom-up segmentation methods often play an important role in the proposed algorithms [1, 5], and thus improving segmentation techniques would entail improvements towards better semantic segmentation [18]. [sent-12, score-0.532]
</p><p>8 [11] stress that the results should be evaluated beyond performance summary measures in order to “help understand how one method could be improved. [sent-15, score-0.265]
</p><p>9 Examples of the meta-measure principles: How good are the evaluation measures at distinguishing these pairs of partitions? [sent-18, score-0.323]
</p><p>10 Back to segmentation assessment, the precision-recall curves for boundaries [20] are good examples of tools that provide richer feedback than the F-measure used as summary. [sent-20, score-0.329]
</p><p>11 Moreover, as pointed out by [2], in addition to boundary-based measures, region-oriented measures should be considered when assessing segmentations. [sent-21, score-0.233]
</p><p>12 However, the current ones are limited to summary measures [30, 22, 20, 2, 15, 13, 7, 4, 25, 24]. [sent-22, score-0.265]
</p><p>13 Summary measures also play a role in performance comparison, thus the question that now arises is how to compare the goodness of an evaluation measure. [sent-25, score-0.328]
</p><p>14 The principle of a meta-measure is to assume a plausible hypothesis about the segmentation evaluation and analyze how well measures match this hypothesis. [sent-27, score-0.466]
</p><p>15 222111223 919  The first approach to an extensive quantitative metameasure was proposed in [19]. [sent-30, score-0.214]
</p><p>16 The hypothesis in this work was that measures should be able to discriminate between two pairs of human-marked partitions coming from different images (for instances, the two partitions in Figure 1. [sent-31, score-0.952]
</p><p>17 In an annotated database with multiple partitions per image, the quantitative meta-measure was defined as the number of same-image partition pairs that the measure judges as less similar than other pairs of partitions coming from different images. [sent-33, score-1.159]
</p><p>18 [14] presented a comparison of some measures in terms of this meta-measure. [sent-34, score-0.201]
</p><p>19 Instead of basing our hypotheses on human-made partitions, we extend the analysis to partitions from six State-of-the-Art (SoA) segmentation algorithms. [sent-36, score-0.537]
</p><p>20 The first assumption is that measures should be capable of distinguishing such partitions from those obtained without taking into account the content of the image. [sent-37, score-0.494]
</p><p>21 The metameasure is then defined as the number of results from SoA algorithms that are judged worse than the quadtree. [sent-41, score-0.239]
</p><p>22 As a qualitative example, we assess how well a measure distinguishes between partitions like Figure 1. [sent-42, score-0.429]
</p><p>23 As a second approach, we assume that any measure should be able to distinguish a partition obtained by a SoA method on an image from a partition obtained by the same method but on a different image, as the two partitions shown in Figure 1. [sent-44, score-0.695]
</p><p>24 The meta-measure in this case is defined as the number of cases in which the measure correctly judges the same-image partition as better. [sent-46, score-0.382]
</p><p>25 The third contribution is to survey and structure a wide set of evaluation measures and the newly-proposed one and compare them using the three previously discussed metameasures. [sent-47, score-0.259]
</p><p>26 We show that the two precision-recall measures (boundary- and objects-and-parts-based) have outstanding results as summary measures with respect to the rest ofmeasures, while providing richer information for researchers to interpret the results. [sent-48, score-0.629]
</p><p>27 We further interpret these two precision-recall environments by comparing six SoA segmentation algorithms. [sent-49, score-0.242]
</p><p>28 We make the code to compute all the measures publicly available in [28], as well as all the segmentation results to make our research reproducible and to make it effortless for researchers to assess their segmentation methods. [sent-50, score-0.598]
</p><p>29 Section 2 reviews and structures the main segmentation measures available in the literature. [sent-52, score-0.381]
</p><p>30 Section 5 presents the experimental comparison of the measures using the  three meta-measures. [sent-55, score-0.201]
</p><p>31 It also shows the applicability of the boundary-based and the newly proposed precision-recall curves for objects and parts in the comparison of six SoA segmentation techniques. [sent-56, score-0.358]
</p><p>32 Measure Review and Structure The state-of-the-art measures can be classified depending on the image partition interpretation on which they are based. [sent-59, score-0.434]
</p><p>33 The most common interpretation is as a clustering of the pixel set into a number of subsets or regions, A partition can also be interpreted as a two-class clustering of the set of pairs of pixels, with some pairs linking pixels from the same region and others linking pixels from different regions. [sent-60, score-0.509]
</p><p>34 Finally, a partition can be represented as a two-class clustering of the pixel contours into boundaries and non-boundaries. [sent-61, score-0.256]
</p><p>35 The following sections review the main measures found under each of these interpretations, keeping the notation from the original papers where possible. [sent-62, score-0.201]
</p><p>36 Pixel-Set Clustering The directional Hamming distance from one partition S to another S? [sent-66, score-0.195]
</p><p>37 In [4] this same  measure was coined as asymmetric partition distance. [sent-75, score-0.233]
</p><p>38 It is equivalent to the achievable segmentation accuracy [23] used in superpixel assessment. [sent-76, score-0.155]
</p><p>39 )  (2)  The segmentation covering of a partition S by a partition S? [sent-80, score-0.493]
</p><p>40 It is shown in [4] that it is equivalent to the minimum number of pixels that must not be taken into account for the two partitions to be identical. [sent-89, score-0.293]
</p><p>41 Measure structure overview for the three interpretations of an image partition  scene interpretation. [sent-91, score-0.202]
</p><p>42 As the author points out, these measures are not suitable for general-purpose image segmentation evaluation. [sent-92, score-0.356]
</p><p>43 (4) The work in [22] introduced a new point of view to the measures of clustering assessment based on informationtheoretic results. [sent-103, score-0.339]
</p><p>44 The author defines a discrete random variable taking N values that consists in randomly picking any pixel in the partition S = {R1, . [sent-104, score-0.169]
</p><p>45 n Assuming all the pixels equally probable to pick, the entropy H(S) associated with a partition is defined as the entropy of such random variable. [sent-108, score-0.169]
</p><p>46 Pairs-of-Pixels Classification An image partition can be viewed as a classification of all the pairs of pixels into two classes: pairs of pixels belonging to the same region, and pairs of pixels from different regions. [sent-117, score-0.361]
</p><p>47 , = we (dpivide )P ∈ ∈in Ito × ×fo Iu|ri d  < γo and classify the regions in both partitions as described in Algorithm 1, where “←” means that a region is sclcarisbsiefdied in only oifr i tth previously edi “d← ←no”t mhaevaen a more f raevgoiroanbl ise classification. [sent-124, score-0.351]
</p><p>48 be the number of object candidates in S and G, respectively (note that they can differ, given that G Algorithm  1  Region  candidates  classification  Algorithm 1Region candidates classification  1:for all Ri∈ S, Rj? [sent-126, score-0.177]
</p><p>49 ← Noise 12: end if 13: end for  can be formed by more than one partition and thus a region in S can be matched as object with more than one region in G), and pc and pc? [sent-131, score-0.271]
</p><p>50 Regarding the fragmentation candidates, we compute the percentage of the object that could be formed from the matched parts. [sent-133, score-0.23]
</p><p>51 Formally, we define the amount of fragmentation fr(Ri) of a region Ri ∈ S as the addition of the relative overlaps of the part cand∈ida Ste ass sm thatech aeddd ttioo Rni o:  fr(Ri) =? [sent-134, score-0.219]
</p><p>52 is computed adding the amount offragmentation among all the fragmentation candidates of S and G, respectively. [sent-143, score-0.247]
</p><p>53 The partition circle is a fragmentation candidate with a fragmentation of 1 (parts cover it totally), and the ground-truth half-circles are parts candidates. [sent-151, score-0.607]
</p><p>54 The opposite holds for the triangles, but in this case the fragmentation is 0. [sent-152, score-0.188]
</p><p>55 Meta-Measures This section is about how to compare the goodness of the segmentation evaluation measures. [sent-159, score-0.282]
</p><p>56 The objective of this section is therefore not to tell which segmentation algorithm to use, but which evaluation measures better summarize the quality of these algorithms. [sent-160, score-0.451]
</p><p>57 To distinguish these two analyses, we will refer to the quantitative metrics to compare segmentation measures as meta-measures. [sent-161, score-0.408]
</p><p>58 A meta-measure analysis must rely on accepted hypotheses about the segmentation results and assess how coherent the measures are with such hypotheses. [sent-162, score-0.456]
</p><p>59 The meta-measure is then defined as a quantization of how coherent the evaluation measures are with this judgment [30, 4]. [sent-164, score-0.328]
</p><p>60 In order to cope with this variability, the Berkeley segmentation dataset (BSDS300 [21] and BSDS500 [2]) consists of a set of images each of them manually segmented by more than one individual. [sent-175, score-0.155]
</p><p>61 The hypothesis behind the first meta-measure is that an evaluation measure should be able to tell apart the groundtruth partitions coming from two different images. [sent-176, score-0.585]
</p><p>62 In other words, given a pair of ground-truth partitions from BSDS500, a measure should be able to tell whether they come from the same image (thus differences are an acceptable refinement) or different images (unacceptable discrepancies). [sent-177, score-0.394]
</p><p>63 As first proposed by [19] to evaluate the coherence of BSDS300, given an evaluation measure m, we compute the Probability Density Function (PDF) of the values of m for all the pairs of partitions in BSDS500, grouped in two classes: those coming from different images and those from the same one. [sent-178, score-0.528]
</p><p>64 Figure 3 shows the PDFs for these two types of pairs of partitions using the Fb measure. [sent-179, score-0.357]
</p><p>65 In gray rectangles, four representative pairs of partitions: a pair of correctly classified as different image (up-left) and as same image (up-right); and a pair incorrectly classified as different image (down-left) and as same image (down-right). [sent-182, score-0.178]
</p><p>66 Swapped-Image Human Discrimination (SIHD) metameasure is defined as the percentage of correct classifi-  cations of that classifier, that is, the sum of the area under the curve above and below the threshold for the sameimage and different-image pairs, respectively. [sent-183, score-0.204]
</p><p>67 ) As qualitative examples, Figure 3 depicts four pairs of partitions as representatives of the type of mistakes and correct classifications using Fb. [sent-185, score-0.385]
</p><p>68 SoA-Baseline Discrimination One of the reasons why SIHD can be criticized is the fact that it is based only on human-made partitions, that is, it does not show how measures handle the real-world discrepancies found between SoA segmentation methods. [sent-188, score-0.398]
</p><p>69 This subsection and the following are devoted to present two meta-mesures based on SoA segmentation results. [sent-189, score-0.155]
</p><p>70 These partitions are interpreted as a baseline, that is, the results that could be obtained by chance. [sent-191, score-0.32]
</p><p>71 In particular, we build the hierarchical partitions starting from the whole 222111333533  image and iteratively dividing the regions into four equal  rectangles. [sent-193, score-0.32]
</p><p>72 b shows an example of partition obtained by a SoA method and by a quadtree. [sent-195, score-0.169]
</p><p>73 For each of the techniques considered as SoA segmentation methods, we compute the number of images in the dataset in which an evaluation measure correctly judges that the baseline result is worse than the SoA generated partition. [sent-196, score-0.426]
</p><p>74 Swapped-Image SoA Discrimination Segmentation evaluation measures are often used to adjust the parameters of a segmentation technique. [sent-200, score-0.414]
</p><p>75 They are therefore used to compare different partitions created by the same algorithm. [sent-201, score-0.323]
</p><p>76 To incorporate this type of comparisons to the meta-measures, we compare (i) the results created by a SoA segmentation technique with (ii) the results created by that same algorithm but on a different image. [sent-202, score-0.215]
</p><p>77 In other words, we compare the ground-truth of a certain image with two results obtained using the same algorithm and parameterization: (i) one segmentation of that same image and (ii) one of a different image. [sent-203, score-0.155]
</p><p>78 The hypothesis in this case is that the evaluation measures should judge that the same-image result is better than the different-image one. [sent-204, score-0.342]
</p><p>79 c, the measure should judge that the first partition is better than the second one compared both with the ground-truth of the former. [sent-206, score-0.264]
</p><p>80 In this meta-measure, evaluation measures have to tackle the potential bias of the  SoA methods towards their specific type of results. [sent-207, score-0.259]
</p><p>81 For each SoA segmentation technique, we compute the number of images in the dataset in which an evaluation measure correctly judges that the same-image SoA result is better than the different-image one. [sent-208, score-0.426]
</p><p>82 We define the metameasure Swapped-Image SoA Discrimination as the percentage of results in the database, for all the SoA methods, that the measures correctly discriminates. [sent-209, score-0.443]
</p><p>83 The exact parameterizations for each algorithm is detailed at [28], where we also publish the code of all measures and meta-measures used in this work. [sent-212, score-0.201]
</p><p>84 Meta-Measures Results: Table 2 shows the three metameasure results for the test set of BSDS500, as well as a global summary meta-measure. [sent-271, score-0.226]
</p><p>85 On top of that, they both provide much richer information in form of precision-recall curves, thus we propose the pair Fb-Fop as the measures of choice. [sent-274, score-0.234]
</p><p>86 To provide an in-depth analysis of the final results, the tandem of precision-recall curves for boundaries and for objects-andparts would be the most adequate option. [sent-286, score-0.185]
</p><p>87 The solid curves represent the six SoA segmentation methods and the quadtree (see legends). [sent-290, score-0.414]
</p><p>88 the six SoA segmentation methods studied and the human performance. [sent-295, score-0.245]
</p><p>89 Prior to the assessment of segmentation techniques, let us focus on the comparison of the two evaluation frameworks. [sent-296, score-0.313]
</p><p>90 Regarding the comparison among segmentation techniques, both frameworks confirm that the gPb-OWT-UCM technique has outstanding results with respect to the rest. [sent-313, score-0.222]
</p><p>91 The advantages of going beyond the summary measures are also clear on these plots. [sent-314, score-0.265]
</p><p>92 For instance, the summary Fb measure of quadtree (0. [sent-315, score-0.263]
</p><p>93 55), but in the precision-recall curves it is clear that quadtree is much worse. [sent-317, score-0.199]
</p><p>94 The measures are coherent also in the fact that  human results have a better precision than recall. [sent-320, score-0.258]
</p><p>95 To sum up, both measures are complementary thus we propose them in tandem as the tool of choice for image segmentation evaluation. [sent-328, score-0.428]
</p><p>96 Conclusions This paper reviews an extensive set of segmentation evaluation measures and presents the new precision-recall measure for objects and parts. [sent-331, score-0.503]
</p><p>97 Three meta-measures are used  (two newly proposed) to quantitatively compare the goodness of the evaluation measures. [sent-332, score-0.174]
</p><p>98 The results show that the tandem boundary and objects-and-parts precision-recall curves is a good candidate for benchmarking segmentation algorithms; since apart from obtaining the best metameasure results, their precision-recall curves provide rich knowledge about the results. [sent-333, score-0.579]
</p><p>99 A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. [sent-477, score-0.185]
</p><p>100 Binary partition tree as an effi-  [28] [29] [30]  [3 1]  [32]  cient representation for image processing, segmentation, and information retrieval. [sent-506, score-0.169]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('soa', 0.481), ('fop', 0.324), ('partitions', 0.293), ('fb', 0.222), ('measures', 0.201), ('fragmentation', 0.188), ('partition', 0.169), ('metameasure', 0.162), ('segmentation', 0.155), ('egb', 0.135), ('quadtree', 0.135), ('judges', 0.111), ('nwmc', 0.108), ('osij', 0.108), ('assessment', 0.1), ('dh', 0.096), ('bce', 0.081), ('oigj', 0.081), ('rj', 0.077), ('judged', 0.077), ('tandem', 0.072), ('goodness', 0.069), ('assessed', 0.068), ('voi', 0.067), ('dvd', 0.067), ('entail', 0.067), ('discrimination', 0.066), ('measure', 0.064), ('summary', 0.064), ('curves', 0.064), ('pairs', 0.064), ('ri', 0.062), ('six', 0.06), ('candidates', 0.059), ('evaluation', 0.058), ('oc', 0.055), ('tpami', 0.054), ('dongen', 0.054), ('sihd', 0.054), ('hypothesis', 0.052), ('quantitative', 0.052), ('fr', 0.052), ('boundaries', 0.049), ('coming', 0.049), ('bgm', 0.048), ('swapped', 0.048), ('newly', 0.047), ('tip', 0.045), ('contour', 0.045), ('asx', 0.044), ('ncuts', 0.044), ('assess', 0.044), ('researchers', 0.043), ('percentage', 0.042), ('discrepancies', 0.042), ('judgment', 0.042), ('pop', 0.042), ('pc', 0.04), ('gi', 0.04), ('marques', 0.04), ('else', 0.039), ('classified', 0.038), ('correctly', 0.038), ('clustering', 0.038), ('frameworks', 0.038), ('tell', 0.037), ('fowlkes', 0.033), ('interpretations', 0.033), ('aez', 0.033), ('richer', 0.033), ('parts', 0.032), ('assessing', 0.032), ('apart', 0.032), ('judge', 0.031), ('arbel', 0.031), ('providing', 0.031), ('region', 0.031), ('sliding', 0.031), ('candidate', 0.03), ('human', 0.03), ('bidirectional', 0.03), ('created', 0.03), ('hypotheses', 0.029), ('outstanding', 0.029), ('qualitative', 0.028), ('feedback', 0.028), ('toward', 0.028), ('coherent', 0.027), ('penalized', 0.027), ('regions', 0.027), ('interpret', 0.027), ('ods', 0.027), ('proposes', 0.027), ('interpreted', 0.027), ('linking', 0.026), ('conversely', 0.026), ('interpretation', 0.026), ('regarding', 0.026), ('directional', 0.026), ('reviews', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="281-tfidf-1" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>Author: Jordi Pont-Tuset, Ferran Marques</p><p>Abstract: This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p><p>2 0.10091262 <a title="281-tfidf-2" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>Author: Guillem Palou, Philippe Salembier</p><p>Abstract: As early stage of video processing, we introduce an iterative trajectory merging algorithm that produces a regionbased and hierarchical representation of the video sequence, called the Trajectory Binary Partition Tree (BPT). From this representation, many analysis and graph cut techniques can be used to extract partitions or objects that are useful in the context of specific applications. In order to define trajectories and to create a precise merging algorithm, color and motion cues have to be used. Both types of informations are very useful to characterize objects but present strong differences of behavior in the spatial and the temporal dimensions. On the one hand, scenes and objects are rich in their spatial color distributions, but these distributions are rather stable over time. Object motion, on the other hand, presents simple structures and low spatial variability but may change from frame to frame. The proposed algorithm takes into account this key difference and relies on different models and associated metrics to deal with color and motion information. We show that the proposed algorithm outperforms existing hierarchical video segmentation algorithms and provides more stable and precise regions.</p><p>3 0.081667319 <a title="281-tfidf-3" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>Author: Saurabh Gupta, Pablo Arbeláez, Jitendra Malik</p><p>Abstract: We address the problems of contour detection, bottomup grouping and semantic segmentation using RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset [27]. We propose algorithms for object boundary detection and hierarchical segmentation that generalize the gPb − ucm approach of [se2]g mbeyn mtaatkioinng t effective use oef t dheep gthP information. Wroea schho owf that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We then turn to the problem of semantic segmentation and propose a simple approach that classifies superpixels into the 40 dominant object categories in NYUD2. We use both generic and class-specific features to encode the appearance and geometry of objects. We also show how our approach can be used for scene classification, and how this contextual information in turn improves object recognition. In all of these tasks, we report significant improvements over the state-of-the-art.</p><p>4 0.079992555 <a title="281-tfidf-4" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>Author: Zhile Ren, Gregory Shakhnarovich</p><p>Abstract: We propose a hierarchical segmentation algorithm that starts with a very fine oversegmentation and gradually merges regions using a cascade of boundary classifiers. This approach allows the weights of region and boundary features to adapt to the segmentation scale at which they are applied. The stages of the cascade are trained sequentially, with asymetric loss to maximize boundary recall. On six segmentation data sets, our algorithm achieves best performance under most region-quality measures, and does it with fewer segments than the prior work. Our algorithm is also highly competitive in a dense oversegmentation (superpixel) regime under boundary-based measures.</p><p>5 0.069938108 <a title="281-tfidf-5" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>Author: Ľ</p><p>Abstract: Our goal is to detect humans and estimate their 2D pose in single images. In particular, handling cases of partial visibility where some limbs may be occluded or one person is partially occluding another. Two standard, but disparate, approaches have developed in the field: the first is the part based approach for layout type problems, involving optimising an articulated pictorial structure; the second is the pixel based approach for image labelling involving optimising a random field graph defined on the image. Our novel contribution is a formulation for pose estimation which combines these two models in a principled way in one optimisation problem and thereby inherits the advantages of both of them. Inference on this joint model finds the set of instances of persons in an image, the location of their joints, and a pixel-wise body part labelling. We achieve near or state of the art results on standard human pose data sets, and demonstrate the correct estimation for cases of self-occlusion, person overlap and image truncation.</p><p>6 0.067743056 <a title="281-tfidf-6" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>7 0.067692839 <a title="281-tfidf-7" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>8 0.066154279 <a title="281-tfidf-8" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>9 0.065958656 <a title="281-tfidf-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.062855519 <a title="281-tfidf-10" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>11 0.061560731 <a title="281-tfidf-11" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>12 0.059706058 <a title="281-tfidf-12" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>13 0.059667211 <a title="281-tfidf-13" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>14 0.058899153 <a title="281-tfidf-14" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>15 0.05769987 <a title="281-tfidf-15" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>16 0.057294011 <a title="281-tfidf-16" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>17 0.055680655 <a title="281-tfidf-17" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>18 0.055559505 <a title="281-tfidf-18" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>19 0.054158527 <a title="281-tfidf-19" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>20 0.053577203 <a title="281-tfidf-20" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.142), (1, -0.007), (2, 0.03), (3, -0.005), (4, 0.063), (5, 0.003), (6, 0.019), (7, 0.014), (8, -0.059), (9, -0.009), (10, 0.038), (11, -0.011), (12, 0.003), (13, -0.017), (14, 0.016), (15, 0.006), (16, 0.012), (17, -0.036), (18, -0.011), (19, 0.03), (20, 0.004), (21, 0.079), (22, -0.016), (23, 0.021), (24, 0.04), (25, 0.007), (26, 0.015), (27, -0.007), (28, -0.007), (29, -0.017), (30, 0.03), (31, 0.079), (32, -0.054), (33, 0.033), (34, 0.043), (35, 0.005), (36, 0.007), (37, 0.029), (38, -0.049), (39, 0.037), (40, -0.041), (41, 0.068), (42, 0.018), (43, 0.016), (44, 0.008), (45, 0.039), (46, 0.0), (47, -0.029), (48, 0.106), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93167204 <a title="281-lsi-1" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>Author: Jordi Pont-Tuset, Ferran Marques</p><p>Abstract: This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p><p>2 0.81817919 <a title="281-lsi-2" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>3 0.8039999 <a title="281-lsi-3" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>Author: Camillo Jose Taylor</p><p>Abstract: In this paper we explore approaches to accelerating segmentation and edge detection algorithms based on the gPb framework. The paper characterizes the performance of a simple but effective edge detection scheme which can be computed rapidly and offers performance that is competitive with the pB detector. The paper also describes an approach for computing a reduced order normalized cut that captures the essential features of the original problem but can be computed in less than half a second on a standard computing platform.</p><p>4 0.75866848 <a title="281-lsi-4" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>Author: Yansheng Ming, Hongdong Li, Xuming He</p><p>Abstract: This paper aims to extract salient closed contours from an image. For this vision task, both region segmentation cues (e.g. color/texture homogeneity) and boundary detection cues (e.g. local contrast, edge continuity and contour closure) play important and complementary roles. In this paper we show how to combine both cues in a unified framework. The main focus is given to how to maintain the consistency (compatibility) between the region cues and the boundary cues. To this ends, we introduce the use of winding number–a well-known concept in topology–as a powerful mathematical device. By this device, the region-boundary consistency is represented as a set of simple linear relationships. Our method is applied to the figure-ground segmentation problem. The experiments show clearly improved results.</p><p>5 0.74527943 <a title="281-lsi-5" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>Author: Zhile Ren, Gregory Shakhnarovich</p><p>Abstract: We propose a hierarchical segmentation algorithm that starts with a very fine oversegmentation and gradually merges regions using a cascade of boundary classifiers. This approach allows the weights of region and boundary features to adapt to the segmentation scale at which they are applied. The stages of the cascade are trained sequentially, with asymetric loss to maximize boundary recall. On six segmentation data sets, our algorithm achieves best performance under most region-quality measures, and does it with fewer segments than the prior work. Our algorithm is also highly competitive in a dense oversegmentation (superpixel) regime under boundary-based measures.</p><p>6 0.73790175 <a title="281-lsi-6" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>7 0.68462312 <a title="281-lsi-7" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>8 0.66384888 <a title="281-lsi-8" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>9 0.6521011 <a title="281-lsi-9" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>10 0.64860368 <a title="281-lsi-10" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>11 0.63557988 <a title="281-lsi-11" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>12 0.62712842 <a title="281-lsi-12" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>13 0.62701958 <a title="281-lsi-13" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>14 0.61455065 <a title="281-lsi-14" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>15 0.59862864 <a title="281-lsi-15" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>16 0.59429061 <a title="281-lsi-16" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>17 0.57591122 <a title="281-lsi-17" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>18 0.57048261 <a title="281-lsi-18" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>19 0.56061012 <a title="281-lsi-19" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>20 0.55997622 <a title="281-lsi-20" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.057), (16, 0.012), (26, 0.52), (33, 0.22), (67, 0.039), (69, 0.029), (87, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79683077 <a title="281-lda-1" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>Author: Adrien Bartoli, Toby Collins</p><p>Abstract: It has been shown that a surface deforming isometrically can be reconstructed from a single image and a template 3D shape. Methods from the literature solve this problem efficiently. However, they all assume that the camera model is calibrated, which drastically limits their applicability. We propose (i) a general variational framework that applies to (calibrated and uncalibrated) general camera models and (ii) self-calibrating 3D reconstruction algorithms for the weak-perspective and full-perspective camera models. In the former case, our algorithm returns the normal field and camera ’s scale factor. In the latter case, our algorithm returns the normal field, depth and camera ’s focal length. Our algorithms are the first to achieve deformable 3D reconstruction including camera self-calibration. They apply to much more general setups than existing methods. Experimental results on simulated and real data show that our algorithms give results with the same level of accuracy as existing methods (which use the true focal length) on perspective images, and correctly find the normal field on affine images for which the existing methods fail.</p><p>2 0.78220844 <a title="281-lda-2" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>Author: Liang Li, Wei Feng, Liang Wan, Jiawan Zhang</p><p>Abstract: This paper addresses a challenging problem of regularizing arbitrary superpixels into an optimal grid structure, which may significantly extend current low-level vision algorithms by allowing them to use superpixels (SPs) conveniently as using pixels. For this purpose, we aim at constructing maximum cohesive SP-grid, which is composed of real nodes, i.e. SPs, and dummy nodes that are meaningless in the image with only position-taking function in the grid. For a given formation of image SPs and proper number of dummy nodes, we first dynamically align them into a grid based on the centroid localities of SPs. We then define the SP-grid coherence as the sum of edge weights, with SP locality and appearance encoded, along all direct paths connecting any two nearest neighboring real nodes in the grid. We finally maximize the SP-grid coherence via cascade dynamic programming. Our approach can take the regional objectness as an optional constraint to produce more semantically reliable SP-grids. Experiments on object localization show that our approach outperforms state-of-the-art methods in terms of both detection accuracy and speed. We also find that with the same searching strategy and features, object localization at SP-level is about 100-500 times faster than pixel-level, with usually better detection accuracy.</p><p>same-paper 3 0.77532649 <a title="281-lda-3" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>Author: Jordi Pont-Tuset, Ferran Marques</p><p>Abstract: This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p><p>4 0.73569947 <a title="281-lda-4" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>5 0.71491152 <a title="281-lda-5" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>6 0.67458832 <a title="281-lda-6" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>7 0.63383847 <a title="281-lda-7" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>8 0.63085699 <a title="281-lda-8" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>9 0.58606535 <a title="281-lda-9" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>10 0.56950009 <a title="281-lda-10" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>11 0.5446915 <a title="281-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.54151052 <a title="281-lda-12" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>13 0.53683138 <a title="281-lda-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.53591496 <a title="281-lda-14" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>15 0.53511494 <a title="281-lda-15" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>16 0.5346328 <a title="281-lda-16" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>17 0.53410441 <a title="281-lda-17" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>18 0.53392112 <a title="281-lda-18" href="./cvpr-2013-Hyperbolic_Harmonic_Mapping_for_Constrained_Brain_Surface_Registration.html">208 cvpr-2013-Hyperbolic Harmonic Mapping for Constrained Brain Surface Registration</a></p>
<p>19 0.53230399 <a title="281-lda-19" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>20 0.53227681 <a title="281-lda-20" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
