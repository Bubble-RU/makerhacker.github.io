<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-243" href="#">cvpr2013-243</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</h1>
<br/><p>Source: <a title="cvpr-2013-243-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Khosla_Large-Scale_Video_Summarization_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>Reference: <a title="cvpr-2013-243-reference" href="../cvpr2013_reference/cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. [sent-8, score-1.042]
</p><p>2 To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. [sent-9, score-0.571]
</p><p>3 Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. [sent-10, score-0.302]
</p><p>4 Such images could therefore be used as prior information to summarize videos containing a similar set of objects. [sent-11, score-0.344]
</p><p>5 In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. [sent-12, score-0.546]
</p><p>6 Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. [sent-13, score-0.965]
</p><p>7 These videos are extremely diverse in their content, and can vary in length from a few minutes to a few hours. [sent-18, score-0.265]
</p><p>8 It is therefore becoming increasingly important to automatically extract a brief yet informative summary of these videos in order to enable a more efficient and engaging viewing experience. [sent-19, score-0.444]
</p><p>9 In this work, we focus on the problem of automatic summarization and evaluation of user-generated videos. [sent-20, score-0.654]
</p><p>10 Summarizing user-generated videos is different from well-produced videos in two important ways. [sent-21, score-0.53]
</p><p>11 Unlike the individual images, that are taken from different canonical viewpoints to capture the car in a maximally informative way, very few of the sampled video-frames are as informative. [sent-28, score-0.406]
</p><p>12 Secondly, most of the user-generated videos contain only a small fraction of frames where some interesting event is happening. [sent-29, score-0.455]
</p><p>13 The main contribution of this work is the idea of using web-images as a prior to facilitate the process of creating summaries of user-generated videos. [sent-31, score-0.512]
</p><p>14 Our intuition is that people tend to take pictures of objects and events from a few canonical viewpoints in order to capture them in a maximally informative way. [sent-32, score-0.57]
</p><p>15 On the other hand, as shown in Figure 1, user-generated videos taken by hand-held cameras often contain many uninformative frames captured while transitioning between the various canonical viewpoints. [sent-33, score-0.603]
</p><p>16 We therefore hypothesize that images of objects and events present on the web contain information that could be used as a prior for building semantically meaningful 222666999866 Assigned Viewpoint Labels  each discovered subclass corresponds to a “canonical viewpoint”. [sent-34, score-0.287]
</p><p>17 To improve our subclass models, we also use the unlabeled video data by first assigning each video frame to a subclass, and then repeating the optimization procedure from Section 2. [sent-37, score-0.55]
</p><p>18 Finally to generate the output summary with k representative frames, we select the k frames from the test video, each of which is closest to the centroid the top k ranked subclasses. [sent-41, score-0.389]
</p><p>19 summaries of user-generated videos in a scene-independent manner. [sent-42, score-0.718]
</p><p>20 In this work, we apply our novel intuition to propose a summarization algorithm that incorporates this image-based prior to automatically select the maximally informative frames from a video. [sent-43, score-0.959]
</p><p>21 An important related question we explore in this work is the evaluation ofvideo summarization algorithms in a largescale setting. [sent-44, score-0.568]
</p><p>22 A majority of the previous work on video summarization uses expert opinion to evaluate their results. [sent-45, score-0.824]
</p><p>23 Moreover, videos considered by these methods are generally well-produced with their content following a strict cinematographic structure. [sent-47, score-0.299]
</p><p>24 Since user-generated videos are produced at scale, and do not follow strict structure, using expert opinion for their evaluation is infeasible. [sent-49, score-0.45]
</p><p>25 To this end, we propose to rely on crowd-sourcing to obtain multiple candidate summaries ofuser-generated videos, to get a general sense of what their summaries should look like. [sent-50, score-0.906]
</p><p>26 We cast the question of matching the output of a summarization algorithm and the crowd-sourced summarization as a graph-theoretic problem. [sent-51, score-1.024]
</p><p>27 The main contributions of our work are: • A novel intuition to incorporate information from web images as a prior t too automatically fseorlemcta maximally ienbformative frames from user-generated videos without using any human annotated summaries for training. [sent-53, score-1.126]
</p><p>28 • A crowd-sourcing based automatic evaluation framewAo crkro wtod e-vsoaluuractien tghe b arseesdult asu otofm multiple lvuiadteioo summarization algorithms. [sent-54, score-0.654]
</p><p>29 • An analysis of our summarization mechanism tested over a large s oetf o ofu user-generated nvid meoesc. [sent-55, score-0.512]
</p><p>30 We then explain how we automatically evaluate different summarization results using a crowd-sourcing platform in Section 3, and present a comparative analysis of our experiments and results in Section 4. [sent-57, score-0.512]
</p><p>31 Using these discovered viewpoints, we want to learn a discriminative model to identify similar frames in a video capturing a different instance of the object class. [sent-63, score-0.484]
</p><p>32 Since many user-generated videos are captured from hand-held mobile devices, they contain a lot of variation in the viewpoints from which they capture an object. [sent-67, score-0.416]
</p><p>33 Therefore, frames from these videos can be used as difficult-to-classify negative examples to further improve models of the canonical viewpoints. [sent-69, score-0.566]
</p><p>34 To this end, we use the pre-trained viewpoint classifiers learned only from webimages to initialize a second round of training, where the labels for both web-images and video frames are consid222666999977  Algorithm 1 Video summarization algorithm (explained in  Section 2). [sent-70, score-0.969]
</p><p>35 wi(j) Input: Unlabeled images xDand videos xV, number of  subclasses K. [sent-72, score-0.399]
</p><p>36 Identifying Canonical Viewpoints In order to discover the canonical viewpoints, we want to identify visually similar images in a corpus of web images. [sent-88, score-0.272]
</p><p>37 Furthermore, we want to reliably identify these viewpoints in a previously unseen set of video frames. [sent-89, score-0.389]
</p><p>38 Using Unlabeled Videos for Training In this section, we assume that we have a classifier for one canonical viewpoint and we want to identify additional examples from the videos from the same viewpoint. [sent-113, score-0.504]
</p><p>39 We break the videos into frames and treat all the frames as independent examples. [sent-114, score-0.645]
</p><p>40 We then assign each frame from all videos to a subclass using the equation ˆyVi = argmaxy(wy · xiV) where the weights learned from the images are used ·f oxr the video frame subclass assignment. [sent-137, score-0.816]
</p><p>41 1 with both video frames and images for a few iterations. [sent-139, score-0.373]
</p><p>42 Given a test video, we assign its frames to the different subclasses using their learned classifiers, and compute the average decision score of the positive examples from each subclass. [sent-140, score-0.349]
</p><p>43 To generate the output summary with k representative frames, we select the k frames from the test video, each of which is closest to the centroid of any one of the top k ranked subclasses. [sent-145, score-0.389]
</p><p>44 Overall, the process consists of obtaining multiple summaries of a single video via AMT, and later comparing those summaries against the ones obtained by applying different algorithms. [sent-149, score-1.089]
</p><p>45 Obtaining Annotation using Mechanical Turk Summarizing a video is a subjective task, and summaries produced by different people are often different, even when done by experts. [sent-153, score-0.67]
</p><p>46 Thus, it is beneficial to obtain multiple summaries of a single video as ground truth to evaluate the performance of different algorithms. [sent-154, score-0.636]
</p><p>47 A turker must select at least 3 and at most 25 frames that he believes adequately summarize the content of the frames shown. [sent-159, score-0.537]
</p><p>48 20 per summary per video, and we obtain a total of 10 summaries per video, for a total of 155 videos. [sent-164, score-0.552]
</p><p>49 Evaluation using Average Precision Since the number of frames to use for a summary is application dependent, we propose to evaluate a variable number of frames from a ranked list (similar to [14]). [sent-167, score-0.518]
</p><p>50 Thus, we can iteratively evaluate the precision and recall of using 1frame ({S1}), 2 frames ({S1, S2}) and so on to plot a precision e r (e{cSall curve. [sent-180, score-0.286]
</p><p>51 Thus, to compute precision, we want to find how well all the retrieved frames match with the reference frames, while to compute recall we want to find how many, and how accurately, are the reference frames returned in the retrieval result. [sent-186, score-0.626]
</p><p>52 W ∈e R Rwant to find a matching between the two sets of frames such that there is one reference frame corresponding to each retrieved frame. [sent-200, score-0.329]
</p><p>53 Dataset For this work, we focus on the “Cars and Trucks” class of objects, since it is one of the most popular categories where users upload both images and videos to ecommerce websites. [sent-215, score-0.341]
</p><p>54 In order to collect our image corpus, we crawled several popular ecommerce websites and downloaded about 300, 000 images of cars and trucks that users had uploaded to their listings. [sent-216, score-0.266]
</p><p>55 To collect video data, we searched for all  the user listings with a “youtube. [sent-217, score-0.28]
</p><p>56 For each of these 180 listings, we downloaded their corresponding videos from youtube. [sent-219, score-0.302]
</p><p>57 We ensured that images from listings containing test videos were not included in our training data. [sent-222, score-0.336]
</p><p>58 The images and video frames were resized to have a maximum dimension of 500 pixels (preserving aspect ratio). [sent-229, score-0.373]
</p><p>59 Random sampling is the simplest baseline where we randomly select n frames from the video. [sent-237, score-0.278]
</p><p>60 In uniform sampling we split the video into n + 1equal segments where the last frame from the first n segments is selected. [sent-238, score-0.304]
</p><p>61 2 and our method, we also compute the average precision (AP) when reference summaries from the AMT workers are used for evaluation. [sent-245, score-0.615]
</p><p>62 Figure 5 shows the summaries produced by different methods for three example videos to give the reader a visual sense of the summaries produced by different algorithms. [sent-251, score-1.171]
</p><p>63 Note that our algorithm produces summaries most similar to the ones generated by human annotators. [sent-252, score-0.481]
</p><p>64 Human Evaluation With 15 human judges, we performed human evaluation of the retrieved summaries from different algorithms to verify the results obtained from the automatic evaluation. [sent-257, score-0.69]
</p><p>65 Each expert was shown a set of 25 randomly sampled videos 222777000200  A MP eti/hAodPAMTU04n5. [sent-258, score-0.359]
</p><p>66 e,uniform,k-means,oursandAMT),showingdifer nt umberof rames for 3 different videos to give a sense of the visual quality of summaries  obtained using the different  summarization  algorithms. [sent-278, score-1.23]
</p><p>67 They watched the video at 3x speed and were then shown 4 sets of summaries constructed using different methods: uniform sampling, kmeans clustering, our proposed algorithm (Section 2), and a reference summary from AMT. [sent-281, score-0.802]
</p><p>68 70 between the scores assigned to each video by human evaluators and our automatic method. [sent-287, score-0.297]
</p><p>69 Finally, the high performance of AMT summaries in both human and automatic evaluation illustrates that our method to obtain summaries using crowdsourcing is effective, allowing us to evaluate video summarization results in a large-scale setting, while keeping costs low. [sent-289, score-1.806]
</p><p>70 51 05 10 150  Sorted video index Figure 6: Improvement of our algorithm over baseline (k-means)  for individual videos sorted by the amount of improvement. [sent-292, score-0.475]
</p><p>71 Our main intuition is that people tend to take pictures of objects from select viewpoints in order to capture them in a maximally informative way. [sent-297, score-0.456]
</p><p>72 We therefore hypothesized that images of objects could be used to create summaries of user-generated videos 222777000311  AMvegt. [sent-298, score-0.718]
</p><p>73 58T Table 2: Human Evaluation– 15 human judges evaluated the summaries from different algorithms on a scale of 1to 10 to verify the results of our automatic evaluation scheme. [sent-303, score-0.701]
</p><p>74 As shown in Table 1, the results of our experiments confirm our hypothesis, where the average precision we obtain while using web-image prior to summarize videos is significantly better (54. [sent-308, score-0.392]
</p><p>75 We also posit that since user-generated videos have a lot of variation in the viewpoints from which they capture an object, frames from these videos could be used in addition to the image based prior information to further improve the summarization performance. [sent-311, score-1.417]
</p><p>76 This hypothesis is confirmed by the results in Table 1, where using video frames and images together performs better (59. [sent-312, score-0.373]
</p><p>77 This is because combining images and video frames results in viewpoint clusters that are largely coherent (see Figure 7 for example clusters). [sent-315, score-0.461]
</p><p>78 This indicates that using image based priors for summarizing user-generated videos captures what humans consider good summaries. [sent-318, score-0.343]
</p><p>79 Furthermore, based on the feedback from the judges we learned that users generally position their cameras at the start and end of recording the videos such that the first and last frames of the videos are usually more informative than a randomly selected frame. [sent-319, score-0.907]
</p><p>80 Adding this information in our summarization algorithm is likely to improve our overall performance. [sent-320, score-0.512]
</p><p>81 generated cooking videos from Youtube and downloaded close to 10, 000 images from Flickr for similar queries, all of which related to the activity of making a salad. [sent-325, score-0.302]
</p><p>82 For these reasons, we found the returned summaries of our algorithm and uniform sampling to be largely similar. [sent-330, score-0.543]
</p><p>83 Furthermore, there are challenges of domain adaptation [33] when training on images in one setting, and testing the learned models on videos in a different setting. [sent-331, score-0.291]
</p><p>84 Related Work Video summarization has been looked at from multiple perspectives [28]. [sent-334, score-0.512]
</p><p>85 While the representation used for the summary might be key-frames [34] [12], image montages [3], or short glimpses [26] [25], the goal of video summarization is nevertheless to produce a compact visual summary that encapsulates the most informative parts of a video. [sent-335, score-0.973]
</p><p>86 Most of the previous summarization techniques are designed for well-produced videos, and rely on low-level appearance and motion cues [22] [15]. [sent-336, score-0.512]
</p><p>87 Our current work is another step in this general direction of content-aware summarization, where unlike previous approaches, we use web-images as a prior to facilitate summarization of user-generated videos. [sent-339, score-0.571]
</p><p>88 222777000422 The lack of an agreed upon notion of the “optimal” summary of a video can make summary evaluation a key challenge for video summarization. [sent-340, score-0.646]
</p><p>89 Similar challenges exist in other domains, such as machine translation [24] and text summarization [19], where previous methods have tried to combine several human-generated candidate summaries to infer a final answer which in expectation is better than any of the individual candidate results. [sent-341, score-0.991]
</p><p>90 Following this approach, there has been previous work in the field of video summarization that also attempts to aggregate multiple summaries of a video to infer a final answer [18] [29] [7]. [sent-342, score-1.331]
</p><p>91 More recently, there has been an interest in the problem of evaluating video summarization results at a large scale [2] [23]. [sent-344, score-0.695]
</p><p>92 However, these approaches use multiple expert summaries which is an expensive and timeconsuming exercise. [sent-345, score-0.547]
</p><p>93 In this work however, we show how to use a crowd-sourcing model to get multiple summarization labels specifically for user-generated videos. [sent-347, score-0.551]
</p><p>94 We demonstrated that web images could be used as a prior to summarize videos that capture objects similar to those present in the image corpus. [sent-350, score-0.416]
</p><p>95 We also focused on the related problem of large-scale automatic evaluation of summarization algorithms. [sent-351, score-0.654]
</p><p>96 We proposed an evaluation framework that uses multiple summaries obtained by crowd-sourcing, and compared the performance of our framework to that of multiple expert users. [sent-352, score-0.603]
</p><p>97 Our main intuition regarding people taking pictures of objects to capture them in an informative way is applicable to videos of events and activities as well. [sent-353, score-0.537]
</p><p>98 Online, simultaneous shot boundary detection and key frame extraction for sports videos using rank tracing. [sent-358, score-0.358]
</p><p>99 Vert: automatic evaluation of video sum-  [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32]  [33] [34] [35]  maries. [sent-462, score-0.325]
</p><p>100 Event driven web video summarization by tag localization and key-shot identification. [sent-554, score-0.739]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('summarization', 0.512), ('summaries', 0.453), ('videos', 0.265), ('frames', 0.19), ('video', 0.183), ('amt', 0.141), ('subclasses', 0.134), ('viewpoints', 0.123), ('subclass', 0.122), ('canonical', 0.111), ('trucks', 0.105), ('summary', 0.099), ('expert', 0.094), ('siftflow', 0.091), ('automatic', 0.086), ('xiv', 0.084), ('informative', 0.08), ('summarizing', 0.078), ('judges', 0.078), ('workers', 0.076), ('listings', 0.071), ('wy', 0.07), ('bipartite', 0.067), ('maximally', 0.064), ('frame', 0.062), ('argmaxy', 0.058), ('evaluation', 0.056), ('instructions', 0.055), ('iew', 0.055), ('ldo', 0.05), ('want', 0.05), ('pictures', 0.048), ('precision', 0.048), ('intuition', 0.048), ('cars', 0.048), ('ecommerce', 0.047), ('examplesoflearnedcano', 0.047), ('partite', 0.047), ('rushes', 0.047), ('turker', 0.047), ('xid', 0.047), ('summarize', 0.045), ('viewpoint', 0.045), ('web', 0.044), ('clusters', 0.043), ('ebay', 0.042), ('yvi', 0.042), ('ap', 0.042), ('labels', 0.039), ('retrieved', 0.039), ('tomccap', 0.039), ('ranked', 0.039), ('reference', 0.038), ('downloaded', 0.037), ('taiwan', 0.037), ('worker', 0.037), ('uninformative', 0.037), ('annotators', 0.035), ('crowdsourcing', 0.035), ('food', 0.035), ('opinion', 0.035), ('corpus', 0.034), ('prior', 0.034), ('events', 0.034), ('content', 0.034), ('people', 0.034), ('fie', 0.033), ('identify', 0.033), ('annotation', 0.032), ('rank', 0.031), ('select', 0.031), ('returned', 0.031), ('sampling', 0.03), ('mechanical', 0.03), ('centroid', 0.03), ('trecvid', 0.03), ('xv', 0.03), ('pritch', 0.03), ('uniform', 0.029), ('optima', 0.029), ('users', 0.029), ('human', 0.028), ('capture', 0.028), ('descending', 0.028), ('acl', 0.028), ('discovered', 0.028), ('baseline', 0.027), ('keyframes', 0.026), ('slack', 0.026), ('challenges', 0.026), ('clustering', 0.026), ('scoring', 0.026), ('user', 0.026), ('notion', 0.026), ('growth', 0.025), ('spectral', 0.025), ('cluster', 0.025), ('semantically', 0.025), ('facilitate', 0.025), ('decision', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="243-tfidf-1" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>2 0.21673495 <a title="243-tfidf-2" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>Author: Yale Song, Louis-Philippe Morency, Randall Davis</p><p>Abstract: Recent progress has shown that learning from hierarchical feature representations leads to improvements in various computer vision tasks. Motivated by the observation that human activity data contains information at various temporal resolutions, we present a hierarchical sequence summarization approach for action recognition that learns multiple layers of discriminative feature representations at different temporal granularities. We build up a hierarchy dynamically and recursively by alternating sequence learning and sequence summarization. For sequence learning we use CRFs with latent variables to learn hidden spatiotemporal dynamics; for sequence summarization we group observations that have similar semantic meaning in the latent space. For each layer we learn an abstract feature representation through non-linear gate functions. This procedure is repeated to obtain a hierarchical sequence summary representation. We develop an efficient learning method to train our model and show that its complexity grows sublinearly with the size of the hierarchy. Experimental results show the effectiveness of our approach, achieving the best published results on the ArmGesture and Canal9 datasets.</p><p>3 0.21607889 <a title="243-tfidf-3" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>4 0.15901574 <a title="243-tfidf-4" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied, we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers, we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components, across other videos in the system, to perform subsequent video analysis. We demonstrate the efficacy of our approach for activity classification and clustering by reporting competitive results on standard datasets such as, HMDB, UCF-50, Olympic Sports and KTH.</p><p>5 0.14684731 <a title="243-tfidf-5" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>6 0.14458032 <a title="243-tfidf-6" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>7 0.14121738 <a title="243-tfidf-7" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>8 0.13547297 <a title="243-tfidf-8" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>9 0.13234012 <a title="243-tfidf-9" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>10 0.11758592 <a title="243-tfidf-10" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>11 0.11061694 <a title="243-tfidf-11" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>12 0.10906269 <a title="243-tfidf-12" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>13 0.10887972 <a title="243-tfidf-13" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>14 0.10667242 <a title="243-tfidf-14" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>15 0.10512511 <a title="243-tfidf-15" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>16 0.098887488 <a title="243-tfidf-16" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>17 0.094873384 <a title="243-tfidf-17" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>18 0.088482149 <a title="243-tfidf-18" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>19 0.087786563 <a title="243-tfidf-19" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>20 0.082647413 <a title="243-tfidf-20" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.064), (2, -0.009), (3, -0.085), (4, -0.066), (5, 0.005), (6, -0.053), (7, -0.051), (8, -0.03), (9, 0.044), (10, 0.062), (11, -0.08), (12, 0.062), (13, -0.026), (14, -0.021), (15, -0.049), (16, 0.056), (17, 0.005), (18, -0.031), (19, -0.143), (20, -0.062), (21, -0.013), (22, 0.025), (23, -0.123), (24, -0.053), (25, -0.041), (26, 0.019), (27, -0.001), (28, 0.029), (29, 0.007), (30, 0.032), (31, 0.004), (32, -0.017), (33, 0.017), (34, 0.044), (35, 0.18), (36, -0.039), (37, 0.018), (38, -0.011), (39, -0.059), (40, 0.013), (41, 0.026), (42, -0.082), (43, -0.084), (44, -0.107), (45, 0.131), (46, -0.128), (47, 0.056), (48, -0.021), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96166533 <a title="243-lsi-1" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>2 0.86808115 <a title="243-lsi-2" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>3 0.80339038 <a title="243-lsi-3" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>Author: Wei-Chen Chiu, Mario Fritz</p><p>Abstract: Video data provides a rich source of information that is available to us today in large quantities e.g. from online resources. Tasks like segmentation benefit greatly from the analysis of spatio-temporal motion patterns in videos and recent advances in video segmentation has shown great progress in exploiting these addition cues. However, observing a single video is often not enough to predict meaningful segmentations and inference across videos becomes necessary in order to predict segmentations that are consistent with objects classes. Therefore the task of video cosegmentation is being proposed, that aims at inferring segmentation from multiple videos. But current approaches are limited to only considering binary foreground/background -inf .mpg . de segmentation and multiple videos of the same object. This is a clear mismatch to the challenges that we are facing with videos from online resources or consumer videos. We propose to study multi-class video co-segmentation where the number of object classes is unknown as well as the number of instances in each frame and video. We achieve this by formulating a non-parametric bayesian model across videos sequences that is based on a new videos segmentation prior as well as a global appearance model that links segments of the same class. We present the first multi-class video co-segmentation evaluation. We show that our method is applicable to real video data from online resources and outperforms state-of-the-art video segmentation and image co-segmentation baselines.</p><p>4 0.78510386 <a title="243-lsi-4" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>5 0.78348917 <a title="243-lsi-5" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>6 0.76128578 <a title="243-lsi-6" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>7 0.71655989 <a title="243-lsi-7" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>8 0.70572221 <a title="243-lsi-8" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>9 0.67555523 <a title="243-lsi-9" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>10 0.66697085 <a title="243-lsi-10" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>11 0.58381659 <a title="243-lsi-11" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>12 0.58130783 <a title="243-lsi-12" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>13 0.58029187 <a title="243-lsi-13" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>14 0.55583775 <a title="243-lsi-14" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>15 0.5534566 <a title="243-lsi-15" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>16 0.53937495 <a title="243-lsi-16" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>17 0.53762013 <a title="243-lsi-17" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>18 0.52428937 <a title="243-lsi-18" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>19 0.5053246 <a title="243-lsi-19" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>20 0.4994089 <a title="243-lsi-20" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.132), (16, 0.02), (26, 0.056), (28, 0.02), (33, 0.255), (50, 0.247), (67, 0.068), (69, 0.027), (77, 0.018), (87, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83436537 <a title="243-lda-1" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>same-paper 2 0.82354569 <a title="243-lda-2" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>3 0.82278764 <a title="243-lda-3" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>Author: Jian Dong, Wei Xia, Qiang Chen, Jianshi Feng, Zhongyang Huang, Shuicheng Yan</p><p>Abstract: In this paper, we introduce a subcategory-aware object classification framework to boost category level object classification performance. Motivated by the observation of considerable intra-class diversities and inter-class ambiguities in many current object classification datasets, we explicitly split data into subcategories by ambiguity guided subcategory mining. We then train an individual model for each subcategory rather than attempt to represent an object category with a monolithic model. More specifically, we build the instance affinity graph by combining both intraclass similarity and inter-class ambiguity. Visual subcategories, which correspond to the dense subgraphs, are detected by the graph shift algorithm and seamlessly integrated into the state-of-the-art detection assisted classification framework. Finally the responses from subcategory models are aggregated by subcategory-aware kernel regression. The extensive experiments over the PASCAL VOC 2007 and PASCAL VOC 2010 databases show the state-ofthe-art performance from our framework.</p><p>4 0.81992847 <a title="243-lda-4" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>Author: Michalis Raptis, Leonid Sigal</p><p>Abstract: In this paper, we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework, where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and minefor hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive with the state of the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.</p><p>5 0.81113499 <a title="243-lda-5" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>Author: Yen H. Le, Uday Kurkure, Ioannis A. Kakadiaris</p><p>Abstract: Statistical shape models, such as Active Shape Models (ASMs), sufferfrom their inability to represent a large range of variations of a complex shape and to account for the large errors in detection of model points. We propose a novel method (dubbed PDM-ENLOR) that overcomes these limitations by locating each shape model point individually using an ensemble of local regression models and appearance cues from selected model points. Our method first detects a set of reference points which were selected based on their saliency during training. For each model point, an ensemble of regressors is built. From the locations of the detected reference points, each regressor infers a candidate location for that model point using local geometric constraints, encoded by a point distribution model (PDM). The final location of that point is determined as a weighted linear combination, whose coefficients are learnt from the training data, of candidates proposed from its ensemble ’s component regressors. We use different subsets of reference points as explanatory variables for the component regressors to provide varying degrees of locality for the models in each ensemble. This helps our ensemble model to capture a larger range of shape variations as compared to a single PDM. We demonstrate the advantages of our method on the challenging problem of segmenting gene expression images of mouse brain.</p><p>6 0.77859938 <a title="243-lda-6" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>7 0.77343518 <a title="243-lda-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.77266794 <a title="243-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.77205992 <a title="243-lda-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.77095711 <a title="243-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.77070296 <a title="243-lda-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.77017522 <a title="243-lda-12" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>13 0.76975596 <a title="243-lda-13" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>14 0.76906645 <a title="243-lda-14" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>15 0.76690578 <a title="243-lda-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.76608586 <a title="243-lda-16" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>17 0.76571381 <a title="243-lda-17" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>18 0.76525271 <a title="243-lda-18" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>19 0.76516593 <a title="243-lda-19" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>20 0.76493299 <a title="243-lda-20" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
