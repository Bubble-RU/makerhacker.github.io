<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-117" href="#">cvpr2013-117</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</h1>
<br/><p>Source: <a title="cvpr-2013-117-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sakurada_Detecting_Changes_in_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>Reference: <a title="cvpr-2013-117-reference" href="../cvpr2013_reference/cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. [sent-6, score-0.895]
</p><p>2 For the images, we consider those captured by a camera mounted on a vehicle running in a city street. [sent-7, score-0.55]
</p><p>3 The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. [sent-8, score-0.926]
</p><p>4 The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. [sent-9, score-0.674]
</p><p>5 The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. [sent-10, score-0.657]
</p><p>6 Introduction This paper considers a problem of detecting temporal changes in the three-dimensional structure of a scene, such as an urban area, from a pair of its multi-view images captured at two separate times. [sent-13, score-0.714]
</p><p>7 For the images, we consider those captured by a camera mounted on a ground vehicle while running it on city streets. [sent-14, score-0.595]
</p><p>8 The underlying motivation is to develop a method for automatically detecting the temporal changes of a whole city when it changes its structure in a relatively short time period because of disasters such as earthquakes and tsunamis. [sent-15, score-0.906]
</p><p>9 A pair of two images of the same scene taken at two separate times. [sent-20, score-0.416]
</p><p>10 We have been periodically (every three to four months) capturing their images using a vehicle having an omni-directional camera on its roof. [sent-23, score-0.34]
</p><p>11 Figure 1 shows examples of these images, which are a pair of two images of the same scene captured three months apart. [sent-25, score-0.476]
</p><p>12 To achieve the goal of detecting temporal 3D scene changes from these images, a naive approach would be to use Multi-View Stereo (MVS) [5, 16] to reconstruct the 3D shapes of the area at different time points from their images and differentiate them to detect changes in 3D structure. [sent-26, score-1.295]
</p><p>13 However, apart from the recon-  struction from aerial imagery, which has achieved great success lately, it is still a difficult task to accurately reconstruct the structure of a scene from its images taken by a ground vehicle-mounted camera. [sent-28, score-0.655]
</p><p>14 ) These may be attributable to several reasons, such as the large depth variations which are contrasted with aerial imagery, the limited variety and number of camera poses (i. [sent-34, score-0.686]
</p><p>15 , the viewpoints  are on a straight line along the vehicle path), and the insufficient scene textures. [sent-36, score-0.466]
</p><p>16 The basic idea is that we want to know not the scene structure of each time point but their changes; thus, we formulate the problem so as to estimate them directly from the images. [sent-39, score-0.338]
</p><p>17 The core of the formulation, which distinguishes it from the above MVS-based one, is a probabilistic treatment of scene structures. [sent-40, score-0.404]
</p><p>18 The camera poses are necessary in this estimation and are estimated in advance by performing SfM for the images of each time point followed by registration of the reconstructions. [sent-43, score-0.308]
</p><p>19 Our aim behind this probabilistic treatment of scene structures is to maximize the accuracy of detecting scene changes. [sent-44, score-0.85]
</p><p>20 If scene structure has to be deterministically determined even though observations give only ambiguous information, the two reconstructions will inevitably have errors, so do the estimated scene changes obtained by differentiat-  ing them. [sent-45, score-1.23]
</p><p>21 Our approach could reduce such errors by appropriately considering the ambiguity of scene structure. [sent-46, score-0.309]
</p><p>22 Related work Many researches have been conducted to develop methods for detecting temporal changes of a scene. [sent-55, score-0.435]
</p><p>23 However, most of them consider the detection of 2D changes (i. [sent-56, score-0.303]
</p><p>24 , those only in image appearance), whereas we want to detect changes in 3D structure of scenes. [sent-58, score-0.376]
</p><p>25 The standard problem formulation of 2D change detection [12, 14] is such that an appearance model of a scene is learned using its n images and then based on n + 1st image, it is determined whether a significant change has occurred. [sent-60, score-0.625]
</p><p>26 Most of the studies of 3D change detection [3, 8, 7, 12, 18] follow a similar formulation; namely, a model of the scene in a “steady state” is built and a newly-captured image(s) is compared against it to detect changes. [sent-61, score-0.518]
</p><p>27 In [12], targeting at aerial images capturing a ground scene, a method is proposed that learns a voxel-based appearance model of a 3D scene from its 20–40 images. [sent-62, score-0.496]
</p><p>28 In [8], a method is proposed that detects scene changes by estimating the appearance or disappearance of line segments in space. [sent-64, score-0.664]
</p><p>29 All of these studies create an appearance model of the target scene from a sufficiently large number of images. [sent-65, score-0.313]
</p><p>30 Several studies assume that a 3D model of the scene is given by using other sensors or methods than the images used for the change detection. [sent-67, score-0.464]
</p><p>31 In [7], assuming that the 3D model of a building is given, the edges extracted in its aerial images are matched with the projection of the 3D model to detect changes. [sent-68, score-0.33]
</p><p>32 Their method detects temporal changes of a scene from its multi-view images, and thus it is close to ours from an application point of view. [sent-71, score-0.663]
</p><p>33 However, their motivation is to minimize the cost needed for updating the 3D model of a large urban area, and thus, a dense 3D model of the target scene is assumed to be given. [sent-72, score-0.4]
</p><p>34 In our formulation, the changes of a scene are detected from two sets of images taken at two different time points. [sent-74, score-0.652]
</p><p>35 do not assume a dense 3D model of the scene to be given, or do not create one from the input images themselves, as it is difficult for the images captured from a ground vehiclemounted camera; see Fig. [sent-106, score-0.562]
</p><p>36 They propose a method that uses a large number of images of a city that are taken over several decades to perform several types of temporal inferences, such as estimating when each building in the city was constructed. [sent-110, score-0.57]
</p><p>37 However, besides the necessity for a large number of images, their method represents scene changes only in the form of point clouds associated with image features. [sent-111, score-0.575]
</p><p>38 The images are captured by a vehicle having an omni-directional camera (Ladybug3 of Point Grey Research Inc. [sent-116, score-0.349]
</p><p>39 An image is captured at about every 2m on each city street to minimize the total size of the data as well as to maintain the running speed of the vehicle under the constraint of the frame rate of the camera. [sent-118, score-0.405]
</p><p>40 The goal of the present study is to detect the temporal changes of a scene from its images thus obtained at two separate times. [sent-119, score-0.788]
</p><p>41 For computational simplicity, our algorithm for change detection takes as inputs not the omni-directional images but the perspective images cropped from them. [sent-121, score-0.34]
</p><p>42 Estimation of relative camera poses The algorithm shown in the next section uses only several perspective images to detect changes of a scene. [sent-126, score-0.635]
</p><p>43 Choosing a portion of the scene for which we want to detect changes, we crop and warp the original images to have two sets of perspective images covering the scene portion just enough, as shown in Fig. [sent-146, score-0.783]
</p><p>44 In this section, we consider the problem of detecting scene changes from these two sets ofmulti-view perspective images. [sent-148, score-0.682]
</p><p>45 For each pixel x1 of I1, the probability that the scene depth has changed is estimated. [sent-162, score-0.753]
</p><p>46 The probability density of the scene depth at a point x1 of I1 is estimated from I1 and  I2. [sent-165, score-0.781]
</p><p>47 2 to estimate the probability that the scene depth changes at x1 between t and t? [sent-168, score-0.939]
</p><p>48 y) ) fr Cahmoeo shinegre, o tnhee i mpraogpeos freodm mm Iet,h soady c Ionsiders the scene depth at each pixel of I1 and estimates whether or not it changes from t to t? [sent-175, score-0.988]
</p><p>49 The output of the method is the probability of a depth change at each pixel of I1. [sent-177, score-0.497]
</p><p>50 For the first image set I1 its images are used to estimate , the Fdoerpt thhe map o imf tahgee scene at t. [sent-178, score-0.376]
</p><p>51 To be specific, not the value of the depth d but its probabilistic density p(d) is estimated. [sent-179, score-0.462]
</p><p>52 , a spatial point having depth d at a certFaoinr thpeixe otl hoefr tsheet I key frame I1 is projected onto I? [sent-181, score-0.397]
</p><p>53 The higher the similarity is, the more the spatial point is likely to belong to the surface of some object in the scene at t? [sent-186, score-0.378]
</p><p>54 d is computed for each depth d, which gives a density function of d that is similar to p(d). [sent-189, score-0.402]
</p><p>55 d, the proposed method calculates the probability of a depth change. [sent-191, score-0.398]
</p><p>56 In this process, the change probability evaluated for each depth d is integrated over d to yield the overall probability of a depth change. [sent-192, score-0.895]
</p><p>57 This makes it unnecessary to explicitly  determine the scene depth neither at t nor t? [sent-193, score-0.595]
</p><p>58 Estimation of the density of scene depths To estimate the density of scene depths, we use the similarity oflocal patches in the images, as is done in multi-view stereo [5, 9, 13, 16, 23]. [sent-200, score-1.035]
</p><p>59 By dividing the inverse depth in a certain range from near to far away into n discrete values, we denote the depth by indexes d = 1, . [sent-201, score-0.648]
</p><p>60 For a point x1 of I1, we denote the projection onto I2 of a spatial point lying on the ray of x1 and having depth d by x2(d). [sent-205, score-0.464]
</p><p>61 ) ×A 5lt phioxueglsh i sd feor e cpoerrriemctenlyt )matched points will ideally be 0, it will not in practice because of image noise, shape changes of the patches, etc. [sent-218, score-0.412]
</p><p>62 Estimating probabilities of scene changes We introduce a binary variable c to represent whether or not the scene depth at a pixel x1 of the key frame I1 has changed from t to t? [sent-232, score-1.326]
</p><p>63 2 a spatial point lying on the ray of x1 and having depth d, as shown in Fig. [sent-236, score-0.391]
</p><p>64 (4)  This directly gives the probability that the scene changes its structure at the pixel x1 of I1. [sent-274, score-0.648]
</p><p>65 (6)  Here, the term p(c = 1) is the prior probability that the scene depth changes at this pixel. [sent-305, score-0.939]
</p><p>66 d|c = 0), we introduce a binary variable δd to| represent w p(hseth|cer = or ,n wote tihnter scene depth (at x1 of I1 at time t) is d, that is, whether or not the  spatial point having depth d belongs to the surface of some object at t; δd = 1 indicates this is the case and δd = 0 otherwise. [sent-340, score-1.057]
</p><p>67 b(8e) dise tchoem pproobseadb iilnity a that the scene depth is d, and thus it is equivalent to p(d) that has been already obtained; thus, p(δd = 1) = p(d). [sent-351, score-0.595]
</p><p>68 The definition of the variables is as follows: c = 1 indicates the scene depth changes from t to t? [sent-361, score-0.865]
</p><p>69 and c = 0 otherwise; δd = 1 indicates the scene depth is d at time t and δd = 0 otherwise; δd? [sent-362, score-0.595]
</p><p>70 −  HcH 01HδHd0 o0 r 1 01  −  whether the scene depth is d at time t? [sent-365, score-0.666]
</p><p>71 For example, (δd, c) = (1, 0) means that the scene depth is d at time t and remains so at t? [sent-367, score-0.595]
</p><p>72 ; (δd, c) = (1, 1) means that the scene depth is d at t and is not so at t? [sent-368, score-0.595]
</p><p>73 d be a binary variable indicating whether or not the scene depth is d at time t? [sent-371, score-0.666]
</p><p>74 Note that the combination (δd, c) = (0, 1), which means that the scene depth is not d at t and changes at t? [sent-378, score-0.865]
</p><p>75 d = 0, which means the scene depth is not d (at t? [sent-398, score-0.595]
</p><p>76 d measures the similarity between the patches of two different scene points. [sent-400, score-0.428]
</p><p>77 d measures the similarity between the patches of the same scene point. [sent-408, score-0.428]
</p><p>78 5, the scene depth has changed at the pixel, and it has not changed, otherwise. [sent-444, score-0.679]
</p><p>79 Assuming tcheat i sth deirseis no prior on the probability of scene changes, we set p(c = 1) = 0. [sent-463, score-0.345]
</p><p>80 Compared methods We compared our method with MVS-based ones, which first reconstruct the structures of a scene based on MVS and differentiate them to obtain scene changes. [sent-469, score-0.765]
</p><p>81 PMVS2 outputs point clouds, from which we create a depth map viewed from the key frame. [sent-476, score-0.358]
</p><p>82 Two depth maps are created for tahgee t awroea ti omfe 7 p ×o i7nt psi xaenlds are doif dfeeprethnti mateadps t aor oeb ctraeinat scene changes. [sent-478, score-0.683]
</p><p>83 Similarly to the above, two depth maps are computed and are differentiated to obtain scene changes. [sent-491, score-0.63]
</p><p>84 Whether the scene changes or not is judged by whether the difference in its disparity is greater than a threshold. [sent-501, score-0.715]
</p><p>85 The red patches in the depth maps of PMVS2 indicate that there is no reconstructed point in the space. [sent-503, score-0.477]
</p><p>86 Comparing the result of the proposed method with the ground truth, it is seen that the proposed method can correctly detect the scene changes, i. [sent-504, score-0.389]
</p><p>87 , the disappearance of the debris and the digger; the shape of the digger arm is extracted very accurately. [sent-506, score-0.348]
</p><p>88 The proposed method cannot detect the disappearance of the building behind the digger and of the thin layer of sands on the ground surface. [sent-508, score-0.528]
</p><p>89 As these methods differentiate the two depth maps, a slight reconstruction error in each will results in a false positive. [sent-513, score-0.455]
</p><p>90 Thus, even though their estimated depths appear to capture the scene structure mostly well, the estimated scene changes tends to be worse than the impression we have for each depth map alone. [sent-514, score-1.292]
</p><p>91 There are in general several causes of errors in MVSbased depth estimation. [sent-515, score-0.324]
</p><p>92 As the proposed method similarly obtains depth information from image similarity, the same difficulties will have bad influence on the proposed method. [sent-522, score-0.324]
</p><p>93 However, it will be minimized by the probabilistic treatment of the depth map; taking all probabilities into account, the proposed method makes a binary decision as to whether a scene point changes or not. [sent-523, score-1.138]
</p><p>94 Conclusions We have described a method for detecting temporal changes of the 3D structure of an outdoor scene from its multi-view images taken at two separate times. [sent-536, score-0.884]
</p><p>95 These images are captured by a vehicle-mounted camera running in a city street. [sent-537, score-0.444]
</p><p>96 The method estimates the scene depth probabilistically, not deterministically, and judges whether or not the scene depth changes in such a way that the ambiguity of the estimated scene depth is well reflected in the final estimates. [sent-538, score-2.249]
</p><p>97 We have shown several experimental results, in which the proposed method is compared with MVS-based methods, which use MVS to reconstruct the scene structures and differentiate two reconstructions to detect changes. [sent-539, score-0.711]
</p><p>98 It should be noted that our method estimates scene changes independently at each image pixel; no prior on the smoothness or continuity of scene structures is used. [sent-541, score-0.965]
</p><p>99 We may say that the reason why MVS needs such priors is because it has to deterministically determine scene  structures able. [sent-544, score-0.534]
</p><p>100 Image based detection of geometric changes in urban environments. [sent-676, score-0.393]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('depth', 0.324), ('scene', 0.271), ('changes', 0.27), ('mvs', 0.239), ('city', 0.168), ('deterministically', 0.166), ('digger', 0.15), ('reconstructions', 0.144), ('aerial', 0.128), ('depths', 0.123), ('disappearance', 0.123), ('vehicle', 0.12), ('sfm', 0.119), ('sd', 0.109), ('camera', 0.107), ('change', 0.099), ('differentiate', 0.094), ('urban', 0.09), ('temporal', 0.088), ('patches', 0.084), ('changed', 0.084), ('months', 0.083), ('density', 0.078), ('probabilistically', 0.077), ('detecting', 0.077), ('viewpoints', 0.075), ('debris', 0.075), ('sakurada', 0.075), ('tsunami', 0.075), ('probability', 0.074), ('detect', 0.073), ('treatment', 0.073), ('similarity', 0.073), ('whether', 0.071), ('captured', 0.07), ('poses', 0.069), ('reconstruct', 0.067), ('earthquake', 0.066), ('sands', 0.066), ('okatani', 0.066), ('perspective', 0.064), ('japan', 0.062), ('structures', 0.062), ('taneja', 0.061), ('periodically', 0.061), ('probabilistic', 0.06), ('taken', 0.059), ('truths', 0.058), ('contrasted', 0.058), ('stereo', 0.057), ('bundle', 0.057), ('pages', 0.057), ('archives', 0.055), ('seitz', 0.055), ('snavely', 0.054), ('tahgee', 0.053), ('judged', 0.053), ('estimates', 0.052), ('images', 0.052), ('densities', 0.05), ('disparity', 0.05), ('running', 0.047), ('imagery', 0.047), ('registration', 0.046), ('ground', 0.045), ('wong', 0.043), ('matched', 0.042), ('studies', 0.042), ('inputs', 0.04), ('observations', 0.04), ('dense', 0.039), ('onto', 0.039), ('correspondences', 0.039), ('noted', 0.039), ('mounted', 0.038), ('ambiguity', 0.038), ('conditional', 0.037), ('reconstruction', 0.037), ('behind', 0.036), ('confirmed', 0.036), ('adjustment', 0.036), ('priors', 0.035), ('probabilities', 0.035), ('inevitably', 0.035), ('hartley', 0.035), ('maps', 0.035), ('building', 0.035), ('schindler', 0.034), ('separate', 0.034), ('point', 0.034), ('points', 0.033), ('lying', 0.033), ('reflected', 0.033), ('detection', 0.033), ('structure', 0.033), ('tihnter', 0.033), ('andw', 0.033), ('vehiclemounted', 0.033), ('gies', 0.033), ('truthg', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="117-tfidf-1" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>2 0.28377995 <a title="117-tfidf-2" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>3 0.20637949 <a title="117-tfidf-3" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>Author: Aparna Taneja, Luca Ballan, Marc Pollefeys</p><p>Abstract: In this paper, we propose a method to detect changes in the geometry of a city using panoramic images captured by a car driving around the city. We designed our approach to account for all the challenges involved in a large scale application of change detection, such as, inaccuracies in the input geometry, errors in the geo-location data of the images, as well as, the limited amount of information due to sparse imagery. We evaluated our approach on an area of 6 square kilometers inside a city, using 3420 images downloaded from Google StreetView. These images besides being publicly available, are also a good example of panoramic images captured with a driving vehicle, and hence demonstrating all the possible challenges resulting from such an acquisition. We also quantitatively compared the performance of our approach with respect to a ground truth, as well as to prior work. This evaluation shows that our approach outperforms the current state of the art.</p><p>4 0.19199173 <a title="117-tfidf-4" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: In this paper, we propose a convex optimization framework for simultaneous estimation of super-resolved depth map and images from a single moving camera. The pixel measurement error in 3D reconstruction is directly related to the resolution of the images at hand. In turn, even a small measurement error can cause significant errors in reconstructing 3D scene structure or camera pose. Therefore, enhancing image resolution can be an effective solution for securing the accuracy as well as the resolution of 3D reconstruction. In the proposed method, depth map estimation and image super-resolution are formulated in a single energy minimization framework with a convex function and solved efficiently by a first-order primal-dual algorithm. Explicit inter-frame pixel correspondences are not required for our super-resolution procedure, thus we can avoid a huge computation time and obtain improved depth map in the accuracy and resolution as well as highresolution images with reasonable time. The superiority of our algorithm is demonstrated by presenting the improved depth map accuracy, image super-resolution results, and camera pose estimation.</p><p>5 0.17774113 <a title="117-tfidf-5" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>6 0.16396302 <a title="117-tfidf-6" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>7 0.16374347 <a title="117-tfidf-7" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>8 0.15527608 <a title="117-tfidf-8" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>9 0.15511984 <a title="117-tfidf-9" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>10 0.15436146 <a title="117-tfidf-10" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>11 0.15220118 <a title="117-tfidf-11" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>12 0.15138955 <a title="117-tfidf-12" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>13 0.13846029 <a title="117-tfidf-13" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>14 0.13652597 <a title="117-tfidf-14" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>15 0.13628416 <a title="117-tfidf-15" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>16 0.13617021 <a title="117-tfidf-16" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>17 0.13608132 <a title="117-tfidf-17" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>18 0.13585986 <a title="117-tfidf-18" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>19 0.13336711 <a title="117-tfidf-19" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>20 0.12573729 <a title="117-tfidf-20" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.267), (1, 0.223), (2, 0.037), (3, 0.017), (4, -0.043), (5, -0.053), (6, -0.072), (7, 0.103), (8, 0.022), (9, 0.014), (10, -0.038), (11, -0.006), (12, 0.079), (13, 0.141), (14, 0.011), (15, -0.119), (16, -0.171), (17, 0.105), (18, -0.013), (19, -0.065), (20, 0.021), (21, -0.005), (22, -0.006), (23, -0.023), (24, 0.009), (25, 0.001), (26, -0.032), (27, -0.026), (28, -0.011), (29, 0.023), (30, -0.071), (31, 0.019), (32, -0.017), (33, 0.003), (34, 0.008), (35, -0.068), (36, -0.007), (37, 0.003), (38, -0.068), (39, 0.006), (40, 0.021), (41, 0.021), (42, -0.024), (43, -0.017), (44, -0.112), (45, -0.056), (46, -0.076), (47, -0.055), (48, -0.1), (49, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97383356 <a title="117-lsi-1" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>2 0.85910749 <a title="117-lsi-2" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>3 0.83850765 <a title="117-lsi-3" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: In this paper, we propose a convex optimization framework for simultaneous estimation of super-resolved depth map and images from a single moving camera. The pixel measurement error in 3D reconstruction is directly related to the resolution of the images at hand. In turn, even a small measurement error can cause significant errors in reconstructing 3D scene structure or camera pose. Therefore, enhancing image resolution can be an effective solution for securing the accuracy as well as the resolution of 3D reconstruction. In the proposed method, depth map estimation and image super-resolution are formulated in a single energy minimization framework with a convex function and solved efficiently by a first-order primal-dual algorithm. Explicit inter-frame pixel correspondences are not required for our super-resolution procedure, thus we can avoid a huge computation time and obtain improved depth map in the accuracy and resolution as well as highresolution images with reasonable time. The superiority of our algorithm is demonstrated by presenting the improved depth map accuracy, image super-resolution results, and camera pose estimation.</p><p>4 0.8232215 <a title="117-lsi-4" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>Author: unkown-author</p><p>Abstract: We tackle the problem of jointly increasing the spatial resolution and apparent measurement accuracy of an input low-resolution, noisy, and perhaps heavily quantized depth map. In stark contrast to earlier work, we make no use of ancillary data like a color image at the target resolution, multiple aligned depth maps, or a database of highresolution depth exemplars. Instead, we proceed by identifying and merging patch correspondences within the input depth map itself, exploiting patchwise scene self-similarity across depth such as repetition of geometric primitives or object symmetry. While the notion of ‘single-image ’ super resolution has successfully been applied in the context of color and intensity images, we are to our knowledge the first to present a tailored analogue for depth images. Rather than reason in terms of patches of 2D pixels as others have before us, our key contribution is to proceed by reasoning in terms of patches of 3D points, with matched patch pairs related by a respective 6 DoF rigid body motion in 3D. In support of obtaining a dense correspondence field in reasonable time, we introduce a new 3D variant of Patch- Match. A third contribution is a simple, yet effective patch upscaling and merging technique, which predicts sharp object boundaries at the target resolution. We show that our results are highly competitive with those of alternative techniques leveraging even a color image at the target resolution or a database of high-resolution depth exemplars.</p><p>5 0.81407875 <a title="117-lsi-5" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>6 0.81066823 <a title="117-lsi-6" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>7 0.75801665 <a title="117-lsi-7" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>8 0.72519839 <a title="117-lsi-8" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>9 0.70808095 <a title="117-lsi-9" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>10 0.7078706 <a title="117-lsi-10" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>11 0.70598578 <a title="117-lsi-11" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>12 0.70159382 <a title="117-lsi-12" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>13 0.69165862 <a title="117-lsi-13" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>14 0.6769262 <a title="117-lsi-14" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>15 0.67577314 <a title="117-lsi-15" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>16 0.65721685 <a title="117-lsi-16" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>17 0.64776075 <a title="117-lsi-17" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>18 0.64661705 <a title="117-lsi-18" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>19 0.63260645 <a title="117-lsi-19" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>20 0.63252103 <a title="117-lsi-20" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.202), (10, 0.116), (16, 0.033), (26, 0.043), (28, 0.022), (33, 0.298), (67, 0.056), (69, 0.045), (87, 0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89249241 <a title="117-lda-1" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>Author: Tsung-Yi Lin, Serge Belongie, James Hays</p><p>Abstract: The recent availability oflarge amounts ofgeotagged imagery has inspired a number of data driven solutions to the image geolocalization problem. Existing approaches predict the location of a query image by matching it to a database of georeferenced photographs. While there are many geotagged images available on photo sharing and street view sites, most are clustered around landmarks and urban areas. The vast majority of the Earth’s land area has no ground level reference photos available, which limits the applicability of all existing image geolocalization methods. On the other hand, there is no shortage of visual and geographic data that densely covers the Earth we examine overhead imagery and land cover survey data but the relationship between this data and ground level query photographs is complex. In this paper, we introduce a cross-view feature translation approach to greatly extend the reach of image geolocalization methods. We can often localize a query even if it has no corresponding ground– – level images in the database. A key idea is to learn the relationship between ground level appearance and overhead appearance and land cover attributes from sparsely available geotagged ground-level images. We perform experiments over a 1600 km2 region containing a variety of scenes and land cover types. For each query, our algorithm produces a probability density over the region of interest.</p><p>same-paper 2 0.873743 <a title="117-lda-2" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>Author: Ken Sakurada, Takayuki Okatani, Koichiro Deguchi</p><p>Abstract: This paper proposes a method for detecting temporal changes of the three-dimensional structure of an outdoor scene from its multi-view images captured at two separate times. For the images, we consider those captured by a camera mounted on a vehicle running in a city street. The method estimates scene structures probabilistically, not deterministically, and based on their estimates, it evaluates the probability of structural changes in the scene, where the inputs are the similarity of the local image patches among the multi-view images. The aim of the probabilistic treatment is to maximize the accuracy of change detection, behind which there is our conjecture that although it is difficult to estimate the scene structures deterministically, it should be easier to detect their changes. The proposed method is compared with the methods that use multi-view stereo (MVS) to reconstruct the scene structures of the two time points and then differentiate them to detect changes. The experimental results show that the proposed method outperforms such MVS-based methods.</p><p>3 0.86588711 <a title="117-lda-3" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>Author: Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy</p><p>Abstract: A number of computer vision problems such as human age estimation, crowd density estimation and body/face pose (view angle) estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalarvalued output. Such a learning problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated. Encouraged by the recent success in using attributes for solving classification problems with sparse training data, this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. More precisely, low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation (a label) that captures how the scalar output value (e.g. age, people count) changes continuously and cumulatively. Extensive experiments show that our cumulative attribute framework gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models, especially when the labelled training data is sparse with imbalanced sampling.</p><p>4 0.8601715 <a title="117-lda-4" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>Author: Bo Wang, Zhuowen Tu</p><p>Abstract: With the increasing availability of high dimensional data and demand in sophisticated data analysis algorithms, manifold learning becomes a critical technique to perform dimensionality reduction, unraveling the intrinsic data structure. The real-world data however often come with noises and outliers; seldom, all the data live in a single linear subspace. Inspired by the recent advances in sparse subspace learning and diffusion-based approaches, we propose a new manifold denoising algorithm in which data neighborhoods are adaptively inferred via sparse subspace reconstruction; we then derive a new formulation to perform denoising to the original data. Experiments carried out on both toy and real applications demonstrate the effectiveness of our method; it is insensitive to parameter tuning and we show significant improvement over the competing algorithms.</p><p>5 0.85815722 <a title="117-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.85449541 <a title="117-lda-6" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>7 0.8533709 <a title="117-lda-7" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>8 0.85299397 <a title="117-lda-8" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>9 0.85280716 <a title="117-lda-9" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>10 0.85200119 <a title="117-lda-10" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>11 0.85034871 <a title="117-lda-11" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>12 0.85021687 <a title="117-lda-12" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>13 0.85008705 <a title="117-lda-13" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>14 0.85001749 <a title="117-lda-14" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>15 0.84994018 <a title="117-lda-15" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>16 0.84987259 <a title="117-lda-16" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>17 0.8497656 <a title="117-lda-17" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>18 0.84954107 <a title="117-lda-18" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>19 0.84928691 <a title="117-lda-19" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>20 0.84909642 <a title="117-lda-20" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
