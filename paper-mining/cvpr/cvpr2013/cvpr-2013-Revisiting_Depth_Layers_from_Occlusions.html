<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>357 cvpr-2013-Revisiting Depth Layers from Occlusions</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-357" href="#">cvpr2013-357</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>357 cvpr-2013-Revisiting Depth Layers from Occlusions</h1>
<br/><p>Source: <a title="cvpr-2013-357-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kowdle_Revisiting_Depth_Layers_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Adarsh Kowdle, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: In this work, we consider images of a scene with a moving object captured by a static camera. As the object (human or otherwise) moves about the scene, it reveals pairwise depth-ordering or occlusion cues. The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. We cast the problem of depth-layer segmentation as a discrete labeling problem on a spatiotemporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently, and using a na¨ ıve combination of the features.</p><p>Reference: <a title="cvpr-2013-357-reference" href="../cvpr2013_reference/cvpr-2013-Revisiting_Depth_Layers_from_Occlusions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Revisiting Depth Layers from Occlusions Adarsh Kowdle  Andrew Gallagher  Tsuhan Chen  Cornell University apk 6 4 @ corne l edu l  Cornell University acg2 2 6 @ cornel l edu  Cornell University t suhan@ ece cornel l edu  . [sent-1, score-0.174]
</p><p>2 Abstract In this work, we consider images of a scene with a moving object captured by a static camera. [sent-5, score-0.877]
</p><p>3 As the object (human or otherwise) moves about the scene, it reveals pairwise depth-ordering or occlusion cues. [sent-6, score-0.604]
</p><p>4 The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. [sent-7, score-1.942]
</p><p>5 We cast the problem of depth-layer segmentation as a discrete labeling problem on a spatiotemporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. [sent-8, score-1.86]
</p><p>6 We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently, and using a na¨ ıve combination of the features. [sent-9, score-1.675]
</p><p>7 Introduction We consider a time-series ofimages of a scene with moving objects captured from a static camera, and our goal is to exploit occlusion cues revealed as the objects move through  the scene to segment the scene into depth layers. [sent-11, score-2.159]
</p><p>8 Recovering the depth layers of a scene from a 2D image sequence has a number of applications. [sent-12, score-0.535]
</p><p>9 Video surveillance often has a fixed camera focused on a scene with one or more moving objects. [sent-13, score-0.727]
</p><p>10 As objects move through the scene over time, we recover a layered representation of the scene. [sent-14, score-0.269]
</p><p>11 This aides tasks such as object detection and recognition in the presence of occlusions since one can reason about partial observations of an occluded object with a better 3D understanding of the scene [6, 15, 22]. [sent-15, score-0.414]
</p><p>12 In addition, a layered representation of the scene is useful in video editing applications, such as composing novel objects into the scene with occlusion reasoning [30] and changing the depth of focus [24]. [sent-16, score-0.915]
</p><p>13 An image sequence captured from a dynamic (moving) camera allows one to leverage powerful stereo matching cues to recover the depth and occlusion information of the scene. [sent-17, score-0.934]
</p><p>14 However, these cues are absent in the case of a static camera. [sent-18, score-0.403]
</p><p>15 For single images, monocular cues help reveal useful depth information [8, 10, 12, 13, 23, 28, 3 1, 32]. [sent-19, score-0.701]
</p><p>16 In this work, we consider a set of images with moving objects captured from a static camera. [sent-20, score-0.671]
</p><p>17 These pairwise cues are powerful, but sparse, which makes our goal of extracting dense pixel-level depth layers a hard problem. [sent-22, score-0.777]
</p><p>18 In this work, we cast the problem of depth-layer segmentation as a discrete labeling problem on a spatio-temporal MRF over the video. [sent-23, score-0.15]
</p><p>19 We accumulate the pairwise ordering cues revealed as the object moves through the scene and include monocular cues to propagate the sparse occlusion cues through the scene. [sent-24, score-2.081]
</p><p>20 We over-segment the background scene (which has no moving objects) and construct a region-level MRF with edges between adjacent regions. [sent-25, score-0.975]
</p><p>21 In each frame, we identify the pixels corresponding to the moving object and add a node corresponding to each moving object for every frame of the video. [sent-26, score-1.371]
</p><p>22 We add temporal edges between the corresponding moving object nodes across frames, allowing us to encode a smooth motion prior for the moving object. [sent-27, score-1.359]
</p><p>23 As the object moves about the scene, we detect motion occlusion events and add edges between the background scene node and the corresponding moving object node, including long range edges between two background scene nodes to encode the pairwise depth-ordering or occlusion cues. [sent-28, score-2.457]
</p><p>24 An overview of our proposed formulation for a single moving object is shown in Figure 1, with the extension to handle multiple objects in Section 3. [sent-29, score-0.61]
</p><p>25 Our paper, for the first time, proposes a framework for recovering depth layers in static camera scenes by combining depth-ordering cues from moving objects and cues from monocular occlusion reasoning. [sent-32, score-2.098]
</p><p>26 Our approach works with any moving object (human or other-  wise) and extends to multiple objects moving in the scene. [sent-33, score-1.112]
</p><p>27 We show that this depth layer reasoning  out-performs  the  current state-of-the-art in terms of depth-layer recovery. [sent-34, score-0.285]
</p><p>28 Each colored node corresponds to the respective colored region in (b). [sent-37, score-0.192]
</p><p>29 The red nodes correspond to the moving object with a node for every frame f in the input sequence ({1, 2, . [sent-38, score-0.884]
</p><p>30 rc Teh teh ree dob nsoerdvesed c pairwise depth-ordering, foborj einctst wanitche abe ntowdeee fno trh eev green-red fnio nde ths at f = 1t ,s eaqnude bncluee- (r{e1d, n2o,. [sent-43, score-0.266]
</p><p>31 The red edges enforce a smooth motion model for the moving object; (d) Shows the inferred depth layers, white = near and black = far. [sent-47, score-0.991]
</p><p>32 Related work Research in cognitive rely on occlusion  science has shown that humans  cues to obtain object  depth discontinuities  and  even in the absence of strong image  cues such as edges and lighting clusion boundaries  boundaries  [17, 25]. [sent-49, score-1.439]
</p><p>33 Recovering  oc-  in a scene is a classic problem that has  been a topic of wide interest. [sent-50, score-0.164]
</p><p>34 We focus on prior work with the similar setting of static camera scenarios. [sent-51, score-0.195]
</p><p>35 classify  these works into learning-based  approaches  that  purely  rely  on motion  We broadly  approaches  and  occlusion  cues  revealed by the moving object. [sent-52, score-1.296]
</p><p>36 approaches  Prior work has explored  for estimating  the depth of the  scene [8, 10, 12, 14,23,28,3 1,32] and estimating depth ordering [13, 16] from a single image for 3D scene understanding. [sent-54, score-0.827]
</p><p>37 Recent work has shown objects (clutter) in the scene  to aid better depth estimation of the scene [9, 11] through affordances. [sent-55, score-0.605]
</p><p>38 [5] showed that the pose of people interacting with a cluttered room can be used to obtain functional regions and recover a coarse 3D geometry of the room. [sent-57, score-0.144]
</p><p>39 Our work is complementary to this work, and in particular is agnostic to priors about the type of moving object and the type of scene (indoor or outdoor). [sent-58, score-0.796]
</p><p>40 In other words, we do not require a human as the moving object. [sent-59, score-0.502]
</p><p>41 We relate back to prior research in cognitive science that show that occlusion cues we observe are agnostic to any prior about the object. [sent-60, score-0.725]
</p><p>42 We use these sparse, yet strong occlusion cues revealed by the moving object to aid the dense depth layer segmentation of the scene. [sent-61, score-1.658]
</p><p>43 We work with a single static camera image sequence that precludes us from using algorithms for multiview occlusion reasoning using a moving object [7]. [sent-63, score-1.074]
</p><p>44 We focus on segmenting a scene captured by a single static camera into depth layers using occlusion cues revealed by the moving objects. [sent-64, score-1.951]
</p><p>45 [29] who use pairwise occlusion cues to “push” and “pop” the regions of the scene affected by the moving object to obtain depth layers at each frame. [sent-67, score-1.833]
</p><p>46 A limitation of these works is that they reason only about the portion of the scene the object interacts with, leaving behind huge portions of the scene at an unknown depth. [sent-68, score-0.591]
</p><p>47 In addition, since  the interaction with each region is treated independently it leads to excessive fragmentation of the scene as we show in Section 4. [sent-69, score-0.288]
</p><p>48 This fragmentation can be partially avoided [29] by making the (possibly over-restrictive) strong assumption that the moving object stays at a constant depth. [sent-70, score-0.671]
</p><p>49 Our model includes a more reasonable model of object motion. [sent-71, score-0.075]
</p><p>50 In summary, we revisit depth layers from occlusions and address limitations of prior work via a unified framework that leverages sparse depth-ordering cues revealed by the moving object and gracefully propagates them throughout the whole scene. [sent-72, score-1.483]
</p><p>51 Algorithm We formulate the task of segmenting the scene into depth layers as a discrete labeling problem. [sent-74, score-0.604]
</p><p>52 In this section, we first describe our formulation as applied to a scene with a single moving object and then extend the same framework to handle multiple moving objects in the scene. [sent-75, score-1.276]
</p><p>53 We refer to the scene without any moving objects as the background scene. [sent-79, score-0.86]
</p><p>54 We use a calibration stage to obtain a clean background image without any moving objects. [sent-80, score-0.663]
</p><p>55 In the absence of the calibration stage we take advantage of the static camera scenario and obtain an estimate of the background image as the median image over the video. [sent-81, score-0.369]
</p><p>56 Given the background  image we obtain an over-segmentation using mean shift segmentation [4] to give us about 300 superpixels. [sent-82, score-0.209]
</p><p>57 We treat this segmentation as a stencil of background superpixels 222000999200  (a)Objectin-front-ofbackgroundsceneregion (b) Object behind background scene region Figure 2: Pairwise depth-ordering cues. [sent-83, score-0.848]
</p><p>58 Left image shows the background scene segmentation and the right image shows an intermediate frame segmentation with the moving object segment. [sent-84, score-1.077]
</p><p>59 It also reveals new relationships via transitivity; the chair occludes the object and at the same instant the object occludes regions on the wall; therefore the chair occludes the regions on the wall. [sent-86, score-1.133]
</p><p>60 Given the superpixel stencil for the background scene, we update this superpixel map for every frame by identifying the pixels corresponding to the moving object via background subtraction. [sent-89, score-1.525]
</p><p>61 We model the appearance of the background using a per-pixel Gaussian distribution (Ap) centered at the mean color (RGB space) of the pixel across the whole video. [sent-90, score-0.161]
</p><p>62 Given Ap, for every frame we estimate the likelihood for each pixel belonging to the background. [sent-91, score-0.11]
</p><p>63 We label pixels with background likelihood above 90% as confident background pixels and below 10% likelihood as confident moving object pixels. [sent-92, score-1.017]
</p><p>64 Using these as confident initial seeds, we learn an appearance model for the background (BG) and the moving object (FG). [sent-93, score-0.797]
</p><p>65 The moving object segmentation is obtained using iterative graph-cuts [1, 2, 20] updating the BG/FG color models with each iteration similar to GrabCut [27]. [sent-94, score-0.654]
</p><p>66 Figure 2 shows examples of the moving object segmentation overlaid on the background segmentation. [sent-95, score-0.786]
</p><p>67 After this stage, we have the background scene superpixel map and the moving object segmentation for each frame. [sent-96, score-1.156]
</p><p>68 A region-level MRF is constructed over the background scene superpixels where each superpixel is a node with an edge to adjacent superpixels. [sent-97, score-0.819]
</p><p>69 We add a node corresponding to the moving object for every frame of the video  and add temporal edges connecting the moving object nodes on adjacent frames. [sent-98, score-1.623]
</p><p>70 Pairwise depth-ordering cues The object moving through the scene is either occluded by or occludes portions of the scene. [sent-102, score-1.389]
</p><p>71 In our superpixel representation of the scene, we accumulate the pairwise cues using a matrix we call Occlusion Matrix (O) where, Oi,j ∈ {−1, 0, +1} indicates the relationship between superpixel i− a1n,d0 superpixel j tie. [sent-104, score-1.103]
</p><p>72 matrix is updated at every frame of the vide=o using detected motion occlusion events or using learnt monocular cues in absence ofocclusion cues. [sent-111, score-0.999]
</p><p>73 Low-level cues revealed by the moving object in the scene serve as sparse, yet strong pairwise depth-ordering cues. [sent-113, score-1.363]
</p><p>74 We work with the abstract superpixel representation of each frame and use cues similar to prior work [3] to obtain pairwise relationship between the moving object segment and the superpixel it interacts with. [sent-114, score-1.68]
</p><p>75 The cues are intuitive, given a background region the moving object is interacting with, we use the moving object pixels and the boundary pixels of the background region to infer whether the object moved in-front-of this region or behind this region, respectively, as illustrated in Figure 2. [sent-115, score-2.139]
</p><p>76 We update the corresponding entry of the occlusion matrix with Oi,j as +1 to indicate that superpixel i oc-  cludes superpixel j and set Oj,i to −1. [sent-116, score-0.742]
</p><p>77 In addition to the pairwise depth-ordering cues bettwoe −en1 . [sent-117, score-0.438]
</p><p>78 th Ien moving object and the superpixel it is interacting with, we also enforce transitivity while updating the matrix. [sent-118, score-1.036]
</p><p>79 If the object is occluded by a region of the background scene and is simultaneously occluding several regions of the background scene, via transitivity it establishes a pairwise relationship between the occluding background region and each of the other background regions as shown in Figure 2(b). [sent-119, score-1.494]
</p><p>80 More formally, if m refers to the moving object segment simultaneously involved in motion occlusion events with superpixels k and l then, Ok,m = +1 and Ol,m = −1, implies Ok,l = +1. [sent-120, score-1.136]
</p><p>81 This provides a strong depth-ordering cue between k and l. [sent-121, score-0.031]
</p><p>82 In addition, since k and l are not constrained to be adjacent superpixels, long-range edges between non-adjacent superpixels are also a result. [sent-122, score-0.29]
</p><p>83 We use monocular cues to provide evidence about occlusions for the other regions of the scene. [sent-124, score-0.555]
</p><p>84 Given the superpixel map for each frame, we use the work of Hoiem et al. [sent-125, score-0.206]
</p><p>85 [13] that uses learnt priors to determine which of two adjacent superpixels occludes the other. [sent-126, score-0.47]
</p><p>86 For each frame, we first update the occlusion matrix using the motion occlusion cues where available and update the matrix for all the other spatially adjacent superpixels using the monocular cues. [sent-127, score-1.349]
</p><p>87 We do not enforce transitivity here since the monocular cues are not as reliable as motion occlusion 222000999311  term will encourage that itakes a depth label closer (lower label) than j via a large penalty for the red terms and zero penalty for the blue terms. [sent-128, score-1.233]
</p><p>88 The occlusion matrix serves as the observations for modulating the terms of the energy function described below. [sent-132, score-0.295]
</p><p>89 Energy minimization problem The goal given the sparse pairwise depth-ordering constraints is to obtain dense depth-layers. [sent-135, score-0.17]
</p><p>90 One approach is a greedy algorithm where the whole scene starts at layer-0 and with every pairwise depth-ordering constraint regions of the scene are “pushed” and “popped” [3] to obtain the final labeling. [sent-136, score-0.545]
</p><p>91 [13] use a graph with boundaries between superpixels are nodes connected to adjacent boundaries to encourage continuity and closure. [sent-138, score-0.39]
</p><p>92 [16] use image junctions as nodes to obtain a globally consistent depth ordering using a minimum spanning tree. [sent-140, score-0.354]
</p><p>93 In  this work, we use superpixels as nodes in the graph. [sent-141, score-0.206]
</p><p>94 This allows us to directly obtain the depth-layer labeling, and also incorporate long range edges between nodes. [sent-142, score-0.069]
</p><p>95 We formulate depth layer segmentation as a discrete labeling problem where every superpixel is assigned a depth label {1, 2, . [sent-143, score-0.813]
</p><p>96 rTeh eL l iasbe slosm aere p dree-pdtehfi-onredder yeedt lfrarogme closer to the camera moving away i. [sent-150, score-0.596]
</p><p>97 s eWre t ofo trhmeu claamte ethrais m mmouvlitnig-la abwela syeig . [sent-153, score-0.036]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('moving', 0.502), ('cues', 0.302), ('occlusion', 0.265), ('occludes', 0.249), ('depth', 0.209), ('superpixel', 0.206), ('scene', 0.164), ('background', 0.161), ('monocular', 0.158), ('revealed', 0.153), ('superpixels', 0.142), ('pairwise', 0.136), ('layers', 0.13), ('transitivity', 0.126), ('interacts', 0.107), ('static', 0.101), ('ordering', 0.081), ('frame', 0.079), ('adjacent', 0.079), ('object', 0.075), ('motion', 0.074), ('cornell', 0.072), ('stencil', 0.072), ('cornel', 0.072), ('edges', 0.069), ('node', 0.067), ('reveals', 0.066), ('interacting', 0.064), ('nodes', 0.064), ('fragmentation', 0.063), ('moves', 0.062), ('camera', 0.061), ('region', 0.061), ('confident', 0.059), ('mrf', 0.057), ('agnostic', 0.055), ('occluded', 0.055), ('regions', 0.05), ('segmentation', 0.048), ('accumulate', 0.047), ('absence', 0.046), ('occlusions', 0.045), ('ellipse', 0.045), ('events', 0.044), ('layered', 0.042), ('portions', 0.042), ('labeling', 0.041), ('add', 0.04), ('orange', 0.04), ('behind', 0.039), ('white', 0.039), ('reasoning', 0.038), ('layer', 0.038), ('cognitive', 0.037), ('boundaries', 0.037), ('occluding', 0.036), ('trhmeu', 0.036), ('eev', 0.036), ('cllo', 0.036), ('popped', 0.036), ('recovering', 0.035), ('chair', 0.035), ('aid', 0.035), ('captured', 0.035), ('enforce', 0.034), ('sparse', 0.034), ('segment', 0.034), ('red', 0.034), ('prior', 0.033), ('bfo', 0.033), ('suhan', 0.033), ('tsuhan', 0.033), ('cludes', 0.033), ('aere', 0.033), ('cisc', 0.033), ('dob', 0.033), ('objects', 0.033), ('update', 0.032), ('colored', 0.032), ('reveal', 0.032), ('sequence', 0.032), ('discrete', 0.031), ('abe', 0.031), ('pop', 0.031), ('strong', 0.031), ('encourage', 0.031), ('every', 0.031), ('corne', 0.03), ('nen', 0.03), ('modulating', 0.03), ('fouhey', 0.03), ('eqn', 0.03), ('fno', 0.03), ('cast', 0.03), ('black', 0.03), ('recover', 0.03), ('updating', 0.029), ('segmenting', 0.029), ('clusion', 0.029), ('vide', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="357-tfidf-1" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>Author: Adarsh Kowdle, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: In this work, we consider images of a scene with a moving object captured by a static camera. As the object (human or otherwise) moves about the scene, it reveals pairwise depth-ordering or occlusion cues. The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. We cast the problem of depth-layer segmentation as a discrete labeling problem on a spatiotemporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently, and using a na¨ ıve combination of the features.</p><p>2 0.23368397 <a title="357-tfidf-2" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>3 0.22416177 <a title="357-tfidf-3" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>Author: Bojan Pepikj, Michael Stark, Peter Gehler, Bernt Schiele</p><p>Abstract: Despite the success of recent object class recognition systems, the long-standing problem of partial occlusion remains a major challenge, and a principled solution is yet to be found. In this paper we leave the beaten path of methods that treat occlusion as just another source of noise instead, we include the occluder itself into the modelling, by mining distinctive, reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular, we evaluate and compare models that range from standard object class detectors to hierarchical, part-based representations of occluder/occludee pairs. In an extensive evaluation we derive insights that can aid further developments in tackling the occlusion challenge. –</p><p>4 0.18322419 <a title="357-tfidf-4" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>Author: Guang Shu, Afshin Dehghan, Mubarak Shah</p><p>Abstract: We propose an approach to improve the detection performance of a generic detector when it is applied to a particular video. The performance of offline-trained objects detectors are usually degraded in unconstrained video environments due to variant illuminations, backgrounds and camera viewpoints. Moreover, most object detectors are trained using Haar-like features or gradient features but ignore video specificfeatures like consistent colorpatterns. In our approach, we apply a Superpixel-based Bag-of-Words (BoW) model to iteratively refine the output of a generic detector. Compared to other related work, our method builds a video-specific detector using superpixels, hence it can handle the problem of appearance variation. Most importantly, using Conditional Random Field (CRF) along with our super pixel-based BoW model, we develop and algorithm to segment the object from the background . Therefore our method generates an output of the exact object regions instead of the bounding boxes generated by most detectors. In general, our method takes detection bounding boxes of a generic detector as input and generates the detection output with higher average precision and precise object regions. The experiments on four recent datasets demonstrate the effectiveness of our approach and significantly improves the state-of-art detector by 5-16% in average precision.</p><p>5 0.17537458 <a title="357-tfidf-5" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>6 0.15942562 <a title="357-tfidf-6" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>7 0.15867981 <a title="357-tfidf-7" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>8 0.15138955 <a title="357-tfidf-8" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>9 0.14980298 <a title="357-tfidf-9" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>10 0.12937553 <a title="357-tfidf-10" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>11 0.12678355 <a title="357-tfidf-11" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>12 0.12546501 <a title="357-tfidf-12" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>13 0.12310389 <a title="357-tfidf-13" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>14 0.1210695 <a title="357-tfidf-14" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>15 0.11559132 <a title="357-tfidf-15" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>16 0.11163848 <a title="357-tfidf-16" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>17 0.11131267 <a title="357-tfidf-17" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>18 0.11087507 <a title="357-tfidf-18" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>19 0.11035211 <a title="357-tfidf-19" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>20 0.10818458 <a title="357-tfidf-20" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.236), (1, 0.141), (2, 0.094), (3, -0.03), (4, 0.018), (5, -0.018), (6, 0.073), (7, 0.145), (8, -0.035), (9, 0.074), (10, 0.102), (11, -0.073), (12, 0.066), (13, 0.161), (14, 0.084), (15, 0.025), (16, -0.09), (17, 0.05), (18, -0.165), (19, 0.053), (20, 0.057), (21, -0.026), (22, -0.067), (23, -0.058), (24, 0.006), (25, -0.05), (26, -0.01), (27, -0.02), (28, -0.009), (29, 0.067), (30, 0.046), (31, -0.02), (32, 0.076), (33, -0.043), (34, -0.016), (35, 0.003), (36, -0.012), (37, 0.009), (38, 0.163), (39, -0.051), (40, -0.07), (41, 0.0), (42, 0.008), (43, 0.002), (44, -0.14), (45, 0.053), (46, -0.132), (47, -0.067), (48, -0.016), (49, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96554029 <a title="357-lsi-1" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>Author: Adarsh Kowdle, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: In this work, we consider images of a scene with a moving object captured by a static camera. As the object (human or otherwise) moves about the scene, it reveals pairwise depth-ordering or occlusion cues. The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. We cast the problem of depth-layer segmentation as a discrete labeling problem on a spatiotemporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently, and using a na¨ ıve combination of the features.</p><p>2 0.62956852 <a title="357-lsi-2" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>Author: Jason Chang, Donglai Wei, John W. Fisher_III</p><p>Abstract: We develop a generative probabilistic model for temporally consistent superpixels in video sequences. In contrast to supervoxel methods, object parts in different frames are tracked by the same temporal superpixel. We explicitly model flow between frames with a bilateral Gaussian process and use this information to propagate superpixels in an online fashion. We consider four novel metrics to quantify performance of a temporal superpixel representation and demonstrate superior performance when compared to supervoxel methods.</p><p>3 0.6128363 <a title="357-lsi-3" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>4 0.61183023 <a title="357-lsi-4" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>Author: Jeremie Papon, Alexey Abramov, Markus Schoeler, Florentin Wörgötter</p><p>Abstract: Unsupervised over-segmentation of an image into regions of perceptually similar pixels, known as superpixels, is a widely used preprocessing step in segmentation algorithms. Superpixel methods reduce the number of regions that must be considered later by more computationally expensive algorithms, with a minimal loss of information. Nevertheless, as some information is inevitably lost, it is vital that superpixels not cross object boundaries, as such errors will propagate through later steps. Existing methods make use of projected color or depth information, but do not consider three dimensional geometric relationships between observed data points which can be used to prevent superpixels from crossing regions of empty space. We propose a novel over-segmentation algorithm which uses voxel relationships to produce over-segmentations which are fully consistent with the spatial geometry of the scene in three dimensional, rather than projective, space. Enforcing the constraint that segmented regions must have spatial connectivity prevents label flow across semantic object boundaries which might otherwise be violated. Additionally, as the algorithm works directly in 3D space, observations from several calibrated RGB+D cameras can be segmented jointly. Experiments on a large data set of human annotated RGB+D images demonstrate a significant reduction in occurrence of clusters crossing object boundaries, while maintaining speeds comparable to state-of-the-art 2D methods.</p><p>5 0.59668308 <a title="357-lsi-5" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>6 0.59010124 <a title="357-lsi-6" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>7 0.57921273 <a title="357-lsi-7" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>8 0.57214844 <a title="357-lsi-8" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>9 0.54467106 <a title="357-lsi-9" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>10 0.53655344 <a title="357-lsi-10" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>11 0.51964337 <a title="357-lsi-11" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>12 0.51817214 <a title="357-lsi-12" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>13 0.51815343 <a title="357-lsi-13" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>14 0.51669931 <a title="357-lsi-14" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>15 0.51384056 <a title="357-lsi-15" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>16 0.50916135 <a title="357-lsi-16" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>17 0.50655526 <a title="357-lsi-17" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>18 0.49892446 <a title="357-lsi-18" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>19 0.49568337 <a title="357-lsi-19" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>20 0.49290103 <a title="357-lsi-20" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.054), (26, 0.026), (33, 0.75), (67, 0.023), (69, 0.04), (87, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99959439 <a title="357-lda-1" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>Author: Adarsh Kowdle, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: In this work, we consider images of a scene with a moving object captured by a static camera. As the object (human or otherwise) moves about the scene, it reveals pairwise depth-ordering or occlusion cues. The goal of this work is to use these sparse occlusion cues along with monocular depth occlusion cues to densely segment the scene into depth layers. We cast the problem of depth-layer segmentation as a discrete labeling problem on a spatiotemporal Markov Random Field (MRF) that uses the motion occlusion cues along with monocular cues and a smooth motion prior for the moving object. We quantitatively show that depth ordering produced by the proposed combination of the depth cues from object motion and monocular occlusion cues are superior to using either feature independently, and using a na¨ ıve combination of the features.</p><p>2 0.99932528 <a title="357-lda-2" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>Author: Amirreza Shaban, Hamid R. Rabiee, Mehrdad Farajtabar, Marjan Ghazvininejad</p><p>Abstract: Bag of words models for feature extraction have demonstrated top-notch performance in image classification. These representations are usually accompanied by a coding method. Recently, methods that code a descriptor giving regard to its nearby bases have proved efficacious. These methods take into account the nonlinear structure of descriptors, since local similarities are a good approximation of global similarities. However, they confine their usage of the global similarities to nearby bases. In this paper, we propose a coding scheme that brings into focus the manifold structure of descriptors, and devise a method to compute the global similarities of descriptors to the bases. Given a local similarity measure between bases, a global measure is computed. Exploiting the local similarity of a descriptor and its nearby bases, a global measure of association of a descriptor to all the bases is computed. Unlike the locality-based and sparse coding methods, the proposed coding varies smoothly with respect to the underlying manifold. Experiments on benchmark image classification datasets substantiate the superiority oftheproposed method over its locality and sparsity based rivals.</p><p>3 0.99878603 <a title="357-lda-3" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>Author: Neill D.F. Campbell, Kartic Subr, Jan Kautz</p><p>Abstract: Conditional Random Fields (CRFs) are used for diverse tasks, ranging from image denoising to object recognition. For images, they are commonly defined as a graph with nodes corresponding to individual pixels and pairwise links that connect nodes to their immediate neighbors. Recent work has shown that fully-connected CRFs, where each node is connected to every other node, can be solved efficiently under the restriction that the pairwise term is a Gaussian kernel over a Euclidean feature space. In this paper, we generalize the pairwise terms to a non-linear dissimilarity measure that is not required to be a distance metric. To this end, we propose a density estimation technique to derive conditional pairwise potentials in a nonparametric manner. We then use an efficient embedding technique to estimate an approximate Euclidean feature space for these potentials, in which the pairwise term can still be expressed as a Gaussian kernel. We demonstrate that the use of non-parametric models for the pairwise interactions, conditioned on the input data, greatly increases expressive power whilst maintaining efficient inference.</p><p>4 0.99871409 <a title="357-lda-4" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>Author: Petr Gronát, Guillaume Obozinski, Josef Sivic, Tomáš Pajdla</p><p>Abstract: The aim of this work is to localize a query photograph by finding other images depicting the same place in a large geotagged image database. This is a challenging task due to changes in viewpoint, imaging conditions and the large size of the image database. The contribution of this work is two-fold. First, we cast the place recognition problem as a classification task and use the available geotags to train a classifier for each location in the database in a similar manner to per-exemplar SVMs in object recognition. Second, as onlyfewpositive training examples are availablefor each location, we propose a new approach to calibrate all the per-location SVM classifiers using only the negative examples. The calibration we propose relies on a significance measure essentially equivalent to the p-values classically used in statistical hypothesis testing. Experiments are performed on a database of 25,000 geotagged street view images of Pittsburgh and demonstrate improved place recognition accuracy of the proposed approach over the previous work. 2Center for Machine Perception, Faculty of Electrical Engineering 3WILLOW project, Laboratoire d’Informatique de l’E´cole Normale Sup e´rieure, ENS/INRIA/CNRS UMR 8548. 4Universit Paris-Est, LIGM (UMR CNRS 8049), Center for Visual Computing, Ecole des Ponts - ParisTech, 77455 Marne-la-Valle, France</p><p>5 0.99857259 <a title="357-lda-5" href="./cvpr-2013-Dynamic_Scene_Classification%3A_Learning_Motion_Descriptors_with_Slow_Features_Analysis.html">137 cvpr-2013-Dynamic Scene Classification: Learning Motion Descriptors with Slow Features Analysis</a></p>
<p>Author: Christian Thériault, Nicolas Thome, Matthieu Cord</p><p>Abstract: In this paper, we address the challenging problem of categorizing video sequences composed of dynamic natural scenes. Contrarily to previous methods that rely on handcrafted descriptors, we propose here to represent videos using unsupervised learning of motion features. Our method encompasses three main contributions: 1) Based on the Slow Feature Analysis principle, we introduce a learned local motion descriptor which represents the principal and more stable motion components of training videos. 2) We integrate our local motion feature into a global coding/pooling architecture in order to provide an effective signature for each video sequence. 3) We report state of the art classification performances on two challenging natural scenes data sets. In particular, an outstanding improvement of 11 % in classification score is reached on a data set introduced in 2012.</p><p>6 0.99857241 <a title="357-lda-6" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>7 0.99852854 <a title="357-lda-7" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>8 0.99850804 <a title="357-lda-8" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>9 0.99817038 <a title="357-lda-9" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>10 0.99668539 <a title="357-lda-10" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>11 0.99660563 <a title="357-lda-11" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>12 0.99634123 <a title="357-lda-12" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>13 0.99419159 <a title="357-lda-13" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>14 0.99384916 <a title="357-lda-14" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>15 0.98831928 <a title="357-lda-15" href="./cvpr-2013-Scalable_Sparse_Subspace_Clustering.html">379 cvpr-2013-Scalable Sparse Subspace Clustering</a></p>
<p>16 0.98812801 <a title="357-lda-16" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>17 0.98779285 <a title="357-lda-17" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>18 0.98483992 <a title="357-lda-18" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>19 0.98449767 <a title="357-lda-19" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>20 0.9840169 <a title="357-lda-20" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
