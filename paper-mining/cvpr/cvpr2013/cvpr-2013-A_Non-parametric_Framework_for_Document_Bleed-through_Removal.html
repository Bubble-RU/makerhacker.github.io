<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-22" href="#">cvpr2013-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</h1>
<br/><p>Source: <a title="cvpr-2013-22-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Rowley-Brooke_A_Non-parametric_Framework_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Róisín Rowley-Brooke, François Pitié, Anil Kokaram</p><p>Abstract: This paper presents recent work on a new framework for non-blind document bleed-through removal. The framework includes image preprocessing to remove local intensity variations, pixel region classification based on a segmentation of the joint recto-verso intensity histogram and connected component analysis on the subsequent image labelling. Finally restoration of the degraded regions is performed using exemplar-based image inpainting. The proposed method is evaluated visually and numerically on a freely available database of 25 scanned manuscript image pairs with ground truth, and is shown to outperform recent non-blind bleed-through removal techniques.</p><p>Reference: <a title="cvpr-2013-22-reference" href="../cvpr2013_reference/cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract This paper presents recent work on a new framework for non-blind document bleed-through removal. [sent-2, score-0.214]
</p><p>2 The framework includes image preprocessing to remove local intensity variations, pixel region classification based on a segmentation of the joint recto-verso intensity histogram and connected component analysis on the subsequent image labelling. [sent-3, score-0.537]
</p><p>3 Finally restoration of the degraded regions is performed using exemplar-based image inpainting. [sent-4, score-0.314]
</p><p>4 The proposed method is evaluated visually and numerically on a freely available database of 25 scanned manuscript image pairs with ground truth, and is shown to outperform recent non-blind bleed-through removal techniques. [sent-5, score-0.198]
</p><p>5 Introduction Ink bleed-through degradation poses one of the most difficult problems in document restoration. [sent-7, score-0.249]
</p><p>6 It occurs when ink has seeped through from one side of the page and interferes with text on the other side. [sent-8, score-0.171]
</p><p>7 Physical restoration of degraded documents is an invasive, expensive, and time con-  suming process that may affect the integrity of the original. [sent-10, score-0.326]
</p><p>8 It is therefore preferable to perform document restoration on a digital copy, where any number of changes may be made whilst leaving the original document intact. [sent-11, score-0.586]
</p><p>9 Previous approaches to bleed-through removal struggle with severe bleed-through, where the intensity ranges of bleed-through and foreground regions overlap significantly. [sent-12, score-0.491]
</p><p>10 Furthermore in previous non-blind approaches [10, 8, 12], though intensity and spatial information from both recto and verso sides of the page are used to locate bleed-through regions, processing is performed separately on each side. [sent-13, score-1.077]
</p><p>11 The aim of this paper is to present a fully automated, nonparametric approach to non-blind bleed-through removal that can deal with a wider degree of degradation than other approaches, whilst producing results which preserve the characteristics of the original document. [sent-14, score-0.236]
</p><p>12 ie processing is performed on recto and verso images separately to enforce uniform global intensity characteristics. [sent-18, score-0.934]
</p><p>13 Secondly a two stage classification is performed on both sides of the document simultaneously to locate regions of bleed-through degradation. [sent-19, score-0.424]
</p><p>14 Thirdly clean background plate images are created using texture synthesis, and finally restored recto and verso images are obtained by blending the original degraded images and the clean background plates in regions classified as bleed-through. [sent-20, score-1.443]
</p><p>15 Visual and numerical comparisons between the proposed method and three recent non-blind removal methods, using the database and methodology proposed in [13] are made in Section 4, and finally the conclusions are presented in Section 5. [sent-28, score-0.185]
</p><p>16 Previous Work Approaches to bleed-through reduction generally fall into one of two groups; blind or non-blind, depending on whether they operate on one or both sides of the document. [sent-30, score-0.139]
</p><p>17 The image intensity is the main source of information used, with spatial information included in some approaches. [sent-31, score-0.15]
</p><p>18 This assumption does not hold for severe cases where the bleed-through intensity can be equivalent to or darker than the foreground text, and so intensity information alone is not enough to remove bleed-through successfully. [sent-33, score-0.436]
</p><p>19 Non-blind methods make use of intensity information  from both sides of the page, however the sides must first be registered so that they are aligned and of the same reso222999555422  lution. [sent-35, score-0.351]
</p><p>20 Some non-blind methods use comparative intensity information from both sides to improve the performance of well known binarisation algorithms. [sent-36, score-0.311]
</p><p>21 [6] are improved for bleedthrough interference by adding in secondary threshold levels in [1], and the Sauvola and Pietikainen adaptive binarisation algorithm [14], improved by fuzzy classification, is used in [2]. [sent-38, score-0.18]
</p><p>22 The ICA method is extended to doublesided documents in [18], using the recto and verso images as the sources for a blind-source separation. [sent-39, score-0.881]
</p><p>23 A model based approach is used by Moghaddam and Cheriet in [9], where a function of the difference in intensities between the two sides is used to indicate bleed-through regions. [sent-40, score-0.204]
</p><p>24 The same authors incorporate this diffusion model into a unified framework [10], using variational models for both blind and non-blind bleed-through removal with spatial smoothness enforced in the wavelet domain. [sent-42, score-0.303]
</p><p>25 in [7] and [8] proposed a user assisted method that classifies each pixel based on the ratio of intensities between the two sides, with spatial smoothness is enforced in  a dual-layer MRF framework. [sent-44, score-0.289]
</p><p>26 The data cost energy is defined from a small set of user input training data, in the form of coloured strokes drawn by the user in foreground, background, and bleed-through regions on both sides. [sent-45, score-0.144]
</p><p>27 More recently Rowley-Brooke and Kokaram [12] proposed to represent the degradation via linear mixing models combined with foreground text masks, and to estimate restored image intensities explicitly, thus preserving the background texture of the document. [sent-46, score-0.567]
</p><p>28 An example of an image with local intensity variations before (top), and after (bottom) detrending. [sent-49, score-0.123]
</p><p>29 As highlighted in Section 1, the method proposed here seeks to emulate the non-parametric approach of [8], in that no assumptions about the document properties need to be  made, whilst maintaining the restoration goal of [12], that is to preserve the intrinsic characteristics of the document. [sent-50, score-0.4]
</p><p>30 Preprocessing Registration of the recto and verso images is an essential preprocessing step for non-blind bleed-through reduction as it ensures that bleed-through pixels are aligned with their originating text pixels from the opposite side. [sent-54, score-0.918]
</p><p>31 For the purposes of this paper, it is assumed that the input recto and verso images are already registered - those in the database used for testing were registered manually. [sent-55, score-0.901]
</p><p>32 Prior to classification it is necessary to compensate for any variations in the intensity profile over the document image, for example due to page binding or water stains. [sent-56, score-0.408]
</p><p>33 These effects can interfere with bleed-through restoration methods that rely on global intensity properties. [sent-57, score-0.228]
</p><p>34 Since many document imaging projects perform little or no image enhancement it can not be assumed that the resultant images have uniform global intensity properties. [sent-58, score-0.337]
</p><p>35 Therefore the recto and verso images are adjusted separately by applying local intensity offsets such that the peaks of the lo-  ×  cal intensity histograms, corresponding to mean local background intensities, are aligned. [sent-60, score-1.161]
</p><p>36 This is performed by examining intensity histograms of overlapping blocks in the original image and storing the corresponding peak intensities. [sent-61, score-0.176]
</p><p>37 Classification  The proposed method aims to create a joint labelling of recto and verso images, from a set of four ‘pair’ labels: background on both sides, bgbg, recto foreground and verso bleed-through, fgbl, recto bleed-through and verso foreground, blfg, or foreground on both sides, fgfg. [sent-66, score-2.928]
</p><p>38 Thus the recto and verso images 푟, 푣 are treated as a joint image 푝, and each pixel pair 푟(푖, 푗) , 푣(푖, 푗) is treated as a single pixel 푝(푖, 푗) with intensity pair x in the range [0, 255] , where 0 corresponds to white, and 255 to black. [sent-67, score-0.96]
</p><p>39 The motivation for considering pair rather than single intensities is to reduce the instances of overlap between labels. [sent-68, score-0.131]
</p><p>40 There are therefore two stages to classification, firstly ajoint histogram of intensity pairs is segmented into four regions 222999555533  Figure 2. [sent-70, score-0.242]
</p><p>41 This histogram labelling is then used as a map to obtain an initial image labelling. [sent-73, score-0.194]
</p><p>42 Secondly a set of rules governing connected label components in the image labelling is applied to produce the final labels for the rectoverso image 푝. [sent-74, score-0.449]
</p><p>43 2, it is clear from the  large peak in the points with lighter intensity that the largest proportion of pixels in 푝 will correspond to regions where both recto and verso are background (bgbg). [sent-79, score-1.134]
</p><p>44 So the labelling is formulated as a MRF framework with a spatial smoothness prior based in the recto-verso image domain rather than the joint histogram domain. [sent-83, score-0.277]
</p><p>45 Document background regions generally have a lower range of intensities than foreground, so to prevent over classification of points as bgbg, 푈x(푙x) is defined as the mahalanobis distance between point x and the centre of the label cluster corresponding to 푙x. [sent-88, score-0.306]
</p><p>46 e  풩x = {y∣y =푝(푖′, 푗′) , x =푝(푖, 푗) , (푖′푗′) ∈ 풩푖,푗 }  (2)  So each in{stance of an intensity pair x is located} in the recto-verso image 푝, then the corresponding points in the joint histogram of the 4-connect neighbours in 푝 of these instances are added to the neighbourhood of x. [sent-97, score-0.21]
</p><p>47 Binary Terms: The pairwise energy 푉 (푙x, 푙y) represents the cost of neighbouring points in the histogram being assigned labels 푙x and 푙y respectively. [sent-98, score-0.142]
</p><p>48 Smoothness Weight: A smoothness weight is applied to 푉 (푙x, 푙y) to balance the influence of the binary and unary energies, and depends on the range of intensities in the recto-verso image. [sent-100, score-0.163]
</p><p>49 When the range of intensities is small, there is a greater overlap between labels, and so there is less information available from the recto-verso intensities. [sent-101, score-0.131]
</p><p>50 2  Image Segmentation  Following colour segmentation, the image labelling is initialised by using the histogram labelling as a look up table for pixels in the recto-verso image 푝. [sent-111, score-0.327]
</p><p>51 A subset of pixels will inevitably be misclassified due to the overlapping nature of the histogram label boundaries, however as the pairwise energy used in the histogram segmentation is derived from neighbourhoods in the image domain, spatial smoothness 222999555644  Figure 3. [sent-112, score-0.327]
</p><p>52 Left to right: recto extract, verso extract, image labelling before rules applied, and after rules applied. [sent-114, score-1.1]
</p><p>53 Row 1: Misclassified bgbg components (dark blue) are corrected. [sent-115, score-0.335]
</p><p>54 Row 2: fgfg components (pink) are replaced with fgbl (green). [sent-116, score-0.538]
</p><p>55 Row 3: fgbl components (green) connected to blfg (light blue), but not fgfg (pink) are replaced with blfg. [sent-117, score-0.822]
</p><p>56 Row 4: A blfg component is connected to fgfg, but not bgbg so is replaced with fgfg. [sent-118, score-0.642]
</p><p>57 Therefore a full per-pixel analysis is not performed on the image labelling, and instead connected components of each label  are examined, and rules governing permitted neighbouring components iteratively applied to correct misclassifications until convergence. [sent-120, score-0.408]
</p><p>58 bgbg: This label covers the greatest proportion of the image, and so connected components will mostly be larger than the average character size. [sent-122, score-0.29]
</p><p>59 Smaller components correspond either to valid within character spaces, such as in ‘a’ and ‘o’, or to misclassifications. [sent-123, score-0.137]
</p><p>60 To avoid relabelling valid within character spaces, only the connected components that are less than 10% of the average character component size are analysed. [sent-124, score-0.311]
</p><p>61 Presumed to be mislabelled these components are relabelled with the label corresponding to the largest proportion of their neighbours. [sent-125, score-0.294]
</p><p>62 fgfg: Conversely, this label covers the smallest proportion of the image, and as very dark bleed-through can often be mislabelled as fgfg, no assumptions can be made about the size of components and all are examined. [sent-126, score-0.248]
</p><p>63 The outer edges of components with this label must contain both fgbl and blfg labels, as overlapping text regions will originate from text alone on both sides. [sent-127, score-0.799]
</p><p>64 If this is not the case the component is relabelled fgbl or blfg according to which is present in the outer edge, or as bgbg if neither. [sent-128, score-0.894]
</p><p>65 fgbl: For this label, again only components less than 10% of the average character size are examined. [sent-129, score-0.137]
</p><p>66 The outer edges of these components must contain either fgfg and bgbg, or bgbg only. [sent-130, score-0.623]
</p><p>67 If the outer edge of a component contains fgfg, but not bgbg also, then the component is relabelled as fgfg. [sent-131, score-0.509]
</p><p>68 If the outer edge of a component contains the label blfg ,but  Figure 4. [sent-132, score-0.379]
</p><p>69 Top row left: degraded recto with feint ruled lines, right: corresponding verso. [sent-134, score-0.624]
</p><p>70 Second row left: image labelling (dark blue=texture source), right: visible artefacts in the recto background plate. [sent-135, score-0.657]
</p><p>71 Bottom left: labelling with 10% of source gradients removed (yellow), right: the improved background plate. [sent-136, score-0.257]
</p><p>72 blfg: Components labelled blfg are processed in exactly the same way as fgbl, with the two labels interchanged. [sent-138, score-0.305]
</p><p>73 Restoration The aim of this method is to preserve as much of the document as possible; the background texture is preserved to ensure that the experience of studying the document image remains close to that of studying the physical docu-  ment. [sent-142, score-0.551]
</p><p>74 The restored recto and verso images ˆ 푟(푥, 푦) , ˆ 푣(푥, 푦) are obtained by replacing identified bleed-through regions, where 푙푖 = fgbl for ˆ 푟(푥, 푦), and 푙푖 = blfg for ˆ 푣(푥, 푦), with background texture from clean background images 푟푏(푥, 푦) , 푣푏(푥, 푦). [sent-143, score-1.56]
</p><p>75 The images 푟푏(푥, 푦) , 푣푏(푥, 푦) for recto and verso sides are generated using regions labelled as bgbg as the texture source, and inpainting all other label regions. [sent-148, score-1.411]
</p><p>76 Problems may be encountered with this approach in regions where feint foreground information might not have been identified during classification, with the result that foreground patterns are replicated in the background images. [sent-149, score-0.439]
</p><p>77 To mitigate this the gradients of the regions labelled as bgbg are examined and the highest 10% of gradients removed from the inpainting source (see Fig 4). [sent-150, score-0.516]
</p><p>78 2  Blending  Using a per-pixel replacement of bleed-through pixels with corresponding clean background pixels creates restored im222999555755  Figure 5. [sent-153, score-0.226]
</p><p>79 An example of blending the background image with the degraded image in bleed-through boundary regions. [sent-154, score-0.279]
</p><p>80 Results & Discussion The proposed method was tested on the database of 25 manuscript recto-verso image pairs with manually created binary foreground text images, presented in [13]. [sent-160, score-0.268]
</p><p>81 The results are evaluated first subjectively via a visual comparison, and then objectively, via a numerical comparison, against three recent non-blind bleed-through removal methods: (i) The dual-layer MRF approach with user trained  likelihood proposed by Huang et al. [sent-161, score-0.202]
</p><p>82 The user assisted method (H) copes well with dark bleed-through when it is isolated, but tends to remove foreground text in overlapping fgfg regions, reducing legibility. [sent-168, score-0.602]
</p><p>83 The Wavelet method (M) preserves the foreground information, but does not cope well with dark bleed-through regions, leaving visible artefacts. [sent-169, score-0.203]
</p><p>84 The linear model based approach (R) also preserves foreground information well in most cases, but again does not cope well with dark-bleed through regions. [sent-170, score-0.159]
</p><p>85 The proposed method removes most of the bleed-through in all the examples whilst preserving the foreground well. [sent-171, score-0.187]
</p><p>86 However this is achieved as the cost of foreground information in fgfg regions and so this method performs worst in terms of FgError. [sent-195, score-0.417]
</p><p>87 The Wavelet based method (M) [10] preserves the foreground well, however does not cope well with severe bleedthrough so has a high average BgError and is ranked fourth for this metric. [sent-196, score-0.28]
</p><p>88 This is due to the fact that the mixing parameters in the model are assigned a very high smoothness such that at each successive estimation iteration the bleed-through removed regions increase in size and regions misclassified in the initial stages are gradually blended into the background. [sent-198, score-0.312]
</p><p>89 In each example from top to bottom: Degraded recto and verso images, results from the user assisted method (H) [8], results from the Wavelet method (M) [10], results from the linear-  based method (R) [12], results from the proposed method. [sent-201, score-0.904]
</p><p>90 disadvantage of such a high smoothness is that valid foreground characters connected to bleed-through regions may also be increasingly blended into the background (as can be seen in the top left example ofFig. [sent-202, score-0.411]
</p><p>91 The pairwise comparison results (Table 3) and RP metric rankings highlight that the proposed method outperforms the other three in terms of foreground preservation, and overall error. [sent-204, score-0.159]
</p><p>92 The preprocessing stage removes intensity trends in the input images. [sent-207, score-0.202]
</p><p>93 The classification stage has the advantage over other methods that both recto and verso images are processed simultaneously, first by performing a joint histogram segmentation, then by applying  rules to label connected components in the corresponding image segmentation. [sent-208, score-1.202]
</p><p>94 The restoration is performed using exemplar based image inpainting to preserve the character of the original document image. [sent-209, score-0.469]
</p><p>95 03054  HMRProposed  Probability of foreground error Figure 7. [sent-214, score-0.134]
</p><p>96 Enhanced bleedthrough correction for early music documents with recto-verso registration. [sent-223, score-0.208]
</p><p>97 Restoring ink bleed-through degraded document images using a recursive unsupervised classification technique. [sent-253, score-0.461]
</p><p>98 Color space transformations for analysis and enhancement of ancient degraded manuscripts. [sent-363, score-0.191]
</p><p>99 Fast correction of bleed-through distortion in grayscale documents by a blind source separation technique. [sent-383, score-0.138]
</p><p>100 Document ink bleed-through removal with two hidden markov random fields and a single observation field. [sent-392, score-0.19]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('recto', 0.428), ('verso', 0.383), ('bgbg', 0.27), ('blfg', 0.225), ('fgfg', 0.225), ('document', 0.214), ('fgbl', 0.203), ('degraded', 0.151), ('foreground', 0.134), ('labelling', 0.133), ('intensity', 0.123), ('removal', 0.12), ('restored', 0.116), ('intensities', 0.106), ('restoration', 0.105), ('sides', 0.098), ('bgerror', 0.09), ('binarisation', 0.09), ('bleedthrough', 0.09), ('fgerror', 0.09), ('relabelled', 0.09), ('rules', 0.078), ('moghaddam', 0.074), ('character', 0.072), ('documents', 0.07), ('ink', 0.07), ('background', 0.068), ('binarised', 0.068), ('gatos', 0.068), ('kokaram', 0.068), ('components', 0.065), ('outer', 0.063), ('histogram', 0.061), ('blending', 0.06), ('connected', 0.059), ('regions', 0.058), ('smoothness', 0.057), ('text', 0.056), ('whilst', 0.053), ('manuscript', 0.052), ('wavelet', 0.052), ('preprocessing', 0.051), ('inpainting', 0.05), ('assisted', 0.05), ('misclassified', 0.05), ('labelled', 0.049), ('label', 0.048), ('music', 0.048), ('proportion', 0.046), ('cheriet', 0.045), ('feint', 0.045), ('mislabelled', 0.045), ('piti', 0.045), ('sauvola', 0.045), ('tonazzini', 0.045), ('replaced', 0.045), ('page', 0.045), ('dark', 0.044), ('component', 0.043), ('user', 0.043), ('lncs', 0.042), ('clean', 0.042), ('blind', 0.041), ('ancient', 0.04), ('hysteresis', 0.04), ('objectively', 0.04), ('numerical', 0.039), ('peaks', 0.036), ('editors', 0.035), ('blended', 0.035), ('governing', 0.035), ('degradation', 0.035), ('rp', 0.033), ('misclassifications', 0.033), ('examined', 0.033), ('enforced', 0.033), ('ica', 0.032), ('mrf', 0.032), ('registered', 0.032), ('severe', 0.031), ('labels', 0.031), ('pink', 0.031), ('removed', 0.029), ('peak', 0.028), ('artefacts', 0.028), ('stage', 0.028), ('preserve', 0.028), ('texture', 0.027), ('source', 0.027), ('classification', 0.026), ('joint', 0.026), ('database', 0.026), ('overlapping', 0.025), ('overlap', 0.025), ('pairwise', 0.025), ('secondly', 0.025), ('remove', 0.025), ('neighbouring', 0.025), ('mixing', 0.025), ('preserves', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="22-tfidf-1" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>Author: Róisín Rowley-Brooke, François Pitié, Anil Kokaram</p><p>Abstract: This paper presents recent work on a new framework for non-blind document bleed-through removal. The framework includes image preprocessing to remove local intensity variations, pixel region classification based on a segmentation of the joint recto-verso intensity histogram and connected component analysis on the subsequent image labelling. Finally restoration of the degraded regions is performed using exemplar-based image inpainting. The proposed method is evaluated visually and numerically on a freely available database of 25 scanned manuscript image pairs with ground truth, and is shown to outperform recent non-blind bleed-through removal techniques.</p><p>2 0.087575451 <a title="22-tfidf-2" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>Author: Ľ</p><p>Abstract: Our goal is to detect humans and estimate their 2D pose in single images. In particular, handling cases of partial visibility where some limbs may be occluded or one person is partially occluding another. Two standard, but disparate, approaches have developed in the field: the first is the part based approach for layout type problems, involving optimising an articulated pictorial structure; the second is the pixel based approach for image labelling involving optimising a random field graph defined on the image. Our novel contribution is a formulation for pose estimation which combines these two models in a principled way in one optimisation problem and thereby inherits the advantages of both of them. Inference on this joint model finds the set of instances of persons in an image, the location of their joints, and a pixel-wise body part labelling. We achieve near or state of the art results on standard human pose data sets, and demonstrate the correct estimation for cases of self-occlusion, person overlap and image truncation.</p><p>3 0.08628758 <a title="22-tfidf-3" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>Author: Ehsan Shahrian, Deepu Rajan, Brian Price, Scott Cohen</p><p>Abstract: In this paper, we present a new image matting algorithm that achieves state-of-the-art performance on a benchmark dataset of images. This is achieved by solving two major problems encountered by current sampling based algorithms. The first is that the range in which the foreground and background are sampled is often limited to such an extent that the true foreground and background colors are not present. Here, we describe a method by which a more comprehensive and representative set of samples is collected so as not to miss out on the true samples. This is accomplished by expanding the sampling range for pixels farther from the foreground or background boundary and ensuring that samples from each color distribution are included. The second problem is the overlap in color distributions of foreground and background regions. This causes sampling based methods to fail to pick the correct samples for foreground and background. Our design of an objective function forces those foreground and background samples to be picked that are generated from well-separated distributions. Comparison on the dataset at and evaluation by www.alphamatting.com shows that the proposed method ranks first in terms of error measures used in the website.</p><p>4 0.085846499 <a title="22-tfidf-4" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>Author: Gautam Singh, Jana Kosecka</p><p>Abstract: This paper presents a nonparametric approach to semantic parsing using small patches and simple gradient, color and location features. We learn the relevance of individual feature channels at test time using a locally adaptive distance metric. To further improve the accuracy of the nonparametric approach, we examine the importance of the retrieval set used to compute the nearest neighbours using a novel semantic descriptor to retrieve better candidates. The approach is validated by experiments on several datasets used for semantic parsing demonstrating the superiority of the method compared to the state of art approaches.</p><p>5 0.082379036 <a title="22-tfidf-5" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>Author: Peng Ye, Jayant Kumar, Le Kang, David Doermann</p><p>Abstract: This paper addresses the problem of general-purpose No-Reference Image Quality Assessment (NR-IQA) with the goal ofdeveloping a real-time, cross-domain model that can predict the quality of distorted images without prior knowledge of non-distorted reference images and types of distortions present in these images. The contributions of our work are two-fold: first, the proposed method is highly efficient. NR-IQA measures are often used in real-time imaging or communication systems, therefore it is important to have a fast NR-IQA algorithm that can be used in these real-time applications. Second, the proposed method has the potential to be used in multiple image domains. Previous work on NR-IQA focus primarily on predicting quality of natural scene image with respect to human perception, yet, in other image domains, the final receiver of a digital image may not be a human. The proposed method consists of the following components: (1) a local feature extractor; (2) a global feature extractor and (3) a regression model. While previous approaches usually treat local feature extraction and regres- sion model training independently, we propose a supervised method based on back-projection, which links the two steps by learning a compact set of filters which can be applied to local image patches to obtain discriminative local features. Using a small set of filters, the proposed method is extremely fast. We have tested this method on various natural scene and document image datasets and obtained stateof-the-art results.</p><p>6 0.081065319 <a title="22-tfidf-6" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>7 0.079195403 <a title="22-tfidf-7" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>8 0.072890982 <a title="22-tfidf-8" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>9 0.067542002 <a title="22-tfidf-9" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>10 0.055355661 <a title="22-tfidf-10" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>11 0.054491546 <a title="22-tfidf-11" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>12 0.054188319 <a title="22-tfidf-12" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>13 0.053808376 <a title="22-tfidf-13" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>14 0.052830338 <a title="22-tfidf-14" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>15 0.052790038 <a title="22-tfidf-15" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>16 0.050327826 <a title="22-tfidf-16" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>17 0.046670884 <a title="22-tfidf-17" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>18 0.046498265 <a title="22-tfidf-18" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>19 0.045491491 <a title="22-tfidf-19" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>20 0.044850141 <a title="22-tfidf-20" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.122), (1, 0.024), (2, 0.017), (3, 0.026), (4, 0.032), (5, 0.02), (6, 0.004), (7, 0.015), (8, -0.02), (9, -0.005), (10, 0.037), (11, -0.015), (12, -0.011), (13, 0.01), (14, 0.002), (15, -0.01), (16, 0.018), (17, -0.034), (18, 0.057), (19, -0.015), (20, -0.006), (21, 0.062), (22, -0.032), (23, -0.045), (24, -0.009), (25, -0.065), (26, 0.11), (27, 0.031), (28, -0.05), (29, 0.001), (30, 0.022), (31, 0.018), (32, -0.008), (33, -0.032), (34, -0.041), (35, -0.008), (36, -0.032), (37, 0.001), (38, -0.073), (39, 0.017), (40, -0.046), (41, 0.039), (42, -0.005), (43, -0.025), (44, -0.014), (45, -0.037), (46, -0.02), (47, 0.015), (48, 0.058), (49, -0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92027062 <a title="22-lsi-1" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>Author: Róisín Rowley-Brooke, François Pitié, Anil Kokaram</p><p>Abstract: This paper presents recent work on a new framework for non-blind document bleed-through removal. The framework includes image preprocessing to remove local intensity variations, pixel region classification based on a segmentation of the joint recto-verso intensity histogram and connected component analysis on the subsequent image labelling. Finally restoration of the degraded regions is performed using exemplar-based image inpainting. The proposed method is evaluated visually and numerically on a freely available database of 25 scanned manuscript image pairs with ground truth, and is shown to outperform recent non-blind bleed-through removal techniques.</p><p>2 0.70795816 <a title="22-lsi-2" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>Author: Atsushi Shimada, Hajime Nagahara, Rin-ichiro Taniguchi</p><p>Abstract: Background modeling and subtraction is an essential task in video surveillance applications. Most traditional studies use information observed in past frames to create and update a background model. To adapt to background changes, the backgroundmodel has been enhancedby introducing various forms of information including spatial consistency and temporal tendency. In this paper, we propose a new framework that leverages information from a future period. Our proposed approach realizes a low-cost and highly accurate background model. The proposed framework is called bidirectional background modeling, and performs background subtraction based on bidirectional analysis; i.e., analysis from past to present and analysis from future to present. Although a result will be output with some delay because information is takenfrom a futureperiod, our proposed approach improves the accuracy by about 30% if only a 33-millisecond of delay is acceptable. Furthermore, the memory cost can be reduced by about 65% relative to typical background modeling.</p><p>3 0.66099912 <a title="22-lsi-3" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>Author: Ehsan Shahrian, Deepu Rajan, Brian Price, Scott Cohen</p><p>Abstract: In this paper, we present a new image matting algorithm that achieves state-of-the-art performance on a benchmark dataset of images. This is achieved by solving two major problems encountered by current sampling based algorithms. The first is that the range in which the foreground and background are sampled is often limited to such an extent that the true foreground and background colors are not present. Here, we describe a method by which a more comprehensive and representative set of samples is collected so as not to miss out on the true samples. This is accomplished by expanding the sampling range for pixels farther from the foreground or background boundary and ensuring that samples from each color distribution are included. The second problem is the overlap in color distributions of foreground and background regions. This causes sampling based methods to fail to pick the correct samples for foreground and background. Our design of an objective function forces those foreground and background samples to be picked that are generated from well-separated distributions. Comparison on the dataset at and evaluation by www.alphamatting.com shows that the proposed method ranks first in terms of error measures used in the website.</p><p>4 0.64189625 <a title="22-lsi-4" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>Author: Xiaobo Ren, Tony X. Han, Zhihai He</p><p>Abstract: We consider video object cut as an ensemble of framelevel background-foreground object classifiers which fuses information across frames and refine their segmentation results in a collaborative and iterative manner. Our approach addresses the challenging issues of modeling of background with dynamic textures and segmentation of foreground objects from cluttered scenes. We construct patch-level bagof-words background models to effectively capture the background motion and texture dynamics. We propose a foreground salience graph (FSG) to characterize the similarity of an image patch to the bag-of-words background models in the temporal domain and to neighboring image patches in the spatial domain. We incorporate this similarity information into a graph-cut energy minimization framework for foreground object segmentation. The background-foreground classification results at neighboring frames are fused together to construct a foreground probability map to update the graph weights. The resulting object shapes at neighboring frames are also used as constraints to guide the energy minimization process during graph cut. Our extensive experimental results and performance comparisons over a diverse set of challenging videos with dynamic scenes, including the new Change Detection Challenge Dataset, demonstrate that the proposed ensemble video object cut method outperforms various state-ofthe-art algorithms.</p><p>5 0.64055312 <a title="22-lsi-5" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>Author: Gunhee Kim, Eric P. Xing</p><p>Abstract: With an explosion of popularity of online photo sharing, we can trivially collect a huge number of photo streams for any interesting topics such as scuba diving as an outdoor recreational activity class. Obviously, the retrieved photo streams are neither aligned nor calibrated since they are taken in different temporal, spatial, and personal perspectives. However, at the same time, they are likely to share common storylines that consist of sequences of events and activities frequently recurred within the topic. In this paper, as a first technical step to detect such collective storylines, we propose an approach to jointly aligning and segmenting uncalibrated multiple photo streams. The alignment task discovers the matched images between different photo streams, and the image segmentation task parses each image into multiple meaningful regions to facilitate the image understanding. We close a loop between the two tasks so that solving one task helps enhance the performance of the other in a mutually rewarding way. To this end, we design a scalable message-passing based optimization framework to jointly achieve both tasks for the whole input image set at once. With evaluation on the new Flickr dataset of 15 outdoor activities that consist of 1.5 millions of images of 13 thousands of photo streams, our empirical results show that the proposed algorithms are more successful than other candidate methods for both tasks.</p><p>6 0.62168324 <a title="22-lsi-6" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>7 0.60128599 <a title="22-lsi-7" href="./cvpr-2013-Image_Matting_with_Local_and_Nonlocal_Smooth_Priors.html">211 cvpr-2013-Image Matting with Local and Nonlocal Smooth Priors</a></p>
<p>8 0.5983575 <a title="22-lsi-8" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>9 0.59437352 <a title="22-lsi-9" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>10 0.59245032 <a title="22-lsi-10" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>11 0.56904668 <a title="22-lsi-11" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>12 0.55296224 <a title="22-lsi-12" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>13 0.55239826 <a title="22-lsi-13" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>14 0.55002052 <a title="22-lsi-14" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>15 0.53365678 <a title="22-lsi-15" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>16 0.53225577 <a title="22-lsi-16" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>17 0.52612984 <a title="22-lsi-17" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>18 0.52406681 <a title="22-lsi-18" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>19 0.52071619 <a title="22-lsi-19" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>20 0.51947004 <a title="22-lsi-20" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.121), (16, 0.046), (26, 0.025), (28, 0.015), (33, 0.246), (67, 0.064), (69, 0.035), (87, 0.057), (98, 0.294)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77843881 <a title="22-lda-1" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>Author: Róisín Rowley-Brooke, François Pitié, Anil Kokaram</p><p>Abstract: This paper presents recent work on a new framework for non-blind document bleed-through removal. The framework includes image preprocessing to remove local intensity variations, pixel region classification based on a segmentation of the joint recto-verso intensity histogram and connected component analysis on the subsequent image labelling. Finally restoration of the degraded regions is performed using exemplar-based image inpainting. The proposed method is evaluated visually and numerically on a freely available database of 25 scanned manuscript image pairs with ground truth, and is shown to outperform recent non-blind bleed-through removal techniques.</p><p>2 0.77388138 <a title="22-lda-2" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>Author: Renato F. Salas-Moreno, Richard A. Newcombe, Hauke Strasdat, Paul H.J. Kelly, Andrew J. Davison</p><p>Abstract: We present the major advantages of a new ‘object oriented’ 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, realtime 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph optimisation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruction, but with a huge representation compression. The object graph enables predictions for accurate ICP-based camera to model tracking at each live frame, and efficient active search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relocalisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction.</p><p>3 0.71142441 <a title="22-lda-3" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>Author: S. Hussain Raza, Matthias Grundmann, Irfan Essa</p><p>Abstract: We present a novel algorithm for estimating the broad 3D geometric structure of outdoor video scenes. Leveraging spatio-temporal video segmentation, we decompose a dynamic scene captured by a video into geometric classes, based on predictions made by region-classifiers that are trained on appearance and motion features. By examining the homogeneity of the prediction, we combine predictions across multiple segmentation hierarchy levels alleviating the need to determine the granularity a priori. We built a novel, extensive dataset on geometric context of video to evaluate our method, consisting of over 100 groundtruth annotated outdoor videos with over 20,000 frames. To further scale beyond this dataset, we propose a semisupervised learning framework to expand the pool of labeled data with high confidence predictions obtained from unlabeled data. Our system produces an accurate prediction of geometric context of video achieving 96% accuracy across main geometric classes.</p><p>4 0.70006371 <a title="22-lda-4" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>5 0.69788098 <a title="22-lda-5" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>6 0.69746035 <a title="22-lda-6" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>7 0.69723952 <a title="22-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.69646299 <a title="22-lda-8" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>9 0.69584072 <a title="22-lda-9" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>10 0.69523937 <a title="22-lda-10" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>11 0.69513386 <a title="22-lda-11" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>12 0.6949898 <a title="22-lda-12" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>13 0.69467413 <a title="22-lda-13" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>14 0.6940912 <a title="22-lda-14" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>15 0.69393826 <a title="22-lda-15" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>16 0.69384998 <a title="22-lda-16" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>17 0.69366509 <a title="22-lda-17" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>18 0.69347173 <a title="22-lda-18" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>19 0.6933586 <a title="22-lda-19" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>20 0.69314897 <a title="22-lda-20" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
