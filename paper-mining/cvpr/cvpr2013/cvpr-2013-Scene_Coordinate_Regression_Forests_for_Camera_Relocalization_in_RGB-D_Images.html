<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-380" href="#">cvpr2013-380</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</h1>
<br/><p>Source: <a title="cvpr-2013-380-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shotton_Scene_Coordinate_Regression_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>Reference: <a title="cvpr-2013-380-reference" href="../cvpr2013_reference/cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. [sent-2, score-1.047]
</p><p>2 The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. [sent-3, score-0.775]
</p><p>3 The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. [sent-4, score-0.685]
</p><p>4 The camera pose is inferred using a robust optimization scheme. [sent-5, score-0.545]
</p><p>5 This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. [sent-6, score-0.818]
</p><p>6 We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines. [sent-8, score-0.48]
</p><p>7 Introduction This paper presents a new, efficient algorithm for estimating the camera pose from a single RGB-D image, relative to a known scene (or environment). [sent-10, score-0.532]
</p><p>8 A standard approach for solving the problem is first to find a set of putative correspondences between image pixels and 3D points in the scene, and second to optimize the camera pose to minimize some energy function defined over these correspondences. [sent-12, score-0.702]
</p><p>9 In this work, we first demonstrate how regression forests can be used to predict the correspondences, and further show how to optimize the camera pose efficiently. [sent-13, score-0.825]
</p><p>10 Our main contribution is the scene coordinate regression forest (SCoRe Forest). [sent-14, score-0.942]
</p><p>11 1, the forest is trained to directly predict correspondences from any image pixel to points in the scene’s 3D world coordinate frame. [sent-16, score-0.945]
</p><p>12 The aim is that, in one go, the forest can remove the need for the traditional pipeline of feature detection, description, and matching. [sent-17, score-0.557]
</p><p>13 (Top)A3Drep-  resentation of a scene’s shared world coordinate frame, with overlaid ground truth camera frusta for the images below. [sent-20, score-0.458]
</p><p>14 A scene coordinate regression forest (SCoRe Forest) is trained to infer the  scene coordinates at any image pixel. [sent-22, score-1.159]
</p><p>15 (Bottom) Three test frames: the input RGB and depth images; the ground truth scene coordinate pixel labels; and the inliers inferred by the SCoRe Forest after camera pose optimization. [sent-23, score-1.102]
</p><p>16 For this visualization we show all inlier pixels, but note that the optimization algorithm only actually evaluates the forest at a much sparser set of pixels. [sent-24, score-0.754]
</p><p>17 The depth maps and camera poses are sufficient to compute scene coordinate training labels at every pixel. [sent-28, score-0.716]
</p><p>18 Our second contribution is an efficient test-time camera pose optimization algorithm based on RANSAC; see Fig. [sent-31, score-0.458]
</p><p>19 We optimize an energy function that measures the number of pixels for which the SCoRe Forest predictions agree with a given pose hypothesis. [sent-34, score-0.399]
</p><p>20 Because we trained the forest densely, we are free to test the forest at whichever (valid) image pixels we please. [sent-35, score-1.256]
</p><p>21 Note that the optimization does not require an explicit 3D model of the scene; instead, the forest implicitly encodes the scene structure. [sent-37, score-0.707]
</p><p>22 We validate the proposed camera pose estimation technique on a new dataset of seven varied scenes which will be made available for research purposes. [sent-42, score-0.483]
</p><p>23 Related work Two common approaches for image-based camera relocalization use global image matching and sparse feature matching, though other approaches exist e. [sent-46, score-0.477]
</p><p>24 [17]), an approximate camera pose is hypothesized by computing whole-image similarity measures between a query image and a set of keyframes with known associated keyposes. [sent-51, score-0.46]
</p><p>25 Given the set of best matching images, the final camera pose is computed as a weighted average of the keyposes. [sent-52, score-0.42]
</p><p>26 Given sparse keypoint matches, pose estimation is achieved either via intermediate image retrieval [2, 8, 14] or by directly establishing 2D-3D correspondences for a robust pose optimization [19, 28]. [sent-60, score-0.597]
</p><p>27 [32] use a regression forest to infer correspondences between each depth image pixels and points in a canonical articulated 3D human mesh. [sent-67, score-1.021]
</p><p>28 Scene Coordinate Regression Forests In this section we provide some background on regression forests, detail our new scene coordinate pixel labeling, and discuss how this labeling is used to train a forest for each scene. [sent-70, score-1.021]
</p><p>29 3 will then describe how the camera pose is optimized by evaluating the forest at a sparse set of pixels. [sent-72, score-1.072]
</p><p>30 Regression forests We give a brief introduction to regression forests; for more details, please see [7]. [sent-75, score-0.373]
</p><p>31 We employ a fairly standard regression forest approach, optimizing a reduction-in-spatial-variance objective. [sent-82, score-0.718]
</p><p>32 A regression forest is an ensemble of T decision trees, each consisting of split (internal) and leaf nodes. [sent-83, score-0.79]
</p><p>33 To evaluate a regression tree at a 2D pixel location p in an image, we start at the root node and descend to a leaf by repeatedly evaluating the weak learner:  h(p;θn) = ? [sent-85, score-0.427]
</p><p>34 Each leaf node in a regression tree stores a distribution Pl (m) over a continuous variable m. [sent-94, score-0.382]
</p><p>35 The final prediction of the forest at pixel p is ? [sent-99, score-0.636]
</p><p>36 Image features We investigate three variants of regression forests, each of which uses a different combination of RGB and depth features. [sent-104, score-0.3]
</p><p>37 (3)  Here, δ indicates a 2D offset, D(p) indicates a depth pixel lookup, and I(p, c) indicates an RGB pixel lookup in channel c. [sent-115, score-0.327]
</p><p>38 Each split node in the forest stores a unique set of parameters φn ⊆ {δ1 , δ2 , c1, c2, z}, with z ∈ {depth, da-rgb} indicating the⊆ type of feature ,toz use. [sent-116, score-0.655]
</p><p>39 We assume a reasonable registration of depth and RGB images, such as is provided by standard RGB-D camera APIs. [sent-120, score-0.36]
</p><p>40 However, the registration need not be perfect as the forest will learn some degree of tolerance to misregistration. [sent-121, score-0.591]
</p><p>41 Scene coordinate labels One of the main contributions of this work is the use of scene coordinates to define the labels used to train the regression forest. [sent-124, score-0.509]
</p><p>42 By using scene coordinate labels, the forest will learn to directly predict the position in the scene’s  world space that corresponds to a test pixel. [sent-125, score-0.926]
</p><p>43 camera pose matrices H that encode the 3D rotation and translation from camera space to world space. [sent-129, score-0.712]
</p><p>44 This data could be captured in several ways, for example by tracking from depth camera input [15, 21], or by using dense reconstruction and tracking from RGB input [22]. [sent-130, score-0.43]
</p><p>45 At pixel p, the calibrated depth D(p) allows us to compute the 3D camera space coordinate x. [sent-132, score-0.551]
</p><p>46 Using homogeneous coordinates, this camera position can be transformed into the scene’s world coordinate frame as m = Hx. [sent-133, score-0.448]
</p><p>47 We train the forest using pixels drawn from all training images, so the forest can be applied at any test image pixel. [sent-135, score-1.272]
</p><p>48 In particular, one can evaluate the forest at any sparse set of test pixels. [sent-136, score-0.662]
</p><p>49 If the forest were a perfect predictor, only three pixel predictions would be required to infer the camera pose. [sent-137, score-0.981]
</p><p>50 In practice, the forest instead makes noisy predictions, and so we employ the efficient optimization described in Sec. [sent-138, score-0.595]
</p><p>51 Forest training Given the scene coordinate pixel labeling defined above, we can now grow the regression forest using the standard greedy forest training algorithm [7], summarized next. [sent-142, score-1.672]
</p><p>52 Once the tree has been grown, the final stage of training is to summarize the distribution over m as a set of modes Ml, for each leaf node l. [sent-156, score-0.282]
</p><p>53 Camera  Pose Optimization  The regression forest described in the previous section is capable of associating scene coordinates with any 2D image pixel. [sent-165, score-0.906]
</p><p>54 Middle: Our RANSAC optimization uses a scene coordinate regression forest (SCoRe Forest) to obtain image to scene correspondences. [sent-169, score-1.092]
</p><p>55 The algorithm maintains a set of inlier pixels for each of several camera pose hypotheses. [sent-170, score-0.531]
</p><p>56 Bottom right: The hypothesis with the lowest energy (highest number of inliers) is chosen as the final inferred pose (shown as the blue frustum; the ground truth is shown in red). [sent-171, score-0.446]
</p><p>57 For clarity of exposition, we show all image pixels that are inliers to each hypothesis; in fact our algorithm samples pixels sparsely and does not need to evaluate the SCoRe Forest at every pixel. [sent-172, score-0.296]
</p><p>58 The problem  is cast as the energy minimization  H∗ = argHminE(H)  (6)  over the camera pose matrix H. [sent-174, score-0.493]
</p><p>59 The energy function above thus counts the number of outliers for a given camera hypothesis H. [sent-189, score-0.381]
</p><p>60 Because the forest has been trained to work at any image pixel, we can randomly sample pixels at test time. [sent-191, score-0.699]
</p><p>61 The summation in (7) is thus computed over a subset I all possible image pixels; the larger the of soivzeer o af sthubiss esut bIse oft, athlle p more lues eimfualg tehe p energy Ee can bre t haet ranking pose hypotheses. [sent-193, score-0.272]
</p><p>62 A consequence of this is that the minimization will infer at each pixel which tree in the forest gave the best prediction under a given hypothesis. [sent-195, score-0.719]
</p><p>63 It then randomly samples a new batch of B pixels, evaluates the forest at these pixels, and updates the energies for each hypothesis based on the forest predictions. [sent-200, score-1.285]
</p><p>64 Each remaining hypothesis is then refined [5] based on the set of inliers computed as a by-product of evaluating the energy at Line 9 of Algorithm 1. [sent-202, score-0.271]
</p><p>65 The algorithm as presented evaluates the forest on-the-fly; alternatively the forest could be evaluated at the B log2 Kinit required pixels in advance, though this would require extra  storage. [sent-205, score-1.298]
</p><p>66 The forest is evaluated at three pixels (the minimal number required), and at each of those pixels a random mode m ∈ Mi is sampled. [sent-211, score-0.776]
</p><p>67 These putative correspondences are passed to the Kabsch algorithm [16] (also known as orthogonal Procrustes alignment) which uses a singular value decomposition (SVD) to solve for the camera pose hypothesis with least squared error. [sent-212, score-0.647]
</p><p>68 , K  13: return best pose H1 and energy E1  ×  the means and covariance matrices used by the SVD. [sent-227, score-0.272]
</p><p>69 Note that the refinement step means the final pose can be much more accurate than the individual forest pixel predictions of the inliers. [sent-228, score-0.965]
</p><p>70 All scenes were recorded from a handheld Kinect RGB-D camera at 640 480 resolfruotimon. [sent-234, score-0.284]
</p><p>71 An overview of the scenes and camera tracks are shown in Fig. [sent-237, score-0.323]
</p><p>72 Metrics As our main test metric we report the percentage of test frames for which the inferred camera pose is essentially ‘correct’ . [sent-247, score-0.591]
</p><p>73 We employ a fairly strict definition of correct: the pose must be within 5cm translational error and 5◦ angular error of the ground truth (for comparison, our scenes have size up to 6m3). [sent-248, score-0.304]
</p><p>74 Baselines We compare our approach against two complementary baseline methods: the ‘sparse’ baseline exploits matches between extracted features and a sparse 3D point cloud model, while the ‘tiny-image’ baseline matches downsampled whole images to hypothesize putative camera poses. [sent-252, score-0.766]
</p><p>75 The final camera pose is refined using a gold-standard method computed on all inlier 2D-3D correspondences. [sent-260, score-0.462]
</p><p>76 All training images are stored with their camera poses, after downsampling to 40 30 pixels and applying a Gaussian blur of σ i=n g2. [sent-263, score-0.337]
</p><p>77 The camera pose is finally computed as a weighted average of the poses of the 100 closest matches [11]. [sent-266, score-0.5]
</p><p>78 The tiny-image baseline fails almost completely under our strict metric: the poses of the images in the training set are simply too far from the poses in the test set, and this approach cannot generalize well. [sent-272, score-0.277]
</p><p>79 Of the three forest feature modalities we test, depth-only always performs worst. [sent-278, score-0.557]
</p><p>80 This is to be expected given the large amount of noise (especially holes) in the depth im2The tiny-image baseline does however provide a sensible rough localization, good enough for model-based pose refinement (Sec. [sent-279, score-0.51]
</p><p>81 ctC7k24o158% - oiFnRgream st red camera tracks show the positions of the cameras in the training and test  trained with three different feature  sequences. [sent-294, score-0.38]
</p><p>82 This slightly surprising result may be due to sub-optimal settings of the forest training parameters (see below), but in any case both modalities outperform the baseline in all but one scene. [sent-307, score-0.704]
</p><p>83 4 we see that our inferred camera poses form a remarkably smooth track compared to the sparse baseline. [sent-309, score-0.445]
</p><p>84 Preliminary investigation suggests considerable insensitivity of the pose optimization to changes in the training parameters, though it is reasonable to assume a more thorough optimization of these parameters would improve our accuracy. [sent-332, score-0.353]
</p><p>85 In comparison, the sparse baseline (also not optimized) takes approximately 250ms per frame, and the (deliberately inefficient brute-force) tiny-image baseline takes 500ms-3000ms according to the number of images in the training database. [sent-339, score-0.31]
</p><p>86 Here, we initialize one hypothesis (out of the Kinit) with the camera pose inferred at the previous frame. [sent-344, score-0.594]
</p><p>87 Our algorithm gives remarkably smooth camera tracks from single frames at a time. [sent-349, score-0.29]
</p><p>88 Scene recognition Beyond relocalizing within a particular scene, one might also want to recognize to which scene a particular camera view belongs. [sent-354, score-0.333]
</p><p>89 For a given test frame, our pose optimization algorithm is run, separately for each  scene’s SCoRe Forest. [sent-356, score-0.279]
</p><p>90 An alternative, not investigated here, would be to embed all the scenes in a shared 3D world so that the coordinate pixel labels uniquely identify a point in a particular scene. [sent-363, score-0.366]
</p><p>91 A single forest could thus encapsulate all scenes, and scene recognition would simply be a by-product of pose estimation. [sent-364, score-0.868]
</p><p>92 Model-based pose refinement The results above demonstrate that accurate relocalization is achievable without an explicit 3D model. [sent-368, score-0.433]
</p><p>93 from [15, 21]), then it is possible to refine our inferred camera pose against that model. [sent-371, score-0.507]
</p><p>94 To investigate, we ran an experiment which performs an ICP-based pose refinement [3] starting at our inferred pose. [sent-372, score-0.358]
</p><p>95 add a brute-force ‘ICP Residual’ baseline which, for each test frame, runs ICP starting from each training keyposes (for tractability, a frame becomes a keypose if it is 5cm away from the previous selected keypose), and takes the resulting pose with lowest ICP residual error. [sent-389, score-0.517]
</p><p>96 Our DA-RGB and DA-RGB + D forests outperform all baselines on all  >  scenes, and even give a perfect result for Chess. [sent-390, score-0.297]
</p><p>97 Note that the complete failure of the tiny-image baseline in Table 1 is much improved, suggesting that this baseline approach is only useful when paired with a refinement step. [sent-391, score-0.315]
</p><p>98 Conclusions We have demonstrated how scene coordinate regression forests (SCoRe Forests) can be trained to directly predict correspondences between image pixels and a scene’s world space. [sent-393, score-0.863]
</p><p>99 The comparison on the challenging new 7 Scenes dataset with two baselines indicates that our algorithm achieves state of the art camera relocalization. [sent-396, score-0.306]
</p><p>100 Real-time human pose recognition in parts from a single depth image. [sent-638, score-0.338]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('forest', 0.557), ('camera', 0.221), ('forests', 0.212), ('pose', 0.199), ('relocalization', 0.162), ('regression', 0.161), ('ransac', 0.144), ('depth', 0.139), ('rgb', 0.138), ('kinit', 0.12), ('coordinate', 0.112), ('scene', 0.112), ('inliers', 0.111), ('baseline', 0.1), ('icp', 0.099), ('hypothesis', 0.087), ('inferred', 0.087), ('evaluates', 0.084), ('preemptive', 0.081), ('stairs', 0.081), ('mode', 0.081), ('pixel', 0.079), ('hypotheses', 0.078), ('putative', 0.077), ('energy', 0.073), ('refinement', 0.072), ('chess', 0.072), ('leaf', 0.072), ('world', 0.071), ('pixels', 0.069), ('node', 0.064), ('correspondences', 0.063), ('sparse', 0.063), ('shotton', 0.063), ('scenes', 0.063), ('score', 0.061), ('predictions', 0.058), ('orb', 0.058), ('frusta', 0.054), ('kabsch', 0.054), ('keypose', 0.054), ('layoutcrf', 0.054), ('redkitchen', 0.054), ('baselines', 0.051), ('tree', 0.051), ('hk', 0.05), ('newcombe', 0.048), ('modes', 0.048), ('sparsely', 0.047), ('training', 0.047), ('izadi', 0.045), ('learner', 0.045), ('frame', 0.044), ('poses', 0.044), ('lepetit', 0.044), ('failure', 0.043), ('translational', 0.042), ('coordinates', 0.042), ('test', 0.042), ('inlier', 0.042), ('labels', 0.041), ('hypothesized', 0.04), ('tracks', 0.039), ('criminisi', 0.038), ('trees', 0.038), ('optimization', 0.038), ('sn', 0.037), ('dmax', 0.036), ('matches', 0.036), ('keypoint', 0.035), ('tracking', 0.035), ('localization', 0.035), ('loop', 0.034), ('art', 0.034), ('perfect', 0.034), ('stores', 0.034), ('terminates', 0.034), ('capable', 0.034), ('visualization', 0.033), ('closing', 0.033), ('hodges', 0.033), ('kinectfusion', 0.033), ('molyneaux', 0.033), ('cloud', 0.033), ('infer', 0.032), ('optimized', 0.032), ('davison', 0.032), ('hilliges', 0.032), ('predict', 0.032), ('heads', 0.032), ('office', 0.032), ('nist', 0.031), ('klein', 0.031), ('though', 0.031), ('trained', 0.031), ('pami', 0.031), ('residual', 0.031), ('robotics', 0.03), ('lookup', 0.03), ('remarkably', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999839 <a title="380-tfidf-1" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>2 0.36465213 <a title="380-tfidf-2" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>3 0.25144967 <a title="380-tfidf-3" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>4 0.18656695 <a title="380-tfidf-4" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>Author: Amaury Dame, Victor A. Prisacariu, Carl Y. Ren, Ian Reid</p><p>Abstract: We propose a formulation of monocular SLAM which combines live dense reconstruction with shape priors-based 3D tracking and reconstruction. Current live dense SLAM approaches are limited to the reconstruction of visible surfaces. Moreover, most of them are based on the minimisation of a photo-consistency error, which usually makes them sensitive to specularities. In the 3D pose recovery literature, problems caused by imperfect and ambiguous image information have been dealt with by using prior shape knowledge. At the same time, the success of depth sensors has shown that combining joint image and depth information drastically increases the robustness of the classical monocular 3D tracking and 3D reconstruction approaches. In this work we link dense SLAM to 3D object pose and shape recovery. More specifically, we automatically augment our SLAMsystem with object specific identity, together with 6D pose and additional shape degrees of freedom for the object(s) of known class in the scene, combining im- age data and depth information for the pose and shape recovery. This leads to a system that allows for full scaled 3D reconstruction with the known object(s) segmented from the scene. The segmentation enhances the clarity, accuracy and completeness of the maps built by the dense SLAM system, while the dense 3D data aids the segmentation process, yieldingfaster and more reliable convergence than when using 2D image data alone.</p><p>5 0.18126544 <a title="380-tfidf-5" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>Author: Michele Fenzi, Laura Leal-Taixé, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models, i.e., regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset [17] and the Pointing’04 face dataset [8]. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.</p><p>6 0.17759725 <a title="380-tfidf-6" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>7 0.1763909 <a title="380-tfidf-7" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>8 0.17022353 <a title="380-tfidf-8" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>9 0.15986964 <a title="380-tfidf-9" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>10 0.15517384 <a title="380-tfidf-10" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>11 0.15511984 <a title="380-tfidf-11" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>12 0.15308866 <a title="380-tfidf-12" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>13 0.15040988 <a title="380-tfidf-13" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>14 0.14581124 <a title="380-tfidf-14" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>15 0.13886954 <a title="380-tfidf-15" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>16 0.136897 <a title="380-tfidf-16" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>17 0.13657619 <a title="380-tfidf-17" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>18 0.12868777 <a title="380-tfidf-18" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>19 0.12556379 <a title="380-tfidf-19" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>20 0.12500456 <a title="380-tfidf-20" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.288), (1, 0.135), (2, 0.01), (3, -0.045), (4, -0.003), (5, -0.01), (6, 0.029), (7, 0.073), (8, 0.043), (9, -0.06), (10, -0.102), (11, 0.159), (12, 0.021), (13, 0.167), (14, -0.035), (15, -0.11), (16, -0.105), (17, 0.035), (18, -0.041), (19, -0.082), (20, 0.049), (21, -0.021), (22, -0.115), (23, -0.014), (24, -0.051), (25, 0.036), (26, -0.084), (27, 0.061), (28, 0.061), (29, 0.046), (30, -0.168), (31, 0.051), (32, -0.119), (33, -0.052), (34, 0.025), (35, 0.049), (36, -0.015), (37, -0.083), (38, 0.081), (39, 0.165), (40, -0.078), (41, 0.056), (42, -0.083), (43, -0.017), (44, 0.129), (45, 0.119), (46, 0.034), (47, 0.044), (48, 0.1), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95668906 <a title="380-lsi-1" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>2 0.75707656 <a title="380-lsi-2" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>3 0.71208441 <a title="380-lsi-3" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>Author: Samuel Schulter, Paul Wohlhart, Christian Leistner, Amir Saffari, Peter M. Roth, Horst Bischof</p><p>Abstract: This paper introduces a novel classification method termed Alternating Decision Forests (ADFs), which formulates the training of Random Forests explicitly as a global loss minimization problem. During training, the losses are minimized via keeping an adaptive weight distribution over the training samples, similar to Boosting methods. In order to keep the method as flexible and general as possible, we adopt the principle of employing gradient descent in function space, which allows to minimize arbitrary losses. Contrary to Boosted Trees, in our method the loss minimization is an inherent part of the tree growing process, thus allowing to keep the benefits ofcommon Random Forests, such as, parallel processing. We derive the new classifier and give a discussion and evaluation on standard machine learning data sets. Furthermore, we show how ADFs can be easily integrated into an object detection application. Compared to both, standard Random Forests and Boosted Trees, ADFs give better performance in our experiments, while yielding more compact models in terms of tree depth.</p><p>4 0.61872327 <a title="380-lsi-4" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>Author: Amaury Dame, Victor A. Prisacariu, Carl Y. Ren, Ian Reid</p><p>Abstract: We propose a formulation of monocular SLAM which combines live dense reconstruction with shape priors-based 3D tracking and reconstruction. Current live dense SLAM approaches are limited to the reconstruction of visible surfaces. Moreover, most of them are based on the minimisation of a photo-consistency error, which usually makes them sensitive to specularities. In the 3D pose recovery literature, problems caused by imperfect and ambiguous image information have been dealt with by using prior shape knowledge. At the same time, the success of depth sensors has shown that combining joint image and depth information drastically increases the robustness of the classical monocular 3D tracking and 3D reconstruction approaches. In this work we link dense SLAM to 3D object pose and shape recovery. More specifically, we automatically augment our SLAMsystem with object specific identity, together with 6D pose and additional shape degrees of freedom for the object(s) of known class in the scene, combining im- age data and depth information for the pose and shape recovery. This leads to a system that allows for full scaled 3D reconstruction with the known object(s) segmented from the scene. The segmentation enhances the clarity, accuracy and completeness of the maps built by the dense SLAM system, while the dense 3D data aids the segmentation process, yieldingfaster and more reliable convergence than when using 2D image data alone.</p><p>5 0.60326433 <a title="380-lsi-5" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>Author: Renato F. Salas-Moreno, Richard A. Newcombe, Hauke Strasdat, Paul H.J. Kelly, Andrew J. Davison</p><p>Abstract: We present the major advantages of a new ‘object oriented’ 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, realtime 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph optimisation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruction, but with a huge representation compression. The object graph enables predictions for accurate ICP-based camera to model tracking at each live frame, and efficient active search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relocalisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction.</p><p>6 0.57908279 <a title="380-lsi-6" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>7 0.57477129 <a title="380-lsi-7" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>8 0.57373947 <a title="380-lsi-8" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>9 0.57187539 <a title="380-lsi-9" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>10 0.55013722 <a title="380-lsi-10" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>11 0.5370934 <a title="380-lsi-11" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>12 0.52694589 <a title="380-lsi-12" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>13 0.52457869 <a title="380-lsi-13" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>14 0.5145579 <a title="380-lsi-14" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>15 0.50983864 <a title="380-lsi-15" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>16 0.50972837 <a title="380-lsi-16" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>17 0.5088408 <a title="380-lsi-17" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>18 0.50756186 <a title="380-lsi-18" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>19 0.49067792 <a title="380-lsi-19" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>20 0.48234418 <a title="380-lsi-20" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.173), (16, 0.049), (26, 0.044), (33, 0.287), (67, 0.096), (69, 0.066), (76, 0.015), (87, 0.084), (91, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94752163 <a title="380-lda-1" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>2 0.94228959 <a title="380-lda-2" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>3 0.93886209 <a title="380-lda-3" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>4 0.93882906 <a title="380-lda-4" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>5 0.93808889 <a title="380-lda-5" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>6 0.9370802 <a title="380-lda-6" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>7 0.9357754 <a title="380-lda-7" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>8 0.93381596 <a title="380-lda-8" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>9 0.93374187 <a title="380-lda-9" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>10 0.93332952 <a title="380-lda-10" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>11 0.93288535 <a title="380-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.93253732 <a title="380-lda-12" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>13 0.93239319 <a title="380-lda-13" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>same-paper 14 0.93173659 <a title="380-lda-14" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>15 0.93088126 <a title="380-lda-15" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>16 0.93086088 <a title="380-lda-16" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>17 0.93004131 <a title="380-lda-17" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>18 0.92925769 <a title="380-lda-18" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>19 0.92888057 <a title="380-lda-19" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>20 0.92873412 <a title="380-lda-20" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
