<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>418 cvpr-2013-Submodular Salient Region Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-418" href="#">cvpr2013-418</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>418 cvpr-2013-Submodular Salient Region Detection</h1>
<br/><p>Source: <a title="cvpr-2013-418-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Jiang_Submodular_Salient_Region_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>Reference: <a title="cvpr-2013-418-reference" href="../cvpr2013_reference/cvpr-2013-Submodular_Salient_Region_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The problem of salient region detection is formulated as the well-studied facility location problem from operations research. [sent-4, score-1.027]
</p><p>2 High-level priors are combined with low-level features to detect salient regions. [sent-5, score-0.323]
</p><p>3 Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i. [sent-6, score-0.231]
</p><p>4 , total profits) between the hypothesized salient region centers (i. [sent-8, score-0.519]
</p><p>5 , clients), and penalizes the number of potential salient regions (i. [sent-12, score-0.384]
</p><p>6 The saliency of a selected region is modeled in terms of appearance and spatial location. [sent-16, score-0.511]
</p><p>7 m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches. [sent-21, score-0.311]
</p><p>8 Introduction  Visual saliency modeling is relevant to a variety of computer vision problems including object detection and recognition [29, 26], image editing [13, 4, 6] and image segmentation [16]. [sent-23, score-0.311]
</p><p>9 Most saliency models [2, 20, 4, 6, 8] are based on a contrast prior between salient objects and backgrounds. [sent-24, score-0.648]
</p><p>10 Saliency models map natural images into saliency maps, in which each image element (pixel, superpixel, region) is assigned a saliency strength or probability. [sent-25, score-0.7]
</p><p>11 For example, Figure 1 illustrates saliency detection results using four state-of-art algorithms [2, 6, 4, 26]. [sent-30, score-0.311]
</p><p>12 However, given the ground truth salient regions in Figure 1(b), even for the first simple example, these approaches either fail to separate the object from the background, as in Figures 1(c) and 1(e), or mostly outline the object but miss the interior as in Figure 1(d). [sent-32, score-0.317]
</p><p>13 (a) Input images; (b) Ground truth salient regions; (c)∼(e): Saliency maps using ;[ (2,b )6 ,G r4o] wnidth t cuothnt sraalsiet priors; (f) Saliency map using [26] with a low-rank prior. [sent-35, score-0.337]
</p><p>14 [26, 28] represent an image as a low-rank matrix plus sparse noise, where the background is modeled by the low-rank matrix and the salient regions are indicated by the sparse noise (i. [sent-39, score-0.457]
</p><p>15 For example, the poor saliency detection results in Figure 1(f) using the low-rank prior are due to the cluttered background. [sent-43, score-0.391]
</p><p>16 We present a submodular objective function for effi-  ciently creating saliency maps from natural images; these maps can then be used to detect multiple salient regions within a single image. [sent-44, score-0.767]
</p><p>17 The diminishing return property of submodularity has been successfully applied in various applications including sensor placement [18], facility location [24] and image segmentations [15]. [sent-45, score-0.716]
</p><p>18 Our objective function consists oftwo terms: a similarity term (between the selected centers of salient regions and image elements (superpixels) assigned to that center), and the ‘facility’ costs for the selected region centers. [sent-46, score-0.717]
</p><p>19 The first term encourages the selected centers to represent the region elements well. [sent-47, score-0.351]
</p><p>20 Hence it favors the extraction of high-quality potential salient regions. [sent-48, score-0.353]
</p><p>21 The second term penalizes the number of selected potential salient region centers, so it avoids oversegmentation of salient regions. [sent-49, score-0.757]
</p><p>22 It reduces the redundancy among selected salient region centers because the small gain obtained by splitting a region through the introduction of an extrane222000444311  ous region center is offset by the facility cost. [sent-50, score-1.538]
</p><p>23 This high level prior is integrated with low level feature information into a unified objective function to identify salient regions. [sent-51, score-0.441]
</p><p>24 This is in contrast to previous approaches based on low level features [2, 4] or high level information only [29, 5], or heuristic integration approaches [13, 6] based on weighted averages on the saliency maps from low level features and high level priors. [sent-52, score-0.504]
</p><p>25 In contrast to some approaches [28, 27] which use uniform image patches to represent an image,  our representation is based on super-pixels, which are less likely to cross object boundaries and lead to more accurately segmented salient regions. [sent-53, score-0.257]
</p><p>26 Unlike approaches that identify only one salient region in an image [7], our approach identifies multiple salient regions simultaneously without any strong assumptions about the statistics of the backgrounds [28]. [sent-54, score-0.773]
</p><p>27 The main contributions of our paper are: • Salient region selection is modeled as the facility locSaalti oenn tp rreogbiloenm, s ewlehcictioh nis sso mlvoedde e bleyd dm aasx tihmeiz faincgil a ysu lbo-modular objective function. [sent-55, score-0.749]
</p><p>28 This provides a new perspective using submodularity for salient region detection, and it achieves state-of-art performance on two public saliency detection benchmarks. [sent-56, score-0.788]
</p><p>29 • The similarities between hypothesized region centers aTnhde sthimeiril region eeltewmeeennts h are tfhoersmizueldate redg as a elanbteelr-s ing problem on the vertices of a graph. [sent-57, score-0.428]
</p><p>30 • We naturally integrate high-level priors with low-level saliency rianltlyo a nutneigfireadte f hraigmh-elwevoerlk fporiro srasl iwenitth region dvee-l tection. [sent-60, score-0.514]
</p><p>31 Related Work Existing salient region detection approaches can be roughly divided into two categories: bottom-up and topdown approaches. [sent-63, score-0.423]
</p><p>32 Recently, [26, 28] decompose an image into a low-rank matrix representing the background (low-rank prior) and a sparse noise matrix indicating the salient regions by low-rank matrix recovery. [sent-68, score-0.428]
</p><p>33 [27] proposes to use the boundary prior, which assumes the image boundary is mostly background for saliency detection. [sent-69, score-0.353]
</p><p>34 Top-down approaches make use of high level knowledge about ‘interesting’ objects to identify salient regions [29, 5, 14]. [sent-70, score-0.382]
</p><p>35 [29] learns interesting region features by dictionary learning and then generates the saliency map by modeling spatial consistency via a CRF model. [sent-71, score-0.491]
</p><p>36 [5] proposes a topdown saliency algorithm by selecting discriminant features from a pre-defined filter bank. [sent-72, score-0.34]
</p><p>37 In addition, some approaches integrate multiple saliency maps generated from different features or priors to detect salient regions. [sent-73, score-0.671]
</p><p>38 The saliency maps are combined by weighted averaging, where the weights are predefined [6, 8], learned by a SVM [13] or estimated by a CRF [20]. [sent-74, score-0.348]
</p><p>39 Unlike previous approaches that are purely top-down or bottom-up, we integrate high level priors with low level information into a unified framework, which is graph-based and is optimized in a submodular framework. [sent-75, score-0.234]
</p><p>40 Preliminaries Facility Location: [17, 22] We solve a facility location problem to generate candidate regions for saliency-based segmentation. [sent-77, score-0.693]
</p><p>41 fj is the cost of opening a facility at location j and cij denotes the profit made by satisfying the demand of client iby facility j. [sent-89, score-1.943]
</p><p>42 y˜j = 1 if facility j is open and y˜j = 0 otherwise; ˜x ij = 1 if the demand of client iis satisfied from facility j and ˜x ij = 0 otherwise. [sent-91, score-1.414]
</p><p>43 h client to an open facility to maximize the overall profit. [sent-100, score-0.71]
</p><p>44 Submodular Saliency There are three main steps in our approach: First, a set of potential region centers are extracted from an image. [sent-161, score-0.329]
</p><p>45 They serve as a set of potential facility locations (denoted by J). [sent-162, score-0.697]
</p><p>46 Second, given that set of potential region centers, we identify the final region centers and cluster superpixels into regions by solving the facility location problem. [sent-163, score-1.285]
</p><p>47 Finally, the saliencies ofthe potential salient regions and their constituent superpixels are computed from color and spatial location information. [sent-166, score-0.554]
</p><p>48 Identifying A Set of Potential Region Centers It is computationally too expensive to use the whole set V as the set, J, of potential region centers to identify the final region centers, denoted by A. [sent-183, score-0.492]
</p><p>49 chosen as a region center, the region extracted is more or less the same. [sent-185, score-0.274]
</p><p>50 Extraction of Potential Salient Regions We model the problem of identifying high quality potential salient regions as selecting a subset, A, of J as the final region centers. [sent-192, score-0.521]
</p><p>51 A is regarded as the set of locations for opening facilities, and the similarities between elements of A and superpixels eventually assigned to elements of A as the profits made by satisfying the demand of clients by facilities from A. [sent-193, score-0.654]
</p><p>52 As discussed previously, this problem can be modeled as the facility location problem [22]. [sent-194, score-0.657]
</p><p>53 With the constraint NA = |A| ≤ K, the combinatorial formulation of the facility lo=ca |tAio|n ≤ problem cino m[2b2i]n can biael applied ttioo our problem:  mAaxH(A) =i? [sent-196, score-0.588]
</p><p>54 A ⊆ J ⊆ V, NA ≤ K  (3)  where cij denotes the similarity between a vertex vi (considered as clients) and its potential region center vj (considered as facilities), and the cost fj of facility opening is fixed to The overall profit H : 2J → R on the graph G is a tsoub λm. [sent-200, score-1.549]
</p><p>55 2 The first term encourages the similarity between vi and its assigned region center to have the greatest value. [sent-202, score-0.31]
</p><p>56 The optimization favors region centers that are visually similar to their ‘clients’ . [sent-203, score-0.291]
</p><p>57 It mitigates against fragmentation of visually homogenous regions, since the small gain in visual similarity to marginally ‘productive’ region centers is more than offset by the cost of opening such a facility. [sent-205, score-0.404]
</p><p>58 K is the maximum number of salient regions that the algorithm might identify, and is a parameter specified by the user. [sent-211, score-0.317]
</p><p>59 Generally, fewer than K locations are chosen because the marginal gain does not outweigh the facility cost. [sent-212, score-0.8]
</p><p>60 1 Computation of cij cij serves as the profit made by satisfying the demand of client ifrom a facility at location j ∈ J. [sent-215, score-1.577]
</p><p>61 Iitt sish an input cvoamri-able for the facility location problem. [sent-217, score-0.633]
</p><p>62 Since not all nodes in G should be assigned to any j, we add a background node vg to G with label 0 so vi ∈ U can also be assigned to background. [sent-222, score-0.326]
</p><p>63 Examples of facility location and facility assignment results (clustering) on two synthetic datasets. [sent-273, score-1.269]
</p><p>64 The selected region centers (facility locations) are marked as circles. [sent-274, score-0.301]
</p><p>65 (b)∼(e): a1, a2, a3 are yse l oecctatedio nb as)se adre on athrkeierd marginal gains ri na H(A c∪h {a}) H(A) ainp t uhrreese hiteer asttriuoncstu. [sent-277, score-0.255]
</p><p>66 (f) Facility assignment d re osnul t hs by using ahal grmaionnsic in so Hl(utAio∪n {toa compute cij . [sent-281, score-0.409]
</p><p>67 e (g) Facility assignment −  results by simply using naive weight  wij  as  cij  . [sent-284, score-0.425]
</p><p>68 (a) Input image; (b) Center prior map; (c) Face prior map; (d) Color prior map; (e) Final combined and smoothed prior map. [sent-287, score-0.32]
</p><p>69 Hence the possibility of a potential region center close to A being selected increases during the subsequent iteration of the optimization. [sent-290, score-0.278]
</p><p>70 cij can be computed by finding the harmonic function on the graph G with the labeled nodes L set to nodej with label 1 and the background node with label 0, while the other nodes in G are the unlabeled nodes U. [sent-291, score-0.737]
</p><p>71 cij is the probability that a random walker starting from vi, will reach j before reaching the background node [9, 32]. [sent-293, score-0.416]
</p><p>72 With cgj = 0 and cjj = 1, we can also obtain cij = hU ∈  Ru×1 for ∀i ∈ U. [sent-295, score-0.414]
</p><p>73 cij is fixed during the subsequent optimization o anf (d3∀). [sent-298, score-0.328]
</p><p>74 Here, the detected face regions Λ are assigned higher probabilities to generate the face prior map pf (x) = σ1 , for x ∈ Λ; otherwise pf (x) = 0. [sent-304, score-0.288]
</p><p>75 We introduce an ‘assignment cost’ 1− PH for each superpixel tarnodd incorporate int minentot tchoes computation of cij as follows. [sent-327, score-0.375]
</p><p>76 Given a labeled set L (comprised of a region center node vj and the background node vg), we augment the graph G to include a set of labeled nodes, by attaching a labeled node vqi to each unlabeled node vi (i ∈ U) as its prior. [sent-328, score-0.713]
</p><p>77 l  data points having high gains are aggregated in the area with high prior values; (i) Final selected facility locations with prior. [sent-343, score-0.772]
</p><p>78 uth |A s|a =lien 5t  region; (l) Saliency map without prior; (m) Saliency map with prior; (n) Salient region mask based on the saliency map in (m). [sent-346, score-0.577]
</p><p>79 We can compute cij = hU ∈ Ru×1while cjj = 1 and cgj = 0. [sent-368, score-0.414]
</p><p>80 The region center with the largest marginal gain is the location with the lowest assignment cost. [sent-371, score-0.435]
</p><p>81 Hence this computation of cij encourages the selected facility locations to be close to low cost areas (i. [sent-373, score-1.022]
</p><p>82 Figures 3(e) and 3(h) show the marginal gain for each point in J in the first iteration of the facility location optimization without and with the high level prior. [sent-378, score-0.842]
</p><p>83 After highlevel prior integration, the points with large marginal gains are more concentrated in the perceptually important areas (indicated by high-level prior map) such as the flower and the flower leaf. [sent-379, score-0.27]
</p><p>84 Compared to the selected region centers A without the prior in Figure 3(f), our approach with priors will select most of the potential region centers for A from the high prior areas as shown in Figure 3(i). [sent-380, score-0.856]
</p><p>85 3 Potential Salient Region Extraction Given a set of selected facility locations A, let the current maximal profit from vi be ρicur = maxj∈A cij, and the facility assignment for vi be xicur = arg maxj∈A cij . [sent-383, score-1.99]
</p><p>86 Hence, we ccolursrteesrp tohned image eeplesm 1e0nt −s t 1ha4t nsh Aarelg tohrei same facility elo, cwaetion as the most profitable to obtain potential salient regions  {ri}i=1. [sent-389, score-0.972]
</p><p>87 Figure 2 show two examples of facility location and facility assignment results (i. [sent-393, score-1.269]
</p><p>88 The results in Figure 2(f) using a harmonic function to compute cij are better than the results in Figure 2(g) that simply uses the edge weight wij as cij . [sent-396, score-0.799]
</p><p>89 The reason is that harmonic solution for cij enforces that nearby points have similar harmonic function values; this better models the geometry of the data induced by the graph structure (edges and weights W). [sent-397, score-0.549]
</p><p>90 Figures 3(g) and 3(j) show the region extraction results for the two sets of selected region centers A shown in Figure 3(f) and Figure 3(i), respectively. [sent-402, score-0.438]
</p><p>91 |A| , we next compute the saliency o efx ri icnt tnegrm {sr of} its color and spatial information. [sent-422, score-0.434]
</p><p>92 The color saliency of ri is defined as: fc(ri) = ? [sent-425, score-0.434]
</p><p>93 A region which has a wider spatial distribution is typically less salient than regions which have small spatial spread [20, 8]. [sent-430, score-0.454]
</p><p>94 The spatial saliency of ri is computed as  fs(ri) = 1  −maxVi (Vri) (ri). [sent-431, score-0.409]
</p><p>95 ances of superpixels from ri to the spatial mean μk of region rk . [sent-434, score-0.374]
</p><p>96 After fc and fs are maximum normalized to [0, 1], the saliency of ri is computed as: f(ri) = fc(ri)fs (ri). [sent-436, score-0.468]
</p><p>97 We generate the final saliency map S by weighted averaging over superpixels, nwahle sarlei etnhec weights are computed by pair-wise feature distances between superpixels using a Gaussian kernel to enforce that similar superpixels should have similar visual saliency. [sent-437, score-0.554]
</p><p>98 Figures 3(l) and 3(m) present the saliency maps using our approach without and with high-level priors, respectively. [sent-438, score-0.348]
</p><p>99 Compared to the ground truth region in Figure 3(k), the saliency maps with priors are better than the result without priors. [sent-439, score-0.551]
</p><p>100 (a) Input images; (b) Ground truth salient regions; (c) High-level prior map; (d) Saliency map without high level prior; (e) Saliency map with  high level prior; (f) Salient Region extraction based on (e) by simple thresholding. [sent-443, score-0.501]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facility', 0.588), ('cij', 0.328), ('saliency', 0.311), ('salient', 0.257), ('region', 0.137), ('icur', 0.129), ('centers', 0.125), ('profit', 0.115), ('clients', 0.108), ('superpixels', 0.1), ('ri', 0.098), ('harmonic', 0.094), ('facilities', 0.091), ('marginal', 0.087), ('wuu', 0.086), ('xicur', 0.086), ('submodularity', 0.083), ('gain', 0.083), ('hu', 0.082), ('prior', 0.08), ('vi', 0.078), ('client', 0.076), ('potential', 0.067), ('priors', 0.066), ('submodular', 0.065), ('iuu', 0.065), ('puu', 0.065), ('regions', 0.06), ('opening', 0.059), ('demand', 0.05), ('maxj', 0.05), ('wij', 0.049), ('hl', 0.048), ('assignment', 0.048), ('cia', 0.048), ('yq', 0.048), ('fj', 0.047), ('superpixel', 0.047), ('na', 0.047), ('satisfying', 0.047), ('node', 0.046), ('open', 0.046), ('vg', 0.046), ('du', 0.046), ('location', 0.045), ('nodes', 0.044), ('map', 0.043), ('centrality', 0.043), ('cgj', 0.043), ('cjj', 0.043), ('jhe', 0.043), ('profits', 0.043), ('wul', 0.043), ('locations', 0.042), ('background', 0.042), ('hb', 0.039), ('uu', 0.039), ('selected', 0.039), ('rk', 0.039), ('level', 0.039), ('vqi', 0.038), ('vj', 0.038), ('maps', 0.037), ('backgrounds', 0.036), ('xc', 0.035), ('assigned', 0.035), ('center', 0.035), ('pf', 0.035), ('hq', 0.033), ('labeled', 0.033), ('ij', 0.033), ('graph', 0.033), ('hs', 0.033), ('xl', 0.032), ('lazy', 0.032), ('minx', 0.032), ('fc', 0.031), ('yl', 0.029), ('topdown', 0.029), ('oiofn', 0.029), ('similarities', 0.029), ('favors', 0.029), ('unlabeled', 0.029), ('indicated', 0.028), ('fs', 0.028), ('greedy', 0.026), ('identify', 0.026), ('purely', 0.025), ('comprised', 0.025), ('figures', 0.025), ('encourages', 0.025), ('elements', 0.025), ('color', 0.025), ('dth', 0.024), ('vertex', 0.024), ('modeled', 0.024), ('ru', 0.024), ('gains', 0.023), ('eh', 0.023), ('matrix', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000017 <a title="418-tfidf-1" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>2 0.36559552 <a title="418-tfidf-2" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>3 0.33659682 <a title="418-tfidf-3" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>4 0.33091545 <a title="418-tfidf-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.30314684 <a title="418-tfidf-5" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>6 0.27578643 <a title="418-tfidf-6" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>7 0.27168509 <a title="418-tfidf-7" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>8 0.20988998 <a title="418-tfidf-8" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>9 0.17766005 <a title="418-tfidf-9" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>10 0.17103313 <a title="418-tfidf-10" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>11 0.16324389 <a title="418-tfidf-11" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>12 0.12044604 <a title="418-tfidf-12" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>13 0.10684686 <a title="418-tfidf-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.10076997 <a title="418-tfidf-14" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>15 0.094545834 <a title="418-tfidf-15" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>16 0.093731649 <a title="418-tfidf-16" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>17 0.090751044 <a title="418-tfidf-17" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>18 0.083489269 <a title="418-tfidf-18" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>19 0.079414375 <a title="418-tfidf-19" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>20 0.071065091 <a title="418-tfidf-20" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, -0.123), (2, 0.352), (3, 0.196), (4, -0.032), (5, -0.019), (6, 0.01), (7, -0.043), (8, 0.001), (9, 0.016), (10, 0.076), (11, 0.013), (12, -0.046), (13, 0.014), (14, -0.034), (15, 0.022), (16, 0.014), (17, 0.007), (18, -0.008), (19, 0.026), (20, -0.0), (21, 0.012), (22, -0.086), (23, 0.033), (24, 0.007), (25, 0.019), (26, -0.041), (27, -0.034), (28, -0.007), (29, 0.044), (30, 0.032), (31, 0.06), (32, 0.043), (33, 0.039), (34, -0.034), (35, 0.027), (36, -0.024), (37, -0.017), (38, 0.083), (39, -0.047), (40, 0.047), (41, -0.032), (42, 0.03), (43, -0.023), (44, 0.008), (45, 0.007), (46, -0.04), (47, 0.026), (48, -0.028), (49, -0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9413482 <a title="418-lsi-1" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>2 0.88843089 <a title="418-lsi-2" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>3 0.8736372 <a title="418-lsi-3" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>Author: Long Mai, Yuzhen Niu, Feng Liu</p><p>Abstract: A variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First, different methods perform differently in saliency analysis. Second, the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neighboringpixels. To account for the dependence of aggregation on an individual image, our approach selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods.</p><p>4 0.87285626 <a title="418-lsi-4" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>5 0.85352737 <a title="418-lsi-5" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>6 0.84820598 <a title="418-lsi-6" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>7 0.79697114 <a title="418-lsi-7" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>8 0.71183443 <a title="418-lsi-8" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>9 0.67553073 <a title="418-lsi-9" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>10 0.56605393 <a title="418-lsi-10" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>11 0.53448534 <a title="418-lsi-11" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>12 0.51185203 <a title="418-lsi-12" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>13 0.41952288 <a title="418-lsi-13" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>14 0.41061333 <a title="418-lsi-14" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>15 0.3875173 <a title="418-lsi-15" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>16 0.37239781 <a title="418-lsi-16" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>17 0.36344206 <a title="418-lsi-17" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>18 0.34745857 <a title="418-lsi-18" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>19 0.34606534 <a title="418-lsi-19" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>20 0.34089953 <a title="418-lsi-20" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.095), (16, 0.031), (26, 0.047), (29, 0.269), (33, 0.244), (67, 0.083), (69, 0.032), (87, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84384954 <a title="418-lda-1" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>Author: Rui Li, Edward H. Adelson</p><p>Abstract: Sensing surface textures by touch is a valuable capability for robots. Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface texxtures. The GelSight sensor, when pressed against a surfacce, delivers a height map. This can be treated as an image, aand processed using the tools of visual texture analysis. WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. Our system can correctly categorize materials fromm this database with high accuracy. This suggests that the GGelSight sensor can be useful for material recognition by roobots.</p><p>2 0.83981949 <a title="418-lda-2" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>same-paper 3 0.81462765 <a title="418-lda-3" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>4 0.80367196 <a title="418-lda-4" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Amir Averbuch, Shai Dekel</p><p>Abstract: One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms.</p><p>5 0.77120119 <a title="418-lda-5" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>6 0.76599443 <a title="418-lda-6" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>7 0.73081797 <a title="418-lda-7" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>8 0.72922611 <a title="418-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.72634822 <a title="418-lda-9" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>10 0.72610867 <a title="418-lda-10" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>11 0.72606587 <a title="418-lda-11" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>12 0.72605824 <a title="418-lda-12" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>13 0.72579074 <a title="418-lda-13" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>14 0.72534478 <a title="418-lda-14" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>15 0.72510934 <a title="418-lda-15" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>16 0.7247858 <a title="418-lda-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.72476381 <a title="418-lda-17" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>18 0.7246471 <a title="418-lda-18" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>19 0.72396183 <a title="418-lda-19" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>20 0.72360349 <a title="418-lda-20" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
