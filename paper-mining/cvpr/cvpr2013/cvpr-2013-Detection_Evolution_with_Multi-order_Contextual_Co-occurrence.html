<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-122" href="#">cvpr2013-122</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</h1>
<br/><p>Source: <a title="cvpr-2013-122-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Detection_Evolution_with_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Guang Chen, Yuanyuan Ding, Jing Xiao, Tony X. Han</p><p>Abstract: Context has been playing an increasingly important role to improve the object detection performance. In this paper we propose an effective representation, Multi-Order Contextual co-Occurrence (MOCO), to implicitly model the high level context using solely detection responses from a baseline object detector. The so-called (1st-order) context feature is computed as a set of randomized binary comparisons on the response map of the baseline object detector. The statistics of the 1st-order binary context features are further calculated to construct a high order co-occurrence descriptor. Combining the MOCO feature with the original image feature, we can evolve the baseline object detector to a stronger context aware detector. With the updated detector, we can continue the evolution till the contextual improvements saturate. Using the successful deformable-partmodel detector [13] as the baseline detector, we test the proposed MOCO evolution framework on the PASCAL VOC 2007 dataset [8] and Caltech pedestrian dataset [7]: The proposed MOCO detector outperforms all known state-ofthe-art approaches, contextually boosting deformable part models (ver.5) [13] by 3.3% in mean average precision on the PASCAL 2007 dataset. For the Caltech pedestrian dataset, our method further reduces the log-average miss rate from 48% to 46% and the miss rate at 1 FPPI from 25% to 23%, compared with the best prior art [6].</p><p>Reference: <a title="cvpr-2013-122-reference" href="../cvpr2013_reference/cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper we propose an effective representation, Multi-Order Contextual co-Occurrence (MOCO), to implicitly model the high level context using solely detection responses from a baseline object detector. [sent-11, score-0.582]
</p><p>2 The so-called (1st-order) context feature is computed as a set of randomized binary comparisons on the response map of the baseline object detector. [sent-12, score-0.683]
</p><p>3 The statistics of the 1st-order binary context features are further calculated to construct a high order co-occurrence descriptor. [sent-13, score-0.402]
</p><p>4 Combining the MOCO feature with the original image feature, we can evolve the baseline object detector to a stronger context aware detector. [sent-14, score-0.597]
</p><p>5 With the updated detector, we can continue the evolution till the contextual improvements saturate. [sent-15, score-0.532]
</p><p>6 For the Caltech pedestrian dataset, our method further reduces the log-average miss rate from 48% to 46% and the miss rate at 1 FPPI from 25% to 23%, compared with the best prior art [6]. [sent-19, score-0.33]
</p><p>7 The framework evolves the detector using high-order context till the convergence. [sent-24, score-0.516]
</p><p>8 At each iteration, response map and 0th-order context is computed using the initial baseline detector (for the 1st iteration) or the evolved detector from the prior iteration (for later iterations). [sent-25, score-0.957]
</p><p>9 Then the 0th-order context is used for computing the 1st-order context, upon which high order co-occurrence descriptors are computed. [sent-26, score-0.335]
</p><p>10 Finally context in all orders are combined to train a evolving detector. [sent-27, score-0.408]
</p><p>11 The evolution eliminates many false positives using implicit contextual information, and fortifies the true detections. [sent-29, score-0.495]
</p><p>12 Such methods extract image features in each scan window and classify the features to determine the confidence of the presence of the target object [25, 32, 16]. [sent-31, score-0.274]
</p><p>13 Naturally, to improve detection accuracy, context in the neighborhood of each scan window can provide rich information and should be explored. [sent-34, score-0.513]
</p><p>14 For example, a scanning window in a pathway region is more likely to be a true detection of human than the one inside a water region. [sent-35, score-0.243]
</p><p>15 In fact, there have been some efforts on utilizing contextual information for object detection and a variety of valuable approaches have been proposed [14, 27, 28]. [sent-36, score-0.424]
</p><p>16 High level image contexts such as semantic context [4], image statistics [27], and 3D geometric context [15], are used as well as low level image contexts, including local pixel context [5] and shape context [23]. [sent-37, score-1.168]
</p><p>17 Besides utilizing context information from the original image directly, another line of works including Spatial Boost [1], Auto-Context [29], and the extensions ele-  gantly integrate the classifier responses from nearby background pixels to help determine the target pixels of interest. [sent-38, score-0.443]
</p><p>18 Inspired by these prior arts, Contextual Boost [6] was proposed to extract multi-scale contextual cues from the detector response map to boost the detection performance. [sent-40, score-0.872]
</p><p>19 In this paper we aim at developing an effective and generic approach to utilize contextual information without resorting to the multiple object detectors. [sent-44, score-0.331]
</p><p>20 The rationale is that, even though there is only one classifier/detector, higher order contextual information such as the co-occurrence of objects of different categories can still be implicitly and effectively used by carefully organizing the responses from a single object detector. [sent-45, score-0.541]
</p><p>21 However, the difference among the responses of the single classifier on different object regions implicitly conveys such contex-  tual information. [sent-47, score-0.228]
</p><p>22 The responses of a pedestrian detector to various object regions such as the sky, streets, and trees, may vary greatly, but a homogeneous region of the response map corresponds to a region with semantic similarity. [sent-50, score-0.635]
</p><p>23 This reasoning hints a possibility to encode higher order contextual information with single object detection response. [sent-53, score-0.447]
</p><p>24 Therefore, if we treat the single classifier response map as an “image”, we can extract descriptors to represent high order contextual information. [sent-54, score-0.618]
</p><p>25 Our multi-order context representation is inspired by the recent success of randomized binary image descriptors [22, 3, 24]. [sent-55, score-0.409]
</p><p>26 First we propose a series of binary features where each bit encodes the relationship of classification response values for a pair of pixels. [sent-56, score-0.241]
</p><p>27 The difference of detector responses at different pixels implicitly captures the contextual co-occurrence patterns pertinent to detection improvements. [sent-57, score-0.666]
</p><p>28 Accordingly we further propose a novel high order contextual descriptor based on the binary pattern of comparisons. [sent-59, score-0.447]
</p><p>29 Our high order contextual descriptor captures the co-occurrence of binary contextual features based on their statistics in the local neighborhood. [sent-60, score-0.792]
</p><p>30 The context features at all different orders are complementary to each other and are therefore combined together to form a multi-order context representation. [sent-61, score-0.671]
</p><p>31 Finally the proposed multi-order context representations  are integrated into an iterative classification framework, where the classifier response map from the previous iteration is further explored to supply more contextual constraints for the current iteration. [sent-62, score-0.88]
</p><p>32 This process is a straightforward extension of our contextual boost algorithm in [6]. [sent-63, score-0.395]
</p><p>33 Similar to [6], since the multi-order contextual feature encodes the contextual relationships between neighborhood image regions, through iterations it naturally evolves to cover greater neighborhoods and incorporates more global contextual information into the classification process. [sent-64, score-1.024]
</p><p>34 As a result our framework effectively enables the detector evolving to be stronger across iterations. [sent-65, score-0.263]
</p><p>35 On the Caltech dataset [7], compared with the best prior art achieved by contextual boost [6], our method further reduces the logaverage miss rate from 48% to 46% and the miss rate at 1 FPPI from 25% to 23%. [sent-72, score-0.627]
</p><p>36 (2) summarizes the flow chart for constructing the  multi-order context representation from an image. [sent-75, score-0.271]
</p><p>37 The detection response maps for each scale are smoothed as in Sec. [sent-78, score-0.291]
</p><p>38 111777999977  (0th-order)  and its position (red solid area) in the smoothed detection responses map. [sent-81, score-0.237]
</p><p>39 Finally we compute the binary comparison based context features, upon which we further extract high order co-occurrence descriptor detailed in Sec. [sent-85, score-0.455]
</p><p>40 We define the context region in terms of spatial and scale for each candidate location. [sent-89, score-0.294]
</p><p>41 We then compute a series of binary features using randomized comparison of detector responses within the context region, as detailed in Sec. [sent-90, score-0.648]
</p><p>42 Context Basis (0th-order) Intuitively, the appearance of the original image patch containing the neighborhood of target objects provides important contextual cues. [sent-99, score-0.385]
</p><p>43 However it is difficult to model this kind of context in original image because the neighborhood around target objects may vary dramatically in differ-  . [sent-100, score-0.362]
</p><p>44 A logical approach to this problem is: firstly convolve the original image with a particular filter to reduce the diversity of the neighborhood of a true target object as foreground with various backgrounds; then extract context feature from the filtered image. [sent-102, score-0.468]
</p><p>45 For object detection tasks, we prefer such a filter to be detector driven. [sent-103, score-0.318]
</p><p>46 (1) that the positive responses cluster densely around humans but occur sparsely in the background, we simply take the object detector as this specific filter and directly extract context information from the classification response map, denoted as M. [sent-105, score-0.783]
</p><p>47 Such a response image thus conveys context information, which we denote as 0th-order context. [sent-121, score-0.462]
</p><p>48 The context structure Ω(P˙) around in the spatial and scale space is defined as:  P˙;  P˙,  P˙,  P˙  Ω(P˙;W,H,L) =? [sent-128, score-0.271]
</p><p>49 For example, (1, 1, 1) means the context structure is a 3 F3 ×r e3x caumbpicle region. [sent-137, score-0.271]
</p><p>50 Binary Pattern of Comparisons (1st-order) Given the 0th-order context structure, we propose to use comparison based binary features to incorporate the cooccurrence of different objects. [sent-140, score-0.386]
</p><p>51 Although we only have a single object detector, the response values at different locations indicate the confidences of the target object existing. [sent-141, score-0.302]
</p><p>52 Therefore, each binary comparison encodes the contextual information of whether one location is more likely to contain the target object than the other. [sent-142, score-0.433]
</p><p>53 1 Comparison of Response Values Specifically, we define the binary comparison  τ  in the 0th-  ××  order context structure  Ω(P˙)  of size W  H  τ(s;a,b) :=? [sent-145, score-0.351]
</p><p>54 Complementarily they provide rich context cues and are combined into the MultiOrder Contextual co-Occurrence (MOCO) descriptor, fc = [fn, fp] . [sent-168, score-0.321]
</p><p>55 Detection Evolution To effectively use the MOCO descriptor for object detection, we propose an iterative framework that allows the detector to evolve and achieve better accuracy. [sent-170, score-0.348]
</p><p>56 Such a concept of detection “evolution” had been successfully used for pedestrian detection in Contextual Boost [6]. [sent-171, score-0.284]
</p><p>57 In this  paper, we straightforwardly extend the MOCO based evolution framework to integrate with deformable-part models [10, 13] for general object detection tasks. [sent-172, score-0.331]
</p><p>58 Feature Selection Our detector uses the MOCO descriptor together with the non-context image features extracted in each scan window in the final classification process. [sent-175, score-0.355]
</p><p>59 General Evolution Algorithm The iterative process ofthe detector evolution framework is similar to Contextual Boost [6]. [sent-187, score-0.351]
</p><p>60 Given an initial baseline  detector, the iteration procedure for training a new evolving detector is as follows. [sent-188, score-0.312]
</p><p>61 First, the baseline detector is used to calculate the response maps. [sent-189, score-0.357]
</p><p>62 Finally, the selected features are fed into a general classification algorithm to construct a new detector, which will serve as the new baseline detector for the next iteration. [sent-193, score-0.231]
</p><p>63 As our MOCO is defined in a context region, the iteration will automatically propagate context cues to larger and larger regions. [sent-194, score-0.592]
</p><p>64 As a result, more and more context will be incorporated through the iterations, and the evolved detectors can yield better performance. [sent-195, score-0.402]
</p><p>65 In the testing stage, the same evolution procedure is applied using the learned detectors respectively. [sent-197, score-0.251]
</p><p>66 1  where sr is the detection score of the root filter, spi and di respectively represent the detection score and deformation cost of the i-th part filter, and Np is the number of part filters. [sent-205, score-0.358]
</p><p>67 From the viewpoint of context, the deformable-partmodel essentially exploits the intra context inside the object region, e. [sent-207, score-0.357]
</p><p>68 Therefore it exploits the inter context around the object region. [sent-211, score-0.333]
</p><p>69 Clearly these two kinds of context are exclusive and complementary to each other. [sent-212, score-0.294]
</p><p>70 This encourages us to combine them together to provide more comprehensive contextual constraints. [sent-213, score-0.294]
</p><p>71 (6) consists of both the final detection response sf and the detection responses spi from the Np part filters. [sent-215, score-0.577]
</p><p>72 Since each response s corresponds to a response map, we calculate the MOCO descriptors using each of the response maps. [sent-216, score-0.506]
</p><p>73 Furthermore, to effectively evolve the baseline deformablepart-model detector using the calculated MOCO, we apply the iterative framework not only on the root filter but also  on part filters and detectors for every component. [sent-219, score-0.482]
</p><p>74 Then we use the latent-SVM to fuse the Nc components and retrain an evolved detector for the next iteration. [sent-228, score-0.231]
</p><p>75 Experiments and Discussion We have conducted extensive experiments to evaluate the proposed MOCO and the detection evolution framework. [sent-232, score-0.294]
</p><p>76 First, to demonstrate the advantage of the MOCO, we compare the performance achieved by using different orders of context information. [sent-235, score-0.322]
</p><p>77 Second, we compare the performance at different iterations as the detector evolves to show that the detectors quickly converge in about 2∼3 iterations. [sent-243, score-0.296]
</p><p>78 Two important parameters that directly affect the computation of context descriptors are the size of Ωp and the number n of binary comparisons. [sent-255, score-0.369]
</p><p>79 Since  Figure 4: Mean AP (mAP) Varies for Different Parameters: the size W H L of context structure Ω(P˙) and the number n of binary com-  ××  parison te ×sts L. [sent-256, score-0.328]
</p><p>80 Only 1st-order context features and the image features is used for evaluation. [sent-259, score-0.329]
</p><p>81 2, we choose type iii of Gaussian sampling for constructing the 1st-order context descriptor. [sent-273, score-0.271]
</p><p>82 The most important parameter for computing high order context descriptor is the dimension m of the histogram. [sent-283, score-0.367]
</p><p>83 Since the high order context descriptor fp is complementary to the 1st-order context feature fn, they are combined when evaluating the detection performance. [sent-284, score-0.823]
</p><p>84 The high order context descriptor together with 1st-order context feature and the image features are used. [sent-292, score-0.667]
</p><p>85 7F L4B5P  Table 2: Mean AP (mAP) varies with the combination of different order context feature, where 0th , 1st , H respectively refers to 0th, 1st and high order descriptors. [sent-300, score-0.393]
</p><p>86 We also compared with SURF [2] or LBP [33] extracted on each level of context structure Ω(P˙). [sent-301, score-0.271]
</p><p>87 7  Table 3 : Mean AP (mAP) varies with respect to the proposed detection evolution algorithm, where 0-iteration in the left refers to the baseline without detection evolution. [sent-309, score-0.515]
</p><p>88 To show that different orders of context provide complimentary constraints for object detection, we compared the detection accuracy using different combinations of the multi-order context descriptors. [sent-312, score-0.723]
</p><p>89 (2), clearly the MOCO descriptor that combines all orders of context achieves the best detection performance. [sent-315, score-0.488]
</p><p>90 Another way of exploring the 1st-order context is to extract the gradientbased features such as SURF [2] or LBP [33] directly on each scale of the context structure Ω(P˙). [sent-317, score-0.602]
</p><p>91 This means that the context across larger spatial neighborhood or different scales can be more effective than the context conveyed by local gradients between adjacent positions. [sent-320, score-0.588]
</p><p>92 Detector Evolution Using the best parameters for the MOCO descriptor ob-  tained using the “train” and “val” datasets, we evaluate the detector evolution process across iterations. [sent-323, score-0.424]
</p><p>93 To better show the trend of the detector evolution process, we keep it running for 6 iterations. [sent-330, score-0.351]
</p><p>94 (6): the best reported log-average miss rate is 48% [6], while our algorithm further lowers the miss rate to 46%. [sent-353, score-0.232]
</p><p>95 Processing Speed Our detection evolution framework needs to evaluate each test image Nd times, where Nd is the number of evolved detectors. [sent-357, score-0.375]
</p><p>96 Conclusion In this paper we have proposed a novel multi-order context representation that effectively exploits co-occurrence contexts of different objects, denoted as MOCO, even though we only use detectors for a single object. [sent-387, score-0.435]
</p><p>97 We preprocess the detector response map and extract the 1st-order context features based on randomized binary comparison and further develop a high order co-occurrence descriptor based on the 1st-order context. [sent-388, score-0.877]
</p><p>98 Furthermore, we have proposed to combine our multi-order context representation with the recently proposed deformable part models [ 13] to supply a comprehensive coverage over both inter-contexts among objects and inner-context inside the target object region. [sent-390, score-0.486]
</p><p>99 As the future work, we plan to further extend our MOCO to temporal context from videos and contexts from multiple object detectors or multi-class problems. [sent-392, score-0.42]
</p><p>100 Proximity distribution kernels for geometric context in category recognition. [sent-528, score-0.271]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('moco', 0.677), ('contextual', 0.294), ('context', 0.271), ('evolution', 0.201), ('response', 0.155), ('detector', 0.15), ('boost', 0.101), ('responses', 0.101), ('pedestrian', 0.098), ('detection', 0.093), ('caltech', 0.085), ('evolved', 0.081), ('miss', 0.08), ('spi', 0.078), ('pascal', 0.076), ('descriptor', 0.073), ('strain', 0.067), ('contexts', 0.062), ('evolve', 0.061), ('evolving', 0.06), ('scanning', 0.059), ('scan', 0.059), ('evolves', 0.058), ('binary', 0.057), ('arts', 0.056), ('baseline', 0.052), ('ap', 0.052), ('orders', 0.051), ('iteration', 0.05), ('epson', 0.05), ('fhog', 0.05), ('gkl', 0.05), ('voc', 0.05), ('detectors', 0.05), ('fppi', 0.048), ('map', 0.048), ('deformable', 0.048), ('varies', 0.046), ('neighborhood', 0.046), ('target', 0.045), ('window', 0.044), ('root', 0.044), ('fp', 0.043), ('smoothed', 0.043), ('descriptors', 0.041), ('randomized', 0.04), ('np', 0.039), ('filter', 0.038), ('iterations', 0.038), ('till', 0.037), ('object', 0.037), ('stops', 0.036), ('rate', 0.036), ('surf', 0.036), ('conveys', 0.036), ('supply', 0.036), ('filters', 0.035), ('calonder', 0.033), ('jose', 0.033), ('lbp', 0.033), ('sf', 0.032), ('viola', 0.032), ('fn', 0.032), ('extract', 0.031), ('categories', 0.031), ('refers', 0.03), ('accordingly', 0.029), ('val', 0.029), ('bootstrapping', 0.029), ('features', 0.029), ('cooccurrence', 0.029), ('hm', 0.029), ('implicitly', 0.028), ('hj', 0.028), ('confidences', 0.028), ('lepetit', 0.027), ('performances', 0.027), ('effectively', 0.027), ('classifier', 0.026), ('stronger', 0.026), ('boosting', 0.026), ('combined', 0.026), ('part', 0.025), ('exploits', 0.025), ('converges', 0.025), ('varma', 0.025), ('jones', 0.025), ('ding', 0.025), ('inside', 0.024), ('fc', 0.024), ('confirms', 0.024), ('order', 0.023), ('windows', 0.023), ('region', 0.023), ('san', 0.023), ('comparisons', 0.023), ('complementary', 0.023), ('statistics', 0.022), ('rvoamlue', 0.022), ('thrown', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="122-tfidf-1" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>Author: Guang Chen, Yuanyuan Ding, Jing Xiao, Tony X. Han</p><p>Abstract: Context has been playing an increasingly important role to improve the object detection performance. In this paper we propose an effective representation, Multi-Order Contextual co-Occurrence (MOCO), to implicitly model the high level context using solely detection responses from a baseline object detector. The so-called (1st-order) context feature is computed as a set of randomized binary comparisons on the response map of the baseline object detector. The statistics of the 1st-order binary context features are further calculated to construct a high order co-occurrence descriptor. Combining the MOCO feature with the original image feature, we can evolve the baseline object detector to a stronger context aware detector. With the updated detector, we can continue the evolution till the contextual improvements saturate. Using the successful deformable-partmodel detector [13] as the baseline detector, we test the proposed MOCO evolution framework on the PASCAL VOC 2007 dataset [8] and Caltech pedestrian dataset [7]: The proposed MOCO detector outperforms all known state-ofthe-art approaches, contextually boosting deformable part models (ver.5) [13] by 3.3% in mean average precision on the PASCAL 2007 dataset. For the Caltech pedestrian dataset, our method further reduces the log-average miss rate from 48% to 46% and the miss rate at 1 FPPI from 25% to 23%, compared with the best prior art [6].</p><p>2 0.20923308 <a title="122-tfidf-2" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>3 0.16913092 <a title="122-tfidf-3" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>Author: Junjie Yan, Xucong Zhang, Zhen Lei, Shengcai Liao, Stan Z. Li</p><p>Abstract: The serious performance decline with decreasing resolution is the major bottleneck for current pedestrian detection techniques [14, 23]. In this paper, we take pedestrian detection in different resolutions as different but related problems, and propose a Multi-Task model to jointly consider their commonness and differences. The model contains resolution aware transformations to map pedestrians in different resolutions to a common space, where a shared detector is constructed to distinguish pedestrians from background. For model learning, we present a coordinate descent procedure to learn the resolution aware transformations and deformable part model (DPM) based detector iteratively. In traffic scenes, there are many false positives located around vehicles, therefore, we further build a context model to suppress them according to the pedestrian-vehicle relationship. The context model can be learned automatically even when the vehicle annotations are not available. Our method reduces the mean miss rate to 60% for pedestrians taller than 30 pixels on the Caltech Pedestrian Benchmark, which noticeably outperforms previous state-of-the-art (71%).</p><p>4 0.11671659 <a title="122-tfidf-4" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>Author: Pramod Sharma, Ram Nevatia</p><p>Abstract: In this work, we present a novel and efficient detector adaptation method which improves the performance of an offline trained classifier (baseline classifier) by adapting it to new test datasets. We address two critical aspects of adaptation methods: generalizability and computational efficiency. We propose an adaptation method, which can be applied to various baseline classifiers and is computationally efficient also. For a given test video, we collect online samples in an unsupervised manner and train a randomfern adaptive classifier . The adaptive classifier improves precision of the baseline classifier by validating the obtained detection responses from baseline classifier as correct detections or false alarms. Experiments demonstrate generalizability, computational efficiency and effectiveness of our method, as we compare our method with state of the art approaches for the problem of human detection and show good performance with high computational efficiency on two different baseline classifiers.</p><p>5 0.11332169 <a title="122-tfidf-5" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>Author: Thomas Dean, Mark A. Ruzon, Mark Segal, Jonathon Shlens, Sudheendra Vijayanarasimhan, Jay Yagnik</p><p>Abstract: Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object’s appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times— four orders of magnitude— when compared withperforming the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.</p><p>6 0.11166264 <a title="122-tfidf-6" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>7 0.10865331 <a title="122-tfidf-7" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>8 0.10409168 <a title="122-tfidf-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.10275661 <a title="122-tfidf-9" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>10 0.10135596 <a title="122-tfidf-10" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>11 0.097009644 <a title="122-tfidf-11" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>12 0.096726149 <a title="122-tfidf-12" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>13 0.095976189 <a title="122-tfidf-13" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>14 0.09462814 <a title="122-tfidf-14" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>15 0.093862481 <a title="122-tfidf-15" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>16 0.090643153 <a title="122-tfidf-16" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>17 0.089946195 <a title="122-tfidf-17" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>18 0.088822849 <a title="122-tfidf-18" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>19 0.088775776 <a title="122-tfidf-19" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>20 0.083858624 <a title="122-tfidf-20" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.06), (2, 0.021), (3, -0.049), (4, 0.055), (5, 0.031), (6, 0.07), (7, 0.049), (8, -0.015), (9, -0.026), (10, -0.086), (11, -0.093), (12, 0.1), (13, -0.128), (14, 0.054), (15, -0.034), (16, -0.043), (17, 0.017), (18, 0.001), (19, 0.06), (20, -0.009), (21, -0.036), (22, -0.048), (23, 0.087), (24, -0.018), (25, -0.017), (26, 0.008), (27, 0.045), (28, 0.01), (29, 0.012), (30, -0.045), (31, -0.01), (32, -0.014), (33, -0.068), (34, -0.022), (35, -0.046), (36, -0.032), (37, 0.028), (38, -0.042), (39, 0.056), (40, 0.008), (41, -0.038), (42, -0.019), (43, -0.028), (44, 0.065), (45, -0.009), (46, 0.014), (47, 0.038), (48, 0.033), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94712359 <a title="122-lsi-1" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>Author: Guang Chen, Yuanyuan Ding, Jing Xiao, Tony X. Han</p><p>Abstract: Context has been playing an increasingly important role to improve the object detection performance. In this paper we propose an effective representation, Multi-Order Contextual co-Occurrence (MOCO), to implicitly model the high level context using solely detection responses from a baseline object detector. The so-called (1st-order) context feature is computed as a set of randomized binary comparisons on the response map of the baseline object detector. The statistics of the 1st-order binary context features are further calculated to construct a high order co-occurrence descriptor. Combining the MOCO feature with the original image feature, we can evolve the baseline object detector to a stronger context aware detector. With the updated detector, we can continue the evolution till the contextual improvements saturate. Using the successful deformable-partmodel detector [13] as the baseline detector, we test the proposed MOCO evolution framework on the PASCAL VOC 2007 dataset [8] and Caltech pedestrian dataset [7]: The proposed MOCO detector outperforms all known state-ofthe-art approaches, contextually boosting deformable part models (ver.5) [13] by 3.3% in mean average precision on the PASCAL 2007 dataset. For the Caltech pedestrian dataset, our method further reduces the log-average miss rate from 48% to 46% and the miss rate at 1 FPPI from 25% to 23%, compared with the best prior art [6].</p><p>2 0.89671981 <a title="122-lsi-2" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>Author: Junjie Yan, Xucong Zhang, Zhen Lei, Shengcai Liao, Stan Z. Li</p><p>Abstract: The serious performance decline with decreasing resolution is the major bottleneck for current pedestrian detection techniques [14, 23]. In this paper, we take pedestrian detection in different resolutions as different but related problems, and propose a Multi-Task model to jointly consider their commonness and differences. The model contains resolution aware transformations to map pedestrians in different resolutions to a common space, where a shared detector is constructed to distinguish pedestrians from background. For model learning, we present a coordinate descent procedure to learn the resolution aware transformations and deformable part model (DPM) based detector iteratively. In traffic scenes, there are many false positives located around vehicles, therefore, we further build a context model to suppress them according to the pedestrian-vehicle relationship. The context model can be learned automatically even when the vehicle annotations are not available. Our method reduces the mean miss rate to 60% for pedestrians taller than 30 pixels on the Caltech Pedestrian Benchmark, which noticeably outperforms previous state-of-the-art (71%).</p><p>3 0.89374465 <a title="122-lsi-3" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>4 0.87759376 <a title="122-lsi-4" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>Author: Rodrigo Benenson, Markus Mathias, Tinne Tuytelaars, Luc Van_Gool</p><p>Abstract: The current state of the art solutions for object detection describe each class by a set of models trained on discovered sub-classes (so called “components ”), with each model itself composed of collections of interrelated parts (deformable models). These detectors build upon the now classic Histogram of Oriented Gradients+linear SVM combo. In this paper we revisit some of the core assumptions in HOG+SVM and show that by properly designing the feature pooling, feature selection, preprocessing, and training methods, it is possible to reach top quality, at least for pedestrian detections, using a single rigid component. We provide experiments for a large design space, that give insights into the design of classifiers, as well as relevant information for practitioners. Our best detector is fully feed-forward, has a single unified architecture, uses only histograms of oriented gradients and colour information in monocular static images, and improves over 23 other methods on the INRIA, ETHand Caltech-USA datasets, reducing the average miss-rate over HOG+SVM by more than 30%.</p><p>5 0.82828367 <a title="122-lsi-5" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>Author: Dan Levi, Shai Silberstein, Aharon Bar-Hillel</p><p>Abstract: In this work we present a new part-based object detection algorithm with hundreds of parts performing realtime detection. Part-based models are currently state-ofthe-art for object detection due to their ability to represent large appearance variations. However, due to their high computational demands such methods are limited to several parts only and are too slow for practical real-time implementation. Our algorithm is an accelerated version of the “Feature Synthesis ” (FS) method [1], which uses multiple object parts for detection and is among state-of-theart methods on human detection benchmarks, but also suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses several strategies for reducing the number of locations searched for each part. The first strategy uses a novel algorithm for approximate nearest neighbor search which we developed, termed “KDFerns ”, to compare each image location to only a subset of the model parts. Candidate part locations for a specific part are further reduced using spatial inhibition, and using an object-level “coarse-to-fine ” strategy. In our empirical evaluation on pedestrian detection benchmarks, AFS main- × tains almost fully the accuracy performance of the original FS, while running more than 4 faster than existing partbased methods which use only several parts. AFS is to our best knowledge the first part-based object detection method achieving real-time running performance: nearly 10 frames per-second on 640 480 images on a regular CPU.</p><p>6 0.75771564 <a title="122-lsi-6" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>7 0.74146563 <a title="122-lsi-7" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>8 0.72885144 <a title="122-lsi-8" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>9 0.70236242 <a title="122-lsi-9" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>10 0.69920301 <a title="122-lsi-10" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>11 0.67436743 <a title="122-lsi-11" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>12 0.66437215 <a title="122-lsi-12" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>13 0.6626665 <a title="122-lsi-13" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>14 0.65450615 <a title="122-lsi-14" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>15 0.65137929 <a title="122-lsi-15" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>16 0.6510123 <a title="122-lsi-16" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>17 0.60890543 <a title="122-lsi-17" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>18 0.60549605 <a title="122-lsi-18" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>19 0.5970422 <a title="122-lsi-19" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>20 0.59320641 <a title="122-lsi-20" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.135), (16, 0.027), (26, 0.039), (28, 0.014), (33, 0.247), (67, 0.159), (69, 0.053), (76, 0.012), (80, 0.011), (86, 0.145), (87, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90184057 <a title="122-lda-1" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>Author: Guang Chen, Yuanyuan Ding, Jing Xiao, Tony X. Han</p><p>Abstract: Context has been playing an increasingly important role to improve the object detection performance. In this paper we propose an effective representation, Multi-Order Contextual co-Occurrence (MOCO), to implicitly model the high level context using solely detection responses from a baseline object detector. The so-called (1st-order) context feature is computed as a set of randomized binary comparisons on the response map of the baseline object detector. The statistics of the 1st-order binary context features are further calculated to construct a high order co-occurrence descriptor. Combining the MOCO feature with the original image feature, we can evolve the baseline object detector to a stronger context aware detector. With the updated detector, we can continue the evolution till the contextual improvements saturate. Using the successful deformable-partmodel detector [13] as the baseline detector, we test the proposed MOCO evolution framework on the PASCAL VOC 2007 dataset [8] and Caltech pedestrian dataset [7]: The proposed MOCO detector outperforms all known state-ofthe-art approaches, contextually boosting deformable part models (ver.5) [13] by 3.3% in mean average precision on the PASCAL 2007 dataset. For the Caltech pedestrian dataset, our method further reduces the log-average miss rate from 48% to 46% and the miss rate at 1 FPPI from 25% to 23%, compared with the best prior art [6].</p><p>2 0.89883405 <a title="122-lda-2" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>Author: Magnus Burenius, Josephine Sullivan, Stefan Carlsson</p><p>Abstract: We consider the problem of automatically estimating the 3D pose of humans from images, taken from multiple calibrated views. We show that it is possible and tractable to extend the pictorial structures framework, popular for 2D pose estimation, to 3D. We discuss how to use this framework to impose view, skeleton, joint angle and intersection constraints in 3D. The 3D pictorial structures are evaluated on multiple view data from a professional football game. The evaluation is focused on computational tractability, but we also demonstrate how a simple 2D part detector can be plugged into the framework.</p><p>3 0.89011544 <a title="122-lda-3" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>Author: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen</p><p>Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.</p><p>4 0.88958859 <a title="122-lda-4" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>Author: Georgia Gkioxari, Pablo Arbeláez, Lubomir Bourdev, Jitendra Malik</p><p>Abstract: We propose a novel approach for human pose estimation in real-world cluttered scenes, and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose, we build on the notion of poselets [4] and train highly discriminative classifiers to differentiate among arm configurations, which we call armlets. We propose a rich representation which, in addition to standardHOGfeatures, integrates the information of strong contours, skin color and contextual cues in a principled manner. Unlike existing methods, we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset, where critical visual phenomena, such as occlusion, truncation, multiple instances and clutter are the norm. Our approach outperforms Yang and Ramanan [26], the state-of-the-art technique, with an improvement from 29.0% to 37.5% PCP accuracy on the arm keypoint prediction task, on this new pose estimation dataset.</p><p>5 0.88929129 <a title="122-lda-5" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>6 0.88913131 <a title="122-lda-6" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>7 0.88741601 <a title="122-lda-7" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>8 0.8841657 <a title="122-lda-8" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>9 0.88208526 <a title="122-lda-9" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>10 0.88191503 <a title="122-lda-10" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>11 0.88119364 <a title="122-lda-11" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>12 0.88077438 <a title="122-lda-12" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>13 0.87938899 <a title="122-lda-13" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>14 0.87796015 <a title="122-lda-14" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>15 0.8771081 <a title="122-lda-15" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>16 0.87623119 <a title="122-lda-16" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>17 0.87375617 <a title="122-lda-17" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>18 0.87117147 <a title="122-lda-18" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>19 0.87099236 <a title="122-lda-19" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>20 0.86909258 <a title="122-lda-20" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
