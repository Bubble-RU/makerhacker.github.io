<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-107" href="#">cvpr2013-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</h1>
<br/><p>Source: <a title="cvpr-2013-107-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kim_Deformable_Spatial_Pyramid_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>Reference: <a title="cvpr-2013-107-reference" href="../cvpr2013_reference/cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pyramid', 0.324), ('dsp', 0.314), ('match', 0.311), ('nod', 0.306), ('patchmatch', 0.261), ('shortl', 0.25), ('lmo', 0.183), ('sift', 0.182), ('pixel', 0.175), ('exempl', 0.157), ('bp', 0.149), ('vij', 0.124), ('flow', 0.115), ('ti', 0.112), ('cel', 0.1), ('link', 0.091), ('smooth', 0.09), ('subst', 0.086), ('di', 0.084), ('mrf', 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="107-tfidf-1" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>2 0.21157824 <a title="107-tfidf-2" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>3 0.20667845 <a title="107-tfidf-3" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>Author: Jiangbo Lu, Hongsheng Yang, Dongbo Min, Minh N. Do</p><p>Abstract: Though many tasks in computer vision can be formulated elegantly as pixel-labeling problems, a typical challenge discouraging such a discrete formulation is often due to computational efficiency. Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. This paper presents a generic and fast computational framework for general multi-labeling problems called PatchMatch Filter (PMF). For the very first time, we explore effective and efficient strategies to weave together these two fundamental techniques developed in isolation, i.e., PatchMatch-based randomized search and efficient edge-aware image filtering. By decompositing an image into compact superpixels, we also propose superpixelbased novel search strategies that generalize and improve the original PatchMatch method. Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases.</p><p>4 0.19840787 <a title="107-tfidf-4" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>Author: Asad A. Butt, Robert T. Collins</p><p>Abstract: We propose a method for global multi-target tracking that can incorporate higher-order track smoothness constraints such as constant velocity. Our problem formulation readily lends itself to path estimation in a trellis graph, but unlike previous methods, each node in our network represents a candidate pair of matching observations between consecutive frames. Extra constraints on binary flow variables in the graph result in a problem that can no longer be solved by min-cost network flow. We therefore propose an iterative solution method that relaxes these extra constraints using Lagrangian relaxation, resulting in a series of problems that ARE solvable by min-cost flow, and that progressively improve towards a high-quality solution to our original optimization problem. We present experimental results showing that our method outperforms the standard network-flow formulation as well as other recent algorithms that attempt to incorporate higher-order smoothness constraints.</p><p>5 0.19453061 <a title="107-tfidf-5" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>6 0.17669076 <a title="107-tfidf-6" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>7 0.1757732 <a title="107-tfidf-7" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>8 0.16923726 <a title="107-tfidf-8" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>9 0.16569053 <a title="107-tfidf-9" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>10 0.15939271 <a title="107-tfidf-10" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>11 0.14788833 <a title="107-tfidf-11" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>12 0.1433474 <a title="107-tfidf-12" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>13 0.13809301 <a title="107-tfidf-13" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>14 0.13725768 <a title="107-tfidf-14" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>15 0.13320628 <a title="107-tfidf-15" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>16 0.12989123 <a title="107-tfidf-16" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>17 0.12755288 <a title="107-tfidf-17" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>18 0.12599814 <a title="107-tfidf-18" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>19 0.12319212 <a title="107-tfidf-19" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>20 0.12263905 <a title="107-tfidf-20" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.285), (1, 0.001), (2, -0.033), (3, 0.016), (4, 0.071), (5, -0.064), (6, 0.051), (7, 0.053), (8, 0.014), (9, 0.028), (10, 0.0), (11, -0.047), (12, -0.023), (13, -0.114), (14, -0.075), (15, -0.126), (16, 0.202), (17, -0.071), (18, -0.027), (19, -0.031), (20, 0.019), (21, 0.101), (22, -0.013), (23, 0.059), (24, -0.012), (25, -0.057), (26, -0.13), (27, 0.124), (28, 0.156), (29, -0.025), (30, -0.028), (31, 0.038), (32, -0.005), (33, 0.155), (34, -0.099), (35, -0.153), (36, -0.022), (37, 0.015), (38, -0.009), (39, -0.006), (40, 0.03), (41, 0.003), (42, -0.06), (43, 0.052), (44, 0.034), (45, -0.016), (46, 0.013), (47, 0.061), (48, 0.07), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96507257 <a title="107-lsi-1" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>2 0.80604285 <a title="107-lsi-2" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>3 0.72138888 <a title="107-lsi-3" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>Author: Quanshi Zhang, Xuan Song, Xiaowei Shao, Ryosuke Shibasaki, Huijing Zhao</p><p>Abstract: An object model base that covers a large number of object categories is of great value for many computer vision tasks. As artifacts are usually designed to have various textures, their structure is the primary distinguishing feature between different categories. Thus, how to encode this structural information and how to start the model learning with a minimum of human labeling become two key challenges for the construction of the model base. We design a graphical model that uses object edges to represent object structures, and this paper aims to incrementally learn this category model from one labeled object and a number of casually captured scenes. However, the incremental model learning may be biased due to the limited human labeling. Therefore, we propose a new strategy that uses the depth information in RGBD images to guide the model learning for object detection in ordinary RGB images. In experiments, the proposed method achieves superior performance as good as the supervised methods that require the labeling of all target objects.</p><p>4 0.71659631 <a title="107-lsi-4" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>Author: Feng Zhou, Fernando De_la_Torre</p><p>Abstract: Graph matching (GM) is a fundamental problem in computer science, and it has been successfully applied to many problems in computer vision. Although widely used, existing GM algorithms cannot incorporate global consistence among nodes, which is a natural constraint in computer vision problems. This paper proposes deformable graph matching (DGM), an extension of GM for matching graphs subject to global rigid and non-rigid geometric constraints. The key idea of this work is a new factorization of the pair-wise affinity matrix. This factorization decouples the affinity matrix into the local structure of each graph and the pair-wise affinity edges. Besides the ability to incorporate global geometric transformations, this factorization offers three more benefits. First, there is no need to compute the costly (in space and time) pair-wise affinity matrix. Second, it provides a unified view of many GM methods and extends the standard iterative closest point algorithm. Third, it allows to use the path-following optimization algorithm that leads to improved optimization strategies and matching performance. Experimental results on synthetic and real databases illustrate how DGM outperforms state-of-the-art algorithms for GM. The code is available at http : / / human s en s ing . c s . cmu .edu / fgm.</p><p>5 0.71024609 <a title="107-lsi-5" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>Author: Liang Li, Wei Feng, Liang Wan, Jiawan Zhang</p><p>Abstract: This paper addresses a challenging problem of regularizing arbitrary superpixels into an optimal grid structure, which may significantly extend current low-level vision algorithms by allowing them to use superpixels (SPs) conveniently as using pixels. For this purpose, we aim at constructing maximum cohesive SP-grid, which is composed of real nodes, i.e. SPs, and dummy nodes that are meaningless in the image with only position-taking function in the grid. For a given formation of image SPs and proper number of dummy nodes, we first dynamically align them into a grid based on the centroid localities of SPs. We then define the SP-grid coherence as the sum of edge weights, with SP locality and appearance encoded, along all direct paths connecting any two nearest neighboring real nodes in the grid. We finally maximize the SP-grid coherence via cascade dynamic programming. Our approach can take the regional objectness as an optional constraint to produce more semantically reliable SP-grids. Experiments on object localization show that our approach outperforms state-of-the-art methods in terms of both detection accuracy and speed. We also find that with the same searching strategy and features, object localization at SP-level is about 100-500 times faster than pixel-level, with usually better detection accuracy.</p><p>6 0.69006348 <a title="107-lsi-6" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>7 0.67868334 <a title="107-lsi-7" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>8 0.67709345 <a title="107-lsi-8" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>9 0.64666432 <a title="107-lsi-9" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>10 0.6290229 <a title="107-lsi-10" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>11 0.61136031 <a title="107-lsi-11" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>12 0.60118502 <a title="107-lsi-12" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>13 0.60082275 <a title="107-lsi-13" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>14 0.59964246 <a title="107-lsi-14" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>15 0.58601528 <a title="107-lsi-15" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>16 0.58205986 <a title="107-lsi-16" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>17 0.58197057 <a title="107-lsi-17" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>18 0.58132941 <a title="107-lsi-18" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>19 0.57385141 <a title="107-lsi-19" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>20 0.56700116 <a title="107-lsi-20" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.066), (4, 0.108), (5, 0.13), (37, 0.072), (81, 0.067), (86, 0.023), (97, 0.462)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97382933 <a title="107-lda-1" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>Author: Long Mai, Yuzhen Niu, Feng Liu</p><p>Abstract: A variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First, different methods perform differently in saliency analysis. Second, the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neighboringpixels. To account for the dependence of aggregation on an individual image, our approach selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods.</p><p>2 0.95360619 <a title="107-lda-2" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>Author: Jonathan Balzer, Stefano Soatto</p><p>Abstract: We describe a method to efficiently generate a model (map) of small-scale objects from video. The map encodes sparse geometry as well as coarse photometry, and could be used to initialize dense reconstruction schemes as well as to support recognition and localization of three-dimensional objects. Self-occlusions and the predominance of outliers present a challenge to existing online Structure From Motion and Simultaneous Localization and Mapping systems. We propose a unified inference criterion that encompasses map building and localization (object detection) relative to the map in a coupled fashion. We establish correspondence in a computationally efficient way without resorting to combinatorial matching or random-sampling techniques. Instead, we use a simpler M-estimator that exploits putative correspondence from tracking after photometric and topological validation. We have collected a new dataset to benchmark model building in the small scale, which we test our algorithm on in comparison to others. Although our system is significantly leaner than previous ones, it compares favorably to the state of the art in terms of accuracy and robustness.</p><p>3 0.9456414 <a title="107-lda-3" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>Author: Yinqiang Zheng, Shigeki Sugimoto, Masatoshi Okutomi</p><p>Abstract: Due to its simplicity, the eight-point algorithm has been widely used in fundamental matrix estimation. Unfortunately, the rank-2 constraint of a fundamental matrix is enforced via a posterior rank correction step, thus leading to non-optimal solutions to the original problem. To address this drawback, existing algorithms need to solve either a very high order polynomial or a sequence of convex relaxation problems, both of which are computationally ineffective and numerically unstable. In this work, we present a new rank-2 constrained eight-point algorithm, which directly incorporates the rank-2 constraint in the minimization process. To avoid singularities, we propose to solve seven subproblems and retrieve their globally optimal solutions by using tailored polynomial system solvers. Our proposed method is noniterative, computationally efficient and numerically stable. Experiment results have verified its superiority over existing algebraic error based algorithms in terms of accuracy, as well as its advantages when used to initialize geometric error based algorithms.</p><p>4 0.93413717 <a title="107-lda-4" href="./cvpr-2013-A_Principled_Deep_Random_Field_Model_for_Image_Segmentation.html">24 cvpr-2013-A Principled Deep Random Field Model for Image Segmentation</a></p>
<p>Author: Pushmeet Kohli, Anton Osokin, Stefanie Jegelka</p><p>Abstract: We discuss a model for image segmentation that is able to overcome the short-boundary bias observed in standard pairwise random field based approaches. To wit, we show that a random field with multi-layered hidden units can encode boundary preserving higher order potentials such as the ones used in the cooperative cuts model of [11] while still allowing for fast and exact MAP inference. Exact inference allows our model to outperform previous image segmentation methods, and to see the true effect of coupling graph edges. Finally, our model can be easily extended to handle segmentation instances with multiple labels, for which it yields promising results.</p><p>5 0.92619622 <a title="107-lda-5" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>same-paper 6 0.91144443 <a title="107-lda-6" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>7 0.89182389 <a title="107-lda-7" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>8 0.89163554 <a title="107-lda-8" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>9 0.79901564 <a title="107-lda-9" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>10 0.79835498 <a title="107-lda-10" href="./cvpr-2013-Continuous_Inference_in_Graphical_Models_with_Polynomial_Energies.html">95 cvpr-2013-Continuous Inference in Graphical Models with Polynomial Energies</a></p>
<p>11 0.76920378 <a title="107-lda-11" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>12 0.76370037 <a title="107-lda-12" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>13 0.76311564 <a title="107-lda-13" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>14 0.76087344 <a title="107-lda-14" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>15 0.75980914 <a title="107-lda-15" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>16 0.75740278 <a title="107-lda-16" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>17 0.75327301 <a title="107-lda-17" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>18 0.75240391 <a title="107-lda-18" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>19 0.74602187 <a title="107-lda-19" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>20 0.74439645 <a title="107-lda-20" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
