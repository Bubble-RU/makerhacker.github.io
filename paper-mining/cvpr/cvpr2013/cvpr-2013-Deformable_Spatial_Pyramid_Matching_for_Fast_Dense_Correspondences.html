<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-107" href="#">cvpr2013-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</h1>
<br/><p>Source: <a title="cvpr-2013-107-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kim_Deformable_Spatial_Pyramid_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>Reference: <a title="cvpr-2013-107-reference" href="../cvpr2013_reference/cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. [sent-7, score-1.011]
</p><p>2 Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. [sent-8, score-0.687]
</p><p>3 Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. [sent-9, score-0.799]
</p><p>4 This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. [sent-10, score-0.42]
</p><p>5 Traditional dense matching problems—such as stereo or optical flow— deal with the “instance matching” scenario, in which the two input images contain different viewpoints of the same scene or object. [sent-14, score-0.482]
</p><p>6 More recently, researchers have pushed the boundaries of dense matching to estimate correspondences between images with different scenes or objects. [sent-15, score-0.516]
</p><p>7 This advance beyond instance matching leads to many interesting new applications, such as semantic image segmentation [15], image completion [2], image classification [11], and video depth estimation [10]. [sent-16, score-0.373]
</p><p>8 There are two major challenges when matching generic images: image variation and computational cost. [sent-17, score-0.323]
</p><p>9 At the same time, the search space is much larger, since generic image matching permits no clean geometric constraints. [sent-20, score-0.43]
</p><p>10 To address these challenges, existing methods have largely focused on imposing geometric regularization on the matching problem. [sent-22, score-0.41]
</p><p>11 Typically, this entails a smoothness constraint preferring that nearby pixels in one image get matched to nearby locations in the second image; such constraints help resolve ambiguities that are common if matching with pixel appearance alone. [sent-23, score-0.865]
</p><p>12 Thus, re-  searchers have explored various computationally efficient solutions, including hierarchical optimization [15], randomized search [2], 1D approximations of 2D layout [11], spectral relaxations [13], and approximate graph matching [5]. [sent-25, score-0.384]
</p><p>13 Despite the variety in the details of prior dense matching methods, we see that their underlying models are surprisingly similar: minimize the appearance matching cost of individual pixels while imposing geometric smoothness between paired pixels. [sent-26, score-1.071]
</p><p>14 , MRF stereo matching [17]), the locality of pixels is problematic for generic image matching; pixels simply lack the discriminating power to resolve matching ambiguity in the face of visual variations. [sent-30, score-0.799]
</p><p>15 To address these limitations, we introduce a deformable spatial pyramid (DSP) model for fast dense matching. [sent-32, score-0.575]
</p><p>16 Rather than reason with pixels alone, the proposed model regularizes match consistency at multiple spatial extents— ranging from an entire image, to coarse grid cells, to every single pixel. [sent-33, score-0.367]
</p><p>17 A key idea behind our approach is to strike a balance between robustness to image variations on the one hand, and accurate localization of pixel correspondences on the other. [sent-34, score-0.43]
</p><p>18 We achieve this balance through a pyramid graph: larger spatial nodes offer greater regularization when appearance matches are ambiguous, while smaller spatial nodes help localize matches with fine detail. [sent-35, score-1.099]
</p><p>19 To validate our idea, we compare against state-of-theart methods on two datasets, reporting results for pixel la222333000755  bel transfer and semantic segmentation tasks. [sent-37, score-0.29]
</p><p>20 Compared to today’s strongest and most widely used methods, SIFT Flow [15] and PatchMatch [2]—both of which rely on a pixel-based model—our method achieves substantial gains in matching accuracy. [sent-38, score-0.388]
</p><p>21 Traditional matching approaches aim to estimate very accurate pixel correspondences (e. [sent-42, score-0.553]
</p><p>22 For such accurate localization, most methods define the matching cost on pixels. [sent-45, score-0.327]
</p><p>23 It casts matching as a graph optimization problem, where pixels are nodes, and edges between neighboring nodes reflect the existence of spatial constraints between them [4]. [sent-47, score-0.75]
</p><p>24 The objective consists of a data term for  each pixel’s matching cost and a smoothness term for the neighbors’ locations. [sent-48, score-0.572]
</p><p>25 Stronger geometric regularization is one way to overcome the matching ambiguity—for example, by enforcing geometric smoothness on all pairs of pixels, not just neighbors [3, 13] (see Fig. [sent-53, score-0.613]
</p><p>26 For efficiency, it uses a multi-resolution image pyramid together with a hierarchical optimization technique inspired by classic flow algorithms. [sent-57, score-0.45]
</p><p>27 That is, no graph edges span between pyramid levels. [sent-60, score-0.423]
</p><p>28 , a pixel) spatial extents, and nodes are linked between pyra-  mid levels in the graph. [sent-66, score-0.359]
</p><p>29 A circle denotes a graph node and its size represents its spatial extent. [sent-115, score-0.303]
</p><p>30 (a) Deformable spatial pyramid (proposed): uses spatial support at various extents. [sent-117, score-0.455]
</p><p>31 (b) Hierarchical pixel model [15]: the matching result from a lower resolution image guides the matching in the next resolution. [sent-118, score-0.757]
</p><p>32 (c) Fullpairwise model [3, 13]: every pair of nodes is linked for strong geometric regularization (though limited to sparse nodes). [sent-119, score-0.352]
</p><p>33 (d) Pixel model with implicit smoothness [2]: geometric smoothness is enforced in an indirect manner via a spatially-constrained correspondence search (dotted lines denote no explicit links). [sent-120, score-0.39]
</p><p>34 addition, SIFT Flow defines the matching cost at each pixel node by a single SIFT descriptor at a given (downsampled) resolution, which risks losing useful visual detail. [sent-122, score-0.739]
</p><p>35 In contrast, we define the matching cost of each node using multiple descriptors computed at the image’s original resolution, thus preserving richer visual information. [sent-123, score-0.534]
</p><p>36 Instead, it progressively searches for correspondences; a reliable match at one pixel subsequently guides the matching locations of its nearby pixels, thus implicitly enforcing geometric smoothness. [sent-126, score-0.742]
</p><p>37 The appearance matching cost is defined at each pixel, and geometric smoothness is imposed between paired pixels. [sent-129, score-0.598]
</p><p>38 In contrast, the proposed deformable spatial pyramid model considers both matching costs and geometric regularization within multiple spatial extents. [sent-130, score-0.951]
</p><p>39 recognition), our work differs substantially from the familiar spatial pyramid, since we model geometric distortions between and across pyramid levels in the matching objective. [sent-134, score-0.779]
</p><p>40 In that sense, our matching relates to deformable part models in object detection [7] and scene classification [16]. [sent-135, score-0.405]
</p><p>41 Whereas all these models use a few tens of patches/parts and target object recognition, our model handles millions of pixels and targets dense pixel matching. [sent-136, score-0.345]
</p><p>42 For such instance matching problems, however, it does not provide a clear win over pixel models in practice. [sent-138, score-0.436]
</p><p>43 In contrast, we show it yields substantial gains when matching generic images of different scenes, and our regular pyramid structure enables an efficient solution. [sent-139, score-0.718]
</p><p>44 Approach We first define our deformable spatial pyramid (DSP) graph for dense pixel matching (Sec. [sent-141, score-1.074]
</p><p>45 Then, we define the matching objective we will optimize on that pyramid (Sec. [sent-144, score-0.614]
</p><p>46 Pyramid Graph Model To build our spatial pyramid, we start from the entire  image and divide it into four rectangular grid cells and keep dividing until we reach the predefined number of pyramid levels (we use 3). [sent-152, score-0.547]
</p><p>47 This is a conventional spatial pyramid as seen in previous work. [sent-153, score-0.372]
</p><p>48 Each grid cell and pixel is a node, and edges link all neighboring nodes within the same level, as well as parent-child nodes across adjacent levels. [sent-157, score-0.713]
</p><p>49 For the pixel level, however, we do not link neighboring pixels; each pixel is linked only to its parent cell. [sent-158, score-0.452]
</p><p>50 Matching Objective Now, we define our matching objective for the proposed pyramid graph. [sent-162, score-0.614]
</p><p>51 We start with a basic formulation for matching images at a single fixed scale, and then extend it to multi-scale matching. [sent-163, score-0.282]
</p><p>52 Fixed-Scale Matching Objective Let pi = (xi , yi) denote the location of node iin the pyramid graph, which is given by the node’s center coordinate. [sent-164, score-0.446]
</p><p>53 Let ti = (ui , vi) be the translation of node ifrom the first to the second image. [sent-165, score-0.301]
</p><p>54 First row shows image 1’s pyramid graph; second row shows the match solution on image 2. [sent-168, score-0.394]
</p><p>55 Single-sided arrow in a node denotes its flow vector ti; double-sided arrows between pyramid levels imply parent-child connections between them (intra-level edges are also used but not displayed). [sent-169, score-0.703]
</p><p>56 We solve the matching problem at different sizes of spatial nodes in two layers. [sent-170, score-0.535]
</p><p>57 Vij(ti,tj), i  i,j∈N  (1)  where Di is a data term, Vij is a smoothness term, α is a constant weight, and N denotes pairs of nodes linked by graph edges. [sent-176, score-0.417]
</p><p>58 Recall that edges span across pyramid levels, as well as within pyramid levels. [sent-177, score-0.649]
</p><p>59 Our data term Di measures the appearance matching cost of node iat translation ti. [sent-178, score-0.599]
</p><p>60 1,λ),  (2)  where q denotes pixel coordinates within a node i from which local descriptors were extracted, z is the total number of descriptors, and d1 and d2 are descriptors extracted at the locations q and q + ti in the first and second image, respectively. [sent-183, score-0.55]
</p><p>61 The smoothness term Vij regularizes the solution by penalizing large discrepancies in the matching locations of neighboring nodes: Vij = min( ? [sent-186, score-0.584]
</p><p>62 In (a), the match for a node in the first image remains at the same fixed scale in the second image. [sent-216, score-0.346]
</p><p>63 graph nodes in our model are defined by cells of varying spatial extents, whereas in prior models they are restricted to pixels. [sent-218, score-0.389]
</p><p>64 Third, we explicitly link the nodes of different spatial extents to impose smoothness, striking a balance between strong regularizationby the larger nodes and accurate localization by the finer nodes. [sent-222, score-0.782]
</p><p>65 Note that the resulting matching is asymmetric, mapping all of the nodes in the  first image to some (possibly subset of) positions in the second image. [sent-227, score-0.452]
</p><p>66 Furthermore, while our method returns matches for all nodes in all levels of the pyramid, we are generally interested in the final dense matches at the pixel level. [sent-228, score-0.667]
</p><p>67 Multi-Scale Extension Thus far, we assume the matching is done at a fixed scale: each grid cell is matched to another region of the same size. [sent-230, score-0.426]
</p><p>68 We add a scale variable si for each node and introduce a scale smoothness term Wij = ? [sent-238, score-0.547]
</p><p>69 1,λ), (4) where we see the corresponding location of descriptor d2 for a descriptor d1 is now determined by a translation ti  followed by a scaling si. [sent-245, score-0.284]
</p><p>70 Dense correspondence for generic image matching is often treated at a fixed scale, though there are some multiscale implementations in related work. [sent-249, score-0.426]
</p><p>71 PatchMatch has a multi-scale extension that expands the correspondence search range according to the scale of the previously found match [2]. [sent-250, score-0.282]
</p><p>72 As in the fixed-scale case, our method has the advantage of modeling geometric distortion and match consistency across multiple spatial extents. [sent-251, score-0.293]
</p><p>73 We initialize the solution by running BP for a graph built on all the nodes except the pixel-level ones (which we will call first-layer), and then refine it at the pixel nodes (which we will call second-layer). [sent-267, score-0.557]
</p><p>74 Moreover, we observe that sparse descriptor sampling is enough for the first-layer BP: as long as a grid cell includes ∼100s of local descriptors within it, its average descriptor distance for the data term (Eq. [sent-274, score-0.319]
</p><p>75 Once we run the first-layer BP, the optimal translation ti at a pixel-level node iis simply determined  by: ti = argtmin(Di(t) + αVij(t,tj)), where a node j is a parent grid cell of a pixel node i, and tj is a fixed value obtained from the first-layer BP. [sent-280, score-0.962]
</p><p>76 Our multi-scale extension incurs additional cost due to the scale smoothness and multi-variate data terms. [sent-281, score-0.295]
</p><p>77 We compare our deformable spatial pyramid (DSP) approach to state-of-the-art dense pixel matching methods, SIFT Flow [15] (SF) and PatchMatch [2] (PM), using the authors’ publicly available code. [sent-298, score-1.011]
</p><p>78 This reduction saves about 1 second per image match without losing matching accuracy. [sent-323, score-0.458]
</p><p>79 i−34  Evaluation metrics: To measure image matching quality, we use label transfer accuracy (LT-ACC) between pixel correspondences [14]. [sent-330, score-0.598]
</p><p>80 Given a test and an exemplar image, we transfer the annotated class labels of the exemplar pixels to the test ones via pixel correspondences, and count how many pixels in the test image are correctly labeled. [sent-331, score-0.721]
</p><p>81 Since there are no available ground-truth pixel matching positions between images, we obtain pixel locations using an object bounding box: pixel  locations are given by the normalized coordinates with respect to the box’s position and size. [sent-335, score-0.822]
</p><p>82 Raw Image Matching Accuracy In this section, we evaluate raw pixel matching quality in two different tasks: object matching and scene matching. [sent-339, score-0.755]
</p><p>83 Object matching under intra-class variations: For this experiment, we randomly pick 15 pairs of images for each object class in the Caltech-101 (total 1,515 pairs of images). [sent-340, score-0.317]
</p><p>84 222333 010 919  image to the first via pixel correspondences, and the right one shows the transferred pixel labels for the first image (white: fg, black: bg). [sent-348, score-0.308]
</p><p>85 This shows our spatial pyramid graph successfully imposes geometric regularization from various spatial extents, overcoming the matching ambiguity that can arise if considering local pixels alone. [sent-362, score-1.002]
</p><p>86 However, this tends to hurt matching quality—the matching positions of even nearby pixels are quite dithered, making the results noisy. [sent-365, score-0.688]
</p><p>87 However, we can do this without hurting accuracy since larger spatial nodes in our model enforce a proper smoothness on pixels. [sent-369, score-0.383]
</p><p>88 Scene matching: Whereas the object matching task is concerned with foreground/background matches, in the scene matching task each pixel in an exemplar is annotated with one of multiple class labels. [sent-370, score-0.907]
</p><p>89 To explain briefly, we match a test image to multiple exemplar images, where pixels in the exemplars are annotated by ground-truth class labels. [sent-391, score-0.429]
</p><p>90 Then, the best matching scores (SIFT descriptor distances) between each test pixel and its corresponding exemplar pixels define the class label likelihood of the test pixel. [sent-392, score-0.802]
</p><p>91 Building on this common framework, we test how the different matching methods influence segmentation quality. [sent-395, score-0.369]
</p><p>92 The test image is matched against only the shortlist exemplars to estimate the class likelihoods. [sent-405, score-0.407]
</p><p>93 Instead, we only use match scores in order to most directly compare the impact of the three matching methods. [sent-409, score-0.387]
</p><p>94 This is because (1) the shortlist misses reasonable exemplar images that share class labels with the test image and (2) SIFT features may not be strong enough to discriminate confusing classes in a noisy shortlist—some classes (e. [sent-427, score-0.443]
</p><p>95 Figure 8 plots the matching accuracy as a function of scale variation. [sent-442, score-0.366]
</p><p>96 In fact, as scale variation increases, we observe that objects undergo more variations in viewpoint or shapes as well, making the matching more challenging. [sent-448, score-0.493]
</p><p>97 mmainx((OO11,,OO22)),  Figure 9 shows some matching examples by our multiscale method, compared to our single-scale counterpart. [sent-449, score-0.328]
</p><p>98 The examples show that our multi-scale matching is flexible to scale variations. [sent-450, score-0.366]
</p><p>99 Conclusion We introduced a deformable spatial pyramid model for dense correspondences across different objects or scenes. [sent-452, score-0.692]
</p><p>100 As such, compared to the existing methods that rely on a flat pixel-based model, we achieve substantial gains in both matching accuracy and runtime. [sent-454, score-0.427]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pyramid', 0.289), ('matching', 0.282), ('dsp', 0.28), ('patchmatch', 0.232), ('shortlist', 0.223), ('extents', 0.187), ('nodes', 0.17), ('lmo', 0.163), ('node', 0.157), ('pixel', 0.154), ('sift', 0.151), ('bp', 0.133), ('smoothness', 0.13), ('flow', 0.122), ('correspondences', 0.117), ('exemplar', 0.117), ('dense', 0.117), ('vij', 0.11), ('match', 0.105), ('ti', 0.1), ('matches', 0.087), ('deformable', 0.086), ('scale', 0.084), ('spatial', 0.083), ('di', 0.075), ('mrf', 0.075), ('pixels', 0.074), ('geometric', 0.073), ('substantial', 0.073), ('cells', 0.073), ('variations', 0.073), ('descriptor', 0.07), ('gist', 0.069), ('iou', 0.064), ('graph', 0.063), ('exemplars', 0.063), ('abandons', 0.06), ('aoo', 0.06), ('mlk', 0.06), ('correspondence', 0.057), ('si', 0.056), ('regularization', 0.055), ('regularizes', 0.055), ('undergo', 0.054), ('linked', 0.054), ('qmin', 0.053), ('levels', 0.052), ('segmentation', 0.052), ('matched', 0.051), ('grid', 0.05), ('descriptors', 0.05), ('nearby', 0.05), ('mosaics', 0.05), ('link', 0.048), ('layer', 0.048), ('connections', 0.047), ('sifts', 0.047), ('multiscale', 0.046), ('stereo', 0.046), ('translations', 0.046), ('localization', 0.046), ('cost', 0.045), ('transfer', 0.045), ('objectives', 0.044), ('translation', 0.044), ('objective', 0.043), ('cell', 0.043), ('neighboring', 0.042), ('generic', 0.041), ('scales', 0.041), ('balance', 0.04), ('saves', 0.04), ('downsampled', 0.04), ('locations', 0.039), ('semantic', 0.039), ('flat', 0.039), ('guides', 0.039), ('hierarchical', 0.039), ('striking', 0.038), ('scene', 0.037), ('edges', 0.036), ('aside', 0.036), ('term', 0.036), ('extension', 0.036), ('affects', 0.035), ('test', 0.035), ('span', 0.035), ('appearance', 0.035), ('class', 0.035), ('permits', 0.034), ('vlfeat', 0.033), ('paired', 0.033), ('gains', 0.033), ('labelme', 0.033), ('dominate', 0.033), ('confusing', 0.033), ('distortion', 0.032), ('robustly', 0.031), ('layered', 0.031), ('losing', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="107-tfidf-1" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>2 0.20254515 <a title="107-tfidf-2" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>Author: Jiangbo Lu, Hongsheng Yang, Dongbo Min, Minh N. Do</p><p>Abstract: Though many tasks in computer vision can be formulated elegantly as pixel-labeling problems, a typical challenge discouraging such a discrete formulation is often due to computational efficiency. Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. This paper presents a generic and fast computational framework for general multi-labeling problems called PatchMatch Filter (PMF). For the very first time, we explore effective and efficient strategies to weave together these two fundamental techniques developed in isolation, i.e., PatchMatch-based randomized search and efficient edge-aware image filtering. By decompositing an image into compact superpixels, we also propose superpixelbased novel search strategies that generalize and improve the original PatchMatch method. Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases.</p><p>3 0.16927347 <a title="107-tfidf-3" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>4 0.16781178 <a title="107-tfidf-4" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>Author: Asad A. Butt, Robert T. Collins</p><p>Abstract: We propose a method for global multi-target tracking that can incorporate higher-order track smoothness constraints such as constant velocity. Our problem formulation readily lends itself to path estimation in a trellis graph, but unlike previous methods, each node in our network represents a candidate pair of matching observations between consecutive frames. Extra constraints on binary flow variables in the graph result in a problem that can no longer be solved by min-cost network flow. We therefore propose an iterative solution method that relaxes these extra constraints using Lagrangian relaxation, resulting in a series of problems that ARE solvable by min-cost flow, and that progressively improve towards a high-quality solution to our original optimization problem. We present experimental results showing that our method outperforms the standard network-flow formulation as well as other recent algorithms that attempt to incorporate higher-order smoothness constraints.</p><p>5 0.15345573 <a title="107-tfidf-5" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>Author: Michael Rubinstein, Armand Joulin, Johannes Kopf, Ce Liu</p><p>Abstract: We present a new unsupervised algorithm to discover and segment out common objects from large and diverse image collections. In contrast to previous co-segmentation methods, our algorithm performs well even in the presence of significant amounts of noise images (images not containing a common object), as typical for datasets collected from Internet search. The key insight to our algorithm is that common object patterns should be salient within each image, while being sparse with respect to smooth transformations across images. We propose to use dense correspondences between images to capture the sparsity and visual variability of the common object over the entire database, which enables us to ignore noise objects that may be salient within their own images but do not commonly occur in others. We performed extensive numerical evaluation on es- tablished co-segmentation datasets, as well as several new datasets generated using Internet search. Our approach is able to effectively segment out the common object for diverse object categories, while naturally identifying images where the common object is not present.</p><p>6 0.15156893 <a title="107-tfidf-6" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>7 0.14754051 <a title="107-tfidf-7" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>8 0.14328717 <a title="107-tfidf-8" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>9 0.13936734 <a title="107-tfidf-9" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>10 0.13813567 <a title="107-tfidf-10" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>11 0.13595879 <a title="107-tfidf-11" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>12 0.13230874 <a title="107-tfidf-12" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>13 0.13000943 <a title="107-tfidf-13" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>14 0.12806383 <a title="107-tfidf-14" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>15 0.12758242 <a title="107-tfidf-15" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>16 0.12748298 <a title="107-tfidf-16" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>17 0.12353875 <a title="107-tfidf-17" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>18 0.11787443 <a title="107-tfidf-18" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>19 0.11436476 <a title="107-tfidf-19" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>20 0.11407873 <a title="107-tfidf-20" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.29), (1, 0.045), (2, 0.038), (3, 0.005), (4, 0.09), (5, -0.007), (6, 0.017), (7, 0.005), (8, -0.078), (9, -0.015), (10, 0.105), (11, 0.087), (12, 0.113), (13, 0.055), (14, 0.081), (15, -0.111), (16, -0.023), (17, -0.096), (18, 0.148), (19, -0.043), (20, 0.034), (21, -0.032), (22, 0.083), (23, -0.019), (24, 0.049), (25, -0.091), (26, 0.037), (27, 0.006), (28, -0.008), (29, 0.104), (30, -0.095), (31, -0.096), (32, 0.107), (33, -0.023), (34, 0.042), (35, 0.023), (36, 0.07), (37, 0.011), (38, 0.05), (39, -0.037), (40, -0.025), (41, 0.034), (42, -0.028), (43, -0.005), (44, 0.104), (45, -0.117), (46, -0.015), (47, -0.018), (48, -0.045), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97531879 <a title="107-lsi-1" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>2 0.79056746 <a title="107-lsi-2" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>3 0.74326825 <a title="107-lsi-3" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>Author: Jiangbo Lu, Hongsheng Yang, Dongbo Min, Minh N. Do</p><p>Abstract: Though many tasks in computer vision can be formulated elegantly as pixel-labeling problems, a typical challenge discouraging such a discrete formulation is often due to computational efficiency. Recent studies on fast cost volume filtering based on efficient edge-aware filters have provided a fast alternative to solve discrete labeling problems, with the complexity independent of the support window size. However, these methods still have to step through the entire cost volume exhaustively, which makes the solution speed scale linearly with the label space size. When the label space is huge, which is often the case for (subpixelaccurate) stereo and optical flow estimation, their computational complexity becomes quickly unacceptable. Developed to search approximate nearest neighbors rapidly, the PatchMatch method can significantly reduce the complexity dependency on the search space size. But, its pixel-wise randomized search and fragmented data access within the 3D cost volume seriously hinder the application of efficient cost slice filtering. This paper presents a generic and fast computational framework for general multi-labeling problems called PatchMatch Filter (PMF). For the very first time, we explore effective and efficient strategies to weave together these two fundamental techniques developed in isolation, i.e., PatchMatch-based randomized search and efficient edge-aware image filtering. By decompositing an image into compact superpixels, we also propose superpixelbased novel search strategies that generalize and improve the original PatchMatch method. Focusing on dense correspondence field estimation in this paper, we demonstrate PMF’s applications in stereo and optical flow. Our PMF methods achieve state-of-the-art correspondence accuracy but run much faster than other competing methods, often giving over 10-times speedup for large label space cases.</p><p>4 0.73279661 <a title="107-lsi-4" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>Author: Mayank Bansal, Kostas Daniilidis</p><p>Abstract: We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night), age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixellevel descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art.</p><p>5 0.7087549 <a title="107-lsi-5" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>Author: Armand Joulin, Sing Bing Kang</p><p>Abstract: An anaglyph is a single image created by selecting complementary colors from a stereo color pair; the user can perceive depth by viewing it through color-filtered glasses. We propose a technique to reconstruct the original color stereo pair given such an anaglyph. We modified SIFT-Flow and use it to initially match the different color channels across the two views. Our technique then iteratively refines the matches, selects the good matches (which defines the “anchor” colors), and propagates the anchor colors. We use a diffusion-based technique for the color propagation, and added a step to suppress unwanted colors. Results on a variety of inputs demonstrate the robustness of our technique. We also extended our method to anaglyph videos by using optic flow between time frames.</p><p>6 0.70265019 <a title="107-lsi-6" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>7 0.69826829 <a title="107-lsi-7" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>8 0.68458283 <a title="107-lsi-8" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>9 0.65905362 <a title="107-lsi-9" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>10 0.65694219 <a title="107-lsi-10" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>11 0.63978392 <a title="107-lsi-11" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>12 0.63147879 <a title="107-lsi-12" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>13 0.62120527 <a title="107-lsi-13" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>14 0.61867517 <a title="107-lsi-14" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<p>15 0.61715013 <a title="107-lsi-15" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>16 0.61314279 <a title="107-lsi-16" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>17 0.60476023 <a title="107-lsi-17" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>18 0.60287064 <a title="107-lsi-18" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>19 0.60171831 <a title="107-lsi-19" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>20 0.59991103 <a title="107-lsi-20" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.1), (16, 0.042), (26, 0.037), (33, 0.271), (67, 0.038), (69, 0.048), (87, 0.403)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95720994 <a title="107-lda-1" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>2 0.95224184 <a title="107-lda-2" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>Author: Christian Häne, Christopher Zach, Andrea Cohen, Roland Angst, Marc Pollefeys</p><p>Abstract: Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being ’too noisy’. Unfortunately, these priors generally yield overly smooth reconstructions and/or segmentations in certain regions whereas they fail in other areas to constrain the solution sufficiently. In this paper we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other’s task. As a consequence, we propose a rigorous mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. Image segmentations provide geometric cues about which surface orientations are more likely to appear at a certain location in space whereas a dense 3D reconstruction yields a suitable regularization for the segmentation problem by lifting the labeling from 2D images to 3D space. We show how appearance-based cues and 3D surface orientation priors can be learned from training data and subsequently used for class-specific regularization. Experimental results on several real data sets highlight the advantages of our joint formulation.</p><p>3 0.94763887 <a title="107-lda-3" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>Author: Eno Töppe, Claudia Nieuwenhuis, Daniel Cremers</p><p>Abstract: We introduce the concept of relative volume constraints in order to account for insufficient information in the reconstruction of 3D objects from a single image. The key idea is to formulate a variational reconstruction approach with shape priors in form of relative depth profiles or volume ratios relating object parts. Such shape priors can easily be derived either from a user sketch or from the object’s shading profile in the image. They can handle textured or shadowed object regions by propagating information. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware. In contrast to existing single view reconstruction algorithms, the proposed algorithm provides substantially more flexibility to recover shape details such as self-occlusions, dents and holes, which are not visible in the object silhouette.</p><p>4 0.92886031 <a title="107-lda-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.91860199 <a title="107-lda-5" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>Author: Yi-Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa, P. Jonathon Phillips</p><p>Abstract: We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification, where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.</p><p>6 0.91422021 <a title="107-lda-6" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>7 0.90828717 <a title="107-lda-7" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>same-paper 8 0.90607429 <a title="107-lda-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.88413376 <a title="107-lda-9" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>10 0.85903287 <a title="107-lda-10" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>11 0.84871846 <a title="107-lda-11" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>12 0.81830102 <a title="107-lda-12" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>13 0.81373268 <a title="107-lda-13" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>14 0.80197054 <a title="107-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.79471624 <a title="107-lda-15" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>16 0.79124212 <a title="107-lda-16" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>17 0.79027009 <a title="107-lda-17" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>18 0.78915852 <a title="107-lda-18" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>19 0.78571236 <a title="107-lda-19" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>20 0.78570861 <a title="107-lda-20" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
