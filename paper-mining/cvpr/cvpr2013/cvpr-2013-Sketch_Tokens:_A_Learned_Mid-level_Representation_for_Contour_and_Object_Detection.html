<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-401" href="#">cvpr2013-401</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</h1>
<br/><p>Source: <a title="cvpr-2013-401-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lim_Sketch_Tokens_A_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Joseph J. Lim, C. Lawrence Zitnick, Piotr Dollár</p><p>Abstract: We propose a novel approach to both learning and detecting local contour-based representations for mid-level features. Our features, called sketch tokens, are learned using supervised mid-level information in the form of hand drawn contours in images. Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. We demonstrate our approach on both topdown and bottom-up tasks. We show state-of-the-art results on the top-down task of contour detection while being over 200× faster than competing methods. We also achieve large improvements ainn dcoetmecptietoinn agc mceutrhaocdys f.o Wr teh ael sboot atochmi-evuep ltaarsgkse of pedestrian and object detection as measured on INRIA [5] and PASCAL [10], respectively. These gains are due to the complementary information provided by sketch tokens to low-level features such as gradient histograms.</p><p>Reference: <a title="cvpr-2013-401-reference" href="../cvpr2013_reference/cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our features, called sketch tokens, are learned using supervised mid-level information in the form of hand drawn contours in images. [sent-9, score-0.606]
</p><p>2 Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. [sent-10, score-1.031]
</p><p>3 We show state-of-the-art results on the top-down task of contour detection while being over 200× faster than competing methods. [sent-12, score-0.217]
</p><p>4 o Wr teh ael sboot atochmi-evuep ltaarsgkse of pedestrian and object detection as measured on INRIA [5] and PASCAL [10], respectively. [sent-14, score-0.126]
</p><p>5 These gains are due to the complementary information provided by sketch tokens to low-level features such as gradient histograms. [sent-15, score-1.212]
</p><p>6 object detection, and top-down tasks, such as contour classification [14] or pixel-level segmentation [37] from object class information. [sent-22, score-0.213]
</p><p>7 Early edge detectors [3] were used to find more complex shapes such as junctions [22], straight lines and curves [8], and were applied to object recognition [34], structure from motion [32], tracking [3 1], and 3D shape recovery [25]. [sent-25, score-0.195]
</p><p>8 Examples of sketch tokens learned from hand drawn sketches represented using their mean contour structure. [sent-27, score-1.48]
</p><p>9 Notice the variety and richness of the sketch tokens. [sent-28, score-0.443]
</p><p>10 Our features, called sketch tokens, capture local edge structure. [sent-30, score-0.511]
</p><p>11 The classes of sketch tokens range from standard shapes such as straight lines and junctions to richer  structures such as curves and sets of parallel lines (Fig. [sent-31, score-1.333]
</p><p>12 Given the vast number of potential local edge structures, we must select an informative subset to represent by the sketch tokens. [sent-33, score-0.511]
</p><p>13 We propose a novel approach to defining token classes using supervised mid-level information, unlike previous approaches that use hand-defined classes [39], high-level supervision [16], or unsupervised information [15]. [sent-34, score-0.505]
</p><p>14 Patches centered on contours are extracted from the hand drawn sketches and clustered to form a set of token classes. [sent-37, score-0.593]
</p><p>15 This results in a diverse, representative set of sketch tokens. [sent-38, score-0.443]
</p><p>16 We typically utilize a few hundred tokens, which captures a majority of the commonly occurring edge structures (Fig. [sent-39, score-0.147]
</p><p>17 333 111555668  Our goal is to efficiently predict the occurrence of sketch tokens given an input color image. [sent-41, score-1.167]
</p><p>18 We propose a data driven approach that classifies each image patch with a token label given a collection of low-level features including oriented gradient channels [7], color channels, and self-similarity channels [26]. [sent-42, score-0.912]
</p><p>19 The token class assignments resulting from clustering the patches of hand drawn contours provide our ground truth labels for training. [sent-43, score-0.541]
</p><p>20 The result is an efficient approach that can compute per-pixel  token labelings in about one second per image. [sent-45, score-0.383]
</p><p>21 We demonstrate the advantages of our mid-level sketch tokens on both top-down and bottom-up tasks. [sent-46, score-1.127]
</p><p>22 While numerous detection approaches [5, 11, 7] utilize such features directly, other papers learn edge-based features [27, 38, 21] or class-specific edges [23, 18] using object level supervision. [sent-55, score-0.161]
</p><p>23 Learned features in these approaches resemble edge filters in early layers and more complex structures in deeper layers [40]. [sent-58, score-0.128]
</p><p>24 Our approach to learning a contourbased representation differs in that we inject mid-level su-  pervision to learn a universal set of sketch tokens. [sent-59, score-0.443]
</p><p>25 [6] cast edge detection as a binary classification problem and used human labeled edges to train a binary patch edge classifier. [sent-62, score-0.307]
</p><p>26 [39] applied a similar approach to detect 17 unique local edge structures including edges, junctions and corners. [sent-64, score-0.141]
</p><p>27 learning sketch tokens as opposed to a semantic segmentation. [sent-72, score-1.127]
</p><p>28 While we utilize fairly standard low-level features [7, 26] and a well known classification algorithm [2, 4], the novelty of our approach is in the definition and use of sketch tokens to effectively encode local image structure. [sent-73, score-1.185]
</p><p>29 Sketch Tokens In this section we describe our approach to learning token classes from hand drawn sketches and for detecting the tokens in novel images. [sent-75, score-1.277]
</p><p>30 We show how to utilize these tokens for contour and object detection in the following sections. [sent-76, score-0.949]
</p><p>31 We begin by describing how we define our token classes. [sent-77, score-0.383]
</p><p>32 Defining sketch token classes Our goal is to define a set of token classes that represent the wide variety of local edge structures that may exist in an image. [sent-80, score-1.382]
</p><p>33 We propose an approach for discovering these classes using human-generated image sketches [1], see Figure 2. [sent-82, score-0.125]
</p><p>34 Let us assume we have a set of images I with a corresponding set of binary images S representing the hand drawn contours from the sketches. [sent-83, score-0.136]
</p><p>35 We define the set of sketch token classes by clustering patches s extracted from the binary images S. [sent-86, score-0.914]
</p><p>36 Fuhrtashear m foixree,d o snilzey p oaftc 3h5es × th 3a5t contain a labeled contour at the center pixel are used (there are approximately two million such patches in the training set of [20]). [sent-88, score-0.216]
</p><p>37 To provide invariance to slight shifts in edge placement, Daisy descriptors [36] are computed on the binary contour labels contained in sj . [sent-89, score-0.262]
</p><p>38 Notice the variety of the sketch tokens, ranging from straight lines to more complex structures. [sent-99, score-0.498]
</p><p>39 Detecting sketch tokens Given a set of sketch token classes, we wish to detect their occurrence in color images. [sent-102, score-1.993]
</p><p>40 We detect the token classes with a learned classifier. [sent-103, score-0.441]
</p><p>41 Ground truth class labels are supplied by the clustering results described above if the patch is centered on a contour in the hand drawn sketches S, otherwise the patch is assigned to the background or “no contour” class. [sent-105, score-0.461]
</p><p>42 [7] and compute multiple feature channels per image where each channel has the same size as the input image and captures a different facet of information. [sent-110, score-0.217]
</p><p>43 Two types of features are then employed: features directly indexing into the channels and self-similarity features. [sent-111, score-0.224]
</p><p>44 Our channels are composed of color, gradient, and oriented gradient information in a patch xi extracted from a color image. [sent-112, score-0.332]
</p><p>45 Three color channels are computed using the CIE-LUV color space. [sent-113, score-0.218]
</p><p>46 We compute several normalized gradient channels that vary in orientation and scale [7, 5, 17]. [sent-114, score-0.212]
</p><p>47 Three gradient magnitude channels are computed with varying amounts of blur (we use Gaussian blurs with σ = 0, 1. [sent-115, score-0.244]
</p><p>48 Additionally, the gradient magnitude channels at σ = 0 and σ = 1. [sent-117, score-0.244]
</p><p>49 5 are split based on orientation to create four additional channels each for a total of eight oriented magnitude channels. [sent-118, score-0.233]
</p><p>50 Finally all channels are post-  L  U  V  σ =  ×  0  σ =  1. [sent-119, score-0.17]
</p><p>51 Frequency of example features being selected by the random forest: (first row) color channels, (second row) gradient magnitude channels, (third row) selected orientation channels. [sent-121, score-0.125]
</p><p>52 Pixels in the resulting channels serve as the first type of feature for our classifier. [sent-123, score-0.17]
</p><p>53 The self-similarity features capture the portions of an image patch that contain similar textures based on color or gradient information. [sent-126, score-0.158]
</p><p>54 la Froitry c fheaantunreel fijk as: fijk =  sjk − sik,  (1)  ××  where sjk is the sum of grid cell j in channel k. [sent-129, score-0.278]
</p><p>55 In summary, we utilize 3 color channels, 3 gradient magnitude channels, and 8 oriented gradient channels for a total of 14 channels. [sent-143, score-0.372]
</p><p>56 Computing the channels given a 640 480 input image sta). [sent-147, score-0.17]
</p><p>57 2  Classification  Two considerations must be taken into account when choosing a classifier for labeling sketch tokens in image patches. [sent-157, score-1.143]
</p><p>58 We randomly sample 150,000 contour patches (1000 per token class) and 160,000 “no contour” patches (800 per training image) for training each tree. [sent-162, score-0.636]
</p><p>59 Notice the heavy use of the image gradients in the center of the patch based on gradient scale. [sent-170, score-0.129]
</p><p>60 We show an illustration of the probabilities for several different sketch tokens in Figure 5. [sent-172, score-1.127]
</p><p>61 Notice the high selectivity of the different sketch tokens. [sent-173, score-0.458]
</p><p>62 Contour detection We now describe our approach to detecting contours using a top-down approach. [sent-175, score-0.118]
</p><p>63 Sketch tokens provide an estimate of the local edge structure in a patch. [sent-176, score-0.752]
</p><p>64 However, contour detection only requires the binary labeling of pixel contours. [sent-177, score-0.232]
</p><p>65 We show that computing mid-level sketch tokens provides accurate and efficient predictions of low-level contours. [sent-178, score-1.127]
</p><p>66 Our random forest classifier predicts the probability that an image patch belongs to each token class or the negative set. [sent-179, score-0.519]
</p><p>67 Since each token has a contour located at its center, we can compute the probability of a contour at the center pixel using the sum of token probabilities. [sent-180, score-1.139]
</p><p>68 If tij is the probability of patch xi belonging to token j, and ti0 is the probability of belonging to the “no contour” class, the estimated probability of the patch’s center containing a contour is: ei  =  ? [sent-181, score-0.695]
</p><p>69 j Once the probability of a contour has been computed at each pixel, a standard non-maximal suppression scheme may be applied to find the peak response of a contour [3]. [sent-184, score-0.388]
</p><p>70 Contour detection results  ×  We test our contour detector on the popular Berkeley Segmentation Dataset and Benchmark (BSDS500) [20, 1]. [sent-187, score-0.217]
</p><p>71 In Figure 6 and Table 1, we compare our contour detection method against competing methods using standard evaluation metrics [1]. [sent-189, score-0.217]
</p><p>72 Additionally, our contour detector shows improved recall and precision at both ends of the precision-recall curve in Figure 6. [sent-191, score-0.179]
</p><p>73 Mfficoiset ncto tmhapnut aaptpioron aisc spent tinh the sketch token detection stage, which can be parallelized. [sent-196, score-0.864]
</p><p>74 Object detection We demonstrate our mid-level sketch token features on two object recognition datasets; the INRIA pedestrian dataset [5] and the PASCAL 2007 object recognition dataset [10]. [sent-211, score-0.981]
</p><p>75 INRIA pedestrian For pedestrian detection we use an improved implementation of Doll a´r et al. [sent-215, score-0.15]
</p><p>76 The addition of our new midlevel sketch token features gives a significant boost to accuracy. [sent-221, score-0.878]
</p><p>77 Mean log-average miss rate on the INRIA pedestrian dataset: notice the considerable improvement over previous techniques using our approach. [sent-262, score-0.118]
</p><p>78 ents) as features for a boosted detector (we utilize similar channels for computing tokens). [sent-264, score-0.228]
</p><p>79 In addition to standard channels, we add channels corresponding to our sketch token probability maps (computed at twice the resolution of the original images). [sent-265, score-1.011]
</p><p>80 Unlike traditional channels that capture low-level information, our channels represent the more complex edge structures that may exist in the scene. [sent-266, score-0.441]
</p><p>81 Our approach using 150 sketch tokens plus the negative “no contour” feature achieves a MR of 19. [sent-270, score-1.142]
</p><p>82 Combining sketch tokens and the 10 low-level features achieves 14. [sent-272, score-1.169]
</p><p>83 An interesting question is how the number of sketch tokens affects detection results. [sent-277, score-1.165]
</p><p>84 In Figure 8, we show results using various numbers of token classes. [sent-278, score-0.383]
</p><p>85 Notice that using increasing numbers of tokens leads to improved accuracy. [sent-279, score-0.684]
</p><p>86 Mean log-error rates on the INRIA dataset using different numbers of sketch tokens. [sent-304, score-0.443]
</p><p>87 We propose adding our sketch tokens to the HOG features for training the DPMs. [sent-313, score-1.154]
</p><p>88 Unlike the boosting technique we used for pedestrian detection [7], linear SVMs are highly sensitive to how the features are normalized. [sent-314, score-0.139]
</p><p>89 The sketch token features are divided into a grid with cells of size 4 4 ptoixkeelns. [sent-316, score-0.887]
</p><p>90 While individually HOG features outperform sketch tokens, in nearly all cases top Average Precision (AP) scores are achieved with a combination of HOG and sketch tokens. [sent-323, score-0.913]
</p><p>91 This demonstrates that sketch tokens may provide a valu-  plane  HOG ST+HOG  bird  boat  27. [sent-324, score-1.127]
</p><p>92 Sketchtokenweightvisualization:Wevisualizethetop  5 sketch tokens multiplied by the learned weight for each cell. [sent-368, score-1.149]
</p><p>93 Notice the many sketch tokens with rich edge structures that are used. [sent-369, score-1.228]
</p><p>94 Discussion Sketch tokens provide a novel approach to feature learning. [sent-374, score-0.684]
</p><p>95 Sketch tokens provide a rich source of information for a variety of tasks. [sent-379, score-0.684]
</p><p>96 For instance, better contour detection may be useful for many image editing tasks. [sent-381, score-0.217]
</p><p>97 For Sketch Tokens we define edge strength according to Equation 2 and apply smoothing and standard non-maximal suppression to obtain peak edge responses [3]. [sent-385, score-0.151]
</p><p>98 333 111666224  In conclusion, we described a new mid-level feature called sketch tokens. [sent-387, score-0.443]
</p><p>99 The tokens are learned from images with ground truth contours and are fast to compute. [sent-388, score-0.764]
</p><p>100 Using the learned tokens we demonstrated state-of-the-art results on several datasets ranging from contour to object detection. [sent-389, score-0.902]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tokens', 0.684), ('sketch', 0.443), ('token', 0.383), ('contour', 0.179), ('channels', 0.17), ('sketches', 0.089), ('hog', 0.072), ('edge', 0.068), ('patch', 0.065), ('fijk', 0.064), ('doll', 0.063), ('contours', 0.058), ('pedestrian', 0.056), ('notice', 0.046), ('drawn', 0.044), ('inria', 0.044), ('gradient', 0.042), ('shotton', 0.042), ('sjk', 0.042), ('forest', 0.04), ('junctions', 0.04), ('detection', 0.038), ('forests', 0.037), ('patches', 0.037), ('ods', 0.036), ('classes', 0.036), ('pascal', 0.035), ('structures', 0.033), ('channel', 0.032), ('dpms', 0.032), ('magnitude', 0.032), ('utilize', 0.031), ('oriented', 0.031), ('supervision', 0.03), ('scg', 0.03), ('mr', 0.029), ('decision', 0.028), ('straight', 0.028), ('lines', 0.027), ('features', 0.027), ('midlevel', 0.025), ('topdown', 0.025), ('color', 0.024), ('tij', 0.023), ('gradients', 0.022), ('detecting', 0.022), ('learned', 0.022), ('edges', 0.021), ('trees', 0.02), ('supervised', 0.02), ('texton', 0.019), ('hand', 0.019), ('cell', 0.019), ('cells', 0.019), ('tu', 0.019), ('boosting', 0.018), ('voc', 0.018), ('fitzgibbon', 0.018), ('pami', 0.017), ('object', 0.017), ('winn', 0.017), ('human', 0.017), ('miss', 0.016), ('complementary', 0.016), ('cro', 0.016), ('classifier', 0.016), ('snol', 0.016), ('cnh', 0.016), ('trmhea', 0.016), ('vsa', 0.016), ('marr', 0.016), ('dies', 0.016), ('ssp', 0.016), ('ap', 0.016), ('leaf', 0.016), ('arbelaez', 0.016), ('occurrence', 0.016), ('ranges', 0.016), ('probability', 0.015), ('curves', 0.015), ('achieves', 0.015), ('binary', 0.015), ('captures', 0.015), ('peak', 0.015), ('grid', 0.015), ('ael', 0.015), ('tchteio', 0.015), ('edgebased', 0.015), ('sydney', 0.015), ('aep', 0.015), ('selectivity', 0.015), ('joseph', 0.015), ('luv', 0.015), ('piotr', 0.015), ('henderson', 0.015), ('opelt', 0.015), ('duda', 0.015), ('impurity', 0.015), ('nsh', 0.015), ('howard', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="401-tfidf-1" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>Author: Joseph J. Lim, C. Lawrence Zitnick, Piotr Dollár</p><p>Abstract: We propose a novel approach to both learning and detecting local contour-based representations for mid-level features. Our features, called sketch tokens, are learned using supervised mid-level information in the form of hand drawn contours in images. Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. We demonstrate our approach on both topdown and bottom-up tasks. We show state-of-the-art results on the top-down task of contour detection while being over 200× faster than competing methods. We also achieve large improvements ainn dcoetmecptietoinn agc mceutrhaocdys f.o Wr teh ael sboot atochmi-evuep ltaarsgkse of pedestrian and object detection as measured on INRIA [5] and PASCAL [10], respectively. These gains are due to the complementary information provided by sketch tokens to low-level features such as gradient histograms.</p><p>2 0.11657015 <a title="401-tfidf-2" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>3 0.095365159 <a title="401-tfidf-3" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>Author: Xiaowei Zhou, Xiaojie Huang, James S. Duncan, Weichuan Yu</p><p>Abstract: Active contours are widely used in image segmentation. To cope with missing or misleading features in images, researchers have introduced various ways to model the prior of shapes and use the prior to constrain active contours. However, the shape prior is usually learnt from a large set of annotated data, which is not always accessible in practice. Moreover, it is often doubted that the existing shapes in the training set will be sufficient to model the new instance in the testing image. In this paper, we propose to use the group similarity of object shapes in multiple images as a prior to aid segmentation, which can be interpreted as an unsupervised approach of shape prior modeling. We show that the rank of the matrix consisting of multiple shapes is a good measure of the group similarity of the shapes, and the nuclear norm minimization is a simple and effective way to impose the proposed constraint on existing active contour models. Moreover, we develop a fast algorithm to solve the proposed model by using the accelerated proximal method. Experiments using echocardiographic image sequences acquired from acute canine experiments demonstrate that the proposed method can consistently improve the performance of active contour models and increase the robustness against image defects such as missing boundaries.</p><p>4 0.092123955 <a title="401-tfidf-4" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>Author: Saurabh Gupta, Pablo Arbeláez, Jitendra Malik</p><p>Abstract: We address the problems of contour detection, bottomup grouping and semantic segmentation using RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset [27]. We propose algorithms for object boundary detection and hierarchical segmentation that generalize the gPb − ucm approach of [se2]g mbeyn mtaatkioinng t effective use oef t dheep gthP information. Wroea schho owf that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We then turn to the problem of semantic segmentation and propose a simple approach that classifies superpixels into the 40 dominant object categories in NYUD2. We use both generic and class-specific features to encode the appearance and geometry of objects. We also show how our approach can be used for scene classification, and how this contextual information in turn improves object recognition. In all of these tasks, we report significant improvements over the state-of-the-art.</p><p>5 0.087184764 <a title="401-tfidf-5" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>Author: Yansheng Ming, Hongdong Li, Xuming He</p><p>Abstract: This paper aims to extract salient closed contours from an image. For this vision task, both region segmentation cues (e.g. color/texture homogeneity) and boundary detection cues (e.g. local contrast, edge continuity and contour closure) play important and complementary roles. In this paper we show how to combine both cues in a unified framework. The main focus is given to how to maintain the consistency (compatibility) between the region cues and the boundary cues. To this ends, we introduce the use of winding number–a well-known concept in topology–as a powerful mathematical device. By this device, the region-boundary consistency is represented as a set of simple linear relationships. Our method is applied to the figure-ground segmentation problem. The experiments show clearly improved results.</p><p>6 0.083714776 <a title="401-tfidf-6" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>7 0.073203601 <a title="401-tfidf-7" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>8 0.072242588 <a title="401-tfidf-8" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>9 0.070777424 <a title="401-tfidf-9" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>10 0.069935046 <a title="401-tfidf-10" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>11 0.069911979 <a title="401-tfidf-11" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>12 0.067293309 <a title="401-tfidf-12" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>13 0.064504422 <a title="401-tfidf-13" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>14 0.063175119 <a title="401-tfidf-14" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>15 0.060928065 <a title="401-tfidf-15" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>16 0.058465227 <a title="401-tfidf-16" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>17 0.056605428 <a title="401-tfidf-17" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>18 0.056309506 <a title="401-tfidf-18" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>19 0.054693107 <a title="401-tfidf-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.053739179 <a title="401-tfidf-20" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, -0.019), (2, 0.011), (3, -0.007), (4, 0.039), (5, 0.013), (6, 0.024), (7, 0.048), (8, -0.019), (9, -0.028), (10, -0.016), (11, -0.046), (12, 0.027), (13, -0.05), (14, 0.051), (15, 0.007), (16, -0.022), (17, -0.038), (18, 0.023), (19, 0.038), (20, 0.009), (21, 0.065), (22, -0.065), (23, -0.028), (24, 0.002), (25, 0.077), (26, 0.041), (27, -0.005), (28, 0.005), (29, 0.043), (30, -0.048), (31, 0.026), (32, -0.037), (33, 0.02), (34, 0.05), (35, 0.067), (36, 0.008), (37, 0.056), (38, -0.06), (39, 0.091), (40, 0.039), (41, 0.084), (42, 0.024), (43, 0.009), (44, 0.135), (45, 0.023), (46, -0.002), (47, 0.033), (48, 0.019), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89736187 <a title="401-lsi-1" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>Author: Joseph J. Lim, C. Lawrence Zitnick, Piotr Dollár</p><p>Abstract: We propose a novel approach to both learning and detecting local contour-based representations for mid-level features. Our features, called sketch tokens, are learned using supervised mid-level information in the form of hand drawn contours in images. Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. We demonstrate our approach on both topdown and bottom-up tasks. We show state-of-the-art results on the top-down task of contour detection while being over 200× faster than competing methods. We also achieve large improvements ainn dcoetmecptietoinn agc mceutrhaocdys f.o Wr teh ael sboot atochmi-evuep ltaarsgkse of pedestrian and object detection as measured on INRIA [5] and PASCAL [10], respectively. These gains are due to the complementary information provided by sketch tokens to low-level features such as gradient histograms.</p><p>2 0.72123927 <a title="401-lsi-2" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>3 0.70564181 <a title="401-lsi-3" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>4 0.66664422 <a title="401-lsi-4" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>Author: Camillo Jose Taylor</p><p>Abstract: In this paper we explore approaches to accelerating segmentation and edge detection algorithms based on the gPb framework. The paper characterizes the performance of a simple but effective edge detection scheme which can be computed rapidly and offers performance that is competitive with the pB detector. The paper also describes an approach for computing a reduced order normalized cut that captures the essential features of the original problem but can be computed in less than half a second on a standard computing platform.</p><p>5 0.62510699 <a title="401-lsi-5" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>6 0.60194486 <a title="401-lsi-6" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>7 0.58699971 <a title="401-lsi-7" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<p>8 0.58518523 <a title="401-lsi-8" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>9 0.56350553 <a title="401-lsi-9" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>10 0.55903596 <a title="401-lsi-10" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>11 0.54647088 <a title="401-lsi-11" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>12 0.53521037 <a title="401-lsi-12" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>13 0.51441878 <a title="401-lsi-13" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>14 0.51241815 <a title="401-lsi-14" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>15 0.49583188 <a title="401-lsi-15" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>16 0.49372113 <a title="401-lsi-16" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>17 0.49013034 <a title="401-lsi-17" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>18 0.48337129 <a title="401-lsi-18" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>19 0.48110169 <a title="401-lsi-19" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>20 0.47721165 <a title="401-lsi-20" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.105), (16, 0.04), (26, 0.041), (28, 0.021), (33, 0.207), (34, 0.208), (67, 0.094), (69, 0.044), (87, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81027478 <a title="401-lda-1" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>Author: Varun Ramakrishna, Takeo Kanade, Yaser Sheikh</p><p>Abstract: The human body is structurally symmetric. Tracking by detection approaches for human pose suffer from double counting, where the same image evidence is used to explain two separate but symmetric parts, such as the left and right feet. Double counting, if left unaddressed can critically affect subsequent processes, such as action recognition, affordance estimation, and pose reconstruction. In this work, we present an occlusion aware algorithm for tracking human pose in an image sequence, that addresses the problem of double counting. Our key insight is that tracking human pose can be cast as a multi-target tracking problem where the ”targets ” are related by an underlying articulated structure. The human body is modeled as a combination of singleton parts (such as the head and neck) and symmetric pairs of parts (such as the shoulders, knees, and feet). Symmetric body parts are jointly tracked with mutual exclusion constraints to prevent double counting by reasoning about occlusion. We evaluate our algorithm on an outdoor dataset with natural background clutter, a standard indoor dataset (HumanEva-I), and compare against a state of the art pose estimation algorithm.</p><p>same-paper 2 0.80538982 <a title="401-lda-2" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>Author: Joseph J. Lim, C. Lawrence Zitnick, Piotr Dollár</p><p>Abstract: We propose a novel approach to both learning and detecting local contour-based representations for mid-level features. Our features, called sketch tokens, are learned using supervised mid-level information in the form of hand drawn contours in images. Patches of human generated contours are clustered to form sketch token classes and a random forest classifier is used for efficient detection in novel images. We demonstrate our approach on both topdown and bottom-up tasks. We show state-of-the-art results on the top-down task of contour detection while being over 200× faster than competing methods. We also achieve large improvements ainn dcoetmecptietoinn agc mceutrhaocdys f.o Wr teh ael sboot atochmi-evuep ltaarsgkse of pedestrian and object detection as measured on INRIA [5] and PASCAL [10], respectively. These gains are due to the complementary information provided by sketch tokens to low-level features such as gradient histograms.</p><p>3 0.77140588 <a title="401-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.76865119 <a title="401-lda-4" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>5 0.76241672 <a title="401-lda-5" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>6 0.76228559 <a title="401-lda-6" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>7 0.76065427 <a title="401-lda-7" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>8 0.75927246 <a title="401-lda-8" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>9 0.75920731 <a title="401-lda-9" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>10 0.75890261 <a title="401-lda-10" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>11 0.75883466 <a title="401-lda-11" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>12 0.75841212 <a title="401-lda-12" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>13 0.75839835 <a title="401-lda-13" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>14 0.75823593 <a title="401-lda-14" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>15 0.75791728 <a title="401-lda-15" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>16 0.75780165 <a title="401-lda-16" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>17 0.75742197 <a title="401-lda-17" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>18 0.75703502 <a title="401-lda-18" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>19 0.75694859 <a title="401-lda-19" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>20 0.75642169 <a title="401-lda-20" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
