<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-292" href="#">cvpr2013-292</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</h1>
<br/><p>Source: <a title="cvpr-2013-292-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kwak_Multi-agent_Event_Detection_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>Reference: <a title="cvpr-2013-292-reference" href="../cvpr2013_reference/cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 kr  Abstract We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. [sent-3, score-1.654]
</p><p>2 Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. [sent-4, score-1.163]
</p><p>3 Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. [sent-5, score-0.972]
</p><p>4 Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. [sent-6, score-0.876]
</p><p>5 We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents. [sent-7, score-0.193]
</p><p>6 Introduction Event detection—identification of a predefined event in videos—typically suffers from an enormous combinatorial complexity. [sent-9, score-0.539]
</p><p>7 It is aggravated by additional challenges such  as the agents’ unknown roles and the extra agents who do not participate in the event. [sent-10, score-0.57]
</p><p>8 We are interested in such challenging problem, event localization and role assignment, which refers to identifying the target event and recognizing its participants in the presence of non-participants, while estimating the roles of the participants automatically at the same time. [sent-11, score-1.842]
</p><p>9 Although computer vision community has broad interest in the problems related to event detection, the joint estimation of event localization and role assignment has rarely been studied before. [sent-12, score-1.531]
</p><p>10 This is particularly due to the computational complexity induced by various factors, for example, multiple agent participation in an event, temporallogical variations of an event or potential activities irrelevant to the target event. [sent-13, score-1.049]
</p><p>11 In this paper, we introduce a novel algorithm for video event understanding, where the event localization problem ∗This work was supported by MEST Basic Science Research Program through the NRF of Korea (2012-0003697), the IT R&D; program of MKE/KEIT (10040246), and Samsung Electronics Co. [sent-14, score-1.165]
</p><p>12 is solved jointly with the role assignment by formulating the problem as an optimization framework. [sent-16, score-0.391]
</p><p>13 Our algorithm incorporates [10] to describe and detect a target event based on a scenario. [sent-17, score-0.637]
</p><p>14 A na¨ ıve extension of [10] for event localization and role assignment however requires the consideration of all possible agent-role combinations, which may be intractable computationally. [sent-18, score-0.992]
</p><p>15 For this purpose, we first generate agent groups, each of which is composed of agents potentially having mutual interactions defined in a scenario. [sent-20, score-0.597]
</p><p>16 The confidence and time interval of each role are estimated for each agent by rolespecific event detection. [sent-21, score-1.302]
</p><p>17 The event localization and role assignment are solved by assigning roles to agents optimally per group and selecting groups whose role assignments are feasible. [sent-22, score-1.883]
</p><p>18 1) The event localization and role assignment problems are jointly managed in a single optimization framework. [sent-24, score-1.014]
</p><p>19 2) Our framework handles some challenging situations in event detection in a principled way, e. [sent-25, score-0.584]
</p><p>20 2 reviews previous work closely related to multi-agent event detection. [sent-31, score-0.539]
</p><p>21 After we discuss the event detection framework based on the constraint flow in Sec. [sent-32, score-0.791]
</p><p>22 3, our multi-agent event localization and role assignment algorithm is presented in Sec. [sent-33, score-0.992]
</p><p>23 Related work The simplest tasks related to event detection include atomic action detection (e. [sent-38, score-0.669]
</p><p>24 An atomic action is a short-term event generated by a single agent, and abnormality detection is a binary decision with respect to video contents;  both approaches are insufficient to deduce semantic interpretations from videos. [sent-43, score-0.716]
</p><p>25 In this paper, we consider an event as a composition of atomic actions, which we call primitives, under a temporal-logical structure. [sent-44, score-0.579]
</p><p>26 The structure of 222666888200  the event is described by a scenario. [sent-45, score-0.539]
</p><p>27 Recently, scenarios are often described by logic such as temporal interval algebra [1] and event logic [20] to represent various aspects of events more fluently. [sent-48, score-0.882]
</p><p>28 Logic-based scenarios have been adopted by various event detection algorithms including hierarchical constraint satisfaction [17], multithread parsing [24], randomized stochastic local search [4], and probabilistic inference on constraint flow [10] and Markov logic network [15, 22]. [sent-49, score-0.979]
</p><p>29 However, most of these approaches have focused on events with a single agent only [2, 12, 14, 19], or events that can be detected without role identification [4, 5, 15]. [sent-50, score-0.802]
</p><p>30 Otherwise, roles should be initialized by prior information or human intervention [8, 10, 17, 24]. [sent-51, score-0.269]
</p><p>31 A few recent approaches discuss role assignment in  the detection of complex multi-agent events. [sent-52, score-0.436]
</p><p>32 In [11], an MRF captures relationships between roles and performs per-video event categorization; the extensions to handle non-participants and localize events in spatio-temporal domain are not straightforward. [sent-53, score-0.96]
</p><p>33 An MCMC technique is introduced to identify agents’ roles and detect an event at the same time in [18]. [sent-54, score-0.808]
</p><p>34 However, it is a heuristic-based method presenting results in videos with a single event occurrence only. [sent-55, score-0.573]
</p><p>35 Our method is formulated in a more principled way and can handle multiple occurrences of an event naturally. [sent-56, score-0.565]
</p><p>36 Scenario-based event detection Our framework incorporates [10] to describe and detect target events based on scenarios. [sent-58, score-0.777]
</p><p>37 In this section, we briefly review the scenario description method and event detection procedure using constraint flow presented in [10]. [sent-59, score-0.882]
</p><p>38 Scenario description A scenario describes a target event based on primitives and their temporal-logical relationships. [sent-62, score-0.869]
</p><p>39 The relationships constrain arrangements of time intervals of the primitives. [sent-63, score-0.153]
</p><p>40 Let ρi and ρj be primitives organizing a target event. [sent-64, score-0.239]
</p><p>41 Four temporal relationships define temporal orders between starting and ending times of two primitives: ρi  <  ρj  ρi ∼ ρj ρi  ∧  ρj  ⇔ end(ρi) is earlier than start(ρj) . [sent-65, score-0.226]
</p><p>42 The quinary conditions to specify flow vertices The last unary relationship is to describe an unknown number of recurrences of a specified primitive; k is an index for the number of recurrences. [sent-79, score-0.278]
</p><p>43 ⇔ ρi  ⇔  In principle, logical relationships take precedence over temporal relationships, but we can use parentheses to override the precedence of the relationships. [sent-82, score-0.232]
</p><p>44 γdel and γrec indicate two roles in Delivery, deliverer and receiver, respectively. [sent-84, score-0.269]
</p><p>45 A role associated with each primitive is specified in the attached bracket. [sent-85, score-0.376]
</p><p>46 Constraint flow and event detection Constraint flow is a state transition machine characterizing the organization of a target event. [sent-88, score-0.918]
</p><p>47 The state of a target event at a time step is represented by a combination of conditions of all primitives. [sent-89, score-0.663]
</p><p>48 The condition of a primitive is active if the primitive occurs at the time step, and it is inactive otherwise; the inactive condition is subdivided into four hypotheses (Table 1) so that various temporal-logical relationships among primitives can be described. [sent-90, score-0.528]
</p><p>49 The constraint flow is a directed graph, whose vertices are combinations of quinary conditions of the primitives (Fig. [sent-91, score-0.545]
</p><p>50 When a scenario is given, the corresponding constraint flow is generated automatically by the scenario parsing technique described in Algorithm 1of [10]. [sent-93, score-0.389]
</p><p>51 The objective of event detection is to find the best feasible interpretation of a given video. [sent-94, score-0.676]
</p><p>52 Because a scenario and its constraint flow are equivalent representations to describe an event, we can generate feasible 222666888311  CBAD[ γ γ21 2] : ×r ×ra×ra ×fra×fr×fa×f BADC[ γ γ1 2 ] : :×r (ra )C×ora nstra×ifrantflo×wfr×fa×f Figure 1. [sent-97, score-0.339]
</p><p>53 (a) There are two initial vertices (red boxes) and two final vertices (blue boxes) in the constraint flow. [sent-100, score-0.199]
</p><p>54 (b) A flow traverse (T13) becomes a feasible interpretation (I13) by projecting quinary conditions to binary conditions; the projection is denoted by The time intervals of the primitives are marked black. [sent-101, score-0.603]
</p><p>55 The time interval of the target event is obtained by  β. [sent-102, score-0.709]
</p><p>56 searching for the first and last activations in the interpretation; in this example, the time interval of the target event is [2, 12]. [sent-103, score-0.739]
</p><p>57 Let Tt be a flow traverse up to time t and It = β(Tt) be the feasible interpretation obtained by the binary projection of Tt. [sent-106, score-0.261]
</p><p>58 Multi-agent event localization The event detector described in Sec. [sent-125, score-1.14]
</p><p>59 3 assumes that roles of all agents are given manually. [sent-126, score-0.529]
</p><p>60 We call a group of agents involved in a target event with assigned roles a true lineup, and there are typically many lineup candidates (i. [sent-127, score-1.495]
</p><p>61 Let m be the number of agents existing in a video, and n be the number of roles defined in a scenario. [sent-130, score-0.529]
</p><p>62 We assume that an agent takes part in at most one target event in a video and that the agents and the roles are bijective in a target event. [sent-131, score-1.626]
</p><p>63 It is impractical to  ×  apply event detection algorithm to all the candidates. [sent-135, score-0.584]
</p><p>64 We introduce a technique that efficiently localizes multiagent events by identifying true lineups in an optimization framework. [sent-136, score-0.2]
</p><p>65 Our method exploits role confidences and time intervals of agents, which are obtained by role-specific event detection per agent (Sec. [sent-137, score-1.365]
</p><p>66 Despite a huge number of potential lineups, we maintain efficiency by separating the event localization procedure into two steps: role estimation per group and group selection (Sec. [sent-140, score-0.98]
</p><p>67 Agent grouping  Suppose that there are n roles in a target event. [sent-146, score-0.367]
</p><p>68 We first form groups with n agents, which are the candidates to be applied to our event detection algorithm. [sent-147, score-0.66]
</p><p>69 An agent can be associated with multiple groups at this stage. [sent-148, score-0.372]
</p><p>70 lineup candidates, where l denotes the number of groups1 , because n! [sent-150, score-0.236]
</p><p>71 role configurations are available per group; it may be infeasible to identify few true lineups from such a large number of candidates. [sent-151, score-0.405]
</p><p>72 The groups are represented by an m l binary matrix A, where [A]i,k = 1 if the i-th agent belongs to the kth group, and 0 otherwise. [sent-152, score-0.372]
</p><p>73 Agent-wise role analysis The role assignment is essential for multi-agent event detection, but typically available after the completion of event detection. [sent-165, score-1.744]
</p><p>74 To estimate the role of an agent, we evaluate how appropriate the properties of the agent for each role are, and also estimate potential time interval of each role. [sent-166, score-0.959]
</p><p>75 In other words, we perform role-specific event detection for each agent; it is done by role-specific constraint flows, each of which focuses only on a single role. [sent-167, score-0.673]
</p><p>76 The role-specific constraint flow is constructed from the original constraint flow by simply excluding primitives not associated with the target role and merging duplicate vertices, as shown in Fig. [sent-168, score-1.011]
</p><p>77 We additionally construct the null constraint flow, which is generated by excluding all primitives (Fig. [sent-170, score-0.281]
</p><p>78 2(d)) to handle outsiders, agents that do not participate in any target events. [sent-171, score-0.399]
</p><p>79 For each agent in a video, we apply the role-specific event detectors to all primitive detections. [sent-172, score-0.977]
</p><p>80 groups in principle, but can reduce the search space significantly in practice by integrating spatio-temporal or temporallogical proximity constraint between agents as described in Sec. [sent-175, score-0.436]
</p><p>81 The scenario and its original constraint flow are the same with those of Fig. [sent-179, score-0.298]
</p><p>82 (a–b) Procedure for generating γ1-specific constraint flow: We first exclude primitives irrelevant to the role we currently focus on (red); some vertices become identical after the exclusion. [sent-181, score-0.56]
</p><p>83 The role-specific constraint flow is obtained by merging the duplicate vertices. [sent-182, score-0.265]
</p><p>84 (c) γ2-specific constraint flow is obtained in the same manner. [sent-183, score-0.207]
</p><p>85 (d) We additionally construct the null constraint flow to handle outsiders. [sent-184, score-0.233]
</p><p>86 the i-th agent is for the j-th role based on the corresponding maximum log-posterior probability; the normalized confidence is given by ? [sent-195, score-0.612]
</p><p>87 k=0  Also, the time interval where the i-th agent plays the jth role is obtained by searching for the first and last activations in β(T? [sent-201, score-0.714]
</p><p>88 Also, the confidences for the outsider role are concatenated to c0 = [c1,0, . [sent-224, score-0.4]
</p><p>89 In our framework, a lineup candidate is evaluated in terms of the role confidences and time intervals of the corresponding agent-role combination, which are precomputed in this role-specific event detection and can be reused to evaluate different lineup candidates. [sent-229, score-1.5]
</p><p>90 Also, lineup candidates with different lengths of occurrence (i. [sent-231, score-0.311]
</p><p>91 , different number of observations) can be evaluated fairly at the role level because role confidences are not affected by the length of occurrence due to the agent-wise normalization in Eq. [sent-233, score-0.657]
</p><p>92 The agent-wise role analysis is significantly faster than the na¨ ıve extension of the original event detection because role-specific event detection evaluates each role of each agent independently. [sent-235, score-2.055]
</p><p>93 However, interactions between roles are ignored in this step, which may result in erro-  T? [sent-236, score-0.269]
</p><p>94 Evaluating role assignment per group For each group, we evaluate the feasibility of the role assignment by considering how appropriate the agents in a group are for the assigned roles and how well their rolespecific time intervals satisfy the scenario constraints. [sent-243, score-1.681]
</p><p>95 , indicates the role assignment of the i-th member in a binary form, i. [sent-256, score-0.467]
</p><p>96 , = 1if the j-th role is assigned to the i-th member of the  xik,j  xik,n]? [sent-258, score-0.351]
</p><p>97 ck = is a coefficient vector of role confidences corresponding to the members for n roles. [sent-266, score-0.409]
</p><p>98 Lk is an n2 n2 coefficient matrix to measure the fidelity of the time intervals of the assigned roles to the scenario constraints:  wherL∞kn=×⎢⎡⎣i−s∞Lan2k. [sent-267, score-0.456]
</p><p>99 Specifically, the element of the matrix, [Lik1 ,i2]j1 ,j2 , is negatively proportional to the length of partial time-intervals violating the scenario constraints, between the j1-th role of the i1-th member and the j2-th role of the i2-th member, which is given by  Lik1,i2=−α·? [sent-274, score-0.717]
</p><p>100 Si and Ei are n n matrices whose columns are composed of sgk (i) and egk (i) , respectively, i. [sent-283, score-0.158]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.539), ('agent', 0.337), ('role', 0.275), ('roles', 0.269), ('agents', 0.26), ('lineup', 0.236), ('primitives', 0.141), ('flow', 0.118), ('assignment', 0.116), ('lineups', 0.105), ('primitive', 0.101), ('target', 0.098), ('intervals', 0.096), ('events', 0.095), ('scenario', 0.091), ('constraint', 0.089), ('rec', 0.087), ('egk', 0.079), ('holdobj', 0.079), ('quinary', 0.079), ('rolespecific', 0.079), ('sgk', 0.079), ('member', 0.076), ('confidences', 0.073), ('del', 0.072), ('interval', 0.072), ('delivery', 0.065), ('localization', 0.062), ('logic', 0.058), ('relationships', 0.057), ('vertices', 0.055), ('badc', 0.052), ('comeclose', 0.052), ('getaway', 0.052), ('getinto', 0.052), ('getoff', 0.052), ('inactive', 0.052), ('irfw', 0.052), ('outsider', 0.052), ('outsiders', 0.052), ('postech', 0.052), ('precedence', 0.052), ('temporallogical', 0.052), ('group', 0.052), ('interpretation', 0.051), ('traverse', 0.051), ('fa', 0.049), ('detection', 0.045), ('feasible', 0.041), ('candidates', 0.041), ('participate', 0.041), ('earlier', 0.041), ('atomic', 0.04), ('ra', 0.04), ('temporal', 0.039), ('members', 0.039), ('combinations', 0.037), ('duplicate', 0.037), ('groups', 0.035), ('abnormality', 0.035), ('occurrence', 0.034), ('gk', 0.034), ('korea', 0.033), ('start', 0.032), ('interpretations', 0.032), ('logical', 0.032), ('participants', 0.03), ('activations', 0.03), ('ending', 0.029), ('occurrences', 0.026), ('conditions', 0.026), ('null', 0.026), ('rs', 0.026), ('excluding', 0.025), ('video', 0.025), ('ei', 0.025), ('infeasible', 0.025), ('active', 0.024), ('tt', 0.024), ('obtai', 0.023), ('mest', 0.023), ('acd', 0.023), ('finished', 0.023), ('neous', 0.023), ('participation', 0.023), ('kwak', 0.023), ('andt', 0.023), ('wfr', 0.023), ('end', 0.023), ('ck', 0.022), ('managed', 0.022), ('nrf', 0.022), ('samsung', 0.022), ('orders', 0.021), ('merging', 0.021), ('programming', 0.021), ('scenarios', 0.021), ('waiting', 0.02), ('elementwise', 0.02), ('satisfaction', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="292-tfidf-1" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>2 0.35223502 <a title="292-tfidf-2" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>3 0.29548645 <a title="292-tfidf-3" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>4 0.218779 <a title="292-tfidf-4" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>5 0.19714576 <a title="292-tfidf-5" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>6 0.1581071 <a title="292-tfidf-6" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>7 0.1309067 <a title="292-tfidf-7" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>8 0.11916425 <a title="292-tfidf-8" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>9 0.088953212 <a title="292-tfidf-9" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>10 0.087271117 <a title="292-tfidf-10" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>11 0.074854195 <a title="292-tfidf-11" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>12 0.071043588 <a title="292-tfidf-12" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>13 0.06493064 <a title="292-tfidf-13" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>14 0.064677954 <a title="292-tfidf-14" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>15 0.063224815 <a title="292-tfidf-15" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>16 0.060113475 <a title="292-tfidf-16" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>17 0.056143094 <a title="292-tfidf-17" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>18 0.055611126 <a title="292-tfidf-18" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>19 0.055567488 <a title="292-tfidf-19" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>20 0.053105682 <a title="292-tfidf-20" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, -0.035), (2, -0.019), (3, -0.081), (4, -0.05), (5, 0.002), (6, -0.071), (7, -0.047), (8, -0.02), (9, 0.101), (10, 0.079), (11, -0.032), (12, 0.051), (13, -0.028), (14, -0.002), (15, 0.004), (16, 0.041), (17, 0.064), (18, 0.025), (19, -0.164), (20, -0.152), (21, -0.018), (22, -0.015), (23, -0.03), (24, 0.004), (25, 0.012), (26, 0.053), (27, -0.166), (28, -0.004), (29, -0.047), (30, 0.087), (31, -0.04), (32, 0.023), (33, -0.136), (34, -0.093), (35, 0.053), (36, 0.121), (37, 0.097), (38, 0.124), (39, 0.118), (40, 0.168), (41, -0.095), (42, -0.131), (43, -0.069), (44, 0.226), (45, 0.024), (46, -0.139), (47, -0.018), (48, 0.041), (49, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98183286 <a title="292-lsi-1" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>2 0.84978658 <a title="292-lsi-2" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>3 0.62209165 <a title="292-lsi-3" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>4 0.60754299 <a title="292-lsi-4" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>5 0.59327656 <a title="292-lsi-5" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>6 0.46616301 <a title="292-lsi-6" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>7 0.40552655 <a title="292-lsi-7" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>8 0.38946223 <a title="292-lsi-8" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>9 0.3704558 <a title="292-lsi-9" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>10 0.35974127 <a title="292-lsi-10" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>11 0.34819859 <a title="292-lsi-11" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>12 0.33874482 <a title="292-lsi-12" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>13 0.335706 <a title="292-lsi-13" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>14 0.31271592 <a title="292-lsi-14" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>15 0.29655907 <a title="292-lsi-15" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>16 0.28982449 <a title="292-lsi-16" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<p>17 0.28354278 <a title="292-lsi-17" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>18 0.27842215 <a title="292-lsi-18" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>19 0.26571897 <a title="292-lsi-19" href="./cvpr-2013-Towards_Efficient_and_Exact_MAP-Inference_for_Large_Scale_Discrete_Computer_Vision_Problems_via_Combinatorial_Optimization.html">436 cvpr-2013-Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization</a></p>
<p>20 0.26569036 <a title="292-lsi-20" href="./cvpr-2013-A_Comparative_Study_of_Modern_Inference_Techniques_for_Discrete_Energy_Minimization_Problems.html">6 cvpr-2013-A Comparative Study of Modern Inference Techniques for Discrete Energy Minimization Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.088), (13, 0.172), (16, 0.033), (26, 0.046), (33, 0.21), (39, 0.02), (67, 0.042), (69, 0.17), (77, 0.052), (87, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86428893 <a title="292-lda-1" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>2 0.8425979 <a title="292-lda-2" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>Author: Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: 3D volumetric reasoning is important for truly understanding a scene. Humans are able to both segment each object in an image, and perceive a rich 3D interpretation of the scene, e.g., the space an object occupies, which objects support other objects, and which objects would, if moved, cause other objects to fall. We propose a new approach for parsing RGB-D images using 3D block units for volumetric reasoning. The algorithm fits image segments with 3D blocks, and iteratively evaluates the scene based on block interaction properties. We produce a 3D representation of the scene based on jointly optimizing over segmentations, block fitting, supporting relations, and object stability. Our algorithm incorporates the intuition that a good 3D representation of the scene is the one that fits the data well, and is a stable, self-supporting (i.e., one that does not topple) arrangement of objects. We experiment on several datasets including controlled and real indoor scenarios. Results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation.</p><p>3 0.84029734 <a title="292-lda-3" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>4 0.82836568 <a title="292-lda-4" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>5 0.82821774 <a title="292-lda-5" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>6 0.82717025 <a title="292-lda-6" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>7 0.8261807 <a title="292-lda-7" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>8 0.80905849 <a title="292-lda-8" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>9 0.80495113 <a title="292-lda-9" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>10 0.79594541 <a title="292-lda-10" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>11 0.7869491 <a title="292-lda-11" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>12 0.77923906 <a title="292-lda-12" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>13 0.77571303 <a title="292-lda-13" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>14 0.76713115 <a title="292-lda-14" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>15 0.76530117 <a title="292-lda-15" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>16 0.7647199 <a title="292-lda-16" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>17 0.76417536 <a title="292-lda-17" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>18 0.76212025 <a title="292-lda-18" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>19 0.75671011 <a title="292-lda-19" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>20 0.75608265 <a title="292-lda-20" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
