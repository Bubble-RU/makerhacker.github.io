<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-72" href="#">cvpr2013-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</h1>
<br/><p>Source: <a title="cvpr-2013-72-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hou_Boundary_Detection_Benchmarking_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>Reference: <a title="cvpr-2013-72-reference" href="../cvpr2013_reference/cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract For an ill-posed problem like boundary detection, human labeled datasets play a critical role. [sent-6, score-0.47]
</p><p>2 Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. [sent-7, score-0.766]
</p><p>3 The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. [sent-8, score-0.377]
</p><p>4 In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. [sent-9, score-0.665]
</p><p>5 Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. [sent-10, score-0.459]
</p><p>6 In the development of boundary detection, datasets [16, 8, 5, 1] - along with their evaluation criteria1 - have played critical roles. [sent-16, score-0.332]
</p><p>7 Top figure shows the original image overlapping with all 6 boundary maps from labelers. [sent-22, score-0.356]
</p><p>8 Red circle gives an example boundary segment that is labeled by only one out of 6 labelers (labeler 4). [sent-24, score-0.732]
</p><p>9 Boundary segment in the orange circle is labeled by two labelers (labeler 3 and 4). [sent-25, score-0.4]
</p><p>10 The boundary segment in green circle is unanimously labeled by all 6 labelers. [sent-26, score-0.497]
</p><p>11 of each newly proposed algorithm, but also because the images, the labels, and the evaluation standards they set forth have heavily influenced the researchers during the development of a boundary detection algorithm. [sent-27, score-0.369]
</p><p>12 A universally accepted definition of a boundary may not exist. [sent-31, score-0.332]
</p><p>13 In today’s most popular benchmark BSDS 300 [16], 28 human labelers contributed a total number of 1667 high quality boundary maps on 300 images of natural scenes (200 training, 100 testing). [sent-33, score-0.75]
</p><p>14 However, the ill-posed nature of boundary detection makes this problem a different scenario. [sent-36, score-0.369]
</p><p>15 There is surprisingly little discussion about ground-truth data reliability for boundary detection. [sent-37, score-0.375]
</p><p>16 Examined separately, each boundary seems to be aligned to some underlying edge structure in the image. [sent-41, score-0.332]
</p><p>17 Even though different labelers may annotate boundaries in different levels of details, they are consistent in a sense that the dense labels “refine” the corresponding sparse labels without contradicting to them. [sent-45, score-0.765]
</p><p>18 It is possible that the labelers may miss some equally important boundaries. [sent-51, score-0.311]
</p><p>19 As for observation #2, the hierarchical organization of boundaries raises more fundamental questions: Can we give equal weights to the strong boundaries where everyone agrees, and the weak boundaries where only one or two labelers have noticed? [sent-53, score-0.967]
</p><p>20 The perceptual strength of a boundary In this paper, the perceptual strength of a boundary segment refers to the composite effect of all factors that influence personal decision during boundary annotation. [sent-58, score-1.828]
</p><p>21 One simple way to approximate the perceptual strength of each boundary segment is to take the proportion  of labelers who have labeled that specific segment. [sent-60, score-1.09]
</p><p>22 To get rid of local alignment noise, we match each pair of human boundary maps using the assignment algorithm proposed in [11], with the same parameter set [15] used for algorithm evaluation. [sent-61, score-0.43]
</p><p>23 For instance, given an image with N labelers, if a boundary pixel from one subject matches with M other labelers, it has a perceptual strength of MN+1 . [sent-62, score-0.724]
</p><p>24 The weakest boundary labels are the ones annotated by only one labeler. [sent-63, score-0.492]
</p><p>25 Clearly, the orphan labels and the consensus labels are not equal. [sent-69, score-0.883]
</p><p>26 A disappointing yet alarming result is that all of the 9 algorithms experience significant performance drops if we test them on strong boundaries only. [sent-75, score-0.336]
</p><p>27 Our analysis shows that none of the 9 algorithms is capable of discovering strong boundaries significantly better than random selection. [sent-79, score-0.292]
</p><p>28 The output values of the algorithms are either independent or weakly correlated with the perceptual strength. [sent-80, score-0.336]
</p><p>29 This result is in sharp contrast to many of today’s popular practice of using the output of a boundary detector algorithm as an informative feature in high-level boundary analysis. [sent-81, score-0.695]
</p><p>30 Related works Over the last 12 years, a great number of boundary detection algorithms have been proposed. [sent-88, score-0.407]
</p><p>31 222111222422  In this paper, we focus on 9 major boundary detection algorithms (shown in Tab. [sent-92, score-0.407]
</p><p>32 Over the past 10 years, it has been accepted as the most important  score to judge a boundary detector. [sent-96, score-0.358]
</p><p>33 Along with boundary detection, a parallel line of work [25, 27, 12] focuses on the detection of “salient boundaries”. [sent-97, score-0.369]
</p><p>34 These works emphasize on finding salient 1-D structures from the ensemble of line segments discovered by a boundary detector. [sent-98, score-0.417]
</p><p>35 Specifically, [24] has proposed strategies to estimate the quality of crowdsourced boundary annotation. [sent-109, score-0.382]
</p><p>36 A psychophysical experiment While collecting the human annotation, BSDS 300 [16] gave the following instructions to each of the labelers: Divide each image into pieces, where each piece represents a distinguished thing in the image. [sent-113, score-0.291]
</p><p>37 On the other hand, we also aware that the annotation of these orphan labels is due to a pure random assignment of labelers. [sent-123, score-0.614]
</p><p>38 In this section, we introduce a two-way forced choice paradigm to test the reliability of a boundary dataset. [sent-125, score-0.413]
</p><p>39 In each trial, a subject2 is asked to compare the relative perceptual strength of two local boundary segments with the following instruction: Boundaries divide each image into pieces, where each piece represents a distinguished thing in the image. [sent-126, score-0.843]
</p><p>40 Choose the relatively stronger boundary segment from the two candidates. [sent-127, score-0.38]
</p><p>41 One of the two boundary segments is chosen from the human label dataset, and the other is a boundary segment produced by an algorithm. [sent-128, score-0.828]
</p><p>42 Epasayris aonnd hard experiments for boundary comUsing different boundary sampling strategies, we can design two experiments: hard and easy. [sent-136, score-0.788]
</p><p>43 On the left image, two boundary segments (high contrast squares with red lines) are superimposed onto the original photo. [sent-141, score-0.399]
</p><p>44 The subject is asked to click on one of two boundary segments that she/he feels stronger. [sent-142, score-0.375]
</p><p>45 B) The Venn diagram of sets of boundary segments: The thick circle encompasses the full human labeled boundary set of the dataset. [sent-144, score-0.809]
</p><p>46 The subset of orphan labels is shown in the green area. [sent-145, score-0.54]
</p><p>47 The algorithm detected boundary set is the dotted ellipsoid. [sent-146, score-0.332]
</p><p>48 In each trial, we randomly select one boundary segment from the green area, and the other one from the red area. [sent-148, score-0.38]
</p><p>49 algorithm false alarms: Some example images with both human orphan labels (shown in green lines) and false alarms of PB algorithm (shown in red lines). [sent-151, score-0.9]
</p><p>50 In many examples, the relative strength between algorithm false alarm and human orphan labels is very hard to tell. [sent-152, score-0.994]
</p><p>51 find false alarms – boundary segments that are considered weaker than human labels. [sent-153, score-0.673]
</p><p>52 And then, for each testing image, we randomly draw one instance of algorithm false alarm, and compare it against another randomly selected human orphan label. [sent-154, score-0.537]
</p><p>53 This experiment is called “hard experiment” because  the relative order between human labeled orphan label and algorithm detected false alarms is not easy to determine (as one can see in Fig. [sent-157, score-0.787]
</p><p>54 First, we remove all the human labels that are not unanimously labeled by everyone. [sent-161, score-0.316]
</p><p>55 This leaves us with a very small but strong subset of labels (perceptual strength equals 1). [sent-162, score-0.372]
</p><p>56 Finally, the competition is made between strong human labels and confident output of algorithm false alarms. [sent-164, score-0.413]
</p><p>57 Ideally, a perfectly constructed dataset should have zero risk, because it does not miss any strong boundary segments, and algorithm false alarms are always weaker than any instance from the perfect boundary dataset. [sent-170, score-0.985]
</p><p>58 3 show that the BSDS 300 especially those orphan labels, are far away from being perfect. [sent-172, score-0.38]
</p><p>59 The first conclusion one can draw from this observation is rather depressing – the orphan labels are extremely unreliable since they falsely classify good algorithm detections into false alarms (or falsely include weak algorithm detections into hits, depending on the thresholds). [sent-185, score-0.903]
</p><p>60 No matter whether to choose the pessimistic or the optimistic perspective, it is clear that the orphan labels are not appropriate to serve as a benchmark or even parts of a benchmark. [sent-189, score-0.645]
</p><p>61 Instead, we should put more focus on the consensus boundaries because the risk is much lower. [sent-190, score-0.402]
</p><p>62 It is worth mentioning that our results on the easy experiment does not necessarily imply that the consensus boundaries is a perfect dataset. [sent-191, score-0.441]
</p><p>63 However, as long as the missed boundaries of consensus labels cannot be accurately detected by an algorithm, this data remains to be valid for a benchmark. [sent-192, score-0.508]
</p><p>64 F-measures and the precision bonus Given the fact that the orphan labels are unreliable, what role do those labels play in the benchmarking process? [sent-195, score-0.972]
</p><p>65 In this section, we show that the orphan labels can create a “precision bonus” during the calculation of the F-measure. [sent-197, score-0.54]
</p><p>66 In the original benchmarking protocol of BSDS 300, the false negative is defined by comparing each human boundary map with the thresholded algorithm map, and count the unmatched human labels. [sent-199, score-0.738]
</p><p>67 In comparison, the false posi-  tive is defined by comparing the algorithm map with all human maps, and then count the algorithm labels that are not matched by any human. [sent-200, score-0.349]
</p><p>68 In other words, the cost of each algorithm missing pixel is proportional to the human labelers who have detected that boundary, whereas the cost of each false alarm pixel is just one. [sent-201, score-0.537]
</p><p>69 This protocol exaggerates the importance of the orphan labels in the dataset, and encourages algorithms to play “safely” by enumerating an excessive number of boundary candidates. [sent-202, score-0.97]
</p><p>70 First we threshold the human labels by different perceptual strengths, from 0, 0. [sent-205, score-0.478]
</p><p>71 And then use each of these subset of the human labels as the ground-truth to benchmark all 9 algorithms. [sent-211, score-0.298]
</p><p>72 What makes today’s benchmarking practice questionable is the joint cause of the following facts: 1) weak boundaries in BSDS 300 are not reliable enough to evaluate today’s algorithms; and 2) precision bonus gives  Perceptual Strength  Perceptual Strength Figure 4. [sent-216, score-0.475]
</p><p>73 By increasing the perceptual strength, we transform the problem from “boundary detection” to “strong boundary detection”. [sent-218, score-0.577]
</p><p>74 extra credits to algorithms working on the low perceptual strength boundaries which according to fact 1, is not a good practice. [sent-222, score-0.595]
</p><p>75 Detecting strong boundaries The simplest way to avoid the problem of weak labels is to benchmark the algorithms using consensus labels only, as shown in Fig. [sent-224, score-0.901]
</p><p>76 However, the performances of the tested algorithms have dropped so significantly that it stimulates us to ask another question: are we detecting strong boundaries better than random? [sent-226, score-0.37]
</p><p>77 In this experiment, we crop out a part of each human boundary map to make the total number of pixels in the remaining 222111222755  Figure5. [sent-228, score-0.405]
</p><p>78 The partial label (bottom right figure) of this image is clearly  an  unrealistic ground-truth because the  majority of the bird boundary is discarded. [sent-232, score-0.356]
</p><p>79 map equals to that of a strong boundary map (see Fig. [sent-233, score-0.397]
</p><p>80 Except for cCut, all other algorithms have suffered severe performance decreases when shifting from detecting all labels to detecting consensus labels only. [sent-241, score-0.615]
</p><p>81 In this experiment, the salient boundary algorithm cCut has the most significant performance drop on partial labels. [sent-243, score-0.398]
</p><p>82 The comparative results of consensus and partial labels contradict our intuitions that algorithm detection strength is correlated with the perceptual strength of a boundary. [sent-245, score-0.965]
</p><p>83 It also questions the practices in computer vision that use boundary detector output as a feature for high-level visual tasks. [sent-246, score-0.398]
</p><p>84 For instance, intervene contour [ 14, 6] is a well-established method that computes the affinity of two points in the image by integrating the boundary strengths along the path that connects those two points. [sent-247, score-0.427]
</p><p>85 Many other works such as [21, 10, 3] also included pB (or gPB) boundary intensity in their feature design. [sent-248, score-0.332]
</p><p>86 To understand the relationship between algorithm output and the perceptual strength of a boundary, we further plot the perceptual strength distribution with respect to algorithm detector output for all 9 algorithms. [sent-249, score-0.846]
</p><p>87 7, we can see that the correlation between algorithm output and perceptual strength of the boundary is rather weak. [sent-251, score-0.755]
</p><p>88 Retrain on strong boundaries Another useful test to evaluate our current progress on strong boundary is to retrain an algorithm. [sent-254, score-0.667]
</p><p>89 If we extract one row with y = k in a sub-figure, the color strips represent the distribution of the human labels that are matched to all algorithm pixels where detection output is equal to k. [sent-317, score-0.333]
</p><p>90 Red area represents human labels with perceptual strength in [0, 0. [sent-318, score-0.625]
</p><p>91 Ideally, the gray area should have a upper triangular shape (XREN is the  closest) – that is, algorithm output being correlated with human perceptual strength. [sent-325, score-0.371]
</p><p>92 Retrain pB algorithm using consensus labels, and compare the results on original (all) and consensus (con) boundaries respectively. [sent-327, score-0.555]
</p><p>93 Using the publicly available MATLAB codes from the authors’ website, we re-generate the training samples with consensus boundaries and learn a new set of parameters. [sent-329, score-0.348]
</p><p>94 The retrained-pB does not gain superior F-measure even if we use consensus labels as the ground-truth. [sent-331, score-0.343]
</p><p>95 According to our analysis, the population of orphan and consensus labels of these 200 new images are 30. [sent-336, score-0.76]
</p><p>96 The optimal F-measure of these algorithms under all boundaries, or consensus boundaries are reported in Fig. [sent-342, score-0.386]
</p><p>97 The comparison is also made by either using original (all) boundaries or consensus (con) boundaries only. [sent-347, score-0.537]
</p><p>98 Discussion In this paper, we have raised doubts on the current way of benchmarking an algorithm on the most popular dataset of boundary detection (Further results are provided in the supplemental material). [sent-352, score-0.492]
</p><p>99 With a psychophysical experiment, we show that the weak, especially the orphan labels are not suitable for benchmarking algorithms. [sent-353, score-0.708]
</p><p>100 The validity of using boundary detector output to reveal high-level semantic information may not have a oneline answer. [sent-357, score-0.363]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bsds', 0.433), ('orphan', 0.38), ('boundary', 0.332), ('labelers', 0.28), ('perceptual', 0.245), ('consensus', 0.183), ('boundaries', 0.165), ('labels', 0.16), ('strength', 0.147), ('labeler', 0.14), ('alarms', 0.119), ('alarm', 0.1), ('benchmarking', 0.094), ('bonus', 0.093), ('ccut', 0.09), ('today', 0.087), ('false', 0.084), ('trial', 0.081), ('psychophysical', 0.074), ('human', 0.073), ('pb', 0.072), ('experiment', 0.068), ('strong', 0.065), ('weak', 0.065), ('benchmark', 0.065), ('conference', 0.063), ('contour', 0.06), ('precision', 0.058), ('gpb', 0.056), ('scg', 0.056), ('thresholds', 0.054), ('risk', 0.054), ('crowdsourced', 0.05), ('instruction', 0.05), ('hard', 0.05), ('annotation', 0.049), ('pages', 0.049), ('segment', 0.048), ('alarming', 0.045), ('pitfalls', 0.045), ('turing', 0.045), ('unanimously', 0.045), ('xiaodi', 0.045), ('caltech', 0.044), ('segments', 0.043), ('reliability', 0.043), ('salient', 0.042), ('dropped', 0.041), ('retrain', 0.04), ('optimistic', 0.04), ('fmeasure', 0.04), ('bubble', 0.04), ('pasadena', 0.04), ('ieee', 0.039), ('labeled', 0.038), ('forced', 0.038), ('algorithms', 0.038), ('population', 0.037), ('detection', 0.037), ('detecting', 0.037), ('fowlkes', 0.037), ('pieces', 0.035), ('strengths', 0.035), ('questions', 0.035), ('organization', 0.034), ('circle', 0.034), ('protocol', 0.033), ('risks', 0.033), ('falsely', 0.032), ('matched', 0.032), ('output', 0.031), ('unreliable', 0.031), ('miss', 0.031), ('raised', 0.029), ('martin', 0.029), ('everyone', 0.028), ('play', 0.027), ('thing', 0.027), ('trust', 0.026), ('judge', 0.026), ('piece', 0.026), ('assignment', 0.025), ('koch', 0.025), ('thresholded', 0.025), ('hou', 0.025), ('easy', 0.025), ('original', 0.024), ('design', 0.024), ('performances', 0.024), ('partial', 0.024), ('none', 0.024), ('yet', 0.023), ('distinguished', 0.023), ('people', 0.023), ('mc', 0.023), ('neural', 0.023), ('chance', 0.022), ('interpret', 0.022), ('weaker', 0.022), ('correlated', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="72-tfidf-1" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>2 0.194493 <a title="72-tfidf-2" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>Author: Camillo Jose Taylor</p><p>Abstract: In this paper we explore approaches to accelerating segmentation and edge detection algorithms based on the gPb framework. The paper characterizes the performance of a simple but effective edge detection scheme which can be computed rapidly and offers performance that is competitive with the pB detector. The paper also describes an approach for computing a reduced order normalized cut that captures the essential features of the original problem but can be computed in less than half a second on a standard computing platform.</p><p>3 0.12335005 <a title="72-tfidf-3" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>4 0.11194326 <a title="72-tfidf-4" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>Author: Vittal Premachandran, Ramakrishna Kakarala</p><p>Abstract: Propagating similarity information along the data manifold requires careful selection of local neighborhood. Selecting a “good” neighborhood in an unsupervised setting, given an affinity graph, has been a difficult task. The most common way to select a local neighborhood has been to use the k-nearest neighborhood (k-NN) selection criterion. However, it has the tendency to include noisy edges. In this paper, we propose a way to select a robust neighborhood using the consensus of multiple rounds of k-NNs. We explain how using consensus information can give better control over neighborhood selection. We also explain in detail the problems with another recently proposed neighborhood selection criteria, i.e., Dominant Neighbors, and show that our method is immune to those problems. Finally, we show the results from experiments in which we compare our method to other neighborhood selection approaches. The results corroborate our claims that consensus ofk-NNs does indeed help in selecting more robust and stable localities.</p><p>5 0.10308036 <a title="72-tfidf-5" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>Author: Zhile Ren, Gregory Shakhnarovich</p><p>Abstract: We propose a hierarchical segmentation algorithm that starts with a very fine oversegmentation and gradually merges regions using a cascade of boundary classifiers. This approach allows the weights of region and boundary features to adapt to the segmentation scale at which they are applied. The stages of the cascade are trained sequentially, with asymetric loss to maximize boundary recall. On six segmentation data sets, our algorithm achieves best performance under most region-quality measures, and does it with fewer segments than the prior work. Our algorithm is also highly competitive in a dense oversegmentation (superpixel) regime under boundary-based measures.</p><p>6 0.10260466 <a title="72-tfidf-6" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>7 0.098365866 <a title="72-tfidf-7" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>8 0.08596348 <a title="72-tfidf-8" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>9 0.08521951 <a title="72-tfidf-9" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>10 0.073175102 <a title="72-tfidf-10" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>11 0.068441778 <a title="72-tfidf-11" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>12 0.060988121 <a title="72-tfidf-12" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>13 0.060183924 <a title="72-tfidf-13" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>14 0.059232309 <a title="72-tfidf-14" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>15 0.058334719 <a title="72-tfidf-15" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>16 0.057134058 <a title="72-tfidf-16" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>17 0.056733873 <a title="72-tfidf-17" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>18 0.055847824 <a title="72-tfidf-18" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>19 0.055085231 <a title="72-tfidf-19" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>20 0.054826993 <a title="72-tfidf-20" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, -0.014), (2, 0.045), (3, 0.007), (4, 0.055), (5, 0.007), (6, 0.009), (7, 0.026), (8, -0.032), (9, -0.005), (10, 0.057), (11, -0.049), (12, 0.001), (13, -0.03), (14, 0.012), (15, -0.013), (16, 0.007), (17, -0.035), (18, 0.003), (19, 0.048), (20, -0.018), (21, 0.091), (22, -0.084), (23, 0.003), (24, 0.009), (25, 0.065), (26, 0.071), (27, -0.039), (28, 0.015), (29, 0.063), (30, 0.041), (31, 0.071), (32, -0.073), (33, 0.071), (34, 0.114), (35, 0.063), (36, 0.021), (37, 0.098), (38, -0.032), (39, 0.07), (40, 0.011), (41, 0.047), (42, -0.01), (43, 0.078), (44, -0.009), (45, 0.047), (46, 0.014), (47, -0.039), (48, 0.051), (49, 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94508672 <a title="72-lsi-1" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>2 0.84456754 <a title="72-lsi-2" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>Author: Yansheng Ming, Hongdong Li, Xuming He</p><p>Abstract: This paper aims to extract salient closed contours from an image. For this vision task, both region segmentation cues (e.g. color/texture homogeneity) and boundary detection cues (e.g. local contrast, edge continuity and contour closure) play important and complementary roles. In this paper we show how to combine both cues in a unified framework. The main focus is given to how to maintain the consistency (compatibility) between the region cues and the boundary cues. To this ends, we introduce the use of winding number–a well-known concept in topology–as a powerful mathematical device. By this device, the region-boundary consistency is represented as a set of simple linear relationships. Our method is applied to the figure-ground segmentation problem. The experiments show clearly improved results.</p><p>3 0.83361787 <a title="72-lsi-3" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>4 0.8146916 <a title="72-lsi-4" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>5 0.79393834 <a title="72-lsi-5" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>Author: Camillo Jose Taylor</p><p>Abstract: In this paper we explore approaches to accelerating segmentation and edge detection algorithms based on the gPb framework. The paper characterizes the performance of a simple but effective edge detection scheme which can be computed rapidly and offers performance that is competitive with the pB detector. The paper also describes an approach for computing a reduced order normalized cut that captures the essential features of the original problem but can be computed in less than half a second on a standard computing platform.</p><p>6 0.73273909 <a title="72-lsi-6" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>7 0.668441 <a title="72-lsi-7" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>8 0.64189923 <a title="72-lsi-8" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>9 0.6180895 <a title="72-lsi-9" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>10 0.61449176 <a title="72-lsi-10" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>11 0.59228146 <a title="72-lsi-11" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>12 0.59003699 <a title="72-lsi-12" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>13 0.57490879 <a title="72-lsi-13" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>14 0.53547782 <a title="72-lsi-14" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>15 0.53357631 <a title="72-lsi-15" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>16 0.51255488 <a title="72-lsi-16" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>17 0.5113188 <a title="72-lsi-17" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>18 0.50106347 <a title="72-lsi-18" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<p>19 0.49945104 <a title="72-lsi-19" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>20 0.49842882 <a title="72-lsi-20" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.104), (16, 0.036), (26, 0.043), (33, 0.222), (61, 0.186), (67, 0.093), (69, 0.041), (72, 0.019), (87, 0.136)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89090502 <a title="72-lda-1" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>same-paper 2 0.85640061 <a title="72-lda-2" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>3 0.84744835 <a title="72-lda-3" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>Author: Hao Jiang, Jianxiong Xiao</p><p>Abstract: We propose a novel linear method to match cuboids in indoor scenes using RGBD images from Kinect. Beyond depth maps, these cuboids reveal important structures of a scene. Instead of directly fitting cuboids to 3D data, we first construct cuboid candidates using superpixel pairs on a RGBD image, and then we optimize the configuration of the cuboids to satisfy the global structure constraints. The optimal configuration has low local matching costs, small object intersection and occlusion, and the cuboids tend to project to a large region in the image; the number of cuboids is optimized simultaneously. We formulate the multiple cuboid matching problem as a mixed integer linear program and solve the optimization efficiently with a branch and bound method. The optimization guarantees the global optimal solution. Our experiments on the Kinect RGBD images of a variety of indoor scenes show that our proposed method is efficient, accurate and robust against object appearance variations, occlusions and strong clutter.</p><p>4 0.81941903 <a title="72-lda-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.81882018 <a title="72-lda-5" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>6 0.81678712 <a title="72-lda-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.81445742 <a title="72-lda-7" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>8 0.81225628 <a title="72-lda-8" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>9 0.81185383 <a title="72-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.81150448 <a title="72-lda-10" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>11 0.80831307 <a title="72-lda-11" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>12 0.80826777 <a title="72-lda-12" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>13 0.80810833 <a title="72-lda-13" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>14 0.8080135 <a title="72-lda-14" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>15 0.80413473 <a title="72-lda-15" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>16 0.80327356 <a title="72-lda-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.80320221 <a title="72-lda-17" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>18 0.80232197 <a title="72-lda-18" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>19 0.8020463 <a title="72-lda-19" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>20 0.80201012 <a title="72-lda-20" href="./cvpr-2013-Bilinear_Programming_for_Human_Activity_Recognition_with_Unknown_MRF_Graphs.html">62 cvpr-2013-Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
