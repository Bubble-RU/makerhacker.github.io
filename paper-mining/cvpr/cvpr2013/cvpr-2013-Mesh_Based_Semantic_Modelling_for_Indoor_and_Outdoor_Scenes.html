<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-284" href="#">cvpr2013-284</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</h1>
<br/><p>Source: <a title="cvpr-2013-284-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Valentin_Mesh_Based_Semantic_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Julien P.C. Valentin, Sunando Sengupta, Jonathan Warrell, Ali Shahrokni, Philip H.S. Torr</p><p>Abstract: Semantic reconstruction of a scene is important for a variety of applications such as 3D modelling, object recognition and autonomous robotic navigation. However, most object labelling methods work in the image domain and fail to capture the information present in 3D space. In this work we propose a principled way to generate object labelling in 3D. Our method builds a triangulated meshed representation of the scene from multiple depth estimates. We then define a CRF over this mesh, which is able to capture the consistency of geometric properties of the objects present in the scene. In this framework, we are able to generate object hypotheses by combining information from multiple sources: geometric properties (from the 3D mesh), and appearance properties (from images). We demonstrate the robustness of our framework in both indoor and outdoor scenes. For indoor scenes we created an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. For outdoor scenes, we created ground truth object labellings for the KITTI odometry dataset (stereo image sequence). We observe a signifi- cant speed-up in the inference stage by performing labelling on the mesh, and additionally achieve higher accuracies.</p><p>Reference: <a title="cvpr-2013-284-reference" href="../cvpr2013_reference/cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, most object labelling methods work in the image domain and fail to capture the information present in 3D space. [sent-13, score-0.352]
</p><p>2 In this work we propose a principled way to generate object labelling in 3D. [sent-14, score-0.307]
</p><p>3 Our method builds a triangulated meshed representation of the scene from multiple depth estimates. [sent-15, score-0.365]
</p><p>4 We demonstrate the robustness of our framework in both indoor and outdoor scenes. [sent-18, score-0.45]
</p><p>5 For indoor scenes we created an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. [sent-19, score-0.978]
</p><p>6 For outdoor scenes, we created ground truth object labellings for the KITTI odometry dataset (stereo image sequence). [sent-20, score-0.375]
</p><p>7 We observe a signifi-  cant speed-up in the inference stage by performing labelling on the mesh, and additionally achieve higher accuracies. [sent-21, score-0.439]
</p><p>8 In our approach, the scene is reconstructed in the form of a mesh, computed from a sequence of depth estimates, and is annotated with semantic object labels. [sent-24, score-0.48]
</p><p>9 This provides a more consistent approach to scene segmentation compared with independent labelling of a sequence of images. [sent-25, score-0.51]
</p><p>10 This form of semantically annotated 3D representation is necessary to allow robotic platforms to understand, interact and navigate in a structured indoor environment [2, 12, 26] or outdoor scenes [1, 30, 7]. [sent-27, score-0.615]
</p><p>11 Our labelling ∗ 3 The authors  assert  equal contribution and joint first authorship. [sent-28, score-0.307]
</p><p>12 For seman-  tic indoor scene reconstruction we use images from RGB-D sensors, and for the outdoor road scenes, we use a sequence of stereo images. [sent-32, score-0.909]
</p><p>13 The problem of semantic object labelling has been studied extensively and has seen major improvements [16, 23, 29]. [sent-33, score-0.409]
</p><p>14 The advent of inexpensive depth sensors has significantly encouraged developments in scene reconstruction using streams of depth and RGB data [18, 19]. [sent-38, score-0.407]
</p><p>15 Large-scale stereo image sequences have also been used to generate dense reconstructions of urban road scenes [10]. [sent-39, score-0.374]
</p><p>16 In most cases, the scene is reconstructed as a mesh [19]. [sent-40, score-0.7]
</p><p>17 Recently, [15] proposed combining both visual and geometric cues for labelling indoor scene point clouds, but they  use a restricted learning framework and an inefficient (slow) inference method. [sent-43, score-0.862]
</p><p>18 Similarly, an attempt to label indoor scene images captured using RGB-D sensor has been made in [24] where a classifier was trained in the image domain, along with a 3D distance prior to aid their classification. [sent-45, score-0.416]
</p><p>19 This was extended in [25] where object surface and support relations of an indoor scene were used as a prior to perform indoor scene segmentation. [sent-46, score-0.748]
</p><p>20 For outdoor scene labelling, most of the work has concentrated on classification in the image domain [27, 4, 17] or using a coarse level 3D interpretation [8]. [sent-48, score-0.33]
</p><p>21 In [22] a semantic 3D reconstruction is generated but the object labelling is performed in the image domain, and then projected to the model. [sent-49, score-0.483]
</p><p>22 In this work, we tackle the problem of semantic scene reconstruction (for both indoor and outdoor scenes) in 3D space by combining both structural and appearance cues. [sent-51, score-0.717]
</p><p>23 Our approach to semantic mesh segmentation is illustrated in Fig. [sent-52, score-0.672]
</p><p>24 The depth estimates are used to generate a triangulated mesh representation of the scene [20, 3 1], enabling us to capture its inherent geometry. [sent-55, score-0.823]
</p><p>25 We propose a cascaded classifier to learn the geometric  cues from the mesh, and appearance cues from images in an efficient learning framework [23]. [sent-56, score-0.266]
</p><p>26 For this purpose, we augment the NYU depth dataset [24] with semantically annotated ground truth meshes for training and evaluation purposes. [sent-57, score-0.433]
</p><p>27 Furthermore we solve the labelling problem in 3D by defining a conditional random field (CRF) over the scene mesh, effectively exploiting the scene geometry. [sent-58, score-0.489]
</p><p>28 As a result, we achieve a significant speedup at the inference stage (20× for outdoor and 1000× for indoor scenes) in comparison tro othuted mooerth aondds 1o0f0 [02×4, 1fo5r, 1n6d]o. [sent-59, score-0.629]
</p><p>29 Fro src eonuetdso)o irn scene labelling, we use the stereo image sequence of the KITTI odometry dataset [9], for which we generated ground truth  Figure 2. [sent-60, score-0.419]
</p><p>30 In summary, our contributions are: • The formulation of the labelling problem in the 3D space resulting inn a significant speedup ilne mthe i ninf tehreen 3cDe stage: §2. [sent-65, score-0.354]
</p><p>31 For indoor scenes, the augmentation of the NYU dFoartas ientd wooitrh ground tr thueth mauegsmheesn ttaot ifoancili otfat eth learning on the mesh. [sent-67, score-0.301]
</p><p>32 For outdoor scenes, the creation of a perpixel object class labelling for the KITTI dataset1 : §4. [sent-68, score-0.535]
</p><p>33 Semantic labelling of the mesh based scene representation To generate a semantically annotated meshed representation of a scene, we use a sequence of depth estimates along with images describing the scene. [sent-70, score-1.329]
</p><p>34 Geometric features are computed on the mesh directly; visual features are computed on the images and then projected to the corresponding faces of the mesh. [sent-72, score-0.569]
</p><p>35 A graph homomorphic to the mesh is then built with the appropriate potentials. [sent-73, score-0.529]
</p><p>36 Finally, an approximate MAP inference on the CRF is solved which gives us the labelling of the mesh. [sent-74, score-0.387]
</p><p>37 Mesh Estimation To estimate a meshed representation of the scene we use a sequence of depth estimates which are obtained from a depth sensor (providing a stream of RGB-D images) for indoor scenes and stereo image pairs for outdoor scenes. [sent-78, score-1.174]
</p><p>38 The depth estimates are incrementally fused into a single 3D reconstruction using the volumetric TSDF representation [6]. [sent-79, score-0.22]
</p><p>39 The representation allows for the efficient combination of multiple noisy surface measurements, obtained from different depth maps by averaging signed the distance measures from every depth map at each voxel. [sent-85, score-0.315]
</p><p>40 A triangulated mesh corresponding to the zero valued iso-surface is extracted using [20, 3 1]. [sent-89, score-0.618]
</p><p>41 CRF energy model We use a Conditional Random Field (CRF) based approach, defined over a mesh structure, to perform the semantic labelling of the mesh. [sent-94, score-0.968]
</p><p>42 s aA v labelling x a re pfreer-sd tefoi any possible assignment of label}s. [sent-102, score-0.307]
</p><p>43 Each mesh face iis registered to a set of images through the camera projection matrix, giving a set τi ⊂ P of image pixels for each mesh face where P is the set ⊂of Pall o othf eim image pixels. [sent-113, score-1.185]
</p><p>44 The corresponding Gibbs energy of the mesh is given as:  =  E(x) ? [sent-116, score-0.559]
</p><p>45 ,j∈Ni  (1)  The most probable or maximum a posteriori labelling x∗ of the CRF corresponds to the minimum energy of the graph. [sent-120, score-0.337]
</p><p>46 The energy minimisation problem is solved using the graph-  cut based alpha expansion algorithm [3], giving us the labelling of the mesh. [sent-121, score-0.37]
</p><p>47 Unary potential The unary potential ψi describes the cost of a mesh face Fi taking a particular object class label. [sent-122, score-0.77]
</p><p>48 From that step, we extract geometric features and geometric contextual features. [sent-150, score-0.268]
</p><p>49 We are then able to optimise the learning on the mesh using that set of graphs and z as input into algorithm 1. [sent-154, score-0.529]
</p><p>50 Experiments  To demonstrate the effectiveness ofour proposed system, we have evaluated it using indoor scenes on the NYU depth dataset [24] augmented with meshes, and for outdoor urban road scenes, we used the publicly available KITTI dataset [9]. [sent-162, score-0.861]
</p><p>51 Reconstruction of indoor scene Dataset The NYU dataset [24] comprises indoor scene video sequences captured using Microsoft Kinect. [sent-165, score-0.753]
</p><p>52 We enhance the NYU indoor dataset [24] with labelled meshes for a representative subset of the total number of scenes, which we use for training and evaluation purposes. [sent-168, score-0.501]
</p><p>53 We then project each ground truth pixel onto the corresponding face in the mesh and increase the vote count for the label associated with that pixel by one. [sent-172, score-0.699]
</p><p>54 Each of these  meshes have associated ground truth labelled images. [sent-177, score-0.341]
</p><p>55 We use 23 mesh scenes (370 ground truth images) for training and 10 (221 ground truth images) for testing, and train two stages for each cascade. [sent-178, score-0.846]
</p><p>56 3 shows the qualitative results on the indoor scenes from the NYU dataset [24], where the semantically segmented mesh is shown along with the corresponding images. [sent-180, score-0.92]
</p><p>57 In the scene, we also see that the bookshelf is classified as cabinet, which has similar geometric and appearance properties. [sent-184, score-0.221]
</p><p>58 3(b) shows the semantic mesh corresponding to a living room. [sent-186, score-0.699]
</p><p>59 3(c) a kitchen scene is shown where the kitchen cabinet and the window are correctly classified in the mesh. [sent-190, score-0.365]
</p><p>60 Table 1compares the accuracy of our method with previous approaches, where we report the percentage of correctly labelled mesh faces. [sent-192, score-0.617]
</p><p>61 We also note that for our method, we report scores obtained in the mesh domain only as noisy camera estimations lead to inferior results when back-projecting labelling results from the mesh domain to the image domain. [sent-196, score-1.504]
</p><p>62 Furthermore, we are comparing the labelling of meshes rather than using images as is done in [24]. [sent-200, score-0.464]
</p><p>63 We also test the effect of the contextual features in our method for indoor scenes. [sent-201, score-0.33]
</p><p>64 In the first stage, the contribution of image level unary features is around 54%, but in the second stage of C3, it is reduced to 35% and the share of geometric and contextual features is increased. [sent-205, score-0.313]
</p><p>65 Timing wise, inference on meshes is computationally more efficient than inference on images as we do not have to perform a per-pixel labelling for all the images describ-  ×  ing the scene. [sent-207, score-0.624]
</p><p>66 Our test set comprises 10 meshed scenes with 221 associated ground truth object labelled images, each of size 640 480. [sent-209, score-0.445]
</p><p>67 The figure shows the percentage of the correctly classified mesh triangles. [sent-221, score-0.586]
</p><p>68 The second column presents our results with the unary potential only and the third column correspons to the results we get with both the unary and pairwise potentials. [sent-222, score-0.261]
</p><p>69 Reconstruction of outdoor scene Outdoor dataset  For the evaluation of our method on  outdoor scenes, we use the KITTI odometry dataset [9]. [sent-229, score-0.532]
</p><p>70 Due to lack of sufficient training data, we do not have ground truth meshes to learn geometric cues. [sent-234, score-0.35]
</p><p>71 Hence we use only the appearance cues from images and a single cascade to perform semantic mesh labelling. [sent-235, score-0.662]
</p><p>72 For reconstruction from a stereo image sequence, we need to estimate the camera poses. [sent-236, score-0.231]
</p><p>73 As the road scene images are subject to high glare, reflection and strong shadows, it is important to generate good feature matches for accurate camera estimation. [sent-242, score-0.255]
</p><p>74 The estimated camera poses are used to fuse the depth estimates (computed from stereo disparity) in an approach similar to [19]. [sent-247, score-0.303]
</p><p>75 Finally, surface reconstruction is performed using the marching tetrahedrons algorithm [20] which extracts a triangulated mesh corresponding to the zero valued iso-surface. [sent-248, score-0.746]
</p><p>76 4 demonstrates the qualitative results of our approach for the outdoor sequence from the KITTI dataset [9]. [sent-250, score-0.265]
</p><p>77 The scene is reconstructed from 150 stereo image pairs taken from the moving vehicle. [sent-251, score-0.279]
</p><p>78 Semantic mesh labelling for an indoor scene [24] along with the images corresponding to the scene. [sent-268, score-1.183]
</p><p>79 (b) Living room sequence: the scene shows the bookshelf (correctly classified) and television that is misclassified as picture. [sent-271, score-0.25]
</p><p>80 The right column shows the sequence of stereo images that are used for reconstruction purposes. [sent-275, score-0.253]
</p><p>81 In all the above cases, we can see the effectiveness of our method in handling large scale stereo data sequence to generate a semantic model of the scene. [sent-283, score-0.281]
</p><p>82 Evaluation is performed by projecting the ground truth labels into the mesh using the estimated camera poses, and taking a vote on the maximally occurring ground truth label for a particular mesh location. [sent-285, score-1.334]
</p><p>83 ‘Global’ refers to the overall percentage of the mesh correctly classified, and ‘Average’ is the average of the per class measures. [sent-286, score-0.563]
</p><p>84 The results from [16] are projected onto the mesh in a similar fashion to the ground truth labels. [sent-288, score-0.625]
</p><p>85 This can be attributed to the fact that the pairwise connections in the 3D mesh respect the structure of the objects, while in the image domain, the connections between adjacent pixels might violate the occlusion boundaries. [sent-291, score-0.629]
</p><p>86 We also evaluate the timing performance for the inference stage for outdoor scene reconstruction. [sent-292, score-0.458]
</p><p>87 We compare our mesh based inference with the image level inference of 222000777200  ALE (1I1m7a7g. [sent-293, score-0.689]
</p><p>88 As we are trying to reconstruct an outdoor scene w81h ×ich 3 spans h wuen adrreed trs oinfg meters, our rcetc aonn osturutdcoteodr mesh has around 704K vertices and 1. [sent-299, score-0.814]
</p><p>89 It is worth noting that our method needs to estimate the mesh to perform the inference. [sent-302, score-0.529]
</p><p>90 Conclusions and Future work We have presented an efficient framework to perform 3D semantic modelling applicable to both indoor and outdoor scenes. [sent-305, score-0.593]
</p><p>91 Further, we proposed a cascaded training framework to combine information from multiple sources: geometric properties (from 3D mesh) and appearance properties (from images). [sent-307, score-0.236]
</p><p>92 To facilitate the training/evaluation of our model, we have augmented the NYU indoor scene datasets with ground truth labelled meshes and KITTI outdoor scene dataset with object class labelling. [sent-308, score-1.046]
</p><p>93 The augmented datasets will be useful for understanding both indoor and outdoor scenes. [sent-309, score-0.489]
</p><p>94 Finally we demonstrate substantial improvement in the inference speed (20-1000× ) and achieve higher accuracy for both indoor (a2n0d- o1u00td0o×or) scenes. [sent-311, score-0.336]
</p><p>95 We would like to incorporate higher order terms in the CRF formulation, forcing similar and neighbouring triangles in the mesh to take the same label and investigate the joint optimisation of object labels and the 3D structure of the scene. [sent-313, score-0.646]
</p><p>96 Depth camera based indoor mobile robot localization and navigation. [sent-322, score-0.305]
</p><p>97 RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments. [sent-391, score-0.362]
</p><p>98 Semantic labeling of 3d point clouds for indoor scenes, 2011. [sent-412, score-0.256]
</p><p>99 Joint optimisation for object class segmentation and dense stereo reconstruction. [sent-431, score-0.224]
</p><p>100 Closeup view of reconstructed semantic model of an urban sequence from KITTI dataset. [sent-455, score-0.313]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mesh', 0.529), ('labelling', 0.307), ('indoor', 0.256), ('outdoor', 0.194), ('nyu', 0.168), ('kitti', 0.161), ('meshes', 0.157), ('road', 0.115), ('meshed', 0.111), ('stereo', 0.108), ('depth', 0.106), ('semantic', 0.102), ('jointboost', 0.1), ('cabinet', 0.097), ('geometric', 0.097), ('scenes', 0.091), ('scene', 0.091), ('unary', 0.09), ('tsdf', 0.089), ('labelled', 0.088), ('crf', 0.084), ('reconstructed', 0.08), ('inference', 0.08), ('pavement', 0.075), ('reconstruction', 0.074), ('contextual', 0.074), ('cascaded', 0.073), ('sequence', 0.071), ('living', 0.068), ('bookshelf', 0.067), ('urban', 0.06), ('kitchen', 0.06), ('comprises', 0.059), ('triangulated', 0.057), ('classified', 0.057), ('fence', 0.056), ('surface', 0.054), ('textonboost', 0.054), ('zi', 0.054), ('odometry', 0.053), ('stage', 0.052), ('truth', 0.051), ('encircled', 0.05), ('fsetaagtuere', 0.05), ('hedge', 0.05), ('colour', 0.05), ('television', 0.05), ('camera', 0.049), ('signed', 0.049), ('arrow', 0.048), ('speedup', 0.047), ('ale', 0.047), ('ground', 0.045), ('domain', 0.045), ('shahrokni', 0.044), ('stereoscan', 0.044), ('semantically', 0.044), ('geiger', 0.043), ('room', 0.042), ('pairwise', 0.042), ('modelling', 0.041), ('neighbouring', 0.041), ('meshing', 0.041), ('ziegler', 0.041), ('segmentation', 0.041), ('rgbd', 0.041), ('timing', 0.041), ('optimisation', 0.041), ('estimates', 0.04), ('faces', 0.04), ('potential', 0.039), ('augmented', 0.039), ('sengupta', 0.039), ('aries', 0.039), ('face', 0.039), ('bundle', 0.038), ('arrows', 0.037), ('icra', 0.036), ('ladicky', 0.036), ('floor', 0.035), ('label', 0.035), ('class', 0.034), ('stages', 0.034), ('classifier', 0.034), ('properties', 0.033), ('alpha', 0.033), ('valued', 0.032), ('labellings', 0.032), ('zm', 0.031), ('cues', 0.031), ('iros', 0.031), ('neighbours', 0.03), ('advent', 0.03), ('newcombe', 0.03), ('energy', 0.03), ('annotated', 0.03), ('connections', 0.029), ('ceiling', 0.029), ('silberman', 0.029), ('neighbourhood', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="284-tfidf-1" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>Author: Julien P.C. Valentin, Sunando Sengupta, Jonathan Warrell, Ali Shahrokni, Philip H.S. Torr</p><p>Abstract: Semantic reconstruction of a scene is important for a variety of applications such as 3D modelling, object recognition and autonomous robotic navigation. However, most object labelling methods work in the image domain and fail to capture the information present in 3D space. In this work we propose a principled way to generate object labelling in 3D. Our method builds a triangulated meshed representation of the scene from multiple depth estimates. We then define a CRF over this mesh, which is able to capture the consistency of geometric properties of the objects present in the scene. In this framework, we are able to generate object hypotheses by combining information from multiple sources: geometric properties (from the 3D mesh), and appearance properties (from images). We demonstrate the robustness of our framework in both indoor and outdoor scenes. For indoor scenes we created an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. For outdoor scenes, we created ground truth object labellings for the KITTI odometry dataset (stereo image sequence). We observe a signifi- cant speed-up in the inference stage by performing labelling on the mesh, and additionally achieve higher accuracies.</p><p>2 0.40880704 <a title="284-tfidf-2" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>Author: Wenbin Li, Darren Cosker, Matthew Brown, Rui Tang</p><p>Abstract: In this paper we present a novel non-rigid optical flow algorithm for dense image correspondence and non-rigid registration. The algorithm uses a unique Laplacian Mesh Energy term to encourage local smoothness whilst simultaneously preserving non-rigid deformation. Laplacian deformation approaches have become popular in graphics research as they enable mesh deformations to preserve local surface shape. In this work we propose a novel Laplacian Mesh Energy formula to ensure such sensible local deformations between image pairs. We express this wholly within the optical flow optimization, and show its application in a novel coarse-to-fine pyramidal approach. Our algorithm achieves the state-of-the-art performance in all trials on the Garg et al. dataset, and top tier performance on the Middlebury evaluation.</p><p>3 0.22907162 <a title="284-tfidf-3" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>Author: Gautam Singh, Jana Kosecka</p><p>Abstract: This paper presents a nonparametric approach to semantic parsing using small patches and simple gradient, color and location features. We learn the relevance of individual feature channels at test time using a locally adaptive distance metric. To further improve the accuracy of the nonparametric approach, we examine the importance of the retrieval set used to compute the nearest neighbours using a novel semantic descriptor to retrieve better candidates. The approach is validated by experiments on several datasets used for semantic parsing demonstrating the superiority of the method compared to the state of art approaches.</p><p>4 0.19883727 <a title="284-tfidf-4" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<p>Author: Won Hwa Kim, Moo K. Chung, Vikas Singh</p><p>Abstract: The analysis of 3-D shape meshes is a fundamental problem in computer vision, graphics, and medical imaging. Frequently, the needs of the application require that our analysis take a multi-resolution view of the shape ’s local and global topology, and that the solution is consistent across multiple scales. Unfortunately, the preferred mathematical construct which offers this behavior in classical image/signal processing, Wavelets, is no longer applicable in this general setting (data with non-uniform topology). In particular, the traditional definition does not allow writing out an expansion for graphs that do not correspond to the uniformly sampled lattice (e.g., images). In this paper, we adapt recent results in harmonic analysis, to derive NonEuclidean Wavelets based algorithms for a range of shape analysis problems in vision and medical imaging. We show how descriptors derived from the dual domain representation offer native multi-resolution behavior for characterizing local/global topology around vertices. With only minor modifications, the framework yields a method for extracting interest/key points from shapes, a surprisingly simple algorithm for 3-D shape segmentation (competitive with state of the art), and a method for surface alignment (without landmarks). We give an extensive set of comparison results on a large shape segmentation benchmark and derive a uniqueness theorem for the surface alignment problem.</p><p>5 0.1649957 <a title="284-tfidf-5" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>Author: Zsolt Sánta, Zoltan Kato</p><p>Abstract: A novel correspondence-less approach is proposed to find a thin plate spline map between a pair of deformable 3D objects represented by triangular surface meshes. The proposed method works without landmark extraction and feature correspondences. The aligning transformation is found simply by solving a system of nonlinear equations. Each equation is generated by integrating a nonlinear function over the object’s domains. We derive recursive formulas for the efficient computation of these integrals. Based on a series of comparative tests on a large synthetic dataset, our triangular mesh-based algorithm outperforms state of the art methods both in terms of computing time and accuracy. The applicability of the proposed approach has been demonstrated on the registration of 3D lung CT volumes.</p><p>6 0.16146505 <a title="284-tfidf-6" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>7 0.15293488 <a title="284-tfidf-7" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>8 0.14608248 <a title="284-tfidf-8" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>9 0.14200412 <a title="284-tfidf-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.13585986 <a title="284-tfidf-10" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>11 0.12988631 <a title="284-tfidf-11" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>12 0.12890531 <a title="284-tfidf-12" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>13 0.12781735 <a title="284-tfidf-13" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<p>14 0.12730154 <a title="284-tfidf-14" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>15 0.12641402 <a title="284-tfidf-15" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>16 0.12083395 <a title="284-tfidf-16" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>17 0.11534869 <a title="284-tfidf-17" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>18 0.11466022 <a title="284-tfidf-18" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>19 0.11322965 <a title="284-tfidf-19" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>20 0.10987543 <a title="284-tfidf-20" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, 0.138), (2, 0.039), (3, -0.003), (4, 0.085), (5, -0.047), (6, -0.012), (7, 0.126), (8, -0.019), (9, -0.01), (10, 0.095), (11, 0.016), (12, -0.025), (13, 0.113), (14, 0.028), (15, -0.085), (16, -0.008), (17, 0.091), (18, -0.042), (19, 0.02), (20, -0.122), (21, -0.036), (22, 0.105), (23, 0.126), (24, -0.297), (25, -0.079), (26, 0.093), (27, 0.002), (28, -0.14), (29, -0.115), (30, -0.084), (31, 0.067), (32, -0.015), (33, -0.01), (34, -0.151), (35, -0.023), (36, -0.059), (37, -0.037), (38, -0.012), (39, 0.029), (40, -0.166), (41, -0.024), (42, 0.071), (43, -0.044), (44, 0.047), (45, -0.024), (46, -0.0), (47, 0.127), (48, -0.043), (49, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92727435 <a title="284-lsi-1" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>Author: Julien P.C. Valentin, Sunando Sengupta, Jonathan Warrell, Ali Shahrokni, Philip H.S. Torr</p><p>Abstract: Semantic reconstruction of a scene is important for a variety of applications such as 3D modelling, object recognition and autonomous robotic navigation. However, most object labelling methods work in the image domain and fail to capture the information present in 3D space. In this work we propose a principled way to generate object labelling in 3D. Our method builds a triangulated meshed representation of the scene from multiple depth estimates. We then define a CRF over this mesh, which is able to capture the consistency of geometric properties of the objects present in the scene. In this framework, we are able to generate object hypotheses by combining information from multiple sources: geometric properties (from the 3D mesh), and appearance properties (from images). We demonstrate the robustness of our framework in both indoor and outdoor scenes. For indoor scenes we created an augmented version of the NYU indoor scene dataset (RGB-D images) with object labelled meshes for training and evaluation. For outdoor scenes, we created ground truth object labellings for the KITTI odometry dataset (stereo image sequence). We observe a signifi- cant speed-up in the inference stage by performing labelling on the mesh, and additionally achieve higher accuracies.</p><p>2 0.71277869 <a title="284-lsi-2" href="./cvpr-2013-Optical_Flow_Estimation_Using_Laplacian_Mesh_Energy.html">316 cvpr-2013-Optical Flow Estimation Using Laplacian Mesh Energy</a></p>
<p>Author: Wenbin Li, Darren Cosker, Matthew Brown, Rui Tang</p><p>Abstract: In this paper we present a novel non-rigid optical flow algorithm for dense image correspondence and non-rigid registration. The algorithm uses a unique Laplacian Mesh Energy term to encourage local smoothness whilst simultaneously preserving non-rigid deformation. Laplacian deformation approaches have become popular in graphics research as they enable mesh deformations to preserve local surface shape. In this work we propose a novel Laplacian Mesh Energy formula to ensure such sensible local deformations between image pairs. We express this wholly within the optical flow optimization, and show its application in a novel coarse-to-fine pyramidal approach. Our algorithm achieves the state-of-the-art performance in all trials on the Garg et al. dataset, and top tier performance on the Middlebury evaluation.</p><p>3 0.64977551 <a title="284-lsi-3" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<p>Author: Henrik Zimmer, Marcel Campen, Leif Kobbelt</p><p>Abstract: In the context of shape segmentation and retrieval object-wide distributions of measures are needed to accurately evaluate and compare local regions ofshapes. Lien et al. [16] proposed two point-wise concavity measures in the context of Approximate Convex Decompositions of polygons measuring the distance from a point to the polygon ’s convex hull: an accurate Shortest Path-Concavity (SPC) measure and a Straight Line-Concavity (SLC) approximation of the same. While both are practicable on 2D shapes, the exponential costs of SPC in 3D makes it inhibitively expensive for a generalization to meshes [14]. In this paper we propose an efficient and straight forward approximation of the Shortest Path-Concavity measure to 3D meshes. Our approximation is based on discretizing the space between mesh and convex hull, thereby reducing the continuous Shortest Path search to an efficiently solvable graph problem. Our approach works outof-the-box on complex mesh topologies and requires no complicated handling of genus. Besides presenting a rigorous evaluation of our method on a variety of input meshes, we also define an SPC-based Shape Descriptor and show its superior retrieval and runtime performance compared with the recently presented results on the Convexity Distribution by Lian et al. [12].</p><p>4 0.60964906 <a title="284-lsi-4" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<p>Author: Won Hwa Kim, Moo K. Chung, Vikas Singh</p><p>Abstract: The analysis of 3-D shape meshes is a fundamental problem in computer vision, graphics, and medical imaging. Frequently, the needs of the application require that our analysis take a multi-resolution view of the shape ’s local and global topology, and that the solution is consistent across multiple scales. Unfortunately, the preferred mathematical construct which offers this behavior in classical image/signal processing, Wavelets, is no longer applicable in this general setting (data with non-uniform topology). In particular, the traditional definition does not allow writing out an expansion for graphs that do not correspond to the uniformly sampled lattice (e.g., images). In this paper, we adapt recent results in harmonic analysis, to derive NonEuclidean Wavelets based algorithms for a range of shape analysis problems in vision and medical imaging. We show how descriptors derived from the dual domain representation offer native multi-resolution behavior for characterizing local/global topology around vertices. With only minor modifications, the framework yields a method for extracting interest/key points from shapes, a surprisingly simple algorithm for 3-D shape segmentation (competitive with state of the art), and a method for surface alignment (without landmarks). We give an extensive set of comparison results on a large shape segmentation benchmark and derive a uniqueness theorem for the surface alignment problem.</p><p>5 0.58066505 <a title="284-lsi-5" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>Author: Bo Zheng, Yibiao Zhao, Joey C. Yu, Katsushi Ikeuchi, Song-Chun Zhu</p><p>Abstract: In this paper, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.</p><p>6 0.55406415 <a title="284-lsi-6" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>7 0.54473466 <a title="284-lsi-7" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>8 0.52136397 <a title="284-lsi-8" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>9 0.4997569 <a title="284-lsi-9" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>10 0.49235222 <a title="284-lsi-10" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>11 0.49143735 <a title="284-lsi-11" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>12 0.48505062 <a title="284-lsi-12" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>13 0.46418062 <a title="284-lsi-13" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>14 0.45281178 <a title="284-lsi-14" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>15 0.44886333 <a title="284-lsi-15" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>16 0.44848859 <a title="284-lsi-16" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>17 0.44838423 <a title="284-lsi-17" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>18 0.44644794 <a title="284-lsi-18" href="./cvpr-2013-Area_Preserving_Brain_Mapping.html">44 cvpr-2013-Area Preserving Brain Mapping</a></p>
<p>19 0.44393641 <a title="284-lsi-19" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>20 0.43926954 <a title="284-lsi-20" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.108), (16, 0.024), (26, 0.056), (28, 0.052), (33, 0.235), (36, 0.114), (39, 0.021), (67, 0.091), (69, 0.077), (70, 0.014), (87, 0.102)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91596794 <a title="284-lda-1" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>2 0.90914196 <a title="284-lda-2" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>3 0.90492797 <a title="284-lda-3" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>4 0.90048665 <a title="284-lda-4" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>5 0.8984018 <a title="284-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.89660883 <a title="284-lda-6" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>7 0.8952921 <a title="284-lda-7" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>8 0.89522958 <a title="284-lda-8" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>9 0.89398652 <a title="284-lda-9" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>10 0.89294469 <a title="284-lda-10" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>11 0.88882351 <a title="284-lda-11" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>12 0.88719827 <a title="284-lda-12" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>13 0.88647699 <a title="284-lda-13" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>14 0.88638127 <a title="284-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>same-paper 15 0.88582414 <a title="284-lda-15" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>16 0.8856281 <a title="284-lda-16" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>17 0.88562638 <a title="284-lda-17" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>18 0.88098323 <a title="284-lda-18" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>19 0.88056213 <a title="284-lda-19" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>20 0.88055849 <a title="284-lda-20" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
