<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-161" href="#">cvpr2013-161</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</h1>
<br/><p>Source: <a title="cvpr-2013-161-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wu_Facial_Feature_Tracking_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>Reference: <a title="cvpr-2013-161-reference" href="../cvpr2013_reference/cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. [sent-3, score-0.36]
</p><p>2 Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. [sent-4, score-1.362]
</p><p>3 To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. [sent-5, score-1.085]
</p><p>4 Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. [sent-6, score-1.165]
</p><p>5 Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses. [sent-7, score-1.787]
</p><p>6 Introduction Due to its relevance to many applications like human head pose estimation and facial expression recognition, facial feature tracking is an active area in computer vision. [sent-9, score-1.772]
</p><p>7 However, tracking facial feature points is challenging, since the face is non-rigid, and it can change its appearance and shape in unexpected ways. [sent-10, score-1.193]
</p><p>8 When faces have varying facial expressions and face poses, it is difficult to construct a prior model that could capture the large range of shape variations for facial feature tracking. [sent-11, score-2.235]
</p><p>9 edu  (a) 26 facial feature points that we track  Figure 1. [sent-14, score-0.825]
</p><p>10 In this paper, we present a work that can effectively track facial feature points using face shape prior models that are constructed based on RBM. [sent-19, score-1.185]
</p><p>11 The facial feature tracker can track 26 facial feature points (Fig. [sent-20, score-1.585]
</p><p>12 1 (a)) even if faces have different facial expressions, varying poses, or occlusion (Fig. [sent-21, score-0.821]
</p><p>13 Specifically, we first construct a model 333444555200  to capture the face shapes with different facial expressions for near frontal face based on Deep Belief Networks. [sent-24, score-1.548]
</p><p>14 Furthermore, to handle pose variations, we propose a model to capture the transition between the frontal face and nonfrontal face with 3-way Restricted Boltzmann Machines. [sent-25, score-0.598]
</p><p>15 Finally, effective ways are proposed to combine the prior model with the image measurements of the facial feature points. [sent-26, score-0.877]
</p><p>16 In section 3, we describe the face shape prior model that deals with expression variations based on DBNs (FrontalRBM). [sent-28, score-0.526]
</p><p>17 In section 4, we describe the face shape prior model that deals with both expression variations and pose variations based on 3-way RBM (PoseRBM). [sent-29, score-0.607]
</p><p>18 In section 5, we discuss how to combine the face shape prior model and measurements to generate final facial feature tracking results. [sent-30, score-1.282]
</p><p>19 Facial feature localization Generally, facial feature localization algorithms in the literature can be classified into two categories: methods without face shape prior model and methods with face shape prior model. [sent-36, score-1.517]
</p><p>20 Methods without shape prior model track  each facial feature point independently and ignore the prior knowledge about the face. [sent-37, score-1.015]
</p><p>21 As a result, they usually are susceptible to facial expression change, face poses change, occlusion etc. [sent-38, score-1.157]
</p><p>22 On the other hand, methods with shape prior model capture the dependence between facial feature points by explicitly modeling the general properties as well as the variations of facial shape or appearance. [sent-39, score-1.814]
</p><p>23 Due to the limited space, we focus on the related methods with face shape prior model. [sent-41, score-0.36]
</p><p>24 More recently, in [20], facial feature points are detected independently based on the response of the support vector regressor. [sent-44, score-0.788]
</p><p>25 Both [7] and [4] emphasize the facial components in their models. [sent-46, score-0.723]
</p><p>26 In [7], the shape variation of each facial component is modeled with a single Gaussian. [sent-47, score-0.797]
</p><p>27 In addition, the nonlinear relationship among facial components is represented as a pre-trained Gaussian Process Latent Variable model. [sent-48, score-0.723]
</p><p>28 In [4], Ding and Martinez propose a method to detect the facial feature points for each facial component with the use of subclass division and context information around features. [sent-49, score-1.511]
</p><p>29 In real world situations, faces usually vary in facial expressions and poses. [sent-50, score-0.999]
</p><p>30 These natural movements make facial feature tracking even more difficult. [sent-51, score-0.877]
</p><p>31 To solve this problem, Tian and Cohn [9] propose a multi-state facial component model, where the state is selected by tracking a few control points. [sent-52, score-0.84]
</p><p>32 [19] propose a model to capture the different states of facial components like mouth open and mouth closed. [sent-55, score-0.808]
</p><p>33 In addition, they project the frontal face to face with poses to handle the varying poses problem. [sent-56, score-0.719]
</p><p>34 In [1], instead of using the parametric model, Belhumeur present methods to represent the face shape variations with non-parametric training data. [sent-59, score-0.336]
</p><p>35 train DBNs for detecting the facial components like the mouth, eyes, nose and eyebrows. [sent-70, score-0.723]
</p><p>36 build a generative model to capture the joint probability of the facial action units, identities, and binary image patch data using 333444555311  the DBNs. [sent-77, score-0.758]
</p><p>37 Then, based on this model, they can generate realistic binary face patches displaying specific identities and  facial actions. [sent-78, score-0.937]
</p><p>38 Prior model for face shapes with different facial expressions (FrontalRBM) Although the appearance of the human face varies from individual to individual, the spatial relationship among facial feature points is similar for a given facial expression. [sent-80, score-2.922]
</p><p>39 2, there exist patterns for human face shapes, but these patterns depend on the facial expressions. [sent-82, score-0.989]
</p><p>40 To capture these patterns, we propose a face shape prior model based on Deep Belief Networks which we call ForntalRBM in this paper. [sent-83, score-0.395]
</p><p>41 Facial feature locations of 10 subjects with different facial expressions Deep belief networks (DBNs) are well known as an approach to automatically extract effective multi-level fea-  tures from data. [sent-87, score-1.12]
</p><p>42 In the application of facial feature points tracking, our goal is to use two-layer DBNs to explicitly capture the face shape patterns under different facial expressions. [sent-100, score-1.86]
</p><p>43 In this case, the observation nodes are the coordinates of facial feature locations, normalized according to the locations of  Part I: Frontal face shape prior Part II: Transfer to with different expressions different poses Figure 3. [sent-101, score-1.453]
</p><p>44 Prior model for face shapes with varying facial expressions and poses (PoseRBM) It is nature to think of extending the DBNs described in the last section to capture shape variations with both facial expression and pose changes. [sent-118, score-2.339]
</p><p>45 However, this raises a problem when the number of possible poses increases, and we cannot expect the DBNs to learn all possible face shape patterns with all poses well unless a large number of hidden nodes are used, which would significantly increase the number of required training data. [sent-119, score-0.505]
</p><p>46 To alleviate this problem, we propose a novel face shape prior model as shown in Fig. [sent-120, score-0.36]
</p><p>47 3, where x represents the locations of facial feature points for frontal face when subjects show different facial expressions, and y represents the corresponding locations of facial feature points for non-frontal face under the same facial expression. [sent-121, score-3.629]
</p><p>48 3, the face shape prior model is factorized into two parts. [sent-124, score-0.386]
</p><p>49 In part I, the two layer DBNs model captures the shape patterns of the frontal face under varying facial expressions as discussed in the previous section. [sent-125, score-1.407]
</p><p>50 In part II, the 3-way RBM model captures the transition between the facial feature locations for frontal face and corresponding non-frontal face. [sent-126, score-1.096]
</p><p>51 Facial feature tracking based on face shape prior model Facial feature tracking accuracy and robustness can be improved by incorporating the face shape prior model. [sent-185, score-1.028]
</p><p>52 Since facial feature points vector is of high dimensions (52 in this case), to globally produce enough samples to cover such a large parameter space is computationally infeasible. [sent-188, score-0.788]
</p><p>53 If the face shape only varies due to facial expression change, we use FrontalRBM discussed in section 3 as shape prior. [sent-191, score-1.203]
</p><p>54 If the face shape varies due to both facial expression and pose change, we use PoseRBM described in section 4 as prior. [sent-193, score-1.162]
</p><p>55 Facial feature tracking under different facial expressions In this section, we test the FrontalRBM model described in section 3 that could track facial feature points for nearfrontal face with different facial expressions . [sent-210, score-3.109]
</p><p>56 We show  the experiments using synthetic data, sequences from the extended Cohn-Kanade database (CK+) [10], the MMI facial expression database [15], the American Sign Language (ASL) database [24] and the ISL Facial Expression database [18]. [sent-211, score-1.133]
</p><p>57 FrontalRBM shows strong power as a face shape prior model. [sent-216, score-0.36]
</p><p>58 (a) face with outlier (left eyebrow tip); (b) Correction of (a); (c) face with corrupted points on left half face; (d) correction of (c). [sent-219, score-0.545]
</p><p>59 Experiments on CK+ database: The CK+ database contains facial behavior videos of 123 subjects showing 7 basic facial expressions including anger, disgust, fear, happiness, sadness, surprise, and contempt. [sent-220, score-1.785]
</p><p>60 Since the number of sequences for contempt is much smaller than the other facial expressions, we exclude the sequences with contempt facial expression. [sent-221, score-1.508]
</p><p>61 It is important to  notice that data with neutral facial expression is included as the first frame of each sequences. [sent-225, score-0.869]
</p><p>62 5 compares the tracking error for different facial expressions. [sent-235, score-0.84]
</p><p>63 We can see that the proposed method can decrease the tracking error for all the facial expressions and the performances are similar. [sent-236, score-1.075]
</p><p>64 6 (a)), the subject shows relatively neutral facial expression at the beginning and then happy facial expression after the forth frame. [sent-240, score-1.793]
</p><p>65 Tracking errors for different facial expressions on CK+ database. [sent-290, score-0.958]
</p><p>66 333444555644  (a) ISL database, sequence 1, subject shows happy facial expression (b) ISL database, sequence 2, subject shows surprise facial expression  (c) ASL database, sequence 1  (d) ASL database,  sequence  2  Figure 7. [sent-306, score-1.915]
</p><p>67 Experiments on MMI database: In our experiments on MMI database [15] , there are 196 sequences of 27 subjects with 6 basic facial expressions. [sent-308, score-0.827]
</p><p>68 We manually label the onset and apex frames for each sequence with facial expressions and the first frame as neutral expression. [sent-311, score-1.083]
</p><p>69 By incorporating the frontalRBM as face shape prior model, the overall errors decrease by 16. [sent-313, score-0.36]
</p><p>70 Dealing with occulsion: In real world situations, the  facial features may be occluded by other objects. [sent-332, score-0.723]
</p><p>71 To test the proposed method under occlusion, we perform tracking on the ISL occlusion facial expression database [18] and some sequence with occlusion from American Sign Language (ASL) database [24]. [sent-333, score-1.164]
</p><p>72 Specifically, the ISL occlusion database consists of 10 sequences of 2 subjects showing happy and surprised facial expressions with near-frontal pose. [sent-334, score-1.217]
</p><p>73 The face shape prior model is trained using the manually labeled data on CK+ database. [sent-335, score-0.385]
</p><p>74 To perform tracking using the proposed method, we use the simple Kalman Filter, local image intensity patch as feature and manually label the facial features on the first frame. [sent-336, score-0.902]
</p><p>75 The proposed tracker can correctly track the facial features under occlusion. [sent-341, score-0.76]
</p><p>76 Facial feature tracking under different facial expressions and varying face poses In this section, we report the test of the model proposed in section 4 (PoseRBM) that could track facial features when faces have simultaneous expression and pose changes. [sent-344, score-2.408]
</p><p>77 4, PoseRBM as a face shape prior model can correct the outliers and even the cor-  rupted points on half of the face. [sent-350, score-0.388]
</p><p>78 Experiments on ISL multi-view facial expression database [18]: Overall, there are 40 sequences of 8 subjects showing happy and surprised facial expressions under varying face poses. [sent-351, score-2.281]
</p><p>79 (a) face with outlier (left eyebrow tip); (b) Correction of (a); (c) face with corrupted points on left half face; (d) correction of (c). [sent-356, score-0.545]
</p><p>80 In order to train part II, we need the corresponding facial feature points for frontal face and non-frontal face with the same expression. [sent-359, score-1.318]
</p><p>81 As far as our knowledge, there is no such database with large number of real corresponding images available to the public, so we project the frontal facial feature points in CK+ database to specific poses based on general z coordinates of the feature points. [sent-360, score-1.139]
</p><p>82 Here, to train the PoseRBM model, we use the frontal face and generated non-frontal face with 22. [sent-361, score-0.53]
</p><p>83 Although it is trained with data from three poses, PoseRBM can adapt to deal with the poses under moderate angles(less  than 50 degrees) where full view of all facial features is available. [sent-363, score-0.801]
</p><p>84 , modeling expressions for frontal face and rotation to different poses. [sent-368, score-0.551]
</p><p>85 This automatically adapts the expressions in frontal face to other poses, removing the necessary of modeling expressions under each pose. [sent-369, score-0.786]
</p><p>86 Tracking is performed by incorporating the PoseRBM model, simple Kalman Filter, local image patch as features, and manually labeled facial features on the first frame. [sent-371, score-0.748]
</p><p>87 It can be seen that the proposed method decreases the tracking error for both happy and surprised facial expressions. [sent-378, score-0.971]
</p><p>88 9 compares the error distributions of facial feature tracking for happy and surprise expressions, respectively. [sent-380, score-1.014]
</p><p>89 10 shows some tracking results when subjects  turn right and left with different facial expressions. [sent-383, score-0.877]
</p><p>90 The tracking error may increase if the subject shows simultaneous extreme face poses and facial expressions(last frame in 10(e)(f)). [sent-385, score-1.151]
</p><p>91 (a) Happy facial expressions (b) Surprised facial expressions Figure 9. [sent-392, score-1.916]
</p><p>92 Conclusions and future work In this paper, we introduce methods to construct a face shape prior model based on Restricted Boltzmann Machines  and their variants to improve the accuracy and robustness of facial feature tracking under simultaneous pose and expression variations. [sent-397, score-1.43]
</p><p>93 Specifically, we first introduce a face shape prior model to capture the face shape patterns under varying facial expressions for near-frontal face based on Deep Belief Networks. [sent-398, score-1.914]
</p><p>94 We then extend the frontal face prior model by a 3-way RBM to capture face shape patterns under simultaneous expression and pose variation. [sent-399, score-0.907]
</p><p>95 Finally, we introduce methods to systematically combine the face prior models with image measurements of facial feature points to perform facial feature point tracking. [sent-400, score-1.879]
</p><p>96 Experimental comparisons with state of the art methods on benchmark data sets show the improved performance of the proposed methods even when faces have varying facial expressions, poses, and occlusion. [sent-401, score-0.797]
</p><p>97 Facial feature tracking results  on  ISL multi-view facial expression database using the proposed method (PoseRBM). [sent-428, score-1.062]
</p><p>98 Features versus context: An approach for precise and detailed detection and delineation of faces and facial features. [sent-433, score-0.764]
</p><p>99 Web-  [16]  based database for facial expression analysis. [sent-507, score-0.908]
</p><p>100 Robust facial feature tracking under varying face pose and facial expression. [sent-538, score-1.88]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facial', 0.723), ('expressions', 0.235), ('frontalrbm', 0.215), ('face', 0.214), ('poserbm', 0.2), ('rbm', 0.193), ('dbns', 0.177), ('expression', 0.118), ('tracking', 0.117), ('boltzmann', 0.106), ('frontal', 0.102), ('isl', 0.095), ('happy', 0.083), ('poses', 0.078), ('shape', 0.074), ('prior', 0.072), ('database', 0.067), ('deep', 0.067), ('ck', 0.066), ('aam', 0.063), ('mmi', 0.06), ('switch', 0.055), ('surprise', 0.054), ('asl', 0.051), ('xm', 0.049), ('variations', 0.048), ('surprised', 0.048), ('hkwkhf', 0.046), ('wix', 0.046), ('yjwjyf', 0.046), ('machines', 0.044), ('belief', 0.043), ('faces', 0.041), ('eyebrow', 0.038), ('restricted', 0.038), ('subjects', 0.037), ('feature', 0.037), ('track', 0.037), ('hidden', 0.035), ('capture', 0.035), ('xd', 0.034), ('contrastive', 0.034), ('valstar', 0.034), ('martinez', 0.034), ('varying', 0.033), ('pose', 0.033), ('anger', 0.031), ('binefa', 0.031), ('contempt', 0.031), ('happiness', 0.031), ('improvementproposed', 0.031), ('imrbm', 0.031), ('lyu', 0.031), ('sadness', 0.031), ('schalk', 0.031), ('wijxihj', 0.031), ('wjy', 0.031), ('xiwxif', 0.031), ('correction', 0.031), ('cohn', 0.031), ('multivariate', 0.029), ('points', 0.028), ('neutral', 0.028), ('susskind', 0.027), ('disgust', 0.027), ('onset', 0.027), ('wkh', 0.027), ('language', 0.027), ('tong', 0.027), ('factorized', 0.026), ('patterns', 0.026), ('mouth', 0.025), ('eslami', 0.025), ('dantone', 0.025), ('fear', 0.025), ('shapes', 0.025), ('manually', 0.025), ('networks', 0.025), ('occlusion', 0.024), ('sequence', 0.024), ('synthetic', 0.024), ('emotion', 0.024), ('purdue', 0.024), ('measurements', 0.024), ('measurement', 0.024), ('conference', 0.023), ('american', 0.023), ('variants', 0.023), ('feret', 0.023), ('kanade', 0.023), ('baseline', 0.022), ('smile', 0.022), ('active', 0.021), ('combine', 0.021), ('apex', 0.021), ('degrees', 0.021), ('locations', 0.02), ('corrupted', 0.02), ('normalizing', 0.02), ('simultaneous', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="161-tfidf-1" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>2 0.56890893 <a title="161-tfidf-2" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>3 0.29010949 <a title="161-tfidf-3" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>4 0.23850672 <a title="161-tfidf-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.231359 <a title="161-tfidf-5" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>Author: Dong Yi, Zhen Lei, Stan Z. Li</p><p>Abstract: Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper, we propose a novel method for pose robust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-ofthe-art methods significantly, meanwhile, the method works well on LFW.</p><p>6 0.20339231 <a title="161-tfidf-6" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>7 0.18626465 <a title="161-tfidf-7" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>8 0.18559062 <a title="161-tfidf-8" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>9 0.17391114 <a title="161-tfidf-9" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>10 0.17211604 <a title="161-tfidf-10" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>11 0.16638087 <a title="161-tfidf-11" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>12 0.16444393 <a title="161-tfidf-12" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>13 0.16097617 <a title="161-tfidf-13" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>14 0.15157299 <a title="161-tfidf-14" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>15 0.15076676 <a title="161-tfidf-15" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>16 0.14997493 <a title="161-tfidf-16" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>17 0.13938576 <a title="161-tfidf-17" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>18 0.13631859 <a title="161-tfidf-18" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>19 0.1363034 <a title="161-tfidf-19" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>20 0.13314597 <a title="161-tfidf-20" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, -0.06), (2, -0.085), (3, -0.024), (4, 0.039), (5, -0.023), (6, -0.011), (7, -0.14), (8, 0.361), (9, -0.21), (10, 0.145), (11, -0.051), (12, 0.023), (13, 0.158), (14, 0.041), (15, 0.23), (16, -0.013), (17, 0.157), (18, 0.11), (19, 0.094), (20, 0.001), (21, -0.136), (22, 0.039), (23, -0.036), (24, -0.007), (25, 0.056), (26, 0.077), (27, -0.093), (28, 0.074), (29, 0.115), (30, -0.032), (31, -0.113), (32, -0.129), (33, -0.033), (34, -0.128), (35, -0.08), (36, 0.037), (37, 0.016), (38, 0.007), (39, -0.047), (40, 0.021), (41, -0.036), (42, -0.043), (43, 0.175), (44, -0.002), (45, -0.0), (46, 0.031), (47, 0.017), (48, 0.153), (49, -0.128)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95357066 <a title="161-lsi-1" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>2 0.86178064 <a title="161-lsi-2" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>3 0.73831624 <a title="161-lsi-3" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>4 0.63507056 <a title="161-lsi-4" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>5 0.63167995 <a title="161-lsi-5" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>Author: Robert Anderson, Björn Stenger, Vincent Wan, Roberto Cipolla</p><p>Abstract: This paper presents a complete system for expressive visual text-to-speech (VTTS), which is capable of producing expressive output, in the form of a ‘talking head’, given an input text and a set of continuous expression weights. The face is modeled using an active appearance model (AAM), and several extensions are proposed which make it more applicable to the task of VTTS. The model allows for normalization with respect to both pose and blink state which significantly reduces artifacts in the resulting synthesized sequences. We demonstrate quantitative improvements in terms of reconstruction error over a million frames, as well as in large-scale user studies, comparing the output of different systems.</p><p>6 0.63093507 <a title="161-lsi-6" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>7 0.61539638 <a title="161-lsi-7" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>8 0.58973801 <a title="161-lsi-8" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>9 0.57723522 <a title="161-lsi-9" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>10 0.52343225 <a title="161-lsi-10" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>11 0.52284467 <a title="161-lsi-11" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>12 0.49656874 <a title="161-lsi-12" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>13 0.47668552 <a title="161-lsi-13" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>14 0.4287141 <a title="161-lsi-14" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>15 0.41732287 <a title="161-lsi-15" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>16 0.41682363 <a title="161-lsi-16" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>17 0.39120281 <a title="161-lsi-17" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>18 0.3905057 <a title="161-lsi-18" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>19 0.3455115 <a title="161-lsi-19" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>20 0.33590624 <a title="161-lsi-20" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.249), (10, 0.16), (16, 0.027), (26, 0.05), (33, 0.23), (55, 0.014), (67, 0.074), (69, 0.039), (87, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81907725 <a title="161-lda-1" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>2 0.7870636 <a title="161-lda-2" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>3 0.75787246 <a title="161-lda-3" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>4 0.75595939 <a title="161-lda-4" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>5 0.75512826 <a title="161-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.75368267 <a title="161-lda-6" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>7 0.75270176 <a title="161-lda-7" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>8 0.75126547 <a title="161-lda-8" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>9 0.75052077 <a title="161-lda-9" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>10 0.75032616 <a title="161-lda-10" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>11 0.74953896 <a title="161-lda-11" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>12 0.74845153 <a title="161-lda-12" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>13 0.74810362 <a title="161-lda-13" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>14 0.74733263 <a title="161-lda-14" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>15 0.74674839 <a title="161-lda-15" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>16 0.74662 <a title="161-lda-16" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>17 0.74614412 <a title="161-lda-17" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>18 0.74435079 <a title="161-lda-18" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>19 0.74372846 <a title="161-lda-19" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>20 0.74309838 <a title="161-lda-20" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
