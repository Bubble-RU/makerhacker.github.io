<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-85" href="#">cvpr2013-85</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</h1>
<br/><p>Source: <a title="cvpr-2013-85-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ma_Complex_Event_Detection_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>Reference: <a title="cvpr-2013-85-reference" href="../cvpr2013_reference/cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. [sent-11, score-1.238]
</p><p>2 Many works have exploited attributes at image level for various applications. [sent-12, score-0.498]
</p><p>3 However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. [sent-13, score-1.405]
</p><p>4 Hence, we propose to leverage attributes at video level (named as video attributes in this work), i. [sent-14, score-1.234]
</p><p>5 , the semantic labels of external videos are used as attributes. [sent-16, score-0.301]
</p><p>6 Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. [sent-17, score-1.045]
</p><p>7 Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. [sent-18, score-2.421]
</p><p>8 Introduction In this paper, we focus on the event detection of largescale real-world videos [2, 3]. [sent-21, score-0.68]
</p><p>9 In the past, detection of events that are simple, well-defined and describable by a short video sequence, e. [sent-23, score-0.385]
</p><p>10 In the real world, however, users are more interested in videos depicting complex events such as celebrating the New Year. [sent-26, score-0.449]
</p><p>11 Complex event detection is very challenging as these events usually contain many people and/or objects, various human actions, multiple scenes; have significant  c ovevmide pnlotex s parkour  tection with video attributes. [sent-27, score-0.895]
</p><p>12 intra-class variations; and take place in much longer video clips [2, 3, 15, 16]. [sent-28, score-0.139]
</p><p>13 Despite the arduousness, the practical significance of complex event detection has drawn increasing interest from researchers [23, 10, 15, 16]. [sent-29, score-0.641]
</p><p>14 have introduced the first exploration of Ad Hoc multimedia event detection when there are only 10 positive examples for training [15]. [sent-31, score-0.602]
</p><p>15 As complex events usually contain visual attributes related to people, scenes, objects and human actions (e. [sent-33, score-0.799]
</p><p>16 , Figure 1 shows that a complex event parkour is relevant withpush ups, building, etc. [sent-35, score-0.668]
</p><p>17 ), leveraging these attributes properly could be helpful for the detection. [sent-36, score-0.532]
</p><p>18 Visual attributes were introduced as describable proper-  ties of an object and have been applied to many applications [5, 7, 9]. [sent-37, score-0.55]
</p><p>19 Visual attributes can be either at local level 222666222755  or global level. [sent-38, score-0.498]
</p><p>20 For example, “many people ” is a locallevel attribute for the event flash mob gathering. [sent-39, score-0.786]
</p><p>21 Yet this kind of attributes is mostly defined manually, which is timeconsuming and requires expertise. [sent-40, score-0.475]
</p><p>22 Instead, we can use attributes at global level which are the semantic labels of images [19]. [sent-41, score-0.582]
</p><p>23 For instance, an image with its semantic label tennis can be leveraged for understanding the event playing tennis. [sent-42, score-0.572]
</p><p>24 Given that this type of attributes is associated with images, we regard it as image attributes. [sent-43, score-0.475]
</p><p>25 Using image attributes for complex event detection is intuitively limited as image attributes usually cannot characterize the dynamic properties of complex event videos (complex event videos refer to the videos depicting complex events). [sent-44, score-3.322]
</p><p>26 In this paper, we therefore propose an idea of video attributes and particularly apply it for complex event detection. [sent-45, score-1.196]
</p><p>27 Video attributes, in our work, indicate the semantic labels of other external videos collected by researchers. [sent-46, score-0.301]
</p><p>28 Note that these external videos are different from complex event videos. [sent-47, score-0.823]
</p><p>29 Compared to complex event videos, the external videos contain simple contents of people, objects, scenes and actions which are basic elements of complex events. [sent-48, score-1.045]
</p><p>30 For example, a video with its semantic label mixing batter is useful for understanding the complex event making a cake. [sent-49, score-0.773]
</p><p>31 As the external videos are used by treating their semantic labels as  video attributes, we call these videos attribute videos. [sent-50, score-0.748]
</p><p>32 To use video attributes, we may refer to a typical approach that involves training attribute classifiers and then using their outputs as intermediate representations for the complex event videos [8, 11]. [sent-51, score-1.107]
</p><p>33 First, when the number of attributes used is limited, it is insufficient to learn a discriminative intermediate representation. [sent-53, score-0.529]
</p><p>34 Second, given a particular event to detect, only some attributes are discriminative while others are comparatively useless or even noisy [16]. [sent-54, score-0.969]
</p><p>35 It is difficult to decide what attributes to use for different events. [sent-55, score-0.475]
</p><p>36 In contrast, we propose to use video attributes as additional information to assist complex event detection. [sent-56, score-1.196]
</p><p>37 Specifically, our framework learns the attribute classifier and event detector simultaneously. [sent-57, score-0.724]
</p><p>38 The observation of a particular event affects the attribute classifier, and in return, attributes characterize the event. [sent-58, score-1.19]
</p><p>39 This kind of mutual influence is explored by a correlation vector, which helps incorporate extra informative cues into the event detector. [sent-59, score-0.54]
</p><p>40 Our approach has two merits: the learning process of event detector is not solely dependent on the video attributes; and the joint framework adapts the knowledge from attributes for different events, i. [sent-61, score-1.109]
</p><p>41 , a particular event obtains dedicated perks via the joint learning of attribute classifier and event detector. [sent-63, score-1.193]
</p><p>42 Moreover, we propose to integrate multiple features from both complex event videos and attribute videos for learning  the detector as combining multiple features has proved to be beneficial for visual analysis [22]. [sent-64, score-1.138]
</p><p>43 On the other hand, existing video collections have different themes. [sent-65, score-0.152]
</p><p>44 As we expect the video attributes to be diverse, in our framework video attributes from different collections are utilized. [sent-66, score-1.217]
</p><p>45 The main contributions ofthis paper are as follows: First, we propose using video attributes for complex event detection. [sent-68, score-1.196]
</p><p>46 Second, video attributes are used latently as additional information for learning the event detector. [sent-69, score-1.125]
</p><p>47 Third, multiple attribute video sets with different features are sewed seamlessly with multiple features from the complex event videos. [sent-70, score-0.975]
</p><p>48 Related Work Visual attributes were advocated as the describable properties of objects [8]. [sent-72, score-0.55]
</p><p>49 For example, an object bear can be described by attributes such as furry and four legs. [sent-73, score-0.475]
</p><p>50 The attributes of an object are treated as latent variables and the correlations among attributes are used to classify object classes. [sent-77, score-0.95]
</p><p>51 A method to learn visual attributes and object classes together has been presented in [20]. [sent-78, score-0.475]
</p><p>52 have presented an interactive approach which discovers local attributes that are both discriminative and semantically meaningful for fine-grained category classification [7]. [sent-80, score-0.475]
</p><p>53 have proposed to explore the  shared features between objects and their attributes for animal and scene classification [9]. [sent-82, score-0.475]
</p><p>54 have leveraged high level describable attributes for selecting high aesthetic quality images and interesting ones from large image collections [5]. [sent-84, score-0.636]
</p><p>55 However, to generate local attributes usually requires a manually defining process which is burdensome. [sent-85, score-0.475]
</p><p>56 An alternative way is to leverage attributes at a global level, i. [sent-86, score-0.506]
</p><p>57 In the past, global-level image attributes have been widely used [19, 14]. [sent-91, score-0.475]
</p><p>58 have presented an object classification method by casting prior features obtained from global image attributes of auxiliary images into their multiple kernel learning framework [14]. [sent-93, score-0.475]
</p><p>59 For recognition or detection tasks in videos, image attributes probably cannot well characterize the dynamic properties which could hamper their contributions. [sent-94, score-0.55]
</p><p>60 have proposed learning an intermediate representation for event detection. [sent-96, score-0.548]
</p><p>61 In their approach, the intermediate representation is the same for the event videos and the attribute videos. [sent-97, score-0.88]
</p><p>62 However, it could be natural to assume that the events and attributes are different depictions of videos at different levels. [sent-98, score-0.786]
</p><p>63 Differently, we leverage 222666222866  video attributes to characterize complex events, which is interpretable. [sent-100, score-0.773]
</p><p>64 In our framework, we also learn an attribute classifier which can be used to predict the attributes of a given video. [sent-101, score-0.68]
</p><p>65 Since the attribute classifier is jointly optimized with the event detector, the related attribute classifier is more accurate in uncovering the attributes from an event video. [sent-102, score-1.873]
</p><p>66 For example, by exploiting the videos of “landing a fish”, the concept classifier “fish” can be more accurately trained and vice versa. [sent-103, score-0.175]
</p><p>67 In addition, as a byproduct of our method, the attribute representation can be further used for other applications such as multimedia event recounting [6]. [sent-104, score-0.789]
</p><p>68 Video Attributes Assisted Event Detection We first correlate the features of attribute videos from m multiple sources with their semantic labels respectively. [sent-106, score-0.444]
</p><p>69 Next we illustrate how t∈o lRearn a detector for the complex event by incorporating the attribute videos. [sent-128, score-0.812]
</p><p>70 Similarly we first map the multiple features of the complex event vid? [sent-129, score-0.606]
</p><p>71 im=1  ∈ Rdi×n,  where n is the number of complex event v? [sent-133, score-0.606]
</p><p>72 Since the attribute videos and the complex event videos are relevant, i. [sent-164, score-1.089]
</p><p>73 , complex events are usually related to people, scenes, objects and human actions, the two domains would have some shared knowledge. [sent-166, score-0.272]
</p><p>74 After Qi, wi and fi are obtained, we fix them and optimize pi. [sent-258, score-0.163]
</p><p>75 Output: Optimized wi ∈ Rdi×1, Qi ∈ Rdi×ci, pi ∈ Rci×1 and fi ∈ Rn×1∈. [sent-266, score-0.207]
</p><p>76 Experiments In this section we present the experiments that evaluate the proposed method for complex event detection. [sent-270, score-0.606]
</p><p>77 Datasets The TRECVID MED 2012 development set (MED12) is used for complex event detection. [sent-273, score-0.606]
</p><p>78 , the UCF50 dataset [17] and the development set from TRECVID 2012 semantic indexing task are used as attribute videos. [sent-277, score-0.233]
</p><p>79 The video set for TRECVID 2012 semantic indexing (SIN) task covers 346 concepts. [sent-279, score-0.167]
</p><p>80 We extract STIP [12] and SIFT [13] descriptors for the videos of MED12, STIP for UCF50 and SIFT for  SIN12. [sent-283, score-0.151]
</p><p>81 (2) Baseline: We set β in Eq (5) to 0 so that no video attributes are exploited in our approach. [sent-289, score-0.59]
</p><p>82 (3) SVM: SVM is an effective tool for complex event detection and has been widely used by several research groups for TRECVID MED, e. [sent-291, score-0.641]
</p><p>83 (4) Attributes Intermediate Representation (AIR): We train attribute classifiers using UCF50 and SIN12. [sent-295, score-0.181]
</p><p>84 SVM is applied on the new representations afterwards for event detection. [sent-297, score-0.494]
</p><p>85 For instance, MCR is 10%-75% better than SVM for 16 events in terms of AP. [sent-328, score-0.16]
</p><p>86 The promising performance of MCR verifies that leveraging video attributes properly is beneficial for complex event detection. [sent-329, score-1.277]
</p><p>87 Results using Single Feature and Single Source  In this part, we only use UCF50+MED12 with STIP feature and SIN12+MED12 with SIFT feature for complex event detection to show the performance change. [sent-332, score-0.641]
</p><p>88 This experiment validates that exploiting multiple attribute video sets together with different features is beneficial for most cases. [sent-340, score-0.32]
</p><p>89 Conclusions We have proposed a method for utilizing the attributes at video level for complex event detection. [sent-342, score-1.219]
</p><p>90 Video attributes are convenient to use for complex event detection as many video collections relevant to people, scenes, objects and actions are available. [sent-343, score-1.32]
</p><p>91 Meanwhile, video attributes have more potentials than image attributes to characterize the dynamic  properties of video data. [sent-344, score-1.22]
</p><p>92 Unlike the traditional approach which maps the video data into attribute space, our method learns a correlation vector which correlates video attributes and a complex event. [sent-345, score-1.052]
</p><p>93 Built upon this, the extra informative cues learnt from attribute videos are further incorporated into the event detector. [sent-346, score-0.85]
</p><p>94 We have performed extensive experiments using a real-world large-scale video dataset to evaluate the efficacy of our method on complex event detection. [sent-347, score-0.721]
</p><p>95 The results are encouraging and have verified the advantage of leveraging video attributes properly. [sent-348, score-0.622]
</p><p>96 High level describable attributes for predicting aesthetics and interestingness. [sent-381, score-0.573]
</p><p>97 Recognizing complex events using large marginjoint low-level event model. [sent-424, score-0.766]
</p><p>98 Learning to detect unseen object classes by between-class attribute transfer. [sent-431, score-0.181]
</p><p>99 Knowledge adaptation for ad hoc multimedia event detection with few exemplars. [sent-456, score-0.634]
</p><p>100 Informedia e-lamp @ TRECVID2012: Multimedia event detection and recounting med and mer. [sent-533, score-0.641]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.494), ('attributes', 0.475), ('mcr', 0.23), ('attribute', 0.181), ('events', 0.16), ('videos', 0.151), ('rdi', 0.145), ('ix', 0.137), ('itqi', 0.124), ('qipi', 0.117), ('video', 0.115), ('eq', 0.115), ('complex', 0.112), ('itbi', 0.103), ('trecvid', 0.102), ('stip', 0.097), ('wi', 0.096), ('itqipi', 0.093), ('iypit', 0.093), ('minndc', 0.093), ('qitx', 0.093), ('qi', 0.084), ('pipit', 0.083), ('pmd', 0.083), ('describable', 0.075), ('multimedia', 0.073), ('med', 0.071), ('itwi', 0.07), ('fi', 0.067), ('sebe', 0.066), ('external', 0.066), ('parkour', 0.062), ('rci', 0.062), ('intermediate', 0.054), ('semantic', 0.052), ('actions', 0.052), ('iv', 0.05), ('singapore', 0.049), ('itqipipit', 0.047), ('metze', 0.047), ('mqiin', 0.047), ('rawat', 0.047), ('schulam', 0.047), ('sewed', 0.047), ('zhongwen', 0.047), ('pi', 0.044), ('ma', 0.043), ('mob', 0.041), ('recounting', 0.041), ('flash', 0.041), ('latently', 0.041), ('characterize', 0.04), ('appliance', 0.038), ('iai', 0.038), ('psc', 0.038), ('im', 0.038), ('ai', 0.037), ('collections', 0.037), ('svm', 0.036), ('detection', 0.035), ('scenes', 0.035), ('ap', 0.034), ('derivative', 0.034), ('burger', 0.033), ('sift', 0.033), ('leveraging', 0.032), ('labels', 0.032), ('vehicle', 0.032), ('hwang', 0.032), ('dhar', 0.032), ('correlates', 0.032), ('hoc', 0.032), ('leverage', 0.031), ('ci', 0.03), ('pages', 0.03), ('fish', 0.03), ('rni', 0.029), ('iy', 0.029), ('people', 0.029), ('iarpa', 0.028), ('correlate', 0.028), ('nist', 0.027), ('hilbert', 0.027), ('rn', 0.026), ('seamlessly', 0.026), ('depicting', 0.026), ('leveraged', 0.026), ('detector', 0.025), ('properly', 0.025), ('classifier', 0.024), ('official', 0.024), ('clips', 0.024), ('informative', 0.024), ('beneficial', 0.024), ('cai', 0.024), ('duan', 0.024), ('level', 0.023), ('contents', 0.023), ('ding', 0.023), ('correlation', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999869 <a title="85-tfidf-1" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>2 0.45530802 <a title="85-tfidf-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.29548645 <a title="85-tfidf-3" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>4 0.28291011 <a title="85-tfidf-4" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>5 0.2725834 <a title="85-tfidf-5" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>6 0.26881558 <a title="85-tfidf-6" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>7 0.24130121 <a title="85-tfidf-7" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>8 0.20929083 <a title="85-tfidf-8" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>9 0.20315754 <a title="85-tfidf-9" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>10 0.20250143 <a title="85-tfidf-10" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>11 0.19814441 <a title="85-tfidf-11" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>12 0.18568018 <a title="85-tfidf-12" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>13 0.16099717 <a title="85-tfidf-13" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>14 0.16086416 <a title="85-tfidf-14" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>15 0.1594779 <a title="85-tfidf-15" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>16 0.15826614 <a title="85-tfidf-16" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>17 0.15285683 <a title="85-tfidf-17" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>18 0.152155 <a title="85-tfidf-18" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>19 0.15029818 <a title="85-tfidf-19" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>20 0.14700457 <a title="85-tfidf-20" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.186), (1, -0.173), (2, -0.058), (3, -0.114), (4, 0.02), (5, 0.129), (6, -0.381), (7, 0.035), (8, 0.072), (9, 0.279), (10, 0.008), (11, 0.03), (12, -0.016), (13, -0.029), (14, 0.046), (15, 0.016), (16, -0.013), (17, 0.021), (18, -0.023), (19, -0.046), (20, -0.111), (21, 0.005), (22, -0.025), (23, -0.077), (24, -0.034), (25, -0.022), (26, -0.038), (27, -0.054), (28, 0.012), (29, 0.012), (30, 0.118), (31, -0.028), (32, 0.032), (33, -0.129), (34, -0.071), (35, 0.095), (36, 0.048), (37, 0.073), (38, 0.067), (39, 0.082), (40, 0.122), (41, -0.038), (42, -0.059), (43, -0.08), (44, 0.07), (45, 0.05), (46, -0.119), (47, 0.041), (48, 0.028), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97203457 <a title="85-lsi-1" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>2 0.76323205 <a title="85-lsi-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.73020405 <a title="85-lsi-3" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>4 0.70950824 <a title="85-lsi-4" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>5 0.70412135 <a title="85-lsi-5" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>Author: Weixin Li, Qian Yu, Harpreet Sawhney, Nuno Vasconcelos</p><p>Abstract: In this work, we propose a novel video representation for activity recognition that models video dynamics with attributes of activities. A video sequence is decomposed into short-term segments, which are characterized by the dynamics of their attributes. These segments are modeled by a dictionary of attribute dynamics templates, which are implemented by a recently introduced generative model, the binary dynamic system (BDS). We propose methods for learning a dictionary of BDSs from a training corpus, and for quantizing attribute sequences extracted from videos into these BDS codewords. This procedure produces a representation of the video as a histogram of BDS codewords, which is denoted the bag-of-words for attribute dynamics (BoWAD). An extensive experimental evaluation reveals that this representation outperforms other state-of-the-art approaches in temporal structure modeling for complex ac- tivity recognition.</p><p>6 0.70086747 <a title="85-lsi-6" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>7 0.66572183 <a title="85-lsi-7" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>8 0.65928179 <a title="85-lsi-8" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>9 0.64683527 <a title="85-lsi-9" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>10 0.59999484 <a title="85-lsi-10" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>11 0.59214103 <a title="85-lsi-11" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>12 0.57326353 <a title="85-lsi-12" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>13 0.56015259 <a title="85-lsi-13" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>14 0.53407115 <a title="85-lsi-14" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>15 0.5224185 <a title="85-lsi-15" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>16 0.4926258 <a title="85-lsi-16" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>17 0.46062016 <a title="85-lsi-17" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>18 0.44598651 <a title="85-lsi-18" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>19 0.42977667 <a title="85-lsi-19" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>20 0.41819382 <a title="85-lsi-20" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.09), (16, 0.045), (26, 0.035), (28, 0.015), (33, 0.276), (67, 0.052), (69, 0.047), (72, 0.015), (77, 0.063), (78, 0.218), (80, 0.022), (87, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86074644 <a title="85-lda-1" href="./cvpr-2013-Area_Preserving_Brain_Mapping.html">44 cvpr-2013-Area Preserving Brain Mapping</a></p>
<p>Author: Zhengyu Su, Wei Zeng, Rui Shi, Yalin Wang, Jian Sun, Xianfeng Gu</p><p>Abstract: Brain mapping transforms the brain cortical surface to canonical planar domains, which plays a fundamental role in morphological study. Most existing brain mapping methods are based on angle preserving maps, which may introduce large area distortions. This work proposes an area preserving brain mapping method based on MongeBrenier theory. The brain mapping is intrinsic to the Riemannian metric, unique, and diffeomorphic. The computation is equivalent to convex energy minimization and power Voronoi diagram construction. Comparing to the existing approaches based on Monge-Kantorovich theory, the proposed one greatly reduces the complexity (from n2 unknowns to n ), and improves the simplicity and efficiency. Experimental results on caudate nucleus surface mapping and cortical surface mapping demonstrate the efficacy and efficiency of the proposed method. Conventional methods for caudate nucleus surface mapping may suffer from numerical instability; in contrast, current method produces diffeomorpic mappings stably. In the study of cortical sur- face classification for recognition of Alzheimer’s Disease, the proposed method outperforms some other morphometry features.</p><p>same-paper 2 0.85869086 <a title="85-lda-2" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>3 0.82095253 <a title="85-lda-3" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>Author: Siddhartha Chandra, Shailesh Kumar, C.V. Jawahar</p><p>Abstract: Understanding the nature of data is the key to building good representations. In domains such as natural images, the data comes from very complex distributions which are hard to capture. Feature learning intends to discover or best approximate these underlying distributions and use their knowledge to weed out irrelevant information, preserving most of the relevant information. Feature learning can thus be seen as a form of dimensionality reduction. In this paper, we describe a feature learning scheme for natural images. We hypothesize that image patches do not all come from the same distribution, they lie in multiple nonlinear subspaces. We propose a framework that uses K Restricted Boltzmann Machines (K-RBMS) to learn multiple non-linear subspaces in the raw image space. Projections of the image patches into these subspaces gives us features, which we use to build image representations. Our algorithm solves the coupled problem of finding the right non-linear subspaces in the input space and associating image patches with those subspaces in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that representations based on our framework outperform the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks.</p><p>4 0.80777103 <a title="85-lda-4" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>Author: Xin Guo, Dong Liu, Brendan Jou, Mojun Zhu, Anni Cai, Shih-Fu Chang</p><p>Abstract: Object co-detection aims at simultaneous detection of objects of the same category from a pool of related images by exploiting consistent visual patterns present in candidate objects in the images. The related image set may contain a mixture of annotated objects and candidate objects generated by automatic detectors. Co-detection differs from the conventional object detection paradigm in which detection over each test image is determined one-by-one independently without taking advantage of common patterns in the data pool. In this paper, we propose a novel, robust approach to dramatically enhance co-detection by extracting a shared low-rank representation of the object instances in multiple feature spaces. The idea is analogous to that of the well-known Robust PCA [28], but has not been explored in object co-detection so far. The representation is based on a linear reconstruction over the entire data set and the low-rank approach enables effective removal of noisy and outlier samples. The extracted low-rank representation can be used to detect the target objects by spectral clustering. Extensive experiments over diverse benchmark datasets demonstrate consistent and significant performance gains of the proposed method over the state-of-the-art object codetection method and the generic object detection methods without co-detection formulations.</p><p>5 0.80710351 <a title="85-lda-5" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>6 0.80696303 <a title="85-lda-6" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>7 0.80646414 <a title="85-lda-7" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>8 0.80619586 <a title="85-lda-8" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>9 0.80466294 <a title="85-lda-9" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>10 0.79812455 <a title="85-lda-10" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>11 0.79480052 <a title="85-lda-11" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>12 0.79450846 <a title="85-lda-12" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>13 0.79436928 <a title="85-lda-13" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>14 0.79400194 <a title="85-lda-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.79393798 <a title="85-lda-15" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>16 0.79371947 <a title="85-lda-16" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>17 0.7936132 <a title="85-lda-17" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>18 0.79324156 <a title="85-lda-18" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>19 0.79289114 <a title="85-lda-19" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>20 0.79271102 <a title="85-lda-20" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
