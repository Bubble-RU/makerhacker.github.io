<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-157" href="#">cvpr2013-157</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</h1>
<br/><p>Source: <a title="cvpr-2013-157-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sun_Exploring_Implicit_Image_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xiaoshuai Sun, Xin-Jing Wang, Hongxun Yao, Lei Zhang</p><p>Abstract: In this paper, we propose a computational model of visual representativeness by integrating cognitive theories of representativeness heuristics with computer vision and machine learning techniques. Unlike previous models that build their representativeness measure based on the visible data, our model takes the initial inputs as explicit positive reference and extend the measure by exploring the implicit negatives. Given a group of images that contains obvious visual concepts, we create a customized image ontology consisting of both positive and negative instances by mining the most related and confusable neighbors of the positive concept in ontological semantic knowledge bases. The representativeness of a new item is then determined by its likelihoods for both the positive and negative references. To ensure the effectiveness of probability inference as well as the cognitive plausibility, we discover the potential prototypes and treat them as an intermediate representation of semantic concepts. In the experiment, we evaluate the performance of representativeness models based on both human judgements and user-click logs of commercial image search engine. Experimental results on both ImageNet and image sets of general concepts demonstrate the superior performance of our model against the state-of-the-arts.</p><p>Reference: <a title="cvpr-2013-157-reference" href="../cvpr2013_reference/cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn , Abstract In this paper, we propose a computational model of visual representativeness by integrating cognitive theories of representativeness heuristics with computer vision and machine learning techniques. [sent-4, score-1.52]
</p><p>2 Unlike previous models that build their representativeness measure based on the visible data, our model takes the initial inputs as explicit positive reference and extend the measure by exploring the implicit negatives. [sent-5, score-0.72]
</p><p>3 Given a group of images that contains obvious visual concepts, we create a customized image ontology consisting of both positive and negative instances by mining the most related and confusable neighbors of the positive concept in ontological semantic knowledge bases. [sent-6, score-0.888]
</p><p>4 The representativeness of a new item is then determined by its likelihoods for both the positive and negative references. [sent-7, score-0.783]
</p><p>5 To ensure the effectiveness of probability inference as well as the cognitive plausibility, we discover the potential prototypes and treat them as an intermediate representation of semantic concepts. [sent-8, score-0.475]
</p><p>6 In the experiment, we evaluate the performance of representativeness models based on both human judgements and user-click logs of commercial image search  engine. [sent-9, score-0.758]
</p><p>7 Introduction Measuring representativeness is an important basis for solving heuristic problems such as “How to choose five words to describe one of your friend? [sent-12, score-0.696]
</p><p>8 In the early studies ofrepresentativeness heuristic, Kahneman and Tversky [1] expressed representativeness according to which the subjective probability ofan item is determined by the degree to which it is similar in essential characteristics to its parent set. [sent-15, score-0.857]
</p><p>9 Based on this expression, the representativeness of an item in a given data set could be quantitatively measured ∗This work 2 {x j wang  ,  was  performed  at  Microsoft Research Asia  le i zhang} @mi cro s o ft . [sent-16, score-0.759]
</p><p>10 An illustration of representativeness function for b) Bayesian measure [6], c) naive prototypes, and d) our proposed model. [sent-18, score-0.734]
</p><p>11 Taking the prototypes as a middle-level representation, our model characterizes visual representativeness based on not only the visible data(*) but also the potential negatives (o) inferred from ontological knowledge bases. [sent-19, score-1.136]
</p><p>12 The paper focuses on three issues: 1) Given a set of images as positives, how to automatically acquire semantic reliable negatives; 2) How to discover all possible prototypes without knowing the precise number; 3) How to measure representativeness based on the customized image knowledge. [sent-20, score-1.235]
</p><p>13 [6] extended the Bayesian measure which defines the representativeness of a sample from a distribution [5] to define a measure of the representativeness of an item to a set. [sent-25, score-1.455]
</p><p>14 During the development of modern computer vision and multimedia technologies, especially for intelligent image browsing systems and search engines, people also investigate computational definition of representativeness to rank the retrieved images under keywords or example queries. [sent-26, score-0.811]
</p><p>15 In [9], a graph-based representation, namely ImageKB, was proposed to efficiently organize semantic categories, entities and billions of images. [sent-31, score-0.117]
</p><p>16 Given an   pair in ImageKB, the representa-  tiveness of an image is measured using semantic similarity, category relevance and representative confidence of its K nearest neighbors. [sent-32, score-0.201]
</p><p>17 Generally, most of the related works are consistent with ([8]-[14]) or directly derived from ([6, 15]) the classic Bayesian definition of representativeness [5]:  R(d,h) = logPP(h(h|d) = log? [sent-38, score-0.721]
</p><p>18 nTohtee representativeness R(d, h) is the ratio of posterior to prior probability, which characterizes the extent to which the observation of d could increase the probability of h. [sent-44, score-0.725]
</p><p>19 As shown in Figure 1, such statistical assumption leads to very coarse middlelevel representations of the items, and will possibly fail to provide an accurate representativeness function because the visual world is undoubtedly much more complex. [sent-51, score-0.753]
</p><p>20 The formal definition of our visual representativeness is as follows. [sent-53, score-0.727]
</p><p>21 Given x∗ as a test image, and Dr a reference data set containing multiple images gwei,th a snidm Dilar semantic concepts. [sent-54, score-0.141]
</p><p>22 The representativeness of x∗ for Dr is defined as:  R(x∗,Dr) =pp((xx∗∗||DDnr) =? [sent-55, score-0.696]
</p><p>23 nrpp((xx∗∗||nr) pp((nr| DDrn) ,  (2)  where r denotes a prototype of? [sent-57, score-0.276]
</p><p>24 Twhhiesr ed refi dneintiootne sh aas p trwotoo advantages: 1d) nIn astperaodt ootyf palel possible alternatives, it only focuses on the reference data and its negatives; 2) Probability inferences are conducted within a small yet compact prototype space. [sent-59, score-0.3]
</p><p>25 In the implementation phase, we focus on two main issues: 1) How to find negative references with reasonable semantic meanings; 2) How to discover all possible prototypes without knowing the pre-  cise number. [sent-60, score-0.401]
</p><p>26 For the first issue, we apply image ontology datasets, e. [sent-61, score-0.26]
</p><p>27 ImageNet [16] and ImageKB [9], to get the required semantic structure and relevant image instances and then group them together to form a negative reference set. [sent-63, score-0.165]
</p><p>28 For the other issue, we proposed a prototype discovery algorithm which automatically determines the number of prototypes based on an efficient statistical unimodality test. [sent-64, score-0.616]
</p><p>29 In summary, this paper makes the following contributions: • We proposed a semantic embedded visual representatWiveen persosp moseodde al. [sent-65, score-0.148]
</p><p>30 eAms a hybrid medoddeedl vdeisriuvaeld r efrpromese prototype theory and the Bayesian measure of representativeness, our model has a solid foundation from cognitive research on representativeness and use dynamic prototypes to get a flexible mid-level representation. [sent-66, score-1.326]
</p><p>31 • Ontological knowledge bases are adopted to embed vOisntuoallo sgeicmalan ktincosw line gthee proposed afrdaompteewdo trok ewmhbicehd avoids modeling the complex visual world with fixed types of statistical distribution and limited number of parameters. [sent-67, score-0.128]
</p><p>32 Together with the prototype theory, semantic embedding ensures a more accurate and meaningful measure of representativeness for general visual concepts. [sent-68, score-1.12]
</p><p>33 In Section 2, we provide some background information, including semantic ontology databases and the prototype theory. [sent-71, score-0.653]
</p><p>34 Section 3 then delivers the details of our semantic embedded representativeness model. [sent-72, score-0.813]
</p><p>35 We present the experimental results in Section 4 and give a discussion in Section 5 focusing on the relationship between the proposed representativeness and other visual properties such as saliency. [sent-73, score-0.727]
</p><p>36 Related Works Semantic Knowledge Base Semantic knowledge bases provide the relationships between words (WordNet[17]) or meaningful semantic concepts (NeedleSeek [18]) which establish the foundation of automatic semantic analysis for natural language processing and information retrieval. [sent-76, score-0.359]
</p><p>37 [16] proposed a largescale hierarchical image database named ImageNet based on the structure of WordNet which serves as an image ontology base containing 21,841 WordNet synsets and over 14 million highly selected images (201 1 Fall release). [sent-78, score-0.295]
</p><p>38 Compared to previous image datasets, ImageNet inherits the semantic hierarchy of WordNet and meanwhile provides high resolution images that are manually verified to contain the relevant concepts. [sent-79, score-0.117]
</p><p>39 The ImageNet distance exploits semantic similarity measured through the ImageNet semantic hierarchy, which outperforms and goes beyond direct visual distances in traditional vision research. [sent-81, score-0.265]
</p><p>40 The promising results [16, 19, 20] inspired us to embed semantic ontological knowledge into visual representativeness computation to make our model more reasonable and consistent with both semantic and appearance aspects of the visual world. [sent-82, score-1.148]
</p><p>41 Prototype Theory In cognitive science, the prototype theory [21, 22] states that categories tends to be defined in terms of prototypes or prototypical instances that contain the attributes most representative of items inside and least representative of items outside a category. [sent-83, score-0.892]
</p><p>42 Sufficiently specific categories can be defined as a single prototype represented by typical shapes and attributes [2, 19]. [sent-84, score-0.276]
</p><p>43 As shown in Figure 1, using prototypes as an intermediate representation has two advantages: 1) consistent with the cognitive understanding of semantic categories and 2) leads to a continuous and well-bounded measure function. [sent-85, score-0.418]
</p><p>44 Our model takes three kinds of input: 1) keywords that represent a concept, 2) keywords + the related images, and 3) unlabeled images. [sent-88, score-0.176]
</p><p>45 Taking the keyword as a seed, we build a customized image ontology based on large-scale semantic ontology databases and image search engines. [sent-90, score-0.888]
</p><p>46 The customized image ontology contains images for both the input concept as well as the confusable semantic neighbors (negative references). [sent-91, score-0.671]
</p><p>47 Potential prototypes are mined by a dynamic prototype discovery algorithm which is designed for arbitrary data dis-  Figure 2. [sent-92, score-0.543]
</p><p>48 Finally we estimate the representativeness of related images by Eq. [sent-95, score-0.696]
</p><p>49 Embedding Ontological Visual Semantics We embed visual semantics in the proposed model by customizing a small image ontology from semantic knowledge bases such as WordNet [17] and NeedleSeek [18]. [sent-99, score-0.479]
</p><p>50 The customized image ontology is built and organized like ImageNet containing both the semantic entities and relevant images. [sent-100, score-0.539]
</p><p>51 Note that ImageNet has covered over twenty thousand synsets of WordNet and most of its attached images have been manually verified, we can directly build our customized image ontology from ImageNet as long as the query entity is available. [sent-102, score-0.48]
</p><p>52 Given an image set, the construction ofthe customized image ontology can be done by the following steps: Step 1: Locating the Semantic Concept If the concept (represented as keyword) of the image set is given, we directly go to Step 2. [sent-103, score-0.483]
</p><p>53 If the concept is not specified, we obtain the keyword by simply applying the on-line annotation service from Google Image Search. [sent-104, score-0.147]
</p><p>54 Step 2: Finding the Negative References Given the keyword, we search for the most related and confusable concepts on either WordNet or NeedleSeek. [sent-105, score-0.179]
</p><p>55 Step 3: Building Customized Image Ontology For those concepts that are available on ImageNet, we directly attach the corresponding images as a part of our customized image ontology. [sent-107, score-0.243]
</p><p>56 Dynamic Prototype Discovery In the above section, we have constructed a customized image ontology which consists of two groups of images including Dr (the original input image set) and Dn (the negcatluivdei n rgef Deren(tchee image asle it)n. [sent-111, score-0.422]
</p><p>57 p Tth iem goal eot)f tahnids Dsection is to discover the prototypes from Dr and Dn as a middle-level representation footro our representativeness model. [sent-112, score-0.933]
</p><p>58 Algorithm 1 shows the dynamic prototype discov-  +  ery algorithm in details. [sent-117, score-0.3]
</p><p>59 Algorithm  1:  Algorithm 1: Dynamic prototype discovery  1 2 3 4  Input: Dataset X = {xi}iN=1 , the initial number of prototypes =kin {itx, a splitting number m, a statistic significance level α for the unimodality test, threshold vthd for spliting the candidate prototype. [sent-122, score-0.625]
</p><p>60 Output: Prototypes P = {pj }jk=1 and the conditional probability p(p|X) k ← kinitp ; Run k-means on X to obtain cluster centers C = {cj }jk=1 ; IRnuintia kl-imzee etahnes prototype sbetta by: Plus ← Ccen; repeat  16 17  18 19  for j = 1, . [sent-123, score-0.276]
</p><p>61 Top: image set of the Golden Gate Bridge; Bottom: the discovered prototypes with conditional probability. [sent-128, score-0.204]
</p><p>62 Our algorithm is able to incrementally discover semantically meaningful prototypes without knowing the exact number of potential topics. [sent-129, score-0.284]
</p><p>63 In this case, the prototypes summarize the image set by environmental conditions. [sent-130, score-0.204]
</p><p>64 Computing Representativeness We obtain the customized image ontology in Section 3. [sent-133, score-0.422]
</p><p>65 1, and the prototypes with conditional probabilities in Section  3. [sent-134, score-0.204]
</p><p>66 For simplicity, the conditional probability p(x∗ |r) is defined as the similarity between item x∗ and the underlying prototype r: p(x∗ |r) = exp{−λ|x∗ r|2}, (3) where λ is a scaling constant to keep p(x∗ |r) in a reasonable interval. [sent-137, score-0.339]
</p><p>67 Based on all pre-computed terms, itnhea frienaaslo score for representativeness is computed according to Eq. [sent-138, score-0.696]
</p><p>68 Note that, the user-click data are acquired from a different image search engine, specifically the Bing Image Search, in order to eliminate the potential ranking bias caused by the usage of query association data. [sent-145, score-0.125]
</p><p>69 By taking user-clicks as votes for the representativeness of tested images, we define another evaluation metric based on the choices made by Web-users: ? [sent-147, score-0.696]
</p><p>70 1  where iis the image index, Ri denotes the ranking position of image i, and UCi is the number of user clicks for test image irecorded in the query association log of Bing Search1 . [sent-151, score-0.17]
</p><p>71 Similar as [6], we asked human subjects to label the representativeness score ranging from 1 to 10 (10 for the best) for each image used in the experiment. [sent-155, score-0.722]
</p><p>72 Bayesian Model - The Bayesian Model [6] is a natural generalization of the cognitive theory of representativeness [5] and implemented based on Bayesian Sets [11] which is a  statistical technique initially proposed for measuring how appropriately a new sample can fit into a given set of data. [sent-163, score-0.848]
</p><p>73 , gxivNe}n ⊂ a dDat representing a conceptual group, Bayesian Sets measures th reep representativeness toufa a given sample x∗ ∈ {D\Ds} by the following equation:  Bscore(x∗,Ds) =p(px(x∗)∗p,(DDs)s). [sent-168, score-0.696]
</p><p>74 −  −  Naive Prototype Model -[2] We implemented the naive prototype model of representativeness following the procedure of [6]. [sent-200, score-1.01]
</p><p>75 Given a dataset D, we select its prototype sample by: ? [sent-201, score-0.276]
</p><p>76 The representativeness score is then defined as the similarity between the input and the prototype:  NPT(x∗, Ds) = exp{−λ|x∗ − xproto|2},  (10)  where xproto, represented as a BoW vector, is the prototype of Ds, and λ is a scaling constant which actually does not oafff Dect the ranking results. [sent-206, score-1.023]
</p><p>77 Ranking Images on ImageNet ImageNet [16] is a large-scale image ontology dataset which provides us the essential knowledge of the visual world including not only semantic hierarchies but also the relevant image instances. [sent-209, score-0.431]
</p><p>78 Although all images in ImageNet are manually verified to contain the relevant concepts, their quality and representativeness are still left un-labeled. [sent-210, score-0.696]
</p><p>79 Practically, given a key-  word, we search for the k most related words using a public available semantic ontology named NeedleSeek [18]. [sent-227, score-0.404]
</p><p>80 Then, we crawl images by querying Google with all the related keywords to build a customized image ontology. [sent-228, score-0.25]
</p><p>81 Based on the auto-built image ontology, we set up the representativeness model following the procedure in Section 3. [sent-229, score-0.696]
</p><p>82 Note that this procedure can be applied to refine the results of commercial image search engines since it is fully automatic, semantic-aware, and psychological plausible. [sent-231, score-0.118]
</p><p>83 We test the representativeness models with three concepts: Wolf(animal), Paris(city) and Rose (flower). [sent-232, score-0.696]
</p><p>84 For each keyword, we crawled 200 images from Google Image Search3 to build the customized image ontology. [sent-234, score-0.162]
</p><p>85 Related keywords of the tested concepts  conceptrelated keywords at NeedleSeek [18]  PRWaorsliefcbRPwhoasimredlf,stbmuloiBSbyeop,cdsralnitec,haylfre ,LoknsWda,xin ldteshvornim,glceTuotmy,nkogtldNeao,iwbBseylau,Dirsjcenablhgri,avMteor,nsbclaowrk,5. [sent-238, score-0.292]
</p><p>86 Thus, our representativeness can be explained as “Likelihood + 2http : //www. [sent-263, score-0.696]
</p><p>87 Unlike the saliency method, our model locates those regions which contain both salient and discriminative contents such as the golden roof of the Chinese Palace Museum and the huge pillars of the German Berlin Museum. [sent-273, score-0.147]
</p><p>88 Saliency”, which favors the items that are not only well fitted into the observed concept but also remarkably salient to other related, confusable concepts. [sent-274, score-0.191]
</p><p>89 Figure 6 shows some comparisons between our representativeness model  ×  and AIM saliency (Attention by Information Maximization [3 1]) on natural images. [sent-275, score-0.769]
</p><p>90 To show the real differences, we use the same features to compute the response map of our representativeness model. [sent-277, score-0.696]
</p><p>91 the building corners and human bodies, whereas our model favors the representative components such as the golden roof and huge pillars which are indeed the most recognizable elements for eastern and western architectural styles. [sent-287, score-0.158]
</p><p>92 Evaluation Bias In the experiment, our second evaluation metric SW explicitly characterizes the representativeness of a given image by the number ofuser-clicks the image has received. [sent-288, score-0.725]
</p><p>93 The potential problem with this metric is that users might click an image according to their person-  al interests instead of the real semantic relevance. [sent-290, score-0.186]
</p><p>94 However, such imperfection does not affect the validity of this evaluation metric for comparing the relative performance of different representativeness models. [sent-292, score-0.696]
</p><p>95 Conclusion In this paper, we have introduced a novel computational model for visual representativeness based on ontological semantic embedding and dynamic prototype discovery. [sent-298, score-1.25]
</p><p>96 The embedded image ontology provides additional image statistics helping the model to identify true outliers. [sent-300, score-0.26]
</p><p>97 Meanwhile, the intermediate prototype representation enhances the cognitive plausibil-  ity of our model and ensures the accuracy and effectiveness of the probabilistic inference. [sent-301, score-0.373]
</p><p>98 Experimental results demonstrate the superior performance of the proposed approach against the state-of-the-art representativeness models as well as commercial image search engines. [sent-302, score-0.758]
</p><p>99 Testing a bayesian measure of representativeness using a large image database. [sent-349, score-0.775]
</p><p>100 Sun: A bayesian framework for saliency using natural  statistics. [sent-502, score-0.152]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('representativeness', 0.696), ('prototype', 0.276), ('ontology', 0.26), ('prototypes', 0.204), ('imagenet', 0.164), ('customized', 0.162), ('needleseek', 0.141), ('semantic', 0.117), ('ontological', 0.106), ('cognitive', 0.097), ('keywords', 0.088), ('wordnet', 0.084), ('representative', 0.084), ('concepts', 0.081), ('bayesian', 0.079), ('saliency', 0.073), ('confusable', 0.071), ('unimodality', 0.071), ('log', 0.065), ('item', 0.063), ('subjective', 0.063), ('keyword', 0.062), ('concept', 0.061), ('google', 0.06), ('items', 0.059), ('abbott', 0.053), ('imagekb', 0.053), ('xproto', 0.053), ('ranking', 0.051), ('jj', 0.05), ('dr', 0.05), ('museum', 0.047), ('npt', 0.047), ('psychology', 0.047), ('sw', 0.043), ('discovery', 0.039), ('golden', 0.039), ('naive', 0.038), ('bm', 0.038), ('bscore', 0.035), ('conceptrelated', 0.035), ('harbin', 0.035), ('hongxun', 0.035), ('kahneman', 0.035), ('lihood', 0.035), ('ofrepresentativeness', 0.035), ('palace', 0.035), ('pillars', 0.035), ('synsets', 0.035), ('vthd', 0.035), ('xiaoshuai', 0.035), ('commercial', 0.035), ('psychological', 0.035), ('mining', 0.033), ('ds', 0.033), ('discover', 0.033), ('paris', 0.033), ('bing', 0.032), ('griffiths', 0.031), ('clicks', 0.031), ('heller', 0.031), ('dn', 0.031), ('visual', 0.031), ('bow', 0.03), ('theory', 0.029), ('ghahramani', 0.029), ('xij', 0.029), ('characterizes', 0.029), ('bruce', 0.027), ('search', 0.027), ('embed', 0.027), ('deng', 0.026), ('statistical', 0.026), ('lik', 0.026), ('skewed', 0.026), ('rational', 0.026), ('kennedy', 0.026), ('iconic', 0.026), ('asked', 0.026), ('practically', 0.025), ('classic', 0.025), ('potential', 0.024), ('reference', 0.024), ('click', 0.024), ('geographic', 0.024), ('service', 0.024), ('doersch', 0.024), ('negative', 0.024), ('dynamic', 0.024), ('knowledge', 0.023), ('knowing', 0.023), ('query', 0.023), ('negatives', 0.023), ('tenenbaum', 0.022), ('ss', 0.021), ('users', 0.021), ('bases', 0.021), ('bodies', 0.021), ('engine', 0.021), ('engines', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="157-tfidf-1" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>Author: Xiaoshuai Sun, Xin-Jing Wang, Hongxun Yao, Lei Zhang</p><p>Abstract: In this paper, we propose a computational model of visual representativeness by integrating cognitive theories of representativeness heuristics with computer vision and machine learning techniques. Unlike previous models that build their representativeness measure based on the visible data, our model takes the initial inputs as explicit positive reference and extend the measure by exploring the implicit negatives. Given a group of images that contains obvious visual concepts, we create a customized image ontology consisting of both positive and negative instances by mining the most related and confusable neighbors of the positive concept in ontological semantic knowledge bases. The representativeness of a new item is then determined by its likelihoods for both the positive and negative references. To ensure the effectiveness of probability inference as well as the cognitive plausibility, we discover the potential prototypes and treat them as an intermediate representation of semantic concepts. In the experiment, we evaluate the performance of representativeness models based on both human judgements and user-click logs of commercial image search engine. Experimental results on both ImageNet and image sets of general concepts demonstrate the superior performance of our model against the state-of-the-arts.</p><p>2 0.25621802 <a title="157-tfidf-2" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>Author: Paul Wohlhart, Martin Köstinger, Michael Donoser, Peter M. Roth, Horst Bischof</p><p>Abstract: The development of complex, powerful classifiers and their constant improvement have contributed much to the progress in many fields of computer vision. However, the trend towards large scale datasets revived the interest in simpler classifiers to reduce runtime. Simple nearest neighbor classifiers have several beneficial properties, such as low complexity and inherent multi-class handling, however, they have a runtime linear in the size of the database. Recent related work represents data samples by assigning them to a set of prototypes that partition the input feature space and afterwards applies linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper, we go a step beyond these approaches and purely focus on 1-nearest prototype classification, where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable, parameter free, avoids noise overfitting and, since during testing only comparisons to the derived prototypes are required, highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods, while even getting close to the results of more complex classifiers.</p><p>3 0.11362593 <a title="157-tfidf-3" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>Author: Quannan Li, Jiajun Wu, Zhuowen Tu</p><p>Abstract: Obtaining effective mid-level representations has become an increasingly important task in computer vision. In this paper, we propose a fully automatic algorithm which harvests visual concepts from a large number of Internet images (more than a quarter of a million) using text-based queries. Existing approaches to visual concept learning from Internet images either rely on strong supervision with detailed manual annotations or learn image-level classifiers only. Here, we take the advantage of having massive wellorganized Google and Bing image data; visual concepts (around 14, 000) are automatically exploited from images using word-based queries. Using the learned visual concepts, we show state-of-the-art performances on a variety of benchmark datasets, which demonstrate the effectiveness of the learned mid-level representations: being able to generalize well to general natural images. Our method shows significant improvement over the competing systems in image classification, including those with strong supervision.</p><p>4 0.090312533 <a title="157-tfidf-4" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>5 0.086656749 <a title="157-tfidf-5" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>6 0.084049381 <a title="157-tfidf-6" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>7 0.082274795 <a title="157-tfidf-7" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>8 0.079987593 <a title="157-tfidf-8" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>9 0.072585016 <a title="157-tfidf-9" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>10 0.071662918 <a title="157-tfidf-10" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>11 0.071548603 <a title="157-tfidf-11" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>12 0.071208738 <a title="157-tfidf-12" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>13 0.07114362 <a title="157-tfidf-13" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>14 0.069571979 <a title="157-tfidf-14" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>15 0.067048505 <a title="157-tfidf-15" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>16 0.066191003 <a title="157-tfidf-16" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>17 0.064576611 <a title="157-tfidf-17" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>18 0.061759416 <a title="157-tfidf-18" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>19 0.060827609 <a title="157-tfidf-19" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>20 0.059106901 <a title="157-tfidf-20" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.121), (1, -0.071), (2, 0.06), (3, 0.048), (4, 0.028), (5, 0.013), (6, -0.056), (7, -0.004), (8, -0.023), (9, 0.013), (10, -0.005), (11, 0.018), (12, -0.014), (13, 0.012), (14, -0.028), (15, -0.037), (16, 0.018), (17, 0.005), (18, 0.014), (19, -0.052), (20, 0.028), (21, -0.02), (22, 0.014), (23, 0.042), (24, -0.025), (25, -0.001), (26, 0.009), (27, 0.032), (28, 0.013), (29, -0.02), (30, -0.028), (31, -0.001), (32, -0.063), (33, -0.002), (34, -0.017), (35, -0.037), (36, -0.099), (37, -0.001), (38, -0.102), (39, -0.09), (40, -0.072), (41, -0.06), (42, 0.003), (43, 0.041), (44, 0.055), (45, 0.051), (46, -0.005), (47, -0.017), (48, 0.018), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91118276 <a title="157-lsi-1" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>Author: Xiaoshuai Sun, Xin-Jing Wang, Hongxun Yao, Lei Zhang</p><p>Abstract: In this paper, we propose a computational model of visual representativeness by integrating cognitive theories of representativeness heuristics with computer vision and machine learning techniques. Unlike previous models that build their representativeness measure based on the visible data, our model takes the initial inputs as explicit positive reference and extend the measure by exploring the implicit negatives. Given a group of images that contains obvious visual concepts, we create a customized image ontology consisting of both positive and negative instances by mining the most related and confusable neighbors of the positive concept in ontological semantic knowledge bases. The representativeness of a new item is then determined by its likelihoods for both the positive and negative references. To ensure the effectiveness of probability inference as well as the cognitive plausibility, we discover the potential prototypes and treat them as an intermediate representation of semantic concepts. In the experiment, we evaluate the performance of representativeness models based on both human judgements and user-click logs of commercial image search engine. Experimental results on both ImageNet and image sets of general concepts demonstrate the superior performance of our model against the state-of-the-arts.</p><p>2 0.68998885 <a title="157-lsi-2" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>Author: Paul Wohlhart, Martin Köstinger, Michael Donoser, Peter M. Roth, Horst Bischof</p><p>Abstract: The development of complex, powerful classifiers and their constant improvement have contributed much to the progress in many fields of computer vision. However, the trend towards large scale datasets revived the interest in simpler classifiers to reduce runtime. Simple nearest neighbor classifiers have several beneficial properties, such as low complexity and inherent multi-class handling, however, they have a runtime linear in the size of the database. Recent related work represents data samples by assigning them to a set of prototypes that partition the input feature space and afterwards applies linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper, we go a step beyond these approaches and purely focus on 1-nearest prototype classification, where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable, parameter free, avoids noise overfitting and, since during testing only comparisons to the derived prototypes are required, highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods, while even getting close to the results of more complex classifiers.</p><p>3 0.6028688 <a title="157-lsi-3" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>Author: Quannan Li, Jiajun Wu, Zhuowen Tu</p><p>Abstract: Obtaining effective mid-level representations has become an increasingly important task in computer vision. In this paper, we propose a fully automatic algorithm which harvests visual concepts from a large number of Internet images (more than a quarter of a million) using text-based queries. Existing approaches to visual concept learning from Internet images either rely on strong supervision with detailed manual annotations or learn image-level classifiers only. Here, we take the advantage of having massive wellorganized Google and Bing image data; visual concepts (around 14, 000) are automatically exploited from images using word-based queries. Using the learned visual concepts, we show state-of-the-art performances on a variety of benchmark datasets, which demonstrate the effectiveness of the learned mid-level representations: being able to generalize well to general natural images. Our method shows significant improvement over the competing systems in image classification, including those with strong supervision.</p><p>4 0.57947886 <a title="157-lsi-4" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>5 0.57745039 <a title="157-lsi-5" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>6 0.56916547 <a title="157-lsi-6" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>7 0.56394845 <a title="157-lsi-7" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>8 0.55671936 <a title="157-lsi-8" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>9 0.55276173 <a title="157-lsi-9" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>10 0.53831518 <a title="157-lsi-10" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>11 0.53107649 <a title="157-lsi-11" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>12 0.52640426 <a title="157-lsi-12" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>13 0.52034479 <a title="157-lsi-13" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>14 0.5122233 <a title="157-lsi-14" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>15 0.50655323 <a title="157-lsi-15" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>16 0.50474656 <a title="157-lsi-16" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>17 0.49884745 <a title="157-lsi-17" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>18 0.4971737 <a title="157-lsi-18" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>19 0.49657893 <a title="157-lsi-19" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>20 0.49057719 <a title="157-lsi-20" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.097), (14, 0.297), (16, 0.026), (26, 0.046), (33, 0.225), (67, 0.058), (69, 0.054), (80, 0.015), (87, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75912863 <a title="157-lda-1" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>Author: Xiaoshuai Sun, Xin-Jing Wang, Hongxun Yao, Lei Zhang</p><p>Abstract: In this paper, we propose a computational model of visual representativeness by integrating cognitive theories of representativeness heuristics with computer vision and machine learning techniques. Unlike previous models that build their representativeness measure based on the visible data, our model takes the initial inputs as explicit positive reference and extend the measure by exploring the implicit negatives. Given a group of images that contains obvious visual concepts, we create a customized image ontology consisting of both positive and negative instances by mining the most related and confusable neighbors of the positive concept in ontological semantic knowledge bases. The representativeness of a new item is then determined by its likelihoods for both the positive and negative references. To ensure the effectiveness of probability inference as well as the cognitive plausibility, we discover the potential prototypes and treat them as an intermediate representation of semantic concepts. In the experiment, we evaluate the performance of representativeness models based on both human judgements and user-click logs of commercial image search engine. Experimental results on both ImageNet and image sets of general concepts demonstrate the superior performance of our model against the state-of-the-arts.</p><p>2 0.73475391 <a title="157-lda-2" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>Author: Xiaogang Chen, Sing Bing Kang, Jie Yang, Jingyi Yu</p><p>Abstract: Patch-based methods such as Non-Local Means (NLM) and BM3D have become the de facto gold standard for image denoising. The core of these approaches is to use similar patches within the image as cues for denoising. The operation usually requires expensive pair-wise patch comparisons. In this paper, we present a novel fast patch-based denoising technique based on Patch Geodesic Paths (PatchGP). PatchGPs treat image patches as nodes and patch differences as edge weights for computing the shortest (geodesic) paths. The path lengths can then be used as weights of the smoothing/denoising kernel. We first show that, for natural images, PatchGPs can be effectively approximated by minimum hop paths (MHPs) that generally correspond to Euclidean line paths connecting two patch nodes. To construct the denoising kernel, we further discretize the MHP search directions and use only patches along the search directions. Along each MHP, we apply a weightpropagation scheme to robustly and efficiently compute the path distance. To handle noise at multiple scales, we conduct wavelet image decomposition and apply PatchGP scheme at each scale. Comprehensive experiments show that our approach achieves comparable quality as the state-of-the-art methods such as NLM and BM3D but is a few orders of magnitude faster.</p><p>3 0.70040542 <a title="157-lda-3" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>Author: Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah</p><p>Abstract: We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark con- trast to datasets usedfor existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.</p><p>4 0.6992209 <a title="157-lda-4" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>5 0.66591012 <a title="157-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.66319561 <a title="157-lda-6" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>7 0.6629678 <a title="157-lda-7" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>8 0.66145313 <a title="157-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.66090631 <a title="157-lda-9" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>10 0.66063839 <a title="157-lda-10" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>11 0.66038144 <a title="157-lda-11" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>12 0.66023499 <a title="157-lda-12" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>13 0.65999669 <a title="157-lda-13" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>14 0.6598013 <a title="157-lda-14" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>15 0.65976715 <a title="157-lda-15" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>16 0.65965241 <a title="157-lda-16" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>17 0.65946507 <a title="157-lda-17" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>18 0.65895087 <a title="157-lda-18" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>19 0.65881956 <a title="157-lda-19" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>20 0.65858388 <a title="157-lda-20" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
