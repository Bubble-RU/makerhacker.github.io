<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-385" href="#">cvpr2013-385</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</h1>
<br/><p>Source: <a title="cvpr-2013-385-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chu_Selective_Transfer_Machine_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>Reference: <a title="cvpr-2013-385-reference" href="../cvpr2013_reference/cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Cohn†‡ †Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 ‡Department of Psychology, University of Pittsburgh, Pittsburgh, PA 15260  Abstract Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. [sent-2, score-0.472]
</p><p>2 Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. [sent-8, score-0.114]
</p><p>3 The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). [sent-10, score-0.25]
</p><p>4 We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. [sent-11, score-0.228]
</p><p>5 STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. [sent-12, score-0.156]
</p><p>6 To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases:  CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. [sent-13, score-0.114]
</p><p>7 FACS segments the visible effects of facial muscle activation into “action units” (AUs). [sent-19, score-0.175]
</p><p>8 Each AU is related to one or more facial muscles. [sent-20, score-0.175]
</p><p>9 FACS describes facial activity on the basis of 33 unique action units (AUs), as well as several categories of head and eye positions and other movements. [sent-21, score-0.225]
</p><p>10 Automatic facial action unit detection (AFA) confronts a  (sTarua)binjeinctgs  for AU12 (lip-corner raiser). [sent-23, score-0.25]
</p><p>11 Selective transfer machine, which personalizes the generic classifier, reliably separates AU12 for the unseen subject. [sent-25, score-0.128]
</p><p>12 Then there is the challenge of automatically detecting facial actions that require significant training and expertise even for human coders, as has been recently reported in the first Facial Expression Recognition and Analysis Challenge [32]. [sent-31, score-0.222]
</p><p>13 While improvements have been achieved, generalizability of classifiers to previously unseen persons remains a continuing challenge. [sent-34, score-0.08]
</p><p>14 1(a) illustrates an example of how a simple linear classifier can separate the positive samples of AU12 (obliquely raised lip corners, seen in smiling) from negative samples (i. [sent-36, score-0.127]
</p><p>15 However, when a classifier is learned using training data from all subjects (Fig. [sent-40, score-0.148]
</p><p>16 1(b)) and tested on a subject excluded from the training set, it fails to generalize well. [sent-41, score-0.068]
</p><p>17 When a classifier is trained on all available subjects, it is referred as generic. [sent-42, score-0.067]
</p><p>18 Our guiding hypothesis is that these factors lead generic classifiers to perform better or worse on some subjects than others. [sent-45, score-0.17]
</p><p>19 To mitigate the person-specific biases, this paper ex-  plores the idea of personalizing a generic classifier. [sent-46, score-0.103]
</p><p>20 Generic classifiers are personalized using no AU labels from test subjects. [sent-47, score-0.135]
</p><p>21 STM personalizes the generic classifier in an unsupervised manner to compensate for person-specific biases, and greatly improves generalizability, see Fig. [sent-49, score-0.134]
</p><p>22 We illustrate the benefits of our approach in the task of facial AU detection in three major datasets of posed and spontaneous facial expressions. [sent-51, score-0.416]
</p><p>23 To the best of our knowledge, this is the first work to investigate personalizing a classifier for facial expression analysis. [sent-52, score-0.307]
</p><p>24 Tracking non-rigid facial features has been a long standing problem in computer vision. [sent-60, score-0.175]
</p><p>25 Common to all of these approaches is the assumption that training and test data come from the same distribution. [sent-76, score-0.08]
</p><p>26 It therefore seeks to personalize the classifier by automatically re-weighting training samples that are most relevant to each test subject. [sent-78, score-0.209]
</p><p>27 Torralba and Efros [3 1] discovered significant biases in object categorization; as a remedy, they encouraged advances in domain adaptation to cope with dataset biases. [sent-83, score-0.111]
</p><p>28 Aytar and Zisserman [1] proposed to transfer pre-learned models to regularize the training of a new object class. [sent-84, score-0.086]
</p><p>29 They cannot be applied to new domains or subjects when one has no prior knowledge of them. [sent-88, score-0.077]
</p><p>30 In contrast, our approach is fully unsupervised, uses no labeled instances, and hence well suited to the problem of generalizing learning to new domains or new subjects in our case. [sent-89, score-0.077]
</p><p>31 Close to our approach is a special case in unsupervised domain adaptation known as covariate shift [28], where training and test domains follow different distributions but the label distributions remain the same. [sent-90, score-0.265]
</p><p>32 SVM-KNN [38] labels a single query using an SVM trained on its k neighborhood of the training data. [sent-96, score-0.069]
</p><p>33 Unlike previous approaches, STM learns weights on individual training instances and hence makes better use of the data. [sent-98, score-0.094]
</p><p>34 Considering distribution mismatch, Kernel Mean Matching (KMM) [16] directly infers the re-sampling weights by matching training and test distributions. [sent-99, score-0.109]
</p><p>35 [36] estimated the relative importance weight and learn from weighted training samples for 3D human pose estimation. [sent-101, score-0.078]
</p><p>36 On the contrary, STM jointly optimize the weights as well as the classifier parameters, and hence preserves discriminant property of the new decision boundary. [sent-104, score-0.096]
</p><p>37 Selective Transfer Machine (STM) This section describes the proposed STM approach for personalizing a generic classifier. [sent-108, score-0.103]
</p><p>38 Problem formulation: The main idea behind the STM is to re-weight more the training samples that are closer to the test samples. [sent-112, score-0.111]
</p><p>39 The classifiers trained on the re-weighted training samples will be more likely to fit the test subject. [sent-113, score-0.184]
</p><p>40 Ωs (Xtr, Xte) measures the distribution mismatch between the training and test distribution as a function of s. [sent-121, score-0.125]
</p><p>41 T trhade goal oof b tahleSTM is to jointly optimize the penalized SVM w as well as the selective coefficient s, such that the resulting personalized classifier can better remove person-specific biases. [sent-129, score-0.223]
</p><p>42 Penalized SVM: The first term in STM, Rw(Dtr, s), is the empirical risk of a penalized SVM, where ea(cDh training instance is weighted by its relevance to the test data. [sent-130, score-0.147]
</p><p>43 Using the representer theorem [8], the penalized iSnVgM ϕ (i·n) . [sent-138, score-0.067]
</p><p>44 Domain mismatch: The second term in STM, Ωs (Xtr, Xte), is the domain mismatch, and it has the objective to find a re-weighting function for minimizing the mismatch between training and test domains. [sent-147, score-0.151]
</p><p>45 An intuitive way to reassign weights is to compute the ratio between training and test densities. [sent-152, score-0.109]
</p><p>46 Here we adopt the Kernel Mean Matching (KMM) [16] method to reduce the difference between the means of the training and test distributions in the Reproducing Kernel Hilbert Space H. [sent-154, score-0.08]
</p><p>47 between training and each test sample, finding a suitable s? [sent-179, score-0.08]
</p><p>48 Circles represent the training data and squares the test data. [sent-183, score-0.08]
</p><p>49 As can be observed, KMM put higher weights in the training samples closer to the test samples. [sent-185, score-0.14]
</p><p>50 T-SVM [17] equally weights all the training data; by contrast, STM gives greater weight to training data that are more relevant to a given test subject. [sent-218, score-0.156]
</p><p>51 On the other hand, STM is formulated as a biconvex problem and therefore assures convergence. [sent-220, score-0.07]
</p><p>52 KMM does re-weighing only once, while STM does so in Algorithm 1: Selective Transfer Machine  Input : Xtr, Xte, parameters C, λ Output: Classifier w and instance-wise weights s 1 Initialize training loss ? [sent-222, score-0.102]
</p><p>53 DA-SVM [5], similar to TSVM, learns a classifier without re-weighting the training data. [sent-228, score-0.092]
</p><p>54 (1) we adopt the Alternate Convex Search method [15] that alternates between solving two convex subproblems over the hyperplane w and the selective coefficient s. [sent-233, score-0.087]
</p><p>55 Figures it # 1,4,8,1 2 with training/test accuracy (Tr% and Te%) show the hyperplanes in corresponding iterations, where grey (shaded) dots denote training data and white (unshaded)  dots denote test  data; circle/square patterns denote positive/negative classes respectively. [sent-267, score-0.08]
</p><p>56 STM improves separation relative to generic SVM as early as the first iteration and converges close to the ideal hyperplane by the 12-th iteration. [sent-269, score-0.123]
</p><p>57 Introducing the training loss helps preserve the discriminant property of the new decision boundary, and hence leads to a personalized classifier that is close to the ideal one. [sent-273, score-0.224]
</p><p>58 On the other hand, STM simultaneously considers training loss and the weightings, and thus encourages the training samples close to the test samples be well classified. [sent-279, score-0.215]
</p><p>59 Minimizing over w: In the case of training loss ? [sent-282, score-0.073]
</p><p>60 2 being quadratic, the gradient and Hessian of the penalized linear SVM in (2) can be written as: ? [sent-283, score-0.067]
</p><p>61 the expansion coefficient β for the penalized nonlinear SVM in (3) as: ? [sent-297, score-0.067]
</p><p>62 Experiments STM was compared for AU detection with generic SVM and cross-domain learning approaches in three widely used databases that vary in duration, extent of out-of-plane head motion, and spontaneity of facial expression. [sent-303, score-0.257]
</p><p>63 Image sequences average about 20 frames in length; they begin with neutral expression and proceed to a peak, which is AU-labelled. [sent-308, score-0.072]
</p><p>64 6  to specific face regions, descriptors were computed within 36 36 pixel regions at predetermined facial landmarks (9 f3o6r× ×th3e6 upper rfeagceio oannsd a a7t fporre tdheete lromwienre dfac fea)c. [sent-360, score-0.198]
</p><p>65 Positive samples were frames in which a given AU was  present, and negative samples in which it was not. [sent-363, score-0.087]
</p><p>66 The other meaning, which we refer to as PS2 or quasiPS, is a classifier that has been tested on a subject that was included among others in a training set. [sent-374, score-0.113]
</p><p>67 For instance, consider the case in which data from five subjects are randomly assigned to training and testing sets. [sent-375, score-0.103]
</p><p>68 A PS2 classifier is trained and then tested on the test set. [sent-376, score-0.1]
</p><p>69 It is not surprising that PS2-SVM perform better than PS1-SVM since PS1-SVM was trained only on limited training data and thus suffers from overfitting. [sent-382, score-0.069]
</p><p>70 As PS2-SVM was trained on all available subjects, it can be viewed as a generic classifier, as used in most literature on AU detection. [sent-383, score-0.085]
</p><p>71 1, generic classifiers could suffer from the biases and lead to suboptimal performance. [sent-385, score-0.155]
</p><p>72 On the other hand, STM consistently outperforms both person-specific classifiers since STM allows to select only relevant training data and fits better the test distribution. [sent-386, score-0.131]
</p><p>73 Each entry shows the portion of selected training samples w. [sent-405, score-0.078]
</p><p>74 Each row sums to 1 and each entry shows the portion of selected samples of training subjects with respect to each test subject. [sent-410, score-0.167]
</p><p>75 4(b), when STM converges, it selects most of the training data that belongs to the target subject (higher diagonal values). [sent-412, score-0.068]
</p><p>76 Comparison with generic classifiers and domain adaptation approaches This experiment compares the performance of STM  against generic classifiers learned on the entire dataset, the covariate shift method KMM [16], a semi-supervised T-SVM [10], and the domain adaptation method DASVM [5]. [sent-415, score-0.462]
</p><p>77 In this experiment, any sample of the test subjects is excluded from training. [sent-417, score-0.089]
</p><p>78 Unlike STM that used a penalized SVM, T-SVM did not consider re-weighting for training instances and make use  of the losses for all training data. [sent-458, score-0.179]
</p><p>79 DA-SVM extends T-SVM by progressively labelling test patterns and removing labelled training patterns. [sent-460, score-0.08]
</p><p>80 Not surprisingly, DA-SVM shows better performance than KMM and T-SVM, because it used more relevant training samples and resulted in a better personalized classifier. [sent-461, score-0.129]
</p><p>81 By contrast, STM is a biconvex formulation, and therefore guarantees to converge to a critical point and outperforms existing approaches. [sent-465, score-0.092]
</p><p>82 Conclusions This paper proposed a transductive method to personalize a generic classifier for facial Action Unit (AU) detection. [sent-491, score-0.381]
</p><p>83 Our STM framework simultaneously learns the parameters of a classifier and the selective weights that minimizes the mismatch between the training and the test distributions. [sent-492, score-0.259]
</p><p>84 We show that STM translates to a biconvex problem, and propose a simple alternated minimization approach to optimize it in the primal. [sent-493, score-0.093]
</p><p>85 By attenuating the influence of inherent biases in morphology and behavior, we have shown that STM can achieve results that surpass non-personalized generic classifiers and approach the performance of classifiers that have been trained for individual persons (i. [sent-494, score-0.25]
</p><p>86 The results have clearly demonstrated that STM outperforms existing classifiers when using the same protocol for training and testing. [sent-497, score-0.098]
</p><p>87 This leads to high values in the estimated weights for training instances that were not reliable. [sent-501, score-0.094]
</p><p>88 Automatic recognition of facial actions in  [3]  [4]  [5]  [6]  [7]  [8] [9]  [10] [11] [12] [13]  spontaneous expressions. [sent-519, score-0.218]
</p><p>89 Learning partiallyobserved hidden conditional random fields for facial expression recognition. [sent-562, score-0.222]
</p><p>90 Facial action coding system: A technique for the measurement of facial movement. [sent-608, score-0.225]
</p><p>91 Biconvex sets and optimization with biconvex functions: a survey and extensions. [sent-614, score-0.07]
</p><p>92 The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression. [sent-655, score-0.075]
</p><p>93 A model of the perception of facial expressions of emotion by humans: Research overview and perspectives. [sent-660, score-0.195]
</p><p>94 Kernel conditional ordinal random fields for temporal segmentation of facial action units. [sent-678, score-0.247]
</p><p>95 Nonparametric discriminant HMM and application to facial expression recognition. [sent-689, score-0.244]
</p><p>96 Direct importance estimation with model selection and its application to covariate shift adaptation. [sent-712, score-0.094]
</p><p>97 Facial action unit recognition by exploiting their dynamic and semantic relationships. [sent-718, score-0.075]
</p><p>98 Fully automatic recognition of the temporal phases of facial actions. [sent-739, score-0.175]
</p><p>99 No bias left behind: Covariate shift adaptation for discriminative 3d pose estimation. [sent-763, score-0.079]
</p><p>100 Dynamic cascades with bidirectional bootstrapping for action unit detection in spontaneous facial behavior. [sent-787, score-0.293]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stm', 0.809), ('kmm', 0.357), ('facial', 0.175), ('au', 0.133), ('biconvex', 0.07), ('aus', 0.068), ('penalized', 0.067), ('generic', 0.063), ('auc', 0.062), ('svm', 0.061), ('selective', 0.06), ('dasvm', 0.059), ('covariate', 0.059), ('ck', 0.056), ('subjects', 0.056), ('personalize', 0.053), ('personalized', 0.051), ('classifiers', 0.051), ('action', 0.05), ('training', 0.047), ('expression', 0.047), ('classifier', 0.045), ('mismatch', 0.045), ('transductive', 0.045), ('ntr', 0.045), ('adaptation', 0.044), ('spontaneous', 0.043), ('biases', 0.041), ('interviews', 0.04), ('personalizing', 0.04), ('sugiyama', 0.04), ('xitr', 0.04), ('transfer', 0.039), ('avg', 0.037), ('shift', 0.035), ('xtr', 0.035), ('ols', 0.035), ('facs', 0.035), ('tsvm', 0.035), ('cohn', 0.035), ('lucey', 0.035), ('torre', 0.034), ('test', 0.033), ('ideal', 0.033), ('dtr', 0.033), ('samples', 0.031), ('xte', 0.031), ('densities', 0.03), ('generalizability', 0.029), ('weights', 0.029), ('hyperplane', 0.027), ('saragih', 0.027), ('ps', 0.027), ('afa', 0.026), ('personalizes', 0.026), ('rudovic', 0.026), ('whitehill', 0.026), ('wols', 0.026), ('domain', 0.026), ('loss', 0.026), ('frames', 0.025), ('unit', 0.025), ('health', 0.024), ('pittsburgh', 0.024), ('alternated', 0.023), ('dud', 0.023), ('chu', 0.023), ('borgwardt', 0.023), ('littlewort', 0.023), ('markedly', 0.023), ('posed', 0.023), ('face', 0.023), ('converge', 0.022), ('trained', 0.022), ('fasel', 0.022), ('yamada', 0.022), ('ordinal', 0.022), ('attenuating', 0.022), ('discriminant', 0.022), ('kernel', 0.021), ('domains', 0.021), ('subject', 0.021), ('la', 0.021), ('emotion', 0.02), ('gretton', 0.02), ('aytar', 0.02), ('aams', 0.02), ('qp', 0.02), ('unweighted', 0.02), ('imbalanced', 0.02), ('valstar', 0.02), ('lip', 0.02), ('afgr', 0.02), ('successive', 0.019), ('databases', 0.019), ('si', 0.019), ('eisr', 0.019), ('smile', 0.019), ('lp', 0.018), ('instances', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="385-tfidf-1" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>2 0.16444393 <a title="385-tfidf-2" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>3 0.14514218 <a title="385-tfidf-3" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>4 0.074868508 <a title="385-tfidf-4" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>5 0.073883936 <a title="385-tfidf-5" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>6 0.063829675 <a title="385-tfidf-6" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>7 0.06123713 <a title="385-tfidf-7" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>8 0.059728771 <a title="385-tfidf-8" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>9 0.058929138 <a title="385-tfidf-9" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>10 0.05637601 <a title="385-tfidf-10" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>11 0.056263693 <a title="385-tfidf-11" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>12 0.055539299 <a title="385-tfidf-12" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>13 0.05498939 <a title="385-tfidf-13" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>14 0.054525997 <a title="385-tfidf-14" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>15 0.053130612 <a title="385-tfidf-15" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>16 0.052894834 <a title="385-tfidf-16" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>17 0.051631756 <a title="385-tfidf-17" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>18 0.050343972 <a title="385-tfidf-18" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>19 0.04960065 <a title="385-tfidf-19" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>20 0.04925175 <a title="385-tfidf-20" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.047), (2, -0.041), (3, -0.019), (4, -0.009), (5, 0.009), (6, -0.021), (7, -0.04), (8, 0.081), (9, -0.058), (10, 0.037), (11, -0.03), (12, 0.007), (13, 0.011), (14, -0.034), (15, 0.021), (16, -0.004), (17, 0.002), (18, 0.038), (19, 0.025), (20, -0.032), (21, -0.057), (22, -0.034), (23, -0.016), (24, -0.011), (25, 0.064), (26, 0.026), (27, -0.048), (28, 0.033), (29, 0.03), (30, -0.049), (31, -0.078), (32, -0.097), (33, -0.036), (34, -0.088), (35, -0.024), (36, -0.021), (37, 0.008), (38, -0.026), (39, 0.004), (40, 0.005), (41, -0.034), (42, -0.013), (43, 0.122), (44, 0.006), (45, 0.024), (46, 0.093), (47, -0.02), (48, 0.114), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86427778 <a title="385-lsi-1" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>2 0.79042178 <a title="385-lsi-2" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>3 0.71874976 <a title="385-lsi-3" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>4 0.67172015 <a title="385-lsi-4" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>Author: Yi Sun, Xiaogang Wang, Xiaoou Tang</p><p>Abstract: We propose a new approach for estimation of the positions of facial keypoints with three-level carefully designed convolutional networks. At each level, the outputs of multiple networks are fused for robust and accurate estimation. Thanks to the deep structures of convolutional networks, global high-level features are extracted over the whole face region at the initialization stage, which help to locate high accuracy keypoints. There are two folds of advantage for this. First, the texture context information over the entire face is utilized to locate each keypoint. Second, since the networks are trained to predict all the keypoints simultaneously, the geometric constraints among keypoints are implicitly encoded. The method therefore can avoid local minimum caused by ambiguity and data corruption in difficult image samples due to occlusions, large pose variations, and extreme lightings. The networks at the following two levels are trained to locally refine initial predictions and their inputs are limited to small regions around the initial predictions. Several network structures critical for accurate and robust facial point detection are investigated. Extensive experiments show that our approach outperforms state-ofthe-art methods in both detection accuracy and reliability1.</p><p>5 0.59475374 <a title="385-lsi-5" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>Author: Robert Anderson, Björn Stenger, Vincent Wan, Roberto Cipolla</p><p>Abstract: This paper presents a complete system for expressive visual text-to-speech (VTTS), which is capable of producing expressive output, in the form of a ‘talking head’, given an input text and a set of continuous expression weights. The face is modeled using an active appearance model (AAM), and several extensions are proposed which make it more applicable to the task of VTTS. The model allows for normalization with respect to both pose and blink state which significantly reduces artifacts in the resulting synthesized sequences. We demonstrate quantitative improvements in terms of reconstruction error over a million frames, as well as in large-scale user studies, comparing the output of different systems.</p><p>6 0.5608744 <a title="385-lsi-6" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>7 0.5477286 <a title="385-lsi-7" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>8 0.49751896 <a title="385-lsi-8" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>9 0.49305934 <a title="385-lsi-9" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>10 0.42056492 <a title="385-lsi-10" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>11 0.4192389 <a title="385-lsi-11" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>12 0.40703091 <a title="385-lsi-12" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>13 0.40270334 <a title="385-lsi-13" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>14 0.39737281 <a title="385-lsi-14" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>15 0.39216933 <a title="385-lsi-15" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>16 0.38217542 <a title="385-lsi-16" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>17 0.37943637 <a title="385-lsi-17" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>18 0.37746137 <a title="385-lsi-18" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>19 0.3728449 <a title="385-lsi-19" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>20 0.36178178 <a title="385-lsi-20" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.093), (16, 0.036), (26, 0.066), (28, 0.014), (33, 0.216), (36, 0.01), (55, 0.012), (67, 0.068), (69, 0.037), (71, 0.26), (80, 0.011), (87, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79278886 <a title="385-lda-1" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>2 0.75686675 <a title="385-lda-2" href="./cvpr-2013-Universality_of_the_Local_Marginal_Polytope.html">448 cvpr-2013-Universality of the Local Marginal Polytope</a></p>
<p>Author: unkown-author</p><p>Abstract: We show that solving the LP relaxation of the MAP inference problem in graphical models (also known as the minsum problem, energy minimization, or weighted constraint satisfaction) is not easier than solving any LP. More precisely, any polytope is linear-time representable by a local marginal polytope and any LP can be reduced in linear time to a linear optimization (allowing infinite weights) over a local marginal polytope.</p><p>3 0.73486227 <a title="385-lda-3" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>4 0.72124195 <a title="385-lda-4" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>Author: Fang Wang, Yi Li</p><p>Abstract: Simple tree models for articulated objects prevails in the last decade. However, it is also believed that these simple tree models are not capable of capturing large variations in many scenarios, such as human pose estimation. This paper attempts to address three questions: 1) are simple tree models sufficient? more specifically, 2) how to use tree models effectively in human pose estimation? and 3) how shall we use combined parts together with single parts efficiently? Assuming we have a set of single parts and combined parts, and the goal is to estimate a joint distribution of their locations. We surprisingly find that no latent variables are introduced in the Leeds Sport Dataset (LSP) during learning latent trees for deformable model, which aims at approximating the joint distributions of body part locations using minimal tree structure. This suggests one can straightforwardly use a mixed representation of single and combined parts to approximate their joint distribution in a simple tree model. As such, one only needs to build Visual Categories of the combined parts, and then perform inference on the learned latent tree. Our method outperformed the state of the art on the LSP, both in the scenarios when the training images are from the same dataset and from the PARSE dataset. Experiments on animal images from the VOC challenge further support our findings.</p><p>5 0.71127003 <a title="385-lda-5" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>Author: Byung-Woo Hong, Zhaojin Lu, Ganesh Sundaramoorthi</p><p>Abstract: In this work, we address the multi-label Mumford-Shah problem, i.e., the problem of jointly estimating a partitioning of the domain of the image, and functions defined within regions of the partition. We create algorithms that are efficient, robust to undesirable local minima, and are easy-toimplement. Our algorithms are formulated by slightly modifying the underlying statistical model from which the multilabel Mumford-Shah functional is derived. The advantage of this statistical model is that the underlying variables: the labels and thefunctions are less coupled than in the original formulation, and the labels can be computed from the functions with more global updates. The resulting algorithms can be tuned to the desired level of locality of the solution: from fully global updates to more local updates. We demonstrate our algorithm on two applications: joint multi-label segmentation and denoising, and joint multi-label motion segmentation and flow estimation. We compare to the stateof-the-art in multi-label Mumford-Shah problems and show that we achieve more promising results.</p><p>6 0.70047927 <a title="385-lda-6" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>7 0.70010841 <a title="385-lda-7" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>8 0.69922656 <a title="385-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.69871587 <a title="385-lda-9" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>10 0.69695866 <a title="385-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.6960783 <a title="385-lda-11" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>12 0.69597828 <a title="385-lda-12" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>13 0.69472212 <a title="385-lda-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.69432724 <a title="385-lda-14" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>15 0.69429374 <a title="385-lda-15" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>16 0.69424117 <a title="385-lda-16" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>17 0.69396967 <a title="385-lda-17" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>18 0.69387174 <a title="385-lda-18" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>19 0.69353247 <a title="385-lda-19" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>20 0.69309872 <a title="385-lda-20" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
