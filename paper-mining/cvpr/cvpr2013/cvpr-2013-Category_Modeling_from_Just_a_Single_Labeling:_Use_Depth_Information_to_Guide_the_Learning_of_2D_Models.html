<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-80" href="#">cvpr2013-80</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</h1>
<br/><p>Source: <a title="cvpr-2013-80-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_Category_Modeling_from_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Quanshi Zhang, Xuan Song, Xiaowei Shao, Ryosuke Shibasaki, Huijing Zhao</p><p>Abstract: An object model base that covers a large number of object categories is of great value for many computer vision tasks. As artifacts are usually designed to have various textures, their structure is the primary distinguishing feature between different categories. Thus, how to encode this structural information and how to start the model learning with a minimum of human labeling become two key challenges for the construction of the model base. We design a graphical model that uses object edges to represent object structures, and this paper aims to incrementally learn this category model from one labeled object and a number of casually captured scenes. However, the incremental model learning may be biased due to the limited human labeling. Therefore, we propose a new strategy that uses the depth information in RGBD images to guide the model learning for object detection in ordinary RGB images. In experiments, the proposed method achieves superior performance as good as the supervised methods that require the labeling of all target objects.</p><p>Reference: <a title="cvpr-2013-80-reference" href="../cvpr2013_reference/cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn s  Abstract An object model base that covers a large number of object categories is of great value for many computer vision tasks. [sent-7, score-0.219]
</p><p>2 Thus, how to encode this structural information and how to start the model learning with a minimum of human labeling become two key challenges for the construction of the model base. [sent-9, score-0.26]
</p><p>3 We design a graphical model that uses object edges to represent object structures, and this paper aims to incrementally learn this category model from one labeled object and a number of casually captured scenes. [sent-10, score-0.988]
</p><p>4 Therefore, we propose a new strategy that uses the depth information in RGBD images to guide the model learning for object detection in ordinary RGB images. [sent-12, score-0.469]
</p><p>5 In experiments, the proposed method achieves superior performance as good as the supervised methods that require the labeling of all target objects. [sent-13, score-0.252]
</p><p>6 Second, if we idealize the spirit of semi-supervised learning, can we learn a category model from the minimum labeling (only one labeled object) and casually captured image sample pools? [sent-18, score-0.716]
</p><p>7 Here, we use the phrase “casually captured” to describe the loose requirement that training samples do not need to be hand-cropped or carefully aligned, and thus can be easily collected by ordinary people in their daily life. [sent-19, score-0.281]
</p><p>8 In casually captured image sample pools, the target objects within an image are usually small with large texture variations and various rigid trans-  Figure1. [sent-20, score-0.688]
</p><p>9 -basedcategorymodelbel arn-  t from one labeled object and a number of casually captured scenes2? [sent-23, score-0.516]
</p><p>10 Accurate part correspondences between target objects are necessary for training the structure-based model, but purely image-based object detection and matching are hampered by texture variations and rigid transformations of objects in these scenes. [sent-24, score-0.718]
</p><p>11 Therefore, we learn models from RGBD images, but apply them to object detection in ordinary RGB images. [sent-25, score-0.281]
</p><p>12 The minimum labeling meets the efficiency requirement for the construction of a category model base. [sent-28, score-0.313]
</p><p>13 These category models are expected to be able to detect objects in complex scenes. [sent-29, score-0.218]
</p><p>14 On the one hand, training the structure-based model requires the collection of small target objects in casually captured scenes, as well as the extraction of part correspondences between these objects. [sent-31, score-0.688]
</p><p>15 On the other hand, without training, object detection and matching based on the only labeled object is hampered by intra-category texture variations and various rigid transformations, which represent a great challenge for state-of-the-art algorithms. [sent-32, score-0.585]
</p><p>16 Worse still, bias and errors in object collection in the initial learning steps will affect subsequent steps, and be accumulated into a signifi-  cant model bias. [sent-33, score-0.273]
</p><p>17 We then use the 3D part correspondences (green arrows) to train a model for object detection in ordinary RGB images. [sent-38, score-0.372]
</p><p>18 In many cases, the 3D structure of a single object is discriminative enough for category detection. [sent-41, score-0.244]
</p><p>19 Thus, we propose a different model learning strategy in which we can train the model from RGBD images, and then apply the model to category detection in ordinary RGB images (Fig. [sent-42, score-0.568]
</p><p>20 At first, we use structure-based 3D matching to collect objects from RGBD images, simultaneously obtaining part correspondences, in spite of texture variations. [sent-44, score-0.265]
</p><p>21 The part correspondences in the 3D space are also used to train the 2D structural knowledge in the category model, as shown in Fig. [sent-46, score-0.285]
</p><p>22 In this way, we use more reliable 3D matching results to guide the learning of not-sodiscriminative image-based models, in order to overcome the bias problem in the incremental model learning. [sent-48, score-0.381]
</p><p>23 To achieve this learning strategy, we propose a novel graphical model that utilizes an object’s edges as a new and concise representation of its structures. [sent-49, score-0.234]
</p><p>24 Object edges have a stronger relationship than textures to the overall object structure, particularly where large texture variations exist. [sent-50, score-0.257]
</p><p>25 In this graphical model, we design different attributes to guide both the collection of 3D objects from RGBD images and the training of category models. [sent-51, score-0.566]
</p><p>26 Both the 3D object collection and the category-modelbased object detection are achieved by graph matching. [sent-52, score-0.328]
</p><p>27 Conventional algorithms for learning graph matching [2, 3, 4] have focused on training the weights of different graphical attributes, given a template graph (the category model) and multiple target graphs. [sent-53, score-0.74]
</p><p>28 In contrast, we train the category model by extending the method proposed by Leordeanu et al. [sent-54, score-0.213]
</p><p>29 [2] to estimate the general prototype of model attributes and eliminate the specificity in the labeled object. [sent-55, score-0.272]
</p><p>30 Facing challenges in the semi-supervised learning of visual models, we propose, for the first time, to use only one labeled object to start learning the structure knowledge from casually captured scenes. [sent-57, score-0.648]
</p><p>31 We apply the novel strategy—using objects collected from RGBD images to train the RGB-image-oriented model, thus avoiding possible bias problems caused by texture variations and various rigid transformations. [sent-58, score-0.272]
</p><p>32 A new type of graphical model based on object edges is designed as a concise representation of object structures in RGB and RGBD images. [sent-59, score-0.308]
</p><p>33 Related Work Object detection: Texture variations, object rotations, and the use of object structures make the task of object detection a great challenge. [sent-61, score-0.267]
</p><p>34 Recently, RGBD images made object detection much easier [14, 15, 16], and even the structure discovery [17] or segmentation of indoor environments [18, 19] produced object-level results. [sent-66, score-0.222]
</p><p>35 In this case, the graph matching has the ability to detect objects with various scales and rotations, an approach that has been widely used [20, 21, 22]. [sent-68, score-0.27]
</p><p>36 Model learning: We limit our discussion to unsuper-  vised and semi-supervised methods, and analyze them with a view to the construction of a category model base. [sent-71, score-0.213]
</p><p>37 The requirement of learning from a single labeling makes this research related to one-shot learning [23]. [sent-72, score-0.232]
</p><p>38 However, we focus on the extraction of the exact structural model from casually captured scenes, rather than the observing probability of patch textures. [sent-73, score-0.584]
</p><p>39 Most methods used bag-of-words models [5] for category representation, and others [25, 26, 27, 28] detected repetitive objects with the similar appearance in the environment. [sent-75, score-0.218]
</p><p>40 [29] manually cropped and aligned target objects in images for training, whereas [30, 25, 3 1] used unsupervised segmentation to generate object candidates, which relied on the foreground-background discrimination. [sent-82, score-0.265]
</p><p>41 This was found to be a more efficient ways of constructing a category model base. [sent-86, score-0.213]
</p><p>42 However, most of the above methods rely on object textures being highly similar, and are thus sensitive to the texture variations of many artifacts. [sent-87, score-0.257]
</p><p>43 Furthermore, the minimum labeling requirement for model base construction worsens the problem of texture variations. [sent-88, score-0.221]
</p><p>44 Graphical  model of object edge segments  and graph matching Considering the need for robustness to viewpoint variation and roll rotations, we use a graphical model to encode the local and pairwise attributes of the object structure, thus achieving object detection via graph matching. [sent-91, score-1.194]
</p><p>45 Edges are detected in RGB images using [35] and then discretized into line segments as the graph nodes, as shown in Fig. [sent-93, score-0.224]
</p><p>46 Using edge segments in the only labeled object, we construct a complete undirected graph G as the initial category model, in which parameters will be refined via learning. [sent-96, score-0.494]
</p><p>47 Given a target scene, we generate a target graph, denoted by G? [sent-97, score-0.214]
</p><p>48 The local attributes of vertex iand pairwise attributes of edge ij in G are denoted by fi and fij, respectively. [sent-99, score-0.542]
</p><p>49 ∈ {0, 1} to define the matching assignments between G and G∈? [sent-101, score-0.271]
</p><p>50 tching is to estimate the best matching assignments as:  yˆ=argmyaxC,  C=? [sent-113, score-0.271]
</p><p>51 In our study, some parts of the target objects in the casually captured scenes may be occluded, so some model nodes should not be matched. [sent-140, score-0.716]
</p><p>52 We use one-to-none matchings to model this case, and thus add a new matching choice— none—that is organized as a node in G? [sent-141, score-0.349]
</p><p>53 Besides, many-to-one matchings should be avoided, as they introduce errors to the learning of pairwise attributes between those multiple nodes. [sent-148, score-0.339]
</p><p>54 By designing differe=nt −lo1ca ilf aanndd pairwise attributes, the graphical model can be used for both object collection from RGBD images and object detection in ordinary images. [sent-154, score-0.583]
</p><p>55 Each pair of neighboring edge points is initialized as a line segment, and then neighboring segments are gradually merged into longer and straighter lines. [sent-156, score-0.189]
</p><p>56 In particular, edge segments in RGBD images are mapped to the 3D space to represent the 3D object structure. [sent-157, score-0.259]
</p><p>57 The penalty of their supplementary angle θu,v is calculated as:  Penaun,gvle = θu,v(1  − Uu,v)  (4)  where, Uu,v = e−τ min{lu∗,lv∗} measures the unreliability of the angle measurement, as angles between shorter segments are more sensitive to local perturbations; τ (= 0. [sent-162, score-0.379]
</p><p>58 Model for object collection from RGBD images The proposed graphical model, as a paradigm, is adapted for collecting objects in RGBD scenes and simultaneously extracting the correspondences of local patches between objects for further learning. [sent-185, score-0.424]
</p><p>59 Sghpabtoirailn angle: θij dt(e·n,o·)te tos Ωthe spatial angle between nodes i and j in G, and it is a conventional pairwise attribute. [sent-223, score-0.256]
</p><p>60 Let oi and oj denote the unit 3D orientation of node segments iand j. [sent-254, score-0.418]
</p><p>61 Note that the orientation of node segment imay be defined as either oi or −oi, so we instead use cij = as the centerl[imnien {co|dord|i,n|dates|. [sent-264, score-0.314]
</p><p>62 } mTahex compatibility dof c]enterline coordi-  [min{|di1j|, |di2j|}, max{|di1j|, |di2j|}, |di3j|]T,  nates for the matching assignment ij → i? [sent-265, score-0.352]
</p><p>63 We define the local and pairwise attributes as fi = [li, Ωi], fij = [θij , cij], and the parameters as w1 = [β, σpatch], w2 = [σangle, σnoise, α]. [sent-275, score-0.372]
</p><p>64 Thus, the overall compatibility for unary and pairwise assignments can be calculated as:  ρi ? [sent-276, score-0.305]
</p><p>65 Finally, we define the matching rate Υ as the simple evaluation of the matching quality: Υ = Ndetect/(Ndetect + Nnone), where Ndetect and Nnone are the number of nodes matched to real segments in the target images and none, respectively. [sent-288, score-0.648]
</p><p>66 Category model for ordinary RGB images As depth information can no longer be used, we design new local and pairwise attributes for object detection in ordinary images. [sent-295, score-0.684]
</p><p>67 (d) Patches collected  from the same part of objects are clustered to generate a sparse local codebook of patch features. [sent-308, score-0.241]
</p><p>68 Different patch features in the codebook represent different local texture styles, thus overcoming the texture variations. [sent-319, score-0.322]
</p><p>69 Model learning We use matching assignments estimated by relatively reliable 3D matchings to guide the training of the category model for ordinary RGB images, in order to avoid the bias problem. [sent-349, score-0.904]
</p><p>70 With the part correspondences from 3D matching, we extract a local codebook for each model node that covers all possible texture styles of a local part. [sent-350, score-0.355]
</p><p>71 [2] for both conventional parameter learning for graph matching and estimation of the general prototype of model attributes. [sent-352, score-0.427]
</p><p>72 Local codebooks extraction For each node in the category model, we extract patches from its matched node segments in target as shown in Fig. [sent-355, score-0.726]
</p><p>73 Model learning The graph matching based on the category model defined by (1) and (12) can be rewritten as argmaxC = argmaxyTMy (13) y ya where M(ii? [sent-368, score-0.505]
</p><p>74 We extend [2] from the pure learning ofmatching parameters w to the learning of both the parameters and the model attributes {w, f} by maximizing the following function: ? [sent-397, score-0.312]
</p><p>75 ci=at1es each target F(w,f) =  (15)  where i = scene used for training; t(i) denotes the predicted matching assignment. [sent-403, score-0.246]
</p><p>76 Intuitively, t mheo dmifaytcinhging assignments can bee directly predicted as tly(i,) h=e ey ˆ m3Dat,(chi),where yˆ3D,(i) denote the 3D matching assignments in the RGBD image. [sent-405, score-0.403]
</p><p>77 These matching states are equivalent in terms of the 3D structure, but they may show different attributes when the object is projected on the image plane. [sent-409, score-0.35]
</p><p>78 The matching assignments predicted by the cat-  egory model (denoted by ˆy img,(i)) are not always the same as ˆy 3D,(i). [sent-410, score-0.31]
</p><p>79 If nodes in the target image iare matched by both ˆy img,(i) and ˆy 3D,(i), the corresponding assignments in ˆy img,(i) are probably correct. [sent-412, score-0.365]
</p><p>80 In iteration k of the EM framework, the matching assignment t(i),k is estimated by (16), and then the model parameters and attributes are modified via gradient ascent:  5. [sent-421, score-0.375]
</p><p>81 However, according to our scenario of learning from casually captured RGBD images, target objects should not be hand-cropped or aligned, and thus have different scales, textures, and rotations. [sent-426, score-0.619]
</p><p>82 Results and evaluation Most of image-based category knowledge mining algorithms are hampered by texture variations and roll rotations. [sent-438, score-0.436]
</p><p>83 In this case, we compare the proposed method with image-based semi-supervised and supervised learning of graph matching, and five competing methods are  notebook PC category projected onto a 2D space. [sent-439, score-0.491]
</p><p>84 Pure graph matching based on TRW-S [36] without learning is denoted by Matching+TRW-S. [sent-445, score-0.292]
</p><p>85 Two methods based on [2] learn graph matching in an unsupervised manner, using spectral techniques [37] and the TRW-S [36] to solve graph matching, respectively. [sent-446, score-0.357]
</p><p>86 The remaining two methods achieve supervised learning of the proposed category model. [sent-448, score-0.328]
</p><p>87 Supervised uses the ground truth, instead of 3D matching assignments, to guide the model learning, whereas Supervised+NIO uses nonlinear inverse optimization (NIO) for model learning [4, 38]. [sent-449, score-0.366]
</p><p>88 Supervised transforms semi-supervised learning into supervised learning by redefining ajj? [sent-451, score-0.22]
</p><p>89 (rk)uth are the graph and the matching  C(fij, w,  ground truth of scene k. [sent-468, score-0.226]
</p><p>90 We label edge segments on the target object in this image, and randomly select 2/3 and 1/3 of the remaining RGBD images in this category as a training set and a testing set, respectively. [sent-471, score-0.54]
</p><p>91 NT and NB denote the number of nodes in the model that are matched to the target object and the background; Nmodel and Ntarget indicate the total number of segments in the model and the target object. [sent-475, score-0.625]
</p><p>92 8 illustrates object detection using the learned category models, and Table 1 gives the quantitative results. [sent-480, score-0.301]
</p><p>93 Table 1 also proves that the performance of 3D matching from RGBD images is superior enough to guide the learning of category models. [sent-481, score-0.462]
</p><p>94 Moreover, the learning algorithm [2] is not sensitive to outliers in training samples for the regression of the prototype model, so our method performs even better than the 3D matching for the drink box category. [sent-486, score-0.295]
</p><p>95 Discussion and conclusions In this paper, we proposed a method for category model learning from a single labeled object and a number of casually captured RGBD images, and the learned model was expected to be applied to object detection in ordinary RGB  images. [sent-488, score-1.115]
</p><p>96 As artifacts for daily use usually have regular shapes and various textures, the proposed category model mainly focuses on structural information, namely object edge segments. [sent-493, score-0.48]
</p><p>97 This design makes the model robust to texture variations, but at the same time unsuitable for largely occluded objects and those with highly deformable or irregular shapes, such as natural scenes and animals. [sent-494, score-0.212]
</p><p>98 Sukthankar, “Beyond local appearance: category recognition from pairwise interactions of simple features”, In CVPR, 2007. [sent-533, score-0.243]
</p><p>99 Fox, “Sparse distance learning for object recognition combining rgb and depth information”, In ICRA, 2011. [sent-574, score-0.222]
</p><p>100 Fergus, “Indoor segmentation and support inference from rgbd images”, In ECCV, 2012. [sent-592, score-0.37]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rgbd', 0.37), ('casually', 0.343), ('category', 0.174), ('ordinary', 0.154), ('attributes', 0.141), ('matching', 0.139), ('segments', 0.137), ('img', 0.133), ('yii', 0.133), ('assignments', 0.132), ('jj', 0.131), ('fij', 0.117), ('node', 0.108), ('target', 0.107), ('centerline', 0.106), ('compatibility', 0.104), ('ibj', 0.102), ('nio', 0.102), ('discovery', 0.095), ('supervised', 0.088), ('graph', 0.087), ('nnone', 0.086), ('rgb', 0.086), ('ii', 0.084), ('patch', 0.084), ('leordeanu', 0.083), ('guide', 0.083), ('texture', 0.082), ('dist', 0.082), ('hebert', 0.081), ('graphical', 0.08), ('nodes', 0.077), ('iaj', 0.076), ('notebook', 0.076), ('codebook', 0.074), ('hampered', 0.07), ('object', 0.07), ('cij', 0.07), ('pairwise', 0.069), ('lv', 0.069), ('learning', 0.066), ('matchings', 0.063), ('angle', 0.062), ('penalty', 0.061), ('grauman', 0.059), ('structural', 0.059), ('captured', 0.059), ('labeling', 0.057), ('icjen', 0.057), ('iijmg', 0.057), ('japans', 0.057), ('loglu', 0.057), ('ndetect', 0.057), ('nmodel', 0.057), ('ooii', 0.057), ('ooj', 0.057), ('shiba', 0.057), ('unreliability', 0.057), ('detection', 0.057), ('roll', 0.057), ('assignment', 0.056), ('fox', 0.055), ('bias', 0.054), ('variations', 0.053), ('ij', 0.053), ('textures', 0.052), ('correspondences', 0.052), ('edge', 0.052), ('deformability', 0.051), ('rotations', 0.05), ('orientation', 0.049), ('lai', 0.049), ('concise', 0.049), ('matched', 0.049), ('segment', 0.048), ('prototype', 0.048), ('conventional', 0.048), ('scenes', 0.047), ('collet', 0.047), ('fi', 0.045), ('daily', 0.045), ('uth', 0.044), ('okyo', 0.044), ('oj', 0.044), ('labeled', 0.044), ('objects', 0.044), ('collection', 0.044), ('unsupervised', 0.044), ('requirement', 0.043), ('patches', 0.043), ('drink', 0.042), ('pools', 0.042), ('aj', 0.042), ('iand', 0.041), ('artifacts', 0.041), ('categories', 0.04), ('oi', 0.039), ('collected', 0.039), ('model', 0.039), ('kinect', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="80-tfidf-1" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>Author: Quanshi Zhang, Xuan Song, Xiaowei Shao, Ryosuke Shibasaki, Huijing Zhao</p><p>Abstract: An object model base that covers a large number of object categories is of great value for many computer vision tasks. As artifacts are usually designed to have various textures, their structure is the primary distinguishing feature between different categories. Thus, how to encode this structural information and how to start the model learning with a minimum of human labeling become two key challenges for the construction of the model base. We design a graphical model that uses object edges to represent object structures, and this paper aims to incrementally learn this category model from one labeled object and a number of casually captured scenes. However, the incremental model learning may be biased due to the limited human labeling. Therefore, we propose a new strategy that uses the depth information in RGBD images to guide the model learning for object detection in ordinary RGB images. In experiments, the proposed method achieves superior performance as good as the supervised methods that require the labeling of all target objects.</p><p>2 0.16578723 <a title="80-tfidf-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.14931652 <a title="80-tfidf-3" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>4 0.13996366 <a title="80-tfidf-4" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Weilong Yang, Greg Mori</p><p>Abstract: We conduct image classification by learning a class-toimage distance function that matches objects. The set of objects in training images for an image class are treated as a collage. When presented with a test image, the best matching between this collage of training image objects and those in the test image is found. We validate the efficacy of the proposed model on the PASCAL 07 and SUN 09 datasets, showing that our model is effective for object classification and scene classification tasks. State-of-the-art image classification results are obtained, and qualitative results demonstrate that objects can be accurately matched.</p><p>5 0.13855085 <a title="80-tfidf-5" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>6 0.13595879 <a title="80-tfidf-6" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>7 0.10971627 <a title="80-tfidf-7" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>8 0.10806175 <a title="80-tfidf-8" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>9 0.10249154 <a title="80-tfidf-9" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>10 0.10235519 <a title="80-tfidf-10" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>11 0.098080523 <a title="80-tfidf-11" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>12 0.098050021 <a title="80-tfidf-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.097678065 <a title="80-tfidf-13" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>14 0.096833885 <a title="80-tfidf-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.096766122 <a title="80-tfidf-15" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>16 0.096087694 <a title="80-tfidf-16" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>17 0.095957331 <a title="80-tfidf-17" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>18 0.094039939 <a title="80-tfidf-18" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>19 0.092463396 <a title="80-tfidf-19" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>20 0.092274569 <a title="80-tfidf-20" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.261), (1, -0.024), (2, 0.004), (3, 0.004), (4, 0.107), (5, 0.035), (6, -0.064), (7, 0.078), (8, -0.024), (9, 0.058), (10, 0.023), (11, 0.018), (12, -0.045), (13, 0.023), (14, -0.004), (15, -0.078), (16, -0.058), (17, 0.022), (18, 0.068), (19, 0.008), (20, -0.004), (21, 0.01), (22, 0.013), (23, -0.073), (24, 0.062), (25, -0.0), (26, -0.034), (27, -0.053), (28, -0.013), (29, 0.019), (30, -0.102), (31, -0.047), (32, 0.072), (33, 0.0), (34, 0.086), (35, 0.058), (36, -0.005), (37, 0.04), (38, 0.024), (39, -0.031), (40, 0.046), (41, 0.056), (42, 0.106), (43, 0.014), (44, 0.043), (45, -0.05), (46, -0.041), (47, -0.046), (48, -0.034), (49, -0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94511122 <a title="80-lsi-1" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>Author: Quanshi Zhang, Xuan Song, Xiaowei Shao, Ryosuke Shibasaki, Huijing Zhao</p><p>Abstract: An object model base that covers a large number of object categories is of great value for many computer vision tasks. As artifacts are usually designed to have various textures, their structure is the primary distinguishing feature between different categories. Thus, how to encode this structural information and how to start the model learning with a minimum of human labeling become two key challenges for the construction of the model base. We design a graphical model that uses object edges to represent object structures, and this paper aims to incrementally learn this category model from one labeled object and a number of casually captured scenes. However, the incremental model learning may be biased due to the limited human labeling. Therefore, we propose a new strategy that uses the depth information in RGBD images to guide the model learning for object detection in ordinary RGB images. In experiments, the proposed method achieves superior performance as good as the supervised methods that require the labeling of all target objects.</p><p>2 0.6948868 <a title="80-lsi-2" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>Author: Jaechul Kim, Ce Liu, Fei Sha, Kristen Grauman</p><p>Abstract: We introduce a fast deformable spatial pyramid (DSP) matching algorithm for computing dense pixel correspondences. Dense matching methods typically enforce both appearance agreement between matched pixels as well as geometric smoothness between neighboring pixels. Whereas the prevailing approaches operate at the pixel level, we propose a pyramid graph model that simultaneously regularizes match consistency at multiple spatial extents—ranging from an entire image, to coarse grid cells, to every single pixel. This novel regularization substantially improves pixel-level matching in the face of challenging image variations, while the “deformable ” aspect of our model overcomes the strict rigidity of traditional spatial pyramids. Results on LabelMe and Caltech show our approach outperforms state-of-the-art methods (SIFT Flow [15] and PatchMatch [2]), both in terms of accuracy and run time.</p><p>3 0.67951703 <a title="80-lsi-3" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>4 0.66243279 <a title="80-lsi-4" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>Author: Feng Zhou, Fernando De_la_Torre</p><p>Abstract: Graph matching (GM) is a fundamental problem in computer science, and it has been successfully applied to many problems in computer vision. Although widely used, existing GM algorithms cannot incorporate global consistence among nodes, which is a natural constraint in computer vision problems. This paper proposes deformable graph matching (DGM), an extension of GM for matching graphs subject to global rigid and non-rigid geometric constraints. The key idea of this work is a new factorization of the pair-wise affinity matrix. This factorization decouples the affinity matrix into the local structure of each graph and the pair-wise affinity edges. Besides the ability to incorporate global geometric transformations, this factorization offers three more benefits. First, there is no need to compute the costly (in space and time) pair-wise affinity matrix. Second, it provides a unified view of many GM methods and extends the standard iterative closest point algorithm. Third, it allows to use the path-following optimization algorithm that leads to improved optimization strategies and matching performance. Experimental results on synthetic and real databases illustrate how DGM outperforms state-of-the-art algorithms for GM. The code is available at http : / / human s en s ing . c s . cmu .edu / fgm.</p><p>5 0.65851194 <a title="80-lsi-5" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Weilong Yang, Greg Mori</p><p>Abstract: We conduct image classification by learning a class-toimage distance function that matches objects. The set of objects in training images for an image class are treated as a collage. When presented with a test image, the best matching between this collage of training image objects and those in the test image is found. We validate the efficacy of the proposed model on the PASCAL 07 and SUN 09 datasets, showing that our model is effective for object classification and scene classification tasks. State-of-the-art image classification results are obtained, and qualitative results demonstrate that objects can be accurately matched.</p><p>6 0.65839177 <a title="80-lsi-6" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>7 0.64158744 <a title="80-lsi-7" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>8 0.6366058 <a title="80-lsi-8" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>9 0.62690639 <a title="80-lsi-9" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>10 0.62007904 <a title="80-lsi-10" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>11 0.61314934 <a title="80-lsi-11" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>12 0.59809875 <a title="80-lsi-12" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>13 0.59410346 <a title="80-lsi-13" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>14 0.59383774 <a title="80-lsi-14" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>15 0.58970225 <a title="80-lsi-15" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>16 0.58280277 <a title="80-lsi-16" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>17 0.58101839 <a title="80-lsi-17" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>18 0.58098507 <a title="80-lsi-18" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>19 0.5798282 <a title="80-lsi-19" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>20 0.57850534 <a title="80-lsi-20" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.122), (16, 0.028), (26, 0.051), (28, 0.02), (33, 0.253), (65, 0.246), (67, 0.06), (69, 0.068), (87, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92502022 <a title="80-lda-1" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>Author: Ajay Kumar, Cyril Kwong</p><p>Abstract: In order to avail the benefits of higher user convenience, hygiene, and improved accuracy, contactless 3D fingerprint recognition techniques have recently been introduced. One of the key limitations of these emerging 3D fingerprint technologies to replace the conventional 2D fingerprint system is their bulk and high cost, which mainly results from the use of multiple imaging cameras or structured lighting employed in these systems. This paper details the development of a contactless 3D fingerprint identification system that uses only single camera. We develop a new representation of 3D finger surface features using Finger Surface Codes and illustrate its effectiveness in matching 3D fingerprints. Conventional minutiae representation is extended in 3D space to accurately match the recovered 3D minutiae. Multiple 2D fingerprint images (with varying illumination profile) acquired to build 3D fingerprints can themselves be used recover 2D features for further improving 3D fingerprint identification and has been illustrated in this paper. The experimental results are shown on a database of 240 client fingerprints and confirm the advantages of the single camera based 3D fingerprint identification.</p><p>2 0.88097095 <a title="80-lda-2" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>Author: Adam Herout, István Szentandrási, Michal Zachariáš, Markéta Dubská, Rudolf Kajan</p><p>Abstract: We introduce here an improved design of the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution.</p><p>3 0.88007092 <a title="80-lda-3" href="./cvpr-2013-Efficient_3D_Endfiring_TRUS_Prostate_Segmentation_with_Globally_Optimized_Rotational_Symmetry.html">139 cvpr-2013-Efficient 3D Endfiring TRUS Prostate Segmentation with Globally Optimized Rotational Symmetry</a></p>
<p>Author: Jing Yuan, Wu Qiu, Eranga Ukwatta, Martin Rajchl, Xue-Cheng Tai, Aaron Fenster</p><p>Abstract: Segmenting 3D endfiring transrectal ultrasound (TRUS) prostate images efficiently and accurately is of utmost importance for the planning and guiding 3D TRUS guided prostate biopsy. Poor image quality and imaging artifacts of 3D TRUS images often introduce a challenging task in computation to directly extract the 3D prostate surface. In this work, we propose a novel global optimization approach to delineate 3D prostate boundaries using its rotational resliced images around a specified axis, which properly enforces the inherent rotational symmetry of prostate shapes to jointly adjust a series of 2D slicewise segmentations in the global 3D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard, we propose a novel coupled continuous max-flow model, which not only provides a powerful mathematical tool to analyze the proposed optimization problem but also amounts to a new and efficient duality-basedalgorithm. Ex- tensive experiments demonstrate that the proposed method significantly outperforms the state-of-art methods in terms ofefficiency, accuracy, reliability and less user-interactions, and reduces the execution time by a factor of 100.</p><p>4 0.86276728 <a title="80-lda-4" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>5 0.85484564 <a title="80-lda-5" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>6 0.83583319 <a title="80-lda-6" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>same-paper 7 0.83526206 <a title="80-lda-7" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>8 0.82001579 <a title="80-lda-8" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>9 0.7756319 <a title="80-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.77314138 <a title="80-lda-10" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>11 0.77068937 <a title="80-lda-11" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>12 0.76873803 <a title="80-lda-12" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>13 0.76764405 <a title="80-lda-13" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>14 0.76732314 <a title="80-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.76724327 <a title="80-lda-15" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>16 0.76711828 <a title="80-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.76680613 <a title="80-lda-17" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>18 0.76668042 <a title="80-lda-18" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>19 0.76651412 <a title="80-lda-19" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>20 0.76649392 <a title="80-lda-20" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
