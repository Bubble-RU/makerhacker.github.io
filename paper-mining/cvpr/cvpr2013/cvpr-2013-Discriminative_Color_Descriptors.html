<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 cvpr-2013-Discriminative Color Descriptors</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-130" href="#">cvpr2013-130</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>130 cvpr-2013-Discriminative Color Descriptors</h1>
<br/><p>Source: <a title="cvpr-2013-130-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Khan_Discriminative_Color_Descriptors_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>Reference: <a title="cvpr-2013-130-reference" href="../cvpr2013_reference/cvpr-2013-Discriminative_Color_Descriptors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. [sent-3, score-0.466]
</p><p>2 The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. [sent-5, score-0.786]
</p><p>3 This results in a drop of discriminative power of the color description. [sent-6, score-0.641]
</p><p>4 We cluster color values together based on their discriminative power in a classification problem. [sent-8, score-0.594]
</p><p>5 We show that such a color description automatically learns a certain degree of photometric invariance. [sent-10, score-0.673]
</p><p>6 We also show  that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. [sent-11, score-0.498]
</p><p>7 Experiments show that the proposed descriptor outperforms existing photometric invariants. [sent-12, score-0.474]
</p><p>8 Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200. [sent-13, score-0.47]
</p><p>9 In this paper we propose a new method to learn discriminative color descriptors. [sent-18, score-0.44]
</p><p>10 This has sparked an extensive literature on photometric invariance which aims to describe color invariants with respect to some of these variations [12]. [sent-21, score-0.908]
</p><p>11 Based on reflection models [20] or assumptions on the illumination  [8] invariance with respect to shadow, shading, specularities and illuminant color can be obtained. [sent-22, score-0.599]
</p><p>12 However, photometric invariance is gained at the cost of discriminative power. [sent-23, score-0.643]
</p><p>13 Therefore, in designing color representations it is important to weight the gains of photometric invariance against the loss in discriminative power. [sent-24, score-0.909]
</p><p>14 An alternative way of describing color is by means of color names. [sent-25, score-0.592]
</p><p>15 Examples of color names are for example ’red’, ’black’ and ’turquoise’ . [sent-27, score-0.485]
</p><p>16 [23] have proposed a method to automatically learn the eleven basic color names of the English language from Google images. [sent-29, score-0.602]
</p><p>17 Then, an eleven dimensions local color descriptor can be deduced simply by counting the occurrence of each color name over a local neighborhood. [sent-31, score-0.866]
</p><p>18 Analyzing the clusters of RGB values which are appointed to a color name, let us consider ’red’ for example, we note that these clusters possess a certain amount of photometric invariance. [sent-32, score-0.903]
</p><p>19 However, when moving towards darker ’reds’, at a certain point the values will 222888666644  be mapped to the color name ’black’ instead, and the photometric invariance breaks down. [sent-34, score-0.886]
</p><p>20 Recently, color names were found to compare favorably against photometric invariant descriptions on several computer vision applications, such  as image classification [16] and object detection [14]. [sent-35, score-0.851]
</p><p>21 These results show that focus on photometric invariance which is at the basis of many color descriptors might not be optimal. [sent-36, score-0.893]
</p><p>22 They further suggest that discarding discriminative power of the color representation will deteriorate final results. [sent-37, score-0.517]
</p><p>23 We propose to learn color descriptors which have optimal discriminative power for a specific classification problem. [sent-38, score-0.642]
</p><p>24 The problem of learning a color descriptor is equal to finding a partition ofthe color space. [sent-39, score-0.771]
</p><p>25 Firstly, the specific color descriptor which is optimized for a single data set. [sent-45, score-0.466]
</p><p>26 Secondly, a universal color descriptor which is trained on multiple data sets, thereby representing a wide range of real-world data sets. [sent-46, score-0.614]
</p><p>27 The advantage of universality is that users can run the learned mapping for an unknown data set without the effort of learning a data set specific color representation. [sent-47, score-0.69]
</p><p>28 In experimental results we will show that these discriminative color descriptors outperform purely photometric color descriptors, and that combined with shape description they can obtain state of the art results on several data sets. [sent-48, score-1.203]
</p><p>29 Photometric  Invariance versus Discrimina-  tive Power Color feature design has been mainly motivated from photometric invariance perspective [10, 11]. [sent-50, score-0.507]
</p><p>30 To obtain invariance with respect to these effects, photometric invariant features can be derived. [sent-52, score-0.542]
</p><p>31 But one could wonder what the cost of photometric invariance is. [sent-81, score-0.537]
</p><p>32 Mapping multiple RGB values to the same photometric invariance will potentially lead to a drop in discriminative power. [sent-82, score-0.767]
</p><p>33 This aspect of photometric invariance  has received relatively little attention. [sent-83, score-0.507]
</p><p>34 We discretize our initial color space into m color words W = {w1, . [sent-89, score-0.661]
</p><p>35 Tsh aer edi rsecprriemsiennatteivde b power of the color words W on the problem of distinguishing the classes C can be computed by the mutual information:  ×=  I (C,W) =? [sent-101, score-0.531]
</p><p>36 [6] proved that the drop of mutual information caused by clustering a word wt to cluster Wj (in our case based on photometric invariance) is equal to:  Δi = πtKL (p (C|wt) ,p (C|Wj))  (3)  where the Kullback-Leibler (KL) divergence is given by  KL(p1,p2) =? [sent-111, score-1.106]
</p><p>37 3 provides a way to assess for each color value the drop in discriminative power Δi which is caused by imposing photometric invariance. [sent-114, score-1.004]
</p><p>38 In Figure 1 we plot the drop in mutual information which occurs when we look at a photometric invariant representation with respect to luminance. [sent-115, score-0.638]
</p><p>39 Graph showing the drop in mutual information for the flower data set caused by grouping bins with equal chromatic values (a and b). [sent-123, score-0.472]
</p><p>40 The plot tells a clear story: the largest loss of discriminative power is occurring for achromatic (or low saturated) colors as is clear from the ridge at sat = 0. [sent-126, score-0.39]
</p><p>41 Even though these achromatic colors cannot be distinguished from a photometric invariance point of view (since they can be generated from each other by viewpoint or shadow variations), this analysis shows that they contain discriminative power. [sent-127, score-0.742]
</p><p>42 This leads us to investigate an alternative approach to color feature computations based on discriminative power. [sent-128, score-0.402]
</p><p>43 In the next section we outline our approach of discrimina-  tive color feature computation, which clusters color values together based on discriminative power on a training data set. [sent-129, score-0.921]
</p><p>44 The expectance is that discriminative clustering will automatically lead to a certain amount of photometric invariance: clustering values of similar hue together. [sent-130, score-0.589]
</p><p>45 However, in these regions especially around the achromatic axis we expect additional clusters to arise, to reduce the drop in discriminative power caused by the clustering. [sent-131, score-0.578]
</p><p>46 Discriminative Color Representations In this section we discuss our discriminative approach to color representations learning. [sent-133, score-0.402]
</p><p>47 In our case the words represent L*a*b* bins of the color histogram. [sent-150, score-0.451]
</p><p>48 (7) The new cluster index for word  wt  is given by wt∗ . [sent-163, score-0.424]
</p><p>49 In this paper, we use the DITC algorithm for a different purpose, namely to automatically learn discriminative color features. [sent-168, score-0.468]
</p><p>50 It is known that photometric variations result in connected trajectories [24]. [sent-174, score-0.44]
</p><p>51 Therefore when learning photometric invariants we expect them to be connected. [sent-175, score-0.405]
</p><p>52 In addition, connectivity has several conceptual advantages: it allows for comparison to photometric invariance, comparison with color names (CN), semantic interpretation (human color names are connected in Lab space), and comparison with human perception (e. [sent-176, score-1.379]
</p><p>53 222888666866  Let wt be the cluster number assigned to word wt, and Wwt is the cluster to which wt is assigned, then the cost of choosing a certain cluster assignment according to Eq. [sent-183, score-0.937]
</p><p>54 (8)  In this standard objective function, the relation of the words is not taken into account, and the final clusters WC can and most likely will — contain words which are not connected in color space. [sent-185, score-0.714]
</p><p>55 (9)  This type of dilation is justified because we use equiquantized bins on a uniform L*a*b* color space. [sent-193, score-0.456]
</p><p>56 We add a penalty term to all the color bins which are not part of Pj? [sent-196, score-0.382]
</p><p>57 To enforce our second objective of smoothness of the color representation we introduce a pairwise cost according to  ψ(ws,wt) =? [sent-199, score-0.39]
</p><p>58 For the three datasets (and their three combinations) used in this paper, we verified that the final color descriptors were connected. [sent-227, score-0.416]
</p><p>59 Photometric Invariance of Learned Clusters Instead of imposing photometric invariance, as is generally done, we follow an information theoretic approach which maximizes the discriminative power of the final representation. [sent-231, score-0.552]
</p><p>60 The underlying idea being that clustering color bins based on their discriminative power would automatically learn a certain degree ofphotometric invariance. [sent-232, score-0.659]
</p><p>61 We learn a 11-dimensional discriminative color descriptor for the Flower data set. [sent-234, score-0.583]
</p><p>62 Here, we replace the color of each pixel by the average color of all the pixels assigned to the same cluster. [sent-237, score-0.592]
</p><p>63 We can see that clusters are constructed so that they allow to discriminate flowers from background and leaves while providing some robustness across some photometric varia222888666977  Figure3. [sent-238, score-0.51]
</p><p>64 For example, note that the pixels under the shadows caused by the wrinkles on the yellow petals are assigned to the same cluster and the stamen part of the red flower is mapped to one cluster in spite of the photometric variations in the pixels. [sent-241, score-0.775]
</p><p>65 Also, the dark pixels that introduce most noises into photometric invariance representation are assigned to a separate cluster. [sent-242, score-0.507]
</p><p>66 The photometric invariance can also be observed from the bottom row of Fig. [sent-243, score-0.507]
</p><p>67 Universal Color Descriptors  In a seminal work named ’Basic color terms: their universality and evolution’ the linguists Berlin and Kay [2] show the universality of the human basic color names. [sent-246, score-1.268]
</p><p>68 With universality they refer to the fact that the basic color names which are used in different cultures have a similar partition of the color space: the Arab azraq refers to a similar set of colors as the English blue. [sent-247, score-1.185]
</p><p>69 In the context of descriptors, we will use the term universality to refer to descriptors which are not specific to a single data set. [sent-248, score-0.455]
</p><p>70 Universality is one ofthe more attractive properties of the computational color names [23][1]. [sent-249, score-0.485]
</p><p>71 As a consequence of universality, users are not required to learn a new color representation for ever new dataset and can just apply the universal color representation to their problem. [sent-250, score-0.834]
</p><p>72 In the previous section, we showed how to learn discriminative color features. [sent-251, score-0.44]
</p><p>73 The same setup can be used to learn universal color vocabulary by joining several training sets together to represent the real-world. [sent-253, score-0.658]
</p><p>74 An advantage over the existing computational color names [23] is that we are not limited to eleven color names and can freely  Figure4. [sent-255, score-1.049]
</p><p>75 We make the universal color descriptors available for the settings with 11, 25, and 50 clusters 1. [sent-259, score-0.699]
</p><p>76 In the experiments we will investigate universal color de-  scriptors, and compare them to specific color descriptors. [sent-260, score-0.794]
</p><p>77 We will do so by training the universal color descriptor from other data sets than the one currently considered. [sent-261, score-0.641]
</p><p>78 However, if the drop is small the advantages of a universal representation can outweigh the drop in performance. [sent-263, score-0.483]
</p><p>79 Then, we compare our proposed color descriptor with several photometric color descriptors on three image datasets. [sent-267, score-1.156]
</p><p>80 Next, we focus on the universality aspect of our descriptor and compare universality with specificity. [sent-268, score-0.819]
</p><p>81 Note the compactness and smoothness of the color clusters computed by the proposed method. [sent-280, score-0.464]
</p><p>82 Discriminative Color Descriptors The aim of this paper is to arrive at a better color descriptors for object recognition directly on the discriminative power of the final representations. [sent-317, score-0.607]
</p><p>83 We start by comparing our discriminative descriptor(DD) to other pure color descriptors and the color name descriptor [23]. [sent-318, score-0.983]
</p><p>84 Note that in several comparisons color names were found to outperform various other pure color descriptors [16][14]. [sent-319, score-0.871]
</p><p>85 We consider two well known photometric invariants: normalized RGB (rg histogram) and a hue histogram (HH) 2  and the Color Names(CN) [23] 3. [sent-320, score-0.387]
</p><p>86 For the case of 11dimensions (equal to the CN descriptor) our descriptor obtains improved results on Flower and Bird, but slightly lower results than color names on PASCAL 2007. [sent-324, score-0.628]
</p><p>87 Note, that it is unclear how to increase the dimensionality of the color name descriptor above the eleven basic color names. [sent-326, score-0.866]
</p><p>88 Universality versus Specificity We discussed universality color descriptors because of their ease of use in section 4. [sent-329, score-0.724]
</p><p>89 It is evident from figure 6 that for larger k, the difference between universality and specificity becomes smaller. [sent-340, score-0.384]
</p><p>90 Also note that, the best results obtained using our universal descriptor, although not better than the specific ones, outperform other state-of-the art color descriptors used in experiments of section 5. [sent-341, score-0.588]
</p><p>91 In conclusion, for larger dimensions the drop of performance due to universality is relatively small, and users could prefer using it, rather than having to train a new dataset specific descriptor. [sent-343, score-0.548]
</p><p>92 Our final result is a combination of late fusion between discriminative color and shape, shape alone and color alone. [sent-363, score-0.766]
</p><p>93 The universal color names result in a slight drop in performance. [sent-366, score-0.843]
</p><p>94 The portmanteau approach employ both color and shape to learn a compact color-shape vocabulary. [sent-368, score-0.431]
</p><p>95 The universal color descriptor results in slight deterioration in performance with a meanAP of 61. [sent-392, score-0.643]
</p><p>96 The method of [16] uses color attention approach to combine with color and shape with a meanAP of 58. [sent-395, score-0.63]
</p><p>97 However, our color descriptor can be used in any encoding framework together with SIFT. [sent-400, score-0.439]
</p><p>98 The universal color descriptor (learned from PASCAL, Birds and Flowers dataset) results in a drop in performance to 26. [sent-407, score-0.768]
</p><p>99 From which we can see that for particular (in a color sense) data sets computing a specific color representation can still yield a large performance gain. [sent-409, score-0.646]
</p><p>100 The green bar (the left bar of each plot) is the state-of-the-art  pure  color descriptor (Color Names). [sent-415, score-0.499]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('universality', 0.338), ('photometric', 0.331), ('ditc', 0.31), ('color', 0.296), ('wt', 0.269), ('names', 0.189), ('invariance', 0.176), ('universal', 0.175), ('drop', 0.154), ('descriptor', 0.143), ('clusters', 0.138), ('cluster', 0.107), ('discriminative', 0.106), ('weijer', 0.102), ('wj', 0.098), ('meanap', 0.095), ('tienne', 0.095), ('descriptors', 0.09), ('bins', 0.086), ('pascal', 0.085), ('power', 0.085), ('flower', 0.083), ('mutual', 0.081), ('eleven', 0.079), ('connected', 0.078), ('invariants', 0.074), ('dhillon', 0.074), ('dilation', 0.074), ('words', 0.069), ('colors', 0.066), ('divisive', 0.063), ('achromatic', 0.063), ('tkl', 0.063), ('khan', 0.061), ('portmanteau', 0.059), ('hue', 0.056), ('pj', 0.055), ('colorsift', 0.055), ('shadows', 0.053), ('name', 0.052), ('van', 0.051), ('illuminant', 0.051), ('clustering', 0.048), ('word', 0.048), ('rahat', 0.048), ('wwt', 0.048), ('specificity', 0.046), ('join', 0.046), ('description', 0.046), ('birds', 0.045), ('joining', 0.044), ('de', 0.043), ('specularities', 0.043), ('invariances', 0.042), ('accidental', 0.042), ('tricos', 0.042), ('vanrell', 0.042), ('vocabulary', 0.042), ('gevers', 0.041), ('flowers', 0.041), ('france', 0.039), ('learn', 0.038), ('shape', 0.038), ('adapt', 0.038), ('lightness', 0.037), ('plot', 0.037), ('equal', 0.036), ('setup', 0.036), ('geusebroek', 0.035), ('vocabularies', 0.035), ('invariant', 0.035), ('events', 0.034), ('objective', 0.034), ('saturated', 0.034), ('reflection', 0.033), ('sat', 0.033), ('bagdanov', 0.033), ('caused', 0.032), ('reflectance', 0.032), ('principle', 0.032), ('english', 0.032), ('mapped', 0.031), ('variations', 0.031), ('cube', 0.031), ('smoothness', 0.03), ('cost', 0.03), ('final', 0.03), ('evolution', 0.03), ('bar', 0.03), ('kl', 0.029), ('users', 0.029), ('slight', 0.029), ('rgb', 0.029), ('namely', 0.028), ('chai', 0.028), ('wc', 0.028), ('berlin', 0.028), ('sande', 0.028), ('specific', 0.027), ('sets', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="130-tfidf-1" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>2 0.1744633 <a title="130-tfidf-2" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><p>3 0.161246 <a title="130-tfidf-3" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>4 0.15174517 <a title="130-tfidf-4" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>Author: Thoma Papadhimitri, Paolo Favaro</p><p>Abstract: We investigate the problem of reconstructing normals, albedo and lights of Lambertian surfaces in uncalibrated photometric stereo under the perspective projection model. Our analysis is based on establishing the integrability constraint. In the orthographicprojection case, it is well-known that when such constraint is imposed, a solution can be identified only up to 3 parameters, the so-called generalized bas-relief (GBR) ambiguity. We show that in the perspective projection case the solution is unique. We also propose a closed-form solution which is simple, efficient and robust. We test our algorithm on synthetic data and publicly available real data. Our quantitative tests show that our method outperforms all prior work of uncalibrated photometric stereo under orthographic projection.</p><p>5 0.10845362 <a title="130-tfidf-5" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>Author: Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials, the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals, and that the intensity distribution of an intensity profile conveys information about the reflectance properties, when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations, we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.</p><p>6 0.10586186 <a title="130-tfidf-6" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>7 0.1042314 <a title="130-tfidf-7" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>8 0.098593391 <a title="130-tfidf-8" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>9 0.097329281 <a title="130-tfidf-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.095996343 <a title="130-tfidf-10" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>11 0.095712215 <a title="130-tfidf-11" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>12 0.092825174 <a title="130-tfidf-12" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>13 0.088947244 <a title="130-tfidf-13" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>14 0.087552674 <a title="130-tfidf-14" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>15 0.086682372 <a title="130-tfidf-15" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>16 0.085680462 <a title="130-tfidf-16" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>17 0.083246641 <a title="130-tfidf-17" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>18 0.08209125 <a title="130-tfidf-18" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>19 0.07255426 <a title="130-tfidf-19" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>20 0.072460264 <a title="130-tfidf-20" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, -0.003), (2, -0.001), (3, 0.022), (4, 0.018), (5, -0.026), (6, -0.069), (7, 0.014), (8, -0.031), (9, -0.021), (10, -0.036), (11, -0.087), (12, -0.012), (13, -0.028), (14, 0.062), (15, -0.004), (16, 0.059), (17, -0.076), (18, 0.085), (19, -0.058), (20, 0.111), (21, 0.015), (22, 0.038), (23, -0.034), (24, 0.012), (25, 0.054), (26, 0.057), (27, 0.084), (28, -0.056), (29, -0.072), (30, 0.079), (31, 0.04), (32, -0.008), (33, -0.027), (34, 0.032), (35, -0.026), (36, -0.047), (37, 0.139), (38, 0.072), (39, 0.002), (40, 0.02), (41, -0.049), (42, -0.125), (43, 0.023), (44, 0.048), (45, -0.059), (46, -0.002), (47, 0.047), (48, 0.018), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95341742 <a title="130-lsi-1" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>2 0.70635289 <a title="130-lsi-2" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>Author: Ivo Everts, Jan C. van_Gemert, Theo Gevers</p><p>Abstract: This paper is concerned with recognizing realistic human actions in videos based on spatio-temporal interest points (STIPs). Existing STIP-based action recognition approaches operate on intensity representations of the image data. Because of this, these approaches are sensitive to disturbing photometric phenomena such as highlights and shadows. Moreover, valuable information is neglected by discarding chromaticity from the photometric representation. These issues are addressed by Color STIPs. Color STIPs are multi-channel reformulations of existing intensity-based STIP detectors and descriptors, for which we consider a number of chromatic representations derived from the opponent color space. This enhanced modeling of appearance improves the quality of subsequent STIP detection and description. Color STIPs are shown to substantially outperform their intensity-based counterparts on the challenging UCF sports, UCF11 and UCF50 action recognition benchmarks. Moreover, the results show that color STIPs are currently the single best low-level feature choice for STIP-based approaches to human action recognition.</p><p>3 0.68232179 <a title="130-lsi-3" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>Author: Takumi Kobayashi</p><p>Abstract: Image classification methods have been significantly developed in the last decade. Most methods stem from bagof-features (BoF) approach and it is recently extended to a vector aggregation model, such as using Fisher kernels. In this paper, we propose a novel feature extraction method for image classification. Following the BoF approach, a plenty of local descriptors are first extracted in an image and the proposed method is built upon the probability density function (p.d.f) formed by those descriptors. Since the p.d.f essentially represents the image, we extract the features from the p.d.f by means of the gradients on the p.d.f. The gradients, especially their orientations, effectively characterize the shape of the p.d.f from the geometrical viewpoint. We construct the features by the histogram of the oriented p.d.f gradients via orientation coding followed by aggregation of the orientation codes. The proposed image features, imposing no specific assumption on the targets, are so general as to be applicable to any kinds of tasks regarding image classifications. In the experiments on object recog- nition and scene classification using various datasets, the proposed method exhibits superior performances compared to the other existing methods.</p><p>4 0.63748682 <a title="130-lsi-4" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>Author: Mayank Bansal, Kostas Daniilidis</p><p>Abstract: We address the problem of matching images with disparate appearance arising from factors like dramatic illumination (day vs. night), age (historic vs. new) and rendering style differences. The lack of local intensity or gradient patterns in these images makes the application of pixellevel descriptors like SIFT infeasible. We propose a novel formulation for detecting and matching persistent features between such images by analyzing the eigen-spectrum of the joint image graph constructed from all the pixels in the two images. We show experimental results of our approach on a public dataset of challenging image pairs and demonstrate significant performance improvements over state-of-the-art.</p><p>5 0.63208759 <a title="130-lsi-5" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>Author: Bing Li, Weihua Xiong, Weiming Hu, Houwen Peng</p><p>Abstract: Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods.</p><p>6 0.62697518 <a title="130-lsi-6" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>7 0.62466413 <a title="130-lsi-7" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>8 0.58799696 <a title="130-lsi-8" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>9 0.58707452 <a title="130-lsi-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.58617848 <a title="130-lsi-10" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>11 0.5833621 <a title="130-lsi-11" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>12 0.56831974 <a title="130-lsi-12" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<p>13 0.54927957 <a title="130-lsi-13" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>14 0.54385108 <a title="130-lsi-14" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>15 0.54312515 <a title="130-lsi-15" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>16 0.53984278 <a title="130-lsi-16" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>17 0.53787172 <a title="130-lsi-17" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>18 0.5363757 <a title="130-lsi-18" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>19 0.5341289 <a title="130-lsi-19" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>20 0.53225154 <a title="130-lsi-20" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.099), (16, 0.072), (26, 0.048), (27, 0.017), (28, 0.021), (33, 0.277), (67, 0.047), (69, 0.057), (80, 0.013), (87, 0.051), (99, 0.221)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88363683 <a title="130-lda-1" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>Author: Yuichi Takeda, Shinsaku Hiura, Kosuke Sato</p><p>Abstract: In this paper we propose a novel depth measurement method by fusing depth from defocus (DFD) and stereo. One of the problems of passive stereo method is the difficulty of finding correct correspondence between images when an object has a repetitive pattern or edges parallel to the epipolar line. On the other hand, the accuracy of DFD method is inherently limited by the effective diameter of the lens. Therefore, we propose the fusion of stereo method and DFD by giving different focus distances for left and right cameras of a stereo camera with coded apertures. Two types of depth cues, defocus and disparity, are naturally integrated by the magnification and phase shift of a single point spread function (PSF) per camera. In this paper we give the proof of the proportional relationship between the diameter of defocus and disparity which makes the calibration easy. We also show the outstanding performance of our method which has both advantages of two depth cues through simulation and actual experiments.</p><p>2 0.86648709 <a title="130-lda-2" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<p>Author: Won Hwa Kim, Moo K. Chung, Vikas Singh</p><p>Abstract: The analysis of 3-D shape meshes is a fundamental problem in computer vision, graphics, and medical imaging. Frequently, the needs of the application require that our analysis take a multi-resolution view of the shape ’s local and global topology, and that the solution is consistent across multiple scales. Unfortunately, the preferred mathematical construct which offers this behavior in classical image/signal processing, Wavelets, is no longer applicable in this general setting (data with non-uniform topology). In particular, the traditional definition does not allow writing out an expansion for graphs that do not correspond to the uniformly sampled lattice (e.g., images). In this paper, we adapt recent results in harmonic analysis, to derive NonEuclidean Wavelets based algorithms for a range of shape analysis problems in vision and medical imaging. We show how descriptors derived from the dual domain representation offer native multi-resolution behavior for characterizing local/global topology around vertices. With only minor modifications, the framework yields a method for extracting interest/key points from shapes, a surprisingly simple algorithm for 3-D shape segmentation (competitive with state of the art), and a method for surface alignment (without landmarks). We give an extensive set of comparison results on a large shape segmentation benchmark and derive a uniqueness theorem for the surface alignment problem.</p><p>same-paper 3 0.86179888 <a title="130-lda-3" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>Author: Rahat Khan, Joost van_de_Weijer, Fahad Shahbaz Khan, Damien Muselet, Christophe Ducottet, Cecile Barat</p><p>Abstract: Color description is a challenging task because of large variations in RGB values which occur due to scene accidental events, such as shadows, shading, specularities, illuminant color changes, and changes in viewing geometry. Traditionally, this challenge has been addressed by capturing the variations in physics-basedmodels, and deriving invariants for the undesired variations. The drawback of this approach is that sets of distinguishable colors in the original color space are mapped to the same value in the photometric invariant space. This results in a drop of discriminative power of the color description. In this paper we take an information theoretic approach to color description. We cluster color values together based on their discriminative power in a classification problem. The clustering has the explicit objective to minimize the drop of mutual information of the final representation. We show that such a color description automatically learns a certain degree of photometric invariance. We also show that a universal color representation, which is based on other data sets than the one at hand, can obtain competing performance. Experiments show that the proposed descriptor outperforms existing photometric invariants. Furthermore, we show that combined with shape description these color descriptors obtain excellent results on four challenging datasets, namely, PASCAL VOC 2007, Flowers-102, Stanford dogs-120 and Birds-200.</p><p>4 0.85495752 <a title="130-lda-4" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>Author: C. Lawrence Zitnick, Devi Parikh</p><p>Abstract: Relating visual information to its linguistic semantic meaning remains an open and challenging area of research. The semantic meaning of images depends on the presence of objects, their attributes and their relations to other objects. But precisely characterizing this dependence requires extracting complex visual information from an image, which is in general a difficult and yet unsolved problem. In this paper, we propose studying semantic information in abstract images created from collections of clip art. Abstract images provide several advantages. They allow for the direct study of how to infer high-level semantic information, since they remove the reliance on noisy low-level object, attribute and relation detectors, or the tedious hand-labeling of images. Importantly, abstract images also allow the ability to generate sets of semantically similar scenes. Finding analogous sets of semantically similar real images would be nearly impossible. We create 1,002 sets of 10 semantically similar abstract scenes with corresponding written descriptions. We thoroughly analyze this dataset to discover semantically important features, the relations of words to visual features and methods for measuring semantic similarity.</p><p>5 0.8365348 <a title="130-lda-5" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>Author: Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu, Cunzhao Shi</p><p>Abstract: In this paper, we propose a novel method for cross-view action recognition via a continuous virtual path which connects the source view and the target view. Each point on this virtual path is a virtual view which is obtained by a linear transformation of the action descriptor. All the virtual views are concatenated into an infinite-dimensional feature to characterize continuous changes from the source to the target view. However, these infinite-dimensional features cannot be used directly. Thus, we propose a virtual view kernel to compute the value of similarity between two infinite-dimensional features, which can be readily used to construct any kernelized classifiers. In addition, there are a lot of unlabeled samples from the target view, which can be utilized to improve the performance of classifiers. Thus, we present a constraint strategy to explore the information contained in the unlabeled samples. The rationality behind the constraint is that any action video belongs to only one class. Our method is verified on the IXMAS dataset, and the experimental results demonstrate that our method achieves better performance than the state-of-the-art methods.</p><p>6 0.83170903 <a title="130-lda-6" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>7 0.80804437 <a title="130-lda-7" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>8 0.80590534 <a title="130-lda-8" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>9 0.80534434 <a title="130-lda-9" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>10 0.80418086 <a title="130-lda-10" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>11 0.80318755 <a title="130-lda-11" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>12 0.80259359 <a title="130-lda-12" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>13 0.80244321 <a title="130-lda-13" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>14 0.80227613 <a title="130-lda-14" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>15 0.80205464 <a title="130-lda-15" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>16 0.8014636 <a title="130-lda-16" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>17 0.80110419 <a title="130-lda-17" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>18 0.80100989 <a title="130-lda-18" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>19 0.8009913 <a title="130-lda-19" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>20 0.80091733 <a title="130-lda-20" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
