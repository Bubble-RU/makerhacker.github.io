<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-422" href="#">cvpr2013-422</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</h1>
<br/><p>Source: <a title="cvpr-2013-422-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zheng_Tag_Taxonomy_Aware_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>Reference: <a title="cvpr-2013-422-reference" href="../cvpr2013_reference/cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. [sent-4, score-0.917]
</p><p>2 Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. [sent-5, score-1.151]
</p><p>3 Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. [sent-7, score-0.56]
</p><p>4 Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. [sent-8, score-2.042]
</p><p>5 Recently several proposed region tagging approaches attempt to explore the contextual constraints among image regions using sparse coding techniques [11, 25, 7]. [sent-16, score-0.86]
</p><p>6 However, these approaches that simply used all training regions as the dictionary for spare coding have three main disadvantages. [sent-17, score-0.559]
</p><p>7 This tag taxonomy has two levels: superclass level and basic-class level. [sent-21, score-0.818]
</p><p>8 We associate each tag node with a node-specific dictionary and concatenate the node-specific dictionaries from each level to create a level-specific dictionary. [sent-23, score-1.172]
</p><p>9 The level-specific dictionaries for this taxonomy are D(1) and D(2) while the node-specific dictionaries are }s=1. [sent-24, score-0.555]
</p><p>10 We reconstruct each image region using different level-specific dictionaries and sum up the sparse codes obtained from different levels as the final feature representation to learn a linear classifier for region tagging. [sent-31, score-0.921]
</p><p>11 Second, the computational complexity of sparse coding increases with the size of dictionary and it is impossible to use all the training regions as the dictionary for large-scale datasets. [sent-33, score-1.041]
</p><p>12 Thus learning a compact and discriminative dictionary for region tagging is desirable. [sent-34, score-0.882]
</p><p>13 Third, for datasets with unbalanced tag classes, the performance of these approaches may decrease drastically. [sent-35, score-0.651]
</p><p>14 This is because unbalanced tag classes result in an unbalanced group structure in the dictionary such that the computed sparse codes become less discriminative for classification task. [sent-36, score-1.579]
</p><p>15 In addition, tags are often arranged into a hierarchical taxonomy based on their semantic meanings, such as the tag taxonomy shown in Figure 1. [sent-37, score-1.243]
</p><p>16 However, the tag taxonomy has not been exploited to improve the accuracy of region tagging, even though the similar category taxonomy has been shown to benefit the accuracy as well as the scalability of learning  {D(k2)  {D(s1)  algorithms [15, 16, 6] for object recognition. [sent-38, score-1.069]
</p><p>17 To overcome the above drawbacks, we present a novel multi-layer hierarchical dictionary learning framework for region tagging when the tag taxonomy is known. [sent-39, score-1.598]
</p><p>18 For il333666779  lustration, a two-layer tag taxonomy and the corresponding dictionary learning framework is depicted in Figure 1. [sent-40, score-1.094]
</p><p>19 To our best knowledge, we are the first to use the supervised dictionary learning to explore the semantic relationship among tags. [sent-41, score-0.527]
</p><p>20 Specifically, we generate a node-specific dictionary for each tag node in the taxonomy and concatenate the node-specific dictionaries in each level to construct a level-specific dictionary. [sent-42, score-1.35]
</p><p>21 Thus the hierarchical semantic relationship among tags is preserved in the relationship among node-specific dictionaries, which enables us to exploit the discriminative information among regions in a hierarchial way. [sent-43, score-0.649]
</p><p>22 Moreover, dictionary items from the same node-specific dictionary are considered as a group so it introduces a group structure for each level-specific dictionary. [sent-44, score-0.975]
</p><p>23 Based on each level-specific dictionary and corresponding group structure, we reconstruct each image region using the group sparse coding algorithm [27] to obtain level-specific sparse codes. [sent-45, score-1.084]
</p><p>24 Compared with single-level sparse codes in existing sparse coding-based region tagging approaches [11, 25, 7], our multi-layer sparse codes not only encodes the contextual constraints among regions, but also encodes the relationship among tags. [sent-46, score-1.463]
</p><p>25 Finally, we sum up the sparse  codes obtained from different levels as the final feature representation to learn a linear class classifier. [sent-47, score-0.486]
</p><p>26 For datasets with unbalanced tag classes, we can create balanced group structure for higher levels and make use of sparse codes obtained from higher levels to help design the classifiers for lower levels. [sent-48, score-1.366]
</p><p>27 Therefore, our approach is robust to datasets with unbalanced tag classes in contrast to existing sparse codingbased region tagging approaches that tend to perform poorly on datasets with unbalanced tag classes. [sent-49, score-1.94]
</p><p>28 Our Contribution The main contributions in our paper are four-fold: • We present a multi-layer supervised dictionary learning framework that simultaneously learns multi-layer dictionaries and classifiers. [sent-52, score-0.605]
</p><p>29 • We are the first to use the supervised dictionary learning to explore the semantic structure among tags, which not only takes advantages of the compactness and efficiency of dictionary learning, but also explores different group structures among image regions. [sent-53, score-0.991]
</p><p>30 • Our approach proposes to sum up sparse codes from different levels as the feature representation to learn a linear classifier, which enables us to make use of discriminative information encoded in sparse codes from different levels. [sent-54, score-0.847]
</p><p>31 • Our approach is robust to datasets with unbalanced tag classes. [sent-55, score-0.651]
</p><p>32 Related Work Recently, several region tagging approaches have used  sparse coding techniques to encode contextual constraints among image regions for region tagging [11, 25, 7]. [sent-57, score-1.341]
</p><p>33 [11] proposed a bi-layer sparse coding framework to reconstruct image regions from over-segmented image patches that belong to a few images, and then propagate image labels of selected patches to the entire label to obtain region assignment. [sent-58, score-0.511]
</p><p>34 [25] considered regions within the same image as a group, and used the group sparse coding with spatial kernels to jointly reconstruct image regions in the same image from other training regions. [sent-62, score-0.626]
</p><p>35 [7] extended group sparse coding with graph-guided fusion penalty to encourage highly correlated regions to be jointly selected for the reconstruction. [sent-64, score-0.505]
</p><p>36 However, the performance of the group sparse coding depends on a balanced group structure which has the similar number of training regions in each group so it might not be robust to datasets that have very unbalanced training regions. [sent-65, score-0.878]
</p><p>37 Other techniques have also been proposed to boost the performance for region tagging or region-based image annotation. [sent-66, score-0.481]
</p><p>38 The idea is that each image is annotated by the tag that has at least one sample region (seen as ‘instance’) within this image (seen as ‘bag’). [sent-68, score-0.641]
</p><p>39 [9] proposed a unified solution to tag refinement and tag-to-region assignment by using a multi-edge graph, where each vertex ofthe graph is a unique image encoded by a region bag with multiple image segmentations. [sent-71, score-0.641]
</p><p>40 [5] proposed a multi-layer group sparse coding framework to encode the mutual dependence between the class labels as well as the tag distribution information. [sent-72, score-0.87]
</p><p>41 Supervised dictionary learning which combines dictionary learning with classifier training into a unified learning framework has been extensively studied [24, 17, 14, 28]. [sent-73, score-0.814]
</p><p>42 [24] performed supervised dictionary learning by minimizing the training error of classifying the image-level features, which are extracted by max pooling over the sparse codes within a spatial pyramid. [sent-74, score-0.819]
</p><p>43 [14] proposed a novel sparse representation of signals belonging to different classes in terms of a shared dictionary and discriminative models. [sent-75, score-0.528]
</p><p>44 This approach alternates between the step of sparse coding and the step of dictionary update and discriminative model learning. [sent-76, score-0.582]
</p><p>45 In addition, [8, 1] proposed to use proximal methods for structured sparse learning where dictionary items are embedded in different structures. [sent-78, score-0.59]
</p><p>46 Tag Taxonomy Aware Dictionary Learning In this section, we first introduce the group sparse coding algorithm and then describe the formulation of our multilayer supervised dictionary learning, its optimization and how to tag image regions using sparse codes. [sent-80, score-1.485]
</p><p>47 , DG] ∈ Rd×J where Dg ∈ Rd×Jg consists of a group of Jg visually correlated dictionary items, an image region x ∈ Rd can be reconstructed from the dictionary with the group LASSO penalty [27] as follows:  z= arg m zin12 | x −g? [sent-86, score-1.073]
</p><p>48 Since the group LASSO uses a group-sparsity-inducing regularization instead of the l1norm as in LASSO [20], we can treat multiple visually similar dictionary items within the same group as a whole and exploit implicit relations among these dictionary items to some extent. [sent-95, score-1.089]
</p><p>49 Multi-layer Supervised Dictionary Learning We consider an image dataset D with a two-layer tag taxonomy whose levels from the top to the bottom are called: super-class level and basic-class level as shown in Figure 1. [sent-98, score-0.891]
</p><p>50 Note that extensions to learning multiple level-specific dictionaries for a multi-layer tag taxonomy can be accomplished in a similar way. [sent-99, score-0.926]
</p><p>51 Let X ∈ Rd×N denote N training image regions from K tag classes. [sent-101, score-0.657]
</p><p>52 According to the tag taxonomy, image regions from these K classes in the basic-class level can be merged into S super-classes in the super-class level, e. [sent-102, score-0.692]
</p><p>53 Let H(2) ∈ {0, 1}K×N denote the class label indicator matrix for all the regions, where = 1 if the jth image region belongs to the ith tag and  H((2i,)j)  tHo( di2,e)jn)o=te 0 th oeth seurpweris-cel. [sent-106, score-0.699]
</p><p>54 Note that we use the superscript to index the level in the tag taxonomy and the subscript to index the node-specific dictionary in that level. [sent-108, score-1.11]
</p><p>55 Given an underlying tag taxonomy, we associate a separate dictionary with each tag node. [sent-109, score-1.405]
</p><p>56 These individual dictionaries are called node-specific dictionaries and they serve as  local viewpoints for exploring the discriminative information among training regions from the same class or superclass. [sent-110, score-0.576]
</p><p>57 We concatenate the node-specific dictionaries in each level to construct a new large dictionary which is called a level-specific dictionary. [sent-111, score-0.611]
</p><p>58 Given level-specific dictionaries D(1) , D(2) and a region xn ∈ Rd×1from the s-th superclass and k-th class, we obtain the group sparse representations and of this region as follows:  z(n1)  z(n2)  zH(n21e)r= waerginmzt(nri21)nod21u|cxenq−(1)Da(n12)dzq(n21)|2 to+dλe21nk? [sent-119, score-0.76]
</p><p>59 In particular, the non-zero values of occur at those indices where the dictionary items long to the node-specific dictionary or We  q(n1)  q(n2)  Ds(1)  [z1(1), z(N1)]  Dk(2). [sent-122, score-0.743]
</p><p>60 , ∈ RJ×N to denote the group sparse codes of all regions at the super-class level. [sent-126, score-0.582]
</p><p>61 The super-class level dictionary is defined as: = while the basic-level dictionary is  D(2)  D(1) [D1(1), D2(1)] [D1(2), D2(2), D3(2), D4(2)]. [sent-149, score-0.738]
</p><p>62 = For each region from one labeled tag, we aim to use only the node-specific dictionary that is associated with the same tag to reconstruct the region. [sent-150, score-1.04]
</p><p>63 We minimize the difference between the true sparse codes and the corresponding ideal sparse codes to encourage the true sparse codes to be close to the ideal sparse codes. [sent-158, score-1.319]
</p><p>64 Note that this fixed and structured relationship between Q(1) and Q(2) regularizes the relationship between Z(1) and Z(2) from two levels, which makes it possible to use sparse codes from different levels to improve classification accuracy. [sent-161, score-0.503]
</p><p>65 For example, there are many training regions for the tag cat but little training  Ds(1)  Dk(2)  regions for dog. [sent-166, score-0.795]
</p><p>66 Given the feature of an image region from dog, it can be reconstructed using the level-specific dictionary from the basic-class level, which may activate multiple node-specific dictionaries in the basic-class level. [sent-167, score-0.67]
</p><p>67 This is due to the little training regions for the tag dog and it will be difficult to classify the class label of this image region. [sent-168, score-0.741]
</p><p>68 However, when using the level-specific dictionary from the super-class level to reconstruct this image region, it may only activate the node-specific dictionary associated with the super-class animal. [sent-169, score-0.815]
</p><p>69 This is because other tags within the same super-class animal may share some features with dog and can help to represent this image region better other than dog itself. [sent-170, score-0.533]
</p><p>70 Even if we cannot classify this image region as dog, we can at least classify this image regions as other tags that belong to the super-class animal instead of totally uncorrelated tags from other super-classes. [sent-171, score-0.778]
</p><p>71 Thus using the sum of sparse codes from two levels as features for designing the class classifiers can support this implicit feature sharing among classes within the same super-class. [sent-172, score-0.608]
</p><p>72 N do 7: Evaluate the group sparse codes z(n1) and z(n2) of the region xn ; 8: Choose the learning rate ρt = min(ρ, ρ ∗ n0 /n) 9: Update the classifiers and dictionaries by a projected gradient step 10: Ws ← ? [sent-217, score-0.874]
</p><p>73 (n2)] end for end for Part 2: Region Tagging Input: (test region) Output: ˆy (predicted tag class) Evaluate the group sparse codes z(1) and z(2) of the test region The predicted tag for this test region is ˆy = arg maxj W(ˆ z(1) + z(2) ). [sent-225, score-1.758]
</p><p>74 Datasets and Feature Extraction We evaluated our approach for region tagging using several benchmarks, including MSRC-v1, MSRC-v2  [19], and  SAIAPR TC-12 datasets [2]. [sent-229, score-0.517]
</p><p>75 For SAIAPR TC-12 dataset, we select the same 27 localized tags out of 276 tags as in [7] for evaluation. [sent-239, score-0.5]
</p><p>76 Then we randomly select 2500 regions whose tags are within the selected subset of 27 tags as the training set and another 500 regions as the test set. [sent-240, score-0.744]
</p><p>77 The effect of parameters λ1 and λ2 on the region tagging performance of our method on three datasets. [sent-245, score-0.481]
</p><p>78 In order to demonstrate that the super-class level can help improve the accuracy of region tagging, we use single-layer supervised dictionary learning (SSDL) corresponding to the basic-class level as another baseline. [sent-255, score-0.665]
</p><p>79 The performance of tagging accuracy (number of correctly classified regions over the total test regions) is reported as the average over 5 different trials corresponding to different partitions of training and test sets. [sent-256, score-0.497]
</p><p>80 The performance of region tagging by our method with different λ1 and λ2 on three datasets are illustrated in Figure 3. [sent-269, score-0.517]
</p><p>81 Experimental Results The accuracies of region tagging using different methods on three datasets are summarized in Table 1. [sent-273, score-0.517]
</p><p>82 In particular, when compared with other sparse coding-based algorithms, SSDL and our method significantly improve the performance for region tagging on MSRC-v1 dataset—by a margin close to 10% and 20% respectively. [sent-275, score-0.617]
</p><p>83 This is because the labeled tag distribution in MSRC-v1 is very unbalanced and the tag with most training regions is more likely to be selected for reconstruction of test regions when using the group sparse coding algorithm. [sent-276, score-1.725]
</p><p>84 And this  good performance by our method demonstrates that, we effectively explored the semantic relationship among tags and make the super-class level help improve the performance for region tagging. [sent-279, score-0.542]
</p><p>85 Since we randomly sampled image regions of the SAIAPR TC-12 dataset and the spatial kernel might not be built, the performance for region tagging by SGSC is not reported in Table 1 as in [7]. [sent-282, score-0.587]
</p><p>86 Figures 4 and 6 illustrate two tag taxonomies associated with MSRC-v1 and MSRC-v2 respectively while Figures 5 and 7 display the corresponding confusion matrices obtained by SSDL and our method under the two datasets. [sent-283, score-0.606]
</p><p>87 Comparing the confusion matrix obtained by SSDL with our method in Figure 5, we can see that tags building, tree, cow, aeroplane, bicycle have large improvements in tagging accuracy using our proposed method. [sent-285, score-0.648]
</p><p>88 Moreover, instead of classifying regions from the tag horse as face by  SSDL, our method classifies them as cow which is also in the same super-class as horse. [sent-286, score-0.661]
</p><p>89 obtained from the super-class level to help improve the accuracy of tag nodes from the basic-class level. [sent-319, score-0.589]
</p><p>90 It is also interesting to note that the tag car has a slight decrease in tagging accuracy because some regions from car are misclassified as bicycle which is also in the same-super class. [sent-320, score-1.067]
</p><p>91 Thus, different tags benefit in different degrees from the implicit sharing of sparse codes and a similar phenomenon has also been observed in [18] which uses a parameter sharing strategy. [sent-321, score-0.686]
</p><p>92 To further investigate the performance of region tagging by SSDL and our method, we select nine tags in each dataset and report the corresponding tagging accuracy of each tag in Figure 8. [sent-322, score-1.631]
</p><p>93 From the detailed tagging performance, we can see that our method obtains better tagging performance for most of the tags. [sent-323, score-0.718]
</p><p>94 One possible reason is  that the visual appearances of image regions from these tags are very different from other tags within the same superclass which introduces a negative transfer. [sent-325, score-0.66]
</p><p>95 Conclusion In this paper, we have proposed a multi-layer hierarchical supervised dictionary learning framework for region tagging by exploring the given tag taxonomy. [sent-347, score-1.45]
</p><p>96 Specifically, we associate each tag node in the taxonomy with one nodespecific dictionary and concatenate the node-specific dic-  tionaries in each level to construct a level-specific dictionary. [sent-348, score-1.193]
</p><p>97 Using the level-specific dictionary and corresponding level-specific group structure, we obtain level-specific sparse codes that are also close to the ideal sparse codes. [sent-349, score-0.996]
</p><p>98 The sparse codes from different levels are summed up as the final feature representation to learn the level-specific classifier. [sent-350, score-0.484]
</p><p>99 This enables us to simultaneously take advantages of the robust encoding ability of group sparse coding as well as the semantic relationship in the tag taxonomy. [sent-351, score-0.913]
</p><p>100 Unified tag analysis  [10] [11]  [12] [13] [14] [15] [16] [17] [18]  [19]  [20] [21]  with multi-edge graph. [sent-437, score-0.519]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tag', 0.519), ('tagging', 0.359), ('dictionary', 0.346), ('tags', 0.25), ('codes', 0.224), ('ssdl', 0.214), ('taxonomy', 0.199), ('dictionaries', 0.178), ('saiapr', 0.161), ('sparse', 0.136), ('ws', 0.124), ('region', 0.122), ('group', 0.116), ('msdl', 0.107), ('regions', 0.106), ('lasso', 0.098), ('unbalanced', 0.096), ('zn', 0.085), ('levels', 0.081), ('coding', 0.075), ('sgsc', 0.054), ('superclass', 0.054), ('wsz', 0.054), ('items', 0.051), ('supervised', 0.051), ('level', 0.046), ('dz', 0.046), ('dog', 0.042), ('concatenate', 0.041), ('jg', 0.04), ('ideal', 0.038), ('reconstruct', 0.037), ('datasets', 0.036), ('semantic', 0.036), ('classifiers', 0.036), ('dk', 0.034), ('rd', 0.034), ('mairal', 0.034), ('among', 0.033), ('multimedia', 0.033), ('animal', 0.033), ('training', 0.032), ('td', 0.032), ('xn', 0.032), ('arbitrariness', 0.032), ('hierarchial', 0.032), ('slep', 0.032), ('relationship', 0.031), ('learning', 0.03), ('implicit', 0.03), ('misclassified', 0.03), ('contextual', 0.029), ('segmented', 0.028), ('proximal', 0.027), ('encourage', 0.027), ('penalty', 0.027), ('matrices', 0.027), ('ds', 0.027), ('discriminative', 0.025), ('activate', 0.024), ('taxonomies', 0.024), ('class', 0.024), ('help', 0.024), ('yuan', 0.024), ('hierarchical', 0.023), ('sharing', 0.023), ('block', 0.022), ('nine', 0.022), ('summed', 0.022), ('associate', 0.021), ('classes', 0.021), ('node', 0.021), ('learn', 0.021), ('correlations', 0.021), ('reconstruction', 0.02), ('confusion', 0.02), ('royal', 0.02), ('share', 0.02), ('bach', 0.02), ('dg', 0.019), ('bicycle', 0.019), ('hierarchies', 0.019), ('classifies', 0.018), ('jointly', 0.018), ('label', 0.018), ('plant', 0.018), ('cow', 0.018), ('yang', 0.018), ('belong', 0.017), ('arranged', 0.017), ('gao', 0.017), ('rj', 0.017), ('balanced', 0.017), ('car', 0.017), ('ponce', 0.017), ('associated', 0.016), ('indicator', 0.016), ('marszalek', 0.016), ('rgb', 0.016), ('preserved', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="422-tfidf-1" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>2 0.51325178 <a title="422-tfidf-2" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>3 0.44097048 <a title="422-tfidf-3" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>4 0.34622842 <a title="422-tfidf-4" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>5 0.30465424 <a title="422-tfidf-5" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>6 0.28997645 <a title="422-tfidf-6" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>7 0.28608418 <a title="422-tfidf-7" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>8 0.26396668 <a title="422-tfidf-8" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>9 0.25444368 <a title="422-tfidf-9" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>10 0.23676474 <a title="422-tfidf-10" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>11 0.18537287 <a title="422-tfidf-11" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>12 0.18002596 <a title="422-tfidf-12" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>13 0.1586145 <a title="422-tfidf-13" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>14 0.15034616 <a title="422-tfidf-14" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>15 0.14387868 <a title="422-tfidf-15" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>16 0.10101051 <a title="422-tfidf-16" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>17 0.097190961 <a title="422-tfidf-17" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>18 0.09541636 <a title="422-tfidf-18" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>19 0.085829698 <a title="422-tfidf-19" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>20 0.084151775 <a title="422-tfidf-20" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, -0.183), (2, -0.223), (3, 0.294), (4, -0.07), (5, -0.099), (6, 0.082), (7, 0.14), (8, -0.075), (9, 0.103), (10, 0.004), (11, 0.053), (12, 0.024), (13, 0.062), (14, 0.045), (15, 0.0), (16, 0.048), (17, 0.059), (18, -0.057), (19, -0.03), (20, 0.024), (21, 0.051), (22, -0.026), (23, 0.078), (24, -0.017), (25, -0.0), (26, 0.234), (27, 0.123), (28, 0.518), (29, -0.145), (30, -0.08), (31, -0.016), (32, 0.11), (33, 0.045), (34, -0.063), (35, 0.01), (36, 0.01), (37, -0.008), (38, 0.079), (39, 0.01), (40, 0.024), (41, -0.007), (42, 0.064), (43, -0.063), (44, -0.023), (45, -0.032), (46, 0.019), (47, -0.026), (48, -0.002), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93365967 <a title="422-lsi-1" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>2 0.89167213 <a title="422-lsi-2" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>Author: Zijia Lin, Guiguang Ding, Mingqing Hu, Jianmin Wang, Xiaojun Ye</p><p>Abstract: Though widely utilized for facilitating image management, user-provided image tags are usually incomplete and insufficient to describe the whole semantic content of corresponding images, resulting in performance degradations in tag-dependent applications and thus necessitating effective tag completion methods. In this paper, we propose a novel scheme denoted as LSR for automatic image tag completion via image-specific and tag-specific Linear Sparse Reconstructions. Given an incomplete initial tagging matrix with each row representing an image and each column representing a tag, LSR optimally reconstructs each image (i.e. row) and each tag (i.e. column) with remaining ones under constraints of sparsity, considering imageimage similarity, image-tag association and tag-tag concurrence. Then both image-specific and tag-specific reconstruction values are normalized and merged for selecting missing related tags. Extensive experiments conducted on both benchmark dataset and web images well demonstrate the effectiveness of the proposed LSR.</p><p>3 0.82154649 <a title="422-lsi-3" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>4 0.53650737 <a title="422-lsi-4" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>5 0.53534448 <a title="422-lsi-5" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>6 0.52855647 <a title="422-lsi-6" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>7 0.52502763 <a title="422-lsi-7" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>8 0.50607038 <a title="422-lsi-8" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>9 0.49514937 <a title="422-lsi-9" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>10 0.45531794 <a title="422-lsi-10" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>11 0.45185348 <a title="422-lsi-11" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>12 0.42338562 <a title="422-lsi-12" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>13 0.35713214 <a title="422-lsi-13" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>14 0.34299871 <a title="422-lsi-14" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>15 0.34224683 <a title="422-lsi-15" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>16 0.31898162 <a title="422-lsi-16" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>17 0.30859351 <a title="422-lsi-17" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>18 0.30653572 <a title="422-lsi-18" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>19 0.27408171 <a title="422-lsi-19" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>20 0.26949519 <a title="422-lsi-20" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.097), (16, 0.037), (19, 0.015), (26, 0.046), (33, 0.277), (58, 0.213), (67, 0.074), (69, 0.059), (77, 0.042), (87, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85343969 <a title="422-lda-1" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>Author: Jingjing Zheng, Zhuolin Jiang</p><p>Abstract: Tags of image regions are often arranged in a hierarchical taxonomy based on their semantic meanings. In this paper, using the given tag taxonomy, we propose to jointly learn multi-layer hierarchical dictionaries and corresponding linear classifiers for region tagging. Specifically, we generate a node-specific dictionary for each tag node in the taxonomy, and then concatenate the node-specific dictionaries from each level to construct a level-specific dictionary. The hierarchical semantic structure among tags is preserved in the relationship among node-dictionaries. Simultaneously, the sparse codes obtained using the levelspecific dictionaries are summed up as the final feature representation to design a linear classifier. Our approach not only makes use of sparse codes obtained from higher levels to help learn the classifiers for lower levels, but also encourages the tag nodes from lower levels that have the same parent tag node to implicitly share sparse codes obtained from higher levels. Experimental results using three benchmark datasets show that theproposed approach yields the best performance over recently proposed methods.</p><p>2 0.82183617 <a title="422-lda-2" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>Author: Xin Guo, Dong Liu, Brendan Jou, Mojun Zhu, Anni Cai, Shih-Fu Chang</p><p>Abstract: Object co-detection aims at simultaneous detection of objects of the same category from a pool of related images by exploiting consistent visual patterns present in candidate objects in the images. The related image set may contain a mixture of annotated objects and candidate objects generated by automatic detectors. Co-detection differs from the conventional object detection paradigm in which detection over each test image is determined one-by-one independently without taking advantage of common patterns in the data pool. In this paper, we propose a novel, robust approach to dramatically enhance co-detection by extracting a shared low-rank representation of the object instances in multiple feature spaces. The idea is analogous to that of the well-known Robust PCA [28], but has not been explored in object co-detection so far. The representation is based on a linear reconstruction over the entire data set and the low-rank approach enables effective removal of noisy and outlier samples. The extracted low-rank representation can be used to detect the target objects by spectral clustering. Extensive experiments over diverse benchmark datasets demonstrate consistent and significant performance gains of the proposed method over the state-of-the-art object codetection method and the generic object detection methods without co-detection formulations.</p><p>3 0.81763202 <a title="422-lda-3" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>4 0.81751126 <a title="422-lda-4" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>5 0.81697619 <a title="422-lda-5" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>Author: Wongun Choi, Yu-Wei Chao, Caroline Pantofaru, Silvio Savarese</p><p>Abstract: Visual scene understanding is a difficult problem interleaving object detection, geometric reasoning and scene classification. We present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the 3D Geometric Phrase Model which captures the semantic and geometric relationships between objects whichfrequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings from a single image, while also improving individual object detections.</p><p>6 0.81696355 <a title="422-lda-6" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>7 0.81673121 <a title="422-lda-7" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>8 0.81628644 <a title="422-lda-8" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>9 0.81533784 <a title="422-lda-9" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>10 0.81519711 <a title="422-lda-10" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>11 0.81511623 <a title="422-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.81430948 <a title="422-lda-12" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>13 0.81408763 <a title="422-lda-13" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>14 0.81374514 <a title="422-lda-14" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>15 0.81272107 <a title="422-lda-15" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>16 0.81251782 <a title="422-lda-16" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>17 0.81225502 <a title="422-lda-17" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>18 0.8121838 <a title="422-lda-18" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>19 0.81206828 <a title="422-lda-19" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>20 0.81200016 <a title="422-lda-20" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
