<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-5" href="#">cvpr2013-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</h1>
<br/><p>Source: <a title="cvpr-2013-5-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Irie_A_Bayesian_Approach_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Go Irie, Dong Liu, Zhenguo Li, Shih-Fu Chang</p><p>Abstract: Despite significant progress, most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. However, Web images are often associated with text data which may carry substantial information regarding image semantics, and may be exploited for visual dictionary learning. This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering, in addition to information of image descriptors. Existing co-clustering methods are not optimal for this problem because they ignore the structure of image descriptors in the continuous space, which is crucial for capturing visual characteristics of images. We propose a novel Bayesian co-clustering model to jointly estimate the underlying distributions of the continuous image descriptors as well as the relationship between such distributions and the textual words through a unified Bayesian inference. Extensive experiments on image categorization and retrieval have validated the substantial value of the proposed joint modeling in improving visual dictionary learning, where our model shows superior performance over several recent methods.</p><p>Reference: <a title="cvpr-2013-5-reference" href="../cvpr2013_reference/cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp ‡ {dongl iu Abstract Despite significant progress, most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. [sent-5, score-0.628]
</p><p>2 However, Web images are often associated with text data which may carry substantial information regarding image semantics, and may be exploited for visual dictionary learning. [sent-6, score-0.468]
</p><p>3 This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering, in addition to information of image descriptors. [sent-7, score-1.1]
</p><p>4 We propose a novel Bayesian co-clustering model to jointly estimate the underlying distributions of the continuous image descriptors as well as the relationship between such distributions and the textual words through a unified Bayesian inference. [sent-9, score-1.213]
</p><p>5 Extensive experiments on image categorization and retrieval have validated the substantial value of the proposed joint modeling in improving visual dictionary learning, where our model shows superior performance over several recent methods. [sent-10, score-0.479]
</p><p>6 The process is to first train a visual dictionary based on the extracted descriptors from an image collection and then to encode the descriptors of each image into a histogram based on the learned dictionary. [sent-14, score-0.728]
</p><p>7 There has been considerable interest in visual dictionary learning which can be classified into two paradigms. [sent-16, score-0.466]
</p><p>8 The second one is the supervised learning, which incorporates class labels into the visual dictionary [12, 30, 9, 10, 17]. [sent-21, score-0.483]
</p><p>9 edu  ,  isting visual dictionary learning methods are based on only single-modal information, i. [sent-25, score-0.466]
</p><p>10 These facts motivate us to consider leveraging textual words for visual dictionary learning. [sent-32, score-1.136]
</p><p>11 Specifically, the problem can be stated as following: given a set of images and their associated textual words, how to learn a visual dictionary that incorporates both image and text information. [sent-33, score-1.035]
</p><p>12 First, there are a large number of local descriptors extracted from the images, whose corresponding relations to the textual words are totally unknown, making it difficult to explore the multimodal correlation. [sent-35, score-1.071]
</p><p>13 Second, the visual and textual spaces are completely different from each other: image descriptors are generally in a continuous space (e. [sent-36, score-0.819]
</p><p>14 , SIFT is typically represented as a 128-dimensional realvalued vector) whereas textual words are in a discrete space. [sent-38, score-0.706]
</p><p>15 Addressing these issues, we propose a novel approach for learning a visual dictionary from both image and text information. [sent-39, score-0.504]
</p><p>16 The clusters of image descriptors are determined based on their relations with respect to the textual word clusters, which well captures the multimodal correlation. [sent-44, score-1.159]
</p><p>17 Note that the textual word clusters are important for discovering the significant multimodal correlation, due to the fact that the individual word is noisy and may not convey beneficial information while the clusters of multiple words can reflect the semantic topic of the consti333222999  ? [sent-45, score-1.527]
</p><p>18 ated textual words, we first form a relational matrix that represents the relationship between image descriptors (rows) and textual words (columns), where each element is 1if the corresponding pair is extracted from an identical image and 0 otherwise. [sent-79, score-1.75]
</p><p>19 (b) Our continuousdiscrete Bayesian co-clustering (CD-BCC) jointly estimates distributions of the continuous image descriptors as well as the relationship between the image descriptor distributions and textual words. [sent-80, score-1.26]
</p><p>20 Each resulting cluster of image descriptors is thus expected to (i) have consistently co-occurring textual words and (ii) be visually different from the other clusters. [sent-82, score-0.897]
</p><p>21 (c) These clusters form the final visual dictionary and are used to encode an image into a single image representation vector (histogram). [sent-83, score-0.545]
</p><p>22 Specifically, we investigate how to perform co-clustering along a continuous image descriptor space and a discrete textual word space simultaneously, and propose continuousdiscrete Bayesian co-clustering (CD-BCC). [sent-85, score-0.956]
</p><p>23 A straightforward approach may be first to quantize image de-  scriptors using K-means and then to co-cluster quantized descriptors (visual words) and textual words. [sent-87, score-0.705]
</p><p>24 Unlike these, our CD-BCC simultaneously estimates the underlying distributions of image descriptors over the continuous space and the relationship between the distributions and textual words via a unified Bayesian inference framework. [sent-89, score-1.237]
</p><p>25 Consequently, each image descriptor cluster used to construct each dimension of the final image representation vector is ideally consistent to a set of textual words with consistent semantic topic as well as visually different from the other clusters. [sent-90, score-1.051]
</p><p>26 Extensive experiments on five different datasets will demonstrate that the proposed multimodal visual dictionary learning approach can achieve significant performance gains when evaluated over various tasks including image classification and content-based image retrieval (CBIR). [sent-91, score-0.671]
</p><p>27 Related Work We review some recent studies on visual dictionary learning, co-clustering, and multimodal topic modeling. [sent-94, score-0.732]
</p><p>28 [12] trains a dictionary so as to maximize the mutual information. [sent-97, score-0.363]
</p><p>29 [30, 9, 10, 17, 16] learn a single visual dictionary by jointly optimizing visual dictionaries and discriminative functions. [sent-99, score-0.557]
</p><p>30 We aim to leverage weak textual words associated with images, instead of assuming strong class labels. [sent-100, score-0.73]
</p><p>31 Prior supervised methods assume that class labels are mutually exclusive, which may not be reasonable in our problem because there is often strong correlation among textual words. [sent-101, score-0.647]
</p><p>32 Contrary, our approach is based on co-clustering and takes into account textual word clusters which may effectively guide visual feature clustering. [sent-102, score-0.883]
</p><p>33 Information theoretic co-clustering (ITCC) [4] determines clusters so as to minimize the loss of mutual information between a given relational matrix and its co-clustering results. [sent-107, score-0.431]
</p><p>34 The most relevant approach to ours is Bayesian co-clustering (BCC) [22] which is a generative model of a relational matrix and estimates clusters of rows and columns in a Bayesian inference framework. [sent-109, score-0.469]
</p><p>35 333333000  Unlike these previous methods, our CD-BCC is for a pair of continuous and discrete variables and jointly estimates the distributions of image descriptors over the continuous space and co-clusters ofthe distributions and textual words. [sent-111, score-1.073]
</p><p>36 For instance, [29, 32] proposed LSA-based methods to model visual and textual words with an underlying latent topic space. [sent-116, score-0.891]
</p><p>37 In particular, Li et al [14] proposed a Bayesian multimodal topic model for visual dictionary learning. [sent-119, score-0.732]
</p><p>38 Our CDBCC is also a Bayesian model for visual dictionary learning but ours is a co-clustering model, not a topic model. [sent-120, score-0.563]
</p><p>39 Specifically, topic models assume some mixture distributions over visual words as well as textual words. [sent-121, score-1.034]
</p><p>40 Our CDBCC also assumes a mixture over image descriptors, but does not assume any distributions over textual words. [sent-122, score-0.731]
</p><p>41 Instead, we assume a mixture over a relational matrix that encourages the model to identify the significant multimodal correlation from sparse and noisy relational data. [sent-123, score-0.832]
</p><p>42 Multimodal Visual Dictionary Learning  ×  We assume the typical image descriptor extraction process: a set of key points are detected from each training image first, and then an image descriptor (e. [sent-125, score-0.348]
</p><p>43 Then our problem is: given the initial relational matri∈x RR and the corresponding set of image descriptors X, the goal is to find K clusters of image descriptors X used to form a visual dictionary with the assistance of the relational matrix R. [sent-141, score-1.368]
</p><p>44 Illustrations of generative processes: (b) image descriptor generation and (c) relational matrix generation. [sent-146, score-0.533]
</p><p>45 , a joint distribution of image descriptors X, the relational matrix R, and image descriptor clusters. [sent-150, score-0.63]
</p><p>46 Based on the joint distribution, the image descriptor clusters are estimated based on its Bayesian posterior distributions under given X and R. [sent-151, score-0.453]
</p><p>47 In the image descriptor generation process, all image descriptors X are generated from a mixture of V descriptor distributions over the continuous image descriptor space. [sent-158, score-0.9]
</p><p>48 Image descriptor generation: N image descriptors X are generated from a mixture of V descriptor distributions Norm(μv , Σx) (v = 1, . [sent-162, score-0.65]
</p><p>49 Draw m∼ix Nturoer proportion of descriptor distributions π; γ, V ∼ Dir(γ/V ). [sent-168, score-0.334]
</p><p>50 Fπ;orγ e,aVch ∼ image descriptor xi (a) Draw descriptor distribution assignment ωi; π ∼ Mult(π) (b) Dr;awπ image descriptor xi |μ, ωi ; Σx ∼ Norm(μωi ; Σx) In Step 1, mean vectors o∼f NVo descriptor distributions, μ, are generated. [sent-170, score-0.767]
</p><p>51 In Step 3, for each image descriptor xi, (a) one descriptor distribution ωi ∈ {1, . [sent-172, score-0.375]
</p><p>52 Relational matrix generation: The relational matrix R is generated from Poisson(θk,l) as the following process: 4. [sent-176, score-0.326]
</p><p>53 , each pair of image descriptor cluster and word cluster), draw co-occurrence frequency θk,l ; φ ∼ Gamma(β, φ). [sent-179, score-0.411]
</p><p>54 For each descriptor distribution v and each word j, draw image descriptor cluster assignment |κ ∼ Mult(κ) and word cluster assignment zjw |λ ∼ Mu|κlt(λ∼) respectively. [sent-183, score-0.894]
</p><p>55 For each pair of descriptor distribution and word (v, j), draw element of relational matrix rv,j |Θ, , zjw ∼ Poisson(θzvx ,zwj ). [sent-185, score-0.771]
</p><p>56 Note that rv,j denotes the number of times image descriptors in v-th descriptor distributions co-occurs with j-th word1 . [sent-202, score-0.449]
</p><p>57 2 Visual Dictionary Inference Observing the image descriptors X and the relational matrix R, we compute the posterior distributions to infer the image descriptor clusters and the mean vectors of the descriptor distributions μ used as the visual dictionary. [sent-205, score-1.26]
</p><p>58 β,  zvx  zvx  zvx  zx  The above generative process determines the joint distribution p(X, R, ω, , , π, κ, λ, μ, Θ). [sent-206, score-0.726]
</p><p>59 μ of V descriptor distributions are estimated as  zx zw  zx  μv = ? [sent-215, score-0.76]
</p><p>60 rs while determines the clusters of these distributions based on their correlation to word clusters. [sent-219, score-0.438]
</p><p>61 Therefore, they can be used as the visual dictionary to generate image representation for the new images. [sent-220, score-0.43]
</p><p>62 R0, where R0 is the initial relational matrix between N image descriptors and W words. [sent-224, score-0.429]
</p><p>63 will explain how to utilize them to generate textual information embedded image representation. [sent-225, score-0.567]
</p><p>64 Innc computer vision, dictionary learning and coding are seen as independent processes (e. [sent-235, score-0.484]
</p><p>65 This can be a clear evidence that our CD-BCC successfully captures discriminative information from textual words via  zx  zx. [sent-281, score-0.885]
</p><p>66 Non-parametric Extension We note that we need to manually setup the following three parameters: the final dictionary size K, the number of descriptor distributions V and the number of word clusters L. [sent-284, score-0.923]
</p><p>67 As long as we are interested in only the final dictionary size K, the other two, V and L, are parameters. [sent-285, score-0.363]
</p><p>68 333333222  sual dictionary trained by our CD-BCC on UIUC-Sport dataset. [sent-288, score-0.363]
</p><p>69 These datasets are selected for direct comparison to a state-of-the-art Bayesian multimodal topic model for dictionary learning [14]. [sent-323, score-0.701]
</p><p>70 9623 badm  bocc  croq  polo  rock  rowi  sail  snow  badm  bocc  croq  polo  rock  rowi  sail  snow  /  (a)  (b)  mcohinefpstraguielnhc8301oas409f3re61i2h058g1 ni06s3m291o7u6n2p1375e4n0s86rt2e0a867l mhcospifnrtaueglhio231c80. [sent-344, score-0.73]
</p><p>71 (a-c) Example of image descriptors (red circles) correlated to the word cluster #7 (“horse”) are overlaid on images. [sent-357, score-0.325]
</p><p>72 For both datasets, we randomly sample 50000 image descriptors (SIFT) extracted from the training data, and use them for visual dictionary learning. [sent-361, score-0.59]
</p><p>73 This may be because that the unified learning of the intermediate descriptor distributions and their correlation to the textual words allows the visual dictionary to capture both visual and textual properties of the training images. [sent-368, score-2.166]
</p><p>74 the dictionary size K where 50000 training samples are used and (b) the number of training samples in which we fix K = 256. [sent-373, score-0.363]
</p><p>75 Numbers of (a) word clusters L and (b) descriptor distributions V at each iteration step. [sent-375, score-0.56]
</p><p>76 Third, most coclustering methods show higher performance than the stateof-the-art Bayesian dictionary learning [14]. [sent-377, score-0.456]
</p><p>77 This suggests that co-clustering can be more promising than multimodal topic modeling for multimodal visual dictionary learning. [sent-378, score-0.937]
</p><p>78 One reason can be that CD-BCC successfully discovers image descriptor clusters related to a specific image category via co-clustered textual words. [sent-390, score-0.856]
</p><p>79 5(a-c) show that image descriptors correlated to the “horse” word cluster are actually extracted from the parts of horses. [sent-394, score-0.347]
</p><p>80 We also analyze the performance when varying dictionary size K and the number of training samples (image descriptors). [sent-395, score-0.387]
</p><p>81 Similar to the most existing visual dictionary learning methods, the performance is somewhat sensitive to K. [sent-398, score-0.466]
</p><p>82 The major reason can be that our CD-BCC trains a visual dictionary based on a statistical relationship between distributions of image descriptors and textual words, which can be stable (robust) against the number of training samples as well as ways to choose them. [sent-403, score-1.298]
</p><p>83 7 shows estimated V (the number of descriptor distributions) and L (the number of clusters for text words) at each Gibbs sampler iteration step. [sent-405, score-0.354]
</p><p>84 We select this dataset because this is frequently used to evaluate the performance of visual dictionary learning methods. [sent-410, score-0.466]
</p><p>85 Caltech101 originally does not include any textual words, we thus directly use the class labels as textual words. [sent-414, score-1.158]
</p><p>86 We employ sparse coding with SPM [13] and linear SVM to perform image categorization based on the visual dictionary trained by our CD-BCC. [sent-415, score-0.59]
</p><p>87 We compare ours to ScSPM [27] (sparse coding and SPM with K-means dictionary + linear SVM), two visual dictionary learning methods (KSVD [1] and LLC [25]), and four co-clustering based methods (SCC, ITTC, NMTF, and BCC). [sent-416, score-0.914]
</p><p>88 Note that our method is also comparable to some other recent supervised visual dictionary learning methods like [12, 30]. [sent-420, score-0.495]
</p><p>89 The performance may be further improved by combining our visual dictionary with more sophisticated coding approaches like LLC. [sent-421, score-0.515]
</p><p>90 Results on various dictionary sizes K ∈ {8, 16, 32, 64, 128, 256, 512} are reported. [sent-432, score-0.363]
</p><p>91 For textual words, we first extracted only noun terms from all the documents, and selected 300 most frequent words (W = 300). [sent-440, score-0.728]
</p><p>92 These results suggest that (i) leveraging textual words via co-clustering is effective for CBIR, and (ii) our CD-BCC is the promising co-clustering approach for this purpose. [sent-449, score-0.706]
</p><p>93 Conclusion Focusing on the scenario where images are associated with textual words, we presented a Bayesian approach to multimodal visual dictionary learning. [sent-454, score-1.202]
</p><p>94 We proposed a novel Bayesian co-clustering, CD-BCC, to learn a single visual dictionary based on the distributions of image descriptors over the continuous space, as well as the relationship between image descriptors and textual words. [sent-455, score-1.483]
</p><p>95 periments validated values of textual words in improving visual dictionary learning, where our model showed superior performance over several recent methods. [sent-462, score-1.16]
</p><p>96 Sampling Distribution The sampling distributions for  ω,  zx, and zw are:  p(ωi = t|X, R, ω−i , ∝ (mt,−i + γ/V )  zx , zw)  ×? [sent-465, score-0.407]
</p><p>97 (9)  where, mt/mt,−i is the number of image descriptors in tth descriptor distribution with/without i-th image descriptor. [sent-488, score-0.339]
</p><p>98 mkx,−v (mlw,−j) is the number of descriptor distributions (kte,−xtvual wl,−orjds) in k-th (lth) cluster without v? [sent-490, score-0.364]
</p><p>99 Learning a discriminative dictionary for sparse coding via label consistent k-svd. [sent-566, score-0.474]
</p><p>100 On the integration of topic modeling and dictionary learning. [sent-596, score-0.46]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('textual', 0.567), ('dictionary', 0.363), ('relational', 0.256), ('multimodal', 0.205), ('zx', 0.179), ('descriptor', 0.174), ('zvx', 0.152), ('words', 0.139), ('descriptors', 0.138), ('distributions', 0.137), ('word', 0.134), ('bayesian', 0.123), ('clusters', 0.115), ('cbir', 0.114), ('topic', 0.097), ('zjw', 0.095), ('zw', 0.091), ('polo', 0.089), ('coding', 0.085), ('zq', 0.078), ('flickr', 0.076), ('wikipedia', 0.074), ('visual', 0.067), ('mult', 0.063), ('dictionaries', 0.06), ('bcc', 0.057), ('cdbcc', 0.057), ('coclustering', 0.057), ('country', 0.057), ('nmtf', 0.057), ('xq', 0.057), ('snow', 0.054), ('cluster', 0.053), ('labelme', 0.051), ('scc', 0.051), ('draw', 0.05), ('categorization', 0.049), ('badminton', 0.047), ('continuous', 0.047), ('ml', 0.045), ('bovw', 0.044), ('croquet', 0.044), ('dir', 0.042), ('llc', 0.042), ('generative', 0.039), ('rock', 0.039), ('badm', 0.038), ('boarding', 0.038), ('bocc', 0.038), ('croq', 0.038), ('itcc', 0.038), ('mbw', 0.038), ('rowi', 0.038), ('text', 0.038), ('learning', 0.036), ('kdd', 0.036), ('matrix', 0.035), ('continuousdiscrete', 0.034), ('bocce', 0.031), ('sail', 0.031), ('texts', 0.031), ('tags', 0.031), ('generation', 0.029), ('dirichlet', 0.029), ('supervised', 0.029), ('gibbs', 0.028), ('articles', 0.028), ('collapsed', 0.028), ('correlation', 0.027), ('distribution', 0.027), ('posterior', 0.027), ('coast', 0.027), ('highway', 0.027), ('sampler', 0.027), ('categories', 0.027), ('mixture', 0.027), ('sparse', 0.026), ('relationship', 0.026), ('frequencies', 0.025), ('determines', 0.025), ('blei', 0.024), ('periments', 0.024), ('analyze', 0.024), ('horse', 0.024), ('class', 0.024), ('documents', 0.024), ('inference', 0.024), ('gamma', 0.023), ('xue', 0.023), ('proportion', 0.023), ('hyperparameters', 0.022), ('extracted', 0.022), ('xi', 0.022), ('bipartite', 0.022), ('unified', 0.022), ('semantic', 0.021), ('nonparametric', 0.021), ('latent', 0.021), ('distinguishes', 0.021), ('spm', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="5-tfidf-1" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>Author: Go Irie, Dong Liu, Zhenguo Li, Shih-Fu Chang</p><p>Abstract: Despite significant progress, most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. However, Web images are often associated with text data which may carry substantial information regarding image semantics, and may be exploited for visual dictionary learning. This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering, in addition to information of image descriptors. Existing co-clustering methods are not optimal for this problem because they ignore the structure of image descriptors in the continuous space, which is crucial for capturing visual characteristics of images. We propose a novel Bayesian co-clustering model to jointly estimate the underlying distributions of the continuous image descriptors as well as the relationship between such distributions and the textual words through a unified Bayesian inference. Extensive experiments on image categorization and retrieval have validated the substantial value of the proposed joint modeling in improving visual dictionary learning, where our model shows superior performance over several recent methods.</p><p>2 0.31167445 <a title="5-tfidf-2" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>3 0.26848182 <a title="5-tfidf-3" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>4 0.26711231 <a title="5-tfidf-4" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>5 0.26503402 <a title="5-tfidf-5" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>6 0.23850198 <a title="5-tfidf-6" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>7 0.23566265 <a title="5-tfidf-7" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>8 0.23378363 <a title="5-tfidf-8" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>9 0.18002596 <a title="5-tfidf-9" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>10 0.16830944 <a title="5-tfidf-10" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>11 0.15837173 <a title="5-tfidf-11" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>12 0.14121701 <a title="5-tfidf-12" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>13 0.12381136 <a title="5-tfidf-13" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>14 0.12092886 <a title="5-tfidf-14" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>15 0.11426979 <a title="5-tfidf-15" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>16 0.11411607 <a title="5-tfidf-16" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>17 0.11342743 <a title="5-tfidf-17" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>18 0.10267235 <a title="5-tfidf-18" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>19 0.10193618 <a title="5-tfidf-19" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>20 0.099240869 <a title="5-tfidf-20" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.183), (1, -0.178), (2, -0.21), (3, 0.233), (4, -0.077), (5, -0.082), (6, 0.043), (7, 0.113), (8, -0.079), (9, 0.056), (10, 0.001), (11, 0.045), (12, 0.044), (13, 0.036), (14, 0.046), (15, -0.023), (16, 0.043), (17, 0.033), (18, 0.024), (19, -0.078), (20, 0.082), (21, -0.0), (22, 0.057), (23, 0.043), (24, -0.036), (25, 0.033), (26, -0.004), (27, 0.084), (28, -0.031), (29, -0.041), (30, 0.061), (31, 0.036), (32, -0.045), (33, 0.051), (34, 0.026), (35, -0.043), (36, -0.023), (37, 0.091), (38, 0.022), (39, -0.069), (40, 0.044), (41, -0.039), (42, -0.103), (43, 0.041), (44, -0.009), (45, 0.019), (46, -0.038), (47, 0.01), (48, 0.046), (49, -0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95337611 <a title="5-lsi-1" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>Author: Go Irie, Dong Liu, Zhenguo Li, Shih-Fu Chang</p><p>Abstract: Despite significant progress, most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. However, Web images are often associated with text data which may carry substantial information regarding image semantics, and may be exploited for visual dictionary learning. This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering, in addition to information of image descriptors. Existing co-clustering methods are not optimal for this problem because they ignore the structure of image descriptors in the continuous space, which is crucial for capturing visual characteristics of images. We propose a novel Bayesian co-clustering model to jointly estimate the underlying distributions of the continuous image descriptors as well as the relationship between such distributions and the textual words through a unified Bayesian inference. Extensive experiments on image categorization and retrieval have validated the substantial value of the proposed joint modeling in improving visual dictionary learning, where our model shows superior performance over several recent methods.</p><p>2 0.84163642 <a title="5-lsi-2" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>3 0.8242498 <a title="5-lsi-3" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>4 0.81043518 <a title="5-lsi-4" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>5 0.80199504 <a title="5-lsi-5" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>Author: Cewu Lu, Jiaping Shi, Jiaya Jia</p><p>Abstract: Online dictionary learning is particularly useful for processing large-scale and dynamic data in computer vision. It, however, faces the major difficulty to incorporate robust functions, rather than the square data fitting term, to handle outliers in training data. In thispaper, wepropose a new online framework enabling the use of ?1 sparse data fitting term in robust dictionary learning, notably enhancing the usability and practicality of this important technique. Extensive experiments have been carried out to validate our new framework.</p><p>6 0.79887319 <a title="5-lsi-6" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>7 0.77384758 <a title="5-lsi-7" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>8 0.72947007 <a title="5-lsi-8" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>9 0.65438342 <a title="5-lsi-9" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>10 0.6525138 <a title="5-lsi-10" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>11 0.62285066 <a title="5-lsi-11" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>12 0.59093595 <a title="5-lsi-12" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>13 0.56570554 <a title="5-lsi-13" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>14 0.54706055 <a title="5-lsi-14" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>15 0.54020321 <a title="5-lsi-15" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>16 0.53885859 <a title="5-lsi-16" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>17 0.53780669 <a title="5-lsi-17" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>18 0.52528203 <a title="5-lsi-18" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>19 0.50752324 <a title="5-lsi-19" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>20 0.50170928 <a title="5-lsi-20" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.219), (10, 0.089), (16, 0.017), (26, 0.041), (28, 0.027), (33, 0.279), (39, 0.011), (67, 0.097), (69, 0.05), (76, 0.012), (77, 0.023), (87, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86501801 <a title="5-lda-1" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>same-paper 2 0.85421288 <a title="5-lda-2" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>Author: Go Irie, Dong Liu, Zhenguo Li, Shih-Fu Chang</p><p>Abstract: Despite significant progress, most existing visual dictionary learning methods rely on image descriptors alone or together with class labels. However, Web images are often associated with text data which may carry substantial information regarding image semantics, and may be exploited for visual dictionary learning. This paper explores this idea by leveraging relational information between image descriptors and textual words via co-clustering, in addition to information of image descriptors. Existing co-clustering methods are not optimal for this problem because they ignore the structure of image descriptors in the continuous space, which is crucial for capturing visual characteristics of images. We propose a novel Bayesian co-clustering model to jointly estimate the underlying distributions of the continuous image descriptors as well as the relationship between such distributions and the textual words through a unified Bayesian inference. Extensive experiments on image categorization and retrieval have validated the substantial value of the proposed joint modeling in improving visual dictionary learning, where our model shows superior performance over several recent methods.</p><p>3 0.85092652 <a title="5-lda-3" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>4 0.81827587 <a title="5-lda-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.81559283 <a title="5-lda-5" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>6 0.81550086 <a title="5-lda-6" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>7 0.81528056 <a title="5-lda-7" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>8 0.81348908 <a title="5-lda-8" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>9 0.81344849 <a title="5-lda-9" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>10 0.81301975 <a title="5-lda-10" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>11 0.81251019 <a title="5-lda-11" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>12 0.81241429 <a title="5-lda-12" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>13 0.8122704 <a title="5-lda-13" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>14 0.81195438 <a title="5-lda-14" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>15 0.81142092 <a title="5-lda-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.81112003 <a title="5-lda-16" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>17 0.81063992 <a title="5-lda-17" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>18 0.80998832 <a title="5-lda-18" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>19 0.80987257 <a title="5-lda-19" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>20 0.80973238 <a title="5-lda-20" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
