<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>259 cvpr-2013-Learning a Manifold as an Atlas</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-259" href="#">cvpr2013-259</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>259 cvpr-2013-Learning a Manifold as an Atlas</h1>
<br/><p>Source: <a title="cvpr-2013-259-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Pitelis_Learning_a_Manifold_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Nikolaos Pitelis, Chris Russell, Lourdes Agapito</p><p>Abstract: In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image.</p><p>Reference: <a title="cvpr-2013-259-reference" href="../cvpr2013_reference/cvpr-2013-Learning_a_Manifold_as_an_Atlas_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 pit e l s , chri s r , i Abstract In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. [sent-2, score-0.824]
</p><p>2 We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. [sent-3, score-0.487]
</p><p>3 In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. [sent-4, score-0.896]
</p><p>4 We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image. [sent-5, score-0.593]
</p><p>5 In high-dimensional  spaces, our day-to-day intuitions about distances are violated, and nearest neighbour (NN) and RBF based classifiers are less predictive and more easily swamped by noise. [sent-8, score-0.129]
</p><p>6 The majority of manifold learning techniques can be characterised as variants of kernel-PCA [9] that learn a smooth mapping from a high dimensional embedding space Rn into a low-dimensional space Rd while preserving properties of a local neighbourhood graph. [sent-10, score-0.587]
</p><p>7 These methods rely on unwrapping the manifold into a single chart isomorphic to Rd to characterise it. [sent-15, score-0.807]
</p><p>8 As such, they have difficulty learning closed manifolds such as the surface of a ball or a Klein bottle. [sent-16, score-0.135]
</p><p>9 This is a fundamental limitation in computer vision where many of the manifolds of interest are closed or contain closed components. [sent-17, score-0.177]
</p><p>10 In practice, existing manifold learning techniques can be adapted to handle closed cycles. [sent-19, score-0.434]
</p><p>11 For example, a one dimensional manifold of a gait cycle can be embedded in a two-dimensional space, and a larger neighbourhood that captures local second-order rather than the usual first-order information can be used to prevent the manifold from collapsing. [sent-20, score-0.875]
</p><p>12 We evaluate these standard approaches for NN based 3D reconstruction of walking and running, and find that they perform worse than using the original space. [sent-21, score-0.174]
</p><p>13 The presence of noise causes further difficulties, as points now lie near, but not on the manifold we wish to recover. [sent-22, score-0.411]
</p><p>14 The local neighbourhood graph produced by k nearest neighbours (k-NN) is extremely vulnerable to the presence of noise, and the properties of the neighbourhood graph which are preserved by the embedding do not correspond to the underlying manifold. [sent-23, score-0.467]
</p><p>15 Here, many existing manifold learning methods fail severely and collapse into degenerate structures in the presence of noise (see Fig. [sent-24, score-0.447]
</p><p>16 In response to these difficulties, we present a novel formulation for manifold learning, which we refer to as learning an atlas. [sent-26, score-0.37]
</p><p>17 As is common in differential geometry, we characterise the manifold as an atlas, or set of overlapping charts, each of which is isomorphic to Rd. [sent-27, score-0.495]
</p><p>18 Unlike existing machine learning approaches to manifold learning, this  allows us to directly learn closed and partially closed manifolds. [sent-28, score-0.498]
</p><p>19 Large stick-men represent the mean shape, while the small are Right: At manifold as overlapping cphreasretsn. [sent-30, score-0.397]
</p><p>20 Membership of additional charts is indicated by a ring drawn around the point. [sent-35, score-0.405]
</p><p>21 manifold; only in that we adaptively select the size of the region assigned to a chart in response to the amount of local noise, the intrinsic curvature of the manifold, and the sparsity of data. [sent-38, score-0.264]
</p><p>22 Unlike methods that preserve unstable numerical properties of the local neighbourhood graph, our method only preserves coarse topological properties. [sent-41, score-0.153]
</p><p>23 Any solution found using our approach has the property that if p is a point lying on or near the manifold there exists a local chart ci ∼= Rd to which both p and  all of its neighbours are assigned1 , and this allows the use of fast NN look up within ci for classification. [sent-43, score-0.815]
</p><p>24 If two points p and q are path connected in the neighbourhood graph then any local charts cp and cq containing them will be path connected, in a dual graph of charts, in which charts are directly connected if and only if they share an assigned point in common. [sent-45, score-1.066]
</p><p>25 We make use of this property in manifold unwrapping. [sent-46, score-0.348]
</p><p>26 As the method we propose finds charts corresponding to affine subspaces of the original space Rn we can directly use these for classification or for out-of-sample reconstruction from incomplete information. [sent-47, score-0.723]
</p><p>27 The notion of a manifold as a collection of overlapping charts has been exploited in the literature. [sent-51, score-0.802]
</p><p>28 Both [3, 15] for1This is possible as the charts are overlapping and a point may belong to more than one chart. [sent-52, score-0.476]
</p><p>29 mulated manifold learning as finding a set of charts that correspond to affine subspaces. [sent-53, score-0.855]
</p><p>30 As a final stage, both methods tried to align the subspaces found in Rd, and cannot learn closed manifolds. [sent-54, score-0.166]
</p><p>31 In motion tracking, [14] performed agglomerative clustering over predefined affine subspaces to learn closed manifolds. [sent-55, score-0.246]
</p><p>32 Within manifold learning, this adaptive shaping of regions is closest to methods such as [3], or the contraction and expansion (C&E;) method of [25] and the sparsity inducing method of [6], that adjust the neighbourhood before unwrapping. [sent-58, score-0.514]
</p><p>33 Outside of manifold learning, there is a strong relationship between our approach and methods of subspace clustering [22]. [sent-61, score-0.385]
</p><p>34 Like us, K-subspaces [21] alternates between assigning points to subspaces, and refitting the subspaces. [sent-62, score-0.127]
</p><p>35 Formulation and optimisation We define an atlas A, as a set of charts A = {c1W , c2 , . [sent-65, score-0.908]
</p><p>36 cinne}, over points P,, awsith a ae saceth c ohfart c ci containing a csubset of points ePri ⊆oi nPts. [sent-68, score-0.12]
</p><p>37 PU,n wlikithe tehaec hfo crhmaartl cdefinition of a manifold that allows⊆ a Pch. [sent-69, score-0.348]
</p><p>38 a Urt ntloi k bee warped as dite fisi mapped into the embedding space using any continuous invertible mapping, we restrict ourselves to affine transforms. [sent-70, score-0.139]
</p><p>39 This restriction to affine transforms in approximating the local manifold does not limit the expressiveness of our approach2 and means that PCA can be used to find embeddings. [sent-71, score-0.428]
</p><p>40 Our aim is to find a compact atlas containing few charts, such that: (i) for every point p, at least one chart contains 2Lie algebras make  use  of this equivalence between the  two  forms. [sent-72, score-0.726]
</p><p>41 111666444311  both p and a predefined neighbourhood3 Np of points in its vicinity, and (ii) the reconstruction error Nassociated with mapping a point from its location in the chart back into the embedding space is as low as possible. [sent-73, score-0.504]
</p><p>42 We decompose the problem of finding the best choice of atlas into two subproblems: Assigning points to charts subject to constraint (i), and choosing the affine mappings to minimise reconstruction error. [sent-74, score-1.118]
</p><p>43 Given an initial excess of chart proposals (generated using PCA on random subsets of the data) we assign points to charts in such a way that they share points (Sec. [sent-75, score-0.757]
</p><p>44 1), and alternate between refitting the subspaces associated with a chart (Sec. [sent-77, score-0.393]
</p><p>45 We define Ic, the interior of chart c, as the set of all points whose neighbours also belong to chart c; and we note that constraint (i) is satisfied if and only if every point lies in the interior of some chart. [sent-83, score-0.754]
</p><p>46 ∈xpUp(c)⎠⎞+ λMDL(x),  (1)  such that each point belongs to a chart’s interior: ∀p ∈ P, ∃c ∈ A : p ∈ Ic, p ∈ Ic =⇒  ∀q ∈ Np c ∈ xq,  (2) (3)  where Up(c) is the reconstruction error of assigning point p to chart c, and MDL(x) =  ? [sent-90, score-0.432]
</p><p>47 MtoDrL fu(nxc)t oisn a tmhianti tmakuems description length prior 0[1o o1th] tehrwati penalises txhe) itsoataml ninuimmubmer of charts used in an assignment. [sent-93, score-0.405]
</p><p>48 Despite the large state-space considered ((2A)P rather than the standard AP), this can be solved with a variant of α-expansion dtharadt assigns points to the interior of only one chart [16]. [sent-94, score-0.342]
</p><p>49 Pairwise regularisation In classification problems and when faced with relatively dense data, it can be useful to add additional regularisation terms to encourage larger charts that cover more of the manifold. [sent-96, score-0.61]
</p><p>50 Writing yp for the assignment of point p to the interior of one chart the regularisation takes the form of the standard Potts potential: ψp,q(yp, yq)  = θΔ(yp  =  yq),  (5)  defined with constant weight θ > 0 over all edges in the k-NN graph. [sent-98, score-0.476]
</p><p>51 We define the d dimensional subspace associated with chart ci in terms of its mean μi, and an orthonormal matrix Ci which describes its principal directions of variance. [sent-103, score-0.352]
</p><p>52 We set the reconstruction error for point p belonging to chart ci to Up(ci) = ||p  − μi  − CiTCi(p − μi)||22,  (6)  i. [sent-104, score-0.44]
</p><p>53 the squared distance between a point and the backprojection of the closest vector on the chart ci. [sent-106, score-0.315]
</p><p>54 Given a set of points Pi assigned to chart ci, the o? [sent-107, score-0.299]
</p><p>55 In practice, both the subspaces corresponding to charts and the assignment of points to charts are initially unknown, so we bootstrap our solution by estimating an excess of possible subspaces, initialised by performing PCA on random samples as described above. [sent-113, score-1.034]
</p><p>56 Then, we perform a hill climbing approach that alternates between assigning points to charts, and refitting the subspaces to minimise the reconstruction error. [sent-114, score-0.339]
</p><p>57 Manifold unwrapping for visualisation Unlike existing approaches to manifold learning, our method has no requirement to unwrap the manifold and after characterising it as an atlas, as in the previous section, we can immediately perform classification (Sec. [sent-116, score-0.909]
</p><p>58 However, one of the principal uses of manifold learning [18, 20, 26] is in creating a mapping from a high dimensional space into R2 or R3 suitable for visualising data. [sent-119, score-0.416]
</p><p>59 As a way of illustrating our method’s robustness to sparse data, the presence of noise, and to systematic holes, we illustrate unwrapping on standard datasets. [sent-120, score-0.122]
</p><p>60 Unwrapping a Manifold Top row: (a) The original data; (b) the unwrapping generated by Atlas; (c) the charts found by Atlas; (d) the charts unwrapped. [sent-123, score-0.952]
</p><p>61 As neither visualisation nor unwrapping is a primary focus of the paper, we defer the precise form of the objective used to the supplementary materials and focus on results. [sent-133, score-0.148]
</p><p>62 Our minimal assumption that neighbouring points in the k-NN graph should belong to the same subspace is robust to the presence of noise. [sent-141, score-0.122]
</p><p>63 The failure of the Gaussian reconstruction is more representative of the general problems faced by any unwrapping method. [sent-142, score-0.229]
</p><p>64 to be expected occasionally, for all nearest neighbours graph based methods, the local degeneracy spreads throughout the manifold and leads to a mapping in which the manifold is folded over itself. [sent-145, score-0.869]
</p><p>65 In the classification and reconstruction problems we discuss next, our method is used without unwrapping and such failures can never propagate throughout the manifold, and local mistakes remain local. [sent-146, score-0.238]
</p><p>66 Nearest neighbour classification Manifold learning classification is typically done by un-  wrapping the manifold and performing 1-NN on the result. [sent-148, score-0.52]
</p><p>67 In our approach, we simply perform nearest-neighbour in each chart independently. [sent-149, score-0.239]
</p><p>68 During the manifold learning, each point p is assigned by α-expansion to the interior of a single chart, and we assign a label to p simply by performing nearest neighbour in this chart. [sent-150, score-0.592]
</p><p>69 It is an empirical question whether we should include all points assigned to the chart when performing nearest neighbour or just those that belong to the interior of the chart. [sent-151, score-0.496]
</p><p>70 The solution found provides local neighbourhood sizes, and dimensionality d. [sent-159, score-0.172]
</p><p>71 111666444533  ×  (coloured straight lines) each of which approximates a local region of the embedding space (coloured curve) as an affine subspace. [sent-162, score-0.139]
</p><p>72 Interior points are mapped into the charts using the associated projection,  and nearest neighbour look up is performed within each chart. [sent-163, score-0.569]
</p><p>73 ’ has a nearest labelled neighbour of ‘1 ’ in the chart space, rather than ‘2 ’ in the original space. [sent-165, score-0.388]
</p><p>74 Table 1(a) shows 1-NN based face recognition in the original space and in lower-dimensional spaces of dimensionality 8 through 10 learnt with SMCE [6] while varying λ, and PCA, Atlas, and Atlas+ keeping their parameters fixed. [sent-194, score-0.146]
</p><p>75 The neighbourhood size is tuned in [2, 20] for LLE and LTSA, while the parameters for Atlas and Atlas+ are fixed. [sent-197, score-0.133]
</p><p>76 (83%5t1l2a9)s#4 c k ∈ [2, 50] and report the lowest error for each choice of mka ∈nif [o2l,d5 dimensionality de. [sent-284, score-0.126]
</p><p>77 oInw contrast, ffoorr our hm cehthooicde we fix our parameters to k = 6 and λ = 100 as we vary the local manifold dimensionality d. [sent-285, score-0.408]
</p><p>78 The additional pairwise regularisation of Atlas+ does not help on this problem, with the best results for Atlas+ occurring as the pairwise regulariser θ → 0, at which point Acutrlarisn+g a ansd hAetl apsa are equivalent. [sent-288, score-0.137]
</p><p>79 Having learnt a manifold on 3D mocap data from one person using Atlas, we estimate the 3D pose of a new person directly from a test 6CMU Motion Capture Database http : / /mocap . [sent-294, score-0.399]
</p><p>80 Assuming an orthographic camera of known orientation, both the affine subspaces of the manifold and the backprojection of points into 3D form hyper-planes. [sent-315, score-0.592]
</p><p>81 The problem of 3D reconstruction from 2D image data can then be posed as out-of-sample reconstruction, or finding the point lying on any of the affine subspaces that is closest to the back-projected hyper-plane. [sent-316, score-0.34]
</p><p>82 The closest point on the  manifold subspaces is picked as the 3D reconstruction. [sent-320, score-0.499]
</p><p>83 This technique cannot be directly applied to standard manifold learning methods, such as LLE and LTSA, which do not provide an explicit embedding of the manifold in the original space. [sent-321, score-0.797]
</p><p>84 For these methods, we learn the motion manifold using the 2D joint locations of all training and test sequences, then reconstruct by finding the nearest neigh7This is fast, as typically, less than 10 subspaces are used to characterise a gait cycle. [sent-322, score-0.587]
</p><p>85 The reconstructions of 21-NN and Atlas have aver-  age RMS reconstruction error (? [sent-328, score-0.129]
</p><p>86 For evaluation we selected two subsets of the CMU mocap database; dataset Icontaining walking sequences and dataset II containing both walking and running sequences. [sent-335, score-0.229]
</p><p>87 Dataset Iuses as training set 4 walking sequences of one subject and as testing set 8 sequences of two different subjects. [sent-336, score-0.134]
</p><p>88 Dataset II has a training set of 4 walking sequences of one subject and 4 running sequences of another subject, testing contains 16 sequences of two different subjects, 4 walking and 4 running from each. [sent-337, score-0.301]
</p><p>89 Each walking or running sequence includes about 4 cycles of motion. [sent-340, score-0.121]
</p><p>90 The error shown in all cases is the root mean squared 3D reconstruction error (RMS) in cm, averaged over all mark-  ers and frames in the testing set. [sent-346, score-0.172]
</p><p>91 7 shows comparative results of the 3D reconstruction error for PCA, k-NN, and Atlas when the scene is  Figure 7. [sent-354, score-0.129]
</p><p>92 CMU average RMS reconstruction error for manifold dimensionality 1 to 20. [sent-355, score-0.537]
</p><p>93 Top row: dataset I walking sequences and bottom row: dataset IIwalking and running sequences. [sent-356, score-0.134]
</p><p>94 6 shows the reconstruction results of a running sequence with 3cm Gaussian noise. [sent-362, score-0.119]
</p><p>95 We observed experimentally that k-NN on the manifold spaces learnt by LLE and LTSA had consistently poorer performance than k-NN in the original space and their results are omitted. [sent-365, score-0.434]
</p><p>96 Conclusion Starting from the formal definition of a manifold, as an atlas of overlapping charts, we have presented a novel approach to manifold learning that directly characterises the manifold in the original embedding space. [sent-367, score-1.346]
</p><p>97 In particular, unlike existing methods, it can potentially learn any form of manifold rather than being restricted to manifolds that can be expressed by a single chart. [sent-369, score-0.417]
</p><p>98 We show a substantial boost in performance classification  and robustness to the choice of both manifold dimensionality and neighbourhood graph. [sent-370, score-0.573]
</p><p>99 Unlike existing approaches, our characterisation of the manifold directly in the embedding space allows us to recover previously unseen and partially missing data easily, and we have shown applications in the reconstruction of hu111666444866  Table 3. [sent-371, score-0.493]
</p><p>100 Acquiring linear subspaces for face recognition under variable lighting. [sent-549, score-0.123]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('atlas', 0.465), ('charts', 0.405), ('manifold', 0.348), ('ltsa', 0.314), ('chart', 0.239), ('lle', 0.209), ('smce', 0.17), ('wltsa', 0.14), ('unwrapping', 0.122), ('neighbourhood', 0.112), ('subspaces', 0.102), ('semeion', 0.093), ('neighbour', 0.09), ('reconstruction', 0.086), ('neighbours', 0.083), ('affine', 0.08), ('pca', 0.08), ('regularisation', 0.077), ('yale', 0.074), ('interior', 0.068), ('walking', 0.068), ('closed', 0.064), ('dimensionality', 0.06), ('embedding', 0.059), ('characterise', 0.057), ('refitting', 0.052), ('ci', 0.05), ('overlapping', 0.049), ('nn', 0.049), ('manifolds', 0.049), ('assignment', 0.044), ('excess', 0.043), ('error', 0.043), ('fayad', 0.041), ('gait', 0.041), ('isomorphic', 0.041), ('cmu', 0.04), ('rms', 0.039), ('misclassified', 0.039), ('nearest', 0.039), ('optimisation', 0.038), ('subspace', 0.037), ('folds', 0.036), ('points', 0.035), ('characterises', 0.035), ('unwrap', 0.035), ('running', 0.033), ('sequences', 0.033), ('russell', 0.032), ('clust', 0.031), ('graph', 0.031), ('eigenmaps', 0.03), ('classification', 0.03), ('minimises', 0.029), ('saul', 0.029), ('uncorrected', 0.029), ('rd', 0.029), ('noise', 0.028), ('backprojection', 0.027), ('contraction', 0.027), ('mocap', 0.027), ('hessian', 0.027), ('closest', 0.027), ('yp', 0.026), ('ic', 0.026), ('dimensional', 0.026), ('coloured', 0.026), ('visualisation', 0.026), ('mdl', 0.026), ('yq', 0.026), ('degenerate', 0.026), ('assigned', 0.025), ('swiss', 0.025), ('learnt', 0.024), ('minimise', 0.024), ('lying', 0.023), ('collapse', 0.023), ('roll', 0.023), ('choice', 0.023), ('learning', 0.022), ('point', 0.022), ('preserve', 0.022), ('consistently', 0.021), ('tail', 0.021), ('spaces', 0.021), ('face', 0.021), ('tuned', 0.021), ('digits', 0.021), ('faced', 0.021), ('np', 0.021), ('alternates', 0.02), ('assigning', 0.02), ('cycles', 0.02), ('mapping', 0.02), ('original', 0.02), ('unlike', 0.02), ('classified', 0.02), ('pairwise', 0.019), ('neighbouring', 0.019), ('preserves', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="259-tfidf-1" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>Author: Nikolaos Pitelis, Chris Russell, Lourdes Agapito</p><p>Abstract: In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image.</p><p>2 0.19138904 <a title="259-tfidf-2" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>Author: Lili Tao, Bogdan J. Matuszewski</p><p>Abstract: In this paper, a novel approach based on a non-linear manifold learning technique is proposed to recover 3D nonrigid structures from 2D image sequences captured by a single camera. Most ofthe existing approaches assume that 3D shapes can be accurately modelled in a linear subspace. These techniques perform well when the deformations are relatively small or simple, but fail when more complex deformations need to be recovered. The non-linear deformations are often observed in highly flexible objects for which the use of the linear model is impractical. A specific type of shape variations might be governed by only a small number of parameters, therefore can be wellrepresented in a low dimensional manifold. We learn a nonlinear shape prior using diffusion maps method. The key contribution in this paper is the introduction of the shape prior that constrain the reconstructed shapes to lie in the learned manifold. The proposed methodology has been validated quantitatively and qualitatively on 2D points sequences projected from the 3D motion capture data and real 2D video sequences. The comparisons oftheproposed man- ifold based method against several state-of-the-art techniques are shown on different types of deformable objects.</p><p>3 0.18005885 <a title="259-tfidf-3" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>Author: Raviteja Vemulapalli, Jaishanker K. Pillai, Rama Chellappa</p><p>Abstract: In computer vision applications, features often lie on Riemannian manifolds with known geometry. Popular learning algorithms such as discriminant analysis, partial least squares, support vector machines, etc., are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. However, for kernel based approaches, poor choice of kernel often results in reduced performance. In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.</p><p>4 0.16984154 <a title="259-tfidf-4" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>Author: Bo Wang, Zhuowen Tu</p><p>Abstract: With the increasing availability of high dimensional data and demand in sophisticated data analysis algorithms, manifold learning becomes a critical technique to perform dimensionality reduction, unraveling the intrinsic data structure. The real-world data however often come with noises and outliers; seldom, all the data live in a single linear subspace. Inspired by the recent advances in sparse subspace learning and diffusion-based approaches, we propose a new manifold denoising algorithm in which data neighborhoods are adaptively inferred via sparse subspace reconstruction; we then derive a new formulation to perform denoising to the original data. Experiments carried out on both toy and real applications demonstrate the effectiveness of our method; it is insensitive to parameter tuning and we show significant improvement over the competing algorithms.</p><p>5 0.14233468 <a title="259-tfidf-5" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>Author: Shaokang Chen, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell</p><p>Abstract: Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once, with fixed clusters used for matching with other image sets. However, this may result in the two closest clusters to represent different characteristics of an object, due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem, we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace, we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error, we approach the nearest point on a Grassmann manifold. Experiments on Honda, ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques, such as Affine Hull based Image Set Distance (AHISD), Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).</p><p>6 0.14178823 <a title="259-tfidf-6" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>7 0.12741423 <a title="259-tfidf-7" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>8 0.12408019 <a title="259-tfidf-8" href="./cvpr-2013-MKPLS%3A_Manifold_Kernel_Partial_Least_Squares_for_Lipreading_and_Speaker_Identification.html">276 cvpr-2013-MKPLS: Manifold Kernel Partial Least Squares for Lipreading and Speaker Identification</a></p>
<p>9 0.10956959 <a title="259-tfidf-9" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>10 0.10915619 <a title="259-tfidf-10" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>11 0.10508125 <a title="259-tfidf-11" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>12 0.10246886 <a title="259-tfidf-12" href="./cvpr-2013-Kernel_Methods_on_the_Riemannian_Manifold_of_Symmetric_Positive_Definite_Matrices.html">238 cvpr-2013-Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices</a></p>
<p>13 0.09201321 <a title="259-tfidf-13" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>14 0.087311581 <a title="259-tfidf-14" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>15 0.074451342 <a title="259-tfidf-15" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>16 0.07187093 <a title="259-tfidf-16" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>17 0.070390821 <a title="259-tfidf-17" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>18 0.070223115 <a title="259-tfidf-18" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>19 0.067037016 <a title="259-tfidf-19" href="./cvpr-2013-Graph-Laplacian_PCA%3A_Closed-Form_Solution_and_Robustness.html">191 cvpr-2013-Graph-Laplacian PCA: Closed-Form Solution and Robustness</a></p>
<p>20 0.065196365 <a title="259-tfidf-20" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, 0.018), (2, -0.044), (3, 0.037), (4, 0.007), (5, -0.029), (6, -0.044), (7, -0.144), (8, -0.035), (9, -0.075), (10, 0.026), (11, 0.001), (12, -0.143), (13, -0.095), (14, -0.035), (15, 0.023), (16, -0.145), (17, 0.007), (18, -0.17), (19, 0.04), (20, 0.082), (21, 0.077), (22, 0.091), (23, 0.055), (24, -0.047), (25, -0.027), (26, 0.015), (27, 0.036), (28, -0.021), (29, -0.001), (30, -0.029), (31, -0.078), (32, -0.008), (33, 0.031), (34, -0.025), (35, -0.025), (36, 0.061), (37, 0.037), (38, 0.006), (39, -0.002), (40, 0.018), (41, -0.021), (42, -0.042), (43, -0.036), (44, -0.027), (45, -0.015), (46, -0.067), (47, -0.032), (48, 0.05), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94401187 <a title="259-lsi-1" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>Author: Nikolaos Pitelis, Chris Russell, Lourdes Agapito</p><p>Abstract: In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image.</p><p>2 0.88365734 <a title="259-lsi-2" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>Author: Rui Caseiro, Pedro Martins, João F. Henriques, Fátima Silva Leite, Jorge Batista</p><p>Abstract: In the past few years there has been a growing interest on geometric frameworks to learn supervised classification models on Riemannian manifolds [31, 27]. A popular framework, valid over any Riemannian manifold, was proposed in [31] for binary classification. Once moving from binary to multi-class classification thisparadigm is not valid anymore, due to the spread of multiple positive classes on the manifold [27]. It is then natural to ask whether the multi-class paradigm could be extended to operate on a large class of Riemannian manifolds. We propose a mathematically well-founded classification paradigm that allows to extend the work in [31] to multi-class models, taking into account the structure of the space. The idea is to project all the data from the manifold onto an affine tangent space at a particular point. To mitigate the distortion induced by local diffeomorphisms, we introduce for the first time in the computer vision community a well-founded mathematical concept, so-called Rolling map [21, 16]. The novelty in this alternate school of thought is that the manifold will be firstly rolled (without slipping or twisting) as a rigid body, then the given data is unwrapped onto the affine tangent space, where the classification is performed.</p><p>3 0.84849799 <a title="259-lsi-3" href="./cvpr-2013-MKPLS%3A_Manifold_Kernel_Partial_Least_Squares_for_Lipreading_and_Speaker_Identification.html">276 cvpr-2013-MKPLS: Manifold Kernel Partial Least Squares for Lipreading and Speaker Identification</a></p>
<p>Author: Amr Bakry, Ahmed Elgammal</p><p>Abstract: Visual speech recognition is a challenging problem, due to confusion between visual speech features. The speaker identification problem is usually coupled with speech recognition. Moreover, speaker identification is important to several applications, such as automatic access control, biometrics, authentication, and personal privacy issues. In this paper, we propose a novel approach for lipreading and speaker identification. Wepropose a new approachfor manifold parameterization in a low-dimensional latent space, where each manifold is represented as a point in that space. We initially parameterize each instance manifold using a nonlinear mapping from a unified manifold representation. We then factorize the parameter space using Kernel Partial Least Squares (KPLS) to achieve a low-dimension manifold latent space. We use two-way projections to achieve two manifold latent spaces, one for the speech content and one for the speaker. We apply our approach on two public databases: AVLetters and OuluVS. We show the results for three different settings of lipreading: speaker independent, speaker dependent, and speaker semi-dependent. Our approach outperforms for the speaker semi-dependent setting by at least 15% of the baseline, and competes in the other two settings.</p><p>4 0.76930654 <a title="259-lsi-4" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>Author: Raviteja Vemulapalli, Jaishanker K. Pillai, Rama Chellappa</p><p>Abstract: In computer vision applications, features often lie on Riemannian manifolds with known geometry. Popular learning algorithms such as discriminant analysis, partial least squares, support vector machines, etc., are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. However, for kernel based approaches, poor choice of kernel often results in reduced performance. In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.</p><p>5 0.74242175 <a title="259-lsi-5" href="./cvpr-2013-Kernel_Methods_on_the_Riemannian_Manifold_of_Symmetric_Positive_Definite_Matrices.html">238 cvpr-2013-Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices</a></p>
<p>Author: Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi</p><p>Abstract: Symmetric Positive Definite (SPD) matrices have become popular to encode image information. Accounting for the geometry of the Riemannian manifold of SPD matrices has proven key to the success of many algorithms. However, most existing methods only approximate the true shape of the manifold locally by its tangent plane. In this paper, inspired by kernel methods, we propose to map SPD matrices to a high dimensional Hilbert space where Euclidean geometry applies. To encode the geometry of the manifold in the mapping, we introduce a family of provably positive definite kernels on the Riemannian manifold of SPD matrices. These kernels are derived from the Gaussian kernel, but exploit different metrics on the manifold. This lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of SPD matrices. We demonstrate the benefits of our approach on the problems of pedestrian detection, object categorization, texture analysis, 2D motion segmentation and Diffusion Tensor Imaging (DTI) segmentation.</p><p>6 0.7095722 <a title="259-lsi-6" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>7 0.67545533 <a title="259-lsi-7" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>8 0.66515058 <a title="259-lsi-8" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>9 0.62906957 <a title="259-lsi-9" href="./cvpr-2013-Graph-Laplacian_PCA%3A_Closed-Form_Solution_and_Robustness.html">191 cvpr-2013-Graph-Laplacian PCA: Closed-Form Solution and Robustness</a></p>
<p>10 0.61829185 <a title="259-lsi-10" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>11 0.48304695 <a title="259-lsi-11" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>12 0.48303366 <a title="259-lsi-12" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>13 0.47080317 <a title="259-lsi-13" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>14 0.47004226 <a title="259-lsi-14" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>15 0.45949915 <a title="259-lsi-15" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>16 0.44409251 <a title="259-lsi-16" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>17 0.4433575 <a title="259-lsi-17" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>18 0.43532991 <a title="259-lsi-18" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>19 0.43151739 <a title="259-lsi-19" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>20 0.42926732 <a title="259-lsi-20" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.077), (16, 0.025), (26, 0.046), (28, 0.015), (33, 0.277), (57, 0.011), (67, 0.045), (69, 0.062), (80, 0.016), (87, 0.06), (91, 0.275)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82262057 <a title="259-lda-1" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>Author: Nikolaos Pitelis, Chris Russell, Lourdes Agapito</p><p>Abstract: In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image.</p><p>2 0.80411315 <a title="259-lda-2" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>3 0.79316974 <a title="259-lda-3" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>4 0.78280973 <a title="259-lda-4" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>5 0.76024067 <a title="259-lda-5" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>6 0.75821364 <a title="259-lda-6" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>7 0.73865128 <a title="259-lda-7" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>8 0.73827493 <a title="259-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.73807448 <a title="259-lda-9" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>10 0.73743314 <a title="259-lda-10" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>11 0.73686671 <a title="259-lda-11" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>12 0.7366389 <a title="259-lda-12" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>13 0.73641163 <a title="259-lda-13" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>14 0.73614156 <a title="259-lda-14" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>15 0.73606217 <a title="259-lda-15" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>16 0.73604161 <a title="259-lda-16" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>17 0.73598576 <a title="259-lda-17" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>18 0.73597616 <a title="259-lda-18" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>19 0.73589486 <a title="259-lda-19" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>20 0.73572588 <a title="259-lda-20" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
