<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-199" href="#">cvpr2013-199</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</h1>
<br/><p>Source: <a title="cvpr-2013-199-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yu_Harry_Potters_Marauders_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Shoou-I Yu, Yi Yang, Alexander Hauptmann</p><p>Abstract: A device just like Harry Potter’s Marauder’s Map, which pinpoints the location ofeachperson-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust lo- calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.</p><p>Reference: <a title="cvpr-2013-199-reference" href="../cvpr2013_reference/cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. [sent-2, score-1.203]
</p><p>2 We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. [sent-3, score-0.478]
</p><p>3 Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. [sent-4, score-0.819]
</p><p>4 Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. [sent-5, score-0.274]
</p><p>5 Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. [sent-6, score-0.718]
</p><p>6 Experiments show that our algorithm performs robust lo-  calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment. [sent-7, score-0.676]
</p><p>7 If we are able to localize and track each person as shown in Figure 1, action recognition of people can be subsequently performed to analyze human behavior. [sent-10, score-0.361]
</p><p>8 To perform reliable localization and tracking, important cues such as color, person detection, face recognition and non-background detection should all be utilized. [sent-11, score-0.679]
</p><p>9 Also, the tracking algorithm has to deal with typical yet complex indoor scenes consisting of different rooms, many walls and corridors. [sent-12, score-0.529]
</p><p>10 information in a seamless way to perform reliable localization and tracking of persons-of-interest in complex indoor environments. [sent-19, score-0.574]
</p><p>11 There are several cues available for trackers to utilize, such as color, person detection, face recognition and nonbackground detection. [sent-20, score-0.637]
</p><p>12 However, many existing tracking systems [3, 11, 1] only use a subset of cues to perform tracking. [sent-22, score-0.408]
</p><p>13 Therefore, we propose an localization and tracking algorithm which  utilizes color, person detection, face recognition and nonbackground detection cues to perform robust tracking. [sent-24, score-1.05]
</p><p>14 Our algorithm follows the tracking by detection paradigm [14, 1], which can handle re-initializations naturally and avoids excessive model drift. [sent-27, score-0.384]
</p><p>15 This paradigm is also less affected by occlusions caused by walls or sparse camera setups. [sent-28, score-0.229]
</p><p>16 If we treat one personof-interest as one class, the tracker needs to assign a class label to each person detection result. [sent-30, score-0.43]
</p><p>17 Sparse label information can be acquired from face recognition output and manually annotated start and end locations of a person. [sent-31, score-0.25]
</p><p>18 However, labels for most person detection results are unknown. [sent-32, score-0.297]
</p><p>19 Given two person detection data points which are spatialtemporal neighbors, if they have similar appearances, it is very likely that the two points correspond to the same person. [sent-34, score-0.413]
</p><p>20 As this is a good fit to the manifold assumption, we propose to uncover the manifold structure of detected data points by leveraging local learning techniques. [sent-35, score-0.555]
</p><p>21 Inspired by [19], instead of directly computing the affinity matrix according to the features of data points, we adopt a statistical approach to exploit the manifold structure, which is more  accurate and robust. [sent-36, score-0.223]
</p><p>22 Simply satisfying the manifold assumption is not sufficient for reliable tracking. [sent-37, score-0.304]
</p><p>23 The mutual exclusion constraint, which constrains one person detection result to be associated with only one person, should also be embedded into the tracker. [sent-38, score-0.542]
</p><p>24 We perform nonnegative discretization, which partitions the detected data points into non-overlapping groups such that mutual exclusion and the manifold assumption are satisfied simultaneously. [sent-39, score-0.943]
</p><p>25 We formulate the problem as a nonnegative optimization problem. [sent-40, score-0.302]
</p><p>26 We propose a novel method which performs robust multiple person-of-interest localization and tracking by incorporating color, non-background, person detection and face recognition in a semi-supervised learning framework. [sent-43, score-0.832]
</p><p>27 We perform experiments on a real-world complex indoor data set with long corridors and many walls causing occlusion. [sent-45, score-0.378]
</p><p>28 To the best of our knowledge, this is the first work that has applied multi-camera multi-object tracking in such a complex indoor environment. [sent-46, score-0.445]
</p><p>29 After a brief review of multi-object tracking and semi-supervised tracking in Section 2, we will detail our algorithm in Section 3. [sent-48, score-0.568]
</p><p>30 Related Work The Marauder’s Map can be viewed as a multi-camera multi-object tracking problem. [sent-51, score-0.284]
</p><p>31 Multi-object tracking has been an active field of research for the past 10 years. [sent-52, score-0.284]
</p><p>32 Earlier work [14] models the multi-modal posterior with a set of samples and uses a color-based particle filter combined with an object detector to perform tracking. [sent-53, score-0.214]
</p><p>33 [13] uses Kalman filters to perform multi-person tracking with 16 cameras. [sent-54, score-0.324]
</p><p>34 Recent work discretizes the solution space of tracking and uses background and non-background information to locate potential objects to track for each frame. [sent-55, score-0.33]
</p><p>35 For example, trackers not using color information [3, 1] will have difficulty avoiding identity switches when multiple people come very close together and split up. [sent-60, score-0.295]
</p><p>36 It is unclear whether the performance of previous methods translates into complex indoor environments with sparse camera setups. [sent-64, score-0.222]
</p><p>37 Recently, there have been papers focusing on leveraging semi-supervised learning to improve monocular tracking performance. [sent-65, score-0.284]
</p><p>38 [9] proposes an online boosting semisupervised framework to find features that can effectively separate the tracking target from the background. [sent-66, score-0.372]
</p><p>39 Methodology Following the tracking by detection paradigm [14], the input to our algorithm is a set of person detection results at each time instant. [sent-72, score-0.681]
</p><p>40 The person detection results from different camera views can be mapped to a common 3D co-  ordinate system using camera calibration and ground plane parameters provided. [sent-73, score-0.419]
</p><p>41 Each person detection result is described by the color histogram of the person detection result. [sent-74, score-0.706]
</p><p>42 Our algorithm’s main task is to predict a label for each person detection result. [sent-75, score-0.34]
</p><p>43 To perform the prediction 333777111533  task, our algorithm incorporates two main innovative components, which are manifold learning in appearance space with spatio-temporal constraints, and trajectory inference by nonnegative discretization. [sent-76, score-0.751]
</p><p>44 Notations Hereafter, we call a person detection result as a data point. [sent-80, score-0.297]
</p><p>45 Suppose there are n data points generated by the person detector. [sent-81, score-0.287]
</p><p>46 Manifold Learning in Appearance Space with Spatio-Temporal Constraints The appearance of a person within a short period of time should not change much. [sent-142, score-0.229]
</p><p>47 As this is a good fit to the manifold assumption, we follow the method used in [19] to learn the manifold structure. [sent-144, score-0.446]
</p><p>48 Nearest neighbor selection is a crucial step in learning manifold structure. [sent-145, score-0.223]
</p><p>49 Therefore, in the following paragraphs, we will first detail the method we used for nearest neighbor selection and then describe how this information is utilized in manifold learning. [sent-146, score-0.223]
</p><p>50 Therefore, the nearest color histogram neighbors of the i-th data point in the spatio-temporal neighbor set Si should correspond tno dhaet as points mfropmor athle n same oinrd sievtid Sual. [sent-158, score-0.229]
</p><p>51 Occlusions may cause the tracking target to be partially or completely occluded. [sent-164, score-0.361]
</p><p>52 However, the tracking target usually reappears after a few frames. [sent-165, score-0.327]
</p><p>53 Instead of trying to explicitly model occlusions, we connect the observations of the tracking target before and after occlusion. [sent-166, score-0.327]
</p><p>54 Following the manifold assumption, we assume that the class labels of the i-th data point and its neighbors can be predicted by a local function gi (·). [sent-171, score-0.317]
</p><p>55 Also, all the elements in F is nonnegative by definition, as defined by Equation 1. [sent-222, score-0.302]
</p><p>56 Furthermore, according to [20], if both the orthogo-  nal constraint and nonnegative constraint are satisfied for a matrix, then there will only be at most one nonzero element in each row of the matrix. [sent-223, score-0.338]
</p><p>57 If there is at most one nonzero element per row in F, this means that each data point belongs to at most one class, which is exactly the mutual exclusion constraint. [sent-224, score-0.245]
</p><p>58 Thus we relax the form of F and only keep the orthogonal and nonnegative constraint. [sent-225, score-0.302]
</p><p>59 In other words, we exploit the orthogonal and nonnegative constraints to perform discretization of F. [sent-226, score-0.518]
</p><p>60 Therefore, we propose to minimize Equation 8 to satisfy mutual exclusion and the manifold assumption simultaneously. [sent-227, score-0.507]
</p><p>61 By incorporating face recognition, we can prevent the tracker from  losing track of the person-of-interest. [sent-266, score-0.305]
</p><p>62 333777111755  Given the predicted label information for each person detection result, the final step is to link together the detections of each individual and infer the trajectory for each individual. [sent-267, score-0.515]
</p><p>63 It is worth noting that our formulation can naturally handle template updates, which is crucial because the color of the tracking target can change gradually from time to time. [sent-270, score-0.404]
</p><p>64 The templates are implicitly encoded in the manifold structure we learn through semi-supervised learning. [sent-271, score-0.223]
</p><p>65 The competing algorithms are [3] and a self implemented 3D color particle filter (CPF) similar to [15]. [sent-276, score-0.251]
</p><p>66 a particle filter starts tracking another particle filter’s object. [sent-284, score-0.585]
</p><p>67 For our method, we used the person detection result from [6, 8]. [sent-291, score-0.297]
</p><p>68 To describe the person detection result, we used the same kind of color histograms as in CPF. [sent-292, score-0.374]
</p><p>69 We used the output of the Probabilistic Occupancy Map [7] to filter out all person detection points situated at locations which were deemed to not contain any non-background object. [sent-293, score-0.402]
</p><p>70 We ran face detection and recognition using the PittPatt software1 . [sent-295, score-0.237]
</p><p>71 Following the evaluation method used in [1], association between tracking results and ground truth are computed in 3D with a hit/miss threshold of 1 meter. [sent-303, score-0.284]
</p><p>72 Given the true positive associations, MOTP is the average 3D distance between the ground truth and tracking output. [sent-305, score-0.284]
</p><p>73 There are many occlusions caused by walls which are typical in indoor scenes. [sent-316, score-0.259]
</p><p>74 There is also no single camera which has a global view of the whole environment, which is typical in many surveillance  camera setups, but atypical in the data sets that have been used to perform multi-camera tracking. [sent-318, score-0.264]
</p><p>75 Furthermore, the data set records activities in a nursing home, where people were focused on their daily tasks and not on the surveillance cameras, which makes the data set a very good representation of situations that may occur in real life. [sent-319, score-0.291]
</p><p>76 Patients and staff are all persons-ofinterest, because staff at nursing homes interact frequently with the patients, and it is valuable information if we are able to localize and track staff as well. [sent-323, score-0.433]
</p><p>77 For each trajectory, the ground truth bounding boxes of each tracking target in the 15 cameras are manually labeled at one second intervals. [sent-325, score-0.415]
</p><p>78 The longest trajectory in the video is 6 minutes 17 seconds, and the shortest trajectory is 4 seconds. [sent-326, score-0.278]
</p><p>79 Faces can be extracted and recognized if a person is close enough to the camera. [sent-329, score-0.229]
</p><p>80 No face recognition information is available on this sequence, so we ran our method without face recognition (Ours-NF). [sent-361, score-0.338]
</p><p>81 To understand the impact of face recognition, we run our tracker without face recognition (Ours-NF) and with face recognition (Ours-F). [sent-366, score-0.562]
</p><p>82 Our tracker, which utilizes all available information, can still achieve reasonable tracking performance. [sent-367, score-0.32]
</p><p>83 Snapshots of our algorithm’s localization and tracking is shown in Figure 1 and 4. [sent-368, score-0.331]
</p><p>84 For [3], surveillance camera setups with nearparallel viewing angles of cameras can cause the Probabilistic Occupancy Map (POM) [7] to be inaccurate. [sent-373, score-0.337]
</p><p>85 Snapshots oflocalization and tracking results from Caremedia data set. [sent-376, score-0.284]
</p><p>86 The first reason is that the cameras are not color calibrated, which makes cross-camera tracking difficult because the same person may have slightly different color appearance when viewed from different cameras. [sent-382, score-0.755]
</p><p>87 The second problem is that the CPF does not have the mutual exclusion constraint, and multiple particle filters may start tracking the same object. [sent-383, score-0.694]
</p><p>88 First, manifold learning coupled with nonnegative discretization is effective in enhancing tracking performance. [sent-385, score-0.985]
</p><p>89 Our algorithm utilizes the manifold assumption to deal with slight color differences of the tracking target at different times. [sent-386, score-0.702]
</p><p>90 Second, we utilize PittPatt face recognition, which is a very reliable source of label information. [sent-388, score-0.255]
</p><p>91 Face recognition provides more labels for each tracking target. [sent-389, score-0.319]
</p><p>92 Face recognition also helps overcome the color-mismatch problem the CPF faces, because if a face is detected in a given camera, then the color 333777111977  histogram of the person for the camera is known. [sent-390, score-0.571]
</p><p>93 Without faces, manifold learning with nonnegative discretization already achieves scores which are already better than our baselines on both data sets. [sent-391, score-0.701]
</p><p>94 In sum, experiments show that by incorporating all available cues and exploiting the manifold assumption with nonnegative discretization, we can achieve substantial improvements in performance. [sent-395, score-0.683]
</p><p>95 Therefore, our algorithm is not effective in very crowded sequences where each person wears the same color clothes, such as the laboratory sequence from [3]. [sent-400, score-0.306]
</p><p>96 Conclusions We propose a novel semi-supervised learning framework with nonnegative discretization to incorporate all available  cues to perform robust person-of-interest localization and tracking in complex indoor environments. [sent-405, score-1.094]
</p><p>97 Available cues such as color, person detection, face recognition and nonbackground information are all utilized in the manifold learning process. [sent-406, score-0.798]
</p><p>98 The nonnegative discretization groups the data points into non-overlapping groups such that mutual exclusion and manifold assumption are satisfied simultaneously. [sent-407, score-1.079]
</p><p>99 Our algorithm is effective because of reliable face recognition and the combination of manifold learning with nonnegative discretization. [sent-409, score-0.736]
</p><p>100 Evaluating multiple object tracking performance: the clear mot metrics. [sent-434, score-0.284]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nonnegative', 0.302), ('tracking', 0.284), ('person', 0.229), ('manifold', 0.223), ('caremedia', 0.218), ('discretization', 0.176), ('ftf', 0.166), ('exclusion', 0.16), ('marauder', 0.156), ('trajectory', 0.139), ('cpf', 0.138), ('nursing', 0.138), ('face', 0.134), ('particle', 0.127), ('mfintr', 0.125), ('indoor', 0.123), ('pets', 0.117), ('ftlf', 0.111), ('surveillance', 0.102), ('corridors', 0.093), ('harry', 0.093), ('nonbackground', 0.093), ('potter', 0.093), ('tracker', 0.09), ('corridor', 0.088), ('mota', 0.088), ('cameras', 0.088), ('mutual', 0.085), ('walls', 0.084), ('cues', 0.084), ('staff', 0.083), ('trajectories', 0.078), ('color', 0.077), ('yij', 0.069), ('detection', 0.068), ('switches', 0.064), ('pittpatt', 0.062), ('trackers', 0.062), ('patients', 0.062), ('camera', 0.061), ('neighbors', 0.059), ('points', 0.058), ('velocity', 0.056), ('uii', 0.055), ('paragraphs', 0.055), ('pom', 0.055), ('occupancy', 0.052), ('setups', 0.052), ('occlusions', 0.052), ('uncover', 0.051), ('si', 0.051), ('people', 0.051), ('equation', 0.05), ('home', 0.049), ('localization', 0.047), ('filter', 0.047), ('prediction', 0.047), ('track', 0.046), ('rhe', 0.046), ('invaluable', 0.046), ('fik', 0.046), ('motp', 0.046), ('semisupervised', 0.045), ('tpami', 0.045), ('freitas', 0.044), ('hauptmann', 0.044), ('outdoor', 0.044), ('label', 0.043), ('target', 0.043), ('wi', 0.043), ('reliable', 0.042), ('nj', 0.042), ('identity', 0.041), ('perform', 0.04), ('nie', 0.04), ('berclaz', 0.04), ('assumption', 0.039), ('start', 0.038), ('complex', 0.038), ('optima', 0.038), ('snapshots', 0.038), ('fij', 0.036), ('utilizes', 0.036), ('satisfied', 0.036), ('detections', 0.036), ('utilize', 0.036), ('incorporating', 0.035), ('rn', 0.035), ('seamlessly', 0.035), ('histogram', 0.035), ('recognition', 0.035), ('gi', 0.035), ('fleuret', 0.034), ('cause', 0.034), ('grabner', 0.034), ('pq', 0.034), ('faces', 0.033), ('ideal', 0.033), ('positives', 0.033), ('paradigm', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="199-tfidf-1" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>Author: Shoou-I Yu, Yi Yang, Alexander Hauptmann</p><p>Abstract: A device just like Harry Potter’s Marauder’s Map, which pinpoints the location ofeachperson-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust lo- calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.</p><p>2 0.30401477 <a title="199-tfidf-2" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>3 0.23555174 <a title="199-tfidf-3" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>4 0.23035346 <a title="199-tfidf-4" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>5 0.22764461 <a title="199-tfidf-5" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>6 0.20254326 <a title="199-tfidf-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.19443318 <a title="199-tfidf-7" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>8 0.17756276 <a title="199-tfidf-8" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>9 0.17283918 <a title="199-tfidf-9" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>10 0.16958517 <a title="199-tfidf-10" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>11 0.15615398 <a title="199-tfidf-11" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>12 0.14313687 <a title="199-tfidf-12" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>13 0.14284942 <a title="199-tfidf-13" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>14 0.14087027 <a title="199-tfidf-14" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>15 0.1394071 <a title="199-tfidf-15" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>16 0.12951863 <a title="199-tfidf-16" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>17 0.11851976 <a title="199-tfidf-17" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>18 0.11806339 <a title="199-tfidf-18" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>19 0.11737138 <a title="199-tfidf-19" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>20 0.11723139 <a title="199-tfidf-20" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.272), (1, 0.005), (2, -0.034), (3, -0.089), (4, 0.03), (5, -0.069), (6, 0.144), (7, -0.227), (8, 0.153), (9, 0.141), (10, 0.0), (11, -0.096), (12, -0.072), (13, 0.061), (14, -0.047), (15, 0.009), (16, -0.055), (17, 0.055), (18, -0.067), (19, 0.003), (20, 0.101), (21, 0.118), (22, -0.018), (23, 0.097), (24, -0.006), (25, -0.0), (26, 0.007), (27, 0.031), (28, -0.094), (29, -0.104), (30, -0.03), (31, -0.057), (32, 0.022), (33, 0.059), (34, 0.042), (35, -0.023), (36, -0.014), (37, 0.083), (38, 0.042), (39, -0.016), (40, 0.003), (41, -0.037), (42, 0.116), (43, -0.042), (44, 0.014), (45, -0.02), (46, -0.029), (47, -0.039), (48, 0.03), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95540804 <a title="199-lsi-1" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>Author: Shoou-I Yu, Yi Yang, Alexander Hauptmann</p><p>Abstract: A device just like Harry Potter’s Marauder’s Map, which pinpoints the location ofeachperson-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust lo- calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.</p><p>2 0.81543517 <a title="199-lsi-2" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>3 0.78306514 <a title="199-lsi-3" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>4 0.75474381 <a title="199-lsi-4" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>5 0.73662841 <a title="199-lsi-5" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>6 0.71481842 <a title="199-lsi-6" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>7 0.69447708 <a title="199-lsi-7" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>8 0.67805654 <a title="199-lsi-8" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>9 0.67681503 <a title="199-lsi-9" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>10 0.66440678 <a title="199-lsi-10" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>11 0.66247237 <a title="199-lsi-11" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>12 0.66222841 <a title="199-lsi-12" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>13 0.6361661 <a title="199-lsi-13" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>14 0.61744112 <a title="199-lsi-14" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>15 0.59728909 <a title="199-lsi-15" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>16 0.59460503 <a title="199-lsi-16" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>17 0.56114787 <a title="199-lsi-17" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>18 0.54367608 <a title="199-lsi-18" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>19 0.53932798 <a title="199-lsi-19" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>20 0.52001673 <a title="199-lsi-20" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.114), (16, 0.03), (26, 0.051), (33, 0.276), (54, 0.201), (67, 0.092), (69, 0.05), (87, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89178991 <a title="199-lda-1" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>Author: Srikumar Ramalingam, Jaishanker K. Pillai, Arpit Jain, Yuichi Taguchi</p><p>Abstract: Junctions are strong cues for understanding the geometry of a scene. In this paper, we consider the problem of detecting junctions and using them for recovering the spatial layout of an indoor scene. Junction detection has always been challenging due to missing and spurious lines. We work in a constrained Manhattan world setting where the junctions are formed by only line segments along the three principal orthogonal directions. Junctions can be classified into several categories based on the number and orientations of the incident line segments. We provide a simple and efficient voting scheme to detect and classify these junctions in real images. Indoor scenes are typically modeled as cuboids and we formulate the problem of the cuboid layout estimation as an inference problem in a conditional random field. Our formulation allows the incorporation of junction features and the training is done using structured prediction techniques. We outperform other single view geometry estimation methods on standard datasets.</p><p>same-paper 2 0.87893879 <a title="199-lda-2" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>Author: Shoou-I Yu, Yi Yang, Alexander Hauptmann</p><p>Abstract: A device just like Harry Potter’s Marauder’s Map, which pinpoints the location ofeachperson-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust lo- calization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.</p><p>3 0.86840439 <a title="199-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.86144888 <a title="199-lda-4" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>Author: Hidekata Hontani, Yuto Tsunekawa, Yoshihide Sawada</p><p>Abstract: In this paper, we propose a new non-rigid robust registration method that registers a point distribution model (PDM) of a surface to given 3D images. The contributions of the paper are (1) a new hierarchical statistical shape model (SSM) of the surface that has better generalization ability is introduced, (2) the registration algorithm of the hierarchical SSM that can estimate the marginal posterior distribution of the surface location is proposed, and (3) the registration performance is improved by (3-1) robustly registering each local shape of the surface with the sparsity regularization and by (3-2) referring to the appearance between the neighboring model points in the likelihood computation. The SSM of a liver was constructed from a set of clinical CT images, and the performance of the proposed method was evaluated. Experimental results demonstrated that the proposed method outperformed some existing methods that use non-hierarchical SSMs.</p><p>5 0.8471058 <a title="199-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.84176344 <a title="199-lda-6" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>7 0.84171933 <a title="199-lda-7" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>8 0.84063643 <a title="199-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.84024918 <a title="199-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.84023947 <a title="199-lda-10" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>11 0.83938581 <a title="199-lda-11" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>12 0.83920342 <a title="199-lda-12" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>13 0.83887267 <a title="199-lda-13" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>14 0.83880943 <a title="199-lda-14" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>15 0.83869553 <a title="199-lda-15" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>16 0.83866221 <a title="199-lda-16" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>17 0.83846515 <a title="199-lda-17" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>18 0.83840764 <a title="199-lda-18" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>19 0.83813542 <a title="199-lda-19" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>20 0.83698791 <a title="199-lda-20" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
