<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-173" href="#">cvpr2013-173</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</h1>
<br/><p>Source: <a title="cvpr-2013-173-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Tighe_Finding_Things_Image_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Joseph Tighe, Svetlana Lazebnik</p><p>Abstract: This paper presents a system for image parsing, or labeling each pixel in an image with its semantic category, aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. The system combines region-level features with per-exemplar sliding window detectors. Per-exemplar detectors are better suited for our parsing task than traditional bounding box detectors: they perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. The proposed system achieves state-of-theart accuracy on three challenging datasets, the largest of which contains 45,676 images and 232 labels.</p><p>Reference: <a title="cvpr-2013-173-reference" href="../cvpr2013_reference/cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract This paper presents a system for image parsing, or labeling each pixel in an image with its semantic category, aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. [sent-3, score-0.23]
</p><p>2 The system combines region-level features with per-exemplar sliding window detectors. [sent-4, score-0.143]
</p><p>3 Per-exemplar detectors are better suited for our parsing task than traditional bounding box detectors: they perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. [sent-5, score-1.045]
</p><p>4 The proposed system achieves state-of-theart accuracy on three challenging datasets, the largest of which contains 45,676 images and 232 labels. [sent-6, score-0.144]
</p><p>5 Our goal is achieving broad coverage the ability to recognize hundreds or thousands of object classes that commonly –  occur in everyday street scenes and indoor environments. [sent-9, score-0.178]
</p><p>6 But a much larger number of “thing” classes people, cars, dogs, mailboxes, vases, stop signs occupy a small percentage of image pixels and have relatively few instances each. [sent-13, score-0.158]
</p><p>7 “Stuff” categories have no consistent shape but fairly consistent texture, so they can be adequately handled by image parsing systems based on pixel- or region-level features [5, 7, 8, 18, 21, 22, 25, 26, 27, 29]. [sent-14, score-0.387]
</p><p>8 In order to improve performance on “things,” a few recent image parsing approaches [1, 10, 12, 14, 16] have attempted to incorporate sliding window detectors. [sent-16, score-0.403]
</p><p>9 Many of these approaches rely on detectors like HOG templates [6] and deformable part-based models (DPMs) [9], which produce only bounding box hypotheses. [sent-17, score-0.333]
</p><p>10 edu l ino s ing to infer a pixel-level segmentation from a bounding box is a complex and error-prone process. [sent-19, score-0.142]
</p><p>11 None of these schemes are well suited for handling large numbers of sparsely-sampled classes with high intra-class variation. [sent-21, score-0.133]
</p><p>12 In this paper, we propose an image parsing system that integrates region-based cues with the promising novel framework of per-exemplar detectors or exemplarSVMs [19]. [sent-22, score-0.673]
</p><p>13 Per-exemplar detectors are more appropriate than traditional sliding window detectors for classes with few training samples and wide variability. [sent-23, score-0.695]
</p><p>14 They also meet our need for pixel-level localization: when a per-exemplar detector fires on a test image, we can take the segmentation mask from the corresponding training exemplar and transfer it into the test image to form a segmentation hypothesis. [sent-24, score-0.524]
</p><p>15 The idea of transferring object segmentation masks from training to test images – either whole or in “fragments” has been explored before in the literature see, e. [sent-25, score-0.271]
</p><p>16 However, most existing work uses local feature matches to transfer mask hypotheses, and focuses on one class at a time. [sent-28, score-0.197]
</p><p>17 To our knowledge, our approach is the first to transfer masks using per-exemplar detectors (Malisiewicz et al. [sent-29, score-0.372]
</p><p>18 It combines the region-based parser from our earlier work [27] –  –  with a novel parser based on per-exemplar detectors. [sent-32, score-0.46]
</p><p>19 Each parser produces a score or data term for each possible label at each pixel location, and the data terms are combined using a support vector machine (SVM) to generate the final labeling. [sent-33, score-0.357]
</p><p>20 In particular, the LM+SUN dataset, with 45,676 images and 232 labels, has the broadest coverage of any image parsing benchmark to date. [sent-35, score-0.4]
</p><p>21 The test image (a) contains a bus – a relatively rare “thing” class. [sent-42, score-0.182]
</p><p>22 Our region-based parsing system  [27] computes class likelihoods (b) based on superpixel features, and it correctly identifies “stuff” regions like sky, road, and trees, but is not able to get the bus (c). [sent-43, score-0.624]
</p><p>23 To find “things” like bus and car, we run per-exemplar detectors [19] on the test image (d) and transfer masks corresponding to detected training exemplars (e). [sent-44, score-0.628]
</p><p>24 Since the detectors are not well suited for “stuff,” the result of detector-based parsing (f) is poor. [sent-45, score-0.623]
</p><p>25 However, combining region-based and detection-based data terms (g) gives the highest accuracy of all and correctly labels most of the bus and part of the car. [sent-46, score-0.153]
</p><p>26 Method This section presents our hybrid image parsing method as illustrated in Figure 1. [sent-48, score-0.35]
</p><p>27 Region-Based Parsing  For region-based parsing, we use the scalable nonparametric system we have developed earlier [27]. [sent-55, score-0.212]
</p><p>28 Given a query image, this system first uses global image descriptors to identify a retrieval set of training images similar to the query. [sent-56, score-0.269]
</p><p>29 t For large-scale datasets with many labels (SIFT Flow and LM+SUN in our experiments), we obtain the loglikelihood ratio score based on nonparametric nearestneighbor estimates (see [27] for details). [sent-60, score-0.207]
</p><p>30 For smaller-scale datasets with few classes (CamVid), we obtain it from the output of a boosted decision tree classifier. [sent-61, score-0.169]
</p><p>31 Either way, we use this score to define our region-based data term ER for each pixel p and class c: ER(p, c) = L(sp, c) , (2) where sp is the region containing p. [sent-62, score-0.196]
</p><p>32 While it may seem intuitive to only train detectors for “thing” categories, we train them for all categories, including ones seemingly inappropriate for a sliding window approach, such as “sky. [sent-67, score-0.415]
</p><p>33 We follow the detector training procedure of [19], with negative mining done on all training images that do not contain an object of the same class. [sent-69, score-0.23]
</p><p>34 For our largest LM+SUN dataset we only do negative mining on 1,000 training images most similar to the positive exemplar’s image (we have found that using more does not increase the detection accuracy). [sent-70, score-0.174]
</p><p>35 At test time, given an image that needs to be parsed, we first obtain a retrieval set of globally similar training images as in Section 2. [sent-71, score-0.204]
</p><p>36 Then we run the detectors associated with the first k instances of each class in that retrieval set (the instances are ranked in decreasing order of the similarity of their image to the test image, and different instances in the same image are ranked arbitrarily). [sent-73, score-0.619]
</p><p>37 For each detection we project =the − a1s saosc siuagtegde object mask into the detected bounding box (Figure 2). [sent-76, score-0.174]
</p><p>38 To compute the detector-based data term ED for a class c and pixel 333000000200  Figure2. [sent-77, score-0.159]
</p><p>39 Foreachpositve  detection (green bounding box) in the test image (middle row) we transfer the mask (red polygon) from the associated exemplar (top) into the test image. [sent-79, score-0.343]
</p><p>40 The data term for “car” (bottom) is obtained by summing all the masks weighted by their detector responses. [sent-80, score-0.199]
</p><p>41 p, we simply take the sum of all detection masks from that class weighted by their detection scores: ED(p,c) =  X  (wd− td),  (3)  d∈XDp,c where Dp,c is the set of all detections for class c whose transferred mask overlaps pixel p and wd is the detection score of d. [sent-81, score-0.376]
</p><p>42 Note that the full training framework of [19] includes computationally intensive calibration and contextual pooling procedures that are meant to make scores of different per-exemplar detectors more consistent. [sent-83, score-0.316]
</p><p>43 SVM Combination and MRF Smoothing Once we run the parsing systems of Sections 2. [sent-87, score-0.35]
</p><p>44 2 on a test image, for each pixel p and each class c we end up with two data terms, ER(p, c) and ED (p, c), as defined by eqs. [sent-89, score-0.17]
</p><p>45 To make SVM training feasible, we must subsample the data – a tricky task given the unbalanced class frequencies in our many-category datasets. [sent-95, score-0.201]
</p><p>46 Conversely, subsampling the data so that  each class has a roughly equal number of points produces a bias towards the rare classes. [sent-100, score-0.116]
</p><p>47 For training one-vs-all SVMs, we normalize each feature dimension by its standard deviation and use fivefold crossvalidation to find the regularization constant. [sent-104, score-0.229]
</p><p>48 Since it is infeasible to train a nonlinear SVM with the RBF kernel on our largest dataset, we approximate it by training a linear SVM on top of the random Fourier feature embedding [23]. [sent-107, score-0.314]
</p><p>49 We set the dimensionality of the embedding to 4,000 and find the kernel bandwidth using fivefold cross-validation. [sent-108, score-0.207]
</p><p>50 Let ESVM (pi, ci) denote the response of the SVM for class ci at pixel pi. [sent-111, score-0.147]
</p><p>51 We smooth the labels with an MRF energy function similar to [18, 25] defined over the field of pixel labels c:  J(c) =  Xmax[0,M − ESVM(pi,ci)]  pXi∈+Iλ  X Esmooth(ci,cj), (piX X,pj X) ∈? [sent-113, score-0.118]
</p><p>52 is the set is of adjacent pixels, M is the highest expected value of the SVM response (about 10 on our data), λ is a smoothing constant (we set λ = 16), and Esmooth(ci, cj) imposes a penalty when two adjacent pixels (pi, pj) are similar but are assigned different labels (ci, cj) (see eq. [sent-115, score-0.118]
</p><p>53 It has 2,488 training images, 200 test images, and 33 labels. [sent-122, score-0.143]
</p><p>54 Region + Thing uses the SVM trained on the full region data term and the subset of the detector data term corresponding to “thing” classes. [sent-126, score-0.199]
</p><p>55 Note that training the exact RBF on the largest LM+SUN dataset was computationally infeasible. [sent-137, score-0.174]
</p><p>56 We use the split of [27], which consists of 45,176 training and 500 test images. [sent-139, score-0.143]
</p><p>57 For training detectors, we fit a bounding box and a segmentation mask to each connected component of the same label type. [sent-145, score-0.299]
</p><p>58 [11], and use boosted decision tree classifiers instead of nonparametric likelihood estimates. [sent-150, score-0.116]
</p><p>59 To obtain training data for the SVM, we compute the responses of the boosted decision tree classifiers on the same images on which they were trained (we have found this to work better than crossvalidation on this dataset). [sent-151, score-0.161]
</p><p>60 On all datasets, we report the overall per-pixel rate (percent of test set pixels correctly labeled), which is dominated by the most common classes, as well as the average of perclass rates, which is dominated by the rarer classes. [sent-153, score-0.218]
</p><p>61 On the LM+SUN dataset, which has the largest number of rare “thing” classes, the detector-based data term actually obtains higher per-class accuracy than the region-based one. [sent-160, score-0.149]
</p><p>62 As observed in [27], MRF inference further raises the per-pixel rate, but often lowers the per-class rate  by smoothing away some of the smaller objects. [sent-162, score-0.161]
</p><p>63 Figure Classification rates of individual classes (ordered from most to least frequent) on the SIFT Flow dataset for region-based, detector-based, and combined parsing. [sent-165, score-0.249]
</p><p>64 Figure Classification rates of the most common individual classes (ordered from most to least frequent) on the LM+SUN dataset for region-based, detector-based, and combined parsing. [sent-176, score-0.249]
</p><p>65 Interestingly, the results for this setup are weaker than those of the full combined system using both “thing” and “stuff” detectors. [sent-183, score-0.15]
</p><p>66 Figures 3 and 4 show the per-class rates of our system on the most common classes in the SIFT Flow and LM+SUN datasets, respectively. [sent-187, score-0.242]
</p><p>67 As expected, adding detectors significantly improves many “thing” classes (including car, sign, and balcony) but also some “stuff” classes (road, sea, sidewalk, fence). [sent-188, score-0.419]
</p><p>68 Figure 5 gives a close-up look at our performance on many small object categories, and Figure 6 shows several parsing examples on the LM+SUN dataset. [sent-189, score-0.35]
</p><p>69 Table 3 compares our combined system to a number of state-of-the-art approaches on the SIFT Flow dataset. [sent-190, score-0.15]
</p><p>70 We  outperform them, in many cases beating the average perclass rate by up to 10% while maintaining or exceeding the per-pixel rates. [sent-191, score-0.12]
</p><p>71 When their system is tuned to a per-pixel rate similar to ours, their average per-class rate drops significantly below ours. [sent-194, score-0.204]
</p><p>72 On LM+SUN, which has an order of magnitude more images and labels than SIFT Flow, the only previously reported results are from our earlier region-based system [27]. [sent-195, score-0.173]
</p><p>73 As Table 4 shows, by augmenting the region-based term with a novel detector-based data term and SVM inference, we are able to raise the per-pixel rate from 54. [sent-196, score-0.155]
</p><p>74 When compared to our region-based system [27], we improve performance for every class except for building and sky, towards which the region-based parser seems to be overly biased. [sent-202, score-0.368]
</p><p>75 Running Time Finally, we examine the computational requirements of our system on our largest dataset, LM+SUN, by timing our MATLAB implementation (feature extraction and file I/O excluded) on a six-core 3. [sent-208, score-0.144]
</p><p>76 There are a total of 354,592 objects in the training set, and we train a per-exemplar detector for each of them. [sent-210, score-0.189]
</p><p>77 For each class we show a crop of an image, the SVM combined output, and the smoothed final result. [sent-230, score-0.13]
</p><p>78 The caption for each class shows: (# of training instances of that class) / (# of test instances) (per-pixel rate on the test set)%. [sent-231, score-0.395]
</p><p>79 Leave-one-out parsing of the training set (see below for average region- and detector-based parsing times per image) takes 939 hours on a single CPU, or about two hours on the cluster. [sent-234, score-0.883]
</p><p>80 Next, training a set of 232 onevs-all SVMs takes a total of one hour on a single machine for the linear SVM and ten hours for the approximate RBF. [sent-235, score-0.167]
</p><p>81 Note that the respective feature dimensionalities are 464 and 4,000; this nearly tenfold dimensionality increase accounts for the tenfold increase in running time. [sent-236, score-0.249]
</p><p>82 Tuning the SVM  parameters by fivefold cross-validation on the cluster only increases the training time by a factor of two. [sent-237, score-0.189]
</p><p>83 At test time, the region-based parsing takes an average of 27. [sent-238, score-0.41]
</p><p>84 The detector-based parser runs an average of 4,842 detectors per image in 47. [sent-240, score-0.441]
</p><p>85 9 seconds for the linear kernel and 124 seconds for the approximate RBF (once again, the tenfold increase in feature dimensionality and the overhead ofcomputing the embedding account for the increase in running time). [sent-243, score-0.417]
</p><p>86 At test time, we would like to to reduce the number of detectors that need to be run per image. [sent-251, score-0.293]
</p><p>87 Instead, we want to develop methods for dynamically selecting detectors for each test image based on context. [sent-255, score-0.293]
</p><p>88 Also, SVM testing with the approximate RBF embedding imposes a heavy overhead in our current implementation. [sent-256, score-0.167]
</p><p>89 Ultimately, we want our system to function on open universe datasets, such as LabelMe [24], that are constantly evolving and do not have a pre-defined list of classes of interest. [sent-258, score-0.183]
</p><p>90 In principle, per-exemplar detectors are also compatible with the open-universe setting, since they can be trained independently as new exemplars come in. [sent-260, score-0.27]
</p><p>91 Our SVM combination step is the only one that relies on batch offline training (including leave-one-out parsing of the entire training set). [sent-261, score-0.516]
</p><p>92 Second through fourth columns: region-based data term (top), detector-based data term (middle), and SVM combination (bottom) for three selected class labels. [sent-266, score-0.168]
</p><p>93 Fifth column: region-based parsing results (top) and detector-based parsing results (bottom) without SVM or MRF smoothing. [sent-267, score-0.7]
</p><p>94 In (b), the system correctly identifies the wheels of the cars and the headlight of the left car. [sent-270, score-0.171]
</p><p>95 In (c), the detectors correctly identify the wall and most of the bed. [sent-271, score-0.271]
</p><p>96 Note that the region-based parser alone mislabels most of the bed as “sea”; the detector-based parser does much better but still mislabels part of the bed as “mountain. [sent-272, score-0.634]
</p><p>97 ” In this example, the detector-based parser also finds two pictures and a lamp that do not survive in the final output. [sent-273, score-0.253]
</p><p>98 Notice how the detectors are able to complete the car in (a) and (b). [sent-288, score-0.288]
</p><p>99 Poselets: Body part detectors trained using 3d human pose annotations. [sent-308, score-0.233]
</p><p>100 Scene parsing with multiscale feature learning, purity trees, and optimal covers. [sent-339, score-0.35]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('parsing', 0.35), ('thing', 0.294), ('lm', 0.271), ('detectors', 0.233), ('parser', 0.208), ('camvid', 0.208), ('svm', 0.198), ('stuff', 0.194), ('mrf', 0.138), ('sun', 0.136), ('rbf', 0.107), ('fivefold', 0.106), ('tenfold', 0.106), ('classes', 0.093), ('system', 0.09), ('masks', 0.086), ('training', 0.083), ('nonparametric', 0.078), ('bus', 0.076), ('mask', 0.074), ('svms', 0.071), ('flow', 0.071), ('mislabels', 0.071), ('class', 0.07), ('things', 0.066), ('instances', 0.065), ('embedding', 0.065), ('road', 0.065), ('plate', 0.065), ('detector', 0.064), ('sift', 0.063), ('sea', 0.063), ('perclass', 0.063), ('retrieval', 0.061), ('combined', 0.06), ('test', 0.06), ('rates', 0.059), ('exemplarsvms', 0.058), ('lowers', 0.058), ('labelme', 0.057), ('rate', 0.057), ('hariharan', 0.056), ('car', 0.055), ('esvm', 0.055), ('farabet', 0.055), ('largest', 0.054), ('sliding', 0.053), ('transfer', 0.053), ('seconds', 0.052), ('loglikelihood', 0.052), ('fik', 0.052), ('esmooth', 0.052), ('coverage', 0.05), ('hours', 0.05), ('bounding', 0.05), ('box', 0.05), ('labeling', 0.05), ('term', 0.049), ('subsample', 0.048), ('td', 0.047), ('tighe', 0.047), ('smoothing', 0.046), ('exemplar', 0.046), ('rare', 0.046), ('inappropriate', 0.045), ('lamp', 0.045), ('suppresses', 0.044), ('ladick', 0.044), ('earlier', 0.044), ('grundmann', 0.043), ('cars', 0.043), ('train', 0.042), ('segmentation', 0.042), ('sky', 0.041), ('pixel', 0.04), ('crossvalidation', 0.04), ('sturgess', 0.04), ('suited', 0.04), ('labels', 0.039), ('alahari', 0.039), ('arxiv', 0.039), ('bed', 0.038), ('datasets', 0.038), ('correctly', 0.038), ('boosted', 0.038), ('categories', 0.037), ('running', 0.037), ('exemplars', 0.037), ('region', 0.037), ('dataset', 0.037), ('ci', 0.037), ('kernel', 0.036), ('wd', 0.036), ('er', 0.035), ('overhead', 0.035), ('indoor', 0.035), ('query', 0.035), ('approximate', 0.034), ('june', 0.034), ('imposes', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="173-tfidf-1" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>Author: Joseph Tighe, Svetlana Lazebnik</p><p>Abstract: This paper presents a system for image parsing, or labeling each pixel in an image with its semantic category, aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. The system combines region-level features with per-exemplar sliding window detectors. Per-exemplar detectors are better suited for our parsing task than traditional bounding box detectors: they perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. The proposed system achieves state-of-theart accuracy on three challenging datasets, the largest of which contains 45,676 images and 232 labels.</p><p>2 0.18371437 <a title="173-tfidf-2" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>Author: Gautam Singh, Jana Kosecka</p><p>Abstract: This paper presents a nonparametric approach to semantic parsing using small patches and simple gradient, color and location features. We learn the relevance of individual feature channels at test time using a locally adaptive distance metric. To further improve the accuracy of the nonparametric approach, we examine the importance of the retrieval set used to compute the nearest neighbours using a novel semantic descriptor to retrieve better candidates. The approach is validated by experiments on several datasets used for semantic parsing demonstrating the superiority of the method compared to the state of art approaches.</p><p>3 0.15308757 <a title="173-tfidf-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.14425994 <a title="173-tfidf-4" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>5 0.13810611 <a title="173-tfidf-5" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>Author: Guang Shu, Afshin Dehghan, Mubarak Shah</p><p>Abstract: We propose an approach to improve the detection performance of a generic detector when it is applied to a particular video. The performance of offline-trained objects detectors are usually degraded in unconstrained video environments due to variant illuminations, backgrounds and camera viewpoints. Moreover, most object detectors are trained using Haar-like features or gradient features but ignore video specificfeatures like consistent colorpatterns. In our approach, we apply a Superpixel-based Bag-of-Words (BoW) model to iteratively refine the output of a generic detector. Compared to other related work, our method builds a video-specific detector using superpixels, hence it can handle the problem of appearance variation. Most importantly, using Conditional Random Field (CRF) along with our super pixel-based BoW model, we develop and algorithm to segment the object from the background . Therefore our method generates an output of the exact object regions instead of the bounding boxes generated by most detectors. In general, our method takes detection bounding boxes of a generic detector as input and generates the detection output with higher average precision and precise object regions. The experiments on four recent datasets demonstrate the effectiveness of our approach and significantly improves the state-of-art detector by 5-16% in average precision.</p><p>6 0.13684262 <a title="173-tfidf-6" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>7 0.12357303 <a title="173-tfidf-7" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>8 0.12355558 <a title="173-tfidf-8" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>9 0.12261891 <a title="173-tfidf-9" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>10 0.1162274 <a title="173-tfidf-10" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>11 0.11579831 <a title="173-tfidf-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.11344586 <a title="173-tfidf-12" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>13 0.1124791 <a title="173-tfidf-13" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>14 0.11169402 <a title="173-tfidf-14" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>15 0.11002173 <a title="173-tfidf-15" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>16 0.10913881 <a title="173-tfidf-16" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>17 0.1075798 <a title="173-tfidf-17" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>18 0.10497475 <a title="173-tfidf-18" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>19 0.10302538 <a title="173-tfidf-19" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>20 0.10166217 <a title="173-tfidf-20" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.257), (1, -0.065), (2, 0.026), (3, -0.063), (4, 0.116), (5, 0.044), (6, 0.046), (7, 0.094), (8, -0.055), (9, -0.033), (10, 0.014), (11, -0.024), (12, 0.067), (13, -0.011), (14, 0.012), (15, -0.033), (16, 0.062), (17, -0.063), (18, -0.056), (19, -0.016), (20, 0.005), (21, -0.036), (22, 0.071), (23, 0.056), (24, 0.022), (25, 0.034), (26, 0.013), (27, 0.055), (28, -0.036), (29, -0.0), (30, -0.034), (31, -0.027), (32, 0.016), (33, 0.019), (34, -0.072), (35, -0.056), (36, 0.022), (37, -0.061), (38, -0.068), (39, 0.003), (40, -0.016), (41, -0.002), (42, -0.036), (43, -0.051), (44, 0.048), (45, -0.003), (46, 0.046), (47, 0.04), (48, 0.034), (49, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95440364 <a title="173-lsi-1" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>Author: Joseph Tighe, Svetlana Lazebnik</p><p>Abstract: This paper presents a system for image parsing, or labeling each pixel in an image with its semantic category, aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. The system combines region-level features with per-exemplar sliding window detectors. Per-exemplar detectors are better suited for our parsing task than traditional bounding box detectors: they perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. The proposed system achieves state-of-theart accuracy on three challenging datasets, the largest of which contains 45,676 images and 232 labels.</p><p>2 0.74606216 <a title="173-lsi-2" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>Author: Payman Yadollahpour, Dhruv Batra, Gregory Shakhnarovich</p><p>Abstract: This paper introduces a two-stage approach to semantic image segmentation. In the first stage a probabilistic model generates a set of diverse plausible segmentations. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the probabilistic model, allowing a better exploration of the solution space than possible by simply producing the most probable solution from the probabilistic model. While our proposed approach already achieves state-of-the-art results (48.1%) on the challenging VOC 2012 dataset, our machine and human analyses suggest that even larger gains are possible with such an approach.</p><p>3 0.73358184 <a title="173-lsi-3" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>Author: Guang-Tong Zhou, Tian Lan, Weilong Yang, Greg Mori</p><p>Abstract: We conduct image classification by learning a class-toimage distance function that matches objects. The set of objects in training images for an image class are treated as a collage. When presented with a test image, the best matching between this collage of training image objects and those in the test image is found. We validate the efficacy of the proposed model on the PASCAL 07 and SUN 09 datasets, showing that our model is effective for object classification and scene classification tasks. State-of-the-art image classification results are obtained, and qualitative results demonstrate that objects can be accurately matched.</p><p>4 0.71752501 <a title="173-lsi-4" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>Author: Roman Shapovalov, Dmitry Vetrov, Pushmeet Kohli</p><p>Abstract: This paper addresses the problem of semantic segmentation of 3D point clouds. We extend the inference machines framework of Ross et al. by adding spatial factors that model mid-range and long-range dependencies inherent in the data. The new model is able to account for semantic spatial context. During training, our method automatically isolates and retains factors modelling spatial dependencies between variables that are relevant for achieving higher prediction accuracy. We evaluate the proposed method by using it to predict 1 7-category semantic segmentations on sets of stitched Kinect scans. Experimental results show that the spatial dependencies learned by our method significantly improve the accuracy of segmentation. They also show that our method outperforms the existing segmentation technique of Koppula et al.</p><p>5 0.70970553 <a title="173-lsi-5" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>Author: Anelia Angelova, Shenghuo Zhu</p><p>Abstract: We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also ‘zoom in ’ on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios [4, 21]. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging largescale flower dataset, containing 578 species of flowers and 250,000 images.</p><p>6 0.70555896 <a title="173-lsi-6" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>7 0.70373052 <a title="173-lsi-7" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>8 0.70273703 <a title="173-lsi-8" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>9 0.70213705 <a title="173-lsi-9" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>10 0.70003629 <a title="173-lsi-10" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>11 0.69936067 <a title="173-lsi-11" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>12 0.69812274 <a title="173-lsi-12" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>13 0.69562191 <a title="173-lsi-13" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>14 0.69492352 <a title="173-lsi-14" href="./cvpr-2013-A_Sentence_Is_Worth_a_Thousand_Pixels.html">25 cvpr-2013-A Sentence Is Worth a Thousand Pixels</a></p>
<p>15 0.68887907 <a title="173-lsi-15" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>16 0.67582935 <a title="173-lsi-16" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>17 0.6670633 <a title="173-lsi-17" href="./cvpr-2013-Learning_for_Structured_Prediction_Using_Approximate_Subgradient_Descent_with_Working_Sets.html">262 cvpr-2013-Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets</a></p>
<p>18 0.66576558 <a title="173-lsi-18" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>19 0.66398448 <a title="173-lsi-19" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>20 0.66098464 <a title="173-lsi-20" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.109), (16, 0.035), (24, 0.153), (26, 0.088), (28, 0.021), (33, 0.26), (67, 0.096), (69, 0.055), (77, 0.011), (87, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92030013 <a title="173-lda-1" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>Author: Shaodi You, Robby T. Tan, Rei Kawakami, Katsushi Ikeuchi</p><p>Abstract: Raindrops adhered to a windscreen or window glass can significantly degrade the visibility of a scene. Detecting and removing raindrops will, therefore, benefit many computer vision applications, particularly outdoor surveillance systems and intelligent vehicle systems. In this paper, a method that automatically detects and removes adherent raindrops is introduced. The core idea is to exploit the local spatiotemporal derivatives ofraindrops. First, it detects raindrops based on the motion and the intensity temporal derivatives of the input video. Second, relying on an analysis that some areas of a raindrop completely occludes the scene, yet the remaining areas occludes only partially, the method removes the two types of areas separately. For partially occluding areas, it restores them by retrieving as much as possible information of the scene, namely, by solving a blending function on the detected partially occluding areas using the temporal intensity change. For completely occluding areas, it recovers them by using a video completion technique. Experimental results using various real videos show the effectiveness of the proposed method.</p><p>same-paper 2 0.88628924 <a title="173-lda-2" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>Author: Joseph Tighe, Svetlana Lazebnik</p><p>Abstract: This paper presents a system for image parsing, or labeling each pixel in an image with its semantic category, aimed at achieving broad coverage across hundreds of object categories, many of them sparsely sampled. The system combines region-level features with per-exemplar sliding window detectors. Per-exemplar detectors are better suited for our parsing task than traditional bounding box detectors: they perform well on classes with little training data and high intra-class variation, and they allow object masks to be transferred into the test image for pixel-level segmentation. The proposed system achieves state-of-theart accuracy on three challenging datasets, the largest of which contains 45,676 images and 232 labels.</p><p>3 0.88058794 <a title="173-lda-3" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>Author: Bojan Pepikj, Michael Stark, Peter Gehler, Bernt Schiele</p><p>Abstract: Despite the success of recent object class recognition systems, the long-standing problem of partial occlusion remains a major challenge, and a principled solution is yet to be found. In this paper we leave the beaten path of methods that treat occlusion as just another source of noise instead, we include the occluder itself into the modelling, by mining distinctive, reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular, we evaluate and compare models that range from standard object class detectors to hierarchical, part-based representations of occluder/occludee pairs. In an extensive evaluation we derive insights that can aid further developments in tackling the occlusion challenge. –</p><p>4 0.87983716 <a title="173-lda-4" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>5 0.87888432 <a title="173-lda-5" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>Author: Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang</p><p>Abstract: This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants, a new tree structure, ”Segment-Tree ”, is proposed for non-local matching cost aggregation. Conceptually, the segment-tree is constructed in a three-step process: first, the pixels are grouped into a set of segments with the reference color or intensity image; second, a tree graph is created for each segment; and in the final step, these independent segment graphs are linked to form the segment-tree structure. In practice, this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights, our method introduces some ’non-local’ decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities, and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree within two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable to previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore, the tree structure can be refined with the estimated disparities, which leads to consistent scene segmentation and significantly better aggregation results.</p><p>6 0.87519151 <a title="173-lda-6" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>7 0.87509871 <a title="173-lda-7" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>8 0.87454641 <a title="173-lda-8" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>9 0.87434447 <a title="173-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.87402672 <a title="173-lda-10" href="./cvpr-2013-Top-Down_Segmentation_of_Non-rigid_Visual_Objects_Using_Derivative-Based_Search_on_Sparse_Manifolds.html">433 cvpr-2013-Top-Down Segmentation of Non-rigid Visual Objects Using Derivative-Based Search on Sparse Manifolds</a></p>
<p>11 0.8735404 <a title="173-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.87095022 <a title="173-lda-12" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>13 0.87003392 <a title="173-lda-13" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>14 0.86971909 <a title="173-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.86905628 <a title="173-lda-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.86852968 <a title="173-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.86844867 <a title="173-lda-17" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>18 0.86840862 <a title="173-lda-18" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>19 0.86815459 <a title="173-lda-19" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>20 0.86707455 <a title="173-lda-20" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
