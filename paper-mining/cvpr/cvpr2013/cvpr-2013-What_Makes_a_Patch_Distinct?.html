<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>464 cvpr-2013-What Makes a Patch Distinct?</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-464" href="#">cvpr2013-464</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>464 cvpr-2013-What Makes a Patch Distinct?</h1>
<br/><p>Source: <a title="cvpr-2013-464-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Margolin_What_Makes_a_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ran Margolin, Ayellet Tal, Lihi Zelnik-Manor</p><p>Abstract: What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns, others on the colors, and several add high-level cues and priors. We propose a simple, yet powerful, algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.</p><p>Reference: <a title="cvpr-2013-464-reference" href="../cvpr2013_reference/cvpr-2013-What_Makes_a_Patch_Distinct%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Most previous work assert that distinctness is the dominating factor. [sent-20, score-0.83]
</p><p>2 We rely on the inner statistics of the patches in the image for identifying unique patterns. [sent-25, score-0.192]
</p><p>3 Introduction The detection of the most salient region of an image has numerous applications, including object detection and recognition [13], image compression [10], video summarization [16], and photo collage [8], to name a few. [sent-28, score-0.253]
</p><p>4 Some algorithms look for regions of distinct color [6, 11]. [sent-31, score-0.213]
</p><p>5 As shown in Figure 1(b) this is insufficient, as some regions of distinct color may be non-salient. [sent-32, score-0.213]
</p><p>6 As illustrated in Figure 1(d), this could lead to missing homogeneous regions of the salient object. [sent-34, score-0.266]
</p><p>7 In this paper, we introduce a new algorithm for salient object detection, which solves the above problems. [sent-35, score-0.195]
</p><p>8 It integrates pattern and color distinctness in a unique manner. [sent-36, score-0.989]
</p><p>9 Our key idea is that the analysis of the inner statistics of patches in the image provides acute insight on the distinctness of regions. [sent-37, score-0.993]
</p><p>10 This is in contrast to previous approaches that compared each patch to its k-  nearest neighbors [9, 5], without taking into account the internal statistics of all the other image patches. [sent-41, score-0.253]
</p><p>11 This benchmark consists of five wellknown datasets of natural images, with one or more salient objects. [sent-44, score-0.234]
</p><p>12 We begin by describing our approach, which consists of three steps: pattern distinctness detection (Section 2. [sent-49, score-0.904]
</p><p>13 Proposed approach The guiding principle of our approach is that a salient object consists of pixels whose local neighborhood (region or patch) is distinctive in both color and pattern. [sent-55, score-0.249]
</p><p>14 As illustrated in Figure 2, integrating pattern and color distinctness is essential for handling complex images. [sent-56, score-0.954]
</p><p>15 Pattern distinctness is determined by considering the internal statistics of the patches in the image. [sent-57, score-1.003]
</p><p>16 A pixel is deemed salient if the  ×  pattern of its surrounding patch cannot be explained well by other image patches. [sent-58, score-0.444]
</p><p>17 Pattern Distinctness The common solution to measure pattern distinctness is based on comparing each image patch to all other image patches [5, 9, 19]. [sent-64, score-1.195]
</p><p>18 A patch that is different from all other image patches, is considered salient. [sent-65, score-0.196]
</p><p>19 Our first observation is that the non-distinct patches of a natural image are mostly concentrated in the high-dimensional space, while distinct patches are more scattered. [sent-69, score-0.392]
</p><p>20 We ttrhaecnt ca alllc 9u l×ate 9 t phaet cdhiesstan acned b ceotmwepeunte every patch a pnadt cthhe. [sent-72, score-0.23]
</p><p>21 System overview: Our pattern distinctness (b), captures the unique textures on the statue, but also part of the tree in the background. [sent-76, score-0.912]
</p><p>22 Our color distinctness (c), detects the statue fully, but also the red podium and part of the sky. [sent-77, score-1.043]
</p><p>23 lines in Figure 3 show the cumulative histograms of the distances between non-distinct patches and the average patch. [sent-79, score-0.171]
</p><p>24 The dashed lines represent statistics of distinct patches only. [sent-80, score-0.33]
</p><p>25 As can be seen, non-distinct patches are much more concentrated around the average patch than distinct patches. [sent-81, score-0.492]
</p><p>26 1, while only less than 20% of the distinct patches are within this distance. [sent-83, score-0.243]
</p><p>27 Scatter distinguishes between distinct and nondistinct patches: This figure presents the cumulative histograms of the distances between distinct (dashed lines) and non-distinct (solid lines) patches to the average patch. [sent-85, score-0.408]
</p><p>28 Both L1 and PCA approaches show that non-distinct patches are significantly more concentrated around the average patch than non-distinct patches. [sent-86, score-0.365]
</p><p>29 1 1 1 111114333400888  The plots of Figure 3 suggest that one could possibly identify the distinct patches by measuring the distance to the average patch. [sent-87, score-0.263]
</p><p>30 In particular, we use the average patch pA under the L1 norm:  pA=N1? [sent-88, score-0.216]
</p><p>31 (1)  An image patch px is considered distinct if it is dissimilar to the average patch pA. [sent-90, score-0.76]
</p><p>32 Note that computing the distance between every patch and the average patch bares some conceptual resemblance to the common approach of [5, 9, 19]. [sent-91, score-0.412]
</p><p>33 They try to measure the isolation of a patch in patch-space by computing the distance to its k-nearest neighbors. [sent-92, score-0.196]
</p><p>34 Instead, we propose a significantly more efficient solution, as all patches are compared to a single patch pA. [sent-93, score-0.312]
</p><p>35 Suppose that a certain patch appears in two different images. [sent-95, score-0.196]
</p><p>36 These two images could have the same average patch, thus the distance of the patch to the average would be equal. [sent-96, score-0.236]
</p><p>37 However, the saliency of this patch should be totally different, when the images have different patch distributions. [sent-97, score-0.488]
</p><p>38 In this figure the patch px (marked in red) should be considered as salient in image Im2 and non-salient in image Im1. [sent-99, score-0.612]
</p><p>39 Yet, the Euclidean distance between px and the average patch pA (dashed purple line) is the same for both images. [sent-100, score-0.459]
</p><p>40 Were we to rely on  this distance to determine distinctness we would likely fail. [sent-101, score-0.83]
</p><p>41 As can be seen in Figure 4, the patch px has the same k-nearest patches in both images (contained within the dashed red circle) and hence will be assigned the same level of distinctness by [5, 9]. [sent-103, score-1.43]
</p><p>42 Using either L2 or L1 to measure distances between patches ignores the internal statistics of the image patches. [sent-105, score-0.173]
</p><p>43 The reason patch px should be considered as distinct in image Im2 is that it is inconsistent with the other patches of image Im2. [sent-106, score-0.66]
</p><p>44 The statistics of patches in each image are different, as evident from the distributions of the patches in Figure 4. [sent-107, score-0.277]
</p><p>45 Our second observation is that the distance to the average patch should consider the patch distribution in the image. [sent-109, score-0.412]
</p><p>46 We then consider a patch distinct if the path connecting it to the average patch, along the principal components, is long. [sent-111, score-0.439]
</p><p>47 For each patch we march along the principal components towards the average patch and compute the accumulated length of this path. [sent-112, score-0.497]
</p><p>48 Mathematically, this boils down to calculating the L1 norm of px in PCA coordinates. [sent-113, score-0.221]
</p><p>49 Saliency should depend on patch distribution: Im1 and Im2 represent two different images whose principal components are marked by the solid lines. [sent-115, score-0.328]
</p><p>50 The patch px is highly probable in the distribution of Im1 and hence should not be considered distinct in Im1, while the same patch is less probable in image Im2 and hence should be considered distinct in Im2 . [sent-117, score-0.949]
</p><p>51 The L2 distance (purple line) and L1 distance (green line) between px and pA are oblivious to the image distributions and therefore will assign the same level of distinctness to px in both images. [sent-118, score-1.272]
</p><p>52 Instead, computing the length of the paths between px and pA, along the principal components of each image, takes under consideration the distribution of patches in each image. [sent-119, score-0.422]
</p><p>53 The path for image Im2 (dashed blue line) is longer than the path for image Im1 (dashed orange line), correctly corresponding to the distinctness level of px in each image. [sent-120, score-1.176]
</p><p>54 P(px) is defined as:  P(px)  = | p˜ x||1,  (2)  where p˜ x is px ’s coordinates in the PCA coordinate system. [sent-121, score-0.221]
</p><p>55 As shown in Figure 4, the path from px to pA along the principal components of image Im2 (marked in blue) is much longer than the path along the principal components of image Im1 (marked in orange). [sent-122, score-0.497]
</p><p>56 Hence, the patch px will be considered more salient in image Im2 than in image Im1. [sent-123, score-0.612]
</p><p>57 Figure 5 provides further visualization of the proposed pattern distinctness measure. [sent-124, score-0.883]
</p><p>58 In this image, the drawings on the wall are salient because they contain unique patterns, compared to the building’s facade. [sent-125, score-0.284]
</p><p>59 The path along the principal components, between the average patch and a patch on the drawings, contains meaningful patterns from the image. [sent-126, score-0.54]
</p><p>60 Implementation details: To disregard lighting effects we a-priori subtract from each patch its mean value. [sent-127, score-0.196]
</p><p>61 To detect distinct regions regardless of their size, we compute the pattern distinctness of Eq. [sent-128, score-1.059]
</p><p>62 Computational efficiency: A major benefit of using the approach described above is its computational efficiency, 1 1 1 1 1 143 341 9 9  pAvatecrhag peA Nonpa-td(acis)htinct  Dpiast cinhct  (b) Pattern distinctness P(c) Figure 5. [sent-131, score-0.83]
</p><p>63 The principal components: (a) An image with its average patch and samples of a non-distinct and a distinct patch. [sent-132, score-0.396]
</p><p>64 (c) The absolute value of the top six principal components, added to the “red” patch along the PCA path to pA. [sent-134, score-0.292]
</p><p>65 It can be seen that the path from the “red” patch to pA adds patterns that can be found in the image. [sent-135, score-0.271]
</p><p>66 Computing pattern distinctness of the input image leads to mediocre detection results for both KNN approaches (Figure 6(b),(c)) as well as for single resolution PCA (Figure 6(d)). [sent-151, score-0.921]
</p><p>67 Color Distinctness While pattern distinctness identifies the unique patterns in the image, it is not sufficient for all images. [sent-155, score-0.944]
</p><p>68 This is illustrated in Figure 7(a), where the golden statue is salient only due to its unique color. [sent-156, score-0.404]
</p><p>69 In this particular image, due solely to color distinctness, the golden statue catchs our attention. [sent-165, score-0.217]
</p><p>70 into regions and then determine which regions are distinct in color. [sent-166, score-0.191]
</p><p>71 We solve the second step by defining the color distinctness of a region as the sum of L2 distances from all other regions in CIE LAB color-space. [sent-169, score-0.916]
</p><p>72 Given M regions, the color distinctness of region rx is computed by: ? [sent-170, score-0.916]
</p><p>73 For further robustness, we compute color distinctness at three resolutions: 100%, 50% and 25% and average them. [sent-176, score-0.904]
</p><p>74 The golden statue was properly detected, however, also a meaningless dark gap between the statues was detected as distinct in color. [sent-178, score-0.29]
</p><p>75 Putting it all together We seek regions that are salient in both color and pattern. [sent-181, score-0.281]
</p><p>76 Therefore, to integrate color and pattern distinctness we simply take the product of the two: D(px) = P(px)  ·  C(px). [sent-182, score-0.937]
</p><p>77 First, we note that the salient pixels tend to be grouped together into clusters, as they typically correspond to real objects in the scene. [sent-185, score-0.195]
</p><p>78 We start by detecting the clusters of distinct pixels by iteratively thresholding the distinctness map D(px) using 10 regularly spaced thresholds between 0 and 1. [sent-188, score-0.957]
</p><p>79 Our final saliency map S(px) is a simple product between the distinctness map and the Gaussian weight map: S(px) = G(px)  ·  D(px). [sent-193, score-0.926]
</p><p>80 The pattern distinctness suffers from non-salient distinct patterns, such as the fish drawings on the blue wall (top row). [sent-196, score-1.07]
</p><p>81 The color distinctness may capture background colors, such as the sky in the penguin road sign (bottom row). [sent-197, score-0.884]
</p><p>82 SED1 [3]: 100 images of a single salient object anno-  tated manually by three users. [sent-208, score-0.212]
</p><p>83 SED2 [3]: 100 images of two salient objects annotated manually by three users. [sent-210, score-0.195]
</p><p>84 SOD [17]: 300 images taken from the Berkeley Segmentation Dataset for which seven users selected the boundaries of the salient objects. [sent-212, score-0.195]
</p><p>85 According to [4], the “Top-4” highest scoring salient object detection algorithms are: SVO [5], CR [6], CNTX [9], and CBS [11]. [sent-213, score-0.216]
</p><p>86 This dataset is aimed at gaze-prediction, which differs from our task of salient object detection. [sent-219, score-0.195]
</p><p>87 1 1 1 1 1 14 4 43 1 1  (a) Input(b) Pat ern(c) Color(d) Pat ern(e) Organization(f) Final distinctness  distinctness  & Color  priors  saliency  Figure 8. [sent-221, score-1.788]
</p><p>88 Combining the three considerations is essential: Given an input image (a), we compute for each pixel its pattern distinctness (b) and its color distinctness (c). [sent-222, score-1.786]
</p><p>89 The two distinctness maps are combined (d) and then integrated with priors of image organization (e), to obtain our final saliency results in (f). [sent-223, score-1.016]
</p><p>90 It can be seen that while SVO [5] detects the salient regions, parts of the background are erroneously detected as salient. [sent-245, score-0.258]
</p><p>91 The CBS method [11] relies on shape priors and therefore often detects only parts of the salient objects (e. [sent-250, score-0.266]
</p><p>92 Our method integrates color and pattern distinctness, and hence captures both the outline, as well as the inner pixels of the salient objects. [sent-255, score-0.369]
</p><p>93 We do not make any assumptions on the shape of the salient regions, hence, we can handle convex as well as concave shapes. [sent-256, score-0.195]
</p><p>94 Conclusion Let’s go back to the title of this paper and ask ourselves what makes a patch distinct. [sent-258, score-0.196]
</p><p>95 In this paper we have shown that the statistics of patches in the image plays a central role in identifying the salient patches. [sent-259, score-0.338]
</p><p>96 We made use of the patch distribution for computing pattern distinctness via PCA. [sent-260, score-1.079]
</p><p>97 This is done by combining our novel pattern distinctness estimation with standard techniques for color uniqueness and organization priors. [sent-262, score-1.014]
</p><p>98 Fusing generic objectness and visual saliency for salient object detection. [sent-300, score-0.314]
</p><p>99 Automatic salient object segmentation based on context and shape prior. [sent-344, score-0.195]
</p><p>100 Design and perceptual validation ofperformance measures for salient object segmentation. [sent-385, score-0.195]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('distinctness', 0.83), ('px', 0.221), ('patch', 0.196), ('salient', 0.195), ('distinct', 0.127), ('statue', 0.12), ('patches', 0.116), ('saliency', 0.096), ('svo', 0.09), ('pca', 0.066), ('organization', 0.058), ('cntx', 0.058), ('msra', 0.057), ('asd', 0.055), ('color', 0.054), ('pattern', 0.053), ('principal', 0.053), ('cbs', 0.051), ('judd', 0.05), ('pa', 0.048), ('chni', 0.048), ('israel', 0.046), ('haifa', 0.045), ('slic', 0.044), ('path', 0.043), ('dashed', 0.043), ('golden', 0.043), ('drawings', 0.043), ('sod', 0.04), ('technion', 0.04), ('detects', 0.039), ('resolutions', 0.036), ('concentrated', 0.033), ('regions', 0.032), ('patterns', 0.032), ('components', 0.032), ('priors', 0.032), ('rx', 0.032), ('internal', 0.03), ('offers', 0.029), ('marked', 0.029), ('unique', 0.029), ('knn', 0.028), ('statistics', 0.027), ('pentium', 0.027), ('borji', 0.027), ('durand', 0.027), ('ern', 0.026), ('goferman', 0.026), ('morphological', 0.025), ('rc', 0.024), ('erroneously', 0.024), ('hence', 0.024), ('fuzzy', 0.024), ('integrates', 0.023), ('pat', 0.023), ('pages', 0.023), ('objectness', 0.023), ('benchmark', 0.022), ('maintaining', 0.022), ('homogeneous', 0.022), ('purple', 0.022), ('detection', 0.021), ('achanta', 0.021), ('inner', 0.02), ('auc', 0.02), ('average', 0.02), ('longer', 0.02), ('uniqueness', 0.019), ('flower', 0.019), ('considerations', 0.019), ('orange', 0.019), ('excluding', 0.018), ('solid', 0.018), ('cumulative', 0.018), ('evident', 0.018), ('colors', 0.018), ('animal', 0.018), ('detect', 0.017), ('wall', 0.017), ('tal', 0.017), ('yuan', 0.017), ('probable', 0.017), ('overlooks', 0.017), ('outshine', 0.017), ('phaet', 0.017), ('tated', 0.017), ('alyze', 0.017), ('pnadt', 0.017), ('lihi', 0.017), ('mediocre', 0.017), ('movahedi', 0.017), ('vissapp', 0.017), ('illustrated', 0.017), ('lines', 0.017), ('datasets', 0.017), ('outline', 0.016), ('compression', 0.016), ('kanan', 0.016), ('frequencytuned', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="464-tfidf-1" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>Author: Ran Margolin, Ayellet Tal, Lihi Zelnik-Manor</p><p>Abstract: What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns, others on the colors, and several add high-level cues and priors. We propose a simple, yet powerful, algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.</p><p>2 0.16448572 <a title="464-tfidf-2" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>3 0.16436969 <a title="464-tfidf-3" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>4 0.15808538 <a title="464-tfidf-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.1462187 <a title="464-tfidf-5" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>6 0.12986267 <a title="464-tfidf-6" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>7 0.10406669 <a title="464-tfidf-7" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>8 0.10076997 <a title="464-tfidf-8" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>9 0.097235762 <a title="464-tfidf-9" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>10 0.089458145 <a title="464-tfidf-10" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>11 0.088323772 <a title="464-tfidf-11" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>12 0.085496061 <a title="464-tfidf-12" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>13 0.083020061 <a title="464-tfidf-13" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>14 0.082458377 <a title="464-tfidf-14" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>15 0.082017764 <a title="464-tfidf-15" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>16 0.081532337 <a title="464-tfidf-16" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>17 0.077444926 <a title="464-tfidf-17" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>18 0.077148706 <a title="464-tfidf-18" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>19 0.076810397 <a title="464-tfidf-19" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>20 0.062796466 <a title="464-tfidf-20" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.115), (1, -0.056), (2, 0.16), (3, 0.094), (4, -0.034), (5, 0.005), (6, -0.01), (7, -0.018), (8, 0.012), (9, -0.021), (10, -0.011), (11, -0.004), (12, 0.009), (13, -0.012), (14, 0.032), (15, -0.015), (16, -0.01), (17, -0.014), (18, 0.052), (19, 0.026), (20, 0.024), (21, 0.075), (22, -0.048), (23, -0.098), (24, -0.033), (25, 0.002), (26, -0.013), (27, -0.088), (28, -0.011), (29, -0.034), (30, -0.034), (31, -0.063), (32, 0.04), (33, 0.009), (34, -0.021), (35, 0.021), (36, -0.038), (37, -0.016), (38, -0.052), (39, 0.003), (40, 0.076), (41, 0.013), (42, 0.055), (43, -0.012), (44, 0.059), (45, 0.106), (46, 0.009), (47, -0.031), (48, -0.032), (49, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92770559 <a title="464-lsi-1" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>Author: Ran Margolin, Ayellet Tal, Lihi Zelnik-Manor</p><p>Abstract: What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns, others on the colors, and several add high-level cues and priors. We propose a simple, yet powerful, algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.</p><p>2 0.73536259 <a title="464-lsi-2" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>Author: Maria Zontak, Inbar Mosseri, Michal Irani</p><p>Abstract: Recurrence of small clean image patches across different scales of a natural image has been successfully used for solving ill-posed problems in clean images (e.g., superresolution from a single image). In this paper we show how this multi-scale property can be extended to solve ill-posed problems under noisy conditions, such as image denoising. While clean patches are obscured by severe noise in the original scale of a noisy image, noise levels drop dramatically at coarser image scales. This allows for the unknown hidden clean patches to “naturally emerge ” in some coarser scale of the noisy image. We further show that patch recurrence across scales is strengthened when using directional pyramids (that blur and subsample only in one direction). Our statistical experiments show that for almost any noisy image patch (more than 99%), there exists a “good” clean version of itself at the same relative image coordinates in some coarser scale of the image.This is a strong phenomenon of noise-contaminated natural images, which can serve as a strong prior for separating the signal from the noise. Finally, incorporating this multi-scale prior into a simple denoising algorithm yields state-of-the-art denois- ing results.</p><p>3 0.70231634 <a title="464-lsi-3" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>Author: Xiaogang Chen, Sing Bing Kang, Jie Yang, Jingyi Yu</p><p>Abstract: Patch-based methods such as Non-Local Means (NLM) and BM3D have become the de facto gold standard for image denoising. The core of these approaches is to use similar patches within the image as cues for denoising. The operation usually requires expensive pair-wise patch comparisons. In this paper, we present a novel fast patch-based denoising technique based on Patch Geodesic Paths (PatchGP). PatchGPs treat image patches as nodes and patch differences as edge weights for computing the shortest (geodesic) paths. The path lengths can then be used as weights of the smoothing/denoising kernel. We first show that, for natural images, PatchGPs can be effectively approximated by minimum hop paths (MHPs) that generally correspond to Euclidean line paths connecting two patch nodes. To construct the denoising kernel, we further discretize the MHP search directions and use only patches along the search directions. Along each MHP, we apply a weightpropagation scheme to robustly and efficiently compute the path distance. To handle noise at multiple scales, we conduct wavelet image decomposition and apply PatchGP scheme at each scale. Comprehensive experiments show that our approach achieves comparable quality as the state-of-the-art methods such as NLM and BM3D but is a few orders of magnitude faster.</p><p>4 0.67616278 <a title="464-lsi-4" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>Author: Christian Scharfenberger, Alexander Wong, Khalil Fergani, John S. Zelek, David A. Clausi</p><p>Abstract: A novel statistical textural distinctiveness approach for robustly detecting salient regions in natural images is proposed. Rotational-invariant neighborhood-based textural representations are extracted and used to learn a set of representative texture atoms for defining a sparse texture model for the image. Based on the learnt sparse texture model, a weighted graphical model is constructed to characterize the statistical textural distinctiveness between all representative texture atom pairs. Finally, the saliency of each pixel in the image is computed based on the probability of occurrence of the representative texture atoms, their respective statistical textural distinctiveness based on the constructed graphical model, and general visual attentive constraints. Experimental results using a public natural image dataset and a variety of performance evaluation metrics show that the proposed approach provides interesting and promising results when compared to existing saliency detection methods.</p><p>5 0.66710329 <a title="464-lsi-5" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>Author: Jianchao Yang, Zhe Lin, Scott Cohen</p><p>Abstract: We propose a fast regression model for practical single image super-resolution based on in-place examples, by leveraging two fundamental super-resolution approaches— learning from an external database and learning from selfexamples. Our in-place self-similarity refines the recently proposed local self-similarity by proving that a patch in the upper scale image have good matches around its origin location in the lower scale image. Based on the in-place examples, a first-order approximation of the nonlinear mapping function from low- to high-resolution image patches is learned. Extensive experiments on benchmark and realworld images demonstrate that our algorithm can produce natural-looking results with sharp edges and preserved fine details, while the current state-of-the-art algorithms are prone to visual artifacts. Furthermore, our model can easily extend to deal with noise by combining the regression results on multiple in-place examples for robust estimation. The algorithm runs fast and is particularly useful for practical applications, where the input images typically contain diverse textures and they are potentially contaminated by noise or compression artifacts.</p><p>6 0.6412133 <a title="464-lsi-6" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>7 0.6319474 <a title="464-lsi-7" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>8 0.63110512 <a title="464-lsi-8" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>9 0.61743486 <a title="464-lsi-9" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>10 0.59390289 <a title="464-lsi-10" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>11 0.57814574 <a title="464-lsi-11" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>12 0.5751515 <a title="464-lsi-12" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>13 0.56268317 <a title="464-lsi-13" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>14 0.5597738 <a title="464-lsi-14" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>15 0.53682125 <a title="464-lsi-15" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>16 0.53425664 <a title="464-lsi-16" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>17 0.52513409 <a title="464-lsi-17" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>18 0.50837547 <a title="464-lsi-18" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>19 0.49831834 <a title="464-lsi-19" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>20 0.48037684 <a title="464-lsi-20" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.104), (16, 0.04), (26, 0.072), (28, 0.011), (33, 0.237), (59, 0.024), (67, 0.116), (69, 0.041), (87, 0.051), (95, 0.197)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8312124 <a title="464-lda-1" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>Author: Ran Margolin, Ayellet Tal, Lihi Zelnik-Manor</p><p>Abstract: What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns, others on the colors, and several add high-level cues and priors. We propose a simple, yet powerful, algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.</p><p>2 0.81777102 <a title="464-lda-2" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>Author: Weixin Li, Qian Yu, Harpreet Sawhney, Nuno Vasconcelos</p><p>Abstract: In this work, we propose a novel video representation for activity recognition that models video dynamics with attributes of activities. A video sequence is decomposed into short-term segments, which are characterized by the dynamics of their attributes. These segments are modeled by a dictionary of attribute dynamics templates, which are implemented by a recently introduced generative model, the binary dynamic system (BDS). We propose methods for learning a dictionary of BDSs from a training corpus, and for quantizing attribute sequences extracted from videos into these BDS codewords. This procedure produces a representation of the video as a histogram of BDS codewords, which is denoted the bag-of-words for attribute dynamics (BoWAD). An extensive experimental evaluation reveals that this representation outperforms other state-of-the-art approaches in temporal structure modeling for complex ac- tivity recognition.</p><p>3 0.8152678 <a title="464-lda-3" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>Author: Jianguo Li, Yimin Zhang</p><p>Abstract: This paper presents a novel learning framework for training boosting cascade based object detector from large scale dataset. The framework is derived from the wellknown Viola-Jones (VJ) framework but distinguished by three key differences. First, the proposed framework adopts multi-dimensional SURF features instead of single dimensional Haar features to describe local patches. In this way, the number of used local patches can be reduced from hundreds of thousands to several hundreds. Second, it adopts logistic regression as weak classifier for each local patch instead of decision trees in the VJ framework. Third, we adopt AUC as a single criterion for the convergence test during cascade training rather than the two trade-off criteria (false-positive-rate and hit-rate) in the VJ framework. The benefit is that the false-positive-rate can be adaptive among different cascade stages, and thus yields much faster convergence speed of SURF cascade. Combining these points together, the proposed approach has three good properties. First, the boosting cascade can be trained very efficiently. Experiments show that the proposed approach can train object detectors from billions of negative samples within one hour even on personal computers. Second, the built detector is comparable to the stateof-the-art algorithm not only on the accuracy but also on the processing speed. Third, the built detector is small in model-size due to short cascade stages.</p><p>4 0.81218415 <a title="464-lda-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.8114382 <a title="464-lda-5" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>Author: Magnus Burenius, Josephine Sullivan, Stefan Carlsson</p><p>Abstract: We consider the problem of automatically estimating the 3D pose of humans from images, taken from multiple calibrated views. We show that it is possible and tractable to extend the pictorial structures framework, popular for 2D pose estimation, to 3D. We discuss how to use this framework to impose view, skeleton, joint angle and intersection constraints in 3D. The 3D pictorial structures are evaluated on multiple view data from a professional football game. The evaluation is focused on computational tractability, but we also demonstrate how a simple 2D part detector can be plugged into the framework.</p><p>6 0.81029743 <a title="464-lda-6" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>7 0.80686611 <a title="464-lda-7" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>8 0.80668461 <a title="464-lda-8" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>9 0.80666912 <a title="464-lda-9" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>10 0.8063156 <a title="464-lda-10" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>11 0.80600542 <a title="464-lda-11" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>12 0.80458236 <a title="464-lda-12" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>13 0.80445826 <a title="464-lda-13" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>14 0.8034485 <a title="464-lda-14" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>15 0.80278802 <a title="464-lda-15" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>16 0.80267137 <a title="464-lda-16" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>17 0.80251759 <a title="464-lda-17" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>18 0.80198264 <a title="464-lda-18" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>19 0.80188972 <a title="464-lda-19" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>20 0.80069196 <a title="464-lda-20" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
