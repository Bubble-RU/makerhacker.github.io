<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-452" href="#">cvpr2013-452</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</h1>
<br/><p>Source: <a title="cvpr-2013-452-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sfar_Vantage_Feature_Frames_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman</p><p>Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependentfeatures computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.</p><p>Reference: <a title="cvpr-2013-452-reference" href="../cvpr2013_reference/cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , birds), focusing on determining botanical species (leaves and orchids) from scanned images. [sent-8, score-0.748]
</p><p>2 The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. [sent-9, score-0.461]
</p><p>3 Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. [sent-12, score-0.485]
</p><p>4 As a result, methods designed for generic categories are usually not well-adapted to isolating and representing the specific information necessary for discriminating among fine-grained visual categories such as species of leaves. [sent-18, score-0.719]
</p><p>5 , species or varieties) are often determined by subtle differ-  ences in shape and texture. [sent-24, score-0.522]
</p><p>6 And whereas the overall shapes may be sufficiently different to distinguish between some species (see e. [sent-27, score-0.545]
</p><p>7 , Figure 1 (a) and (g)), other species may display only subtle differences: see the instances of two different species of orchids in Figure 1 (e) and (f) and two instances of leaves in Figure 1 (h) and (i) from two different genera (and hence species). [sent-29, score-1.761]
</p><p>8 Our approach to categorizing botanical species is motivated by the strategy used by botanists, in which attention is focused on visual properties of the object in the vicinity of a small number of distinguished landmarks. [sent-30, score-0.708]
</p><p>9 Whereas these landmarks are the same for each species, it is the local features which permit disambiguation. [sent-31, score-0.168]
</p><p>10 The vehicle for translating this into a computer vision algorithm, and our main contribution, is the notion of a vantage feature frame; we provide algorithms for learning discriminating ones, detecting them online and pooling the features computed in these frames to identify the categories. [sent-33, score-0.611]
</p><p>11 We refer to the origins of the frames as vantage points - special locations from which observing the leaf or flower and in a particular direction can provide discriminating information about the species. [sent-34, score-0.826]
</p><p>12 Besides location, scale and orientation,  each vantage feature frame is also equipped with a possi888883333355333  Figure 1. [sent-35, score-0.435]
</p><p>13 Intra-class similarity and inter-class differences for leaves and orchids. [sent-36, score-0.246]
</p><p>14 We will demonstrate that this type of non-distributed representation can be highly effective in distinguishing between closely-related categories, and in particular improves upon the accuracy of existing methods for simple leaves on standard databases. [sent-41, score-0.27]
</p><p>15 Related work There is a growing body of work investigating finegrained image classification of birds [8, 24, 27], insects [15, 18], flowers [6, 19] and leaves [1, 6, 14]. [sent-43, score-0.38]
</p><p>16 Several shape-based approaches, including boundary analyses, have been adapted for fine-grained categorization, especially for leaves [1, 4, 17]. [sent-44, score-0.268]
</p><p>17 Our work on leaves and orchids is somewhat similar in that the detection of the vantage frames primes the identification of the botanical species; however, unlike the work in [9, 28], our representation is based on frames not poselets, i. [sent-57, score-1.274]
</p><p>18 Of course birds and leaves present different kinds of challenges; the former exhibit higher intra-species variation (e. [sent-62, score-0.331]
</p><p>19 In the botanical applications whi}ch d emnoottieva Nte cthatise gworoierks. [sent-72, score-0.156]
</p><p>20 s Iunch t as assigning a species to a scanned image of a leaf, there is often useful domain knowledge, for instance named landmarks L = {l1, . [sent-74, score-0.738]
</p><p>21 , lK} around which botanists focus in order to separate one species furnodm w wanhiocthhe br. [sent-77, score-0.6]
</p><p>22 In fact, landmarks are more like “vantage points” in  that orientation plays a role as well, in other words, where the landmarks are in relation to one another. [sent-80, score-0.325]
</p><p>23 Naturally, species tend to have certain signature appearance properties and consequently what to look for in the neighborhood ofthe landmarks may be species-dependent. [sent-81, score-0.688]
</p><p>24 , higher taxa than species such as family or genus) to the species level. [sent-87, score-1.091]
</p><p>25 With these considerations in mind, a vantage feature frame F has two components. [sent-89, score-0.435]
</p><p>26 d T simply a elotrciacl ccoomorpdoi-nate system centered at one of the landmarks l; the scale and orientation are discussed below. [sent-92, score-0.199]
</p><p>27 Learning the frames Learning the most discriminating frames from scratch would evidently be a major challenge, and we do not attempt this. [sent-105, score-0.232]
</p><p>28 , scanned images of leaves) we declare the orientation of the frame to be determined by the centroid of the object, that is, the landmark points to the centroid, and the unit distance to be the approximate scale of the object. [sent-113, score-0.272]
</p><p>29 The choice of landmarks or vantage points is performancebased. [sent-114, score-0.573]
</p><p>30 Assume we are given a classifier for each set of vantage feature frames; our particular choice is described in §6. [sent-115, score-0.407]
</p><p>31 However, for simple leaves and orchids only three “universal” landmarks L = {l1, l2, l3} have been suggested by vbeotrasnali”sts la; they are d Les =crib {eld in §6. [sent-118, score-0.625]
</p><p>32 As a result, the errors that are inevitably made in automatically detecting the landmarks (see Section §6. [sent-126, score-0.174]
</p><p>33 The best performance is obtained with two frames corresponding to apex and base of the leaf. [sent-131, score-0.251]
</p><p>34 The scale is taken to be the radius of the bounding circle as illustrated for leaves in Figure 5. [sent-136, score-0.275]
</p><p>35 The landmarks are detected by dedicated classifiers trained on manually annotated images. [sent-137, score-0.178]
</p><p>36 Since we are only using landmarks on the object boundaries (as determined by the segmentation process), we restrict the search to a sample of boundary points to minimize the computation. [sent-138, score-0.188]
</p><p>37 In order to detect each vantage point, a classifier (see §5) based on SVM scores is built from positive and neg-  a§t5i)ve b training examples. [sent-140, score-0.433]
</p><p>38 The features for SVM learning are defined in the local coordinate system centered on the candidate landmarks (i. [sent-142, score-0.218]
</p><p>39 Basically, given a frame consisting of two distinguished points and a distinguished scale, there is a candidate feature X = X(w, j) for each (local) window w in frame coordinates and for each local image property j: the feature X is just the property histogram in w. [sent-146, score-0.166]
</p><p>40 Whereas we use the same class of features to learn landmark detectors, we construct a separate binary classifier for each category Ct for distinguishing that category from all others and which employs a learned subset of features Xt. [sent-152, score-0.181]
</p><p>41 Cross-validated recognition rates for leaves (from the Smithsonian database) for each of seven possible sets of frames sets with centers l1, l2, l3. [sent-161, score-0.362]
</p><p>42 The best result (in bold) is obtained with two frames centered at the base l1 and apex l3. [sent-162, score-0.271]
</p><p>43 of leaves in the neighborhood of landmarks that some features are far more discriminating than others, and the discriminating ones can depend as well on the vantage point. [sent-163, score-0.993]
</p><p>44 For example, the discriminating features around the leaf base for estimating the genera might be different from those around the apex for estimating either the genera or the species; and the best features in any given frame may be genus- and species-dependent. [sent-164, score-0.898]
</p><p>45 Figure 3 illustrates the recognition rate for leaf genera for various M. [sent-171, score-0.411]
</p><p>46 For instance, we achieve over 75% recognition rate of leaf genus while considering only the first genus returned and using between about 500 and 2500 categorydependent features against only 67% without any selection i. [sent-173, score-0.522]
</p><p>47 Recognition rates for leaf genera from the Smithsonian data (see §6. [sent-182, score-0.421]
</p><p>48 Fine-grained categorization As indicated above, the category identification is also hierarchical, coarse-grained to fine-grained, which is another way of exploiting domain knowledge. [sent-186, score-0.155]
</p><p>49 In our experiments we consider two-levels, the first for genera and second for the species, the ultimate target. [sent-187, score-0.194]
</p><p>50 3d, ratio:  Lt(I) =PP((FFtt== FFtt((II))||II ∈∈/ CCtt)) Several detected genera may be considered for species identification. [sent-196, score-0.716]
</p><p>51 If Ct corresponds to a genus, we define ft(I) =  01 ifel lsoeg(Lt(I)) > ρ  Here, ρ is a threshoßld used to control the false negative genus rate, that is to ßallow only a very small number of instances in which I ßCt but ft (I) = 0 (missed detections). [sent-197, score-0.197]
</p><p>52 Hence only the classifiers for species which belong to the retained genera are performed. [sent-205, score-0.749]
</p><p>53 Finally, those species for which ft(I) = 1( where t is the node for  the genus of the species) are then sorted according to their likelihood ratios. [sent-206, score-0.694]
</p><p>54 This is illustrated in Figure 4, which shows two pairs of distributions for two classes of leaf species C1 and C2. [sent-210, score-0.711]
</p><p>55 Comparison between the SVM score distributions of two different genera from the Smithsonian database (see §6. [sent-217, score-0.194]
</p><p>56 Note that the same framework was used to learn the vantage point detectors i. [sent-226, score-0.407]
</p><p>57 However, for those points only a single estimate is retained, namely the one corresponding to the candidate at which the likelihood ratio is maximized and thus a single classifier (and thus a single SVM) is learned to detect each vantage point. [sent-229, score-0.477]
</p><p>58 Experiments In this section we describe the landmarks for leaves and orchids, the datasets we have used to evaluate our approach, and compare our results with those previously obtained. [sent-231, score-0.392]
</p><p>59 Botanical landmarks To analyze leaves, experts usually focus on the apex, the base and the leaf margin, whereas an orchid specialist focuses on the sepals, petals and the labellum. [sent-234, score-0.528]
</p><p>60 Let l1 denote the leaf apex (respectively, the central sepal for orchids), l2 the first intersection point between the perpendicular to the apex-base line throughout the centroid of the blade and the leaf boundary (resp. [sent-236, score-0.638]
</p><p>61 , the petal on the right of l1for orchids) and l3 the leaf base (resp. [sent-237, score-0.242]
</p><p>62 Note that for leaves, the centroid corresponds to the center of mass of the blade; the leaf petiole is removed before computing the centroid (see Figure 5). [sent-239, score-0.36]
</p><p>63 Figure 5 illustrates the vantage point detection process for a leaf image, namely the leaf base and the leaf apex detection. [sent-241, score-1.147]
</p><p>64 removed in order to compute the centroid (green point) as well as the approximate bounding circle of the leaf blade (red dashed circle). [sent-242, score-0.358]
</p><p>65 The base (blue point) and the apex (red point) are estimated using learned classifiers (f1, f2). [sent-243, score-0.173]
</p><p>66 The proposed locations for both landmarks are restricted to the boundary points. [sent-244, score-0.168]
</p><p>67 The neighborhood  of the first landmark detected is excluded from the list of candidate points for the next detection (blue dashed circle). [sent-245, score-0.171]
</p><p>68 Smithsonian: This dataset has 5466 simple-leaf images containing 148 different species from the Northeastern U. [sent-250, score-0.522]
</p><p>69 The number of exemplars per species varies from 2 to 63. [sent-252, score-0.522]
</p><p>70 These images were provided by the Smithsonian botanical institution within the framework of the US National Herbarium. [sent-253, score-0.156]
</p><p>71 One particularity of these data is that the images present various poses and orientations of leaves as well as different structures of basal and apical parts as shown in Figure 6. [sent-254, score-0.246]
</p><p>72 Swedish: This is the subset of simple leaf images of the first publicly available leaf data for research, introduced by the authors of [22]. [sent-256, score-0.378]
</p><p>73 It has 975 images containing 75 images from each of 13 different Swedish simple species (af8 8 83 3 39 7 7  ter removing the two compound species from the original dataset[? [sent-257, score-1.091]
</p><p>74 It is composed of 46 species from the French Mediterranean and was constructed through a citizen science initiative conducted by Telabotanica2, a French social network of amateur and expert botanists. [sent-259, score-0.541]
</p><p>75 Orchids3 : There are 1610 images representing 23 species of a relatively rare orchid flower family provided by the ”Mediterranean Orchid Society” (Soci ´et e´ M ´editerran e´enne d’Orchidologie). [sent-262, score-0.715]
</p><p>76 Note that the color is not a discriminative feature; many differently colored orchids could belong to the same genus or species. [sent-266, score-0.362]
</p><p>77 Detection results First, we present the results of the vantage point detection for all the data introduced in the previous section in Table 2, achieving over 90% accuracy in each case and thereby confirming reasonable invariance to shape and structure. [sent-272, score-0.407]
</p><p>78 Figure 8 shows vantage point detection results for orchids and different type of leaves (e. [sent-273, score-0.886]
</p><p>79 Identification results To evaluate the performance of species identification, we provide the rate on the holdout test data at which the true  species appears among our top n estimates for n = 1, . [sent-278, score-1.072]
</p><p>80 org 3Courtesy of Roland Martin and Errol Vela  tage points for both Smithsonian leaves and orchids. [sent-285, score-0.266]
</p><p>81 Note that the entire detection process is considered erroneous if any vantage point is not accurately detected. [sent-287, score-0.407]
</p><p>82 Rate of well detected vantage points  n12345  SOmrcithhisdo dnaitana data8719%%9826%%9849%%9906%%9917%% Table 3. [sent-289, score-0.427]
</p><p>83 Recognition rates using vantage feature frames on both Smithsonian leaves and Orchids for the Smithsonian, Swedish and orchid subsets. [sent-290, score-0.886]
</p><p>84 genera apnicda allbyou setl e15ct00 a bfeoua-t tures for the species. [sent-294, score-0.194]
</p><p>85 We achieve 79% accuracy for the top-ranked species (n = 1) and 91% for n = 5. [sent-297, score-0.522]
</p><p>86 Of particular note is the similarity between the appearance of the true and estimated species in the misclassified cases and the impact of poor vantage point estimation; for example, note the errors in estimating both the base and the apex for the third test leaf in Figure 9. [sent-299, score-1.291]
</p><p>87 In [1], the IDSC was used with a KNN classifier to identify the species within two subsets of the Smithsonian database, achieving a recognition rate of 60% − 70% 888884333400888  points and the top three species returned by our algorithm. [sent-300, score-1.117]
</p><p>88 For each test image, the red point refers to the estimated leaf apex and the blue point to the estimated leaf base. [sent-301, score-0.498]
</p><p>89 The examples framed in green come from the same species as the test image. [sent-302, score-0.549]
</p><p>90 Consequently, we applied the IDSC method to our subset of simple leaves from the Smithsonian dataset using the same parameters as in [1, 17]. [sent-305, score-0.246]
</p><p>91 Swedish Data: We also compare our results with the IDSC on the Swedish leaves [22] since the IDSC achieved better results than other methods on this dataset according to [17]. [sent-308, score-0.246]
</p><p>92 However, it should be noted that compound leaves have very different characteristics than simple leaves. [sent-316, score-0.293]
</p><p>93 In particular, they exhibit greater inter-species variation, and thus identifying the species of compound leaves is easier. [sent-317, score-0.839]
</p><p>94 ImageCLEF2011 Data: Finally, we compare our method  with the entries to the ImageCLEF201 1plant identification task on the scanned simple leaves (46 species). [sent-319, score-0.392]
</p><p>95 Classification scores on the scanned simple leaves of the ImageCLEF201 1 dataset. [sent-322, score-0.342]
</p><p>96 We applied the vantage feature frame approach on this data to demonstrate how it could be readily applied to a different type ofcloselyrelated botanical species. [sent-329, score-0.591]
</p><p>97 We achieve 81% accuracy for the top-ranked species (n = 1) and 97% for n = 5 as shown in the second row of Table 3. [sent-330, score-0.522]
</p><p>98 Conclusion We have introduced a novel approach for fine-grained categorization using the concept of vantage feature frames. [sent-332, score-0.457]
</p><p>99 The different characteristics of these frames, namely, the geometric and the appearance-based components, combine to provide the cues needed to distinguish between closelyrelated categories such as botanical species. [sent-333, score-0.201]
</p><p>100 Future work is aimed at applications involving cluttered backgrounds and at automatically determining candidate landmarks for constructing the vantage feature frames. [sent-335, score-0.583]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('species', 0.522), ('vantage', 0.407), ('leaves', 0.246), ('orchids', 0.233), ('smithsonian', 0.214), ('genera', 0.194), ('leaf', 0.189), ('botanical', 0.156), ('landmarks', 0.146), ('genus', 0.129), ('swedish', 0.128), ('apex', 0.12), ('orchid', 0.117), ('idsc', 0.112), ('taxonomic', 0.096), ('frames', 0.078), ('botanists', 0.078), ('discriminating', 0.076), ('identification', 0.076), ('scanned', 0.07), ('centroid', 0.066), ('birds', 0.061), ('plant', 0.059), ('mediterranean', 0.058), ('landmark', 0.055), ('base', 0.053), ('blade', 0.052), ('categorization', 0.05), ('compound', 0.047), ('family', 0.047), ('ft', 0.046), ('categories', 0.045), ('asma', 0.039), ('clef', 0.039), ('eau', 0.039), ('fftt', 0.039), ('ilex', 0.039), ('larios', 0.039), ('lytle', 0.039), ('moldenke', 0.039), ('mortensen', 0.039), ('nozha', 0.039), ('ophrys', 0.039), ('paasch', 0.039), ('petiole', 0.039), ('rates', 0.038), ('ct', 0.037), ('svm', 0.036), ('saclay', 0.034), ('palaiseau', 0.034), ('orientation', 0.033), ('retained', 0.033), ('dedicated', 0.032), ('otsu', 0.032), ('shapiro', 0.032), ('insects', 0.032), ('boujemaa', 0.032), ('inria', 0.031), ('generic', 0.031), ('candidate', 0.03), ('distinctions', 0.03), ('distinguished', 0.03), ('circle', 0.029), ('xt', 0.029), ('flower', 0.029), ('french', 0.029), ('category', 0.029), ('frame', 0.028), ('rate', 0.028), ('detecting', 0.028), ('geman', 0.028), ('farrell', 0.028), ('framed', 0.027), ('origins', 0.027), ('poselets', 0.026), ('scores', 0.026), ('pages', 0.025), ('returned', 0.025), ('ping', 0.025), ('exhibit', 0.024), ('distinguishing', 0.024), ('sub', 0.024), ('list', 0.024), ('node', 0.023), ('whereas', 0.023), ('instances', 0.022), ('participants', 0.022), ('flowers', 0.022), ('boundary', 0.022), ('features', 0.022), ('dashed', 0.022), ('ling', 0.021), ('france', 0.021), ('centered', 0.02), ('points', 0.02), ('neighborhood', 0.02), ('likelihood', 0.02), ('usa', 0.02), ('expert', 0.019), ('classification', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="452-tfidf-1" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>Author: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman</p><p>Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependentfeatures computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.</p><p>2 0.26573202 <a title="452-tfidf-2" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: From a set ofimages in aparticular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.</p><p>3 0.23998752 <a title="452-tfidf-3" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>Author: Anelia Angelova, Shenghuo Zhu</p><p>Abstract: We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also ‘zoom in ’ on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios [4, 21]. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging largescale flower dataset, containing 578 species of flowers and 250,000 images.</p><p>4 0.08475294 <a title="452-tfidf-4" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>Author: Baoyuan Liu, Fereshteh Sadeghi, Marshall Tappen, Ohad Shamir, Ce Liu</p><p>Abstract: Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. In this paper, we show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p><p>5 0.084444195 <a title="452-tfidf-5" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>Author: Subhransu Maji, Gregory Shakhnarovich</p><p>Abstract: We study the problem of part discovery when partial correspondence between instances of a category are available. For visual categories that exhibit high diversity in structure such as buildings, our approach can be used to discover parts that are hard to name, but can be easily expressed as a correspondence between pairs of images. Parts naturally emerge from point-wise landmark matches across many instances within a category. We propose a learning framework for automatic discovery of parts in such weakly supervised settings, and show the utility of the rich part library learned in this way for three tasks: object detection, category-specific saliency estimation, and fine-grained image parsing.</p><p>6 0.076046325 <a title="452-tfidf-6" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>7 0.070103623 <a title="452-tfidf-7" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>8 0.068485975 <a title="452-tfidf-8" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>9 0.060491487 <a title="452-tfidf-9" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>10 0.057789247 <a title="452-tfidf-10" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>11 0.051867362 <a title="452-tfidf-11" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>12 0.050032232 <a title="452-tfidf-12" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>13 0.049973778 <a title="452-tfidf-13" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>14 0.049450055 <a title="452-tfidf-14" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>15 0.049279153 <a title="452-tfidf-15" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>16 0.047868162 <a title="452-tfidf-16" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>17 0.047707766 <a title="452-tfidf-17" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>18 0.046562143 <a title="452-tfidf-18" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>19 0.045815438 <a title="452-tfidf-19" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>20 0.043578282 <a title="452-tfidf-20" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, -0.036), (2, -0.005), (3, -0.016), (4, 0.052), (5, 0.015), (6, -0.019), (7, 0.009), (8, 0.028), (9, -0.006), (10, -0.025), (11, -0.006), (12, 0.022), (13, -0.023), (14, 0.018), (15, -0.051), (16, 0.014), (17, -0.03), (18, 0.006), (19, 0.005), (20, 0.009), (21, 0.045), (22, 0.032), (23, -0.02), (24, 0.019), (25, 0.097), (26, 0.08), (27, 0.047), (28, -0.03), (29, -0.024), (30, -0.01), (31, 0.072), (32, -0.026), (33, 0.027), (34, 0.089), (35, 0.023), (36, 0.014), (37, -0.071), (38, -0.039), (39, 0.108), (40, -0.053), (41, 0.027), (42, -0.069), (43, -0.038), (44, 0.021), (45, 0.02), (46, 0.04), (47, 0.092), (48, -0.112), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86945778 <a title="452-lsi-1" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>Author: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman</p><p>Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependentfeatures computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.</p><p>2 0.74054211 <a title="452-lsi-2" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>Author: Thomas Berg, Peter N. Belhumeur</p><p>Abstract: From a set ofimages in aparticular domain, labeled with part locations and class, we present a method to automatically learn a large and diverse set of highly discriminative intermediate features that we call Part-based One-vs-One Features (POOFs). Each of these features specializes in discrimination between two particular classes based on the appearance at a particular part. We demonstrate the particular usefulness of these features for fine-grained visual categorization with new state-of-the-art results on bird species identification using the Caltech UCSD Birds (CUB) dataset and parity with the best existing results in face verification on the Labeled Faces in the Wild (LFW) dataset. Finally, we demonstrate the particular advantage of POOFs when training data is scarce.</p><p>3 0.68521154 <a title="452-lsi-3" href="./cvpr-2013-Efficient_Object_Detection_and_Segmentation_for_Fine-Grained_Recognition.html">145 cvpr-2013-Efficient Object Detection and Segmentation for Fine-Grained Recognition</a></p>
<p>Author: Anelia Angelova, Shenghuo Zhu</p><p>Abstract: We propose a detection and segmentation algorithm for the purposes of fine-grained recognition. The algorithm first detects low-level regions that could potentially belong to the object and then performs a full-object segmentation through propagation. Apart from segmenting the object, we can also ‘zoom in ’ on the object, i.e. center it, normalize it for scale, and thus discount the effects of the background. We then show that combining this with a state-of-the-art classification algorithm leads to significant improvements in performance especially for datasets which are considered particularly hard for recognition, e.g. birds species. The proposed algorithm is much more efficient than other known methods in similar scenarios [4, 21]. Our method is also simpler and we apply it here to different classes of objects, e.g. birds, flowers, cats and dogs. We tested the algorithm on a number of benchmark datasets for fine-grained categorization. It outperforms all the known state-of-the-art methods on these datasets, sometimes by as much as 11%. It improves the performance of our baseline algorithm by 3-4%, consistently on all datasets. We also observed more than a 4% improvement in the recognition performance on a challenging largescale flower dataset, containing 578 species of flowers and 250,000 images.</p><p>4 0.62608117 <a title="452-lsi-4" href="./cvpr-2013-Fine-Grained_Crowdsourcing_for_Fine-Grained_Recognition.html">174 cvpr-2013-Fine-Grained Crowdsourcing for Fine-Grained Recognition</a></p>
<p>Author: Jia Deng, Jonathan Krause, Li Fei-Fei</p><p>Abstract: Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles ” that reveals discriminative features humans use. The player’s goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ( “bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.</p><p>5 0.57488221 <a title="452-lsi-5" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>Author: Hua Wang, Feiping Nie, Heng Huang, Chris Ding</p><p>Abstract: To better understand, search, and classify image and video information, many visual feature descriptors have been proposed to describe elementary visual characteristics, such as the shape, the color, the texture, etc. How to integrate these heterogeneous visual features and identify the important ones from them for specific vision tasks has become an increasingly critical problem. In this paper, We propose a novel Sparse Multimodal Learning (SMML) approach to integrate such heterogeneous features by using the joint structured sparsity regularizations to learn the feature importance of for the vision tasks from both group-wise and individual point of views. A new optimization algorithm is also introduced to solve the non-smooth objective with rigorously proved global convergence. We applied our SMML method to five broadly used object categorization and scene understanding image data sets for both singlelabel and multi-label image classification tasks. For each data set we integrate six different types of popularly used image features. Compared to existing scene and object cat- egorization methods using either single modality or multimodalities of features, our approach always achieves better performances measured.</p><p>6 0.56259215 <a title="452-lsi-6" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>7 0.56092012 <a title="452-lsi-7" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>8 0.54903257 <a title="452-lsi-8" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>9 0.52119279 <a title="452-lsi-9" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>10 0.51078397 <a title="452-lsi-10" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>11 0.49619943 <a title="452-lsi-11" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>12 0.48869047 <a title="452-lsi-12" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>13 0.48501697 <a title="452-lsi-13" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>14 0.48294255 <a title="452-lsi-14" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>15 0.46647665 <a title="452-lsi-15" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>16 0.4622609 <a title="452-lsi-16" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>17 0.46163484 <a title="452-lsi-17" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>18 0.45976105 <a title="452-lsi-18" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>19 0.4551411 <a title="452-lsi-19" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>20 0.45420608 <a title="452-lsi-20" href="./cvpr-2013-Seeking_the_Strongest_Rigid_Detector.html">383 cvpr-2013-Seeking the Strongest Rigid Detector</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.115), (16, 0.022), (18, 0.291), (26, 0.047), (27, 0.017), (28, 0.011), (33, 0.221), (67, 0.048), (69, 0.051), (77, 0.016), (87, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78042191 <a title="452-lda-1" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>Author: Laurent Sifre, Stéphane Mallat</p><p>Abstract: An affine invariant representation is constructed with a cascade of invariants, which preserves information for classification. A joint translation and rotation invariant representation of image patches is calculated with a scattering transform. It is implemented with a deep convolution network, which computes successive wavelet transforms and modulus non-linearities. Invariants to scaling, shearing and small deformations are calculated with linear operators in the scattering domain. State-of-the-art classification results are obtained over texture databases with uncontrolled viewing conditions.</p><p>same-paper 2 0.76740134 <a title="452-lda-2" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>Author: Asma Rejeb Sfar, Nozha Boujemaa, Donald Geman</p><p>Abstract: We study fine-grained categorization, the task of distinguishing among (sub)categories of the same generic object class (e.g., birds), focusing on determining botanical species (leaves and orchids) from scanned images. The strategy is to focus attention around several vantage points, which is the approach taken by botanists, but using features dedicated to the individual categories. Our implementation of the strategy is based on vantage feature frames, a novel object representation consisting of two components: a set of coordinate systems centered at the most discriminating local viewpoints for the generic object class and a set of category-dependentfeatures computed in these frames. The features are pooled over frames to build the classifier. Categorization then proceeds from coarse-grained (finding the frames) to fine-grained (finding the category), and hence the vantage feature frames must be both detectable and discriminating. The proposed method outperforms state-of-the art algorithms, in particular those using more distributed representations, on standard databases of leaves.</p><p>3 0.69433981 <a title="452-lda-3" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>4 0.69264084 <a title="452-lda-4" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>5 0.67486399 <a title="452-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.67080158 <a title="452-lda-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.67027551 <a title="452-lda-7" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>8 0.66884786 <a title="452-lda-8" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>9 0.66847152 <a title="452-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.66846764 <a title="452-lda-10" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>11 0.66821331 <a title="452-lda-11" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>12 0.66802692 <a title="452-lda-12" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>13 0.66790062 <a title="452-lda-13" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>14 0.66763258 <a title="452-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.66748875 <a title="452-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.66725665 <a title="452-lda-16" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>17 0.6669023 <a title="452-lda-17" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>18 0.66645896 <a title="452-lda-18" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>19 0.66644704 <a title="452-lda-19" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>20 0.6664235 <a title="452-lda-20" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
