<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>282 cvpr-2013-Measuring Crowd Collectiveness</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-282" href="#">cvpr2013-282</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>282 cvpr-2013-Measuring Crowd Collectiveness</h1>
<br/><p>Source: <a title="cvpr-2013-282-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhou_Measuring_Crowd_Collectiveness_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>Reference: <a title="cvpr-2013-282-reference" href="../cvpr2013_reference/cvpr-2013-Measuring_Crowd_Collectiveness_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. [sent-9, score-1.022]
</p><p>2 By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. [sent-10, score-1.702]
</p><p>3 The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. [sent-11, score-0.565]
</p><p>4 We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. [sent-12, score-0.761]
</p><p>5 We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. [sent-13, score-1.303]
</p><p>6 Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1. [sent-14, score-2.298]
</p><p>7 From bacterial colonies and insect swarms to fish shoal, collective motions widely exist in different crowd systems and reflect the ordered macroscopic behaviors of constituent individuals. [sent-17, score-1.131]
</p><p>8 Since individuals in a crowd system only coordinate their behaviors in their neighborhood, individuals at a distance may have low velocity correlation even though they are on the same collective manifold (Such as the red individual and the green individual. [sent-31, score-1.297]
</p><p>9 Thus, accurately measuring the collectiveness of crowd and its constituent individuals are challenging. [sent-34, score-1.282]
</p><p>10 nate their behaviors with their neighbors then the crowd is self-organized into collective motion without external control [15, 17]. [sent-35, score-0.94]
</p><p>11 Meanwhile, animal aggregation is considered as an evolutionary advantage for species survival, since the integrated whole of individuals can generate complex patterns, quickly process information, and engage in collective decision-making [6]. [sent-36, score-0.653]
</p><p>12 One remarkable observation of collective motions in different crowd systems is that some spatially coherent structures often emerge from the movements of crowd, such as the arch-like macro structures in the human crowd, the fish shoal and the bacterial colony shown in Fig. [sent-37, score-1.135]
</p><p>13 We refer to  the spatially coherent structure of collective motion as Collective Manifold. [sent-39, score-0.545]
</p><p>14 1 illustrates one important structural property of collective manifold: behavior consistency remains high among individuals in local neighborhood, but 333000444977  low among those that are far apart, even on the same collective manifold. [sent-41, score-1.195]
</p><p>15 Some empirical studies have explored the importance of topological relations and information transmission among neighboring individuals in crowd [2]. [sent-43, score-0.536]
</p><p>16 Collectiveness describes the degree of individuals acting as a union in collective motion. [sent-45, score-0.644]
</p><p>17 Quantitatively measuring this universal property and comparing it across different crowd systems are important in order to understand the general principles of various crowd behaviors. [sent-47, score-0.762]
</p><p>18 Most existing  crowd surveillance technologies [12, 27] cannot compare crowd behaviors across different scenes because they lack universal descriptors with which to characterize crowd dynamics. [sent-49, score-1.094]
</p><p>19 Monitoring collectiveness is also useful in crowd management, control of swarming desert locusts [5], prevention of disease spreading [22], and many other fields. [sent-50, score-1.067]
</p><p>20 Existing works [3, 19] simply measured the average velocity of all the individuals to indicate the collectiveness of the whole crowd, which is neither accurate nor robust. [sent-52, score-0.914]
</p><p>21 The collectiveness of individuals in crowd is also illdefined. [sent-53, score-1.216]
</p><p>22 In this paper, by quantifying the topological properties of collective manifold of crowds, we propose a descriptor of collectiveness for crowd systems as well as their constituent individuals. [sent-54, score-1.676]
</p><p>23 Based on collectiveness, an algorithm called Collective Merging is proposed to detect collective motions from random motions. [sent-55, score-0.565]
</p><p>24 We validate the effectiveness and robustness of the proposed collectiveness on selfdriven particles [19]. [sent-56, score-0.826]
</p><p>25 It is further compared to human motion perception for collective motion on a new video dataset with ground-truth. [sent-57, score-0.604]
</p><p>26 In addition, our experiments of detecting collective motions and measuring crowd collectiveness in videos of pedestrian crowds and bacterial colony demonstrate the wide applications of the collectiveness descriptor. [sent-58, score-2.617]
</p><p>27 Related Works Scientific studies on collective motion in crowd system-  s can be categorized as empirical or theoretical; a compact review can be found in [20]. [sent-61, score-0.885]
</p><p>28 However, none of the abovementioned measured the collectiveness of crowd behaviors or explored its potential applications. [sent-80, score-1.122]
</p><p>29 Under certain circumstances, individuals in a crowd are organized into a unity with different levels of collective motions. [sent-83, score-0.983]
</p><p>30 Thus, crowd collectiveness should be determined by the collectiveness of its constituent individuals, which reflects the similarity of the individual’s behavior to others in the same crowd system. [sent-84, score-2.219]
</p><p>31 We introduce collectiveness in a bottom-up manner: from behavior consistency in neighborhoods of individuals to that among all pairwise individuals, then from individual collectiveness to crowd collectiveness. [sent-85, score-2.026]
</p><p>32 A better behavior consistency based on the structural property of collective manifold is proposed below. [sent-97, score-0.588]
</p><p>33 Individual Collectiveness from Path Similarity Since l-path similarity νl (i, j) measures the behavior consistency between iand j at l-path scale, we define the individual collectiveness of individual iat l-path scale as φl(i) =  ? [sent-126, score-0.867]
</p><p>34 To further measure crowd collectiveness, we should integrate the individual collectiveness at all path scales; that is, {φ1 , . [sent-133, score-1.133]
</p><p>35 Individual collectiveness at lower l-path scales make a greater contribution to the overall individual collectiveness. [sent-151, score-0.756]
</p><p>36 Individual collectiveness from the generating function regularization on all the path similarities can be written as ? [sent-170, score-0.778]
</p><p>37 The summation of regularized individual collectiveness from all path scales converges. [sent-179, score-0.794]
</p><p>38 The crowd collectiveness of a crowd system C is then defineTdh as trhoew mean oecft aivl e tnhees sin odfi avi cdruoawl cdo syllsetcetimve Cne isss t,h wenh dicehcan be explicitly written in a closed form as  Φ =|1C|i? [sent-181, score-1.418]
</p><p>39 TzKhis property will be used in the following algorithm for detecting collective motion patterns from clutters. [sent-211, score-0.574]
</p><p>40 Collective Motion Detection Based on the collectiveness descriptor, we propose an algorithm called Collective Merging to detect collective motions from time-series data with noises (see Algorithm 1). [sent-213, score-1.293]
</p><p>41 tli Bery particles dwinithg low collectiveness and get the clusters of collective motion patterns as the connected components from thresholded Z. [sent-216, score-1.388]
</p><p>42 In the experiment section, we demonstrate its effectiveness for detecting collective motions in various videos. [sent-222, score-0.565]
</p><p>43 Evaluation on Self-Driven Particles We take the Self-Driven Particle model (SDP) [19] to evaluate the proposed collectiveness, because SDP has been used extensively for studying collective motion and shows high similarity with various crowd systems in nature [5, 22]. [sent-224, score-0.909]
</p><p>44 Importantly, the groundtruth of collectiveness in SDP is  Φ0 . [sent-225, score-0.728]
</p><p>45 The behaviors of individuals gradually turn into collective motion from random movements, and Φ accurately reflects the phase transition of crowd dynamics. [sent-232, score-1.135]
</p><p>46 SDP was firstly proposed to investigate the emergence of collective motion in a system of particles. [sent-237, score-0.563]
</p><p>47 It is shown that the level of random perturbation η on the aligned direction in neighborhood would cause the phase transition of this crowd system from disordered movements into collective motion. [sent-239, score-0.929]
</p><p>48 3, we compute crowd collectiveness Φ at each time t. [sent-249, score-1.067]
</p><p>49 Φ monitors the emergence of collective motion over time. [sent-250, score-0.551]
</p><p>50 The crowd gradually turns into the state of collective motion. [sent-252, score-0.835]
</p><p>51 4, Φ accurately measures the collectiveness of crowd systems under different levels of random perturbation η. [sent-256, score-1.112]
</p><p>52 B) For a low η, all the individuals are in a global collective motion, and Φ is close to the upper bound. [sent-289, score-0.65]
</p><p>53 For a relatively larger η , individuals form multiple clusters of collective motions. [sent-290, score-0.646]
</p><p>54 After a while, self-driven  particles are organized into clusters of collective motions. [sent-293, score-0.595]
</p><p>55 B) By removing particles with individual collectiveness lower than 0. [sent-295, score-0.854]
</p><p>56 To evaluate the robustness of our collectiveness descriptor, we extend SDP to a mixture model by adding outlier particles, which do not have alignment in neighborhoods and move randomly all the time. [sent-303, score-0.728]
</p><p>57 5A, individuals are randomly initialized at the start, so the histogram of individual collectiveness has a single mode. [sent-306, score-0.905]
</p><p>58 When self-driven particles gradually turn into clusters of collective motions, there is a clear separation between two modes in the histogram of individual collectiveness. [sent-307, score-0.636]
</p><p>59 We let z = K1 and plot Wtheh regularized find zivi ≥dual collectiveness of l-path scales [zφ1 , z2φ2 , . [sent-328, score-0.728]
</p><p>60 There are two parameters z and K for computing collectiveness in practical applications. [sent-337, score-0.728]
</p><p>61 Thus, by tuning z and K we can control the sensitivity of collectiveness in practical applications. [sent-348, score-0.728]
</p><p>62 The three rows are from the three collectiveness categories. [sent-371, score-0.728]
</p><p>63 Further Evaluation and Applications  We evaluate the consistency between our collectiveness and human perception, and apply the proposed descriptor and algorithm to various videos of pedestrian crowds and bacterial colony. [sent-373, score-0.95]
</p><p>64 Human Perception for Collective Motion  To quantitatively evaluate the proposed crowd collectiveness, we compare it with human motion perception on a new Collective Motion Database, and then analyze the consistency and correlation with human-labeled ground-truth for collective motion. [sent-376, score-0.937]
</p><p>65 This database contains different levels of collective motions with 100 frames per clips. [sent-379, score-0.577]
</p><p>66 A subject is asked to rate the level of collective motions in a video from three options: low, medium, and high. [sent-383, score-0.565]
</p><p>67 We propose two criteria to evaluate the consistency between human-labeled ground-truth and the proposed collectiveness descriptor. [sent-384, score-0.752]
</p><p>68 The first is the correlation between the human scores and our collectiveness descriptor. [sent-385, score-0.762]
</p><p>69 We compute the crowd collectiveness Φ at each frame using the motion features extracted with a generalized KLT (gKLT) tracker derived from [18], and take Φ averaged over 100 frames as the collectiveΦ  = 0. [sent-391, score-1.117]
</p><p>70 scatters the collective scores with Φ and v of all the  motion collecFig. [sent-421, score-0.547]
</p><p>71 There is a high correlation between collective scores and Φ, and the proposed collectiveness is consistent with human perception. [sent-423, score-1.245]
</p><p>72 The second is the classification accuracy based on the collectiveness descriptor. [sent-424, score-0.728]
</p><p>73 We divide all the videos into three categories by majority voting of subjects’ rating, and then evaluate how the proposed collectiveness descriptor can classify them. [sent-425, score-0.773]
</p><p>74 Φ can better classify different levels of collective motions than v, especially on the binary classification of high-medium categories and medium-low categories of videos. [sent-431, score-0.577]
</p><p>75 It indicates our collectiveness descriptor can delicately measure the dynamic state of crowds. [sent-432, score-0.749]
</p><p>76 Meanwhile collectiveness may not be properly computed due to tracking failures, projective distortion and 333000555422  ROC curves and best accuracies for high-low, high-medium, and medium-low classification. [sent-435, score-0.728]
</p><p>77 The computed collectiveness in the two videos is low because the KLT tracker does not capture the motions well due to the perspective distortion and the extremely low frame rate, while all subjects give high scores because of the regular pedestrian and traffic flows in the scenes. [sent-440, score-0.877]
</p><p>78 Collective Motion Detection in Videos We detect collective motions from videos in the Collective Motion Database. [sent-443, score-0.589]
</p><p>79 9A shows the detected collective motions by Collective Merging on nine videos, along with their computed Φ and v. [sent-446, score-0.577]
</p><p>80 The detected collective motion patterns correspond to a variety of behaviors, such as group walking, lane formation, and different traffic modes, which are of a great interest for further video analysis and scene understanding. [sent-447, score-0.56]
</p><p>81 The estimated crowd collectiveness also varies across scenes and reflects different levels of collective mo-  tions in videos. [sent-448, score-1.562]
</p><p>82 However, v cannot accurately reflect the collectiveness of crowd motions in these videos. [sent-449, score-1.161]
</p><p>83 Furthermore, the proposed crowd collectiveness can be used to monitor crowd dynamics. [sent-450, score-1.406]
</p><p>84 9B shows an example in which the collectiveness changes abruptly when two groups of pedestrians pass each other. [sent-452, score-0.728]
</p><p>85 Collective Motions in Bacterial Colony In this experiment, we use the proposed collectiveness to study collective motions emerging in a bacterial colony. [sent-457, score-1.375]
</p><p>86 C) Representative frames of collective motion patterns detected by Collective Merging and their Φ and v. [sent-461, score-0.56]
</p><p>87 10C shows representative frames and collective motion patterns detected by Collective Merging. [sent-469, score-0.574]
</p><p>88 Crowd density was proved to be one of the key factors for the formation of collective motion [22, 19]. [sent-470, score-0.545]
</p><p>89 For the same type of bacteria in the same environment, bacteria collectiveness should monotonically increase with density. [sent-472, score-0.928]
</p><p>90 Our proposed collectiveness measurement has good potentials for scientific studies. [sent-475, score-0.764]
</p><p>91 Conclusions and Future Work We have proposed a collectiveness descriptor for crowd systems as well as their constituent individuals along with the efficient computation. [sent-477, score-1.297]
</p><p>92 Collective Merging can be used to detect collective motions from randomly moving outliers. [sent-478, score-0.579]
</p><p>93 We have validated the effectiveness and robustness of the proposed collectiveness on the system of self-driven particles, and shown the high consistency with human perception for collective motion. [sent-479, score-1.268]
</p><p>94 As a new universal descriptor for various types of crowd systems, the proposed crowd collectiveness should inspire many interesting applications and extensions in future work. [sent-481, score-1.449]
</p><p>95 Individuals in a crowd system can move collectively in a single group or in several groups with different collective patterns, even though the system has the same value of Φ. [sent-482, score-0.864]
</p><p>96 Our single collectiveness measurement can be well extended to a spectrum vector of characterizing collectiveness at  different length scales. [sent-483, score-1.473]
</p><p>97 It is also desirable to enhance the descriptive power of collectiveness by modeling its spatial and temporal variations. [sent-484, score-0.728]
</p><p>98 The enhanced descriptor can be applied to cross-scene crowd video retrieval, which is difficult previously because universal properties of crowd systems could not be well quantitatively measured. [sent-485, score-0.742]
</p><p>99 Interaction ruling animal collective behavior depends on topological rather than metric distance: Evidence from a field study. [sent-509, score-0.554]
</p><p>100 Understanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents. [sent-657, score-0.822]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('collectiveness', 0.728), ('collective', 0.483), ('crowd', 0.339), ('individuals', 0.149), ('bacteria', 0.1), ('particles', 0.098), ('bacterial', 0.082), ('motions', 0.082), ('sdp', 0.076), ('colony', 0.065), ('behaviors', 0.055), ('motion', 0.05), ('zl', 0.043), ('crowds', 0.042), ('constituent', 0.039), ('path', 0.038), ('velocity', 0.037), ('crowded', 0.031), ('behavior', 0.03), ('pedestrian', 0.029), ('individual', 0.028), ('property', 0.026), ('manifold', 0.025), ('merging', 0.025), ('videos', 0.024), ('flocks', 0.024), ('kratz', 0.024), ('neighborhood', 0.024), ('consistency', 0.024), ('universal', 0.022), ('perception', 0.021), ('descriptor', 0.021), ('systems', 0.021), ('animal', 0.021), ('movements', 0.021), ('topological', 0.02), ('correlation', 0.02), ('bound', 0.019), ('scientific', 0.019), ('zw', 0.019), ('upper', 0.018), ('wt', 0.018), ('agar', 0.018), ('ballerini', 0.018), ('cabibbo', 0.018), ('shoal', 0.018), ('swarms', 0.018), ('vicsek', 0.018), ('zeta', 0.018), ('zzk', 0.018), ('emergence', 0.018), ('collectively', 0.018), ('transition', 0.017), ('phase', 0.017), ('measurement', 0.017), ('velocities', 0.017), ('similarity', 0.016), ('pl', 0.016), ('disordered', 0.016), ('medium', 0.016), ('patterns', 0.015), ('transmission', 0.015), ('measuring', 0.015), ('monitoring', 0.015), ('clusters', 0.014), ('paths', 0.014), ('unstable', 0.014), ('moving', 0.014), ('univ', 0.014), ('academy', 0.014), ('clips', 0.014), ('representative', 0.014), ('scores', 0.014), ('theorem', 0.013), ('entry', 0.013), ('zhou', 0.013), ('klt', 0.013), ('iand', 0.013), ('gradually', 0.013), ('neighbors', 0.013), ('miller', 0.013), ('studies', 0.013), ('hong', 0.013), ('kong', 0.013), ('acting', 0.012), ('similarities', 0.012), ('coherent', 0.012), ('sciences', 0.012), ('exponentially', 0.012), ('mechanics', 0.012), ('abnormality', 0.012), ('behavioral', 0.012), ('accurately', 0.012), ('detected', 0.012), ('levels', 0.012), ('crosses', 0.012), ('fish', 0.012), ('fluid', 0.012), ('system', 0.012), ('formation', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="282-tfidf-1" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>2 0.22504666 <a title="282-tfidf-2" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>Author: Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah</p><p>Abstract: We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark con- trast to datasets usedfor existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.</p><p>3 0.1342231 <a title="282-tfidf-3" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>4 0.1273638 <a title="282-tfidf-4" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>Author: Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy</p><p>Abstract: A number of computer vision problems such as human age estimation, crowd density estimation and body/face pose (view angle) estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalarvalued output. Such a learning problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated. Encouraged by the recent success in using attributes for solving classification problems with sparse training data, this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. More precisely, low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation (a label) that captures how the scalar output value (e.g. age, people count) changes continuously and cumulatively. Extensive experiments show that our cumulative attribute framework gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models, especially when the labelled training data is sparse with imbalanced sampling.</p><p>5 0.056779601 <a title="282-tfidf-5" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>Author: Carlos Arteta, Victor Lempitsky, J. Alison Noble, Andrew Zisserman</p><p>Abstract: The objective of this work is to detect all instances of a class (such as cells or people) in an image. The instances may be partially overlapping and clustered, and hence quite challenging for traditional detectors, which aim at localizing individual instances. Our approach is to propose a set of candidate regions, and then select regions based on optimizing a global classification score, subject to the constraint that the selected regions are non-overlapping. Our novel contribution is to extend standard object detection by introducing separate classes for tuples of objects into the detection process. For example, our detector can pick a region containing two or three object instances, while assigning such region an appropriate label. We show that this formulation can be learned within the structured output SVM framework, and that the inference in such model can be accomplished using dynamic programming on a tree structured region graph. Furthermore, the learning only requires weak annotations – a dot on each instance. The improvement resulting from the addition of the capability to detect tuples of objects is demonstrated on quite disparate data sets: fluorescence microscopy images and UCSD pedestrians.</p><p>6 0.055860132 <a title="282-tfidf-6" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>7 0.05566778 <a title="282-tfidf-7" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>8 0.048483081 <a title="282-tfidf-8" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>9 0.04430294 <a title="282-tfidf-9" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>10 0.043773644 <a title="282-tfidf-10" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<p>11 0.039561294 <a title="282-tfidf-11" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>12 0.038943425 <a title="282-tfidf-12" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>13 0.037698854 <a title="282-tfidf-13" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<p>14 0.037197385 <a title="282-tfidf-14" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>15 0.037110399 <a title="282-tfidf-15" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>16 0.034522835 <a title="282-tfidf-16" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>17 0.031368095 <a title="282-tfidf-17" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>18 0.031269975 <a title="282-tfidf-18" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>19 0.030318098 <a title="282-tfidf-19" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>20 0.030256426 <a title="282-tfidf-20" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.075), (1, -0.001), (2, 0.001), (3, -0.026), (4, -0.008), (5, 0.005), (6, -0.008), (7, -0.034), (8, 0.009), (9, 0.037), (10, 0.012), (11, -0.0), (12, 0.029), (13, -0.049), (14, 0.047), (15, 0.029), (16, -0.014), (17, 0.038), (18, 0.004), (19, -0.055), (20, 0.021), (21, 0.095), (22, -0.081), (23, 0.021), (24, -0.033), (25, -0.044), (26, -0.086), (27, -0.078), (28, 0.022), (29, -0.091), (30, -0.033), (31, 0.001), (32, 0.001), (33, 0.124), (34, 0.005), (35, -0.059), (36, 0.042), (37, -0.044), (38, 0.113), (39, -0.087), (40, -0.057), (41, 0.088), (42, -0.169), (43, 0.055), (44, -0.009), (45, -0.088), (46, 0.098), (47, 0.072), (48, -0.04), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9399963 <a title="282-lsi-1" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>2 0.86865145 <a title="282-lsi-2" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>3 0.81824785 <a title="282-lsi-3" href="./cvpr-2013-Multi-source_Multi-scale_Counting_in_Extremely_Dense_Crowd_Images.html">299 cvpr-2013-Multi-source Multi-scale Counting in Extremely Dense Crowd Images</a></p>
<p>Author: Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah</p><p>Abstract: We propose to leverage multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image. Due to problems including perspective, occlusion, clutter, and few pixels per person, counting by human detection in such images is almost impossible. Instead, our approach relies on multiple sources such as low confidence head detections, repetition of texture elements (using SIFT), and frequency-domain analysis to estimate counts, along with confidence associated with observing individuals, in an image region. Secondly, we employ a global consistency constraint on counts using Markov Random Field. This caters for disparity in counts in local neighborhoods and across scales. We tested our approach on a new dataset of fifty crowd images containing 64K annotated humans, with the head counts ranging from 94 to 4543. This is in stark con- trast to datasets usedfor existing methods which contain not more than tens of individuals. We experimentally demonstrate the efficacy and reliability of the proposed approach by quantifying the counting performance.</p><p>4 0.58304894 <a title="282-lsi-4" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>Author: Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy</p><p>Abstract: A number of computer vision problems such as human age estimation, crowd density estimation and body/face pose (view angle) estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalarvalued output. Such a learning problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated. Encouraged by the recent success in using attributes for solving classification problems with sparse training data, this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. More precisely, low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation (a label) that captures how the scalar output value (e.g. age, people count) changes continuously and cumulatively. Extensive experiments show that our cumulative attribute framework gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models, especially when the labelled training data is sparse with imbalanced sampling.</p><p>5 0.5265193 <a title="282-lsi-5" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>Author: Alessandro Perina, Nebojsa Jojic</p><p>Abstract: Recently, the Counting Grid (CG) model [5] was developed to represent each input image as a point in a large grid of feature counts. This latent point is a corner of a window of grid points which are all uniformly combined to match the (normalized) feature counts in the image. Being a bag of word model with spatial layout in the latent space, the CG model has superior handling of field of view changes in comparison to other bag of word models, but with the price of being essentially a mixture, mapping each scene to a single window in the grid. In this paper we introduce a family of componential models, dubbed the Componential Counting Grid, whose members represent each input image by multiple latent locations, rather than just one. In this way, we make a substantially more flexible admixture model which captures layers or parts of images and maps them to separate windows in a Counting Grid. We tested the models on scene and place classification where their com- ponential nature helped to extract objects, to capture parallax effects, thus better fitting the data and outperforming Counting Grids and Latent Dirichlet Allocation, especially on sequences taken with wearable cameras.</p><p>6 0.46197447 <a title="282-lsi-6" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>7 0.33579898 <a title="282-lsi-7" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>8 0.32534921 <a title="282-lsi-8" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>9 0.30788741 <a title="282-lsi-9" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>10 0.30608615 <a title="282-lsi-10" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>11 0.27317524 <a title="282-lsi-11" href="./cvpr-2013-Dynamic_Scene_Classification%3A_Learning_Motion_Descriptors_with_Slow_Features_Analysis.html">137 cvpr-2013-Dynamic Scene Classification: Learning Motion Descriptors with Slow Features Analysis</a></p>
<p>12 0.25088477 <a title="282-lsi-12" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>13 0.23844236 <a title="282-lsi-13" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>14 0.23527244 <a title="282-lsi-14" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>15 0.22877727 <a title="282-lsi-15" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>16 0.22392093 <a title="282-lsi-16" href="./cvpr-2013-Discrete_MRF_Inference_of_Marginal_Densities_for_Non-uniformly_Discretized_Variable_Space.html">128 cvpr-2013-Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space</a></p>
<p>17 0.22281834 <a title="282-lsi-17" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>18 0.22155148 <a title="282-lsi-18" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>19 0.22016878 <a title="282-lsi-19" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>20 0.21183547 <a title="282-lsi-20" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.016), (5, 0.287), (10, 0.082), (16, 0.031), (26, 0.034), (28, 0.01), (33, 0.166), (67, 0.076), (69, 0.076), (77, 0.013), (87, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76842028 <a title="282-lda-1" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>2 0.66760302 <a title="282-lda-2" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>Author: Yinghuan Shi, Shu Liao, Yaozong Gao, Daoqiang Zhang, Yang Gao, Dinggang Shen</p><p>Abstract: Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. In this paper, a novel semi-automated prostate segmentation method is presented. Specifically, to segment the prostate in the current treatment image, the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. Then, the prostate is segmented automatically by theproposed two steps: (i) Thefirst step of prostate-likelihood estimation to predict the prostate likelihood for each voxel in the current treatment image, aiming to generate the 3-D prostate-likelihood map by the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) The second step of multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from the planning and previous treatment images. The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset, consisting of 24 patients with 330 images. Moreover, it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification of the first and last slices of the prostate.</p><p>3 0.6558277 <a title="282-lda-3" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>Author: Ning Zhu, Albert C.S. Chung</p><p>Abstract: In this paper, we propose a graph-based method for 3D vessel tree structure segmentation based on a new tubularity Markov tree model ( TMT), which works as both new energy function and graph construction method. With the help of power-watershed implementation [7], a global optimal segmentation can be obtained with low computational cost. Different with other graph-based vessel segmentation methods, the proposed method does not depend on any skeleton and ROI extraction method. The classical issues of the graph-based methods, such as shrinking bias and sensitivity to seed point location, can be solved with the proposed method thanks to vessel data fidelity obtained with TMT. The proposed method is compared with some classical graph-based image segmentation methods and two up-to-date 3D vessel segmentation methods, and is demonstrated to be more accurate than these methods for 3D vessel tree segmentation. Although the segmentation is done without ROI extraction, the computational cost for the proposed method is low (within 20 seconds for 256*256*144 image).</p><p>4 0.63205159 <a title="282-lda-4" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>Author: Dan Levi, Shai Silberstein, Aharon Bar-Hillel</p><p>Abstract: In this work we present a new part-based object detection algorithm with hundreds of parts performing realtime detection. Part-based models are currently state-ofthe-art for object detection due to their ability to represent large appearance variations. However, due to their high computational demands such methods are limited to several parts only and are too slow for practical real-time implementation. Our algorithm is an accelerated version of the “Feature Synthesis ” (FS) method [1], which uses multiple object parts for detection and is among state-of-theart methods on human detection benchmarks, but also suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses several strategies for reducing the number of locations searched for each part. The first strategy uses a novel algorithm for approximate nearest neighbor search which we developed, termed “KDFerns ”, to compare each image location to only a subset of the model parts. Candidate part locations for a specific part are further reduced using spatial inhibition, and using an object-level “coarse-to-fine ” strategy. In our empirical evaluation on pedestrian detection benchmarks, AFS main- × tains almost fully the accuracy performance of the original FS, while running more than 4 faster than existing partbased methods which use only several parts. AFS is to our best knowledge the first part-based object detection method achieving real-time running performance: nearly 10 frames per-second on 640 480 images on a regular CPU.</p><p>5 0.61997175 <a title="282-lda-5" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>6 0.59340459 <a title="282-lda-6" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>7 0.5921098 <a title="282-lda-7" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>8 0.59053576 <a title="282-lda-8" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>9 0.58982223 <a title="282-lda-9" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>10 0.58906066 <a title="282-lda-10" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>11 0.58678472 <a title="282-lda-11" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>12 0.58652478 <a title="282-lda-12" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>13 0.58613789 <a title="282-lda-13" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>14 0.58604193 <a title="282-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.58330494 <a title="282-lda-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.58256352 <a title="282-lda-16" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>17 0.58233118 <a title="282-lda-17" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>18 0.58142829 <a title="282-lda-18" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>19 0.58113021 <a title="282-lda-19" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>20 0.58111584 <a title="282-lda-20" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
