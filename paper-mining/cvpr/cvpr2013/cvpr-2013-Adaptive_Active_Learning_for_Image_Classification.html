<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 cvpr-2013-Adaptive Active Learning for Image Classification</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-34" href="#">cvpr2013-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 cvpr-2013-Adaptive Active Learning for Image Classification</h1>
<br/><p>Source: <a title="cvpr-2013-34-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Adaptive_Active_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>Reference: <a title="cvpr-2013-34-reference" href="../cvpr2013_reference/cvpr-2013-Adaptive_Active_Learning_for_Image_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. [sent-2, score-0.549]
</p><p>2 Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. [sent-3, score-1.058]
</p><p>3 Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. [sent-4, score-1.4]
</p><p>4 In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. [sent-5, score-1.6]
</p><p>5 To build a robust image classifier, it typically requires a large number of labeled training instances. [sent-9, score-0.075]
</p><p>6 For example, 10,000 instances of handwriting digits are used for training classifiers in [33]. [sent-10, score-0.284]
</p><p>7 It is time and cost consuming to prepare such a large set of labeled training instances. [sent-11, score-0.175]
</p><p>8 On the other hand, one fascinating characteristic of human vision system is that we can categorize image objects with only few labeled training instances. [sent-12, score-0.108]
</p><p>9 We aim to develop an effective active learning method to build a competitive classifier with a limited amount of labeled training instances. [sent-15, score-0.505]
</p><p>10 Training a good classifier with minimal labeling cost is a critical challenge posed in machine learning research. [sent-16, score-0.151]
</p><p>11 Randomly selecting unlabeled instances to label is inefficient in many situations, since non-informative or redundant instances might be selected. [sent-17, score-1.176]
</p><p>12 Aiming to reduce labeling effort, active learning methods have been adopted to control the labeling process. [sent-18, score-0.374]
</p><p>13 Recently, active learning has been studied in computer vision [3, 14, 13, 15, 16], focusing on poolbased setting. [sent-19, score-0.374]
</p><p>14 These works however merely evaluate the informativeness of instances with most uncertainty measures, which assume an instance with higher classification uncertainty is more critical to label. [sent-20, score-1.473]
</p><p>15 This may lead to selecting non-useful instances to label. [sent-22, score-0.353]
</p><p>16 For example, an outlier can be most uncertain to classify, but useless to label. [sent-23, score-0.14]
</p><p>17 This suggests representativeness of the candidate instance in addition to the classification uncertainty should be considered in developing an active learning strategy. [sent-24, score-1.218]
</p><p>18 In this paper, we propose a novel adaptive active learning strategy that exploits information provided by both the labeled instances and the unlabeled instances for query selection. [sent-25, score-1.688]
</p><p>19 Our new query selection measure is an adaptive combination of two terms: an uncertainty term based on the current classifier trained on the labeled instances; and an information density term that measures the mutual information between the candidate instance and the remaining unlabeled instances. [sent-26, score-2.041]
</p><p>20 We seek to obtain an adaptive combination of the two terms by selecting the weight parameter to minimize the expected classification error on unlabeled instances. [sent-28, score-0.775]
</p><p>21 We conduct experiments on a few benchmark image classification datasets and present promising results for the proposed active learning method. [sent-29, score-0.477]
</p><p>22 Related Work A large number of active learning techniques have been developed in the literature. [sent-31, score-0.374]
</p><p>23 Most of them have been focused 888885555599777  on selecting a single most informative unlabeled instance to label each time. [sent-32, score-0.852]
</p><p>24 Many such approaches make myopic decisions based solely on the current learned classifiers and employ an uncertainty sampling principle to select the unlabeled instance they are most uncertain to label. [sent-33, score-1.269]
</p><p>25 In [18, 26], the most uncertain instance is taken as the one that has the largest entropy on the conditional distribution over its labels. [sent-34, score-0.505]
</p><p>26 Support vector machine methods choose the most uncertain instance as the one that is closest to the classification boundary [2, 25, 28]. [sent-35, score-0.398]
</p><p>27 Query-by-committee algorithms train a committee of classifiers and choose the instance on which the committee members most disagree [9, 19]. [sent-36, score-0.356]
</p><p>28 One apparent shortcoming of the active learning strategies reviewed above is that they select a query based only on how that instance relates to the current classifier(s), whereas ignoring the large set of unlabeled instances. [sent-37, score-1.223]
</p><p>29 One immediate problem is that these approaches are prone to querying outliers, as we discussed before. [sent-38, score-0.074]
</p><p>30 Moreover, the goal of active learning is producing a classifier that has good generalization performance on unseen instances in the problem domain. [sent-39, score-0.787]
</p><p>31 Although it might not be possible to access the domain distribution directly, relevant information can be obtained from the large pool of unlabeled instances. [sent-40, score-0.543]
</p><p>32 Many active learning methods have been proposed to exploit unlabeled data to minimize the generalization error of the trained classifier. [sent-41, score-0.964]
</p><p>33 In [24], queries are selected to minimize the generalization error in a direct way by maximizing the  expected error reduction on unlabeled data with respect to the estimated posterior label probabilities. [sent-42, score-0.687]
</p><p>34 Another class of active learning approaches minimize the generalization error indirectly by reducing model variances, including a statistical approach [4], and a similar approach that selects optimal queries based on Fisher information [35]. [sent-43, score-0.573]
</p><p>35 These generalization error minimization approaches are generally computationally expensive. [sent-44, score-0.101]
</p><p>36 An alternative class of active learning methods use a number of heuristic measures to exploit the information in unlabeled data. [sent-45, score-0.964]
</p><p>37 The methods in [19, 32] employ the unlabeled data by using the prior density p(x) as weights for uncertainty measures. [sent-46, score-1.0]
</p><p>38 A similar framework is employed in [26], which uses a cosine distance to measure an information density. [sent-47, score-0.178]
</p><p>39 The methods in [6, 20] explicitly combine clustering and active learning together to exploit both labeled and unlabeled instances. [sent-48, score-0.91]
</p><p>40 In [10, 17], instances are selected to maximize the increase of mutual information between the selected set of instances and the remaining ones based on Gaussian Process models. [sent-49, score-0.735]
</p><p>41 The method in [23] extends the query-by-committee algorithm by exploiting unlabeled data. [sent-50, score-0.461]
</p><p>42 The work [11] seeks the instance whose optimistic label provides maximum mutual information about the labels of the remaining unlabeled instances, which implicitly exploits the clustering information contained in the unlabeled data in an optimistic way. [sent-51, score-1.527]
</p><p>43 In the realm of computer vision, researchers have adopted active learning in image/video annotation [16, 34, 3 1], image/video retrieval [29, 12] and image/video recog-  nition [30, 15, 13, 22, 14]. [sent-52, score-0.403]
</p><p>44 The work [29] applies active learning on object detection and the approach aims to deal with a large amount of images crawled online. [sent-53, score-0.374]
</p><p>45 The work [14] generalizes the margin-based uncertainty measure to the multi-class case. [sent-54, score-0.471]
</p><p>46 In [22], a two dimensional active learning method is proposed to conduct selection over instancelabel pairs instead of solely instances. [sent-55, score-0.458]
</p><p>47 The work [13] introduces a probabilistic variant of a KNN method used for active learning. [sent-56, score-0.345]
</p><p>48 The work [15] uses Gaussian Process as a probabilistic prediction model to gain a direct estimate of uncertainty measure for active learning in binary classification case. [sent-57, score-0.941]
</p><p>49 Although different prediction models have been employed in these methods, they all used the simple uncertainty sampling active learning strategy for instance selection. [sent-58, score-1.023]
</p><p>50 Therefore these methods have the drawback of ignoring the distributional information contained in the large number of unlabeled instances, as we discussed above. [sent-59, score-0.621]
</p><p>51 In this paper, we develop a new active learning method for image classification tasks, which overcomes the inherent limitation of uncertainty sampling. [sent-60, score-0.82]
</p><p>52 Proposed Approach Different active learning strategies have different strengths in identifying which instance to query given current classifier. [sent-62, score-0.742]
</p><p>53 In this section, we present a novel active learning method that combines the strengths of different active learning strategies in an adaptive way. [sent-63, score-0.916]
</p><p>54 The proposed active learning method has three key components: an un-  certainty measure, an information density measure and an adaptive combination framework. [sent-64, score-0.783]
</p><p>55 Moreover, our approach is based on probabilistic classification models. [sent-66, score-0.096]
</p><p>56 We use logistic regression as our probabilistic classification model in the experiments. [sent-67, score-0.096]
</p><p>57 We use xi ∈ Rd to denote the input feature vector of the ith instance,∈ ∈an Rd yi ∈ {1, · · · , K} to denote its class label. [sent-70, score-0.166]
</p><p>58 We use L and U to d∈en {o1te,· t·h·e , iKnd}e xto se detsn ootfe eth iets la cblaeslse dla abneld. [sent-71, score-0.094]
</p><p>59 Uncertainty Measure Uncertainty sampling is one simplest and most commonly used active active learning strategy. [sent-76, score-0.727]
</p><p>60 It aims to choose the most uncertain instance to label. [sent-77, score-0.335]
</p><p>61 For probabilistic classification models, the uncertainty measure is defined as the conditional entropy of the label variable Y given the candi888886555600888  date instance  xi:  f(xi) = H(Y |xi, θL)  (1)  = −? [sent-78, score-0.966]
</p><p>62 y∈Y where Y denotes the set of all class values, θL represents wtheh crelas Ysif diecantoiotens m thoede sel ttr oaifn aeldl over t vhael ulaesbe,l θed set L, and the conditional distribution P(y|xi , θL) is determined using tthhies cmonoddietil. [sent-80, score-0.218]
</p><p>63 o nTahlis d uncertainty measure captures the informativeness of the candidate instance with respect to the labeled instances. [sent-81, score-0.989]
</p><p>64 Information Density Measure To cope with the drawback of uncertainty sampling, we next take the unlabeled instances into consideration when selecting an instance to query. [sent-85, score-1.422]
</p><p>65 Our motivation is that the representative instances of the input distribution can be very informative for improving the generalization performance of the target classifier. [sent-86, score-0.516]
</p><p>66 Although the input distribution is usually not given, we have a large set of unlabeled instances that can be used to approximate the input space. [sent-87, score-0.788]
</p><p>67 It has been shown in previous semi-supervised learning work that  the distribution of unlabeled data is very useful for training good classification models [5, 27]. [sent-88, score-0.629]
</p><p>68 Intuitively, one would prefer to select the instance that is located in a dense region regarding the other unlabeled instances, since such an instance will be much more informative about other unlabeled instances than the ones located in a sparse region. [sent-89, score-1.734]
</p><p>69 We thus use the term information density to indicate the informativeness of a candidate instance for the remaining unlabeled instances. [sent-90, score-1.138]
</p><p>70 Specifically, in this work, we define the information density measure as the mutual information between the candidate instance and the remaining unlabeled instances within a Gaussian Process framework. [sent-91, score-1.506]
</p><p>71 A Gaussian Process is a joint distribution over a (possibly infinite) set of random variables, such that the marginal distribution over any fi-  nite subset of variables is multivariate Gaussian. [sent-95, score-0.199]
</p><p>72 ix T dheufsin tehde over arilal ntchee unlabeled instances indexed by Ui. [sent-103, score-0.798]
</p><p>73 = =  (6) (7)  Using (6) and (7), the information density definition given in (3) can finally be rewritten into the following form  d(xi) =21ln? [sent-115, score-0.195]
</p><p>74 A Combination Framework Given the uncertainty measure and the information density measure defined above, we aim to develop a combination framework to integrate the strengths of both. [sent-120, score-0.87]
</p><p>75 The main idea is to pick the instance that is not only most uncertain 8 8 865 561 9 9  to classify based on the current classifier, but also very informative about the remaining unlabeled instances. [sent-121, score-0.964]
</p><p>76 Thus after adding this instance to the labeled set, the new classifier produced can make more accurate predictions on the unlabeled instances. [sent-122, score-0.787]
</p><p>77 Specifically, we propose to combine the two measures in a general product form of combination framework as below  hβ(xi) = f(xi)βd(xi)1−β (9) where 0 ≤ β ≤ 1is a tradeoff controlling parameter over twheh trweo 0 te ≤rm βs. [sent-123, score-0.12]
</p><p>78 ≤F 1or i tsh ae croadmeobfinfa ctioonntr measure given irn o Eq. [sent-124, score-0.088]
</p><p>79 (9), although the uncertainty term f(xi)β is a discriminative measure, the information density term d(xi)1−β is computed in the input space and has no direct connection with the target discriminative classification model. [sent-125, score-0.641]
</p><p>80 Using such a heuristic combination measure, we aim to pick the most informative instance for reducing the generalization error of the classification model without the computationally expensive steps of retraining classification model for each candidate instance. [sent-126, score-0.753]
</p><p>81 The only computationally expensive operation for this information density assisted combination measure is the matrix inversion operation Σ−U1iUi used to compute the conditional covariance σi2|Ui in Eq. [sent-127, score-0.484]
</p><p>82 It is very inefficient to compute a matrix inverse Σ−U1iUi for each candidate instance i ∈ U. [sent-129, score-0.351]
</p><p>83 Thus we only need to conduct one matrix inversion at the beginning of the active learning process. [sent-132, score-0.501]
</p><p>84 Moreover, one can use subsampling to further reduce the computational cost for large unlabeled sets. [sent-133, score-0.461]
</p><p>85 That is, in each iteration of active learning, one can first randomly sample a subset of unlabeled instances, and then restrain the candi-  date instance selection to this subset. [sent-134, score-1.084]
</p><p>86 A similar combination strategy to our proposed one in Eq. [sent-135, score-0.088]
</p><p>87 However, it uses the average cosine distance between the candidate instance and all unlabeled instances as its information density measure. [sent-137, score-1.302]
</p><p>88 Below we propose to adaptively select the best β from a range of pre-defined values to use in each iteration of active learning. [sent-139, score-0.392]
</p><p>89 Adaptive Combination One important issue regarding the combination strategy we proposed above is to select a proper weight parameter β for 0 ≤ β ≤ 1. [sent-142, score-0.137]
</p><p>90 5, the uncertainty measure is treated as a more important measure than the information density measure since more weights are put on the uncertainty measure. [sent-146, score-1.225]
</p><p>91 In the extreme case of β = 1, it is equivalent to most uncertainty sampling. [sent-147, score-0.383]
</p><p>92 5, more weights are put on the information density measure. [sent-149, score-0.195]
</p><p>93 Moreover, the relative importance of the two measures can be dynamically changing across different iterations and stages of the active learning process. [sent-151, score-0.478]
</p><p>94 To achieve the best possible instance selection in each iteration, one thus needs to dynamically evaluate the relative informativeness of the two measures and determine the β value for each instance selection. [sent-152, score-0.67]
</p><p>95 In this work, we propose to take a simple nonmyopic step to adaptively pick the β value from a set of pre-defined candidate values. [sent-154, score-0.187]
</p><p>96 Specifically, in each iteration of active learning, we compute the uncertainty measure f(xi) and the information density measure d(xi) for each candidate instance xi. [sent-155, score-1.377]
</p><p>97 Then we select a set of b instances using b different β values from a pre-defined set B according to the combination measure hβ (xi) defined in Eq. [sent-156, score-0.479]
</p><p>98 Then selecting the best β value is equivalent to selecting the most informative instance from the b selected instances. [sent-165, score-0.422]
</p><p>99 We propose to make this selection by minimizing the expected classification error on the unlabeled instances. [sent-166, score-0.596]
</p><p>100 For each candidate instance x from the set S, we label it with a label value y with probability P(y|x, θL). [sent-168, score-0.387]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('unlabeled', 0.461), ('uncertainty', 0.383), ('active', 0.312), ('instances', 0.284), ('xui', 0.224), ('instance', 0.195), ('xi', 0.166), ('density', 0.156), ('uncertain', 0.14), ('informativeness', 0.132), ('candidate', 0.116), ('ui', 0.09), ('informative', 0.089), ('mutual', 0.089), ('measure', 0.088), ('inversion', 0.087), ('representativeness', 0.087), ('labeled', 0.075), ('query', 0.073), ('generalization', 0.073), ('entropy', 0.071), ('selecting', 0.069), ('adaptive', 0.068), ('committee', 0.066), ('optimistic', 0.066), ('classification', 0.063), ('measures', 0.062), ('learning', 0.062), ('prepare', 0.061), ('strengths', 0.058), ('combination', 0.058), ('conditional', 0.056), ('classifier', 0.056), ('temple', 0.055), ('indexed', 0.053), ('multivariate', 0.052), ('cosine', 0.051), ('select', 0.049), ('xin', 0.044), ('selection', 0.044), ('querying', 0.043), ('distribution', 0.043), ('strategies', 0.042), ('dynamically', 0.042), ('sampling', 0.041), ('conduct', 0.04), ('pick', 0.04), ('inefficient', 0.04), ('mx', 0.04), ('date', 0.039), ('information', 0.039), ('remaining', 0.039), ('consuming', 0.039), ('gaussian', 0.038), ('label', 0.038), ('contained', 0.034), ('probabilistic', 0.033), ('ootfe', 0.033), ('exdi', 0.033), ('fascinating', 0.033), ('iets', 0.033), ('otom', 0.033), ('restrain', 0.033), ('thoede', 0.033), ('uii', 0.033), ('critical', 0.033), ('queries', 0.031), ('prone', 0.031), ('adaptively', 0.031), ('voa', 0.031), ('argmaxh', 0.031), ('aun', 0.031), ('khe', 0.031), ('nite', 0.031), ('nwgit', 0.031), ('nxi', 0.031), ('ocft', 0.031), ('stances', 0.031), ('tixo', 0.031), ('strategy', 0.03), ('marginal', 0.03), ('drawback', 0.03), ('ignoring', 0.029), ('disagree', 0.029), ('pdr', 0.029), ('axl', 0.029), ('ifrom', 0.029), ('realm', 0.029), ('tthhies', 0.029), ('ttr', 0.029), ('yta', 0.029), ('heuristic', 0.028), ('error', 0.028), ('minimize', 0.028), ('distributional', 0.028), ('dla', 0.028), ('oaifn', 0.028), ('el', 0.027), ('motivation', 0.027), ('moreover', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="34-tfidf-1" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>2 0.28326738 <a title="34-tfidf-2" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>3 0.19596173 <a title="34-tfidf-3" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>4 0.18430254 <a title="34-tfidf-4" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>5 0.13779137 <a title="34-tfidf-5" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>Author: Chao-Yeh Chen, Kristen Grauman</p><p>Abstract: We propose an approach to learn action categories from static images that leverages prior observations of generic human motion to augment its training process. Using unlabeled video containing various human activities, the system first learns how body pose tends to change locally in time. Then, given a small number of labeled static images, it uses that model to extrapolate beyond the given exemplars and generate “synthetic ” training examples—poses that could link the observed images and/or immediately precede or follow them in time. In this way, we expand the training set without requiring additional manually labeled examples. We explore both example-based and manifold-based methods to implement our idea. Applying our approach to recognize actions in both images and video, we show it enhances a state-of-the-art technique when very few labeled training examples are available.</p><p>6 0.13778776 <a title="34-tfidf-6" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>7 0.12688634 <a title="34-tfidf-7" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>8 0.12215028 <a title="34-tfidf-8" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>9 0.12158998 <a title="34-tfidf-9" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>10 0.1143818 <a title="34-tfidf-10" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>11 0.10460111 <a title="34-tfidf-11" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>12 0.090312533 <a title="34-tfidf-12" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>13 0.089777626 <a title="34-tfidf-13" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>14 0.084228285 <a title="34-tfidf-14" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>15 0.082848676 <a title="34-tfidf-15" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>16 0.081840858 <a title="34-tfidf-16" href="./cvpr-2013-Discrete_MRF_Inference_of_Marginal_Densities_for_Non-uniformly_Discretized_Variable_Space.html">128 cvpr-2013-Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space</a></p>
<p>17 0.07614626 <a title="34-tfidf-17" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>18 0.074722923 <a title="34-tfidf-18" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>19 0.072651386 <a title="34-tfidf-19" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>20 0.072634727 <a title="34-tfidf-20" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, -0.067), (2, -0.038), (3, -0.001), (4, 0.075), (5, 0.034), (6, -0.062), (7, -0.028), (8, -0.016), (9, 0.022), (10, 0.018), (11, -0.016), (12, -0.05), (13, -0.048), (14, -0.122), (15, -0.07), (16, -0.047), (17, -0.112), (18, 0.023), (19, -0.038), (20, -0.085), (21, -0.115), (22, -0.102), (23, 0.015), (24, 0.075), (25, 0.042), (26, 0.013), (27, -0.002), (28, -0.009), (29, -0.019), (30, -0.058), (31, 0.036), (32, -0.124), (33, 0.111), (34, -0.032), (35, 0.046), (36, -0.046), (37, -0.064), (38, 0.0), (39, -0.191), (40, -0.123), (41, -0.007), (42, 0.115), (43, 0.114), (44, -0.0), (45, 0.014), (46, -0.063), (47, -0.128), (48, -0.042), (49, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9730106 <a title="34-lsi-1" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>Author: Xin Li, Yuhong Guo</p><p>Abstract: Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.</p><p>2 0.74631447 <a title="34-lsi-2" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>Author: Xiao Liu, Mingli Song, Dacheng Tao, Zicheng Liu, Luming Zhang, Chun Chen, Jiajun Bu</p><p>Abstract: Node splitting is an important issue in Random Forest but robust splitting requires a large number of training samples. Existing solutions fail to properly partition the feature space if there are insufficient training data. In this paper, we present semi-supervised splitting to overcome this limitation by splitting nodes with the guidance of both labeled and unlabeled data. In particular, we derive a nonparametric algorithm to obtain an accurate quality measure of splitting by incorporating abundant unlabeled data. To avoid the curse of dimensionality, we project the data points from the original high-dimensional feature space onto a low-dimensional subspace before estimation. A unified optimization framework is proposed to select a coupled pair of subspace and separating hyperplane such that the smoothness of the subspace and the quality of the splitting are guaranteed simultaneously. The proposed algorithm is compared with state-of-the-art supervised and semi-supervised algorithms for typical computer vision applications such as object categorization and image segmen- tation. Experimental results on publicly available datasets demonstrate the superiority of our method.</p><p>3 0.68702865 <a title="34-lsi-3" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>4 0.64562273 <a title="34-lsi-4" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>Author: Mingsheng Long, Guiguang Ding, Jianmin Wang, Jiaguang Sun, Yuchen Guo, Philip S. Yu</p><p>Abstract: Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. It has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-ofthe-art methods on three types of computer vision datasets.</p><p>5 0.6441496 <a title="34-lsi-5" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>Author: Peter Welinder, Max Welling, Pietro Perona</p><p>Abstract: How many labeled examples are needed to estimate a classifier’s performance on a new dataset? We study the case where data is plentiful, but labels are expensive. We show that by making a few reasonable assumptions on the structure of the data, it is possible to estimate performance curves, with confidence bounds, using a small number of ground truth labels. Our approach, which we call Semisupervised Performance Evaluation (SPE), is based on a generative model for the classifier’s confidence scores. In addition to estimating the performance of classifiers on new datasets, SPE can be used to recalibrate a classifier by reestimating the class-conditional confidence distributions.</p><p>6 0.60815412 <a title="34-lsi-6" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>7 0.57691264 <a title="34-lsi-7" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>8 0.55652708 <a title="34-lsi-8" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>9 0.54583293 <a title="34-lsi-9" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>10 0.53579712 <a title="34-lsi-10" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>11 0.53463244 <a title="34-lsi-11" href="./cvpr-2013-Discrete_MRF_Inference_of_Marginal_Densities_for_Non-uniformly_Discretized_Variable_Space.html">128 cvpr-2013-Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space</a></p>
<p>12 0.53315741 <a title="34-lsi-12" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>13 0.52973765 <a title="34-lsi-13" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>14 0.52301168 <a title="34-lsi-14" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>15 0.52271098 <a title="34-lsi-15" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>16 0.52055871 <a title="34-lsi-16" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>17 0.52009815 <a title="34-lsi-17" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>18 0.50154996 <a title="34-lsi-18" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>19 0.49845791 <a title="34-lsi-19" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>20 0.49566925 <a title="34-lsi-20" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.172), (16, 0.025), (26, 0.032), (33, 0.3), (67, 0.091), (69, 0.038), (87, 0.101), (94, 0.157)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94749373 <a title="34-lda-1" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>Author: Hu Ding, Branislav Stojkovic, Ronald Berezney, Jinhui Xu</p><p>Abstract: Computing accurate and robust organizational patterns of chromosome territories inside the cell nucleus is critical for understanding several fundamental genomic processes, such as co-regulation of gene activation, gene silencing, X chromosome inactivation, and abnormal chromosome rearrangement in cancer cells. The usage of advanced fluorescence labeling and image processing techniques has enabled researchers to investigate interactions of chromosome territories at large spatial resolution. The resulting high volume of generated data demands for high-throughput and automated image analysis methods. In this paper, we introduce a novel algorithmic tool for investigating association patterns of chromosome territories in a population of cells. Our method takes as input a set of graphs, one for each cell, containing information about spatial interaction of chromosome territories, and yields a single graph that contains essential information for the whole population and stands as its structural representative. We formulate this combinato- rial problem as a semi-definite programming and present novel techniques to efficiently solve it. We validate our approach on both artificial and real biological data; the experimental results suggest that our approach yields a nearoptimal solution, and can handle large-size datasets, which are significant improvements over existing techniques.</p><p>2 0.9394235 <a title="34-lda-2" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>3 0.91463119 <a title="34-lda-3" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>4 0.91044831 <a title="34-lda-4" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>5 0.90995473 <a title="34-lda-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.90822703 <a title="34-lda-6" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>7 0.90670204 <a title="34-lda-7" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>8 0.90488356 <a title="34-lda-8" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>9 0.90474278 <a title="34-lda-9" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>10 0.90465271 <a title="34-lda-10" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>11 0.90417844 <a title="34-lda-11" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>12 0.90410775 <a title="34-lda-12" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>13 0.90406871 <a title="34-lda-13" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>14 0.90216732 <a title="34-lda-14" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>15 0.90204519 <a title="34-lda-15" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>16 0.90190428 <a title="34-lda-16" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>same-paper 17 0.90147257 <a title="34-lda-17" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>18 0.90126663 <a title="34-lda-18" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>19 0.90066189 <a title="34-lda-19" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>20 0.90026003 <a title="34-lda-20" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
