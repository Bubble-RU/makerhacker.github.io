<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-138" href="#">cvpr2013-138</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-138-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hao_Efficient_2D-to-3D_Correspondence_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Qiang Hao, Rui Cai, Zhiwei Li, Lei Zhang, Yanwei Pang, Feng Wu, Yong Rui</p><p>Abstract: 3D model-based object recognition has been a noticeable research trend in recent years. Common methods find 2D-to-3D correspondences and make recognition decisions by pose estimation, whose efficiency usually suffers from noisy correspondences caused by the increasing number of target objects. To overcome this scalability bottleneck, we propose an efficient 2D-to-3D correspondence filtering approach, which combines a light-weight neighborhoodbased step with a finer-grained pairwise step to remove spurious correspondences based on 2D/3D geometric cues. On a dataset of 300 3D objects, our solution achieves ∼10 times speed improvement over the baseline, with a comparable recognition accuracy. A parallel implementation on a quad-core CPU can run at ∼3fps for 1280× 720 images.</p><p>Reference: <a title="cvpr-2013-138-reference" href="../cvpr2013_reference/cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Common methods find 2D-to-3D correspondences and make recognition decisions by pose estimation, whose efficiency usually suffers from noisy correspondences caused by the increasing number of target objects. [sent-8, score-1.253]
</p><p>2 To overcome this scalability bottleneck, we propose an efficient 2D-to-3D correspondence filtering approach, which combines a light-weight neighborhoodbased step with a finer-grained pairwise step to remove spurious correspondences based on 2D/3D geometric cues. [sent-9, score-1.303]
</p><p>3 Putative 2D-to-3D correspondences between the ∗This work was performed at Microsoft Research Asia. [sent-20, score-0.468]
</p><p>4 models in the database as the targets, the task is to efficiently recognize an arbitrary number of objects appearing in each query image and estimate the pose for each identified object. [sent-53, score-0.447]
</p><p>5 Object recognition results are determined by the number of correspondences that are consistent with the estimated pose. [sent-59, score-0.502]
</p><p>6 The quality of putative 2D-to-3D correspondences is crucial for such a framework. [sent-60, score-0.536]
</p><p>7 , the proportion of true correspondences, called inliers) of the correspondences determines the expected number of RANSAC iterations for finding a reliable camera pose, because RANSAC, as a hypothesis-and-test scheme, relies on a randomly drawn set of all-inlier samples to reach a consensus. [sent-63, score-0.644]
</p><p>8 Consequently, a very low inlier ratio may require too many RANSAC iterations and thus lead to the failure of recognition within a limited time [15, 21]. [sent-64, score-0.283]
</p><p>9 Challenges in scalable 3D object recognition Inspired by the success of registration-based 3D object recognition, in this paper we consider a more scalable scenario (as illustrated in Fig. [sent-67, score-0.404]
</p><p>10 (a) The inlier ratio of correspondences decreases, leading to much more necessary RANSAC iterations for pose estimation. [sent-191, score-0.867]
</p><p>11 (b) To achieve stable recognition performance (recall), an increasing number of object hypotheses have to be verified by pose estimation. [sent-192, score-0.431]
</p><p>12 (2) the query images may depict cluttered scenes where an arbitrary number of target objects appear; (3) the recognition should be done efficiently (i. [sent-193, score-0.286]
</p><p>13 Such a scenario requires a stable recognition accuracy and a sub-linear computational complexity, with an increasing number of target objects and an increasing complexity of query images. [sent-199, score-0.373]
</p><p>14 However, this strategy inevitably decreases the inlier ratio of discovered correspondences, due to the following reasons: There is an increasing risk that an image feature from a foreground object accidently matches with a 3D point of irrelevant models that have locally similar appearance. [sent-202, score-0.393]
</p><p>15 To compensate for the loss of true correspondences, it is usually necessary to retain multiple 3D points possibly matched with each image feature, at the expense of more spurious correspondences. [sent-205, score-0.43]
</p><p>16 –  –  –  –  The noisy correspondences increase the computational cost of RANSAC-based pose estimation from two aspects: 1) Number of RANSAC iterations. [sent-206, score-0.662]
</p><p>17 The decreased inlier ratio of correspondences results in much more RANSAC iterations for pose estimation, as illustrated in Fig. [sent-207, score-0.838]
</p><p>18 Noisy correspondences cause spurious object hypotheses which are non-trivial to remove. [sent-210, score-0.964]
</p><p>19 Therefore, to ensure the recall of truly appearing objects whose number is unknown, we have to verify a considerable number of object hypotheses  by pose estimation. [sent-211, score-0.547]
</p><p>20 To reduce the above burden of “many RANSAC iterations multiplied by many hypotheses”, it is crucial to improve the quality of 2D-to-3D correspondences before pose estimation. [sent-214, score-0.682]
</p><p>21 A common strategy is to discard unpromising correspondences instantly during feature matching [15, 21]. [sent-216, score-0.468]
</p><p>22 However, in a scalable scenario where feature matching is less reliable, this strategy has a high risk of missing true correspondences and thus a low recall of recognition. [sent-217, score-0.715]
</p><p>23 , co-visibility [12], co-occurrence [14]) between correspondences to guide the random sampling of RANSAC to reduce the number of iterations. [sent-220, score-0.494]
</p><p>24 Such methods do not filter correspondences prior to RANSAC and thus suffer from the burden of many hypotheses. [sent-221, score-0.556]
</p><p>25 In [20], an explicit procedure is proposed to filter correspondences before RANSAC, but the solution is designed for interimage correspondences rather than 2D-to-3D ones. [sent-222, score-0.972]
</p><p>26 Scalable solution In this paper, we propose an efficient filtering method to bridge the gap between noisy 2D-to-3D correspondences and efficient pose estimation. [sent-225, score-0.895]
</p><p>27 First, a light-weight local filtering step is conducted for each correspondence by  considering both 2D and 3D neighborhoods. [sent-227, score-0.441]
</p><p>28 This step prioritizes the object hypotheses and identifies a promising subset for subsequent processing. [sent-228, score-0.283]
</p><p>29 Then, a global filtering step leverages finer-grained geometric cues to filter each promising object hypothesis separately, in order to identify reliable correspondences. [sent-229, score-0.662]
</p><p>30 , an over 10 times faster speed with a comparable recognition accuracy, than the baseline method without the correspondence filtering stage. [sent-233, score-0.459]
</p><p>31 An illustrative example of different types of spurious 2D-to-3D correspondences (shown in different colors and shapes). [sent-241, score-0.732]
</p><p>32 In the literature, some methods directly remove unreliable correspondences by ratio test on descriptor distances [15, 21] or by geometric cues of pre-defined 3D visual phrases [9], while some others leverage co-visibility [12] or co-occurrence [14] between correspondences to guide RANSAC. [sent-250, score-1.159]
</p><p>33 However, these methods are not applicable to the scalable scenario where correspondences are extremely noisy and related to various 3D models. [sent-251, score-0.661]
</p><p>34 Our work on correspondence filtering before RANSAC is complementary to such methods. [sent-257, score-0.399]
</p><p>35 Solution overview In this section, we first analyze typical spurious correspondences and then briefly introduce our solution for 2Dto-3D correspondence filtering. [sent-259, score-0.866]
</p><p>36 The flowchart of scalable 3D object recognition with the proposed correspondence filtering procedure. [sent-290, score-0.599]
</p><p>37 The amount of 2Dto-3D correspondences (depicted by the cylinder size) consistently decreases after local filtering and global filtering. [sent-291, score-0.775]
</p><p>38 To effectively filter correspondences, we first summarize typical spurious correspondences into three types (as shown in Fig. [sent-293, score-0.768]
</p><p>39 In contrast to various types of spurious correspondences, almost all the true correspondences are geometrically compatible with each other. [sent-302, score-0.854]
</p><p>40 Therefore, an intuitive idea is to filter the correspondences according to the geometric relationships between them. [sent-303, score-0.562]
</p><p>41 However, an exhaustive pairwise filtering could be time-consuming for a large number of correspondences and thus will decay the contribution to efficiency. [sent-304, score-0.799]
</p><p>42 So it is more efficient to first reduce a proportion of spurious correspondences in a light-weight manner. [sent-305, score-0.78]
</p><p>43 According to the above considerations, we propose to filter out different types of spurious correspondences in two steps, as illustrated in Fig. [sent-306, score-0.768]
</p><p>44 First, a local filtering step efficiently checks every individual correspondence in a local region, based on both statistical and geometric 89 8 09 901 9 9  cues including spatial consistency and co-visibility. [sent-308, score-0.638]
</p><p>45 Such a step has a linear time complexity to the total number of correspondences, and can remove most of BKGDNOISE and FALSEMODEL spurious correspondences, which are generated almost randomly and thus are unlikely to pass the checks. [sent-309, score-0.295]
</p><p>46 In this way, local filtering can remove a large proportion of spurious object hypotheses which have few valid correspondences left. [sent-310, score-1.35]
</p><p>47 To further filter the spurious correspondences that survive after local filtering, a global filtering step is performed on each remaining object hypothesis separately to verify its related correspondences in a pairwise manner. [sent-311, score-1.79]
</p><p>48 This step leverages finer-grained 3D geometric cues to evaluate the compatibility between every two correspondences, and finally identifies mutually compatible correspondences for efficient pose estimation. [sent-312, score-0.903]
</p><p>49 Although the pairwise checks have a quadratic complexity, the overall computational cost is well controlled as there are only a few promising object hypotheses to process, and only a reduced set of correspondences in each hypothesis. [sent-313, score-0.822]
</p><p>50 After such a two-step filtering stage, the remaining correspondences are much more accurate and are related to only a few object hypotheses, resulting in a much less effort of the following pose estimation. [sent-314, score-0.906]
</p><p>51 A 2Dto-3D correspondence ci is a triplet (fi, pi , mi), indicating that image feature fi is matched with 3D point pi of object model mi in the database. [sent-318, score-0.774]
</p><p>52 ge, the whole set of correspondences is denoted by C = ? [sent-320, score-0.468]
</p><p>53 , possibly matched models), and Ch = {ci ∈ C | mi = h} is the subset of correspondences related to hypothesis h. [sent-323, score-0.685]
</p><p>54 Local filtering Local filtering estimates the confidence of each correspondence by aggregating support from 2D/3D neighbors. [sent-328, score-0.743]
</p><p>55 From some preliminary experiments, we observe that image features in true correspondences tend to be close to each other, whereas image features in BKGDNOISE and FALSEMODEL spurious correspondences are irregularly distributed. [sent-330, score-1.248]
</p><p>56 If we treat each correspondence ci = (fi, pi, mi) as a vote for model mi at the 2D location of feature fi, true correspondences tend to reach local consensus whereas spurious ones vote for inconsistent models irregularly. [sent-331, score-1.104]
</p><p>57 h∈H Ch of putative 2D-to-3D correspondences Output: Camera pose Ph∗ for each object hypothesis h ∈ H begin / / Local F i ering lt  endf Honrd←eaECchGostmeich pyoturMaptore ts2h cpeotDsnCi-f3doiDhne lf∈ocie Hadcle s∈dnou tpCfEol(dhret)mLferSon2mtDsC-3(hDHc)/ Eq. [sent-333, score-0.823]
</p><p>58 2D local support is useful for filtering most BKGDNOISE and FALSEMODEL spurious correspondences. [sent-337, score-0.601]
</p><p>59 However, there are still a proportion of such correspondences (e. [sent-338, score-0.516]
</p><p>60 Since an object appearing in an image is actually a 3Dto-2D projection of the corresponding 3D point cloud, we note that if two 3D points pi and pj are close enough in a 3D model, their 2D projections (i. [sent-342, score-0.334]
</p><p>61 , up to 5%) of 3D points in model mi that are not only spatially closest to pi but also co-visible with pi from some viewpoints. [sent-347, score-0.34]
</p><p>62 As the 2D-3D local support roughly rates the confidence of each correspondence, it is straightforward to estimate the confidence of each object hypothesis h by aggregating the local support of related correspondences Ch as  conf(h)  = ? [sent-351, score-0.876]
</p><p>63 Each retained hypothesis is also filtered to remove correspondences that have no local support. [sent-354, score-0.682]
</p><p>64 Global filtering After local filtering, putative 2D-to-3D correspondences are roughly filtered and grouped into a small set of promising object hypotheses. [sent-357, score-0.946]
</p><p>65 However, there still might be spurious correspondences (especially FALSEPOINT ones) survival, leading to inefficiency in pose estimation. [sent-358, score-0.853]
</p><p>66 It  is therefore necessary to further filter correspondences for each hypothesis from a complementary perspective by checking the 3D geometric compatibility extensively between correspondences. [sent-359, score-0.799]
</p><p>67 To efficiently check whether a set of correspondences are mutually compatible, i. [sent-361, score-0.506]
</p><p>68 Therefore, if two correspondences ci and cj are compatible, the distance between 3D points pi andpj in the model coordinate system should be preserved in the camera coordinate system. [sent-370, score-0.838]
</p><p>69 (a) The compatibility between two correspondences is verified by comparing the inter-point Euclidean distance in model coordinates and that in camera coordinates (obtained by backprojecting 2D local features to 3D locations). [sent-375, score-0.607]
</p><p>70 in camera coordinates based on the pinhole camera model, by back-projecting local feature fi in image coordinates  v(fi))? [sent-377, score-0.276]
</p><p>71 Meanwhile, the observed size of pi in a 2D image, represented by the scale of local feature fi, is correlated with the depth and focal length in a pinhole camera model (as shown in Fig. [sent-382, score-0.29]
</p><p>72 For each hypothesis h, we check pairwise compatibility between correspondences in Ch and record the results in an undirected graph 1In this work we use a webcam with a fixed focal length, which is estimated by calibration. [sent-389, score-0.781]
</p><p>73 Gh = (Ch, Eh), in which each vertex is a correspondence and the edges are created for compatible vertices as Eh = {{ci, cj } | fi fj , pi pj , IsCovis(pi, pj) , (7) ? [sent-396, score-0.612]
</p><p>74 Such a graph can accelerate RANSAC-based pose estimation by guiding the random sampling of L 3 mutually compatible correspondences, which correspond to complete subgraphs of order L. [sent-407, score-0.3]
</p><p>75 With L = 3 in this work (for P3P pose solver), it is straightforward to randomly sample L correspondences from the refined graph, by first drawing two adjacent vertices and then drawing another vertex adjacent to both of them. [sent-416, score-0.589]
</p><p>76 The computation of feature matching, correspondence filtering, and pose estimation is  #M30od0els3#DI7O4m,7ab0gj8e sct M#13,4Dod2 Pe,lo5iDn21atsa#bD1as0e, s9c4r5i,p1t o5rs#Im20a Tge s t#SOe5bt0j ects  Table 1. [sent-428, score-0.287]
</p><p>77 Finally, these feature matches are aggregated into correspondences between image features and 3D points. [sent-457, score-0.468]
</p><p>78 Evaluation of correspondence filtering Local filtering and global filtering were evaluated to verify their respective contributions to the proposed solution. [sent-460, score-0.97]
</p><p>79 We first evaluated the effectiveness of local filtering in identifying promising object 3The dataset is publicly available at http://research. [sent-462, score-0.41]
</p><p>80 Performance comparison of different object hypothesis ranking methods with increasing object database size. [sent-546, score-0.335]
</p><p>81 The improvement in correspondence inlier ratio (shown in logarithmic scale) after local filtering and global filtering, computed on the entire test set. [sent-592, score-0.649]
</p><p>82 7 indicate that 2D-3D-LF significantly outperforms the baselines, in which 2D-LF lacks consistency check in 3D and thus overestimates the confidence of some spurious hypotheses. [sent-597, score-0.349]
</p><p>83 7 (b), 2D-3D-LF can rank most of (≥ 70%) true hypotheses at top N, where N increases slowly with the database size and is set to N= 25 for all the database sizes in the following experiments. [sent-599, score-0.402]
</p><p>84 We evaluated the effectiveness of local filtering (LF) and global filtering (GF) in improving the inlier ratio, i. [sent-601, score-0.73]
</p><p>85 , the proportion oftrue correspondences out of all correspondences in the test set. [sent-603, score-0.984]
</p><p>86 The ground-truth of true correspondences were obtained by exhaustive RANSAC. [sent-604, score-0.542]
</p><p>87 The correspondences after LF are collected from top N = 25 object hypotheses, while the remaining correspondences after GF are those connected by strong edges in the refined correspondence graphs. [sent-605, score-1.122]
</p><p>88 With the database size increasing, the improved inlier ratio drops much more slowly than the raw inlier ratio, which is very sensitive to the database size. [sent-608, score-0.54]
</p><p>89 Evaluation of scalable 3D object recognition In this subsection, we report the evaluation results of the proposed solution in the scalable 3D object recognition task, with the database size M ranging from 25 to 300. [sent-650, score-0.473]
</p><p>90 Our full solution, abbreviated as LF+GF, were compared with several baseline methods as follows: GF (pure Global Filtering) directly performs global filtering and pose estimation for all the object hypotheses. [sent-652, score-0.504]
</p><p>91 LF (pure Local Filtering) directly performs pose estimation for top N object hypotheses after local filtering. [sent-653, score-0.427]
</p><p>92 LF+VC (View-Constrained RANSAC [12]) leverages co-visibility between 3D points to guide pose estimation for top N hypotheses identified by local filtering. [sent-654, score-0.492]
</p><p>93 EV (Exhaustive Verification) verifies all the object hypotheses by RANSAC-based pose estimation. [sent-655, score-0.353]
</p><p>94 An object hypothesis is accepted as a recognized object if the estimated pose has at least 8 inlier correspondences. [sent-658, score-0.497]
</p><p>95 The effectiveness of object recognition is measured by the precision and recall computed from the recognized objects and the known objects in query images. [sent-660, score-0.346]
</p><p>96 2) With the database size M increasing, the performance of LF+GF and GF is more stable than that of other methods, especially EV and EV+VC which suffer much from noisy correspondences on large databases (M ≥ 200). [sent-666, score-0.582]
</p><p>97 4) Global filtering greatly reduces the number of necessary RANSAC iterations due to the power of pairwise geometric validation 9 9 90 0 05 3 3  aontCm)stplmuoi(1 . [sent-668, score-0.433]
</p><p>98 With the database size increasing from 25 to 300, GF and EV+VC have an over 6-fold increase in time cost, less scalable than LF+GF and LF+VC which rely on local filtering to reduce spurious hypotheses. [sent-688, score-0.802]
</p><p>99 Conclusion and future work We have presented an efficient approach for filtering highly noisy 2D-to-3D correspondences. [sent-693, score-0.306]
</p><p>100 The proposed method leverages several 2D/3D geometric cues to remove spurious correspondences, and can significantly reduce the computational burden of RANSAC-based pose estimation. [sent-694, score-0.612]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('correspondences', 0.468), ('filtering', 0.265), ('spurious', 0.264), ('gf', 0.227), ('ransac', 0.213), ('lf', 0.213), ('hypotheses', 0.18), ('inlier', 0.158), ('query', 0.147), ('bkgdnoise', 0.143), ('falsemodel', 0.143), ('vc', 0.142), ('correspondence', 0.134), ('fi', 0.13), ('pi', 0.128), ('pose', 0.121), ('hypothesis', 0.114), ('scalable', 0.114), ('ci', 0.099), ('ev', 0.083), ('compatible', 0.074), ('database', 0.073), ('appearing', 0.073), ('cj', 0.069), ('putative', 0.068), ('accidently', 0.063), ('geometric', 0.058), ('compatibility', 0.058), ('leverages', 0.056), ('focal', 0.055), ('matched', 0.054), ('agh', 0.053), ('burden', 0.052), ('object', 0.052), ('pages', 0.051), ('ann', 0.051), ('promising', 0.051), ('ch', 0.05), ('ratio', 0.05), ('mi', 0.049), ('confidence', 0.049), ('true', 0.048), ('proportion', 0.048), ('falsepoint', 0.048), ('iscovis', 0.048), ('recall', 0.047), ('pj', 0.046), ('snavely', 0.046), ('webcam', 0.046), ('efficiency', 0.044), ('increasing', 0.044), ('scalability', 0.043), ('qi', 0.043), ('local', 0.042), ('noisy', 0.041), ('iterations', 0.041), ('verify', 0.041), ('pairwise', 0.04), ('camera', 0.039), ('cluttered', 0.039), ('ipi', 0.039), ('execution', 0.038), ('scenario', 0.038), ('mutually', 0.038), ('eh', 0.038), ('checking', 0.036), ('consistency', 0.036), ('filter', 0.036), ('accelerate', 0.035), ('points', 0.035), ('survival', 0.035), ('recognition', 0.034), ('abbreviated', 0.034), ('objects', 0.033), ('target', 0.033), ('estimation', 0.032), ('viewpoint', 0.032), ('fj', 0.031), ('checks', 0.031), ('remove', 0.031), ('meanwhile', 0.03), ('support', 0.03), ('cues', 0.03), ('trend', 0.029), ('sattler', 0.029), ('cloud', 0.029), ('necessary', 0.029), ('viewpoints', 0.029), ('phrases', 0.028), ('slowly', 0.028), ('photo', 0.028), ('retained', 0.027), ('rui', 0.027), ('enhances', 0.027), ('hundreds', 0.026), ('irrelevant', 0.026), ('pinhole', 0.026), ('guide', 0.026), ('exhaustive', 0.026), ('speed', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="138-tfidf-1" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>Author: Qiang Hao, Rui Cai, Zhiwei Li, Lei Zhang, Yanwei Pang, Feng Wu, Yong Rui</p><p>Abstract: 3D model-based object recognition has been a noticeable research trend in recent years. Common methods find 2D-to-3D correspondences and make recognition decisions by pose estimation, whose efficiency usually suffers from noisy correspondences caused by the increasing number of target objects. To overcome this scalability bottleneck, we propose an efficient 2D-to-3D correspondence filtering approach, which combines a light-weight neighborhoodbased step with a finer-grained pairwise step to remove spurious correspondences based on 2D/3D geometric cues. On a dataset of 300 3D objects, our solution achieves ∼10 times speed improvement over the baseline, with a comparable recognition accuracy. A parallel implementation on a quad-core CPU can run at ∼3fps for 1280× 720 images.</p><p>2 0.23091722 <a title="138-tfidf-2" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>3 0.20302151 <a title="138-tfidf-3" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>4 0.19996966 <a title="138-tfidf-4" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>5 0.17759725 <a title="138-tfidf-5" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>Author: Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, Andrew Fitzgibbon</p><p>Abstract: We address the problem of inferring the pose of an RGB-D camera relative to a known 3D scene, given only a single acquired image. Our approach employs a regression forest that is capable of inferring an estimate of each pixel’s correspondence to 3D points in the scene ’s world coordinate frame. The forest uses only simple depth and RGB pixel comparison features, and does not require the computation of feature descriptors. The forest is trained to be capable of predicting correspondences at any pixel, so no interest point detectors are required. The camera pose is inferred using a robust optimization scheme. This starts with an initial set of hypothesized camera poses, constructed by applying the forest at a small fraction of image pixels. Preemptive RANSAC then iterates sampling more pixels at which to evaluate the forest, counting inliers, and refining the hypothesized poses. We evaluate on several varied scenes captured with an RGB-D camera and observe that the proposed technique achieves highly accurate relocalization and substantially out-performs two state of the art baselines.</p><p>6 0.16225018 <a title="138-tfidf-6" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>7 0.14223564 <a title="138-tfidf-7" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>8 0.13965815 <a title="138-tfidf-8" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>9 0.13520472 <a title="138-tfidf-9" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>10 0.12638135 <a title="138-tfidf-10" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>11 0.12497072 <a title="138-tfidf-11" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>12 0.12090281 <a title="138-tfidf-12" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>13 0.12011804 <a title="138-tfidf-13" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>14 0.11736652 <a title="138-tfidf-14" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>15 0.11407873 <a title="138-tfidf-15" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>16 0.11306064 <a title="138-tfidf-16" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>17 0.11202654 <a title="138-tfidf-17" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>18 0.11103817 <a title="138-tfidf-18" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>19 0.1092441 <a title="138-tfidf-19" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>20 0.10499782 <a title="138-tfidf-20" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.224), (1, 0.078), (2, 0.016), (3, -0.002), (4, 0.056), (5, -0.034), (6, -0.015), (7, -0.05), (8, -0.026), (9, -0.052), (10, -0.024), (11, 0.168), (12, 0.063), (13, 0.036), (14, -0.006), (15, -0.238), (16, 0.022), (17, 0.041), (18, 0.074), (19, -0.06), (20, 0.065), (21, -0.075), (22, -0.001), (23, 0.007), (24, 0.067), (25, -0.173), (26, -0.087), (27, -0.023), (28, 0.022), (29, 0.095), (30, -0.004), (31, -0.034), (32, -0.048), (33, -0.024), (34, 0.104), (35, 0.021), (36, 0.016), (37, 0.09), (38, 0.054), (39, 0.078), (40, -0.033), (41, 0.0), (42, 0.055), (43, -0.039), (44, -0.008), (45, 0.099), (46, 0.18), (47, 0.015), (48, 0.072), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96235991 <a title="138-lsi-1" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>Author: Qiang Hao, Rui Cai, Zhiwei Li, Lei Zhang, Yanwei Pang, Feng Wu, Yong Rui</p><p>Abstract: 3D model-based object recognition has been a noticeable research trend in recent years. Common methods find 2D-to-3D correspondences and make recognition decisions by pose estimation, whose efficiency usually suffers from noisy correspondences caused by the increasing number of target objects. To overcome this scalability bottleneck, we propose an efficient 2D-to-3D correspondence filtering approach, which combines a light-weight neighborhoodbased step with a finer-grained pairwise step to remove spurious correspondences based on 2D/3D geometric cues. On a dataset of 300 3D objects, our solution achieves ∼10 times speed improvement over the baseline, with a comparable recognition accuracy. A parallel implementation on a quad-core CPU can run at ∼3fps for 1280× 720 images.</p><p>2 0.87858713 <a title="138-lsi-2" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>3 0.81202692 <a title="138-lsi-3" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>Author: Victor Fragoso, Matthew Turk</p><p>Abstract: We present SWIGS, a Swift and efficient Guided Sampling method for robust model estimation from image feature correspondences. Our method leverages the accuracy of our new confidence measure (MR-Rayleigh), which assigns a correctness-confidence to a putative correspondence in an online fashion. MR-Rayleigh is inspired by Meta-Recognition (MR), an algorithm that aims to predict when a classifier’s outcome is correct. We demonstrate that by using a Rayleigh distribution, the prediction accuracy of MR can be improved considerably. Our experiments show that MR-Rayleigh tends to predict better than the often-used Lowe ’s ratio, Brown’s ratio, and the standard MR under a range of imaging conditions. Furthermore, our homography estimation experiment demonstrates that SWIGS performs similarly or better than other guided sampling methods while requiring fewer iterations, leading to fast and accurate model estimates.</p><p>4 0.70537472 <a title="138-lsi-4" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>5 0.66033047 <a title="138-lsi-5" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>Author: Gim Hee Lee, Friedrich Faundorfer, Marc Pollefeys</p><p>Abstract: In this paper, we present a visual ego-motion estimation algorithm for a self-driving car equipped with a closeto-market multi-camera system. By modeling the multicamera system as a generalized camera and applying the non-holonomic motion constraint of a car, we show that this leads to a novel 2-point minimal solution for the generalized essential matrix where the full relative motion including metric scale can be obtained. We provide the analytical solutions for the general case with at least one inter-camera correspondence and a special case with only intra-camera correspondences. We show that up to a maximum of 6 solutions exist for both cases. We identify the existence of degeneracy when the car undergoes straight motion in the special case with only intra-camera correspondences where the scale becomes unobservable and provide a practical alternative solution. Our formulation can be efficiently implemented within RANSAC for robust estimation. We verify the validity of our assumptions on the motion model by comparing our results on a large real-world dataset collected by a car equipped with 4 cameras with minimal overlapping field-of-views against the GPS/INS ground truth.</p><p>6 0.6567294 <a title="138-lsi-6" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>7 0.63846099 <a title="138-lsi-7" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>8 0.63278061 <a title="138-lsi-8" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>9 0.61859369 <a title="138-lsi-9" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>10 0.60249609 <a title="138-lsi-10" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>11 0.58129036 <a title="138-lsi-11" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>12 0.5426622 <a title="138-lsi-12" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>13 0.53911477 <a title="138-lsi-13" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>14 0.53531957 <a title="138-lsi-14" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>15 0.52438217 <a title="138-lsi-15" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>16 0.51242512 <a title="138-lsi-16" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>17 0.51055855 <a title="138-lsi-17" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>18 0.50869274 <a title="138-lsi-18" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>19 0.50803965 <a title="138-lsi-19" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>20 0.50664133 <a title="138-lsi-20" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.087), (16, 0.332), (26, 0.043), (33, 0.283), (67, 0.073), (69, 0.035), (87, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97987807 <a title="138-lda-1" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>Author: Hyeongwoo Kim, Hailin Jin, Sunil Hadap, Inso Kweon</p><p>Abstract: We present a novel method to separate specular reflection from a single image. Separating an image into diffuse and specular components is an ill-posed problem due to lack of observations. Existing methods rely on a specularfree image to detect and estimate specularity, which however may confuse diffuse pixels with the same hue but a different saturation value as specular pixels. Our method is based on a novel observation that for most natural images the dark channel can provide an approximate specular-free image. We also propose a maximum a posteriori formulation which robustly recovers the specular reflection and chromaticity despite of the hue-saturation ambiguity. We demonstrate the effectiveness of the proposed algorithm on real and synthetic examples. Experimental results show that our method significantly outperforms the state-of-theart methods in separating specular reflection.</p><p>2 0.92199105 <a title="138-lda-2" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>Author: Guha Balakrishnan, Fredo Durand, John Guttag</p><p>Abstract: We extract heart rate and beat lengths from videos by measuring subtle head motion caused by the Newtonian reaction to the influx of blood at each beat. Our method tracks features on the head and performs principal component analysis (PCA) to decompose their trajectories into a set of component motions. It then chooses the component that best corresponds to heartbeats based on its temporal frequency spectrum. Finally, we analyze the motion projected to this component and identify peaks of the trajectories, which correspond to heartbeats. When evaluated on 18 subjects, our approach reported heart rates nearly identical to an electrocardiogram device. Additionally we were able to capture clinically relevant information about heart rate variability.</p><p>3 0.89914 <a title="138-lda-3" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>4 0.88680911 <a title="138-lda-4" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>Author: Ahmed T. Kamal, Jay A. Farrell, Amit K. Roy-Chowdhury</p><p>Abstract: Due to their high fault-tolerance, ease of installation and scalability to large networks, distributed algorithms have recently gained immense popularity in the sensor networks community, especially in computer vision. Multitarget tracking in a camera network is one of the fundamental problems in this domain. Distributed estimation algorithms work by exchanging information between sensors that are communication neighbors. Since most cameras are directional sensors, it is often the case that neighboring sensors may not be sensing the same target. Such sensors that do not have information about a target are termed as “naive ” with respect to that target. In this paper, we propose consensus-based distributed multi-target tracking algorithms in a camera network that are designed to address this issue of naivety. The estimation errors in tracking and data association, as well as the effect of naivety, are jointly addressed leading to the development of an informationweighted consensus algorithm, which we term as the Multitarget Information Consensus (MTIC) algorithm. The incorporation of the probabilistic data association mecha- nism makes the MTIC algorithm very robust to false measurements/clutter. Experimental analysis is provided to support the theoretical results.</p><p>5 0.87863803 <a title="138-lda-5" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>Author: Wei Li, Xiaogang Wang</p><p>Abstract: In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person reidentification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsityinducing norm and information theoretical regularization. . cuhk . edu .hk (a) Camera view A (b) Camera view B This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.</p><p>6 0.87422824 <a title="138-lda-6" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>same-paper 7 0.87323052 <a title="138-lda-7" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>8 0.84787458 <a title="138-lda-8" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>9 0.83355033 <a title="138-lda-9" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>10 0.78456873 <a title="138-lda-10" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>11 0.77638245 <a title="138-lda-11" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>12 0.77565295 <a title="138-lda-12" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>13 0.77448779 <a title="138-lda-13" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>14 0.77286929 <a title="138-lda-14" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>15 0.7659055 <a title="138-lda-15" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>16 0.76544046 <a title="138-lda-16" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>17 0.76248413 <a title="138-lda-17" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>18 0.7616291 <a title="138-lda-18" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>19 0.75946575 <a title="138-lda-19" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>20 0.7531594 <a title="138-lda-20" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
