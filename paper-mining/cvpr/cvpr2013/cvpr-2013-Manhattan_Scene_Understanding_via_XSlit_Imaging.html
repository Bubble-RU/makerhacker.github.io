<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-279" href="#">cvpr2013-279</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</h1>
<br/><p>Source: <a title="cvpr-2013-279-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ye_Manhattan_Scene_Understanding_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>Reference: <a title="cvpr-2013-279-reference" href="../cvpr2013_reference/cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ,yu} s  Abstract A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. [sent-3, score-0.237]
</p><p>2 Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. [sent-4, score-0.335]
</p><p>3 Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). [sent-7, score-0.224]
</p><p>4 In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). [sent-8, score-0.212]
</p><p>5 Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. [sent-11, score-0.175]
</p><p>6 Introduction  A pinhole camera collects rays passing through a common Center-of-Projection (CoP) and has been the dominating imaging model for computer vision tasks. [sent-14, score-0.246]
</p><p>7 First, pinhole geometry is simple; it is uniquely defined by only 3 parameters (the position of CoP in 3D) and its imaging process can be uniformly described by the classic 3 4 pinhole camera umnaitfroixrm [1ly2 d]. [sent-16, score-0.287]
</p><p>8 , they observe lines as lines and parallel lines converging at a vanishing point. [sent-19, score-0.533]
</p><p>9 In this paper, we demonstrate using a special multi-perspective camera, the XSlit camera [30], for reconstructing the Manhattan World scenes. [sent-25, score-0.104]
</p><p>10 A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned  with three mutually orthogonal principal axes. [sent-27, score-0.237]
</p><p>11 Tremendous efforts have been focused on reconstructing MW from images [1, 5, 10, 11] and using the MW assumption for camera calibration [3, 15, 24]. [sent-29, score-0.104]
</p><p>12 [10, 11] assign a plane to each pixel and then apply graph-cut on discretized plane parameters. [sent-32, score-0.11]
</p><p>13 Most previous approaches exploit monocular cues such as the vanishing points and the reference planes (e. [sent-34, score-0.19]
</p><p>14 Kosecka and Zhang [15] detect line structures in the image for recovering the vanishing points and camera parameters [29]. [sent-42, score-0.262]
</p><p>15 [4] use the vanishing points and ground plane as priors for recovering affine scene structures. [sent-44, score-0.216]
</p><p>16 We observe that the core challenge in pinhole-based solutions is coplanar ambiguities: although one can easily detect the vanishing point of a group of parallel 3D lines, there is an ambiguity on which lines belong to the same plane. [sent-52, score-0.435]
</p><p>17 Conceptually, 3D parallel lines will be mapped to 2D curves in a multi-perspective camera and these curves will intersect at multiple points instead of a single vanishing point. [sent-54, score-0.506]
</p><p>18 [2] examine the curves in a non-centric catadioptric camera for line localization. [sent-56, score-0.145]
</p><p>19 In this paper, we show how to group parallel 3D lines on the same plane by analyzing their images in a special multi-perspective cameras, the XSlit camera [30]. [sent-59, score-0.347]
</p><p>20 We show that same as in the pinhole camera, images of parallel lines in an XSlit image, although curved, will still converge at a vanishing point, i. [sent-60, score-0.385]
</p><p>21 What is different though is that images of coplanar 3D lines will generally intersect at a second common point that we call Coplanar Common Point or CCP. [sent-63, score-0.294]
</p><p>22 We show that the geometry of 3D lines can be directly recovered from their XVPs and CCPs. [sent-66, score-0.176]
</p><p>23 Finally, we construct a prototype XSlit camera by using two layers of cylindrical lenses. [sent-68, score-0.175]
</p><p>24 Experimental methods on both synthetic and real data show that our XSlit camera based solution provides an effective and reliable solution for MW scene understanding. [sent-69, score-0.128]
</p><p>25 Our contributions include: • A new theory to characterize the XVP and CCP of coplanar parallel 3D lines in an XSlit image. [sent-70, score-0.316]
</p><p>26 • A prototype XSlit camera for validating our theory. [sent-72, score-0.132]
</p><p>27 collected in the camera and the mapping from the ray to a pixel. [sent-77, score-0.16]
</p><p>28 In 2PP, each ray is parameterized as [u, v, s, t], where [u, v] and [s, t] are the intersections with the two parallel planes Πuv and Πst lying at z = 0 and z = 1respectively. [sent-79, score-0.306]
</p><p>29 XSlit Camera Geometry  An XSlit camera collects rays that simultaneously pass through two oblique (neither parallel nor coplanar) slits in 3D space [30]. [sent-86, score-0.38]
</p><p>30 Given two slits l1 and l2, we construct the 2PP as follows: we choose Πuv and Πst that are parallel to both slits but do not contain them, as shown in Fig. [sent-87, score-0.335]
</p><p>31 Next, we orthogonally project both slits on Πuv and use their intersection point as the origin of the coordinate system. [sent-89, score-0.137]
</p><p>32 We first explore ray geometry constraints for all rays in an XSlit camera. [sent-93, score-0.162]
</p><p>33 τσ == ( CAuu + + D Bvv) / EE 888222  where  (2)  A = d2xdy1Z2 − dx1d2yZ1,  B = d1xdx2(Z1 − Z2),  D = d2xdy1Z1 − dx1d2yZ2,  C = d1ydy2(Z2 − Z1),  E = (d1xdy2 − dx2d1y)Z1Z2  =  Recall that the two slits are oblique, therefore E 0. [sent-96, score-0.12]
</p><p>34 a ray r[u, v, σ, τ] passes through l, there must exist some λ and λl so that [u, v, 0] + λ[σ, τ, 1] = [xl , yl , zl] + λl [dlx , dly, 0] It is easy to see that λ = linear constraint:  zl  (3)  . [sent-113, score-0.121]
</p><p>35 ec Πtion of land therefore all 3D lines parallel to Πuv will be mapped to 2D parallel lines. [sent-124, score-0.301]
</p><p>36 Similar to case 1, there must exist some and λl so that  λ  [u, v, 0] + λ[σ,  τ,  1] = [ul , vl , 0] + λl [σl , τl , 1]  λ  λ  andλl, we obtain a  σ − σl  (6)  We have = λl and eliminating bilinear constraint: u − ul v vl −  =  (5)  τ  −  τl  We call Eqn. [sent-126, score-0.14]
</p><p>37 , the image of las 1Although slits are essentially lines, we distinguish them two for clarity: slits refer to the XSlit camera geometry and lines refer to 3D scene. [sent-133, score-0.472]
</p><p>38 The last crucial ray geometry constraint is for rays lying on a common plane. [sent-146, score-0.219]
</p><p>39 This lies at the core of this paper as our goal is to disambiguate parallel 3D lines lying on different planes. [sent-147, score-0.248]
</p><p>40 Given a plane Π Πuv : nxx + nyy + nzz + d = 0, with = [nx , ny, nz] being the normal and d the offset, we can intersect Π with Πuv at line: nxu + nyv + d = 0 (9) All rays [u, v, σ, τ] that lie on Π must originate from this line, i. [sent-148, score-0.189]
</p><p>41 XSlit Vanishing Points (XVP) Next, we use the ray geometry constraints for studying the vanishing points of parallel 3D lines in an XSlit image. [sent-156, score-0.435]
</p><p>42 Given a set of parallel lines L ∦ Πuv, their images on 1Π. [sent-158, score-0.206]
</p><p>43 The results are independent aosf tthhee origin of the line and therefore the images of parallel 3D lines, although being hyperbolas, have a vanishing point. [sent-164, score-0.241]
</p><p>44 Coplanar Common Points (CCP)  What differs XSlit from pinhole cameras and hence makes it appealing is that parallel 3D lines lying on a plane will converge at a second common point in an XSlit image. [sent-168, score-0.392]
</p><p>45 Given a set of lines L that lie on plane Π uTnhpeaorraelmlel 2to. [sent-170, score-0.189]
</p><p>46 t Ghei tewno asl sites,t t ohfei lrin images hina tth leie e X oSnlit p camera generally intersect at a second common point, the Coplanar Common Point or CCP. [sent-171, score-0.159]
</p><p>47 Notice that the CCP corresponds to some ray r that 1) is collected by the XSlit, 2) lies on Π, and 3) will intersect all lines in L, as shown in Fig. [sent-173, score-0.243]
</p><p>48 For the pinhole camera to have a ray to lie completely on a 3D plane, the plane has to pass the pinhole. [sent-191, score-0.312]
</p><p>49 Recovering Planes We first show how to recover a plane Π that contains parallel 3D lines L using their CCP and XVP. [sent-197, score-0.276]
</p><p>50 Given a set of coplanar parallel lines L, if they hTahveeo a CmC 3P. [sent-199, score-0.316]
</p><p>51 Notice that the CCP corresponds to a ray that intersect all lines at a finite distance whereas the XVP corresponds to a ray that intersects the lines at the infinite distance. [sent-202, score-0.428]
</p><p>52 στv == ( CAuuvv++ D Bvv v) / EE  (17)  Now consider the CCP [uc, vc] that also corresponds to a ray lying on Π. [sent-209, score-0.116]
</p><p>53 Manhattan World An important requirement for applying the plane recovery algorithm is to know which point is CCP and which one is XVP, as they both appear as the common intersection points ofthe curves. [sent-221, score-0.117]
</p><p>54 In particular, if only one set ofcoplanar parallel lines is available, we cannot distinguish CCP from XVP. [sent-222, score-0.206]
</p><p>55 In reality, a typical Manhattan scene contains multiple sets of coplanar parallel lines for resolving this ambiguity. [sent-224, score-0.342]
</p><p>56 Manhattan World (MW) Assumption [3]: We assume that objects in the scene are composed of planes and lines aligned with three mutually orthogonal principal axes, i. [sent-226, score-0.238]
</p><p>57 , it is parallel to ΠL2L3, lies on the line XVP2-XVP3. [sent-233, score-0.136]
</p><p>58 Therefore, the CCPs for all planes parallel to ΠL2L3 will lie on the line XVP2-XVP3. [sent-254, score-0.229]
</p><p>59 Similar conclusions hold for planes parallel to the other two principal  (a)(b)  Figure 3. [sent-255, score-0.181]
</p><p>60 The blue dots are the outliers and the yellow ones are either XVPs or CCPs; (b) We fit three lines using only the yellow dots. [sent-258, score-0.125]
</p><p>61 Therefore, Theorem 4 provides an effective and robust means for disambiguating CCPs and XVPs: in a MW scene, all CCPs and XVPs should lie on a triangle where XVPs correspond to the triangle vertices and CCPs lie on triangle edges (or the extension of edges). [sent-261, score-0.156]
</p><p>62 MW Scene Reconstruction In order to use Theorem 4 for reconstructing a MW scene, we strategically tilt our XSlit camera to make the slits unparallel to the principal axes L1, L2, and L3 of the buildings so that we can use XVPs and CCPs of different building faces. [sent-264, score-0.304]
</p><p>63 We first fit conics to images of the lines  and compute pairwise intersections. [sent-266, score-0.208]
</p><p>64 2 that the images of lines are hyperbolas with the form: + + C˜v2 + + + = 0 where  A˜, B˜,  A˜u2  C˜  B˜uv  D˜u E˜v F˜  and are uniquely determined by the XSlit camera intrinsics that can be precomputed and are identical for all hyperbolas. [sent-269, score-0.239]
</p><p>65 We apply a similar curve fitting scheme as [7] by forming an over-determined linear system of conic coefficients using the sampled points on the curves. [sent-270, score-0.126]
</p><p>66 Notice that in addition to XVPs and CCPs, every two conics that correspond to two unparallel 3D lines may also intersect. [sent-274, score-0.225]
</p><p>67 Therefore, we fit three lines using the rest intersections and use the resulting triangle vertices and edges to separate the XVPs from the CCPs. [sent-277, score-0.196]
</p><p>68 3 illustrates this process for a simple scene composed of 18 lines on 6 planes. [sent-279, score-0.137]
</p><p>69 Each plane has 3 parallel lines lying on it and the directions of all lines are aligned with the three principal axes. [sent-280, score-0.43]
</p><p>70 We further map each curve segment back to a 3D line segment by intersecting the XSlit rays originated from the conic with the reconstructed plane. [sent-293, score-0.185]
</p><p>71 , a POX-Slit [30]) and the two slits and the image plane are evenly spaced. [sent-308, score-0.175]
</p><p>72 We rotate the camera 45◦ around the y axis and 15◦ around both x and z axes so that axis-aligned lines will have XVPs and CCPs in the XSlit image. [sent-309, score-0.215]
</p><p>73 We also re-render the recovered faces using a perspective camera as shown in Fig. [sent-326, score-0.151]
</p><p>74 In this example, we use the image synthesis technique for producing an XSlit panorama [25, 30]: we translate a perspective camera horizontally from left to right with constant speed and then stitch linearly varying columns of pixels. [sent-337, score-0.121]
</p><p>75 Each view is rendered at resolution of 300 600 and the perspective camera views the scene forfo m30 top t 6o0 d0o awnnd a tnhde t pieltersdp by 1v5e◦ c aarmoeunrad vthieew z tahxeis. [sent-338, score-0.147]
</p><p>76 (a) We use two layers of cylindrical lenses, each with a slit aperture; (b) We mount the XSlit lens on an interchangeable lens camera. [sent-351, score-0.189]
</p><p>77 We re-render the reconstructed geometry using a perspective camera and compare it with the ground truth (Fig. [sent-354, score-0.156]
</p><p>78 Real Scene Experiments Finally, we have constructed a prototype XSlit camera by modifying a commercial interchangeable lens camera (Sony NEX-5N). [sent-359, score-0.284]
</p><p>79 We replace its lens with a pair of cylindrical lenses, each using two slit apertures as shown in Fig. [sent-360, score-0.144]
</p><p>80 We choose to modify an interchangeable lens camera rather than an SLR is that it has a shorter flange focal distance (FFD), i. [sent-362, score-0.152]
</p><p>81 This indicates that we need to put the camera closer to the objects as well as use lenses with a large field-of-view and a smaller focal length. [sent-370, score-0.118]
</p><p>82 The mirror-free interchangeable camera has a much shorter FFD than SLRs and therefore highly suitable. [sent-371, score-0.123]
</p><p>83 To calibrate the XSlit camera, we use a pattern of five lines and use an auxiliary perspective camera to determine line positions and orientations. [sent-374, score-0.273]
</p><p>84 7, we construct a scene composed of the parallel lines lying on two different planes. [sent-377, score-0.274]
</p><p>85 When viewed by a perspective camera, the lines appear nearly identical: although they intersect at a common vanishing point, it is difficult to tell if they belong to different planes, as shown in Fig. [sent-379, score-0.339]
</p><p>86 In contrast, these lines are apparently different in our XSlit camera image, as shown in Fig. [sent-381, score-0.197]
</p><p>87 Next, we apply the conic fitting and CCP detection methods on these curves and we are able to identify one XVP and two CCPs. [sent-383, score-0.107]
</p><p>88 7(d) maps the recovered planes (highlighted in red and green) back onto  plane 2plane 1(a)CaXmSelitr a(b) Figure7. [sent-385, score-0.155]
</p><p>89 (a) Scene acquisition; (b) A perspective image; (c) An XSlit image; (d) We detect the two planes (highlighted in red and green) using the XSlit image. [sent-387, score-0.105]
</p><p>90 e7ct”iv ×e and orient the XSlilt camera to guarantee it observes enough curviness of vertical parallel lines. [sent-398, score-0.202]
</p><p>91 Without any processing, the XSlit image shows that the four groups of parallel lines exhibit different curviness. [sent-399, score-0.206]
</p><p>92 Our solution directly resolves parallel line ambiguity by utilizing a unique class of image features in XSlit images, 888777  i. [sent-405, score-0.166]
</p><p>93 , the XSlit Vanishing Point (XVP) and Coplanar Common Point (CCP), for grouping coplanar parallel lines. [sent-407, score-0.205]
</p><p>94 Our main contribution is a new theory that shows each group of coplanar parallel lines will intersect at an XVP and a CCP in their XSlit image and its geometry can be directly recovered from the XVP and CCP. [sent-408, score-0.439]
</p><p>95 Our solution relies on accurately detecting curves and fitting conics to locate XVPs and CCPs. [sent-410, score-0.138]
</p><p>96 If a captured curve is too short or too straight, our conic fitting scheme can introduce large errors and therefore generate incorrect XVPs and CCPs. [sent-411, score-0.111]
</p><p>97 For example, due to the small distance between the slits, only 3D lines lying close to the camera will appear sufficiently curved. [sent-415, score-0.254]
</p><p>98 Finally, our prototype XSlit camera may also benefit several other vision tasks. [sent-421, score-0.132]
</p><p>99 For example, we can combine an area light source with two cylindrical lenses for creating a prototype XSlit light source. [sent-428, score-0.159]
</p><p>100 On the localization of straight lines in 3D space from single 2D images. [sent-444, score-0.111]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xslit', 0.789), ('xvps', 0.239), ('ccp', 0.221), ('mw', 0.206), ('ccps', 0.147), ('xvp', 0.135), ('slits', 0.12), ('lines', 0.111), ('coplanar', 0.11), ('vanishing', 0.105), ('uv', 0.095), ('parallel', 0.095), ('camera', 0.086), ('conics', 0.083), ('ray', 0.074), ('pinhole', 0.074), ('planes', 0.07), ('conic', 0.069), ('manhattan', 0.068), ('intersect', 0.058), ('plane', 0.055), ('rays', 0.053), ('slit', 0.051), ('prototype', 0.046), ('vl', 0.043), ('cylindrical', 0.043), ('theorem', 0.042), ('lying', 0.042), ('hyperbolas', 0.042), ('xsrc', 0.042), ('line', 0.041), ('interchangeable', 0.037), ('perspective', 0.035), ('geometry', 0.035), ('triangle', 0.032), ('lenses', 0.032), ('delage', 0.031), ('unparallel', 0.031), ('recovered', 0.03), ('lens', 0.029), ('curved', 0.028), ('scene', 0.026), ('oblique', 0.026), ('det', 0.025), ('intersections', 0.025), ('ul', 0.024), ('lie', 0.023), ('world', 0.022), ('curve', 0.022), ('apertures', 0.021), ('ny', 0.021), ('bvv', 0.021), ('curviness', 0.021), ('dly', 0.021), ('ffd', 0.021), ('nonpinhole', 0.021), ('vul', 0.021), ('xslits', 0.021), ('notice', 0.021), ('seitz', 0.02), ('fitting', 0.02), ('light', 0.019), ('saxena', 0.019), ('zl', 0.019), ('furukawa', 0.019), ('highlighted', 0.019), ('caglioti', 0.018), ('cop', 0.018), ('nx', 0.018), ('reconstructing', 0.018), ('curves', 0.018), ('imaging', 0.018), ('axes', 0.018), ('flint', 0.017), ('kosecka', 0.017), ('swaminathan', 0.017), ('skyscraper', 0.017), ('feldman', 0.017), ('intersection', 0.017), ('aperture', 0.017), ('locate', 0.017), ('parametrization', 0.017), ('synthetic', 0.016), ('eliminating', 0.016), ('principal', 0.016), ('unique', 0.016), ('orthogonal', 0.015), ('building', 0.015), ('reconstruction', 0.015), ('appear', 0.015), ('common', 0.015), ('recover', 0.015), ('recovering', 0.015), ('points', 0.015), ('vertices', 0.014), ('fit', 0.014), ('yl', 0.014), ('exist', 0.014), ('ambiguity', 0.014), ('canny', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="279-tfidf-1" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>2 0.10346144 <a title="279-tfidf-2" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>3 0.10193984 <a title="279-tfidf-3" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>4 0.10086729 <a title="279-tfidf-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.078507558 <a title="279-tfidf-5" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>6 0.072166018 <a title="279-tfidf-6" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>7 0.059376098 <a title="279-tfidf-7" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>8 0.05886944 <a title="279-tfidf-8" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>9 0.05408201 <a title="279-tfidf-9" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>10 0.052571457 <a title="279-tfidf-10" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>11 0.048072204 <a title="279-tfidf-11" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>12 0.047548734 <a title="279-tfidf-12" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>13 0.047236215 <a title="279-tfidf-13" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>14 0.046964452 <a title="279-tfidf-14" href="./cvpr-2013-Determining_Motion_Directly_from_Normal_Flows_Upon_the_Use_of_a_Spherical_Eye_Platform.html">124 cvpr-2013-Determining Motion Directly from Normal Flows Upon the Use of a Spherical Eye Platform</a></p>
<p>15 0.046413634 <a title="279-tfidf-15" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>16 0.045696054 <a title="279-tfidf-16" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>17 0.044275749 <a title="279-tfidf-17" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>18 0.042952817 <a title="279-tfidf-18" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>19 0.042922612 <a title="279-tfidf-19" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>20 0.042863399 <a title="279-tfidf-20" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.073), (1, 0.086), (2, 0.006), (3, 0.017), (4, 0.001), (5, -0.038), (6, -0.026), (7, 0.007), (8, 0.011), (9, 0.027), (10, -0.005), (11, 0.044), (12, 0.069), (13, -0.027), (14, -0.11), (15, 0.028), (16, 0.027), (17, 0.064), (18, -0.017), (19, 0.042), (20, 0.004), (21, 0.031), (22, -0.012), (23, -0.02), (24, 0.018), (25, 0.007), (26, -0.012), (27, -0.021), (28, -0.021), (29, 0.045), (30, -0.026), (31, 0.03), (32, 0.012), (33, 0.087), (34, -0.038), (35, 0.058), (36, -0.029), (37, 0.01), (38, 0.009), (39, 0.041), (40, 0.044), (41, -0.049), (42, 0.034), (43, 0.008), (44, -0.002), (45, -0.058), (46, -0.033), (47, 0.025), (48, -0.027), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91526467 <a title="279-lsi-1" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>2 0.76696557 <a title="279-lsi-2" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>Author: Nathan Jacobs, Mohammad T. Islam, Scott Workman</p><p>Abstract: We propose cloud motion as a natural scene cue that enables geometric calibration of static outdoor cameras. This work introduces several new methods that use observations of an outdoor scene over days and weeks to estimate radial distortion, focal length and geo-orientation. Cloud-based cues provide strong constraints and are an important alternative to methods that require specific forms of static scene geometry or clear sky conditions. Our method makes simple assumptions about cloud motion and builds upon previous work on motion-based and line-based calibration. We show results on real scenes that highlight the effectiveness of our proposed methods.</p><p>3 0.76284307 <a title="279-lsi-3" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>4 0.74432397 <a title="279-lsi-4" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>5 0.73863715 <a title="279-lsi-5" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>6 0.73533899 <a title="279-lsi-6" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>7 0.73314708 <a title="279-lsi-7" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>8 0.70854521 <a title="279-lsi-8" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>9 0.65752691 <a title="279-lsi-9" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>10 0.64513832 <a title="279-lsi-10" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>11 0.63520348 <a title="279-lsi-11" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<p>12 0.59937966 <a title="279-lsi-12" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>13 0.57659453 <a title="279-lsi-13" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>14 0.55802125 <a title="279-lsi-14" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>15 0.52792978 <a title="279-lsi-15" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>16 0.51600206 <a title="279-lsi-16" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>17 0.50757629 <a title="279-lsi-17" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>18 0.50219572 <a title="279-lsi-18" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>19 0.50142062 <a title="279-lsi-19" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>20 0.49594373 <a title="279-lsi-20" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.104), (16, 0.023), (26, 0.044), (33, 0.15), (45, 0.018), (60, 0.326), (67, 0.035), (69, 0.059), (87, 0.103)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71499044 <a title="279-lda-1" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>Author: Jinwei Ye, Yu Ji, Jingyi Yu</p><p>Abstract: A Manhattan World (MW) [3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results × on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.</p><p>2 0.58340657 <a title="279-lda-2" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>Author: Kiwon Yun, Yifan Peng, Dimitris Samaras, Gregory J. Zelinsky, Tamara L. Berg</p><p>Abstract: Weposit that user behavior during natural viewing ofimages contains an abundance of information about the content of images as well as information related to user intent and user defined content importance. In this paper, we conduct experiments to better understand the relationship between images, the eye movements people make while viewing images, and how people construct natural language to describe images. We explore these relationships in the context of two commonly used computer vision datasets. We then further relate human cues with outputs of current visual recognition systems and demonstrate prototype applications for gaze-enabled detection and annotation.</p><p>3 0.53765136 <a title="279-lda-3" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>Author: Yiliang Xu, Sangmin Oh, Anthony Hoogs</p><p>Abstract: We present a novel vanishing point detection algorithm for uncalibrated monocular images of man-made environments. We advance the state-of-the-art by a new model of measurement error in the line segment extraction and minimizing its impact on the vanishing point estimation. Our contribution is twofold: 1) Beyond existing hand-crafted models, we formally derive a novel consistency measure, which captures the stochastic nature of the correlation between line segments and vanishing points due to the measurement error, and use this new consistency measure to improve the line segment clustering. 2) We propose a novel minimum error vanishing point estimation approach by optimally weighing the contribution of each line segment pair in the cluster towards the vanishing point estimation. Unlike existing works, our algorithm provides an optimal solution that minimizes the uncertainty of the vanishing point in terms of the trace of its covariance, in a closed-form. We test our algorithm and compare it with the state-of-the-art on two public datasets: York Urban Dataset and Eurasian Cities Dataset. The experiments show that our approach outperforms the state-of-the-art.</p><p>4 0.53622472 <a title="279-lda-4" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>Author: Michael Kolomenkin, Ilan Shimshoni, Ayellet Tal</p><p>Abstract: This paper extends to surfaces the multi-scale approach of edge detection on images. The common practice for detecting curves on surfaces requires the user to first select the scale of the features, apply an appropriate smoothing, and detect the edges on the smoothed surface. This approach suffers from two drawbacks. First, it relies on a hidden assumption that all the features on the surface are of the same scale. Second, manual user intervention is required. In this paper, we propose a general framework for automatically detecting the optimal scale for each point on the surface. We smooth the surface at each point according to this optimal scale and run the curve detection algorithm on the resulting surface. Our multi-scale algorithm solves the two disadvantages of the single-scale approach mentioned above. We demonstrate how to realize our approach on two commonly-used special cases: ridges & valleys and relief edges. In each case, the optimal scale is found in accordance with the mathematical definition of the curve.</p><p>5 0.53537518 <a title="279-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.53510165 <a title="279-lda-6" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>7 0.53296041 <a title="279-lda-7" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>8 0.53286856 <a title="279-lda-8" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>9 0.53177744 <a title="279-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.53027183 <a title="279-lda-10" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>11 0.53011072 <a title="279-lda-11" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>12 0.52863479 <a title="279-lda-12" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>13 0.52842468 <a title="279-lda-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.52814877 <a title="279-lda-14" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>15 0.52814043 <a title="279-lda-15" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>16 0.52633357 <a title="279-lda-16" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>17 0.5255686 <a title="279-lda-17" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>18 0.52341902 <a title="279-lda-18" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>19 0.52242357 <a title="279-lda-19" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>20 0.5222376 <a title="279-lda-20" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
