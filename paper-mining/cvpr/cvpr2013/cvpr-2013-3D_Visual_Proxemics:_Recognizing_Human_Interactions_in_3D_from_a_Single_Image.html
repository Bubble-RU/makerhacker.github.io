<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-4" href="#">cvpr2013-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</h1>
<br/><p>Source: <a title="cvpr-2013-4-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chakraborty_3D_Visual_Proxemics_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>Reference: <a title="cvpr-2013-4-reference" href="../cvpr2013_reference/cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 chakraborty, hui Abstract We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. [sent-2, score-0.399]
</p><p>2 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. [sent-3, score-0.757]
</p><p>3 Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. [sent-6, score-0.292]
</p><p>4 The proposed approach also estimates camera pose and uses it to capture the intent of the photo. [sent-8, score-0.35]
</p><p>5 To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. [sent-9, score-0.539]
</p><p>6 This enables our model to handle large variations in people size, heights (e. [sent-10, score-0.344]
</p><p>7 To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. [sent-14, score-0.47]
</p><p>8 We train a discriminative classifier that classifies 3D arrangements of people into visual  proxemes and quantitatively evaluate the performance on a large, challenging dataset. [sent-15, score-0.576]
</p><p>9 Studying people interactions by analyzing their spatial configuration, also known as Proxemics in anthropology, is an important step towards understanding web images and videos. [sent-18, score-0.36]
</p><p>10 Speaker and audience Figure 1: People configurations and the camera-person’s perspective provide strong cues about the type of social interaction that the people are participating in. [sent-38, score-0.747]
</p><p>11 The proposed method uses 2D face locations from a single image to estimate the camera pose and the spatial arrangement of people in 3D. [sent-39, score-0.698]
</p><p>12 Figure 1 shows six typical types of people interactions that are often seen in Internet images and video frames: They are (1) Group Interaction, (2) Family photo, (3) Group photo, (4) Couple with an audience, (5) Crowd, and (6) Speaker with audience. [sent-40, score-0.36]
</p><p>13 From these images, it is important to note that the people configurations in the 3D space would better reflect the type of interaction than the configurations in a 2D image space. [sent-41, score-0.395]
</p><p>14 Additionally, not only how people are organized spatially, but also how the shots are framed (i. [sent-43, score-0.328]
</p><p>15 the relative camera location, direction and pose) convey the type of proxemes depicted in these images. [sent-45, score-0.416]
</p><p>16 For example, in order to capture the whole group and to avoid occlusion, high-angle shots are used for group interaction (Figure 1(a)) and crowd (e). [sent-46, score-0.35]
</p><p>17 On the other hand, to capture the focus of attention, or principals in an event, such as a family portrait (Figure 1(b)), couples in a ceremony (Figure 1(d)) and speaker with an audience (Figure 1(f)), eye level shots are used. [sent-47, score-0.386]
</p><p>18 For artistic impression and better capture of the people in foreground without concerns of occluding the background, low-angle shots are used, especially for group photos as shown in Figure 1(c). [sent-48, score-0.387]
</p><p>19 A number of research groups [19, 5, 10, 9] have conducted insightful studies for understanding people interactions in images and videos, though with limited scope. [sent-49, score-0.394]
</p><p>20 Although these approaches demonstrated their effectiveness, their robustness is fundamentally limited by the 2D analysis paradigm and cannot handle the diversity in camera pose and people depths often seen in user generated Internet content. [sent-51, score-0.565]
</p><p>21 In recent works, [10] proposes to estimate 3D location of people using faces and use these locations to detect social interaction among people. [sent-52, score-0.686]
</p><p>22 In [9], locations of faces in the 3D space around a camera wearing person are used to detect attention patterns. [sent-53, score-0.403]
</p><p>23 However, these approaches only attempt to detect a very limited set of human interactions and their 3D estimation cannot effectively handle the diversity of people in terms of age  (big adults vs. [sent-54, score-0.452]
</p><p>24 Additionally, these approaches do not take camera location and pose into account when analyzing people interactions, which can be an important clue about the intent of the shot. [sent-57, score-0.52]
</p><p>25 Proxemics has been applied in the field of cinematography where it is used for optimizing the scene layout and the position of the camera with respect to the characters in the scene. [sent-60, score-0.281]
</p><p>26 In this paper, we propose a unified framework called 3D Visual Proxemics Analysis (VPA3D), for detecting and classifying people interactions from a single image. [sent-63, score-0.36]
</p><p>27 VPA3D first estimates people/face depths in 3D, then performs perspective rectification to map people locations from the scene space to the 3D space. [sent-64, score-0.545]
</p><p>28 Finally, a set of spatial and structural features are used to detect and recognize the six types of people interaction classes. [sent-65, score-0.321]
</p><p>29 The proposed VPA3D approach surpasses state-of-the-art people configuration analysis in the following three aspects. [sent-66, score-0.299]
</p><p>30 First, VPA3D uses 3D reasoning for robust depth estimation in the presence of age, size, height and human pose variation in a single image. [sent-67, score-0.366]
</p><p>31 Second, a set of shape descriptors derived from the attributes of Proxemics is used to capture type of people interaction in the eyes of each individual participant not only for robust classification but also for classification of individuals role in a visual proxeme. [sent-68, score-0.413]
</p><p>32 Additionally, the types of camera pose are used as a  prior indicating possible intent of the camera-person who took the picture. [sent-69, score-0.285]
</p><p>33 Third, to characterize the human interaction space, we introduce visualproxemes; a set ofprototypicalpatterns that represent commonly occurring people interactions in social events. [sent-70, score-0.66]
</p><p>34 The source ofour visual proxemes is the NIST TRECVID Multimedia Event Detection dataset [2] which contains annotated data for 15 high-level events. [sent-71, score-0.284]
</p><p>35 A set of6 commonly occurring visual proxemes (shown in Figure 1) are selected fromkeyframes containing groups of people. [sent-72, score-0.356]
</p><p>36 We train a discriminative classifier that classifies 3D arrangements of people into these visual proxemes and quantitatively evaluate the performance on this large, challenging dataset. [sent-73, score-0.576]
</p><p>37 Social force model was proposed in [18] to understand pedestrian behavior and interaction energies between people tracks was used in [8] to detect abnormal and violent interactions. [sent-76, score-0.387]
</p><p>38 Proxemics is a subfield of anthropology that involves study of people configurations [11]. [sent-81, score-0.337]
</p><p>39 Its core idea that spatial configuration among people is strongly related to social interactions has been recently adopted by the computer vision community. [sent-82, score-0.534]
</p><p>40 Most of these works consider face detections to localize people in images. [sent-83, score-0.388]
</p><p>41 from facial attributes and face subgraphs, and [19], in which authors label pairs of people with a physically-based touch codes such as Hand-hand, Hand-torso etc. [sent-88, score-0.435]
</p><p>42 These methods use rough estimates of 3D locations determined from face size variations to seed the directions of gazelines. [sent-91, score-0.267]
</p><p>43 In [17], by detecting 3D gaze volumes, the authors find if people are looking at each other. [sent-92, score-0.311]
</p><p>44 [10] have presented a systematic study for understanding images of groups of people where they look into 3D spatial structure by recovering camera parameters from face locations. [sent-95, score-0.599]
</p><p>45 However, their method for camera calibration does not allow variations in people poses and camera viewpoints. [sent-96, score-0.617]
</p><p>46 These algorithms estimate vanishing points based on camera motion constraints in [16] and rigidity constraints of architectural scenes in [7]. [sent-98, score-0.271]
</p><p>47 [12] have proposed an approximate camera calibration model that assumes grounded objects of known heights and restricted intrinsic camera parameters. [sent-100, score-0.517]
</p><p>48 In contrast, we introduce constraints to estimate perspective-corrected, explicit positions of faces in 3D space as well as the camera height relative to the faces. [sent-104, score-0.541]
</p><p>49 Then, we describe a novel Perspective Rectification algorithm to estimate people/face depths in 3D and camera view from face detections in images. [sent-108, score-0.43]
</p><p>50 , proxemes) in the Visual Proxeme Classification stage by combining the estimates of face positions and camera view with our knowledge of Visual Proxemics through spatial and structural features in the 3D space. [sent-111, score-0.395]
</p><p>51 Additionally,  people configuration needs to support the communicative factors such as physical contacts, touching, visual, and voice factors needed in an interaction [1]. [sent-119, score-0.353]
</p><p>52 For example, in Figure 1a, to enable direct eye contact between any pair of participants in a group interaction, people align themselves in a semi-circular shape. [sent-121, score-0.294]
</p><p>53 In contrast, if two people are the focus of attention, as in Figure 1d, we have multiple shape layers, where the two people at the center of attention share an intimate space, while the audience forms a distinct second layer in the background. [sent-122, score-0.785]
</p><p>54 One area of interest is the application of proxemics to cinematography where the shot composition and camera viewpoint is optimized for visual weight [1]. [sent-123, score-0.873]
</p><p>55 To assure full visibility of every character in the scene, a high-angle shot is chosen whereas for intimate scenes and closeups, an eye-level shot or low-angle shot is more suitable. [sent-126, score-0.49]
</p><p>56 For example, “Group Interaction” in Figure 1(a) shows people within social distance in a single layer with a  concave shape and is captured using high-angle, medium shot. [sent-128, score-0.377]
</p><p>57 Perspective Rectification Module Given the 2D face locations in an image, the goal is to recover the camera height and the face positions in the X-Z plane relative to the camera center. [sent-135, score-0.942]
</p><p>58 In addition to the parameters, we also detect outliers; face locations that do not fit the model hypothesis of uniform people heights and poses. [sent-139, score-0.546]
</p><p>59 Refining the parameter estimates by 3D reasoning about position of outliers in relation to the inliers based on domain constraints that relate people’s heights and poses. [sent-142, score-0.358]
</p><p>60 1  RANSAC based Outlier Detection  This section describes an algorithm to estimate face depths, horizon line and camera height from 2D face locations in an image. [sent-149, score-0.857]
</p><p>61 The coordinate transformation of a point using a typical pinhole camera model with uniform aspect ratio, zero skew and restricted camera rotation is given by,  ⎛⎝vu1ii⎠⎞=z1w? [sent-153, score-0.4]
</p><p>62 We assume that the camera is located at (xcw = 0,zcw = 0) and tilted slightly along x axis by is the camera height and fw is the focal length. [sent-157, score-0.544]
</p><p>63 At this stage some simplifying assumptions are made - (a) faces have constant heights, (b) faces rest on ground plane, which implies = 0. [sent-158, score-0.29]
</p><p>64 Using these (xw,zw) coordinates, we can undo the perspective projection of the 2D image and recover the perspective rectified face layout in the 3D coordinate system. [sent-163, score-0.298]
</p><p>65 We estimate the face height in the image according to the model using hˆi = hw(vb vˆ 0)/ yˆc and compute the deviation from the observed height using eiM = ||hi || to find the estimator error for that face. [sent-183, score-0.562]
</p><p>66 However 333444000977  (a) Visibility constraint  (b) Localized pose constraint Figure 4: Circled faces depict outliers and the connected faces show the related inliers discovered through semantic constraints. [sent-188, score-0.608]
</p><p>67 However, outlier faces may occur because of variations in face sizes and heights arising due to difference in age, pose (sitting versus standing) and physical planes of reference (ground level or on a platform). [sent-193, score-0.648]
</p><p>68 For doing this, we make use of semantics of Visual Proxemics to constrain the possible depth orderings of the outlier faces in the image. [sent-195, score-0.391]
</p><p>69 RANSAC estimates the sitting person’s face to be an outlier because it violates the common ground plane assumption (assumption (b) in the linear model). [sent-198, score-0.492]
</p><p>70 We formulate this visibility constraint as follows - The only way for two faces to be visible at the same horizontal location is if the lower face is closer in depth than the face above it3. [sent-200, score-0.605]
</p><p>71 aF−or be)ach outlier in the image, we determine if it shares a visibility constraint with any of the inliers and maintain an index of all such pairs. [sent-204, score-0.358]
</p><p>72 Based on this assumption the height estimates  zˆw  of the outliers are refined, as described in Section 3. [sent-206, score-0.334]
</p><p>73 Localized pose Constraint This constraint assumes that the people who are physically close to each other also share the same pose. [sent-209, score-0.327]
</p><p>74 RANSAC estimation (top plot) detects the child’s face location as an outlier and incorrectly estimates its depth because of the height difference from the remaining members of the group. [sent-211, score-0.654]
</p><p>75 Now, if we assume that the inliers that are physically close to the outlier in the world also have a similar pose, then we can localize the ground plane level at the outlier based on the face locations of the neighboring inliers. [sent-212, score-0.695]
</p><p>76 If the difference in the face size of the outlier to its inlier neighbors is within a threshold, then we can fix the depth of the outlier within the vicinity of the neighboring inliers. [sent-215, score-0.585]
</p><p>77 zw  (5) (6)  For each outlier in the image, we perform this constraint test to determine (outlier, inliers) pairs that satisfy the localized pose  constraint. [sent-220, score-0.394]
</p><p>78 These are used to refine the height estimates of the outliers in the following section. [sent-221, score-0.334]
</p><p>79 3  Model update  The height estimates of the outliers are refined using the semantically constrained set of inliers. [sent-224, score-0.334]
</p><p>80 Specifically, we make use of a piecewise constant ground plane assumption in the image to estimate the outlier heights in the world. [sent-225, score-0.367]
</p><p>81 By assuming that the outliers are located at the same level as the related inliers, the world height (hw) of the outliers can calculated in proportion to the inliers. [sent-226, score-0.348]
</p><p>82 LetBojut is the body height of an outlier and be the ground plane approximation for a neighboring inlier. [sent-227, score-0.419]
</p><p>83 The ground level is calculated by translating the vertical position ofthe face by a quantity proportional to the image height (we assume face size is 7 times the body size). [sent-228, score-0.496]
</p><p>84 The body height of the outlier is based on the average ground plane estimated from its inliers. [sent-229, score-0.419]
</p><p>85 The face height is then calculated as a fraction of the estimated body height. [sent-230, score-0.343]
</p><p>86 t ratios are inputs to the next round of RANSAC step that produce new estimates of face depths and camera heights. [sent-234, score-0.466]
</p><p>87 Camera pose cues: The camera height is quantized into three lCevamelse -r lao pwo-aseng clue,e esye-level and high-angle. [sent-264, score-0.422]
</p><p>88 In each image, each face is treated as an anchor and all other faces are color coded according to whether they are estimated to be ahead, behind or at the same depth as the face anchor. [sent-299, score-0.511]
</p><p>89 946e386rimage  The scale-ratios method systematically misconstrues depth estimates because it only uses face size and ignores the location in the image. [sent-309, score-0.278]
</p><p>90 However, combining a robust model with proxemic semantics clearly outperforms other methods in its ability to handle wide variations in pose and heights of people. [sent-312, score-0.28]
</p><p>91 Camera height estimation  Our model jointly computes 3D coordinates and camera height from faces in an image. [sent-315, score-0.741]
</p><p>92 The images in our dataset are annotated by a human annotator with three camera views - low-angle (camera looking upwards at the scene), eye level, high-angle (camera looking downwards) (Table 1). [sent-322, score-0.329]
</p><p>93 We noticed a large drop in performance of face detection on small face sizes and non-frontal face shots, which are typical in images of large groups. [sent-330, score-0.459]
</p><p>94 Visual Proxeme Classification The classes of visual proxemes that we consider are listed in Table 1. [sent-335, score-0.284]
</p><p>95 A group of human annotators mined through a large Table 3: Comparison of camera height estimation from our framework and from the vertical-object intersection method proposed in [16]  A. [sent-336, score-0.46]
</p><p>96 h61-90a8%n gle  number of image keyframes of people and decided on this set of 6 commonly occurring visual proxemes. [sent-340, score-0.318]
</p><p>97 A different human annotator classified each image into one of the 6 visual proxemes or as unknown. [sent-341, score-0.368]
</p><p>98 Their features are mainly based on 2D face locations and an additional cue for predicting face size. [sent-347, score-0.355]
</p><p>99 Discussion In this paper we present 3D Visual Proxemics Analysis, a framework that integrates Visual Proxemics with 3D arrangements of people to identify typical social interactions in Internet images. [sent-365, score-0.559]
</p><p>100 Our results demonstrate that this unified approach surpasses the state-of-the-art both in 3D estimation of people layout from detected faces as well as in classification of social interactions. [sent-366, score-0.615]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('proxemics', 0.471), ('layoutof', 0.261), ('proxemes', 0.239), ('people', 0.235), ('height', 0.19), ('outlier', 0.186), ('camera', 0.177), ('audience', 0.174), ('face', 0.153), ('faces', 0.145), ('social', 0.142), ('proxeme', 0.13), ('interactions', 0.125), ('proxemic', 0.116), ('heights', 0.109), ('intimate', 0.109), ('shot', 0.108), ('ransac', 0.098), ('shots', 0.093), ('zw', 0.089), ('interaction', 0.086), ('rectification', 0.083), ('outliers', 0.079), ('inliers', 0.078), ('speaker', 0.074), ('horizon', 0.072), ('yw', 0.072), ('depths', 0.071), ('personal', 0.068), ('anthropology', 0.065), ('vbi', 0.065), ('ycw', 0.065), ('estimates', 0.065), ('xw', 0.063), ('layout', 0.061), ('depth', 0.06), ('group', 0.059), ('visibility', 0.057), ('arrangements', 0.057), ('pose', 0.055), ('intent', 0.053), ('crowd', 0.053), ('annotator', 0.05), ('locations', 0.049), ('ikn', 0.048), ('internet', 0.048), ('attributes', 0.047), ('skew', 0.046), ('family', 0.045), ('couple', 0.045), ('sitting', 0.045), ('visual', 0.045), ('standing', 0.045), ('cinematography', 0.043), ('plane', 0.043), ('perspective', 0.042), ('photo', 0.042), ('gaze', 0.042), ('bf', 0.041), ('trecvid', 0.041), ('gallagher', 0.04), ('abnormal', 0.04), ('vanishing', 0.039), ('coordinates', 0.039), ('hui', 0.039), ('occurring', 0.038), ('hoiem', 0.037), ('nist', 0.037), ('constraint', 0.037), ('configurations', 0.037), ('omar', 0.036), ('sigmoid', 0.035), ('taxonomy', 0.035), ('looking', 0.034), ('human', 0.034), ('line', 0.034), ('groups', 0.034), ('informed', 0.034), ('concepts', 0.033), ('configuration', 0.032), ('semantic', 0.032), ('interpersonal', 0.032), ('surpasses', 0.032), ('attention', 0.032), ('cues', 0.031), ('age', 0.031), ('efros', 0.029), ('estimate', 0.029), ('composition', 0.029), ('mst', 0.029), ('calibration', 0.028), ('hot', 0.028), ('reasoning', 0.027), ('localized', 0.027), ('diversity', 0.027), ('additionally', 0.027), ('iarpa', 0.026), ('behavior', 0.026), ('architectural', 0.026), ('grounded', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="4-tfidf-1" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>2 0.21563119 <a title="4-tfidf-2" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>3 0.14785886 <a title="4-tfidf-3" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>4 0.13063289 <a title="4-tfidf-4" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>5 0.13046795 <a title="4-tfidf-5" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>Author: Baoyuan Wu, Yifan Zhang, Bao-Gang Hu, Qiang Ji</p><p>Abstract: In this paper, we focus on face clustering in videos. Given the detected faces from real-world videos, we partition all faces into K disjoint clusters. Different from clustering on a collection of facial images, the faces from videos are organized as face tracks and the frame index of each face is also provided. As a result, many pairwise constraints between faces can be easily obtained from the temporal and spatial knowledge of the face tracks. These constraints can be effectively incorporated into a generative clustering model based on the Hidden Markov Random Fields (HMRFs). Within the HMRF model, the pairwise constraints are augmented by label-level and constraint-level local smoothness to guide the clustering process. The parameters for both the unary and the pairwise potential functions are learned by the simulated field algorithm, and the weights of constraints can be easily adjusted. We further introduce an efficient clustering framework specially for face clustering in videos, considering that faces in adjacent frames of the same face track are very similar. The framework is applicable to other clustering algorithms to significantly reduce the computational cost. Experiments on two face data sets from real-world videos demonstrate the significantly improved performance of our algorithm over state-of-theart algorithms.</p><p>6 0.12473469 <a title="4-tfidf-6" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>7 0.11607129 <a title="4-tfidf-7" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>8 0.11293171 <a title="4-tfidf-8" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>9 0.11227986 <a title="4-tfidf-9" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>10 0.1115109 <a title="4-tfidf-10" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>11 0.10128543 <a title="4-tfidf-11" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>12 0.10020982 <a title="4-tfidf-12" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>13 0.097672239 <a title="4-tfidf-13" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>14 0.094511069 <a title="4-tfidf-14" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>15 0.093560733 <a title="4-tfidf-15" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>16 0.092123672 <a title="4-tfidf-16" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>17 0.091865398 <a title="4-tfidf-17" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>18 0.091847479 <a title="4-tfidf-18" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>19 0.090548106 <a title="4-tfidf-19" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>20 0.087586775 <a title="4-tfidf-20" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.22), (1, 0.028), (2, -0.012), (3, -0.042), (4, 0.016), (5, -0.01), (6, -0.064), (7, -0.008), (8, 0.178), (9, -0.019), (10, 0.015), (11, 0.031), (12, 0.106), (13, 0.028), (14, -0.02), (15, -0.006), (16, 0.008), (17, 0.166), (18, -0.084), (19, -0.074), (20, -0.038), (21, 0.071), (22, -0.055), (23, 0.034), (24, 0.034), (25, -0.075), (26, -0.018), (27, -0.01), (28, -0.004), (29, -0.066), (30, -0.001), (31, 0.053), (32, 0.013), (33, 0.062), (34, -0.009), (35, 0.023), (36, 0.006), (37, 0.117), (38, 0.074), (39, 0.046), (40, 0.011), (41, -0.018), (42, -0.069), (43, 0.056), (44, 0.1), (45, -0.032), (46, -0.007), (47, -0.024), (48, 0.049), (49, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94607323 <a title="4-lsi-1" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>2 0.66385263 <a title="4-lsi-2" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>3 0.64295489 <a title="4-lsi-3" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>4 0.6338582 <a title="4-lsi-4" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>5 0.58823866 <a title="4-lsi-5" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>6 0.58639789 <a title="4-lsi-6" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>7 0.57766193 <a title="4-lsi-7" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>8 0.56275028 <a title="4-lsi-8" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>9 0.54727846 <a title="4-lsi-9" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>10 0.54331917 <a title="4-lsi-10" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>11 0.52805263 <a title="4-lsi-11" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>12 0.513919 <a title="4-lsi-12" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>13 0.51364613 <a title="4-lsi-13" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>14 0.51154739 <a title="4-lsi-14" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>15 0.50928694 <a title="4-lsi-15" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>16 0.507011 <a title="4-lsi-16" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>17 0.5035755 <a title="4-lsi-17" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>18 0.50012499 <a title="4-lsi-18" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>19 0.49817979 <a title="4-lsi-19" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>20 0.49628282 <a title="4-lsi-20" href="./cvpr-2013-Manhattan_Junction_Catalogue_for_Spatial_Reasoning_of_Indoor_Scenes.html">278 cvpr-2013-Manhattan Junction Catalogue for Spatial Reasoning of Indoor Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.113), (16, 0.021), (26, 0.076), (28, 0.242), (33, 0.219), (65, 0.011), (67, 0.094), (69, 0.067), (80, 0.02), (87, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83650696 <a title="4-lda-1" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>2 0.82571918 <a title="4-lda-2" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>3 0.81791139 <a title="4-lda-3" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>4 0.80927485 <a title="4-lda-4" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>Author: Austin Abrams, Kylia Miskell, Robert Pless</p><p>Abstract: Shadows encode a powerful geometric cue: if one pixel casts a shadow onto another, then the two pixels are colinear with the lighting direction. Given many images over many lighting directions, this constraint can be leveraged to recover the depth of a scene from a single viewpoint. For outdoor scenes with solar illumination, we term this the episolar constraint, which provides a convex optimization to solve for the sparse depth of a scene from shadow correspondences, a method to reduce the search space when finding shadow correspondences, and a method to geometrically calibrate a camera using shadow constraints. Our method constructs a dense network of nonlocal constraints which complements recent work on outdoor photometric stereo and cloud based cues for 3D. We demonstrate results across a variety of time-lapse sequences from webcams “in . wu st l. edu (b)(c) the wild.”</p><p>5 0.8021394 <a title="4-lda-5" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>Author: Minh Hoai, Andrew Zisserman</p><p>Abstract: The objective of this work is to learn sub-categories. Rather than casting this as a problem of unsupervised clustering, we investigate a weakly supervised approach using both positive and negative samples of the category. We make the following contributions: (i) we introduce a new model for discriminative sub-categorization which determines cluster membership for positive samples whilst simultaneously learning a max-margin classifier to separate each cluster from the negative samples; (ii) we show that this model does not suffer from the degenerate cluster problem that afflicts several competing methods (e.g., Latent SVM and Max-Margin Clustering); (iii) we show that the method is able to discover interpretable sub-categories in various datasets. The model is evaluated experimentally over various datasets, and itsperformance advantages over k-means and Latent SVM are demonstrated. We also stress test the model and show its resilience in discovering sub-categories as the parameters are varied.</p><p>6 0.79795069 <a title="4-lda-6" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>7 0.770298 <a title="4-lda-7" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>8 0.75714946 <a title="4-lda-8" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>9 0.75499904 <a title="4-lda-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.75260192 <a title="4-lda-10" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>11 0.74658298 <a title="4-lda-11" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>12 0.7441557 <a title="4-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.74359167 <a title="4-lda-13" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>14 0.74333483 <a title="4-lda-14" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>15 0.74320114 <a title="4-lda-15" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>16 0.73860323 <a title="4-lda-16" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>17 0.73847473 <a title="4-lda-17" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>18 0.7369805 <a title="4-lda-18" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>19 0.73658878 <a title="4-lda-19" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>20 0.7364139 <a title="4-lda-20" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
