<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-360" href="#">cvpr2013-360</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</h1>
<br/><p>Source: <a title="cvpr-2013-360-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ma_Robust_Estimation_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>Reference: <a title="cvpr-2013-360-reference" href="../cvpr2013_reference/cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a new point matching algorithm for robust nonrigid registration. [sent-8, score-0.58]
</p><p>2 The method iteratively recovers the point correspondence and estimates the transformation between two point sets. [sent-9, score-0.815]
</p><p>3 In the second step, we estimate the transformation using a robust estimator called L2E. [sent-11, score-0.532]
</p><p>4 This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. [sent-12, score-0.538]
</p><p>5 The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. [sent-13, score-0.463]
</p><p>6 We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. [sent-14, score-0.611]
</p><p>7 Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications. [sent-16, score-0.326]
</p><p>8 Introduction  Point set registration is a fundamental problem which frequently arises in computer vision, medical image analysis, and pattern recognition [5, 4, 6]. [sent-18, score-0.41]
</p><p>9 Many tasks in these fields such as stereo matching, shape matching, image registration and content-based image retrieval can be formulated as a point matching problems because point representations are general and easy to extract [5]. [sent-19, score-0.753]
</p><p>10 The registration problem then reduces to determining the correct correspondence and to find the underlying spatial transformation between two point sets extracted from the input data. [sent-21, score-1.098]
</p><p>11 The registration problem can be categorized into rigid or nonrigid registration depending on the application and the form of the data. [sent-22, score-1.091]
</p><p>12 By contrast, nonrigid registration is more difficult because the underlying nonrigid transformations are often unknown, complex, and hard to model [6]. [sent-24, score-1.141]
</p><p>13 But nonrigid registration is very important because it is required for many real world tasks including hand-written character recognition, shape recognition, deformable motion tracking and medical image registration. [sent-25, score-0.825]
</p><p>14 In this paper, we focus on the nonrigid case and present a robust algorithm for nonrigid point set registration. [sent-26, score-0.839]
</p><p>15 In this iterative process, the estimate of the correspondence is used to refine the estimate of the transformation, and vice versa. [sent-29, score-0.312]
</p><p>16 But a problem arises if there are errors in the correspondence which occurs in many applications particularly if the transformation is large and/or there are outliers in the data (e. [sent-30, score-0.779]
</p><p>17 In this situation, the estimate of the transformation will degrade badly unless it is performed robustly. [sent-33, score-0.388]
</p><p>18 The main contribution of our approach is to robustly estimate the transformations from the correspondences using a robust estimator named the L2Minimizing Estimate (L2E) [20, 2]. [sent-34, score-0.573]
</p><p>19 More precisely, our approach iteratively recovers the point correspondences and estimates the transformation between two point sets. [sent-35, score-0.821]
</p><p>20 In the second step, we estimate the transformation using the robust estimator L2E. [sent-37, score-0.532]
</p><p>21 This estimator enable us to deal with the noise and outliers in the correspondences. [sent-38, score-0.473]
</p><p>22 The nonrigid transformation is modeled in a functional space, called the reproducing kernel Hilbert space (RKHS) [1], in which the transformation function has an explicit kernel representation. [sent-39, score-1.093]
</p><p>23 Related Work  The iterated closest point (ICP) algorithm [4] is one of the best known point registration approaches. [sent-42, score-0.684]
</p><p>24 [3] introduced a method for registration based on the shape context descriptor, which incorporates the neighborhood structure of the point set and thus helps establish correspondence between the point sets. [sent-45, score-1.088]
</p><p>25 But these methods ignore robustness when they recover the transformation from the correspondence. [sent-46, score-0.275]
</p><p>26 In related work, Chui and Rangarajan [6] established a general framework for estimating correspondence and transformations for nonrigid point matching. [sent-47, score-0.776]
</p><p>27 They modeled the transformation as a thin-plate spline and did robust point matching by an algorithm (TRS-RPM) which involved deterministic annealing and soft-assignment. [sent-48, score-0.708]
</p><p>28 Zheng and Doermann [27] introduced the notion of a neighborhood structure for the general point matching problem, and proposed a matching method, the robust point matching-preserving local neighborhood structures (RPMLNS) algorithm. [sent-51, score-0.56]
</p><p>29 The main contributions of our work include: (i) we propose a new robust algorithm to estimate a spatial transformation/mapping from correspondences with noise and outliers; (ii) we apply the robust algorithm to nonrigid point set registration and also to sparse image feature correspondence. [sent-53, score-1.356]
</p><p>30 More precisely, an inlier point correspondence (xi, yi) satisfies yi − f(xi) ∼ N(0, σ2I), where I an identity matrix of size d−×fd,( xwit)h ∼ ∼d is being theI )d,im wehnesreioIn sofa tnheid point. [sent-57, score-0.576]
</p><p>31 , the underlying inlier set) that “matches” the normal density model, and hence estimate the transformation f for the inlier set. [sent-61, score-0.706]
</p><p>32 Next, we introduce a robust estimator named L2-minimizing estimate  (L2E) which we use to estimate the transformation f. [sent-62, score-0.615]
</p><p>33 In many point matching problems, it is desirable to have a robust estimator of the transformation f because the point correspondence set S usually contains outliers. [sent-68, score-1.028]
</p><p>34 In this paper, we use the second method and adopt the L2E estimator [20, 2], a robust estimator which minimizes the L2 distance between densities, and is particularly appropriate for analyzing massive data sets where data cleaning (to remove outliers) is impractical. [sent-70, score-0.404]
</p><p>35 5 (the correct value for α) but MLE’s estimates become steadily worse as the amount of outliers increases. [sent-98, score-0.296]
</p><p>36 Top row: data samples, where the inliers are shown by cyan pluses, and the outliers by magenta circles. [sent-109, score-0.382]
</p><p>37 By contrast, (see third row) L2E estimates α correctly even when half the data is outliers and also develops a local minimum to fit the outliers when appropriate (third row, right column). [sent-115, score-0.584]
</p><p>38 We now apply the L2E formulation in (1) to the point matching problem, assuming that the noise of the inliers is given by a normal distribution, and obtain the following functional criterion:  L2E(f,σ2) =2d(π1σ)d/2−n2i? [sent-120, score-0.468]
</p><p>39 ) We model the nonrigid transformation f by requiring it to lie within a specific functional space, namely a reproducing kernel Hilbert space (RKHS) [1, 24, 16]. [sent-125, score-0.788]
</p><p>40 Note that other parameterized transformation models, for example, thin-plate splines (TPS) [23, 15], can also be easily incorporated into our formulation. [sent-126, score-0.275]
</p><p>41 But in point correspondence problem the point set typically contains hundreds or thousands of points, which causes significant complexity problems (in time and space). [sent-134, score-0.507]
</p><p>42 , the quasi-Newton algorithm with C as the old value); Update the parameter C ← arg minC L2E(C, σ2) ; AUnpdneatael tσh2e = pa rγaσm2;e The transformation f is determined by equation (3). [sent-176, score-0.275]
</p><p>43 Hence to improve convergence we  =×  use a coarse-to-fine strategy by applying deterministic annealing on the inlier noise parameter σ2. [sent-180, score-0.37]
</p><p>44 In our implementation, the number m of the control points required to construct the transformation f in equation (3) is in general not large, and so use m = 15 for all the results in this paper (increasing m only gave small changes to the results). [sent-188, score-0.319]
</p><p>45 The dimension d of the data in feature point matching for vision applications is typically 2 or 3. [sent-189, score-0.268]
</p><p>46 We define the transformation f as the initial position plus a displacement function v: f(x) = x v(x) [17], and solve for v instead of f. [sent-197, score-0.333]
</p><p>47 The parameters β and λ control the influence of the smoothness constraint on the transformation f. [sent-202, score-0.275]
</p><p>48 Nonrigid Point Set Registration Point set registration aims to align two point sets {xi}in=1 (theP ominotdseelt point saetito) naanidm {yj }lj=1 (twthoe target point }set). [sent-211, score-0.861]
</p><p>49 Typically, iln p pthoien nonrigid case, i}t requires estimating a nonrigid transformation f which warps the model point set to the target point set. [sent-212, score-1.273]
</p><p>50 We have shown above that once we have established the correspondence between the two point sets even with noise and outliers, we are able to estimate the underlying transformation between them. [sent-213, score-0.824]
</p><p>51 Next, we discuss how to find correspondences between two point sets. [sent-214, score-0.353]
</p><p>52 Establishment of Point Correspondence  Recall that our method described above does not jointly solve the transformation and point correspondence. [sent-217, score-0.406]
</p><p>53 In order to use algorithm 1to solve the transformation between two point sets, we need initial correspondences. [sent-218, score-0.464]
</p><p>54 In general, if the two point sets have similar shapes, the corresponding points have similar neighborhood structures which could be incorporated into a feature descriptor. [sent-219, score-0.315]
</p><p>55 Thus finding correspondences between two point sets is equivalent to finding for each point in one point set (e. [sent-220, score-0.659]
</p><p>56 Fortunately, the initial correspondences need not be very accurate, since our method is robust to noise and outliers. [sent-225, score-0.399]
</p><p>57 The two steps of estimating correspondences and transformations are iterated to obtain a reliable result. [sent-230, score-0.365]
</p><p>58 In this paper, we use a fixed number of iterations, typically 10 but more when the noise is big or when there are a large percentage of outliers contained in the original point sets. [sent-231, score-0.509]
</p><p>59 We summarize our point set registration method in algorithm 2. [sent-232, score-0.514]
</p><p>60 Application to Image Feature Correspondence The image feature correspondence task aims to find visual correspondences between two sets of sparse feature points {xi}in=1 and {yj }lj=1 with corresponding feature descriptors e}xtractaendd f{ryom} two input images. [sent-235, score-0.719]
</p><p>61 Our method for this task is to esti-  mate correspondences by matching feature descriptors using a smooth spatial mapping f. [sent-237, score-0.369]
</p><p>62 More specifically, we first estimate the initial correspondences based on the feature descriptors, and then use the correspondences to learn a spatial mapping f fitting the inliers by algorithm 1. [sent-238, score-0.713]
</p><p>63 We predefine a threshold τ and judge a correspondence (xi, yj) to be an inlier provided it satisfies the following condition: > τ. [sent-240, score-0.383]
</p><p>64 Note that the feature descriptors in the point set registration problem are calculated based on the point sets themselves, and are recalculated in each iteration. [sent-243, score-0.77]
</p><p>65 In practice, we find that our method works well without iteration, since we focus on determining the right correspondences which does not need precise recovery of the underlying transformation, and our approach then plays a role of rejecting outliers. [sent-246, score-0.311]
</p><p>66 Experimental Results In order to evaluate the performance of our algorithm, we conducted two types of experiments: i) nonrigid point set registration for 2D shapes; ii) sparse image feature correspondence on 2D images and 3D surfaces. [sent-250, score-1.125]
</p><p>67 For each model, there are five sets of data designed to measure the robustness of registration algorithms under deformation, occlusion, rotation, noise and outliers. [sent-255, score-0.488]
</p><p>68 We use the shape context as the feature descriptor to establish initial correspondences. [sent-257, score-0.302]
</p><p>69 2 shows the registration results of our method on solving different degrees of deformations and occlusions. [sent-261, score-0.383]
</p><p>70 Consider the results on the occlusion test in the fifth column, it is interesting that even when the occlusion ratio is 50 percent our method can still achieve a satisfactory registration result. [sent-264, score-0.463]
</p><p>71 Therefore our method can be used to provide a good initial alignment for more complicated problem-specific registration algorithms. [sent-265, score-0.441]
</p><p>72 The registration error on a pair of shapes is quantified as the average Euclidean distance between a point in the warped model and the corresponding point in the target. [sent-267, score-0.711]
</p><p>73 Then the registration performance of each algorithm is compared by the mean and standard deviation of the registration error of all the 100 samples in each distortion level. [sent-268, score-0.766]
</p><p>74 , 1st and 3rd rows), five algorithms achieve similar registration performance in both fish and Chinese character at low deformation levels, and our method generally gives better performance as the degree of 222111445 919  Ero? [sent-273, score-0.62]
</p><p>75 Point set registration results of our method on the fish (top) and Chinese character (bottom) shapes [6, 27], with deformation and occlusion presented in every two rows. [sent-300, score-0.662]
</p><p>76 The goal is to align the model point set (blue pluses) onto the target point set (red circles). [sent-301, score-0.303]
</p><p>77 For each group of experiments, the upper figure is the model and target point sets, and the lower figure is the registration result. [sent-302, score-0.582]
</p><p>78 The rightmost figures are comparisons of the registration performance of our method with shape context (SC) [3], TPS-RPM [6], RPM-LNS [27] and CPD [17] on the corresponding datasets. [sent-304, score-0.494]
</p><p>79 The error bars indicate the registration error means and standard deviations over 100 trials. [sent-305, score-0.383]
</p><p>80 More experiments on rotation, noise and outliers are also performed on the two shape models, as shown in Fig. [sent-310, score-0.364]
</p><p>81 In conclusion, our method is efficient for most non-rigid point set registration problems with moderate, and in some cases severe, distortions. [sent-314, score-0.514]
</p><p>82 It can also be used to provide  a good initial alignment for more complicated specific registration algorithms. [sent-315, score-0.441]
</p><p>83 Results of image feature correspondence on 2D image pairs of deformable objects. [sent-318, score-0.316]
</p><p>84 The inlier percentages in the initial correspondences are 79. [sent-320, score-0.481]
</p><p>85 From top to bottom, results on rotation, noise and outliers presented in every two rows. [sent-336, score-0.322]
</p><p>86 For each group of experiments, the upper figure is the data, and the lower figure is the registration result. [sent-337, score-0.41]
</p><p>87 We aim to establish correspondences between sparse image features in each image pair. [sent-345, score-0.343]
</p><p>88 In our evaluation, we first extract SIFT [13] feature points in each input image, and estimate the initial correspondences based on the corresponding SIFT descriptors. [sent-346, score-0.414]
</p><p>89 Our goal is then to reject the outliers contained in the initial correspondences and, at the same time, to keep as many inliers as possible. [sent-347, score-0.732]
</p><p>90 There are 466 initial correspondences with 95 outliers, and the inlier percentage is about 79. [sent-352, score-0.447]
</p><p>91 After using our method to establish accurate correspondences, 370 out of the 371 inliers are preserved, and simultaneously all the 95 outliers are rejected. [sent-354, score-0.475]
</p><p>92 On the rightmost pair, the deformation is relatively large and the inlier percentage in the initial  correspondences is only about 45. [sent-390, score-0.609]
</p><p>93 In addition, we also compared our method to two stateof-the-art methods, such as identifying point correspondences by correspondence function (ICF) [12] and vector field consensus (VFC) [26]. [sent-399, score-0.569]
</p><p>94 The ICF uses support vector regression to learn a correspondence function pair which maps points in one image to their corresponding points in another, and then reject outliers by the estimated correspondence functions. [sent-400, score-0.855]
</p><p>95 While the VFC converts the outlier rejection problem into a robust vector field learning problem, and learns a smooth field to fit the potential inliers as well as estimates a consensus inlier set. [sent-401, score-0.408]
</p><p>96 But VFC and our method seem to be relatively unaffected even when the number of outliers exceeds the number of inliers. [sent-405, score-0.292]
</p><p>97 Conclusion In this paper, we have presented a new approach for nonrigid point set registration. [sent-422, score-0.456]
</p><p>98 A key characteristic of our approach is the estimation of transformation from correspondences based on a robust estimator named L2E. [sent-423, score-0.741]
</p><p>99 The computational complexity of estimation of transformation is linear in the scale of correspondences. [sent-424, score-0.275]
</p><p>100 Experiments on a public dataset for nonrigid point registration, 2D and 3D real images for sparse image feature correspondence demonstrate that our approach yields results superior to those of state-of-the-art methods when there is significant noise and/or outliers in the data. [sent-426, score-1.064]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('registration', 0.383), ('nonrigid', 0.325), ('transformation', 0.275), ('outliers', 0.261), ('mle', 0.251), ('correspondences', 0.222), ('correspondence', 0.216), ('inlier', 0.167), ('estimator', 0.151), ('point', 0.131), ('inliers', 0.121), ('vfc', 0.118), ('xi', 0.111), ('deformation', 0.1), ('annealing', 0.095), ('establish', 0.093), ('functional', 0.089), ('icf', 0.079), ('degradation', 0.076), ('yj', 0.073), ('reproducing', 0.069), ('tpami', 0.068), ('hilbert', 0.067), ('matching', 0.066), ('cpd', 0.065), ('rkhs', 0.063), ('lj', 0.063), ('yi', 0.062), ('noise', 0.061), ('transformations', 0.059), ('pluses', 0.059), ('initial', 0.058), ('robust', 0.058), ('fish', 0.057), ('neighborhood', 0.054), ('dmn', 0.052), ('newspaper', 0.052), ('underlying', 0.049), ('chui', 0.049), ('estimate', 0.048), ('character', 0.047), ('deterministic', 0.047), ('gram', 0.046), ('estimating', 0.045), ('rotation', 0.044), ('points', 0.044), ('ero', 0.044), ('sets', 0.044), ('reject', 0.043), ('feature', 0.042), ('shape', 0.042), ('target', 0.041), ('rejecting', 0.04), ('occlusion', 0.04), ('preserved', 0.04), ('moderate', 0.039), ('descriptors', 0.039), ('iterated', 0.039), ('context', 0.038), ('badly', 0.038), ('chinese', 0.037), ('ucla', 0.037), ('suspect', 0.037), ('spline', 0.036), ('estimates', 0.035), ('slope', 0.035), ('named', 0.035), ('shapes', 0.035), ('yuille', 0.034), ('contaminated', 0.034), ('mismatch', 0.034), ('percentages', 0.034), ('degree', 0.033), ('gradually', 0.032), ('pair', 0.031), ('rightmost', 0.031), ('relatively', 0.031), ('icp', 0.031), ('pairs', 0.03), ('kernel', 0.03), ('numerical', 0.03), ('precisely', 0.03), ('degrades', 0.03), ('typically', 0.029), ('descriptor', 0.029), ('award', 0.029), ('sparse', 0.028), ('deformable', 0.028), ('false', 0.027), ('fit', 0.027), ('parametric', 0.027), ('degrade', 0.027), ('recovers', 0.027), ('contained', 0.027), ('arises', 0.027), ('belongie', 0.027), ('group', 0.027), ('quasinewton', 0.026), ('wina', 0.026), ('hfe', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999845 <a title="360-tfidf-1" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>2 0.29877299 <a title="360-tfidf-2" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>Author: Ming Zeng, Jiaxiang Zheng, Xuan Cheng, Xinguo Liu</p><p>Abstract: This paper presents a method for quasi-rigid objects modeling from a sequence of depth scans captured at different time instances. As quasi-rigid objects, such as human bodies, usually have shape motions during the capture procedure, it is difficult to reconstruct their geometries. We represent the shape motion by a deformation graph, and propose a model-to-partmethod to gradually integrate sampled points of depth scans into the deformation graph. Under an as-rigid-as-possible assumption, the model-to-part method can adjust the deformation graph non-rigidly, so as to avoid error accumulation in alignment, which also implicitly achieves loop-closure. To handle the drift and topological error for the deformation graph, two algorithms are introduced. First, we use a two-stage registration to largely keep the rigid motion part. Second, in the step of graph integration, we topology-adaptively integrate new parts and dynamically control the regularization effect of the deformation graph. We demonstrate the effectiveness and robustness of our method by several depth sequences of quasi-rigid objects, and an application in human shape modeling.</p><p>3 0.25154945 <a title="360-tfidf-3" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>Author: Erik Ask, Olof Enqvist, Fredrik Kahl</p><p>Abstract: This paper is concerned with model fitting in the presence of noise and outliers. Previously it has been shown that the number of outliers can be minimized with polynomial complexity in the number of measurements. This paper improves on these results in two ways. First, it is shown that for a large class of problems, the statistically more desirable truncated L2-norm can be optimized with the same complexity. Then, with the same methodology, it is shown how to transform multi-model fitting into a purely combinatorial problem—with worst-case complexity that is polynomial in the number of measurements, though exponential in the number of models. We apply our framework to a series of hard registration and stitching problems demonstrating that the approach is not only of theoretical interest. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers for fitting problems with lowdimensional models.</p><p>4 0.20302151 <a title="360-tfidf-4" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>Author: Qiang Hao, Rui Cai, Zhiwei Li, Lei Zhang, Yanwei Pang, Feng Wu, Yong Rui</p><p>Abstract: 3D model-based object recognition has been a noticeable research trend in recent years. Common methods find 2D-to-3D correspondences and make recognition decisions by pose estimation, whose efficiency usually suffers from noisy correspondences caused by the increasing number of target objects. To overcome this scalability bottleneck, we propose an efficient 2D-to-3D correspondence filtering approach, which combines a light-weight neighborhoodbased step with a finer-grained pairwise step to remove spurious correspondences based on 2D/3D geometric cues. On a dataset of 300 3D objects, our solution achieves ∼10 times speed improvement over the baseline, with a comparable recognition accuracy. A parallel implementation on a quad-core CPU can run at ∼3fps for 1280× 720 images.</p><p>5 0.19398528 <a title="360-tfidf-5" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>Author: Zsolt Sánta, Zoltan Kato</p><p>Abstract: A novel correspondence-less approach is proposed to find a thin plate spline map between a pair of deformable 3D objects represented by triangular surface meshes. The proposed method works without landmark extraction and feature correspondences. The aligning transformation is found simply by solving a system of nonlinear equations. Each equation is generated by integrating a nonlinear function over the object’s domains. We derive recursive formulas for the efficient computation of these integrals. Based on a series of comparative tests on a large synthetic dataset, our triangular mesh-based algorithm outperforms state of the art methods both in terms of computing time and accuracy. The applicability of the proposed approach has been demonstrated on the registration of 3D lung CT volumes.</p><p>6 0.17740019 <a title="360-tfidf-6" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>7 0.17729081 <a title="360-tfidf-7" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>8 0.15187648 <a title="360-tfidf-8" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>9 0.14558627 <a title="360-tfidf-9" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>10 0.12487312 <a title="360-tfidf-10" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>11 0.12185223 <a title="360-tfidf-11" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>12 0.120741 <a title="360-tfidf-12" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>13 0.11730918 <a title="360-tfidf-13" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>14 0.11554746 <a title="360-tfidf-14" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>15 0.11019818 <a title="360-tfidf-15" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>16 0.10915451 <a title="360-tfidf-16" href="./cvpr-2013-Hyperbolic_Harmonic_Mapping_for_Constrained_Brain_Surface_Registration.html">208 cvpr-2013-Hyperbolic Harmonic Mapping for Constrained Brain Surface Registration</a></p>
<p>17 0.099264733 <a title="360-tfidf-17" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>18 0.097831599 <a title="360-tfidf-18" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>19 0.097656518 <a title="360-tfidf-19" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>20 0.097122304 <a title="360-tfidf-20" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.222), (1, 0.108), (2, -0.024), (3, 0.05), (4, 0.044), (5, -0.048), (6, -0.019), (7, -0.102), (8, -0.021), (9, -0.044), (10, 0.053), (11, 0.128), (12, -0.067), (13, -0.08), (14, 0.024), (15, -0.262), (16, -0.007), (17, 0.085), (18, 0.137), (19, 0.035), (20, -0.073), (21, -0.074), (22, 0.048), (23, -0.002), (24, 0.081), (25, -0.112), (26, -0.14), (27, -0.001), (28, 0.106), (29, 0.086), (30, 0.029), (31, -0.05), (32, -0.033), (33, -0.004), (34, 0.155), (35, -0.12), (36, -0.221), (37, 0.119), (38, 0.08), (39, 0.039), (40, -0.04), (41, 0.006), (42, 0.028), (43, -0.024), (44, 0.003), (45, -0.033), (46, 0.001), (47, -0.037), (48, 0.06), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96367246 <a title="360-lsi-1" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>2 0.79561174 <a title="360-lsi-2" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>Author: Simon Korman, Daniel Reichman, Gilad Tsur, Shai Avidan</p><p>Abstract: Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences (SAD) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the SAD error using a sublinear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.</p><p>3 0.79264796 <a title="360-lsi-3" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>Author: Ming Zeng, Jiaxiang Zheng, Xuan Cheng, Xinguo Liu</p><p>Abstract: This paper presents a method for quasi-rigid objects modeling from a sequence of depth scans captured at different time instances. As quasi-rigid objects, such as human bodies, usually have shape motions during the capture procedure, it is difficult to reconstruct their geometries. We represent the shape motion by a deformation graph, and propose a model-to-partmethod to gradually integrate sampled points of depth scans into the deformation graph. Under an as-rigid-as-possible assumption, the model-to-part method can adjust the deformation graph non-rigidly, so as to avoid error accumulation in alignment, which also implicitly achieves loop-closure. To handle the drift and topological error for the deformation graph, two algorithms are introduced. First, we use a two-stage registration to largely keep the rigid motion part. Second, in the step of graph integration, we topology-adaptively integrate new parts and dynamically control the regularization effect of the deformation graph. We demonstrate the effectiveness and robustness of our method by several depth sequences of quasi-rigid objects, and an application in human shape modeling.</p><p>4 0.78122425 <a title="360-lsi-4" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>Author: Erik Ask, Olof Enqvist, Fredrik Kahl</p><p>Abstract: This paper is concerned with model fitting in the presence of noise and outliers. Previously it has been shown that the number of outliers can be minimized with polynomial complexity in the number of measurements. This paper improves on these results in two ways. First, it is shown that for a large class of problems, the statistically more desirable truncated L2-norm can be optimized with the same complexity. Then, with the same methodology, it is shown how to transform multi-model fitting into a purely combinatorial problem—with worst-case complexity that is polynomial in the number of measurements, though exponential in the number of models. We apply our framework to a series of hard registration and stitching problems demonstrating that the approach is not only of theoretical interest. It gives a practical method for simultaneously dealing with measurement noise and large amounts of outliers for fitting problems with lowdimensional models.</p><p>5 0.77165216 <a title="360-lsi-5" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>6 0.74519777 <a title="360-lsi-6" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>7 0.73272061 <a title="360-lsi-7" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>8 0.69791442 <a title="360-lsi-8" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>9 0.66350079 <a title="360-lsi-9" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>10 0.64443296 <a title="360-lsi-10" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>11 0.63483936 <a title="360-lsi-11" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>12 0.61442959 <a title="360-lsi-12" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>13 0.57558322 <a title="360-lsi-13" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>14 0.55607927 <a title="360-lsi-14" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>15 0.52064723 <a title="360-lsi-15" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>16 0.51301777 <a title="360-lsi-16" href="./cvpr-2013-A_Practical_Rank-Constrained_Eight-Point_Algorithm_for_Fundamental_Matrix_Estimation.html">23 cvpr-2013-A Practical Rank-Constrained Eight-Point Algorithm for Fundamental Matrix Estimation</a></p>
<p>17 0.50872117 <a title="360-lsi-17" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>18 0.50626338 <a title="360-lsi-18" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>19 0.50003475 <a title="360-lsi-19" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>20 0.49640924 <a title="360-lsi-20" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.181), (16, 0.066), (26, 0.102), (28, 0.01), (33, 0.236), (66, 0.15), (67, 0.057), (69, 0.047), (87, 0.058), (96, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92594659 <a title="360-lda-1" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>Author: Zhe Wu, Ping Tan</p><p>Abstract: Under unknown directional lighting, the uncalibrated Lambertian photometric stereo algorithm recovers the shape of a smooth surface up to the generalized bas-relief (GBR) ambiguity. We resolve this ambiguity from the halfvector symmetry, which is observed in many isotropic materials. Under this symmetry, a 2D BRDF slice with low-rank structure can be obtained from an image, if the surface normals and light directions are correctly recovered. In general, this structure is destroyed by the GBR ambiguity. As a result, we can resolve the ambiguity by restoring this structure. We develop a simple algorithm of auto-calibration from separable homogeneous specular reflection of real images. Compared with previous methods, this method takes a holistic approach to exploiting reflectance symmetry and produces superior results.</p><p>2 0.90994215 <a title="360-lda-2" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>Author: Ilya Reshetouski, Alkhazur Manakov, Ayush Bandhari, Ramesh Raskar, Hans-Peter Seidel, Ivo Ihrke</p><p>Abstract: We investigate the problem of identifying the position of a viewer inside a room of planar mirrors with unknown geometry in conjunction with the room’s shape parameters. We consider the observations to consist of angularly resolved depth measurements of a single scene point that is being observed via many multi-bounce interactions with the specular room geometry. Applications of this problem statement include areas such as calibration, acoustic echo cancelation and time-of-flight imaging. We theoretically analyze the problem and derive sufficient conditions for a combination of convex room geometry, observer, and scene point to be reconstructable. The resulting constructive algorithm is exponential in nature and, therefore, not directly applicable to practical scenarios. To counter the situation, we propose theoretically devised geometric constraints that enable an efficient pruning of the solution space and develop a heuristic randomized search algorithm that uses these constraints to obtain an effective solution. We demonstrate the effectiveness of our algorithm on extensive simulations as well as in a challenging real-world calibration scenario.</p><p>3 0.8866542 <a title="360-lda-3" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>Author: Zhenglong Zhou, Zhe Wu, Ping Tan</p><p>Abstract: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple, which consists of only a digital camera and a handheld light source. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distributionfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments, the captured shapes are accurate to 0.3 millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9%.</p><p>same-paper 4 0.88507754 <a title="360-lda-4" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>Author: Jiayi Ma, Ji Zhao, Jinwen Tian, Zhuowen Tu, Alan L. Yuille</p><p>Abstract: We present a new point matching algorithm for robust nonrigid registration. The method iteratively recovers the point correspondence and estimates the transformation between two point sets. In the first step of the iteration, feature descriptors such as shape context are used to establish rough correspondence. In the second step, we estimate the transformation using a robust estimator called L2E. This is the main novelty of our approach and it enables us to deal with the noise and outliers which arise in the correspondence step. The transformation is specified in a functional space, more specifically a reproducing kernel Hilbert space. We apply our method to nonrigid sparse image feature correspondence on 2D images and 3D surfaces. Our results quantitatively show that our approach outperforms state-ofthe-art methods, particularly when there are a large number of outliers. Moreover, our method of robustly estimating transformations from correspondences is general and has many other applications.</p><p>5 0.88419634 <a title="360-lda-5" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>Author: Jonathan T. Barron, Jitendra Malik</p><p>Abstract: In this paper we extend the “shape, illumination and reflectance from shading ” (SIRFS) model [3, 4], which recovers intrinsic scene properties from a single image. Though SIRFS performs well on images of segmented objects, it performs poorly on images of natural scenes, which contain occlusion and spatially-varying illumination. We therefore present Scene-SIRFS, a generalization of SIRFS in which we have a mixture of shapes and a mixture of illuminations, and those mixture components are embedded in a “soft” segmentation of the input image. We additionally use the noisy depth maps provided by RGB-D sensors (in this case, the Kinect) to improve shape estimation. Our model takes as input a single RGB-D image and produces as output an improved depth map, a set of surface normals, a reflectance image, a shading image, and a spatially varying model of illumination. The output of our model can be used for graphics applications, or for any application involving RGB-D images.</p><p>6 0.88120437 <a title="360-lda-6" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>7 0.86612916 <a title="360-lda-7" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>8 0.86043113 <a title="360-lda-8" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>9 0.85978687 <a title="360-lda-9" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>10 0.85950649 <a title="360-lda-10" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>11 0.85894144 <a title="360-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.85721803 <a title="360-lda-12" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>13 0.85652316 <a title="360-lda-13" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>14 0.8564961 <a title="360-lda-14" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>15 0.85593873 <a title="360-lda-15" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>16 0.85574949 <a title="360-lda-16" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>17 0.85568178 <a title="360-lda-17" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>18 0.85549438 <a title="360-lda-18" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>19 0.85470855 <a title="360-lda-19" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>20 0.8534618 <a title="360-lda-20" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
