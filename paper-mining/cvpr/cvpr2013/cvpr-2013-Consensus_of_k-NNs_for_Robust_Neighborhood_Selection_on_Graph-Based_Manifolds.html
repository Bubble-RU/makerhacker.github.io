<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-91" href="#">cvpr2013-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</h1>
<br/><p>Source: <a title="cvpr-2013-91-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Premachandran_Consensus_of_k-NNs_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Vittal Premachandran, Ramakrishna Kakarala</p><p>Abstract: Propagating similarity information along the data manifold requires careful selection of local neighborhood. Selecting a “good” neighborhood in an unsupervised setting, given an affinity graph, has been a difficult task. The most common way to select a local neighborhood has been to use the k-nearest neighborhood (k-NN) selection criterion. However, it has the tendency to include noisy edges. In this paper, we propose a way to select a robust neighborhood using the consensus of multiple rounds of k-NNs. We explain how using consensus information can give better control over neighborhood selection. We also explain in detail the problems with another recently proposed neighborhood selection criteria, i.e., Dominant Neighbors, and show that our method is immune to those problems. Finally, we show the results from experiments in which we compare our method to other neighborhood selection approaches. The results corroborate our claims that consensus ofk-NNs does indeed help in selecting more robust and stable localities.</p><p>Reference: <a title="cvpr-2013-91-reference" href="../cvpr2013_reference/cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Selecting a “good” neighborhood in an unsupervised setting, given an affinity graph, has been a difficult task. [sent-5, score-0.673]
</p><p>2 The most common way to select a local neighborhood has been to use the k-nearest neighborhood (k-NN) selection criterion. [sent-6, score-1.089]
</p><p>3 In this paper, we propose a way to select a robust neighborhood using the consensus of multiple rounds of k-NNs. [sent-8, score-1.089]
</p><p>4 We explain how using consensus information can give better control over neighborhood selection. [sent-9, score-1.056]
</p><p>5 We also explain in detail the problems with another recently proposed neighborhood selection criteria, i. [sent-10, score-0.586]
</p><p>6 Finally, we show the results from experiments in which we compare our method to other neighborhood selection approaches. [sent-13, score-0.557]
</p><p>7 The results corroborate our claims that consensus ofk-NNs does indeed help in selecting more robust and stable localities. [sent-14, score-0.603]
</p><p>8 [16] perform diffusion on a locally constrained sparse graph, while in [18], the diffusion process is performed on a tensor product graph, thus allowing the capture of higher order information. [sent-36, score-0.439]
</p><p>9 Both [16] and [18] follow different styles of graph sparsification (k-nearest neighbors and dominant neighbors, respectively). [sent-38, score-0.522]
</p><p>10 The selection of a proper neighborhood is critical for diffusion to work. [sent-39, score-0.791]
</p><p>11 This paper addresses the question of how to build a strong local neighborhood given data in the form of a graph. [sent-40, score-0.504]
</p><p>12 We propose a new way for neighborhood selection by making use of the consensus information from various neighborhoods, and show that such a neighborhood is much more robust to parameter selections. [sent-44, score-1.584]
</p><p>13 111555999422  We also show that using our consensus neighborhood information, we are able to achieve better retrieval results on standard databases (Ex: MPEG7 shape database), as opposed to the use of k-NN or DN. [sent-45, score-1.116]
</p><p>14 Using the NxN matrix allows us to exploit much more information about the neighborhood structure of the data manifold than using just the pairwise information. [sent-54, score-0.708]
</p><p>15 The authors in [15] convert the cost matrix into a similarity matrix (also named as affinity matrix) and use this affinity matrix to learn the geodesics on the manifold. [sent-58, score-0.523]
</p><p>16 Both these methods rely on the proper choice of the kernel neighborhood parameter, K, and choosing a “bad” K, would adversely affect the generation of a good affinity matrix. [sent-67, score-0.77]
</p><p>17 Hence, graph sparsification tries to prune out edges between nodes that are not in a local neighborhood. [sent-74, score-0.534]
</p><p>18 For a graph G(V, E), we can have different variants of the neighborhood graph G′, such as, • Symmetric k-NN graph neighborhood G′(V, E), where there is an edge E(vi, vj) if vj ∈ k-NN(vi) or vi ∈ k-NN(vj). [sent-75, score-1.615]
</p><p>19 • Mutual k-NN graph neighborhood G′(V, E), where there is an edge E(vi, vj) if vj ∈ k-NN(vi) and vi ∈ k-NN(vj). [sent-76, score-0.889]
</p><p>20 So, selecting a single ǫ for all nodes in the graph might not properly capture the neighborhood structure of the nodes. [sent-81, score-0.831]
</p><p>21 However, as pointed to in [17], the k-NN graph has a tendency to include noisy edges in the neighborhood of a  ≤  node. [sent-83, score-0.813]
</p><p>22 Moreover, using a fixed-size neighborhood might not adequately capture the locality in the manifold. [sent-84, score-0.555]
</p><p>23 The need for a variable-size neighborhood for manifold structure learning was pointed to in [17] and [19]. [sent-85, score-0.636]
</p><p>24 [19] propose an adaptive neighborhood selection for manifold learning in the feature space, while Yang et al. [sent-87, score-0.689]
</p><p>25 [17] make use of dominant set computation method [9] for selecting the dominant subset of the k-nearest neighbors in the data space. [sent-88, score-0.474]
</p><p>26 The idea behind selecting a dominant neighborhood as opposed to just the k-nearest neighbors is that, the dominant neighborhood usually forms tight clusters and is therefore composed of nodes that are highly similar to each other. [sent-89, score-1.653]
</p><p>27 While the dominant neighborhood graph can select variable sized neighborhoods, it is still dependent on the selection of the sparsification parameter, k1. [sent-91, score-1.037]
</p><p>28 We will also explain how our consensus neighborhood selection strategy will help mitigate those problems. [sent-94, score-1.109]
</p><p>29 Consensus k-NNs Both k-NN and DN can select good local neighborhoods  as long as the graph sparsification parameter, k, is properly selected. [sent-96, score-0.492]
</p><p>30 Looking back at symmetric k-NN neighborhood selection, we can see that an edge between vi and vj is selected if vj ∈ kNN(vi) or vi ∈ kNN(vj). [sent-97, score-1.062]
</p><p>31 Accurate edges are those edges that connect a particular node to other nodes, which are part of the true neighborhood, while noisy edges are those that connect a node to other nodes that are not part of the true neighborhood. [sent-103, score-0.622]
</p><p>32 As the neighborhood size, k, increases, so do the chances of adding in noisy edges. [sent-104, score-0.61]
</p><p>33 To make the neighborhood more stable even for large values of k, we propose to make use of consensus information from the multiple k-NN procedures that are applied to the graph. [sent-105, score-1.073]
</p><p>34 We define a consensus matrix, C, to keep track of the number of times a pair of nodes (vp, vq) appear together among all rounds of k-NN. [sent-107, score-0.715]
</p><p>35 A simple pseudocode to populate our consensus matrix is given below. [sent-108, score-0.571]
</p><p>36 C = 0; for i= 1 : N do  Algorithm 1: Algorithm to collect the consensus information from multiple rounds of k-NNs. [sent-109, score-0.557]
</p><p>37 The first advantage that we obtain from having such a consensus matrix is that it allows us to capture stronger relations between pairs of nodes. [sent-110, score-0.596]
</p><p>38 Whereas, if we use the consensus of k-NNs, we can be far more certain about the similarity, or dissimilarity, between pairs of vertices. [sent-112, score-0.548]
</p><p>39 The relation between a pair of nodes (vp, vq), which are a part of k-NN(vi), were ignored in the case of symmetric k-NN (just the edges between vi and vp, and between vi and vq, were added to the graph). [sent-113, score-0.495]
</p><p>40 However, the fact that the two nodes vp and vq are a part of the same neighborhood, albeit some other node vi, shows that vp and vq are similar to each other as well. [sent-114, score-0.485]
</p><p>41 This points to the second advantage of using consensus information. [sent-116, score-0.523]
</p><p>42 Probabilistic Neighborhood Information: A rownormalized consensus matrix can be viewed as a probability matrix, where each value specifies the probability of that pair ofnodes being similar to each other. [sent-117, score-0.637]
</p><p>43 We are not privy to such soft measures if we use the symmetric k-NN for neighborhood generation. [sent-118, score-0.546]
</p><p>44 Such noisy edges might have been included in the neighborhood by chance. [sent-120, score-0.678]
</p><p>45 With the use of the consensus matrix, we can easily identify such noisy edges as those edges that have a low probability value, and can hence be ignored. [sent-122, score-0.785]
</p><p>46 Figure 2b shows a consensus matrix from one of our experiments. [sent-123, score-0.571]
</p><p>47 With the use of such probabilistic information, we get more control over neighborhood tuning. [sent-124, score-0.504]
</p><p>48 More formally, the new probabilistic neighborhood graph G′(V, E), has an edge E(vi , vj), if C(i, j) ≥ τ. [sent-126, score-0.647]
</p><p>49 Unlike symmetric knearest neighborhoods, even for a fixed value of τ we can get variable-sized neighborhoods that can adaptively represent the local neighborhood structure. [sent-128, score-0.736]
</p><p>50 Our consensus neighborhood identification method is better in many ways compared to the dominant set extraction, especially when used for manifold learning. [sent-132, score-1.344]
</p><p>51 Advantages of Consensus Neighborhood over Dominant Sets As mentioned in the previous sections, the goal of any graph sparsification method is to produce a neighborhood that best preserves the locally linear neighborhood property of manifolds. [sent-136, score-1.311]
</p><p>52 Also, note that class 1 (rows 1to 20) has chosen a neighborhood that belongs to a completely different class, class 7 (columns 121 to 140). [sent-142, score-0.558]
</p><p>53 This is because of the false neighborhood problem that was described in Section 3. [sent-143, score-0.504]
</p><p>54 (b) Figure shows a probabilistic version of the neighborhood matrix obtained from consensus of k-NNs. [sent-145, score-1.075]
</p><p>55 Note that the neighborhood of class 1 still includes others nodes from the same class, with high probability, unlike DN. [sent-146, score-0.669]
</p><p>56 , its tendency include noisy edges), which forced the adoption of DN, is overcome by our method of using consensus information from multiple k-NNs. [sent-151, score-0.669]
</p><p>57 The graph sparsification step should not output a graph with two or more subgraphs that have no connections between them. [sent-157, score-0.427]
</p><p>58 If diffusion was performed on such  a graph, the true geodesic distances between pairs of nodes would be learnt only among the nodes within a connected subgraph. [sent-164, score-0.694]
</p><p>59 This meant that, while using the dominant set for neighborhood extraction, we were never able to learn the geodesic distance between nodes belonging to mutually disconnected subgraphs for particular values of k. [sent-167, score-1.009]
</p><p>60 Summary: To summarize, consensus of k-NNs has lots of advantages over dominant neighbors. [sent-173, score-0.708]
</p><p>61 There are no such problems while using consensus of k-NNs as k-NN guarantee that the most similar nodes to a particular node are always part of the local neighborhood. [sent-177, score-0.706]
</p><p>62 Finally,  the chances of graph fragmentation is much less (in fact, hardly ever the case) in the case of consensus k-NNs when compared to the dominant neighbors. [sent-178, score-0.946]
</p><p>63 Diffusion Using Consensus Information The consensus information just gathered can be used in two ways before performing diffusion. [sent-182, score-0.523]
</p><p>64 Using the consensus information, the parameter can be chosen as the mean of the distances between pairs of nodes that have a probability greater than, say, τ. [sent-187, score-0.767]
</p><p>65 Secondly, consensus information has significant uses during the graph sparsification stage i. [sent-189, score-0.79]
</p><p>66 We have explained above, how a good neighborhood graph, G′, can be obtained from the consensus information. [sent-192, score-1.047]
</p><p>67 Given a neighborhood graph G′ generated using consensus of k-NNs, one can obtain a probabilistic transition matrix P as  P(i,j) =PEj′E(i′,(ji),j),  (8)  where, E′ is the edge set obtained from the sparse graph G′. [sent-193, score-1.329]
</p><p>68 Once P is calculated, we can now perform diffusion using any of the graph diffusion procedures (Ex: LCDP [16], or TPG [18]). [sent-194, score-0.485]
</p><p>69 Experiments To demonstrate the stability of using consensus neighborhood, we compare the diffusion process when using kNN, DN, and consensus of k-NNs for many different values of k. [sent-197, score-1.233]
</p><p>70 Here, we compare the performance of consensus of k-NNs with the simple k-NN, by purposely setting the value of k to be “large”. [sent-205, score-0.572]
</p><p>71 The first row is obtained by using the naive k-NN, and the sec-  ond row from consensus of k-NNs. [sent-207, score-0.523]
</p><p>72 0 6543 241201 20 40 (c)60 80 10 (d) Figure 3: Top row shows the diffusion results from using simple k-NN sparse graph, and the bottom row, using the sparse graph generated using consensus of k-NNs. [sent-217, score-0.821]
</p><p>73 Clearly, consensus of k-NNs has learnt a better neighborhood than the simple k-NN. [sent-222, score-1.063]
</p><p>74 On the other hand, Figure 3c shows a clear oneto-one mapping of the coordinates to the arc length, which shows that the neighborhood generated by consensus of kNNs was more robust to noise. [sent-225, score-1.08]
</p><p>75 We have tried for many different k’s and found that consensus of k-NNs performs better than the simple k-NN. [sent-227, score-0.523]
</p><p>76 neighborhood size, for four different affinity matrices, while using TPG diffusion process. [sent-239, score-0.86]
</p><p>77 The affinity matrices were generated using different kernels, which are calculated using different neighborhood sizes of (a) 25, (b) 50, (c) 75, and (d) 100. [sent-240, score-0.707]
</p><p>78 Consensus of k-NNs performs better than DN and k-NN for almost all neighborhood sizes. [sent-243, score-0.504]
</p><p>79 Also, the performance deterioration rate is significantly slower than both k-NN and DN for larger neighborhood sizes, thus pointing towards stable  localities. [sent-244, score-0.55]
</p><p>80 has 20 objects  Since the MPEG-7 shape database  per class,  it comes  as no surprise  that  the previous state-of-the-art Bullseye scores were reported for neighborhood sizes that were purposely selected to be smaller than or equal to 20 (k = 20 in [16] and k = 10 in [18]). [sent-250, score-0.681]
</p><p>81 Such parameter selections are an example of supervised neighborhood selection. [sent-251, score-0.504]
</p><p>82 Therefore, we would like our neighborhoods to be stable enough even when k is chosen to be greater than the number of examples in a particular class, and thus including more objects into the local neighborhood than there would be in the “true neighborhood”. [sent-253, score-0.725]
</p><p>83 neighborhood sizes, when using k-NN, DN and consensus k-NN. [sent-255, score-1.027]
</p><p>84 Remember from Section 2 that the generation of a good sparse affinity matrix requires the choice of two neighborhood parameters: one while generating the affinity matrix and one while sparsifying the matrix. [sent-256, score-1.034]
</p><p>85 For each of these affinity matrices, we experiment over different neighborhood sizes while sparsifying the graph. [sent-258, score-0.737]
</p><p>86 From the plots, we can see that the neighborhood generated by using consensus of k-NNs is quite stable to the neighborhood size parameter. [sent-259, score-1.577]
</p><p>87 Once the neighborhood size goes above the true neighborhood size (i. [sent-260, score-1.042]
</p><p>88 , k > 20), the performance obtained from both k-NN and DN starts dete-  riorating at a quicker rate than consensus k-NNs. [sent-262, score-0.523]
</p><p>89 We can also see that, up to a neighborhood size of 35-40, DN outperforms k-NN, but after that, k-NN performs better. [sent-263, score-0.504]
</p><p>90 Moreover, even while selecting small neighborhood sizes of up to 35-40, we can see that consensus of k-NNs has learnt a 111556990088  )%10905Ck−oNnNsensus k−NNs  ro(e ullseye Sc898050 B 75  702 030405060708090100110120130140150 Neighborhood Size  Figure 5: Bullseye score v. [sent-266, score-1.131]
</p><p>91 Consensus of k-NNs produces a more stable neighborhood than k-NN for larger values of k. [sent-270, score-0.55]
</p><p>92 better neighborhood than DN, thus producing better Bullseye scores after diffusion. [sent-271, score-0.525]
</p><p>93 For extremely small values of k, there is a relative drop in performance of our method because there is hardly any consensus information that can be extracted while using such small neighborhood sizes. [sent-272, score-1.064]
</p><p>94 We also compared the effect of the using k-NN versus  consensus of k-NNs using the Locally Constrained Diffusion Process (LCDP). [sent-273, score-0.523]
</p><p>95 We found that the neighborhood selection had a similar effect as while using TPG. [sent-274, score-0.557]
</p><p>96 Ex: We can generate a consensus of ǫ-neighborhood graphs while using ǫ-neighborhood sparsification. [sent-278, score-0.568]
</p><p>97 In our experiments we found that consensus of ǫ-neighborhoods outperformed the simple ǫ-neighborhood by a Bullseye score of 1-3%, when we experimented with multiple threshold values (ǫ). [sent-279, score-0.523]
</p><p>98 Conclusion In this paper, we have identified some of the problems with the currently used neighborhood selection methods. [sent-283, score-0.557]
</p><p>99 We also propose a new way for neighborhood selection, which makes use of the consensus information from different k-NNs. [sent-284, score-1.027]
</p><p>100 In the future, we would like to explore how such consensus information can be used to obtain adaptive  neighborhoods on graph-based manifolds. [sent-286, score-0.678]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('consensus', 0.523), ('neighborhood', 0.504), ('bullseye', 0.188), ('diffusion', 0.187), ('dominant', 0.185), ('affinity', 0.169), ('sparsification', 0.156), ('neighborhoods', 0.155), ('dn', 0.148), ('vj', 0.138), ('nodes', 0.138), ('manifold', 0.132), ('graph', 0.111), ('vi', 0.104), ('edges', 0.087), ('spiral', 0.078), ('vp', 0.077), ('geodesic', 0.075), ('tpg', 0.075), ('vq', 0.074), ('neighbors', 0.07), ('noisy', 0.065), ('shapes', 0.059), ('arc', 0.053), ('selection', 0.053), ('shape', 0.052), ('subgraphs', 0.049), ('fragmentation', 0.049), ('purposely', 0.049), ('matrix', 0.048), ('latecki', 0.047), ('tendency', 0.046), ('stable', 0.046), ('graphs', 0.045), ('node', 0.045), ('plots', 0.043), ('kdc', 0.042), ('pavan', 0.042), ('prune', 0.042), ('symmetric', 0.042), ('similarity', 0.041), ('chances', 0.041), ('ij', 0.039), ('distances', 0.038), ('lcdp', 0.038), ('nxn', 0.038), ('hardly', 0.037), ('retrieval', 0.037), ('ex', 0.036), ('locally', 0.036), ('learnt', 0.036), ('dissimilarity', 0.036), ('forced', 0.035), ('disconnected', 0.035), ('yang', 0.035), ('knearest', 0.035), ('selecting', 0.034), ('susceptible', 0.034), ('rounds', 0.034), ('sizes', 0.034), ('true', 0.034), ('clusters', 0.033), ('sj', 0.032), ('edge', 0.032), ('knn', 0.031), ('sparsifying', 0.03), ('propagating', 0.03), ('tensor', 0.029), ('dimensionality', 0.029), ('explain', 0.029), ('adversely', 0.029), ('locality', 0.029), ('select', 0.028), ('class', 0.027), ('generation', 0.026), ('fragmented', 0.025), ('pairs', 0.025), ('critical', 0.025), ('bai', 0.024), ('pairwise', 0.024), ('probability', 0.023), ('belonging', 0.023), ('propagate', 0.023), ('connected', 0.023), ('properly', 0.022), ('proper', 0.022), ('might', 0.022), ('roweis', 0.022), ('si', 0.022), ('coherent', 0.022), ('dissimilar', 0.021), ('scores', 0.021), ('subsets', 0.021), ('database', 0.021), ('greater', 0.02), ('generating', 0.02), ('noticed', 0.02), ('pair', 0.02), ('good', 0.02), ('sg', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="91-tfidf-1" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>Author: Vittal Premachandran, Ramakrishna Kakarala</p><p>Abstract: Propagating similarity information along the data manifold requires careful selection of local neighborhood. Selecting a “good” neighborhood in an unsupervised setting, given an affinity graph, has been a difficult task. The most common way to select a local neighborhood has been to use the k-nearest neighborhood (k-NN) selection criterion. However, it has the tendency to include noisy edges. In this paper, we propose a way to select a robust neighborhood using the consensus of multiple rounds of k-NNs. We explain how using consensus information can give better control over neighborhood selection. We also explain in detail the problems with another recently proposed neighborhood selection criteria, i.e., Dominant Neighbors, and show that our method is immune to those problems. Finally, we show the results from experiments in which we compare our method to other neighborhood selection approaches. The results corroborate our claims that consensus ofk-NNs does indeed help in selecting more robust and stable localities.</p><p>2 0.29633415 <a title="91-tfidf-2" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>Author: Michael Donoser, Horst Bischof</p><p>Abstract: In this paper we revisit diffusion processes on affinity graphs for capturing the intrinsic manifold structure defined by pairwise affinity matrices. Such diffusion processes have already proved the ability to significantly improve subsequent applications like retrieval. We give a thorough overview of the state-of-the-art in this field and discuss obvious similarities and differences. Based on our observations, we are then able to derive a generic framework for diffusion processes in the scope of retrieval applications, where the related work represents specific instances of our generic formulation. We evaluate our framework on several retrieval tasks and are able to derive algorithms that e. g. achieve a 100% bullseye score on the popular MPEG7 shape retrieval data set.</p><p>3 0.19580945 <a title="91-tfidf-3" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>Author: Ahmed T. Kamal, Jay A. Farrell, Amit K. Roy-Chowdhury</p><p>Abstract: Due to their high fault-tolerance, ease of installation and scalability to large networks, distributed algorithms have recently gained immense popularity in the sensor networks community, especially in computer vision. Multitarget tracking in a camera network is one of the fundamental problems in this domain. Distributed estimation algorithms work by exchanging information between sensors that are communication neighbors. Since most cameras are directional sensors, it is often the case that neighboring sensors may not be sensing the same target. Such sensors that do not have information about a target are termed as “naive ” with respect to that target. In this paper, we propose consensus-based distributed multi-target tracking algorithms in a camera network that are designed to address this issue of naivety. The estimation errors in tracking and data association, as well as the effect of naivety, are jointly addressed leading to the development of an informationweighted consensus algorithm, which we term as the Multitarget Information Consensus (MTIC) algorithm. The incorporation of the probabilistic data association mecha- nism makes the MTIC algorithm very robust to false measurements/clutter. Experimental analysis is provided to support the theoretical results.</p><p>4 0.16457383 <a title="91-tfidf-4" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>Author: Song Cao, Noah Snavely</p><p>Abstract: Recognizing the location of a query image by matching it to a database is an important problem in computer vision, and one for which the representation of the database is a key issue. We explore new ways for exploiting the structure of a database by representing it as a graph, and show how the rich information embedded in a graph can improve a bagof-words-based location recognition method. In particular, starting from a graph on a set of images based on visual connectivity, we propose a method for selecting a set of subgraphs and learning a local distance function for each using discriminative techniques. For a query image, each database image is ranked according to these local distance functions in order to place the image in the right part of the graph. In addition, we propose a probabilistic method for increasing the diversity of these ranked database images, again based on the structure of the image graph. We demonstrate that our methods improve performance over standard bag-of-words methods on several existing location recognition datasets.</p><p>5 0.14218423 <a title="91-tfidf-5" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>Author: Lili Tao, Bogdan J. Matuszewski</p><p>Abstract: In this paper, a novel approach based on a non-linear manifold learning technique is proposed to recover 3D nonrigid structures from 2D image sequences captured by a single camera. Most ofthe existing approaches assume that 3D shapes can be accurately modelled in a linear subspace. These techniques perform well when the deformations are relatively small or simple, but fail when more complex deformations need to be recovered. The non-linear deformations are often observed in highly flexible objects for which the use of the linear model is impractical. A specific type of shape variations might be governed by only a small number of parameters, therefore can be wellrepresented in a low dimensional manifold. We learn a nonlinear shape prior using diffusion maps method. The key contribution in this paper is the introduction of the shape prior that constrain the reconstructed shapes to lie in the learned manifold. The proposed methodology has been validated quantitatively and qualitatively on 2D points sequences projected from the 3D motion capture data and real 2D video sequences. The comparisons oftheproposed man- ifold based method against several state-of-the-art techniques are shown on different types of deformable objects.</p><p>6 0.13030434 <a title="91-tfidf-6" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>7 0.11535572 <a title="91-tfidf-7" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>8 0.11194326 <a title="91-tfidf-8" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>9 0.093524285 <a title="91-tfidf-9" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>10 0.087339655 <a title="91-tfidf-10" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>11 0.084969729 <a title="91-tfidf-11" href="./cvpr-2013-Fully-Connected_CRFs_with_Non-Parametric_Pairwise_Potential.html">180 cvpr-2013-Fully-Connected CRFs with Non-Parametric Pairwise Potential</a></p>
<p>12 0.080482543 <a title="91-tfidf-12" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>13 0.079010613 <a title="91-tfidf-13" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>14 0.078795299 <a title="91-tfidf-14" href="./cvpr-2013-Rolling_Riemannian_Manifolds_to_Solve_the_Multi-class_Classification_Problem.html">367 cvpr-2013-Rolling Riemannian Manifolds to Solve the Multi-class Classification Problem</a></p>
<p>15 0.078533866 <a title="91-tfidf-15" href="./cvpr-2013-Multi-target_Tracking_by_Lagrangian_Relaxation_to_Min-cost_Network_Flow.html">300 cvpr-2013-Multi-target Tracking by Lagrangian Relaxation to Min-cost Network Flow</a></p>
<p>16 0.07718832 <a title="91-tfidf-16" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>17 0.076598689 <a title="91-tfidf-17" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>18 0.076170154 <a title="91-tfidf-18" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>19 0.070754193 <a title="91-tfidf-19" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>20 0.070390821 <a title="91-tfidf-20" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.158), (1, 0.014), (2, 0.001), (3, 0.032), (4, 0.071), (5, -0.009), (6, -0.015), (7, -0.104), (8, -0.08), (9, -0.046), (10, 0.065), (11, 0.04), (12, -0.119), (13, -0.036), (14, -0.026), (15, -0.055), (16, -0.093), (17, -0.001), (18, 0.003), (19, -0.03), (20, 0.089), (21, 0.11), (22, -0.004), (23, 0.091), (24, 0.037), (25, -0.034), (26, 0.139), (27, -0.037), (28, -0.039), (29, 0.164), (30, -0.004), (31, -0.11), (32, 0.068), (33, 0.034), (34, 0.07), (35, 0.128), (36, 0.093), (37, 0.009), (38, 0.004), (39, -0.048), (40, 0.027), (41, -0.082), (42, 0.003), (43, 0.16), (44, -0.025), (45, -0.091), (46, -0.078), (47, 0.177), (48, 0.035), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97026342 <a title="91-lsi-1" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>Author: Vittal Premachandran, Ramakrishna Kakarala</p><p>Abstract: Propagating similarity information along the data manifold requires careful selection of local neighborhood. Selecting a “good” neighborhood in an unsupervised setting, given an affinity graph, has been a difficult task. The most common way to select a local neighborhood has been to use the k-nearest neighborhood (k-NN) selection criterion. However, it has the tendency to include noisy edges. In this paper, we propose a way to select a robust neighborhood using the consensus of multiple rounds of k-NNs. We explain how using consensus information can give better control over neighborhood selection. We also explain in detail the problems with another recently proposed neighborhood selection criteria, i.e., Dominant Neighbors, and show that our method is immune to those problems. Finally, we show the results from experiments in which we compare our method to other neighborhood selection approaches. The results corroborate our claims that consensus ofk-NNs does indeed help in selecting more robust and stable localities.</p><p>2 0.89141965 <a title="91-lsi-2" href="./cvpr-2013-Diffusion_Processes_for_Retrieval_Revisited.html">126 cvpr-2013-Diffusion Processes for Retrieval Revisited</a></p>
<p>Author: Michael Donoser, Horst Bischof</p><p>Abstract: In this paper we revisit diffusion processes on affinity graphs for capturing the intrinsic manifold structure defined by pairwise affinity matrices. Such diffusion processes have already proved the ability to significantly improve subsequent applications like retrieval. We give a thorough overview of the state-of-the-art in this field and discuss obvious similarities and differences. Based on our observations, we are then able to derive a generic framework for diffusion processes in the scope of retrieval applications, where the related work represents specific instances of our generic formulation. We evaluate our framework on several retrieval tasks and are able to derive algorithms that e. g. achieve a 100% bullseye score on the popular MPEG7 shape retrieval data set.</p><p>3 0.66871065 <a title="91-lsi-3" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>4 0.63704658 <a title="91-lsi-4" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>Author: Feng Zhou, Fernando De_la_Torre</p><p>Abstract: Graph matching (GM) is a fundamental problem in computer science, and it has been successfully applied to many problems in computer vision. Although widely used, existing GM algorithms cannot incorporate global consistence among nodes, which is a natural constraint in computer vision problems. This paper proposes deformable graph matching (DGM), an extension of GM for matching graphs subject to global rigid and non-rigid geometric constraints. The key idea of this work is a new factorization of the pair-wise affinity matrix. This factorization decouples the affinity matrix into the local structure of each graph and the pair-wise affinity edges. Besides the ability to incorporate global geometric transformations, this factorization offers three more benefits. First, there is no need to compute the costly (in space and time) pair-wise affinity matrix. Second, it provides a unified view of many GM methods and extends the standard iterative closest point algorithm. Third, it allows to use the path-following optimization algorithm that leads to improved optimization strategies and matching performance. Experimental results on synthetic and real databases illustrate how DGM outperforms state-of-the-art algorithms for GM. The code is available at http : / / human s en s ing . c s . cmu .edu / fgm.</p><p>5 0.63291663 <a title="91-lsi-5" href="./cvpr-2013-Constraints_as_Features.html">93 cvpr-2013-Constraints as Features</a></p>
<p>Author: Shmuel Asafi, Daniel Cohen-Or</p><p>Abstract: In this paper, we introduce a new approach to constrained clustering which treats the constraints as features. Our method augments the original feature space with additional dimensions, each of which derived from a given Cannot-link constraints. The specified Cannot-link pair gets extreme coordinates values, and the rest of the points get coordinate values that express their spatial influence from the specified constrained pair. After augmenting all the new features, a standard unconstrained clustering algorithm can be performed, like k-means or spectral clustering. We demonstrate the efficacy of our method for active semi-supervised learning applied to image segmentation and compare it to alternative methods. We also evaluate the performance of our method on the four most commonly evaluated datasets from the UCI machine learning repository.</p><p>6 0.57667118 <a title="91-lsi-6" href="./cvpr-2013-Gauging_Association_Patterns_of_Chromosome_Territories_via_Chromatic_Median.html">184 cvpr-2013-Gauging Association Patterns of Chromosome Territories via Chromatic Median</a></p>
<p>7 0.56125164 <a title="91-lsi-7" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>8 0.51957887 <a title="91-lsi-8" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>9 0.51522839 <a title="91-lsi-9" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>10 0.50781411 <a title="91-lsi-10" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>11 0.5031985 <a title="91-lsi-11" href="./cvpr-2013-Graph-Laplacian_PCA%3A_Closed-Form_Solution_and_Robustness.html">191 cvpr-2013-Graph-Laplacian PCA: Closed-Form Solution and Robustness</a></p>
<p>12 0.503093 <a title="91-lsi-12" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>13 0.48333704 <a title="91-lsi-13" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>14 0.47419435 <a title="91-lsi-14" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>15 0.46303147 <a title="91-lsi-15" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>16 0.46136752 <a title="91-lsi-16" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>17 0.45169622 <a title="91-lsi-17" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>18 0.44912559 <a title="91-lsi-18" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>19 0.43839505 <a title="91-lsi-19" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>20 0.43504828 <a title="91-lsi-20" href="./cvpr-2013-MKPLS%3A_Manifold_Kernel_Partial_Least_Squares_for_Lipreading_and_Speaker_Identification.html">276 cvpr-2013-MKPLS: Manifold Kernel Partial Least Squares for Lipreading and Speaker Identification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.193), (10, 0.128), (26, 0.038), (28, 0.011), (33, 0.341), (67, 0.053), (69, 0.068), (87, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95509297 <a title="91-lda-1" href="./cvpr-2013-Monocular_Template-Based_3D_Reconstruction_of_Extensible_Surfaces_with_Local_Linear_Elasticity.html">289 cvpr-2013-Monocular Template-Based 3D Reconstruction of Extensible Surfaces with Local Linear Elasticity</a></p>
<p>Author: Abed Malti, Richard Hartley, Adrien Bartoli, Jae-Hak Kim</p><p>Abstract: We propose a new approach for template-based extensible surface reconstruction from a single view. We extend the method of isometric surface reconstruction and more recent work on conformal surface reconstruction. Our approach relies on the minimization of a proposed stretching energy formalized with respect to the Poisson ratio parameter of the surface. We derive a patch-based formulation of this stretching energy by assuming local linear elasticity. This formulation unifies geometrical and mechanical constraints in a single energy term. We prevent local scale ambiguities by imposing a set of fixed boundary 3D points. We experimentally prove the sufficiency of this set of boundary points and demonstrate the effectiveness of our approach on different developable and non-developable surfaces with a wide range of extensibility.</p><p>2 0.93740869 <a title="91-lda-2" href="./cvpr-2013-A_Fast_Semidefinite_Approach_to_Solving_Binary_Quadratic_Problems.html">9 cvpr-2013-A Fast Semidefinite Approach to Solving Binary Quadratic Problems</a></p>
<p>Author: Peng Wang, Chunhua Shen, Anton van_den_Hengel</p><p>Abstract: Many computer vision problems can be formulated as binary quadratic programs (BQPs). Two classic relaxation methods are widely used for solving BQPs, namely, spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high for large scale problems. We present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. Extensive experiments on various applications including clustering, image segmentation, co-segmentation and registration demonstrate the usefulness of our SDP formulation for solving large-scale BQPs.</p><p>3 0.9262765 <a title="91-lda-3" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>Author: Tony Tung, Takashi Matsuyama</p><p>Abstract: This paper presents a novel approach to characterize deformable surface using intrinsic property dynamics. 3D dynamic surfaces representing humans in motion can be obtained using multiple view stereo reconstruction methods or depth cameras. Nowadays these technologies have become capable to capture surface variations in real-time, and give details such as clothing wrinkles and deformations. Assuming repetitive patterns in the deformations, we propose to model complex surface variations using sets of linear dynamical systems (LDS) where observations across time are given by surface intrinsic properties such as local curvatures. We introduce an approach based on bags of dynamical systems, where each surface feature to be represented in the codebook is modeled by a set of LDS equipped with timing structure. Experiments are performed on datasets of real-world dynamical surfaces and show compelling results for description, classification and segmentation.</p><p>same-paper 4 0.91480863 <a title="91-lda-4" href="./cvpr-2013-Consensus_of_k-NNs_for_Robust_Neighborhood_Selection_on_Graph-Based_Manifolds.html">91 cvpr-2013-Consensus of k-NNs for Robust Neighborhood Selection on Graph-Based Manifolds</a></p>
<p>Author: Vittal Premachandran, Ramakrishna Kakarala</p><p>Abstract: Propagating similarity information along the data manifold requires careful selection of local neighborhood. Selecting a “good” neighborhood in an unsupervised setting, given an affinity graph, has been a difficult task. The most common way to select a local neighborhood has been to use the k-nearest neighborhood (k-NN) selection criterion. However, it has the tendency to include noisy edges. In this paper, we propose a way to select a robust neighborhood using the consensus of multiple rounds of k-NNs. We explain how using consensus information can give better control over neighborhood selection. We also explain in detail the problems with another recently proposed neighborhood selection criteria, i.e., Dominant Neighbors, and show that our method is immune to those problems. Finally, we show the results from experiments in which we compare our method to other neighborhood selection approaches. The results corroborate our claims that consensus ofk-NNs does indeed help in selecting more robust and stable localities.</p><p>5 0.90575397 <a title="91-lda-5" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>6 0.90369499 <a title="91-lda-6" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>7 0.89521796 <a title="91-lda-7" href="./cvpr-2013-Large_Displacement_Optical_Flow_from_Nearest_Neighbor_Fields.html">244 cvpr-2013-Large Displacement Optical Flow from Nearest Neighbor Fields</a></p>
<p>8 0.89028043 <a title="91-lda-8" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>9 0.88951272 <a title="91-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.88905239 <a title="91-lda-10" href="./cvpr-2013-Area_Preserving_Brain_Mapping.html">44 cvpr-2013-Area Preserving Brain Mapping</a></p>
<p>11 0.88805884 <a title="91-lda-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.88671029 <a title="91-lda-12" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>13 0.88652164 <a title="91-lda-13" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>14 0.8864308 <a title="91-lda-14" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>15 0.88642836 <a title="91-lda-15" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>16 0.88638353 <a title="91-lda-16" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>17 0.88636702 <a title="91-lda-17" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>18 0.88596904 <a title="91-lda-18" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>19 0.8855902 <a title="91-lda-19" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>20 0.88550544 <a title="91-lda-20" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
