<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>392 cvpr-2013-Separable Dictionary Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-392" href="#">cvpr2013-392</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>392 cvpr-2013-Separable Dictionary Learning</h1>
<br/><p>Source: <a title="cvpr-2013-392-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hawe_Separable_Dictionary_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>Reference: <a title="cvpr-2013-392-reference" href="../cvpr2013_reference/cvpr-2013-Separable_Dictionary_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 se ibert Abstract Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. [sent-3, score-0.312]
</p><p>2 Dictionaries are either available analytically, or can be learned from a suitable training set. [sent-4, score-0.037]
</p><p>3 While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. [sent-5, score-1.103]
</p><p>4 In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. [sent-6, score-1.485]
</p><p>5 The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. [sent-7, score-1.075]
</p><p>6 On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. [sent-8, score-0.772]
</p><p>7 The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper-  ,  ties such as mutual coherence explicitly during the learning procedure. [sent-9, score-1.772]
</p><p>8 In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD. [sent-10, score-1.082]
</p><p>9 Introduction Exploiting the fact that a signal s ∈ Rn has a sparse represeEnxtpaltoioitni over some dictionary D s ∈∈ R Rn×d is the backrbeosneen oatfi many rsu scomceess dfuicl signal D rec ∈ons Rtruction and data analysis algorithms. [sent-12, score-1.03]
</p><p>10 Having a sparse representation means that s is the linear combination of only a few columns of D, referred to as atoms. [sent-13, score-0.123]
</p><p>11 Formally, this reads as s = Dx, (1) where the transform coefficient vector x ∈ Rd is sparse, iw. [sent-14, score-0.063]
</p><p>12 mreo thste o trfa intssf oernmtri ceos are zero or rsm xall ∈ i nR magnitude. [sent-16, score-0.123]
</p><p>13 de  ,  For the performance of algorithms exploiting this model, it is crucial to find a dictionary that allows the signal of interest to be represented most accurately with a coefficient vector x that is as sparse as possible. [sent-18, score-0.806]
</p><p>14 Basically, dictionaries can be assigned to two classes: analytic dictionaries and learned dictionaries. [sent-19, score-0.896]
</p><p>15 Analytic dictionaries are built on mathematical models of a general type of signal they should represent. [sent-20, score-0.459]
</p><p>16 They can be used universally and allow a fast implementation. [sent-21, score-0.062]
</p><p>17 It is well known that learned dictionaries yield  a sparser representation than analytic ones. [sent-23, score-0.576]
</p><p>18 Given a set of representative training signals, dictionary learning algorithms aim at finding the dictionary over which the training set admits a maximally sparse representation. [sent-24, score-1.493]
</p><p>19 , xm] ∈ Rd×m contain the corresponding m sparse transform co]e ∈ffic Rient vectors, then learning a dictionary can be stated as the minimization problem minXim,Dizeg(X)  subject to  ? [sent-31, score-0.79]
</p><p>20 (2) Therein, g : Rd×m → R is a function that promotes sparsity, ? [sent-35, score-0.033]
</p><p>21 reflects the no→ise R power, nanctdio Cn tish some predefined admissible set of solutions. [sent-36, score-0.036]
</p><p>22 Common dictionary learning approaches employing optimization problems related to (2) include probabilistic ones like [11, 14, 26], and clustering based ones such as K-SVD [3], see [20] for a more comprehensive overview. [sent-37, score-0.748]
</p><p>23 The dictionaries produced by these techniques are unstructured matrices that allow highly sparse representations of the signals of interest. [sent-38, score-0.716]
</p><p>24 However, the dimension of the signals which are sparsely represented and, consequently, the possible dictionaries’ dimensions are inherently restricted by limited memory and limited computational resources. [sent-39, score-0.183]
</p><p>25 Furthermore, when used within signal reconstruction algorithms where many matrix vector multiplications have to be performed, those dictionaries are computationally expensive to apply. [sent-40, score-0.539]
</p><p>26 In this paper, we present a method for learning dictio-  444333668  naries that are efficiently applicable in reconstruction tasks. [sent-41, score-0.108]
</p><p>27 The crucial idea is to allow the dictionary to have a separable structure, where separable means that the dictionary D is given by the Kronecker product of two smaller dictionaries A ∈ Rh×a and B ∈ Rw×b, i. [sent-42, score-2.321]
</p><p>28 (3)  The relation between a signal s ∈ Rhw and its sparse reprTehseen retaltaiotino x ∈tw Reeanb as given i ∈n R(1) is accordingly s = (reBs e⊗nt aAti)oxn = x ve ∈c( RAvec−1(x)B? [sent-45, score-0.17]
</p><p>29 ), where the vector space isomorphism vec : ARav×ebc → Rab is defined as the operation that stacks the columns on top ofeach other. [sent-46, score-0.161]
</p><p>30 Employing this separable structure instead of a full, unstructured dictionary clearly reduces the computational costs of both the learning algorithm and the reconstruction tasks. [sent-47, score-1.218]
</p><p>31 More precisely, for a separation with h, w √∼ the computational burden freodru ac seesp afrroamtio On w(ni)th t oh Ow(√ ∼n). [sent-48, score-0.083]
</p><p>32 We will refer to this new learning approach as SeDiL (Separable Dictionary Learning). [sent-49, score-0.06]
</p><p>33 However, we will focus on signals that have an inherently two dimensional structure such as images. [sent-51, score-0.26]
</p><p>34 However, it is worth mentioning that SeDiL can straightforwardly be extended to signals with higher dimensional structure, such as volumetric 3D-signals, by employ-  √n,  ××  ing multiple Kronecker products. [sent-52, score-0.249]
</p><p>35 To fix the notation for the rest of this work, if A and B are as above, the two dimensional signal S ∈ Rh×w has the sparse representation mXe n∈s iRona×abl, s i g. [sent-53, score-0.242]
</p><p>36 T∈h Re proposed dictionary learning scheme SeDiL is based on an adaption of Problem (2) to a product of unit spheres. [sent-57, score-0.768]
</p><p>37 Furthermore, it incorporates a regularization term that allows to control the dictionary’s mutual coherence. [sent-58, score-0.112]
</p><p>38 For the general separable case, the method is able to learn dictionaries for large patch dimensions where conventional learning techniques fail while if we define B = 1 SeDiL yields a new algorithm for learning standard unstructured dictionaries. [sent-60, score-0.939]
</p><p>39 A denoising experiment is given that shows the performance of both a separable and a non-separable dictionary learned by SeDiL on (8 8)-dimensional image patches. [sent-61, score-0.986]
</p><p>40 From this experiment (it8 can b)e-d seen tshioatn tahle i separable dictionary outperforms eintst analytic counterpart, the overcomplete discrete cosine transform, and the non-separable one achieves similar performance as state-of-the-art learning methods like K-SVD. [sent-62, score-1.199]
</p><p>41 Besides that, to show that a learned separable dictionary is able to extract and to recover the global information contained in the training data, a separable dictionary is learned on a face database with each face image having a resolution of 64 64 pixels. [sent-63, score-2.001]
</p><p>42 This dictionary is then applied in a face inpainting experiment dwichetiroen large missing regions are re-  covered solely based on the information contained in the dictionary. [sent-64, score-0.665]
</p><p>43 Structured Dictionary Learning Instead of learning dense unstructured dictionaries, which are costly to apply in reconstruction tasks and are unable to deal with high dimensional signals, techniques exist that aim at learning dictionaries which bypass these limitations. [sent-66, score-0.807]
</p><p>44 In the following, we shortly review some existing techniques that focus on learning efficiently applicable and high dimensional dictionaries, followed by introducing our approach. [sent-67, score-0.194]
</p><p>45 Related Work In [17] and [24], two different algorithms have been proposed following the same idea of finding a dictionary such that the atoms themselves are sparse over some fixed analytic base dictionary. [sent-70, score-0.907]
</p><p>46 The algorithm proposed in [17] enforces each atom to have a fixed number of non-zero coefficients, while the one suggested in [24] imposes a less restrictive constraint by enforcing sparsity over the entire dictionary. [sent-71, score-0.173]
</p><p>47 However, both algorithms employ optimization problems that are not capable of finding a large dictionary for high dimensional signals. [sent-72, score-0.708]
</p><p>48 In [2] an alternative structure for dictionaries has been proposed. [sent-73, score-0.383]
</p><p>49 The so called signature dictionary is a small image itself, where every patch at varying locations and size is a possible dictionary atom. [sent-74, score-1.272]
</p><p>50 The advantages of this structure include near-translation-  invariance, reduced overfitting, and less memory and computational requirements, compared to unstructured dictionary approaches. [sent-75, score-0.825]
</p><p>51 However, the small number of parameters in this model also makes this dictionary more restrictive than other structures. [sent-76, score-0.687]
</p><p>52 Hierarchical frameworks for tackling high dimensional dictionary learning are presented in [13] and [23]. [sent-78, score-0.796]
</p><p>53 The latter work uses this framework in conjunction with a screening technique and random projections. [sent-79, score-0.043]
</p><p>54 Proposed Approach We aim at learning a separable dictionary D = B ⊗ A fromW a given s leeta ronfi training samples cSti =n (rSy1 D , . [sent-83, score-1.038]
</p><p>55 We den)ot ∈e the collection of the m sparse representations by X = t(hXe1 , . [sent-87, score-0.061]
</p><p>56 , Xtiomn) aonfd t measure airtss eov reerparlel sparsity vsia b ? [sent-90, score-0.091]
</p><p>57 1  (4)  where xklj is the (k, l)-entry of Xj ∈ Ra×b and ρ > 0 is a weighting factor. [sent-102, score-0.097]
</p><p>58 We impose the following regularization 444333779  on the dictionary. [sent-103, score-0.03]
</p><p>59 Constraint (i) is commonly employed in various dictionary learning procedures to avoid the scale ambiguity problem, i. [sent-106, score-0.696]
</p><p>60 the entries of D tend to infinity, while the entries of X ti. [sent-108, score-0.098]
</p><p>61 d t htoe zero as tfh Dis ties nthde t global my,iwn ihmilizee thr eo fe nthtrei unconstrained sparsity measure g(X). [sent-110, score-0.168]
</p><p>62 Matrices with normalized csotrlauinmends sapdamrsiitt a mmeaansifuoreld g structure, rkicnoewsn w as tnhoer product of spheres, which we denote by S(n, d) := {D ∈ Rn×d| ddiag(D? [sent-111, score-0.084]
</p><p>63 (5)  Here, ddiag(Z) forms a diagonal matrix with the diagonal entries of the square matrix Z, and Id is the (d d)-identity matrix. [sent-113, score-0.049]
</p><p>64 Consequently, we require thaits tAh ei s( an edl)e-mideenntt toyf S(h, a) and that B is an element of S(w, b). [sent-114, score-0.04]
</p><p>65 The soft constraint (ii) of requiring a moderate mutual coherence of the dictionary is a well known regularization procedure in dictionary learning, and is motivated by the compressive sensing theory. [sent-115, score-1.487]
</p><p>66 Roughly speaking, the mutual coherence of D measures the similarity between the dictionary’s atoms, or, ”a value that exposes the dictionary’s vulnerability, as [. [sent-116, score-0.222]
</p><p>67 ] two closely related columns may confuse any pursuit technique. [sent-119, score-0.091]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dictionary', 0.636), ('dictionaries', 0.35), ('separable', 0.313), ('sedil', 0.242), ('analytic', 0.159), ('unstructured', 0.128), ('signals', 0.117), ('signal', 0.109), ('coherence', 0.103), ('ddiag', 0.097), ('xklj', 0.097), ('mutual', 0.082), ('nchen', 0.079), ('dimensional', 0.072), ('admits', 0.071), ('spheres', 0.064), ('columns', 0.062), ('sparse', 0.061), ('learning', 0.06), ('kronecker', 0.056), ('sparsity', 0.055), ('rn', 0.054), ('ties', 0.053), ('burden', 0.053), ('employing', 0.052), ('restrictive', 0.051), ('atoms', 0.051), ('rh', 0.05), ('entries', 0.049), ('reconstruction', 0.048), ('screening', 0.043), ('edl', 0.043), ('isomorphism', 0.043), ('ceos', 0.043), ('mxe', 0.043), ('ibert', 0.043), ('seibert', 0.043), ('oatfi', 0.043), ('aati', 0.043), ('tnhoer', 0.043), ('product', 0.041), ('rsm', 0.04), ('rsu', 0.04), ('toyf', 0.04), ('technische', 0.04), ('xall', 0.04), ('competes', 0.04), ('rd', 0.04), ('inherently', 0.038), ('matthias', 0.037), ('exposes', 0.037), ('kle', 0.037), ('learned', 0.037), ('enforces', 0.037), ('dx', 0.037), ('tum', 0.036), ('aonfd', 0.036), ('admissible', 0.036), ('ons', 0.034), ('shortly', 0.034), ('therein', 0.034), ('structure', 0.033), ('id', 0.033), ('mw', 0.033), ('permit', 0.033), ('promotes', 0.033), ('overcoming', 0.033), ('transform', 0.033), ('allow', 0.032), ('thr', 0.032), ('bypass', 0.032), ('multiplications', 0.032), ('oxn', 0.032), ('straightforwardly', 0.032), ('rec', 0.032), ('asl', 0.032), ('tahle', 0.031), ('wavelets', 0.031), ('adaption', 0.031), ('regularization', 0.03), ('sparser', 0.03), ('universit', 0.03), ('oh', 0.03), ('reads', 0.03), ('atom', 0.03), ('universally', 0.03), ('aim', 0.029), ('contained', 0.029), ('formally', 0.029), ('confuse', 0.029), ('consequently', 0.029), ('memory', 0.028), ('techniques', 0.028), ('ofeach', 0.028), ('infinity', 0.028), ('stacks', 0.028), ('mentioning', 0.028), ('tackling', 0.028), ('permits', 0.028), ('nthde', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="392-tfidf-1" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>2 0.56104934 <a title="392-tfidf-2" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>3 0.52426177 <a title="392-tfidf-3" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>4 0.45885617 <a title="392-tfidf-4" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>5 0.43988907 <a title="392-tfidf-5" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>6 0.4321999 <a title="392-tfidf-6" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>7 0.41087735 <a title="392-tfidf-7" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>8 0.32042238 <a title="392-tfidf-8" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>9 0.30465424 <a title="392-tfidf-9" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>10 0.26848182 <a title="392-tfidf-10" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>11 0.22691381 <a title="392-tfidf-11" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>12 0.22611804 <a title="392-tfidf-12" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>13 0.21830429 <a title="392-tfidf-13" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>14 0.19560589 <a title="392-tfidf-14" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>15 0.1685847 <a title="392-tfidf-15" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>16 0.160228 <a title="392-tfidf-16" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>17 0.09574458 <a title="392-tfidf-17" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>18 0.092412375 <a title="392-tfidf-18" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>19 0.084510237 <a title="392-tfidf-19" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>20 0.082339115 <a title="392-tfidf-20" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, -0.207), (2, -0.367), (3, 0.429), (4, -0.175), (5, -0.177), (6, 0.155), (7, 0.178), (8, -0.033), (9, 0.118), (10, 0.014), (11, 0.091), (12, 0.009), (13, 0.034), (14, 0.046), (15, 0.018), (16, 0.038), (17, 0.055), (18, -0.026), (19, 0.014), (20, 0.001), (21, 0.039), (22, 0.001), (23, 0.046), (24, -0.01), (25, -0.034), (26, -0.029), (27, -0.034), (28, -0.067), (29, 0.031), (30, 0.017), (31, -0.024), (32, 0.002), (33, -0.013), (34, 0.025), (35, 0.007), (36, 0.056), (37, -0.012), (38, -0.026), (39, 0.045), (40, 0.052), (41, 0.025), (42, 0.001), (43, -0.0), (44, -0.018), (45, -0.002), (46, 0.006), (47, -0.005), (48, 0.021), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98507303 <a title="392-lsi-1" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>2 0.94846356 <a title="392-lsi-2" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>3 0.92626244 <a title="392-lsi-3" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>4 0.91135389 <a title="392-lsi-4" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>Author: Cewu Lu, Jiaping Shi, Jiaya Jia</p><p>Abstract: Online dictionary learning is particularly useful for processing large-scale and dynamic data in computer vision. It, however, faces the major difficulty to incorporate robust functions, rather than the square data fitting term, to handle outliers in training data. In thispaper, wepropose a new online framework enabling the use of ?1 sparse data fitting term in robust dictionary learning, notably enhancing the usability and practicality of this important technique. Extensive experiments have been carried out to validate our new framework.</p><p>5 0.87284487 <a title="392-lsi-5" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>6 0.85741901 <a title="392-lsi-6" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>7 0.79586047 <a title="392-lsi-7" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>8 0.78627843 <a title="392-lsi-8" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>9 0.67495584 <a title="392-lsi-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.6517874 <a title="392-lsi-10" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>11 0.59764469 <a title="392-lsi-11" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>12 0.57072324 <a title="392-lsi-12" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>13 0.52565086 <a title="392-lsi-13" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>14 0.47068882 <a title="392-lsi-14" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>15 0.43394029 <a title="392-lsi-15" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>16 0.42219713 <a title="392-lsi-16" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>17 0.40299171 <a title="392-lsi-17" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>18 0.35455993 <a title="392-lsi-18" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>19 0.3209683 <a title="392-lsi-19" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>20 0.31962684 <a title="392-lsi-20" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.071), (26, 0.027), (33, 0.241), (67, 0.03), (69, 0.479), (87, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86278683 <a title="392-lda-1" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>Author: Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: 3D volumetric reasoning is important for truly understanding a scene. Humans are able to both segment each object in an image, and perceive a rich 3D interpretation of the scene, e.g., the space an object occupies, which objects support other objects, and which objects would, if moved, cause other objects to fall. We propose a new approach for parsing RGB-D images using 3D block units for volumetric reasoning. The algorithm fits image segments with 3D blocks, and iteratively evaluates the scene based on block interaction properties. We produce a 3D representation of the scene based on jointly optimizing over segmentations, block fitting, supporting relations, and object stability. Our algorithm incorporates the intuition that a good 3D representation of the scene is the one that fits the data well, and is a stable, self-supporting (i.e., one that does not topple) arrangement of objects. We experiment on several datasets including controlled and real indoor scenarios. Results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation.</p><p>2 0.83320779 <a title="392-lda-2" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>Author: Ruonan Li, Parker Porfilio, Todd Zickler</p><p>Abstract: We consider the problem of finding distinctive social interactions involving groups of agents embedded in larger social gatherings. Given a pre-defined gallery of short exemplar interaction videos, and a long input video of a large gathering (with approximately-tracked agents), we identify within the gathering small sub-groups of agents exhibiting social interactions that resemble those in the exemplars. The participants of each detected group interaction are localized in space; the extent of their interaction is localized in time; and when the gallery ofexemplars is annotated with group-interaction categories, each detected interaction is classified into one of the pre-defined categories. Our approach represents group behaviors by dichotomous collections of descriptors for (a) individual actions, and (b) pairwise interactions; and it includes efficient algorithms for optimally distinguishing participants from by-standers in every temporal unit and for temporally localizing the extent of the group interaction. Most importantly, the method is generic and can be applied whenever numerous interacting agents can be approximately tracked over time. We evaluate the approach using three different video collections, two that involve humans and one that involves mice.</p><p>3 0.83121264 <a title="392-lda-3" href="./cvpr-2013-Discriminative_Subspace_Clustering.html">135 cvpr-2013-Discriminative Subspace Clustering</a></p>
<p>Author: Vasileios Zografos, Liam Ellis, Rudolf Mester</p><p>Abstract: We present a novel method for clustering data drawn from a union of arbitrary dimensional subspaces, called Discriminative Subspace Clustering (DiSC). DiSC solves the subspace clustering problem by using a quadratic classifier trained from unlabeled data (clustering by classification). We generate labels by exploiting the locality of points from the same subspace and a basic affinity criterion. A number of classifiers are then diversely trained from different partitions of the data, and their results are combined together in an ensemble, in order to obtain the final clustering result. We have tested our method with 4 challenging datasets and compared against 8 state-of-the-art methods from literature. Our results show that DiSC is a very strong performer in both accuracy and robustness, and also of low computational complexity.</p><p>4 0.82832927 <a title="392-lda-4" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>5 0.80581772 <a title="392-lda-5" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>Author: Fuxin Li, Joao Carreira, Guy Lebanon, Cristian Sminchisescu</p><p>Abstract: In this paper we present an inference procedure for the semantic segmentation of images. Differentfrom many CRF approaches that rely on dependencies modeled with unary and pairwise pixel or superpixel potentials, our method is entirely based on estimates of the overlap between each of a set of mid-level object segmentation proposals and the objects present in the image. We define continuous latent variables on superpixels obtained by multiple intersections of segments, then output the optimal segments from the inferred superpixel statistics. The algorithm is capable of recombine and refine initial mid-level proposals, as well as handle multiple interacting objects, even from the same class, all in a consistent joint inference framework by maximizing the composite likelihood of the underlying statistical model using an EM algorithm. In the PASCAL VOC segmentation challenge, the proposed approach obtains high accuracy and successfully handles images of complex object interactions.</p><p>same-paper 6 0.79480433 <a title="392-lda-6" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>7 0.79031169 <a title="392-lda-7" href="./cvpr-2013-Joint_Detection%2C_Tracking_and_Mapping_by_Semantic_Bundle_Adjustment.html">231 cvpr-2013-Joint Detection, Tracking and Mapping by Semantic Bundle Adjustment</a></p>
<p>8 0.74682713 <a title="392-lda-8" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>9 0.69482481 <a title="392-lda-9" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>10 0.65315706 <a title="392-lda-10" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>11 0.63025188 <a title="392-lda-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.61998624 <a title="392-lda-12" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>13 0.61676466 <a title="392-lda-13" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>14 0.61660612 <a title="392-lda-14" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>15 0.61328709 <a title="392-lda-15" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>16 0.61313754 <a title="392-lda-16" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>17 0.61200362 <a title="392-lda-17" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>18 0.60870284 <a title="392-lda-18" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>19 0.60761142 <a title="392-lda-19" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>20 0.60547554 <a title="392-lda-20" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
