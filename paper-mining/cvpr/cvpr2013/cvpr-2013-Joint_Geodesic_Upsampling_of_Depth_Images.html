<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-232" href="#">cvpr2013-232</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</h1>
<br/><p>Source: <a title="cvpr-2013-232-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Liu_Joint_Geodesic_Upsampling_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ming-Yu Liu, Oncel Tuzel, Yuichi Taguchi</p><p>Abstract: We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. Specifically, it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. Though this is closely related to the all-pairshortest-path problem which has O(n2 log n) complexity, we develop a novel approximation algorithm whose complexity grows linearly with the image size and achieve realtime performance. We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. In addition, we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps, an important sensor fusion application.</p><p>Reference: <a title="cvpr-2013-232-reference" href="../cvpr2013_reference/cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. [sent-2, score-1.291]
</p><p>2 Specifically, it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. [sent-3, score-1.377]
</p><p>3 We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. [sent-5, score-1.035]
</p><p>4 In addition, we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps, an important sensor fusion application. [sent-6, score-1.103]
</p><p>5 However, unlike the conventional optical camera, the resolution of depth sensors advances at a much slower pace. [sent-10, score-0.56]
</p><p>6 While the resolution of mainstream optical cameras is in the order of 10 megapixels, the resolution of mainstream time-of-flight depth sensors is still lower than 0. [sent-12, score-0.776]
</p><p>7 One way to improve the resolution of depth images is to use a high resolution optical camera in tandem with the depth sensor. [sent-14, score-1.068]
</p><p>8 In general, geometric and color boundaries of a scene are correlated: abrupt depth transition often leads to abrupt color transition. [sent-15, score-0.541]
</p><p>9 In this paper, we study depth image upsampling problem using registered color images and propose a new algorithm based on geodesic curves in joint color and depth images. [sent-17, score-1.977]
</p><p>10 Our algorithm is inspired by the joint bilateral upsampling algorithm [11] (current state of the art in terms of both accuracy and efficiency), which interpolates low resolution depths on the high resolution grid based on a set of weights computed as multiplications of spatial and color kernels. [sent-18, score-1.423]
</p><p>11 These kernels utilize Euclidean distance to quantify the dissimilarities of the pixels and the word “joint“ is due to utilization of two channels: optical image for computing color distance and depth image for computing spatial distance. [sent-19, score-0.569]
</p><p>12 We argue that using two separate kernels causes blurry depth boundaries and depth bleeding artifacts par-  ticularly when the colors of the surfaces across the depth boundaries are similar. [sent-20, score-1.243]
</p><p>13 We compute geodesic distances—lengths of the geodesic curves—from each pixel in the target high resolution depth image to all the pixels whose depths are known from the low resolution depth image. [sent-23, score-2.072]
</p><p>14 These distances are used to propagate the known depths to the high resolution grid in a smooth and depth discontinuity preserving manner. [sent-24, score-0.852]
</p><p>15 Since the geodesic distance integrates joint color and spatial changes along the curve, it is sensitive to thin contours around the surfaces, providing sharp depth boundaries even when the color difference between two sides of a contour is subtle. [sent-25, score-1.117]
</p><p>16 In addition, the geodesic path can follow thin segments with uniform colors and therefore produce high quality depth images with fine details. [sent-26, score-0.896]
</p><p>17 An example motivating the use of geodesic distances for depth upsampling is illustrated in Figure 1 (see Section 2 for further details). [sent-27, score-1.453]
</p><p>18 For real-time processing, we propose a new approximation algorithm for simultaneously finding K nearest (in geodesic sense) nodes from each source node and show that its complexity grows linearly with the image size and K. [sent-29, score-0.542]
</p><p>19 The result shows that the  proposed algorithm produces more accurate high resolution depth images for both smooth surfaces and boundary re111666999  gions. [sent-31, score-0.666]
</p><p>20 We also show that our method is well suited for upsampling depth images using binary edge maps (e. [sent-32, score-1.075]
</p><p>21 Related Work Depth upsampling methods can be categorized as global or local. [sent-37, score-0.661]
</p><p>22 Global methods [6, 14, 19] formulate depth upsampling as an optimization problem where a large cost is induced if two neighboring pixels having a similar color are assigned two very different depths. [sent-38, score-1.121]
</p><p>23 Among them, joint bilateral upsampling is particularly popular, which uses bilateral filtering in a joint color-spatial space. [sent-41, score-1.057]
</p><p>24 The proposed algorithm is also based on filtering where the upsampling filter weights are determined by geodesic distances. [sent-42, score-1.113]
</p><p>25 Here, we define joint geodesic filtering in a color-spatial space and show its application for depth upsampling. [sent-44, score-0.868]
</p><p>26 The fast marching algorithm [23] and  the geodesic distance transform [18] are two common implementations for geodesic distance computation, both of which have a linear time complexity. [sent-45, score-0.979]
</p><p>27 We derive a novel approximation algorithm for simultaneously finding K nearest nodes from each source node based on geodesic distance transforms and achieve real-time performance. [sent-47, score-0.558]
</p><p>28 Recently, [8] proposed using geodesic distances to compute Voronoi cells for image tessellation, which are used for fitting planes to sparse depth measurements for depth interpolation. [sent-48, score-1.145]
</p><p>29 • We propose a new joint filtering algorithm using geodesic odsiestan ac nese wfor j upsampling a depth image using a registered high resolution color image. [sent-53, score-1.796]
</p><p>30 • We develop a fast optimization technique for finding approximate sKt noeptairmesitz antioodnes t ebcahsendiq on geodesic distance and achieving real-time upsampling performance. [sent-54, score-1.103]
</p><p>31 In Section 2, we present the depth upsampling formulation. [sent-57, score-1.014]
</p><p>32 Joint Geodesic Upsampling Let D↓ and I the low resolution depth and high resobe lution optical images, respectively, where the resolution of I r times larger than that of D↓. [sent-62, score-0.743]
</p><p>33 Our goal is to construct a high resolution depth image D whose resolution is equal to that of I. [sent-65, score-0.682]
</p><p>34 Depths of pixels in a sparse grid in D are known (which we refer to as seed pixels) from the corresponding low resolution depth image D↓. [sent-66, score-0.771]
</p><p>35 We propose computing the affinity measure between two pixels using geodesic distances defined on the image grid1, which can be considered as a two dimensional embedding in a joint color-spatial space. [sent-72, score-0.583]
</p><p>36 Comparison between joint bilateral upsampling and joint geodesic upsampling. [sent-87, score-1.302]
</p><p>37 The triangles indicate the locations where depth measurements are collected using a low resolution depth sensor. [sent-91, score-0.888]
</p><p>38 (c) The upsampled depth profile using joint bilateral upsampling. [sent-93, score-0.608]
</p><p>39 (d) The upsampled depth profile using the proposed joint geodesic upsampling algorithm. [sent-94, score-1.562]
</p><p>40 The proposed method integrates color changes along the geodesic path and accurately recovers high resolution depth profile, whereas joint bilateral upsampling smooths depths across occlusion boundary resulting in blurring effect. [sent-95, score-2.021]
</p><p>41 We use the Gaussian kernel to convert the geodesic distance into the affinity measure:  gG(x,y) = exp(−d2G2σ(x2,y))  (4)  where σ is the kernel bandwidth parameter. [sent-97, score-0.494]
</p><p>42 Figure 1 shows a 1D illustration comparing joint bilateral upsampling and joint geodesic upsampling. [sent-98, score-1.302]
</p><p>43 Joint geodesic upsampling integrates color changes along the geodesic curves; therefore it is sensitive to thin structures and fine scale changes, producing smooth surfaces with sharp occlusion boundaries (Figure 1d). [sent-100, score-1.766]
</p><p>44 In contrast, the joint bilateral upsampling algorithm incorrectly propagates depths across the depth boundary due to Euclidean color distance computation (Figure 1c). [sent-101, score-1.425]
</p><p>45 Fast Geodesic Upsampling Computation The upsampling formulation in (1) requires  computa-  tion of shortest paths from each pixel to all the seed pixels, which is equivalent to the all-pair-shortest-path problem. [sent-103, score-0.943]
</p><p>46 Here we derive an approximate formulation of the upsampling operation and present an O(Kn) algorithm which achieves real-time performance. [sent-106, score-0.661]
</p><p>47 First, we make the assumption that to compute the depth of a pixel it is sufficient to propagate information from its K “nearest“ depth pixels. [sent-110, score-0.743]
</p><p>48 TKh(ex )K b-ene thaeres set approximation teos geodesic upsampling (e4l) xis. [sent-113, score-1.132]
</p><p>49 K(x) Our second assumption is that if the two seed pixels are spatially far away, they are unlikely to be simultaneously in the y↓  set of K nearest depth pixels of a given pixel. [sent-117, score-0.643]
</p><p>50 Our algorithm consists of three major processing steps, as summarized in Figure 2: 1) we demultiplex the pixels into K channels, 2) for each channel we compute geodesic distance transform, and 3) we interpolate the depths according to computed geodesic distances. [sent-119, score-1.098]
</p><p>51 1) Demultiplexing: For an input low resolution depth image, we first partition its pixels into K separate channels which is called demultiplexing. [sent-121, score-0.618]
</p><p>52 2) Geodesic Distance Transform (GDT): For each channel, we compute the geodesic distance transform [18], which provides the shortest distance from each pixel to the nearest seed point. [sent-127, score-0.797]
</p><p>53 2) For each channel, we compute the geodesic distance transform, which provides the shortest distance from each pixel to the nearest seed pixel. [sent-131, score-0.749]
</p><p>54 3) The depths of the seed pixels are propagated to the high resolution grid using the computed distances. [sent-132, score-0.519]
</p><p>55 The geodesic distance transform solves the following optimization problem  Mk(x) =y m∈SinkdG(x,y)  (6)  where dG (x, y) is defined in (3). [sent-134, score-0.49]
</p><p>56 3) Interpolation: After computing geodesic distance transform for each channel, we propagate the sparse depths given by the low resolution depth image to the high resolution grid using the computed distances. [sent-146, score-1.359]
</p><p>57 The geodesic distance transform not only provides the geodesic distance but also the nearest seed coordinate and hence its depth. [sent-147, score-1.11]
</p><p>58 Let Mk be the geodesic distance transform for channel k, where the distance from a pixel x to its nearest seed point in channel k is given by Mk (x) and its coordinate is given by y↓k (x). [sent-148, score-0.849]
</p><p>59 The approximate geodesic upsampling is then given by  D(x) =k? [sent-149, score-1.065]
</p><p>60 The shortest paths are defined on the high resolution grid which is sparsely covered by the seed pixels, and a path can reach a spatially distant seed pixel if the pixels along the path have similar colors. [sent-152, score-0.761]
</p><p>61 We also show a new upsampling application using binary edge maps. [sent-157, score-0.682]
</p><p>62 The DISC metric measures the error rate in the depth discontinuity region, while SRMS  6 scenes, namely art, books, dolls, laundry, moebius, and reindeer. [sent-164, score-0.492]
</p><p>63 We generate the low resolution depth images by downsampling the original ones with the downsampling rate varying from 2x to 16x. [sent-166, score-0.583]
</p><p>64 The task is to upsample the low resolution depth image to the original resolution using the registered high resolution color image. [sent-167, score-1.006]
</p><p>65 Metrics: We use three performance metrics for evaluation: 1) the error rate on the depth discontinuity regions (DISC), 2) the root-mean-square error (RMS), and 3) the root-mean-square error in the smooth region (SRMS). [sent-168, score-0.558]
</p><p>66 DISC measures the reconstruction error in the depth discontinuity regions, a standard metric for benchmarking stereo reconstruction algorithms [16]. [sent-169, score-0.514]
</p><p>67 We first extract depth edges in the ground truth depth image. [sent-170, score-0.706]
</p><p>68 We compare the upsampled depth map to the ground truth only in the extracted discontinuity regions. [sent-173, score-0.503]
</p><p>69 RMS is commonly used for comparing depth upsampling algorithms [11, 14]. [sent-176, score-1.014]
</p><p>70 However, we argue that RMS favors blurry depth boundaries, which produce major artifacts for depth upsampling. [sent-177, score-0.746]
</p><p>71 Square error magnifies large few pixel errors, which are commonly generated by algorithms producing sharp depth boundaries but minimized by blurry edges. [sent-178, score-0.528]
</p><p>72 Therefore, this metric is less suited for evaluation of upsampling performances. [sent-179, score-0.723]
</p><p>73 SRMS is a modified version ofRMS error where the error is computed only in the smooth region given by the complement of the extracted depth discontinuity region. [sent-180, score-0.533]
</p><p>74 This metric ensures that error in smooth regions is not dominated by few boundary pixels having large errors and is better suited for measuring upsampling accuracy in the smooth region. [sent-181, score-0.92]
</p><p>75 Algorithms: We compare the proposed algorithm with several filtering-based depth upsampling algorithms, 3Depths  are  given by disparities in the dataset. [sent-182, score-1.014]
</p><p>76 bilinear interpolation (BL), joint bilateral upsampling (JBU) [11], non-local means filtering (NLM) [3], minimalspanning-tree based cost aggregation (MST) [20], and guided image filtering (GIF) [9]. [sent-183, score-0.98]
</p><p>77 JBU (discussed in Section 2) is a popular choice for depth upsampling [4, 7]. [sent-186, score-1.014]
</p><p>78 We note that real-time upsampling  performance can be achieved by using the fast implementations described in [21, 1]. [sent-187, score-0.683]
</p><p>79 NLM was used as a system component for a recent depth upsampling work [14], which transfers depth information from similarly-colored patches. [sent-191, score-1.367]
</p><p>80 For depth upsampling, a linear regression function from the color image to the depth image is estimated for each low resolution patch, which is then used to transfer higher resolution color image to the output depth image. [sent-194, score-1.497]
</p><p>81 We modify the algorithm for depth upsampling by prorogating depths based on the distances on the spanning tree. [sent-198, score-1.157]
</p><p>82 QUAD was proposed for colorization of gray-scale images [12] and recently modified for depth upsampling [19]. [sent-199, score-1.066]
</p><p>83 Upsampling is formulated as a quadratic optimization problem where the cost function enforces color and depth correlation subject to the linear constraints given by the low resolution depth image. [sent-200, score-0.939]
</p><p>84 Visual comparison of the depth upsampling results at 8x upsampling rate. [sent-205, score-1.675]
</p><p>85 The results were averaged over all the images in the dataset at different upsampling rates. [sent-215, score-0.661]
</p><p>86 It can be seen that the proposed algorithm provides better depth recovery in the smooth regions at 4x, 8x, and 16x upsampling rates. [sent-225, score-1.052]
</p><p>87 The results obtained by the JBU algorithm tends to include depth bleeding artifacts especially when the upsampling rate is large. [sent-230, score-1.04]
</p><p>88 Existence of a thin edge in-between has no effect on the upsampling process. [sent-232, score-0.745]
</p><p>89 Our algorithm uses geodesic paths for interpolation and is free from this drawback. [sent-233, score-0.491]
</p><p>90 Approximation and Computation Analysis Our optimization scheme produces an approximation to the geodesic upsampling operation as explained in Section 3. [sent-302, score-1.162]
</p><p>91 Table 2 reports the processing time for a 695 555 image aatb l8ex 2 upsampling rpartoec. [sent-306, score-0.661]
</p><p>92 s Wsien compare rth are 6e9 5v×ar5ia5n5ts i mofthe geodesic upsampling algorithm: 1) exact, 2) approximation using multi-pass geodesic distance transform, and 3) approximation using two-pass geodesic distance trans-  Table 2. [sent-307, score-2.083]
</p><p>93 Depth Upsampling Using Binary Edge Maps In the last experiment, we show that our method is well suited for upsampling depth images using binary images and present an application for sensor fusion. [sent-324, score-1.082]
</p><p>94 Specifically, we upsample a depth image from a low resolution depth sensor with a high resolution depth boundary map from a multi-flash camera (MFC)4 as shown in Figure 6. [sent-325, score-1.534]
</p><p>95 This forces only those  depth pixels in the same depth continuous region are used to compute the depths of pixels in the region, achieving boundary-confined upsampling. [sent-328, score-0.926]
</p><p>96 Several advantages exist for upsampling using a depth 4MFC is a camera design that exploits illumination pattern change due to LED flashes from different directions to compute a depth boundary map. [sent-330, score-1.407]
</p><p>97 Depth upsampling using binary edge (c) Input  high resolution depth boundary  map  maps  given by multi-flash  from multi-flash  camera. [sent-333, score-1.25]
</p><p>98 (d) 16x upsampled depth image using joint bilateral upsampling  (e) 16x upsampled depth image using the proposed algorithm. [sent-337, score-1.657]
</p><p>99 Conclusion  We presented a new joint filtering algorithm using geodesic distances for upsampling depth images using high resolution color images. [sent-344, score-1.79]
</p><p>100 We developed an efficient approximation to the upsampling filter and achieved real-time performance. [sent-345, score-0.728]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('upsampling', 0.661), ('geodesic', 0.404), ('depth', 0.353), ('resolution', 0.154), ('seed', 0.129), ('bilateral', 0.111), ('depths', 0.108), ('jbu', 0.106), ('discontinuity', 0.092), ('srms', 0.088), ('quad', 0.087), ('disc', 0.082), ('rms', 0.069), ('approximation', 0.067), ('thin', 0.063), ('joint', 0.063), ('paths', 0.062), ('upsampled', 0.058), ('pixels', 0.056), ('shortest', 0.054), ('demultiplexing', 0.053), ('channel', 0.053), ('colorization', 0.052), ('grid', 0.051), ('color', 0.051), ('upsample', 0.05), ('nearest', 0.049), ('transaction', 0.048), ('filtering', 0.048), ('transform', 0.048), ('mk', 0.046), ('boundaries', 0.044), ('xp', 0.044), ('backward', 0.042), ('registered', 0.041), ('nlm', 0.041), ('mfc', 0.041), ('blurry', 0.04), ('suited', 0.04), ('boundary', 0.04), ('smooth', 0.038), ('dg', 0.038), ('distance', 0.038), ('pass', 0.038), ('pixel', 0.037), ('traverses', 0.036), ('backsurfacefrontsurface', 0.035), ('demultiplex', 0.035), ('dolls', 0.035), ('oncel', 0.035), ('distances', 0.035), ('path', 0.034), ('exact', 0.034), ('optical', 0.033), ('middlebury', 0.031), ('gif', 0.031), ('mainstream', 0.031), ('megapixels', 0.031), ('produces', 0.03), ('surfaces', 0.03), ('sharp', 0.029), ('sharpest', 0.029), ('sensor', 0.028), ('low', 0.028), ('sensitivity', 0.027), ('baek', 0.027), ('yatziv', 0.027), ('bandwidth', 0.027), ('channels', 0.027), ('forward', 0.026), ('bleeding', 0.026), ('taguchi', 0.025), ('marching', 0.025), ('error', 0.025), ('interpolation', 0.025), ('affinity', 0.025), ('merl', 0.024), ('rounded', 0.024), ('downsampling', 0.024), ('aggregation', 0.024), ('profile', 0.023), ('mst', 0.023), ('grows', 0.022), ('stereo', 0.022), ('metric', 0.022), ('implementations', 0.022), ('joining', 0.021), ('lischinski', 0.021), ('abrupt', 0.021), ('fine', 0.021), ('integrates', 0.021), ('edge', 0.021), ('high', 0.021), ('neighbors', 0.021), ('state', 0.021), ('door', 0.021), ('industrial', 0.02), ('band', 0.02), ('sensors', 0.02), ('shaped', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="232-tfidf-1" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>Author: Ming-Yu Liu, Oncel Tuzel, Yuichi Taguchi</p><p>Abstract: We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. Specifically, it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. Though this is closely related to the all-pairshortest-path problem which has O(n2 log n) complexity, we develop a novel approximation algorithm whose complexity grows linearly with the image size and achieve realtime performance. We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. In addition, we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps, an important sensor fusion application.</p><p>2 0.31616768 <a title="232-tfidf-2" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>3 0.18368454 <a title="232-tfidf-3" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>Author: unkown-author</p><p>Abstract: We tackle the problem of jointly increasing the spatial resolution and apparent measurement accuracy of an input low-resolution, noisy, and perhaps heavily quantized depth map. In stark contrast to earlier work, we make no use of ancillary data like a color image at the target resolution, multiple aligned depth maps, or a database of highresolution depth exemplars. Instead, we proceed by identifying and merging patch correspondences within the input depth map itself, exploiting patchwise scene self-similarity across depth such as repetition of geometric primitives or object symmetry. While the notion of ‘single-image ’ super resolution has successfully been applied in the context of color and intensity images, we are to our knowledge the first to present a tailored analogue for depth images. Rather than reason in terms of patches of 2D pixels as others have before us, our key contribution is to proceed by reasoning in terms of patches of 3D points, with matched patch pairs related by a respective 6 DoF rigid body motion in 3D. In support of obtaining a dense correspondence field in reasonable time, we introduce a new 3D variant of Patch- Match. A third contribution is a simple, yet effective patch upscaling and merging technique, which predicts sharp object boundaries at the target resolution. We show that our results are highly competitive with those of alternative techniques leveraging even a color image at the target resolution or a database of high-resolution depth exemplars.</p><p>4 0.18273744 <a title="232-tfidf-4" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>Author: Peter Kontschieder, Pushmeet Kohli, Jamie Shotton, Antonio Criminisi</p><p>Abstract: Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both stateof-the-art forest models and the conventional pairwise CRF.</p><p>5 0.18223678 <a title="232-tfidf-5" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: In this paper, we propose a convex optimization framework for simultaneous estimation of super-resolved depth map and images from a single moving camera. The pixel measurement error in 3D reconstruction is directly related to the resolution of the images at hand. In turn, even a small measurement error can cause significant errors in reconstructing 3D scene structure or camera pose. Therefore, enhancing image resolution can be an effective solution for securing the accuracy as well as the resolution of 3D reconstruction. In the proposed method, depth map estimation and image super-resolution are formulated in a single energy minimization framework with a convex function and solved efficiently by a first-order primal-dual algorithm. Explicit inter-frame pixel correspondences are not required for our super-resolution procedure, thus we can avoid a huge computation time and obtain improved depth map in the accuracy and resolution as well as highresolution images with reasonable time. The superiority of our algorithm is demonstrated by presenting the improved depth map accuracy, image super-resolution results, and camera pose estimation.</p><p>6 0.16406097 <a title="232-tfidf-6" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>7 0.15436146 <a title="232-tfidf-7" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>8 0.15401882 <a title="232-tfidf-8" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>9 0.14471814 <a title="232-tfidf-9" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>10 0.14015898 <a title="232-tfidf-10" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>11 0.12205018 <a title="232-tfidf-11" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>12 0.12001012 <a title="232-tfidf-12" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>13 0.11518624 <a title="232-tfidf-13" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>14 0.11392651 <a title="232-tfidf-14" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>15 0.10711596 <a title="232-tfidf-15" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>16 0.098173589 <a title="232-tfidf-16" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>17 0.096295513 <a title="232-tfidf-17" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>18 0.096052773 <a title="232-tfidf-18" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>19 0.095040433 <a title="232-tfidf-19" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>20 0.092468105 <a title="232-tfidf-20" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, 0.193), (2, 0.045), (3, 0.065), (4, -0.043), (5, -0.01), (6, -0.053), (7, 0.097), (8, -0.003), (9, -0.038), (10, -0.03), (11, -0.066), (12, 0.001), (13, 0.149), (14, 0.053), (15, -0.057), (16, -0.238), (17, -0.023), (18, -0.01), (19, -0.065), (20, 0.01), (21, 0.033), (22, -0.055), (23, -0.066), (24, 0.018), (25, 0.045), (26, 0.005), (27, -0.001), (28, -0.014), (29, 0.002), (30, -0.04), (31, 0.036), (32, 0.104), (33, -0.005), (34, -0.026), (35, -0.051), (36, 0.05), (37, -0.03), (38, -0.091), (39, 0.167), (40, 0.054), (41, -0.051), (42, -0.068), (43, 0.081), (44, -0.108), (45, 0.014), (46, -0.057), (47, -0.031), (48, -0.002), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97370917 <a title="232-lsi-1" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>Author: Ming-Yu Liu, Oncel Tuzel, Yuichi Taguchi</p><p>Abstract: We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. Specifically, it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. Though this is closely related to the all-pairshortest-path problem which has O(n2 log n) complexity, we develop a novel approximation algorithm whose complexity grows linearly with the image size and achieve realtime performance. We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. In addition, we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps, an important sensor fusion application.</p><p>2 0.88108242 <a title="232-lsi-2" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>3 0.85989606 <a title="232-lsi-3" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>Author: unkown-author</p><p>Abstract: We tackle the problem of jointly increasing the spatial resolution and apparent measurement accuracy of an input low-resolution, noisy, and perhaps heavily quantized depth map. In stark contrast to earlier work, we make no use of ancillary data like a color image at the target resolution, multiple aligned depth maps, or a database of highresolution depth exemplars. Instead, we proceed by identifying and merging patch correspondences within the input depth map itself, exploiting patchwise scene self-similarity across depth such as repetition of geometric primitives or object symmetry. While the notion of ‘single-image ’ super resolution has successfully been applied in the context of color and intensity images, we are to our knowledge the first to present a tailored analogue for depth images. Rather than reason in terms of patches of 2D pixels as others have before us, our key contribution is to proceed by reasoning in terms of patches of 3D points, with matched patch pairs related by a respective 6 DoF rigid body motion in 3D. In support of obtaining a dense correspondence field in reasonable time, we introduce a new 3D variant of Patch- Match. A third contribution is a simple, yet effective patch upscaling and merging technique, which predicts sharp object boundaries at the target resolution. We show that our results are highly competitive with those of alternative techniques leveraging even a color image at the target resolution or a database of high-resolution depth exemplars.</p><p>4 0.79261726 <a title="232-lsi-4" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>Author: Zhe Yang, Zhiwei Xiong, Yueyi Zhang, Jiao Wang, Feng Wu</p><p>Abstract: This paper proposes novel density modulated binary patterns for depth acquisition. Similar to Kinect, the illumination patterns do not need a projector for generation and can be emitted by infrared lasers and diffraction gratings. Our key idea is to use the density of light spots in the patterns to carry phase information. Two technical problems are addressed here. First, we propose an algorithm to design the patterns to carry more phase information without compromising the depth reconstruction from a single captured image as with Kinect. Second, since the carried phase is not strictly sinusoidal, the depth reconstructed from the phase contains a systematic error. We further propose a pixelbased phase matching algorithm to reduce the error. Experimental results show that the depth quality can be greatly improved using the phase carried by the density of light spots. Furthermore, our scheme can achieve 20 fps depth reconstruction with GPU assistance.</p><p>5 0.76483184 <a title="232-lsi-5" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>Author: Hee Seok Lee, Kuoung Mu Lee</p><p>Abstract: In this paper, we propose a convex optimization framework for simultaneous estimation of super-resolved depth map and images from a single moving camera. The pixel measurement error in 3D reconstruction is directly related to the resolution of the images at hand. In turn, even a small measurement error can cause significant errors in reconstructing 3D scene structure or camera pose. Therefore, enhancing image resolution can be an effective solution for securing the accuracy as well as the resolution of 3D reconstruction. In the proposed method, depth map estimation and image super-resolution are formulated in a single energy minimization framework with a convex function and solved efficiently by a first-order primal-dual algorithm. Explicit inter-frame pixel correspondences are not required for our super-resolution procedure, thus we can avoid a huge computation time and obtain improved depth map in the accuracy and resolution as well as highresolution images with reasonable time. The superiority of our algorithm is demonstrated by presenting the improved depth map accuracy, image super-resolution results, and camera pose estimation.</p><p>6 0.72743362 <a title="232-lsi-6" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>7 0.71806055 <a title="232-lsi-7" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>8 0.68455857 <a title="232-lsi-8" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>9 0.67646945 <a title="232-lsi-9" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>10 0.64433444 <a title="232-lsi-10" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>11 0.62546754 <a title="232-lsi-11" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>12 0.58472377 <a title="232-lsi-12" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>13 0.56383765 <a title="232-lsi-13" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>14 0.55655736 <a title="232-lsi-14" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>15 0.55603379 <a title="232-lsi-15" href="./cvpr-2013-Dense_3D_Reconstruction_from_Severely_Blurred_Images_Using_a_Single_Moving_Camera.html">108 cvpr-2013-Dense 3D Reconstruction from Severely Blurred Images Using a Single Moving Camera</a></p>
<p>16 0.55249125 <a title="232-lsi-16" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>17 0.54922575 <a title="232-lsi-17" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>18 0.54656208 <a title="232-lsi-18" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>19 0.51697397 <a title="232-lsi-19" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>20 0.47472736 <a title="232-lsi-20" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.167), (16, 0.061), (26, 0.035), (28, 0.206), (33, 0.243), (67, 0.036), (69, 0.04), (87, 0.089), (91, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88262624 <a title="232-lda-1" href="./cvpr-2013-Graph_Matching_with_Anchor_Nodes%3A_A_Learning_Approach.html">192 cvpr-2013-Graph Matching with Anchor Nodes: A Learning Approach</a></p>
<p>Author: Nan Hu, Raif M. Rustamov, Leonidas Guibas</p><p>Abstract: In this paper, we consider the weighted graph matching problem with partially disclosed correspondences between a number of anchor nodes. Our construction exploits recently introduced node signatures based on graph Laplacians, namely the Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel map on the edges. In this paper, without assuming an explicit form of parametric dependence nor a distance metric between node signatures, we formulate an optimization problem which incorporates the knowledge of anchor nodes. Solving this problem gives us an optimized proximity measure specific to the graphs under consideration. Using this as a first order compatibility term, we then set up an integer quadratic program (IQP) to solve for a near optimal graph matching. Our experiments demonstrate the superior performance of our approach on randomly generated graphs and on two widelyused image sequences, when compared with other existing signature and adjacency matrix based graph matching methods.</p><p>2 0.87755841 <a title="232-lda-2" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<p>Author: Austin Abrams, Kylia Miskell, Robert Pless</p><p>Abstract: Shadows encode a powerful geometric cue: if one pixel casts a shadow onto another, then the two pixels are colinear with the lighting direction. Given many images over many lighting directions, this constraint can be leveraged to recover the depth of a scene from a single viewpoint. For outdoor scenes with solar illumination, we term this the episolar constraint, which provides a convex optimization to solve for the sparse depth of a scene from shadow correspondences, a method to reduce the search space when finding shadow correspondences, and a method to geometrically calibrate a camera using shadow constraints. Our method constructs a dense network of nonlocal constraints which complements recent work on outdoor photometric stereo and cloud based cues for 3D. We demonstrate results across a variety of time-lapse sequences from webcams “in . wu st l. edu (b)(c) the wild.”</p><p>3 0.86833805 <a title="232-lda-3" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>Author: Ishani Chakraborty, Hui Cheng, Omar Javed</p><p>Abstract: We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. 1 Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes; a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.</p><p>4 0.86037236 <a title="232-lda-4" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>same-paper 5 0.85952413 <a title="232-lda-5" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>Author: Ming-Yu Liu, Oncel Tuzel, Yuichi Taguchi</p><p>Abstract: We propose an algorithm utilizing geodesic distances to upsample a low resolution depth image using a registered high resolution color image. Specifically, it computes depth for each pixel in the high resolution image using geodesic paths to the pixels whose depths are known from the low resolution one. Though this is closely related to the all-pairshortest-path problem which has O(n2 log n) complexity, we develop a novel approximation algorithm whose complexity grows linearly with the image size and achieve realtime performance. We compare our algorithm with the state of the art on the benchmark dataset and show that our approach provides more accurate depth upsampling with fewer artifacts. In addition, we show that the proposed algorithm is well suited for upsampling depth images using binary edge maps, an important sensor fusion application.</p><p>6 0.85425526 <a title="232-lda-6" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>7 0.8396554 <a title="232-lda-7" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<p>8 0.82363766 <a title="232-lda-8" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>9 0.82214886 <a title="232-lda-9" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>10 0.82203239 <a title="232-lda-10" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>11 0.82166475 <a title="232-lda-11" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>12 0.81980163 <a title="232-lda-12" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>13 0.81972045 <a title="232-lda-13" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>14 0.81832665 <a title="232-lda-14" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>15 0.81766474 <a title="232-lda-15" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>16 0.81759942 <a title="232-lda-16" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>17 0.81750906 <a title="232-lda-17" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>18 0.81405443 <a title="232-lda-18" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>19 0.81383508 <a title="232-lda-19" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>20 0.81293774 <a title="232-lda-20" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
