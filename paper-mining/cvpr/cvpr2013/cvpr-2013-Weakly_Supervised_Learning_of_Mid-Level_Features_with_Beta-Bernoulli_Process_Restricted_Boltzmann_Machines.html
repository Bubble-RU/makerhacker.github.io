<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-462" href="#">cvpr2013-462</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</h1>
<br/><p>Source: <a title="cvpr-2013-462-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Mittelman_Weakly_Supervised_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Roni Mittelman, Honglak Lee, Benjamin Kuipers, Silvio Savarese</p><p>Abstract: The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. Attributes provide an intermediate feature representation in between low-level features and the class categories, leading to improved learning on novel categories from few examples. However, a major caveat is that learning semantic attributes is a laborious task, requiring a significant amount of time and human intervention to provide labels. In order to address this issue, we propose a weakly supervised approach to learn mid-level features, where only class-level supervision is provided during training. We develop a novel extension of the restricted Boltzmann machine (RBM) by incorporating a Beta-Bernoulli process factor potential for hidden units. Unlike the standard RBM, our model uses the class labels to promote category-dependent sharing of learned features, which tends to improve the generalization performance. By using semantic attributes for which annotations are available, we show that we can find correspondences between the learned mid-level features and the labeled attributes. Therefore, the mid-level features have distinct semantic characterization which is similar to that given by the semantic attributes, even though their labeling was not provided during training. Our experimental results on object recognition tasks show significant performance gains, outperforming existing methods which rely on manually labeled semantic attributes.</p><p>Reference: <a title="cvpr-2013-462-reference" href="../cvpr2013_reference/cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu lvi  Abstract The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. [sent-2, score-0.276]
</p><p>2 Attributes provide an intermediate feature representation in between low-level features and the class categories, leading to improved learning on novel categories from few examples. [sent-3, score-0.109]
</p><p>3 However, a major caveat is that learning semantic attributes is a laborious task, requiring a significant amount of time and human intervention to provide labels. [sent-4, score-0.276]
</p><p>4 We develop a novel extension of the restricted Boltzmann machine (RBM) by incorporating a Beta-Bernoulli process factor potential for hidden units. [sent-6, score-0.215]
</p><p>5 Unlike the standard RBM, our model uses the class labels to promote category-dependent sharing of learned features, which tends to improve the generalization performance. [sent-7, score-0.122]
</p><p>6 By using semantic attributes for which annotations are available, we show that we can find correspondences between the learned mid-level features and the labeled attributes. [sent-8, score-0.476]
</p><p>7 Therefore, the mid-level features have  distinct semantic characterization which is similar to that given by the semantic attributes, even though their labeling was not provided during training. [sent-9, score-0.333]
</p><p>8 Our experimental results on object recognition tasks show significant performance gains, outperforming existing methods which rely on manually labeled semantic attributes. [sent-10, score-0.185]
</p><p>9 Instead, significant performance gains can be achieved by introducing an intermediate set of features that capture higher-level semantic concepts beyond the plain visual cues that low-level features offer [29, 27, 13]. [sent-13, score-0.309]
</p><p>10 One popular approach to introducing such mid-level features is to use semantic attributes [7, 16, 9]. [sent-14, score-0.325]
</p><p>11 Specifically, each category can be represented by a set of semantic attributes, where some of these attributes can be shared by other categories. [sent-15, score-0.276]
</p><p>12 Typically, the attribute representation is obtained using the following process. [sent-17, score-0.149]
</p><p>13 First, a set of concepts is defined by the designer, and each instance in the training set has to be labeled with the presence or absence of each attribute. [sent-18, score-0.168]
</p><p>14 Subsequently, a classifier is trained for each of the attributes using the constructed training set. [sent-19, score-0.19]
</p><p>15 Furthermore, as was  reported in [7], some additional feature selection schemes which utilize the attribute labels may be necessary in order to achieve satisfactory performance. [sent-20, score-0.149]
</p><p>16 Obtaining the semantic attribute representation is clearly a highly labor-intensive process. [sent-21, score-0.291]
</p><p>17 Furthermore, it is not clear how to choose the constituent semantic concepts for problems in which the shared semantic content is less intuitive (e. [sent-22, score-0.382]
</p><p>18 One approach to learning a semantic mid-level feature representation is based on latent Dirichlet allocation (LDA) [2], which uses a set of topics to describe the semantic content. [sent-25, score-0.309]
</p><p>19 However, unlike linguistic words, visual words often do not carry much semantic interpretation beyond basic appearance cues. [sent-27, score-0.142]
</p><p>20 Another line of work is the deep learning approach (see [1] for a survey), such as deep belief networks (DBNs) [12], which tries to learn a hierarchical set of features from unlabeled and labeled data. [sent-29, score-0.148]
</p><p>21 It has been shown that features in the upper levels of the hierarchy capture distinct semantic concepts, such as object parts [19]. [sent-30, score-0.191]
</p><p>22 The RBM is a bi-partite undirected graphical model that is 444777446 capable of learning a dictionary of patterns from the unla-  beled data. [sent-32, score-0.104]
</p><p>23 By expanding the RBM into a hierarchical representation, relevant semantic concepts can be revealed at the higher levels. [sent-33, score-0.211]
</p><p>24 We propose to learn mid-level features using the replicated softmax RBM (RS-RBM), which is an undirected topic model applied to bag-of-words data [24]. [sent-38, score-0.213]
</p><p>25 Unlike other topic models, such as LDA, the RS-RBM can be expanded into a DBN hierarchy by stacking additional RBM layers with binary inputs on-top of the first RS-RBM layer. [sent-39, score-0.179]
</p><p>26 Therefore, we expect that features in higher levels can capture important semantic concepts that could not be captured by standard topic models with only a single layer (e. [sent-40, score-0.463]
</p><p>27 Our experimental results on object recognition show that the proposed model outperforms other baseline methods, such as LDA, RBMs, and previous state-of-the-art methods using attribute labels. [sent-53, score-0.149]
</p><p>28 In order to analyze the semantic content that is captured by the mid-level features learned with the BBP-RBM, we used the datasets from [7] which include annotations of manually specified semantic attributes. [sent-54, score-0.437]
</p><p>29 By using the learned features to predict each of the labeled attributes in the training set, we found the correspondences between the learned mid-level features and the labeled attributes. [sent-55, score-0.521]
</p><p>30 We performed localization experiments where we try to predict the bounding boxes of the mid-level features in the image and compare them to their corresponding attributes. [sent-56, score-0.172]
</p><p>31 We demonstrate that our method can localize semantic concepts like snout, skin, and furry, even though no information about these at-  tributes was used during the training process. [sent-57, score-0.267]
</p><p>32 We review two forms of the RBM which are both used in this work: the first assumes binary observations, and the second is the RS-RBM which uses word count observations. [sent-65, score-0.242]
</p><p>33 RBM with binary observations The RBM [25] defines a joint probability distribution over a hidden layer h = [h1, . [sent-68, score-0.406]
</p><p>34 , hK]T, where hk ∈ {0, 1}, and a visible layer v = [v1, . [sent-71, score-0.235]
</p><p>35 Inference can be performed using Gibbs sampling, alternating between sampling the hidden and visible layers. [sent-82, score-0.209]
</p><p>36 The Replicated Softmax RBM The RBM can be extended to the case where the observations are word counts in a document [24]. [sent-86, score-0.233]
</p><p>37 The word counts are transformed into a vector of binary digits, where the number of 1’s for each word in the document equals its word count. [sent-87, score-0.523]
</p><p>38 A single hidden layer of a binary RBM then connects to each of these binary observation vectors 444777557  (with weight sharing), which allows for modeling of the word counts. [sent-88, score-0.538]
</p><p>39 The model can be further simplified such that it deals with the word count observations directly, rather than with the intermediate binary vectors. [sent-89, score-0.269]
</p><p>40 InferenPce is performed using Gibbs sampling, where the posterior Pfor the hidden layer takes the form p(hk = 1|v) = σ? [sent-94, score-0.369]
</p><p>41 (6)  Xi  Sampling from the posterior of the visible layer is performed by sampling D times from the following multinomial distribution:  pi=PiN=ex1pe(xPp(kKP=1kKh=k1whkk,wik+,i c+i) ci), i = 1,. [sent-97, score-0.335]
</p><p>42 Parameter estimation is performed in the same manner as the case of the RBM with binary observations. [sent-101, score-0.116]
</p><p>43 , fK ∈ {0, 1} denote the elements of a binary vector, then the BB∈P { generates oftek  according to πk  ∼  fk  ∼  Beta(α/K, β(K − 1)/K), Bernoulli(πk) ,  (8)  where α, β are positive constants (hyperparameters), and we use the notation π = [π1, . [sent-109, score-0.166]
</p><p>44 Forb th  ×  the RS-RBM and the binary RBM, we propose their extensions by incorporating the Beta-Bernoulli process factor potentials. [sent-123, score-0.141]
</p><p>45 The multitask paradigm promotes sharing of information between related groups, and therefore can lead to improved generalization performance. [sent-125, score-0.121]
</p><p>46 In order to obtain the bag-of-words representation, we first compute the histogram over the visual words, and then obtain the word counts by multiplying each histogram with a constant (we used the constant 200 throughout this work) and rounding the numbers to the nearest integer values. [sent-130, score-0.229]
</p><p>47 The word counts are used as the inputs to RS-RBMs (or BBP-RS-RBMs which we describe in Section 4), where dif-  ferent RS-RBM units are used for each of the histograms. [sent-131, score-0.324]
</p><p>48 The binary outputs of all the RS-RBM units are concatenated and fed into a binary RBM (or a binary BBP-RBM) at the second layer. [sent-132, score-0.354]
</p><p>49 The outputs of the hidden units of the second layer are then used as input to the third layer binary RBM, and similarly to any higher layers. [sent-133, score-0.626]
</p><p>50 Training the DBN is performed in a greedy layer-wise fashion, starting with the first layer and proceeding in the upward direction [12]. [sent-134, score-0.201]
</p><p>51 Each of the RS-RBM units independently captures important patterns which are observed within its defined feature type and spatial extent. [sent-135, score-0.123]
</p><p>52 The binary RBM in the second layer captures higher-order dependencies between the different histograms in the first layer. [sent-136, score-0.239]
</p><p>53 The binary RBMs in higher levels could model further high-order dependencies, which we hypothesize to be related to some semantic 444777668  concepts. [sent-137, score-0.219]
</p><p>54 In Section 5, we find associations between the learned features and manually specified semantic attributes. [sent-138, score-0.236]
</p><p>55 The feature vector which is used for classification is obtained by concatenating the outputs of all the hidden units from all the layers of the learned DBN. [sent-139, score-0.333]
</p><p>56 It is also related to “dropout”, which randomly sets individual hidden units to zeros during training and has been reported to reduce overfitting when training deep convolutional neural networks [15]. [sent-146, score-0.365]
</p><p>57 The BBP-RBM uses a factor graph formulation to combine two different types of factors: the first factor is related to the RBM, and the second factor is related to the BBP. [sent-147, score-0.192]
</p><p>58 Combining these factors together leads to an undirected graphical model for which we develop efficient inference and parameter estimation schemes. [sent-148, score-0.133]
</p><p>59 , fK]T that is used to choose which of the K hidden units to activate. [sent-154, score-0.225]
</p><p>60 Our approach is to define an undirected graphical model in the form of a factor graph with two types of factors, as shown in Figure 2(a) for the single-task case and Figure 2(b) for the multi-task cases. [sent-155, score-0.143]
</p><p>61 The first factor is obtained as an unnormalized RBM-like probability distribution which includes the binary selection variables f: ga(v, h, f) = exp(−E(v, h, f)) ,  (9)  ×  where the energy term takes the form E(v, h, f) = −(f ? [sent-156, score-0.179]
</p><p>62 ga  and  gb  are the  ×  two factor types, and M denotes the total number of training samples. [sent-180, score-0.236]
</p><p>63 C denotes the number of classes in the training set, and Mc denotes the number of training instances belonging to class c. [sent-181, score-0.196]
</p><p>64 Using the factor graph description in Figure 2(a), the probability distribution for the single-task BBP-RBM takes the form  p({v(j), h(j), f(j)}jM=1, π) ∝  gb({f(j)}jM=1,  π)  (12)  YM  Y ga(v(j), h(j), f(j)). [sent-182, score-0.102]
</p><p>65 We only provide the posterior probability distributions for the multi-task case, since the single-task can be obtained as a special case by setting  C = 1. [sent-187, score-0.104]
</p><p>66 δw(kjhec)r  wk,iv(iPjc)  wdekn,iovt(eisjc)  eδ(k djce)f  The posterior probability for π(c) takes the form  p(πk(c)  |−) =  (15) XMc  jXc=1  Beta(α/K +  XMc  jXc=1  X fk(jc),β(K − 1)/K + X(1− fk(jc))). [sent-189, score-0.104]
</p><p>67 Sampling from the posterior of the visible formed in a similar way that was discussed for the RBM with either binary or word count where the only difference is that h is replaced From Equation (14), we observe that if  layer is perin Section 2 observations, by f ? [sent-190, score-0.496]
</p><p>68 = 1then  π(kjc)  the BBP-RBM reduces to the standard RBM, since the posterior probability distribution for becomes = 1|−) = ) (i. [sent-192, score-0.104]
</p><p>69 , the standard RBM has the same posterior probability for 4. [sent-194, score-0.104]
</p><p>70 (16)  The expression cannot be evaluated analytically; however, we note that the first inner expectation does admit an analytical expression, whereas the second inner expectation is intractable. [sent-204, score-0.118]
</p><p>71 Our solution is to train each layer of a BBP-RBM as described in the previous section. [sent-214, score-0.162]
</p><p>72 However, when computing the output of the hidden units to be fed into the consecutive layer, we choose = 1, ∀c = 1, . [sent-215, score-0.225]
</p><p>73 Experimental results We evaluated the features learned by the BBP-RBM using two datasets that were developed in [7], which include annotation for labeled attributes. [sent-259, score-0.137]
</p><p>74 , learning the BBP-RBM features using the PASCAL training set, and performing classification on the Yahoo dataset). [sent-263, score-0.135]
</p><p>75 Finally, we examined the semantic content of the features by finding correspondences between the learned features and the manually labeled attributes available for the PASCAL dataset. [sent-264, score-0.524]
</p><p>76 We also used these correspondences to perform attribute localization experiments, by predicting the bounding boxes for several of the learned mid-level features. [sent-265, score-0.287]
</p><p>77 Additionally, there are annotations for 64 attributes 444787880  # Layers1Ove2rall31Mean p2er-class3  Table1. [sent-271, score-0.164]
</p><p>78 We used the same low-level features (referred to as base features) which were employed in [7] and are available online. [sent-280, score-0.103]
</p><p>79 Note that not using the edge features in our methods may give an unfair disadvantage when comparing to the results in [7] and [29] that used all the base features. [sent-283, score-0.103]
</p><p>80 When learning an RBM based model, we used 800 hidden units for the HOG histogram, 200 hidden units for the color histogram, and 300 units for the texture histogram. [sent-285, score-0.573]
</p><p>81 The  number of hidden units for the upper layers was 4000 for the second layer, and 2000 for the third layer. [sent-286, score-0.258]
</p><p>82 The LDA features were the topic proportions learned for each of the histograms (see Section 3), and we used 50 topics for each histogram. [sent-290, score-0.194]
</p><p>83 Note that the baseline methods were adapted to exploit the information from the labeled attributes (which the BBP-RBM did not use). [sent-306, score-0.177]
</p><p>84 In [7], scores from attribute classifiers were used as input for a multi-class linear SVM. [sent-307, score-0.149]
</p><p>85 In [29], the attribute classifier scores were used in a latent SVM [8] formulation, using two different loss functions (referred to as “loss-1” and “loss-2” in the table). [sent-308, score-0.149]
</p><p>86 Note that attribute annotations are very expensive to obtain, and for many visual recognition problems, such as activity recognition in videos [22], it is even harder to identify and label the semantic content that is shared by different types of classes. [sent-309, score-0.35]
</p><p>87 The results show that, even though our method did not use the attribute annotation, it significantly improved both the overall classification accuracy and the mean perclass accuracy in comparison to the baseline methods. [sent-310, score-0.179]
</p><p>88 To this end, we used the PASCAL training set to learn the features and evaluated their performance on the Yahoo dataset. [sent-314, score-0.105]
</p><p>89 We partitioned the Yahoo dataset into different proportions of training samples and compared the performance when using the multi-task BBP-RBM and base features, respectively. [sent-315, score-0.177]
</p><p>90 The results suggest that our method using the BBP-RBM features can recognize new categories from the Yahoo dataset with fewer training samples, as compared to using the base features. [sent-317, score-0.195]
</p><p>91 6a801tse  Training %OvBeraaslel fePateurr-celsassOBBvePra-Rl BMP feera-tculraesss  when using the base features and when using the BBP-RBM features learned from the PASCAL training set. [sent-324, score-0.253]
</p><p>92 For example, the overall classification performance with the BBP-RBM features using only 20% of the dataset for training is comparable to or better than that with the base features using 60% of the dataset for training. [sent-325, score-0.238]
</p><p>93 Correspondence between mid-level and semantic attributes  features  In this experiment, we evaluated the degree to which the features learned using the BBP-RBM demonstrate identifiable semantic concepts. [sent-328, score-0.561]
</p><p>94 For each feature and labeled attribute pair, we used the score given by Equation (3) to predict the presence of manually labeled semantic attributes in each training example and computed the area under the ROC curve over the PASCAL training data. [sent-329, score-0.647]
</p><p>95 The feature corresponding to each attribute is determined as that which has the largest area under the ROC curve. [sent-330, score-0.149]
</p><p>96 Figure 3 shows the corresponding area under the ROC curve for every attribute on the PASCAL test data (i. [sent-331, score-0.149]
</p><p>97 The area under the ROC curve obtained using attribute classifiers (linear SVMs trained using the attribute labels and the base features [7]) is also shown together. [sent-334, score-0.401]
</p><p>98 The figure shows that the learned features without using attribute labels performed reasonably well, and some learned features performed comparably to the attribute classifiers that were trained using the attribute labels. [sent-335, score-0.713]
</p><p>99 We note that all the semantic attributes were associated to features in either the second layer or the third layer in Figure 1, which  supports our hypothesis that the higher levels of the DBN can capture semantic concepts. [sent-336, score-0.791]
</p><p>100 1 Predicting attribute bounding boxes We also performed experiments where the mid-level features corresponding to the attributes “snout”, “skin”, and “furry” were used to predict the bounding boxes of these attributes. [sent-339, score-0.515]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rbm', 0.643), ('jc', 0.32), ('yahoo', 0.189), ('bbp', 0.179), ('layer', 0.162), ('attribute', 0.149), ('semantic', 0.142), ('attributes', 0.134), ('units', 0.123), ('word', 0.12), ('lda', 0.111), ('kjc', 0.102), ('hidden', 0.102), ('pascal', 0.099), ('dbn', 0.094), ('fk', 0.089), ('jm', 0.083), ('rbms', 0.079), ('binary', 0.077), ('ctv', 0.077), ('contrastive', 0.076), ('concepts', 0.069), ('beta', 0.066), ('posterior', 0.066), ('factor', 0.064), ('gibbs', 0.064), ('dbns', 0.063), ('roc', 0.058), ('training', 0.056), ('boltzmann', 0.054), ('base', 0.054), ('counts', 0.053), ('mc', 0.052), ('dbk', 0.051), ('discrbm', 0.051), ('htwv', 0.051), ('iappears', 0.051), ('jxc', 0.051), ('singletask', 0.051), ('snout', 0.051), ('xmc', 0.051), ('undirected', 0.05), ('features', 0.049), ('hk', 0.047), ('count', 0.045), ('learned', 0.045), ('gb', 0.045), ('sparsity', 0.044), ('labeled', 0.043), ('sampling', 0.042), ('topic', 0.041), ('ga', 0.041), ('performed', 0.039), ('skin', 0.038), ('softmax', 0.038), ('probability', 0.038), ('cc', 0.038), ('categories', 0.036), ('furry', 0.035), ('replicated', 0.035), ('promotes', 0.035), ('boxes', 0.034), ('proportions', 0.034), ('vi', 0.033), ('correspondences', 0.033), ('partitioned', 0.033), ('document', 0.033), ('multitask', 0.033), ('layers', 0.033), ('divergence', 0.033), ('expectation', 0.032), ('bernoulli', 0.031), ('pin', 0.03), ('denotes', 0.03), ('classification', 0.03), ('annotations', 0.03), ('content', 0.029), ('graphical', 0.029), ('generalization', 0.029), ('equation', 0.028), ('histogram', 0.028), ('inputs', 0.028), ('deep', 0.028), ('observations', 0.027), ('nce', 0.027), ('inference', 0.027), ('factors', 0.027), ('inner', 0.027), ('bounding', 0.026), ('visible', 0.026), ('bayesian', 0.025), ('dictionary', 0.025), ('ce', 0.025), ('topics', 0.025), ('restricted', 0.025), ('sharing', 0.024), ('hog', 0.024), ('extension', 0.024), ('predict', 0.024), ('class', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="462-tfidf-1" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>Author: Roni Mittelman, Honglak Lee, Benjamin Kuipers, Silvio Savarese</p><p>Abstract: The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. Attributes provide an intermediate feature representation in between low-level features and the class categories, leading to improved learning on novel categories from few examples. However, a major caveat is that learning semantic attributes is a laborious task, requiring a significant amount of time and human intervention to provide labels. In order to address this issue, we propose a weakly supervised approach to learn mid-level features, where only class-level supervision is provided during training. We develop a novel extension of the restricted Boltzmann machine (RBM) by incorporating a Beta-Bernoulli process factor potential for hidden units. Unlike the standard RBM, our model uses the class labels to promote category-dependent sharing of learned features, which tends to improve the generalization performance. By using semantic attributes for which annotations are available, we show that we can find correspondences between the learned mid-level features and the labeled attributes. Therefore, the mid-level features have distinct semantic characterization which is similar to that given by the semantic attributes, even though their labeling was not provided during training. Our experimental results on object recognition tasks show significant performance gains, outperforming existing methods which rely on manually labeled semantic attributes.</p><p>2 0.42372474 <a title="462-tfidf-2" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>Author: Yujia Li, Daniel Tarlow, Richard Zemel</p><p>Abstract: When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs); we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss;and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.</p><p>3 0.40228665 <a title="462-tfidf-3" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>Author: Siddhartha Chandra, Shailesh Kumar, C.V. Jawahar</p><p>Abstract: Understanding the nature of data is the key to building good representations. In domains such as natural images, the data comes from very complex distributions which are hard to capture. Feature learning intends to discover or best approximate these underlying distributions and use their knowledge to weed out irrelevant information, preserving most of the relevant information. Feature learning can thus be seen as a form of dimensionality reduction. In this paper, we describe a feature learning scheme for natural images. We hypothesize that image patches do not all come from the same distribution, they lie in multiple nonlinear subspaces. We propose a framework that uses K Restricted Boltzmann Machines (K-RBMS) to learn multiple non-linear subspaces in the raw image space. Projections of the image patches into these subspaces gives us features, which we use to build image representations. Our algorithm solves the coupled problem of finding the right non-linear subspaces in the input space and associating image patches with those subspaces in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that representations based on our framework outperform the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks.</p><p>4 0.28839377 <a title="462-tfidf-4" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>5 0.18814695 <a title="462-tfidf-5" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>6 0.17391114 <a title="462-tfidf-6" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>7 0.13919695 <a title="462-tfidf-7" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>8 0.13778019 <a title="462-tfidf-8" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>9 0.13397451 <a title="462-tfidf-9" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>10 0.13108598 <a title="462-tfidf-10" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>11 0.12294004 <a title="462-tfidf-11" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>12 0.12044435 <a title="462-tfidf-12" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>13 0.11762629 <a title="462-tfidf-13" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>14 0.11605848 <a title="462-tfidf-14" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>15 0.11396223 <a title="462-tfidf-15" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>16 0.11091744 <a title="462-tfidf-16" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>17 0.10251661 <a title="462-tfidf-17" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>18 0.10194018 <a title="462-tfidf-18" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>19 0.099556103 <a title="462-tfidf-19" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>20 0.09593568 <a title="462-tfidf-20" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, -0.126), (2, -0.043), (3, 0.004), (4, 0.116), (5, 0.086), (6, -0.137), (7, 0.1), (8, 0.039), (9, 0.057), (10, 0.025), (11, 0.01), (12, -0.06), (13, 0.01), (14, 0.01), (15, 0.254), (16, -0.111), (17, 0.148), (18, 0.12), (19, 0.044), (20, 0.156), (21, -0.317), (22, 0.072), (23, -0.145), (24, -0.116), (25, -0.126), (26, 0.05), (27, -0.001), (28, 0.03), (29, 0.028), (30, 0.039), (31, 0.076), (32, -0.08), (33, 0.089), (34, 0.188), (35, 0.002), (36, 0.035), (37, -0.012), (38, -0.01), (39, -0.029), (40, 0.065), (41, -0.043), (42, -0.024), (43, -0.047), (44, 0.016), (45, -0.023), (46, 0.058), (47, 0.025), (48, 0.009), (49, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89451921 <a title="462-lsi-1" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>Author: Roni Mittelman, Honglak Lee, Benjamin Kuipers, Silvio Savarese</p><p>Abstract: The use of semantic attributes in computer vision problems has been gaining increased popularity in recent years. Attributes provide an intermediate feature representation in between low-level features and the class categories, leading to improved learning on novel categories from few examples. However, a major caveat is that learning semantic attributes is a laborious task, requiring a significant amount of time and human intervention to provide labels. In order to address this issue, we propose a weakly supervised approach to learn mid-level features, where only class-level supervision is provided during training. We develop a novel extension of the restricted Boltzmann machine (RBM) by incorporating a Beta-Bernoulli process factor potential for hidden units. Unlike the standard RBM, our model uses the class labels to promote category-dependent sharing of learned features, which tends to improve the generalization performance. By using semantic attributes for which annotations are available, we show that we can find correspondences between the learned mid-level features and the labeled attributes. Therefore, the mid-level features have distinct semantic characterization which is similar to that given by the semantic attributes, even though their labeling was not provided during training. Our experimental results on object recognition tasks show significant performance gains, outperforming existing methods which rely on manually labeled semantic attributes.</p><p>2 0.79932582 <a title="462-lsi-2" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>Author: Siddhartha Chandra, Shailesh Kumar, C.V. Jawahar</p><p>Abstract: Understanding the nature of data is the key to building good representations. In domains such as natural images, the data comes from very complex distributions which are hard to capture. Feature learning intends to discover or best approximate these underlying distributions and use their knowledge to weed out irrelevant information, preserving most of the relevant information. Feature learning can thus be seen as a form of dimensionality reduction. In this paper, we describe a feature learning scheme for natural images. We hypothesize that image patches do not all come from the same distribution, they lie in multiple nonlinear subspaces. We propose a framework that uses K Restricted Boltzmann Machines (K-RBMS) to learn multiple non-linear subspaces in the raw image space. Projections of the image patches into these subspaces gives us features, which we use to build image representations. Our algorithm solves the coupled problem of finding the right non-linear subspaces in the input space and associating image patches with those subspaces in an iterative EM like algorithm to minimize the overall reconstruction error. Extensive empirical results over several popular image classification datasets show that representations based on our framework outperform the traditional feature representations such as the SIFT based Bag-of-Words (BoW) and convolutional deep belief networks.</p><p>3 0.72453922 <a title="462-lsi-3" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>Author: Yujia Li, Daniel Tarlow, Richard Zemel</p><p>Abstract: When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs); we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss;and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.</p><p>4 0.71172297 <a title="462-lsi-4" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>5 0.55581641 <a title="462-lsi-5" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>Author: Fei Chen, Huimin Yu, Roland Hu, Xunxun Zeng</p><p>Abstract: In this paper we introduce a new shape-driven approach for object segmentation. Given a training set of shapes, we first use deep Boltzmann machine to learn the hierarchical architecture of shape priors. This learned hierarchical architecture is then used to model shape variations of global and local structures in an energetic form. Finally, it is applied to data-driven variational methods to perform object extraction of corrupted data based on shape probabilistic representation. Experiments demonstrate that our model can be applied to dataset of arbitrary prior shapes, and can cope with image noise and clutter, as well as partial occlusions.</p><p>6 0.52481467 <a title="462-lsi-6" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>7 0.45047665 <a title="462-lsi-7" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>8 0.44524178 <a title="462-lsi-8" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>9 0.42460102 <a title="462-lsi-9" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>10 0.41760761 <a title="462-lsi-10" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>11 0.41175246 <a title="462-lsi-11" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>12 0.40350509 <a title="462-lsi-12" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>13 0.40289119 <a title="462-lsi-13" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>14 0.3934288 <a title="462-lsi-14" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>15 0.38198698 <a title="462-lsi-15" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>16 0.38006401 <a title="462-lsi-16" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>17 0.37333357 <a title="462-lsi-17" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>18 0.37078527 <a title="462-lsi-18" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>19 0.36962664 <a title="462-lsi-19" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>20 0.35834339 <a title="462-lsi-20" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.493), (16, 0.019), (26, 0.023), (33, 0.227), (67, 0.052), (69, 0.046), (87, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9526754 <a title="462-lda-1" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>Author: Haichao Zhang, David Wipf, Yanning Zhang</p><p>Abstract: This paper presents a robust algorithm for estimating a single latent sharp image given multiple blurry and/or noisy observations. The underlying multi-image blind deconvolution problem is solved by linking all of the observations together via a Bayesian-inspired penalty function which couples the unknown latent image, blur kernels, and noise levels together in a unique way. This coupled penalty function enjoys a number of desirable properties, including a mechanism whereby the relative-concavity or shape is adapted as a function of the intrinsic quality of each blurry observation. In this way, higher quality observations may automatically contribute more to the final estimate than heavily degraded ones. The resulting algorithm, which requires no essential tuning parameters, can recover a high quality image from a set of observations containing potentially both blurry and noisy examples, without knowing a priorithe degradation type of each observation. Experimental results on both synthetic and real-world test images clearly demonstrate the efficacy of the proposed method.</p><p>2 0.94194418 <a title="462-lda-2" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>Author: Chandramouli Paramanand, Ambasamudram N. Rajagopalan</p><p>Abstract: We address the problem of estimating the latent image of a static bilayer scene (consisting of a foreground and a background at different depths) from motion blurred observations captured with a handheld camera. The camera motion is considered to be composed of in-plane rotations and translations. Since the blur at an image location depends both on camera motion and depth, deblurring becomes a difficult task. We initially propose a method to estimate the transformation spread function (TSF) corresponding to one of the depth layers. The estimated TSF (which reveals the camera motion during exposure) is used to segment the scene into the foreground and background layers and determine the relative depth value. The deblurred image of the scene is finally estimated within a regularization framework by accounting for blur variations due to camera motion as well as depth.</p><p>3 0.94088078 <a title="462-lda-3" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>4 0.93392885 <a title="462-lda-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.93078691 <a title="462-lda-5" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>6 0.91807526 <a title="462-lda-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.90108985 <a title="462-lda-7" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>8 0.90072507 <a title="462-lda-8" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>9 0.86857939 <a title="462-lda-9" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>10 0.86633873 <a title="462-lda-10" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>same-paper 11 0.84886628 <a title="462-lda-11" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>12 0.81941634 <a title="462-lda-12" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>13 0.81138617 <a title="462-lda-13" href="./cvpr-2013-Graph_Transduction_Learning_with_Connectivity_Constraints_with_Application_to_Multiple_Foreground_Cosegmentation.html">193 cvpr-2013-Graph Transduction Learning with Connectivity Constraints with Application to Multiple Foreground Cosegmentation</a></p>
<p>14 0.79945457 <a title="462-lda-14" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>15 0.79837406 <a title="462-lda-15" href="./cvpr-2013-Discriminative_Non-blind_Deblurring.html">131 cvpr-2013-Discriminative Non-blind Deblurring</a></p>
<p>16 0.78454763 <a title="462-lda-16" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>17 0.78088731 <a title="462-lda-17" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>18 0.76982808 <a title="462-lda-18" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>19 0.76849782 <a title="462-lda-19" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>20 0.76371342 <a title="462-lda-20" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
