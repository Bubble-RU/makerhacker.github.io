<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-430" href="#">cvpr2013-430</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-430-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wolf_The_SVM-Minus_Similarity_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Lior Wolf, Noga Levy</p><p>Abstract: Face recognition in unconstrained videos requires specialized tools beyond those developed for still images: the fact that the confounding factors change state during the video sequence presents a unique challenge, but also an opportunity to eliminate spurious similarities. Luckily, a major source of confusion in visual similarity of faces is the 3D head orientation, for which image analysis tools provide an accurate estimation. The method we propose belongs to a family of classifierbased similarity scores. We present an effective way to discount pose induced similarities within such a framework, which is based on a newly introduced classifier called SVMminus. The presented method is shown to outperform existing techniques on the most challenging and realistic publicly available video face recognition benchmark, both by itself, and in concert with other methods.</p><p>Reference: <a title="cvpr-2013-430-reference" href="../cvpr2013_reference/cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Luckily, a major source of confusion in visual similarity of faces is the 3D head orientation, for which image analysis tools provide an accurate estimation. [sent-2, score-0.325]
</p><p>2 The method we propose belongs to a family of classifierbased similarity scores. [sent-3, score-0.184]
</p><p>3 We present an effective way to discount pose induced similarities within such a framework, which is based on a newly introduced classifier called SVMminus. [sent-4, score-0.183]
</p><p>4 The presented method is shown to outperform existing techniques on the most challenging and realistic publicly available video face recognition benchmark, both by itself, and in concert with other methods. [sent-5, score-0.292]
</p><p>5 However, looking into future applications of face recognition, the role of video-based methods might become more and more dominant. [sent-8, score-0.157]
</p><p>6 The required technologies for video and images are obviously related, but video presents additional challenges that require a dedicated consideration. [sent-9, score-0.194]
</p><p>7 In both images and video, the most significant challenge for real-world face recognition systems might be that of head pose. [sent-10, score-0.261]
</p><p>8 When the subjects are not required to collaborate with the system, the 3D orientation of the head can cause changes in appearance within the captured faces of the same person that are larger than changes among faces of different people. [sent-11, score-0.358]
</p><p>9 Even with advanced face alignment techniques, the practical implications of pose variations seem to suppress those of other factors such as expression, illumination, and image quality. [sent-12, score-0.195]
</p><p>10 In this paper, we present a similarity score which specifically asks given two videos: how much is the face in one video sequence similar to that of the other, where this similarity is uncorrelated with the pose-induced similarity. [sent-13, score-0.681]
</p><p>11 The novel similarity score belongs to a family of classifier based similarities that were shown previously to be much more effective for face recognition in unconstrained video than all other methods in the literature, and pushes the performance envelope even further. [sent-14, score-0.741]
</p><p>12 Within the novel similarity score we employ a new learning method called SVM? [sent-15, score-0.23]
</p><p>13 positive VanMd negative exam-  ples in a way that is uncorrelated with the discriminative function learned on an additional feature set. [sent-17, score-0.153]
</p><p>14 Previous work Video face recognition is used for various tasks such as real-time face recognition [27], searching people in surveillance videos [26, 32], aligning subtitle information with faces [9, 29] and clustering by subject identity [24]. [sent-20, score-0.614]
</p><p>15 Frames of a video showing the same face are often represented as sets of vectors, one vector per frame. [sent-21, score-0.32]
</p><p>16 Thus, recognition becomes a problem of determining the similarity between vector sets, which can be modeled as distributions [26], subspaces [40], or more general manifolds [16, 25, 34]. [sent-22, score-0.188]
</p><p>17 Different choices of similarity measures are then used to compare sets [34, 35]. [sent-23, score-0.216]
</p><p>18 Algebraic methods that compare sets regard each video as a linear subspace, spanned by the vectors encoding the frames in the video. [sent-24, score-0.281]
</p><p>19 The Pyramid Match Kernel (PMK) [13] is a nonalgebraic kernel for encoding similarities between sets of  vectors, which was shown to be extremely effective in several object recognition tasks. [sent-28, score-0.17]
</p><p>20 Following the success of comprehensive face image benchmarks taken under natural conditions, out of which ’Labeled Faces in the Wild’ [15] might be the most prominent, the ‘YouTube Faces DB’ database of labeled videos of faces was presented and made available [1] . [sent-31, score-0.385]
</p><p>21 The recognition ability of a wide variety of video face recognition approaches was tested on this video dataset in [36], and compared to the Matched Background Similarity (MBGS) method suggested in that paper. [sent-32, score-0.427]
</p><p>22 3, differs from the methods mentioned above in that it employs a classifier that is trained to distinguish between the set being modeled and confusing samples from a preselected background set. [sent-34, score-0.166]
</p><p>23 Side information is used to learn the relevant structures in the data by reducing irrelevant variability while amplifying relevant variability [28]. [sent-37, score-0.182]
</p><p>24 Both relevant and irrelevant additional information can be provided as in [12, 4], where relevant structures in the data are learned by maximizing the mutual information with relevant data and minimizing mutual information with irrelevant data. [sent-38, score-0.231]
</p><p>25 The learning using privileged information (LUPI) paradigm suggested in [3 1] utilizes privileged information supplied by the teacher during the training phase. [sent-42, score-0.772]
</p><p>26 The SVM+ algorithm [22] is a LUPI classification method that is based on SVM, where the ’plus’ sign refers to the additional discriminative power gained from the privileged information. [sent-44, score-0.386]
</p><p>27 Instead, it describes a misleading factor, such as pose or lighting conditions in face images, which needs to be eliminated when considering the faces’ identities. [sent-49, score-0.231]
</p><p>28 Building classifiers that minimize correlations with other classifiers have been studied before in the context of ensemble methods [20, 19] and dimensionality reduction [17] with no privileged or side information supplied. [sent-51, score-0.547]
</p><p>29 The One-Shot Family of Similarities The similarity methods described in this section build upon the common idea of finding the association between two objects using a background set of samples. [sent-56, score-0.237]
</p><p>30 Given two vectors x1 and x2, their OSS score is computed by considering a training set of background sample vectors B. [sent-59, score-0.301]
</p><p>31 First, a discriminative model is learned with x1 as a single positive example and B as a set of background examples. [sent-61, score-0.157]
</p><p>32 In [37] an LDA classifier was used, and the score is the signed distance of x2 from the decision boundary learned using x1 (“positive” example) and B (“negative” examples). [sent-63, score-0.238]
</p><p>33 A second such score is then obtained by repeating the same process with the roles of x1 and x2 switched: this time, a model learned with x2 as the positive example is used to classify x1, thus obtaining a second classification score. [sent-64, score-0.199]
</p><p>34 Using this information, multiple background sets are considered, each such set reflecting either a different identity or a different pose. [sent-70, score-0.198]
</p><p>35 The intuition guiding MSS is that a whole background set contains variability due to a multitude of factors including pose, identity and expression while the positive sample is an image of one person captured at one pose under a particular viewing condition. [sent-74, score-0.319]
</p><p>36 When the background set contains a single person or a single pose, the classifier is more likely to distinguish based on the approximately constant factor. [sent-76, score-0.205]
</p><p>37 3) is a set-to-set similarity designed for comparing the frames of two face-videos to determine if the faces appearing in the two sets are of the same person. [sent-78, score-0.408]
</p><p>38 In order to highlight similarities of identity, a discriminative classifier is trained for the frames of each video sequence vs. [sent-79, score-0.293]
</p><p>39 a subset of background frames that are selected to best represent misleading sources of variation such as pose, lighting, and viewing conditions. [sent-80, score-0.174]
</p><p>40 This subset is selected from within a large set of background videos put aside for this purpose. [sent-81, score-0.157]
</p><p>41 Given two videos, X1 and X2, likewise represented as two sets of feature vectors in Rd, their MBGS is computed as the mean of two one-side MBGS scores obtained via the OneSideMBGS method. [sent-86, score-0.177]
</p><p>42 The OneSideMBGS method first constructs a subset of the background set B1 matching the vectors in X1. [sent-87, score-0.154]
</p><p>43 Typically, a Linear SVM classifier is used, and the confidence values are signed distances from the separating hyperplane. [sent-92, score-0.213]
</p><p>44 These confidence values are averaged and produce a single score, which is related to the likelihood that X2 represents the same person appearing in X1. [sent-93, score-0.158]
</p><p>45 The final, two-sided MBGS is obtained by repeating this process, this time reversing the roles of X1 and X2, which requires the selection of B2, a subset of the background set matching the vectors in X2. [sent-94, score-0.203]
</p><p>46 The average of the two one sided similarities is the final MBGS score computed for the video pair. [sent-95, score-0.28]
</p><p>47 The Multiple OSS method cannot be directly used in video to eliminate the pose effect, since each video contains a multitude of poses and expressions. [sent-97, score-0.272]
</p><p>48 S imi larity  = OS S ( x1  ,  ,  x2  ,  B)  Mode l = t rain ( x1 B ) 1 S im1 = cla s s i ( x2 fy Mode l 1)  ,  ,  Mode l = t rain ( x2 2 B) S im2 = clas s i ( x1 Mode l ) fy 2 S imi larity  =  ,  ( S im1+S im2 ) / 2  Figure 1. [sent-104, score-1.02]
</p><p>49 One-Shot similarity computation for two vectors, x2, given a set B of background samples. [sent-105, score-0.237]
</p><p>50 S imi larity  = MS S (x1,  x2  x1  and  , {B1, B2 , . [sent-106, score-0.279]
</p><p>51 k S im( ( i = OS S ( x1 x2 ) Bi ) end S imi larity = clas s i ( S im, SVMmode l fy )  ,  ,  Figure 2. [sent-112, score-0.481]
</p><p>52 Multi-Shot Similarity score for two vectors, x1 and x2, using k background sets B1, . [sent-113, score-0.233]
</p><p>53 The one-side similarity is taken as the mean of the calculated confidences, since this operator was shown in [36] to outperform the other operators tested: median, minimum, and maximum. [sent-120, score-0.18]
</p><p>54 mifi-, a matching osedt toakf privileged i anf troarimnaintigon se {tx {? [sent-130, score-0.386]
</p><p>55 nd cthlaes con333555222533  fidences c, and solves an SVM-like optimization problem  with the additional constraint that the confidences of the second learned model are uncorrelated with c. [sent-144, score-0.187]
</p><p>56 The additional constraint of low correlation is applied to the vectors labeled as positive (yi = +1) and to the vectors labeled as negative (yi = −1) separately. [sent-145, score-0.352]
</p><p>57 Xn o containing the vectors labeled as positive and the vectors labeled as negative respectively. [sent-150, score-0.306]
</p><p>58 Similarly, the confidences vector c is split into two vectors, cp and cn. [sent-152, score-0.191]
</p><p>59 at oiopnbetween cp and the confidence values of the positive vectors wTXp is  σw(wTXTXpcpp). [sent-156, score-0.242]
</p><p>60 Similarly, the correlation constraint between cn and the confidence values of the negative vectors added to the objective  function is (wTXncn)2. [sent-159, score-0.236]
</p><p>61 similarity between sets Xi and Xj is computed using Mth? [sent-236, score-0.216]
</p><p>62 , and a background set B with privileged information B? [sent-239, score-0.473]
</p><p>63 First, a background subset Bi is chosen from the background set B as described in Sec. [sent-241, score-0.174]
</p><p>64 classifier is trained on [Xi, Bi] and the matching privileged siniffioerrm iasti torani n[eXdi? [sent-246, score-0.465]
</p><p>65 333555222644  S = SVM-minus_S imi larity ( X1,X1? [sent-263, score-0.279]
</p><p>66 ,C ) 1 Confidence s 1 = clas s i ( X2 , Mode l fy 1) S im1 = mean ( Confidence s 1)  , , , ,  Mode l = One_S ide_SVM-minus ( X2 X2? [sent-268, score-0.202]
</p><p>67 C ) 2 Confidence s 2 = clas s i ( X1 Mode l ) fy 2 S im2 = mean ( Confidence s 2 )  ,  S = ( S im1+ S im2 ) / 2  , , , ,  Mode l = One_S ide_SVM-minus ( X X? [sent-270, score-0.202]
</p><p>68 Mode l ) fy ’ Mode l = SVM-minu s_opt imi z at i ( X y Confidence s ’ ) on  , , ,  Figure 4. [sent-280, score-0.259]
</p><p>69 similarity dto o compare aell c gallery image sonetes stiod tehde S prob set, as the prob set manifests itself frame by frame. [sent-293, score-0.224]
</p><p>70 The dataset contains a large collection of videos along with labels indicating the identity of a person appearing in each video. [sent-299, score-0.186]
</p><p>71 It also contains scripts and meta-data defining benchmark protocols for the task of video pair-matching, where given a pair of videos each tested method answers a binary same/not-same query. [sent-300, score-0.167]
</p><p>72 These 3D vectors are taken as the privileged information in the SVM? [sent-306, score-0.453]
</p><p>73 Specifically, 5, 000 video pairs from the database, half of which are pairs of videos of the same person, and half of different people were selected at random and divided into 10 splits. [sent-309, score-0.167]
</p><p>74 The splits were sampled to be subject mutually-exclusive; if videos of a subject appear in one split, no video of that subject is included in any other split. [sent-311, score-0.206]
</p><p>75 In [36], the performance of an extensive set of baseline video face recognition methods was evaluated and compared to the performance of the MBGS method. [sent-315, score-0.292]
</p><p>76 To define the background set, in each of the ten cross validation rounds, the frames of the videos of one out of the nine training splits are used. [sent-318, score-0.247]
</p><p>77 i iTohnis r score cisc computed by applying a L ouint-ear SVM classifier to the similarity scores treated as 1D feature vectors. [sent-345, score-0.353]
</p><p>78 In each of the 10 cross-validation rounds, this classifier is trained on the 8 training splits (leaving the split used for background frames aside), and applied to the 10th. [sent-355, score-0.289]
</p><p>79 similarity teo S produce similarity scores wthitaht ndif tfheer fSrVomM ? [sent-361, score-0.344]
</p><p>80 To examine this effect we have computed the correlations between the similarity scores produced by each method on the 5, 000 benchmark pairs. [sent-363, score-0.232]
</p><p>81 As can be seen, each similarity score is more similar to other similarities of the same type (MBGS or SVM? [sent-365, score-0.296]
</p><p>82 As expected, among tihtiee ss)im thialanri ttoies th oofs eth oef o ththeer o type, ytphee correlation to the similarity that is derived from the same face descriptors is the highest. [sent-367, score-0.385]
</p><p>83 5, for on-line applications of the similarity score, one might be interested in a one-sided version: when the one-sided version is used, there is no need to retrain the underlying classifiers given the new video, and the score can be computed incrementally one frame at a time. [sent-379, score-0.274]
</p><p>84 similarity was given a score of 5, 000 minus the ranking siti ombiltaairniteyd using LivBePn- aba ssceodr eMoB fG 5S,. [sent-395, score-0.3]
</p><p>85 05 were the ones between the FPLBP ranking or the LBP ranking and the measured variance of the yaw head orientation angle (p-values of 0. [sent-408, score-0.177]
</p><p>86 The three rows correspond to the three face descriptors: CSLBP, FPLBP, and LBP. [sent-413, score-0.157]
</p><p>87 First and foremost is the intuitive expectation that face recognition in video should be at least as accurate as image-based face recognition. [sent-426, score-0.449]
</p><p>88 ) issues such as video resolution and compression artifacts, we believe that the additional information in video should be more than enough to compensate for these. [sent-428, score-0.194]
</p><p>89 Initial approaches for face recognition in video were based on the linear subspace or manifold models. [sent-429, score-0.292]
</p><p>90 More generally, the problem of comparing sets of vectors is a corner stone in modern object recognition, where PMK and LLC have been shown to provide excellent results when  applied to sets of image descriptors. [sent-431, score-0.199]
</p><p>91 However, algorithms designed for large sets of local pieces of information are not effective for the problem at hand, which is characterized by smaller sets of very informative vectors containing a large amount of overlapping information. [sent-432, score-0.199]
</p><p>92 In this work we rely on the fact that the most prominent confounding factor the 3D head orientation is observable, and derive a new similarity score which discounts the spurious likeness that is induced by pose similarity. [sent-436, score-0.454]
</p><p>93 We note that in contrast to the conventional privileged knowledge scenario, the side information is available but unused even when the SVM? [sent-440, score-0.421]
</p><p>94 i / ˜wo l / l f  [2]  [3] [4] [5] [6] [7]  [8]  [9]  [10]  [11] [12] [13]  yt face s . [sent-452, score-0.188]
</p><p>95 Labeled faces in the wild: A database for studying face recognition in unconstrained environments. [sent-550, score-0.344]
</p><p>96 Unsupervised face recognition from image sequences based on clustering with attraction and repulsion. [sent-610, score-0.195]
</p><p>97 A unified learning framework for real time face detection and classification. [sent-622, score-0.157]
</p><p>98 Manifold-manifold distance with application to face recognition based on image set. [sent-674, score-0.195]
</p><p>99 Kernel grassmannian distances and discriminant analysis for face recognition from image sets. [sent-679, score-0.195]
</p><p>100 Face recognition in unconstrained videos with matched background similarity. [sent-687, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mbgs', 0.428), ('privileged', 0.386), ('oss', 0.242), ('svm', 0.225), ('mode', 0.165), ('face', 0.157), ('similarity', 0.15), ('imi', 0.15), ('onesidembgs', 0.145), ('larity', 0.129), ('faces', 0.109), ('fy', 0.109), ('confidences', 0.108), ('video', 0.097), ('fplbp', 0.097), ('xncn', 0.097), ('xpcp', 0.097), ('clas', 0.093), ('background', 0.087), ('confidence', 0.087), ('score', 0.08), ('classifier', 0.079), ('lbp', 0.074), ('lupi', 0.072), ('pmk', 0.072), ('videos', 0.07), ('vectors', 0.067), ('head', 0.066), ('bx', 0.066), ('sets', 0.066), ('similarities', 0.066), ('wolf', 0.061), ('rain', 0.059), ('db', 0.057), ('youtube', 0.056), ('frames', 0.051), ('bi', 0.05), ('cp', 0.05), ('repeating', 0.049), ('labeled', 0.049), ('chechik', 0.048), ('cmsm', 0.048), ('cslbp', 0.048), ('discounts', 0.048), ('ighbors', 0.048), ('sev', 0.048), ('sevt', 0.048), ('ssv', 0.048), ('trhoeb', 0.048), ('vanmd', 0.048), ('wtxp', 0.048), ('ytxta', 0.048), ('signed', 0.047), ('uncorrelated', 0.047), ('correlation', 0.046), ('identity', 0.045), ('classifiers', 0.044), ('irrelevant', 0.044), ('scores', 0.044), ('hassner', 0.043), ('llt', 0.043), ('unconstrained', 0.04), ('multitude', 0.04), ('im', 0.039), ('lx', 0.039), ('splits', 0.039), ('person', 0.039), ('ranking', 0.038), ('positive', 0.038), ('recognition', 0.038), ('auc', 0.038), ('pose', 0.038), ('correlations', 0.038), ('relevant', 0.037), ('levy', 0.037), ('sided', 0.037), ('confounding', 0.037), ('prob', 0.037), ('negative', 0.036), ('gesture', 0.036), ('misleading', 0.036), ('algebraic', 0.036), ('orientation', 0.035), ('side', 0.035), ('family', 0.034), ('cla', 0.033), ('split', 0.033), ('os', 0.032), ('wild', 0.032), ('variability', 0.032), ('xj', 0.032), ('minus', 0.032), ('shakhnarovich', 0.032), ('learned', 0.032), ('descriptors', 0.032), ('appearing', 0.032), ('xi', 0.031), ('yt', 0.031), ('operator', 0.03), ('xp', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="430-tfidf-1" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>Author: Lior Wolf, Noga Levy</p><p>Abstract: Face recognition in unconstrained videos requires specialized tools beyond those developed for still images: the fact that the confounding factors change state during the video sequence presents a unique challenge, but also an opportunity to eliminate spurious similarities. Luckily, a major source of confusion in visual similarity of faces is the 3D head orientation, for which image analysis tools provide an accurate estimation. The method we propose belongs to a family of classifierbased similarity scores. We present an effective way to discount pose induced similarities within such a framework, which is based on a newly introduced classifier called SVMminus. The presented method is shown to outperform existing techniques on the most challenging and realistic publicly available video face recognition benchmark, both by itself, and in concert with other methods.</p><p>2 0.183612 <a title="430-tfidf-2" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>3 0.17572893 <a title="430-tfidf-3" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>Author: Baoyuan Wu, Yifan Zhang, Bao-Gang Hu, Qiang Ji</p><p>Abstract: In this paper, we focus on face clustering in videos. Given the detected faces from real-world videos, we partition all faces into K disjoint clusters. Different from clustering on a collection of facial images, the faces from videos are organized as face tracks and the frame index of each face is also provided. As a result, many pairwise constraints between faces can be easily obtained from the temporal and spatial knowledge of the face tracks. These constraints can be effectively incorporated into a generative clustering model based on the Hidden Markov Random Fields (HMRFs). Within the HMRF model, the pairwise constraints are augmented by label-level and constraint-level local smoothness to guide the clustering process. The parameters for both the unary and the pairwise potential functions are learned by the simulated field algorithm, and the weights of constraints can be easily adjusted. We further introduce an efficient clustering framework specially for face clustering in videos, considering that faces in adjacent frames of the same face track are very similar. The framework is applicable to other clustering algorithms to significantly reduce the computational cost. Experiments on two face data sets from real-world videos demonstrate the significantly improved performance of our algorithm over state-of-theart algorithms.</p><p>4 0.17438352 <a title="430-tfidf-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.15738335 <a title="430-tfidf-5" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>6 0.15366393 <a title="430-tfidf-6" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>7 0.12695375 <a title="430-tfidf-7" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>8 0.11728413 <a title="430-tfidf-8" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>9 0.1113534 <a title="430-tfidf-9" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>10 0.1067673 <a title="430-tfidf-10" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>11 0.10414089 <a title="430-tfidf-11" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>12 0.10308319 <a title="430-tfidf-12" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>13 0.10239583 <a title="430-tfidf-13" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>14 0.09699247 <a title="430-tfidf-14" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>15 0.087735198 <a title="430-tfidf-15" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>16 0.0866841 <a title="430-tfidf-16" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>17 0.086011529 <a title="430-tfidf-17" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>18 0.083679892 <a title="430-tfidf-18" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>19 0.083170146 <a title="430-tfidf-19" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>20 0.081782602 <a title="430-tfidf-20" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.223), (1, -0.089), (2, -0.065), (3, -0.019), (4, 0.032), (5, 0.005), (6, -0.04), (7, -0.109), (8, 0.121), (9, -0.096), (10, 0.058), (11, -0.063), (12, 0.069), (13, 0.007), (14, -0.061), (15, -0.032), (16, -0.011), (17, -0.066), (18, -0.048), (19, -0.05), (20, -0.065), (21, 0.031), (22, -0.022), (23, 0.012), (24, -0.018), (25, -0.0), (26, -0.02), (27, 0.091), (28, -0.01), (29, -0.044), (30, 0.007), (31, 0.058), (32, 0.033), (33, 0.045), (34, 0.052), (35, 0.054), (36, -0.005), (37, 0.013), (38, -0.006), (39, -0.027), (40, 0.066), (41, 0.006), (42, -0.011), (43, -0.026), (44, -0.07), (45, 0.032), (46, 0.026), (47, 0.042), (48, 0.017), (49, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96683872 <a title="430-lsi-1" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>Author: Lior Wolf, Noga Levy</p><p>Abstract: Face recognition in unconstrained videos requires specialized tools beyond those developed for still images: the fact that the confounding factors change state during the video sequence presents a unique challenge, but also an opportunity to eliminate spurious similarities. Luckily, a major source of confusion in visual similarity of faces is the 3D head orientation, for which image analysis tools provide an accurate estimation. The method we propose belongs to a family of classifierbased similarity scores. We present an effective way to discount pose induced similarities within such a framework, which is based on a newly introduced classifier called SVMminus. The presented method is shown to outperform existing techniques on the most challenging and realistic publicly available video face recognition benchmark, both by itself, and in concert with other methods.</p><p>2 0.85535628 <a title="430-lsi-2" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>3 0.85531861 <a title="430-lsi-3" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>Author: Martin Bäuml, Makarand Tapaswi, Rainer Stiefelhagen</p><p>Abstract: We address the problem of person identification in TV series. We propose a unified learning framework for multiclass classification which incorporates labeled and unlabeled data, and constraints between pairs of features in the training. We apply the framework to train multinomial logistic regression classifiers for multi-class face recognition. The method is completely automatic, as the labeled data is obtained by tagging speaking faces using subtitles and fan transcripts of the videos. We demonstrate our approach on six episodes each of two diverse TV series and achieve state-of-the-art performance.</p><p>4 0.84543407 <a title="430-lsi-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.8279987 <a title="430-lsi-5" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>6 0.75689858 <a title="430-lsi-6" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>7 0.72630483 <a title="430-lsi-7" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>8 0.70318252 <a title="430-lsi-8" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>9 0.70062643 <a title="430-lsi-9" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>10 0.69895673 <a title="430-lsi-10" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>11 0.69817847 <a title="430-lsi-11" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>12 0.69074255 <a title="430-lsi-12" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>13 0.68155104 <a title="430-lsi-13" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>14 0.6500206 <a title="430-lsi-14" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>15 0.64097488 <a title="430-lsi-15" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>16 0.63215542 <a title="430-lsi-16" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>17 0.63107198 <a title="430-lsi-17" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>18 0.61450332 <a title="430-lsi-18" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>19 0.61377829 <a title="430-lsi-19" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>20 0.61206317 <a title="430-lsi-20" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.106), (16, 0.037), (26, 0.039), (28, 0.017), (33, 0.243), (39, 0.012), (59, 0.012), (67, 0.085), (68, 0.235), (69, 0.053), (87, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83255965 <a title="430-lda-1" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>Author: Shihui Ying, Guorong Wu, Qian Wang, Dinggang Shen</p><p>Abstract: Recently, groupwise registration has been investigated for simultaneous alignment of all images without selecting any individual image as the template, thus avoiding the potential bias in image registration. However, none of current groupwise registration method fully utilizes the image distribution to guide the registration. Thus, the registration performance usually suffers from large inter-subject variations across individual images. To solve this issue, we propose a novel groupwise registration algorithm for large population dataset, guided by the image distribution on the manifold. Specifically, we first use a graph to model the distribution of all image data sitting on the image manifold, with each node representing an image and each edge representing the geodesic pathway between two nodes (or images). Then, the procedure of warping all images to theirpopulation center turns to the dynamic shrinking ofthe graph nodes along their graph edges until all graph nodes become close to each other. Thus, the topology ofimage distribution on the image manifold is always preserved during the groupwise registration. More importantly, by modeling , the distribution of all images via a graph, we can potentially reduce registration error since every time each image is warped only according to its nearby images with similar structures in the graph. We have evaluated our proposed groupwise registration method on both synthetic and real datasets, with comparison to the two state-of-the-art groupwise registration methods. All experimental results show that our proposed method achieves the best performance in terms of registration accuracy and robustness.</p><p>same-paper 2 0.81512797 <a title="430-lda-2" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>Author: Lior Wolf, Noga Levy</p><p>Abstract: Face recognition in unconstrained videos requires specialized tools beyond those developed for still images: the fact that the confounding factors change state during the video sequence presents a unique challenge, but also an opportunity to eliminate spurious similarities. Luckily, a major source of confusion in visual similarity of faces is the 3D head orientation, for which image analysis tools provide an accurate estimation. The method we propose belongs to a family of classifierbased similarity scores. We present an effective way to discount pose induced similarities within such a framework, which is based on a newly introduced classifier called SVMminus. The presented method is shown to outperform existing techniques on the most challenging and realistic publicly available video face recognition benchmark, both by itself, and in concert with other methods.</p><p>3 0.79231513 <a title="430-lda-3" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>Author: Chen Li, Shuochen Su, Yasuyuki Matsushita, Kun Zhou, Stephen Lin</p><p>Abstract: We present a method that enhances the performance of depth-from-defocus (DFD) through the use of shading information. DFD suffers from important limitations namely coarse shape reconstruction and poor accuracy on textureless surfaces that can be overcome with the help of shading. We integrate both forms of data within a Bayesian framework that capitalizes on their relative strengths. Shading data, however, is challenging to recover accurately from surfaces that contain texture. To address this issue, we propose an iterative technique that utilizes depth information to improve shading estimation, which in turn is used to elevate depth estimation in the presence of textures. With this approach, we demonstrate improvements over existing DFD techniques, as well as effective shape reconstruction of textureless surfaces. – –</p><p>4 0.77411091 <a title="430-lda-4" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>5 0.77078295 <a title="430-lda-5" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>Author: Brandon Rothrock, Seyoung Park, Song-Chun Zhu</p><p>Abstract: In this paper we present a compositional and-or graph grammar model for human pose estimation. Our model has three distinguishing features: (i) large appearance differences between people are handled compositionally by allowingparts or collections ofparts to be substituted with alternative variants, (ii) each variant is a sub-model that can define its own articulated geometry and context-sensitive compatibility with neighboring part variants, and (iii) background region segmentation is incorporated into the part appearance models to better estimate the contrast of a part region from its surroundings, and improve resilience to background clutter. The resulting integrated framework is trained discriminatively in a max-margin framework using an efficient and exact inference algorithm. We present experimental evaluation of our model on two popular datasets, and show performance improvements over the state-of-art on both benchmarks.</p><p>6 0.76926214 <a title="430-lda-6" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>7 0.76901793 <a title="430-lda-7" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>8 0.76891953 <a title="430-lda-8" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>9 0.76847023 <a title="430-lda-9" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>10 0.76620662 <a title="430-lda-10" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>11 0.76612979 <a title="430-lda-11" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>12 0.76612937 <a title="430-lda-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.76598608 <a title="430-lda-13" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>14 0.76589602 <a title="430-lda-14" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>15 0.76544327 <a title="430-lda-15" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>16 0.76541173 <a title="430-lda-16" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>17 0.76532084 <a title="430-lda-17" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>18 0.76508367 <a title="430-lda-18" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>19 0.76502693 <a title="430-lda-19" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>20 0.76493049 <a title="430-lda-20" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
