<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-251" href="#">cvpr2013-251</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</h1>
<br/><p>Source: <a title="cvpr-2013-251-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Liu_Learning_Discriminative_Illumination_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chao Liu, Geifei Yang, Jinwei Gu</p><p>Abstract: We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials, such as wood, fabric, and granite. At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.</p><p>Reference: <a title="cvpr-2013-251-reference" href="../cvpr2013_reference/cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. [sent-3, score-0.333]
</p><p>2 To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. [sent-4, score-0.552]
</p><p>3 We also studied the effects of texture rotation and scale variation for material classification. [sent-5, score-0.453]
</p><p>4 We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification. [sent-6, score-0.424]
</p><p>5 , fabric, wood, granite) and others exhibits texture at appropriate scales (e. [sent-10, score-0.238]
</p><p>6 Subsets of this function have been utilized previously as “3D Textons” for texture classification [5, 17]. [sent-15, score-0.327]
</p><p>7 The 3D texton approach has shown higher performance for classification compared to conventional texture classification [3 1, 32]. [sent-16, score-0.544]
</p><p>8 Most prior work uses the responses to a set of filters (e. [sent-20, score-0.333]
</p><p>9 Finding the optimal filters is expected to reduce the dimensionality of the classification task and thus increase performance. [sent-25, score-0.468]
</p><p>10 Motivated by these two questions, in this paper, we present an efficient method to directly capture discriminative features of BTFs for material classification with computational lighting and filters optimized for given classification tasks. [sent-26, score-0.821]
</p><p>11 (1) To reduce the acquisition cost, we observe that each measured image is a linear projection of the BTFs to the illumination pattern. [sent-27, score-0.245]
</p><p>12 Thus, by appropriately setting the illumination pattern, we can directly capture discrimina-  tive projections of BTFs without actually measuring the full BTFs. [sent-28, score-0.262]
</p><p>13 (2) To find good filters for texture classification, we model the filters as linear combinations from a general filter dictionary and optimize the weights by minimizing a discriminant functional. [sent-29, score-0.846]
</p><p>14 Both the optimal illumination pattern — called “discriminative illumination”— and the weights for filters are optimized iteratively to maximize the distance between different materials. [sent-30, score-0.568]
</p><p>15 Our method is closely related to two recent works [9, 13] which also seek optimal illumination for material classification. [sent-31, score-0.433]
</p><p>16 However, they focused on point-wise material classification using BRDF features only. [sent-32, score-0.325]
</p><p>17 Nevertheless, similar to these prior works, our method also has signal-to-noise benefit due to illumination multiplexing and can also be extended for multi-class and nonlinear classification by using multiple discriminative illumination patterns. [sent-35, score-0.589]
</p><p>18 Since BTFs are generally neither rotational-invariant nor scale-invariant, we also studied the effects of rotation and scale to texture classification. [sent-36, score-0.262]
</p><p>19 Experimental results showed that the optimal illumination and filters learned from this “augmented” training set is much more stable to rotation and scale change for texture  ×  ×  classification. [sent-39, score-0.858]
</p><p>20 We evaluated the proposed method for a variety of materials classification tasks. [sent-58, score-0.274]
</p><p>21 We also compared with prior work on texture classification [3 1] and BRDF classification [9]. [sent-59, score-0.482]
</p><p>22 Related Work Material Classification There are two categories of prior work on material classification in computer vision. [sent-62, score-0.346]
</p><p>23 The passive approach aims to study material perception in order to classify materials from regular images via statistical learning [1, 21, 18]. [sent-63, score-0.396]
</p><p>24 The active approach, especially in machine vision, employs any useful visual features for classification,  such as 2D slices of BRDFs [34, 13], BRDF projections [9], polarization [3], and spectral reflectance [10, 26]. [sent-64, score-0.247]
</p><p>25 To deal with the change of orientation and scale, researchers either estimated the dominant orientation/scale of a texture image [16, 4, 3 1], or utilized rotational-invariant or scaleinvariant features [24, 15, 20, 25]. [sent-67, score-0.305]
</p><p>26 Another body of prior work focused on optimizing filters for texture classification [19, 11, 29]. [sent-68, score-0.633]
</p><p>27 Unlike these prior works that deal with single texture images, we focus on BTF classification. [sent-69, score-0.238]
</p><p>28 Learning Discriminative Filters and Illumination for BTF Classification  Let us define the spectral BTF as a 7D reflectance field f(p, ωi, ωo, λ) with incident light in the direction of ωi, and reflected light in the direction in ωo at wavelength λ for a pixel at point p. [sent-73, score-0.347]
</p><p>29 λ,ωif(p,ωi,ωo,λ)L(ωi,λ)V (ωi)S(λ)dλdωi, (1) where L(ωi , λ) is the incident light in the direction ωi at wavelength λ, V (ωi) = max(0, cos θi) is the visibility term, and S(λ) is the camera spectral response function. [sent-77, score-0.267]
</p><p>30 Since the appearance of the texture is linear with respect to the incident illumination, the vector of filter responses will also be linear with respect to the incident illumination. [sent-79, score-0.396]
</p><p>31 Therefore, we can employ a similar approach as in [9] to learn discriminative illumination as optimal projection directions for texture classification. [sent-80, score-0.535]
</p><p>32 We note I(p) is a dot product between the apparent BTF feature vector of pixel p [fˆ(p, j) = f(p, ωi , ωo, λ)V (ωi)S(λ)] and the incident illumination [Lj], j = 1, · · · , M. [sent-96, score-0.254]
</p><p>33 · w, ··· ,LM]T  (3)  where w = [L1, is a vector of the illumination, A is an N K m,·a·tr·ix , consisting of the coefficients of the K filters (i. [sent-101, score-0.285]
</p><p>34 i Tgh oef illumination vector w projects the original texture feature AT · FT of dimension K M to a feature vector r in a lowe·r Fdimeonfs diiomnaeln subspace ×o fM Mdim toe ans ifoenat uKre × v e1c. [sent-106, score-0.386]
</p><p>35 ad of using a set of predefined filters, we solve for optimal filters (together with solving for optimal illumination) by maximizing some discriminant functional (e. [sent-109, score-0.383]
</p><p>36 We model each filter in A as a linear combination of basis filters from a general dictionary B consisting of Gabor filters, S-filters, and maximal  αi(j)  αi(j)  response filters. [sent-112, score-0.432]
</p><p>37 We have A = BW, with vectorized basis filters in columns of B and weights of basis filters in columns of the weights matrix W. [sent-113, score-0.636]
</p><p>38 Thus we have r = WTBTFTw = WTRTw,  (4)  where R = FB is the filter response matrix for the basis filters, with Rij being the filter response of the i-th basis filter under the j-th light source. [sent-114, score-0.342]
</p><p>39 The optimal illumination w and filters W for the classification task are learned by maximizing the between-class distance while minimizing the within-class distance (with a unit norm constraint on w to avoid scale ambiguity), similar to LDA. [sent-116, score-0.724]
</p><p>40 1  where ri,c is the texture descriptor for the i-th sample in the c-th class, rc is the average descriptor for class c, and  r is the average descriptor for all samples in the C classes. [sent-130, score-0.307]
</p><p>41 Figure 2: Texture classification with discriminative illumination and filters. [sent-181, score-0.375]
</p><p>42 (a) Classifying aluminum and stainless steel under conventional lighting with regular color camera is challenging, since they have similar color and gloss. [sent-182, score-0.612]
</p><p>43 (b) We proposed to capture projections of BTFs for material classification with coded illumination, implemented as a LED-based multispectral dome. [sent-183, score-0.529]
</p><p>44 (f)(g)(h) show the classificatio−nw w rates on test data using the VZ texture classifier [3 1], BRDF Projection [9] and our method with the same number of measurements. [sent-188, score-0.247]
</p><p>45 In the classification stage, a test sample is captured when the illumination pattern is set to w∗. [sent-194, score-0.391]
</p><p>46 x tFuorer descriptor ,w withe the learned optimal filters A∗ = BW∗ for classification. [sent-197, score-0.377]
</p><p>47 Figure 2 shows a challenging example for classifying aluminum and stainless steel, which has similar color and gloss. [sent-198, score-0.534]
</p><p>48 As shown, compared to two state-of-art material classification methods (the VZ classifier [3 1] and the BRDF projections [9]), our method has better performance on the testing data, with the same number of measurements. [sent-199, score-0.424]
</p><p>49 Because of self-occlusion and inter-reflection, we often cannot approximate the texture appearance at a different orientation or scale by simply rotating or scaling a captured texture image. [sent-205, score-0.515]
</p><p>50 How does the change of orientation and scale affect the classification performance? [sent-206, score-0.263]
</p><p>51 3(a), we prepared two classes of materials which are coated with the same paint one is paper and the other is grooved clay tile. [sent-210, score-0.251]
</p><p>52 3(c), if we apply the classifier trained with one orientation to images captured at an unknown orientation, the classification rate is less than 50% for both BRDF and BTF projections. [sent-214, score-0.252]
</p><p>53 Second, we limited the basis filters in the dictionary to be rotational-invariant (e. [sent-217, score-0.347]
</p><p>54 Figure 3(b) shows the trained K = 16 filters when we use one orientation, four orientations, and 8 orientations in training, respectively. [sent-225, score-0.341]
</p><p>55 As shown, the more orientations added in training, the more directional filters are learned. [sent-226, score-0.341]
</p><p>56 In comparison, adding rotated samples does not change much the classification results of the BRDF projection method [9], as expected. [sent-227, score-0.324]
</p><p>57 (a) We prepared two materials coated with the same blue paint for material classification with texture only. [sent-279, score-0.769]
</p><p>58 (c) As expected, the accuracy of our BTF projection method increases with the number of rotated samples added to the training set, while the BRDF projection method [9] does not vary significantly. [sent-284, score-0.243]
</p><p>59 4, we aimed to classify carpet and paper  at two scales. [sent-289, score-0.31]
</p><p>60 4(g), the BTF-based classifier trained at the lower scale (f=12mm) fails to classify the carpet at the higher scale (f=50mm). [sent-293, score-0.422]
</p><p>61 (a)(b) show the images of carpet and paper captured at two different scales. [sent-363, score-0.31]
</p><p>62 (c)(e): the optimal illumination (w+, w−) and filters trained with samples in one scale. [sent-364, score-0.578]
</p><p>63 (d)(f): the optimal illumination (w+, w−) and filters trained with samples in both scales. [sent-365, score-0.578]
</p><p>64 The differences in the trained illumination and filters confirm that BTF is not scale-invariant. [sent-366, score-0.498]
</p><p>65 (g): classification results when only samples in one scale are included in the training set. [sent-367, score-0.25]
</p><p>66 (h) classification results when samples in both scales are included in training set. [sent-368, score-0.254]
</p><p>67 Figures 4(c)(d) and (e)(f) show the corresponding discriminative illumination and filters. [sent-375, score-0.241]
</p><p>68 Each cluster has six LEDs of different colors: blue, green, am9458:4  Figure 5: The LED-based multi-spectral dome for discriminative illumination and filters for BTF classification. [sent-380, score-0.684]
</p><p>69 We acquired a spectral BTF database using this dome,  × ××  covering eight classes of materials (i. [sent-385, score-0.226]
</p><p>70 6, we perform experiments for three classification tasks: aluminum vs. [sent-396, score-0.465]
</p><p>71 The middle column shows the classification rates with angular distribution of reflectance for classification, i. [sent-404, score-0.271]
</p><p>72 We found in both cases, although the performance increases with the number of colors/angular samples used, the classification rate is not as high as that if we use both spectral and angular distribution of reflectance as shown in the right column. [sent-407, score-0.363]
</p><p>73 These plots also show that six colors and 25 LED clusters are sufficient for these classification tasks. [sent-408, score-0.246]
</p><p>74 The general filter dictionary has 541 basis filters, including 13 S-filters [27], 16 circular ring filters, and 512 Gabor filters of different scales and orientations, The circular ring  filters are the filters with unit intensity at the pixels with a 111444333422  ? [sent-409, score-1.07]
</p><p>75 Figure 6: Discriminative ability of spectral BTF for material classification. [sent-498, score-0.256]
</p><p>76 We evaluate the performance of the learned discriminative illumination with different amounts of color, LED clusters and LEDs. [sent-499, score-0.292]
</p><p>77 The plots show that the spectral and angular distribution of reflectance are complimentary to each other in material classification. [sent-500, score-0.369]
</p><p>78 We evaluated the classification performance of the proposed method with respect to the filter size and the number of filters to learn. [sent-503, score-0.473]
</p><p>79 We train K = 16 filters that are linear comtboin baetio 1n9 o×f 1t9he. [sent-506, score-0.285]
</p><p>80 c Tomheiterative algorithm to learn optimal discriminative illumination and filters is usually converged in 3 to 5 iterations. [sent-508, score-0.575]
</p><p>81 The VZ classifier is a “bagof-words” method for regular texture classification with a single image as input. [sent-512, score-0.377]
</p><p>82 ×T4he9 BRDF projection uses discriminative projections of BRDF slices for material classification, which also uses a single coded image as input (implemented as the subtraction of two images since light cannot be negative). [sent-515, score-0.5]
</p><p>83 Second, is the joint-learning of the illumination and filters necessary? [sent-517, score-0.478]
</p><p>84 To answer this question, we compare with a method in which the discriminative illumination and the  filters are learned separately. [sent-518, score-0.569]
</p><p>85 Then the filters are optimized based on the illumination. [sent-520, score-0.285]
</p><p>86 Note that this method is a fairly strong competitor, because it is in fact the first iteration of our method with the initial filters being the 2D delta functions1 . [sent-521, score-0.285]
</p><p>87 Thus we conclude it is necessary to co-learn the illumination and filter patterns for classification. [sent-523, score-0.247]
</p><p>88 One well-known method in this category is the 3D texton [17], in which multiple images under different illuminations are used for texture classification. [sent-526, score-0.276]
</p><p>89 Filter responses of the stack of images at different orientations and scales are learned with the bagof-words approach and used as features for texture classification. [sent-527, score-0.343]
</p><p>90 As shown in Table 1, the 3D texton needs 150 input images and achieves the highest classification rates among all methods for some tasks. [sent-528, score-0.241]
</p><p>91 Table 2 shows the classification rates for some multi-class classification tasks, for which we use the one-vs-all scheme to generalize the binary classifiers. [sent-536, score-0.292]
</p><p>92 Conclusion and Discussions In this paper, we presented a method that uses coded illumination to directly measure projections of BTFs for material classification. [sent-540, score-0.516]
</p><p>93 Optimal illumination patterns and filters are learned via a training stage. [sent-541, score-0.524]
</p><p>94 We also proposed ways to deal with the change of orientation and scale for material classification. [sent-542, score-0.344]
</p><p>95 A polarization phase-based method for material classification in computer vision. [sent-616, score-0.354]
</p><p>96 Discrminative illumination: Per-pixel classification of raw materials based optimal projections of spectral brdfs. [sent-657, score-0.457]
</p><p>97 Spectral imaging  method for material classification and inspection of printed circuit boards. [sent-663, score-0.363]
</p><p>98 Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. [sent-800, score-0.355]
</p><p>99 A statistical approach to texture classification from single images. [sent-836, score-0.327]
</p><p>100 A statistical approach to material classification using image patch exemplars. [sent-841, score-0.325]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('btf', 0.344), ('aluminum', 0.331), ('carpet', 0.287), ('filters', 0.285), ('brdf', 0.242), ('btfs', 0.221), ('granite', 0.221), ('texture', 0.193), ('illumination', 0.193), ('material', 0.191), ('stainless', 0.181), ('wood', 0.154), ('materials', 0.14), ('classification', 0.134), ('vz', 0.125), ('plastic', 0.113), ('dome', 0.109), ('texton', 0.083), ('leds', 0.075), ('multispectral', 0.072), ('projections', 0.069), ('spectral', 0.065), ('orientation', 0.065), ('rotated', 0.064), ('coded', 0.063), ('gabor', 0.062), ('incident', 0.061), ('reflectance', 0.059), ('wavelength', 0.058), ('conference', 0.056), ('orientations', 0.056), ('proceedings', 0.054), ('angular', 0.054), ('filter', 0.054), ('light', 0.052), ('projection', 0.052), ('samples', 0.051), ('steel', 0.051), ('six', 0.049), ('optimal', 0.049), ('metal', 0.049), ('bidirectional', 0.048), ('discriminative', 0.048), ('scales', 0.045), ('gplraasnti', 0.044), ('neifeld', 0.044), ('tce', 0.044), ('pattern', 0.041), ('scale', 0.041), ('prepared', 0.039), ('sharan', 0.039), ('textured', 0.039), ('led', 0.038), ('imaging', 0.038), ('coated', 0.036), ('paint', 0.036), ('colors', 0.034), ('varma', 0.033), ('fabric', 0.033), ('bw', 0.033), ('basis', 0.033), ('response', 0.031), ('rit', 0.03), ('classifier', 0.03), ('polarization', 0.029), ('leung', 0.029), ('dictionary', 0.029), ('clusters', 0.029), ('lighting', 0.029), ('rotation', 0.028), ('scatter', 0.028), ('bar', 0.028), ('responses', 0.027), ('cis', 0.027), ('ring', 0.027), ('transactions', 0.026), ('international', 0.026), ('imaged', 0.025), ('dat', 0.025), ('slices', 0.025), ('rates', 0.024), ('deal', 0.024), ('effect', 0.024), ('surface', 0.024), ('training', 0.024), ('optics', 0.024), ('lj', 0.023), ('classify', 0.023), ('change', 0.023), ('captured', 0.023), ('classifying', 0.022), ('perception', 0.022), ('learned', 0.022), ('database', 0.021), ('descriptor', 0.021), ('lda', 0.021), ('answer', 0.021), ('prior', 0.021), ('regular', 0.02), ('confirm', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="251-tfidf-1" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>Author: Chao Liu, Geifei Yang, Jinwei Gu</p><p>Abstract: We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials, such as wood, fabric, and granite. At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.</p><p>2 0.2111468 <a title="251-tfidf-2" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>Author: Jirí Filip, Radomír Vávra, Michal Haindl, Pavel Žid, Mikuláš Krupika, Vlastimil Havran</p><p>Abstract: In this paper we introduce unique publicly available dense anisotropic BRDF data measurements. We use this dense data as a reference for performance evaluation of the proposed BRDF sparse angular sampling and interpolation approach. The method is based on sampling of BRDF subspaces at fixed elevations by means of several adaptively-represented, uniformly distributed, perpendicular slices. Although this proposed method requires only a sparse sampling of material, the interpolation provides a very accurate reconstruction, visually and computationally comparable to densely measured reference. Due to the simple slices measurement and method’s robustness it allows for a highly accurate acquisition of BRDFs. This in comparison with standard uniform angular sampling, is considerably faster yet uses far less samples.</p><p>3 0.19647835 <a title="251-tfidf-3" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>Author: Manmohan Chandraker, Dikpal Reddy, Yizhou Wang, Ravi Ramamoorthi</p><p>Abstract: We present a theory that addresses the problem of determining shape from the (small or differential) motion of an object with unknown isotropic reflectance, under arbitrary unknown distant illumination, , for both orthographic and perpsective projection. Our theory imposes fundamental limits on the hardness of surface reconstruction, independent of the method involved. Under orthographic projection, we prove that three differential motions suffice to yield an invariant that relates shape to image derivatives, regardless of BRDF and illumination. Under perspective projection, we show that four differential motions suffice to yield depth and a linear constraint on the surface gradient, with unknown BRDF and lighting. Further, we delineate the topological classes up to which reconstruction may be achieved using the invariants. Finally, we derive a general stratification that relates hardness of shape recovery to scene complexity. Qualitatively, our invariants are homogeneous partial differential equations for simple lighting and inhomogeneous for complex illumination. Quantitatively, our framework shows that the minimal number of motions required to resolve shape is greater for more complex scenes. Prior works that assume brightness constancy, Lambertian BRDF or a known directional light source follow as special cases of our stratification. We illustrate with synthetic and real data how potential reconstruction methods may exploit our framework.</p><p>4 0.19563073 <a title="251-tfidf-4" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>Author: Roberto Rigamonti, Amos Sironi, Vincent Lepetit, Pascal Fua</p><p>Abstract: Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-theart methods on the linear structure extraction task, in terms ofboth accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions.</p><p>5 0.19409591 <a title="251-tfidf-5" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>Author: Zhe Wu, Ping Tan</p><p>Abstract: Under unknown directional lighting, the uncalibrated Lambertian photometric stereo algorithm recovers the shape of a smooth surface up to the generalized bas-relief (GBR) ambiguity. We resolve this ambiguity from the halfvector symmetry, which is observed in many isotropic materials. Under this symmetry, a 2D BRDF slice with low-rank structure can be obtained from an image, if the surface normals and light directions are correctly recovered. In general, this structure is destroyed by the GBR ambiguity. As a result, we can resolve the ambiguity by restoring this structure. We develop a simple algorithm of auto-calibration from separable homogeneous specular reflection of real images. Compared with previous methods, this method takes a holistic approach to exploiting reflectance symmetry and produces superior results.</p><p>6 0.15476337 <a title="251-tfidf-6" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>7 0.12523259 <a title="251-tfidf-7" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>8 0.11589535 <a title="251-tfidf-8" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>9 0.1141239 <a title="251-tfidf-9" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>10 0.11083844 <a title="251-tfidf-10" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>11 0.10885327 <a title="251-tfidf-11" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>12 0.098890074 <a title="251-tfidf-12" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>13 0.092882283 <a title="251-tfidf-13" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>14 0.088669807 <a title="251-tfidf-14" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>15 0.086950153 <a title="251-tfidf-15" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>16 0.085279092 <a title="251-tfidf-16" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>17 0.084557809 <a title="251-tfidf-17" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>18 0.076504722 <a title="251-tfidf-18" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>19 0.074611224 <a title="251-tfidf-19" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>20 0.069462918 <a title="251-tfidf-20" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.058), (2, -0.041), (3, 0.09), (4, -0.009), (5, -0.064), (6, -0.076), (7, 0.02), (8, 0.017), (9, -0.028), (10, -0.088), (11, -0.113), (12, -0.038), (13, -0.113), (14, 0.054), (15, 0.095), (16, 0.154), (17, -0.135), (18, 0.067), (19, -0.018), (20, 0.077), (21, 0.016), (22, -0.008), (23, -0.01), (24, -0.024), (25, -0.042), (26, -0.03), (27, 0.003), (28, -0.035), (29, 0.029), (30, -0.163), (31, 0.002), (32, -0.051), (33, -0.128), (34, -0.033), (35, 0.055), (36, -0.01), (37, 0.088), (38, -0.033), (39, 0.071), (40, -0.047), (41, 0.017), (42, 0.059), (43, -0.024), (44, -0.101), (45, -0.115), (46, -0.016), (47, -0.0), (48, 0.044), (49, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91780233 <a title="251-lsi-1" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>Author: Chao Liu, Geifei Yang, Jinwei Gu</p><p>Abstract: We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials, such as wood, fabric, and granite. At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.</p><p>2 0.78905755 <a title="251-lsi-2" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>Author: Jirí Filip, Radomír Vávra, Michal Haindl, Pavel Žid, Mikuláš Krupika, Vlastimil Havran</p><p>Abstract: In this paper we introduce unique publicly available dense anisotropic BRDF data measurements. We use this dense data as a reference for performance evaluation of the proposed BRDF sparse angular sampling and interpolation approach. The method is based on sampling of BRDF subspaces at fixed elevations by means of several adaptively-represented, uniformly distributed, perpendicular slices. Although this proposed method requires only a sparse sampling of material, the interpolation provides a very accurate reconstruction, visually and computationally comparable to densely measured reference. Due to the simple slices measurement and method’s robustness it allows for a highly accurate acquisition of BRDFs. This in comparison with standard uniform angular sampling, is considerably faster yet uses far less samples.</p><p>3 0.7678656 <a title="251-lsi-3" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>Author: Zhe Wu, Ping Tan</p><p>Abstract: Under unknown directional lighting, the uncalibrated Lambertian photometric stereo algorithm recovers the shape of a smooth surface up to the generalized bas-relief (GBR) ambiguity. We resolve this ambiguity from the halfvector symmetry, which is observed in many isotropic materials. Under this symmetry, a 2D BRDF slice with low-rank structure can be obtained from an image, if the surface normals and light directions are correctly recovered. In general, this structure is destroyed by the GBR ambiguity. As a result, we can resolve the ambiguity by restoring this structure. We develop a simple algorithm of auto-calibration from separable homogeneous specular reflection of real images. Compared with previous methods, this method takes a holistic approach to exploiting reflectance symmetry and produces superior results.</p><p>4 0.70518029 <a title="251-lsi-4" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>Author: Antony Lam, Imari Sato</p><p>Abstract: Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally, we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.</p><p>5 0.64125377 <a title="251-lsi-5" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>Author: Shireen Y. Elhabian, Aly A. Farag</p><p>Abstract: Conventional subspace construction approaches suffer from the need of “large-enough ” image ensemble rendering numerical methods intractable. In this paper, we propose an analytic formulation for low-dimensional subspace construction in which shading cues lie while preserving the natural structure of an image sample. Using the frequencyspace representation of the image irradiance equation, the process of finding such subspace is cast as establishing a relation between its principal components and that of a deterministic set of basis functions, termed as irradiance harmonics. Representing images as matrices further lessen the number of parameters to be estimated to define a bilinear projection which maps the image sample to a lowerdimensional bilinear subspace. Results show significant impact on dimensionality reduction with minimal loss of information as well as robustness against noise.</p><p>6 0.63375533 <a title="251-lsi-6" href="./cvpr-2013-Learning_Separable_Filters.html">255 cvpr-2013-Learning Separable Filters</a></p>
<p>7 0.61386734 <a title="251-lsi-7" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>8 0.60039592 <a title="251-lsi-8" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>9 0.59662414 <a title="251-lsi-9" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>10 0.5564816 <a title="251-lsi-10" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>11 0.54667878 <a title="251-lsi-11" href="./cvpr-2013-Real-Time_No-Reference_Image_Quality_Assessment_Based_on_Filter_Learning.html">346 cvpr-2013-Real-Time No-Reference Image Quality Assessment Based on Filter Learning</a></p>
<p>12 0.52654928 <a title="251-lsi-12" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>13 0.48716268 <a title="251-lsi-13" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>14 0.48116192 <a title="251-lsi-14" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>15 0.45087487 <a title="251-lsi-15" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>16 0.44415495 <a title="251-lsi-16" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>17 0.43048784 <a title="251-lsi-17" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>18 0.40952191 <a title="251-lsi-18" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>19 0.39694616 <a title="251-lsi-19" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>20 0.39624491 <a title="251-lsi-20" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.283), (10, 0.083), (16, 0.042), (26, 0.037), (33, 0.275), (67, 0.054), (69, 0.038), (80, 0.011), (87, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83038038 <a title="251-lda-1" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>Author: Jirí Filip, Radomír Vávra, Michal Haindl, Pavel Žid, Mikuláš Krupika, Vlastimil Havran</p><p>Abstract: In this paper we introduce unique publicly available dense anisotropic BRDF data measurements. We use this dense data as a reference for performance evaluation of the proposed BRDF sparse angular sampling and interpolation approach. The method is based on sampling of BRDF subspaces at fixed elevations by means of several adaptively-represented, uniformly distributed, perpendicular slices. Although this proposed method requires only a sparse sampling of material, the interpolation provides a very accurate reconstruction, visually and computationally comparable to densely measured reference. Due to the simple slices measurement and method’s robustness it allows for a highly accurate acquisition of BRDFs. This in comparison with standard uniform angular sampling, is considerably faster yet uses far less samples.</p><p>2 0.8037824 <a title="251-lda-2" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>Author: Jan D. Wegner, Javier A. Montoya-Zegarra, Konrad Schindler</p><p>Abstract: The aim of this work is to extract the road network from aerial images. What makes the problem challenging is the complex structure of the prior: roads form a connected network of smooth, thin segments which meet at junctions and crossings. This type of a-priori knowledge is more difficult to turn into a tractable model than standard smoothness or co-occurrence assumptions. We develop a novel CRF formulation for road labeling, in which the prior is represented by higher-order cliques that connect sets of superpixels along straight line segments. These long-range cliques have asymmetric PN-potentials, which express a preference to assign all rather than just some of their constituent superpixels to the road class. Thus, the road likelihood is amplified for thin chains of superpixels, while the CRF is still amenable to optimization with graph cuts. Since the number of such cliques of arbitrary length is huge, we furthermorepropose a sampling scheme which concentrates on those cliques which are most relevant for the optimization. In experiments on two different databases the model significantly improves both the per-pixel accuracy and the topological correctness of the extracted roads, and outper- forms both a simple smoothness prior and heuristic rulebased road completion.</p><p>same-paper 3 0.79805779 <a title="251-lda-3" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>Author: Chao Liu, Geifei Yang, Jinwei Gu</p><p>Abstract: We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials, such as wood, fabric, and granite. At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.</p><p>4 0.75267869 <a title="251-lda-4" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<p>Author: Yujia Li, Daniel Tarlow, Richard Zemel</p><p>Abstract: When modeling structured outputs such as image segmentations, prediction can be improved by accurately modeling structure present in the labels. A key challenge is developing tractable models that are able to capture complex high level structure like shape. In this work, we study the learning of a general class of pattern-like high order potential, which we call Compositional High Order Pattern Potentials (CHOPPs). We show that CHOPPs include the linear deviation pattern potentials of Rother et al. [26] and also Restricted Boltzmann Machines (RBMs); we also establish the near equivalence of these two models. Experimentally, we show that performance is affected significantly by the degree of variability present in the datasets, and we define a quantitative variability measure to aid in studying this. We then improve CHOPPs performance in high variability datasets with two primary contributions: (a) developing a loss-sensitive joint learning procedure, so that internal pattern parameters can be learned in conjunction with other model potentials to minimize expected loss;and (b) learning an image-dependent mapping that encourages or inhibits patterns depending on image features. We also explore varying how multiple patterns are composed, and learning convolutional patterns. Quantitative results on challenging highly variable datasets show that the joint learning and image-dependent high order potentials can improve performance.</p><p>5 0.74931693 <a title="251-lda-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.72783482 <a title="251-lda-6" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>7 0.72782993 <a title="251-lda-7" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>8 0.72760808 <a title="251-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.72719526 <a title="251-lda-9" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>10 0.72699022 <a title="251-lda-10" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>11 0.72691619 <a title="251-lda-11" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>12 0.72650754 <a title="251-lda-12" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>13 0.72623515 <a title="251-lda-13" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>14 0.72615063 <a title="251-lda-14" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>15 0.72568184 <a title="251-lda-15" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>16 0.725631 <a title="251-lda-16" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>17 0.72561127 <a title="251-lda-17" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>18 0.72534317 <a title="251-lda-18" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>19 0.7253328 <a title="251-lda-19" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>20 0.72526968 <a title="251-lda-20" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
