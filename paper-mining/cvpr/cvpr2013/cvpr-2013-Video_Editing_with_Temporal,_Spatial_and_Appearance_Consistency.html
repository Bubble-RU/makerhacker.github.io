<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-453" href="#">cvpr2013-453</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</h1>
<br/><p>Source: <a title="cvpr-2013-453-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Guo_Video_Editing_with_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xiaojie Guo, Xiaochun Cao, Xiaowu Chen, Yi Ma</p><p>Abstract: Given an area of interest in a video sequence, one may want to manipulate or edit the area, e.g. remove occlusions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency, spatial pose, and visual realism. The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment, pose rectification, as well as precise recovery of the occlusion. To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames, and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust alignment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. Experimental results on both simulated and real data demonstrate the accurate and robust performance of our method.</p><p>Reference: <a title="cvpr-2013-453-reference" href="../cvpr2013_reference/cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com ie  Abstract Given an area of interest in a video sequence, one may want to manipulate or edit the area, e. [sent-9, score-0.164]
</p><p>2 remove occlusions from or replace with an advertisement on it. [sent-11, score-0.167]
</p><p>3 The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment, pose rectification, as well as precise recovery of the occlusion. [sent-13, score-0.134]
</p><p>4 To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames, and then show how to align the remaining frames incrementally to the aligned base images. [sent-14, score-0.592]
</p><p>5 From the error residual of the robust alignment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. [sent-15, score-0.992]
</p><p>6 For instance, if one wants to change the original facade bounded by the green window (Fig. [sent-19, score-0.207]
</p><p>7 But editing a long sequence of images remains extremely difficult. [sent-22, score-0.188]
</p><p>8 Since the sequence is usually captured by a hand-held camera, images of the region of interest will appear to be scaled, rotated or deformed throughout the sequence. [sent-23, score-0.194]
</p><p>9 Left: an original frame, where the facade of a selected building and the occlusion map are shown within the small windows. [sent-26, score-0.312]
</p><p>10 Right: the result by replacing the facade with a new texture. [sent-27, score-0.207]
</p><p>11 The three small windows are the new facade (top-left), the trimap (bottom-left), and the mask of  occlusion (bottom-right), respectively. [sent-28, score-0.466]
</p><p>12 To guarantee visual consistency and alleviate human interaction, we first need to automatically align the region of interest precisely across the sequence. [sent-30, score-0.188]
</p><p>13 [8] and [7] propose to align images by minimizing the sum of entropies of pixel values at each pixel location in the batch of aligned images. [sent-31, score-0.239]
</p><p>14 Conversely, the least squares congealing procedure of [3], [4] seeks an alignment that minimizes the sum of squared distances between pairs of images. [sent-32, score-0.343]
</p><p>15 The major drawback of the above approaches is that they do not simultaneously handle large illumination variations and gross pixel corruptions or partial occlusions that usually occur in real images. [sent-35, score-0.355]
</p><p>16 [12] propose an algorithm named RASL to solve the task of image alignment by low-rank and sparse decomposition. [sent-37, score-0.283]
</p><p>17 This method takes into account gross corruptions, and thus gives better results in real applications. [sent-38, score-0.101]
</p><p>18 RASL only takes care of temporal alignment but not the 2D deformation of the region being aligned. [sent-40, score-0.385]
</p><p>19 The rectified versions of the objects (bounded by red dotted windows) are displayed on top-left. [sent-44, score-0.151]
</p><p>20 circular Starbucks logo looks awkward if directly pasted on the deformed facade. [sent-45, score-0.162]
</p><p>21 One way of improving visual quality of editing is to transform the logo according to the facade pose. [sent-46, score-0.42]
</p><p>22 Another way is to rectify the facade as shown in Fig. [sent-48, score-0.305]
</p><p>23 This can be done by TILT [19], which assumes that the rectified texture has lower rank than its deformed versions. [sent-50, score-0.356]
</p><p>24 Actually, it is pervasive that many natural and man-made things (of interest for editing) have low-rank appearance in their rectified position. [sent-51, score-0.151]
</p><p>25 2, in which the rank of each rectified (top-left) image is much lower than that of its original. [sent-53, score-0.221]
</p><p>26 Hence it is natural to try to combine them together to handle the alignment and rectification problem for video editing. [sent-58, score-0.335]
</p><p>27 Besides, to make the editing results look natural and real, it is crucial to preserve the occluding foreground. [sent-59, score-0.225]
</p><p>28 That means the foreground need to be precisely matted, extracted, and synthesized too. [sent-60, score-0.151]
</p><p>29 Plenty of literature about alpha matting has been published [9][2] [15]. [sent-61, score-0.278]
</p><p>30 As discussed above, temporal alignment, spatial rectification, and occlusion of the area are three main issues we have to handle properly for the desired video editing task. [sent-64, score-0.38]
</p><p>31 With the consideration of the limitation of sparse and low-rank decomposition on large data, we first construct the area basis based on several images. [sent-67, score-0.191]
</p><p>32 Every individual area is then parsed into its intrinsic appearance and the corresponding residual, according to the well-constructed basis. [sent-68, score-0.096]
</p><p>33 We automatically generate the trimap from the area and its residual, then adopt alpha matting methods to extract the foreground. [sent-69, score-0.56]
</p><p>34 (a) The median of initial inputs without either alignment or rectification. [sent-73, score-0.335]
</p><p>35 (c) The median with both alignment and rectification by our method. [sent-75, score-0.392]
</p><p>36 Initial Batch Frame Alignment In the simplest case, the areas of interest are clean (without occlusions or gross corruptions) and temporally wellaligned. [sent-82, score-0.297]
</p><p>37 If we stack the 2D areas as columns of a matrix B ∈ Rm×n, where m is the number of pixels of the area, Bthe ∈ m Ratrix B is low rank. [sent-83, score-0.124]
</p><p>38 In the real world, the areas are very likely to have partial occlusions or specular reflections, which may break the low rankness of the matrix  B. [sent-84, score-0.198]
</p><p>39 Therefore, we have B + E = A, where E is the sparse residual matrix between the corrupted matrix A and the clean (low rank) area matrix B. [sent-86, score-0.489]
</p><p>40 , τn ∈ G transform all the misaligned areas to well-aligned A1∈ ∈◦ τ1 , Aans2 ◦o τ2,. [sent-96, score-0.19]
</p><p>41 To fix this, we may assume that the region of interest has some spatial structures and at its “normalized” pose, the region as a 2D matrix achieves the lowest-rank. [sent-108, score-0.151]
</p><p>42 1 Based on these assumptions, we can naturally formulate our task as the fol-  ΓΓ  1Please note that, the 2D region does not have to be a strictly low-rank texture for this to be helpful for resolving the ambiguity. [sent-109, score-0.107]
</p><p>43 0-norm, ω and λ are the coeffiwciheentrse controlling the weights of the terms of the rank of each individual area in spatial space and the sparse matrix E, and R(·) represents the linear operator of reshaping a Evec,t aonrd db Rack(· t)o r eitps original h2eD l ifnoermar. [sent-117, score-0.242]
</p><p>44 Alternatively, minimizing the natural convex surrogate for the objective function in (1) can exactly recover the low rank matrix B, as long as the rank of the matrix B to be recovered is not too high and the number of non-zero elements in E is not too large [1]. [sent-122, score-0.334]
</p><p>45 Given a set of n frames are already aligned as above, when a new frame, say a arrives, we can try to rectify and align it to the basis B obtained from Algorithm 1, subject to some error e. [sent-255, score-0.267]
</p><p>46 In general, the well-aligned and rectified region can be represented by a linear combination of the columns from the basis, i. [sent-256, score-0.208]
</p><p>47 So the formulation of aligning a new frame is: min ? [sent-259, score-0.135]
</p><p>48 Initialization: initial support Ω = 1m;  while not converged do  Output: Optimal solution (x∗ =  xk;  τ∗ = τ)  where e represents the residual, and τ is the transformation matrix of a. [sent-264, score-0.112]
</p><p>49 When the region to be aligned is corrupted by sparse occlusions or corruptions, the procedure proposed above can handle well. [sent-283, score-0.26]
</p><p>50 In such case, we gradually remove some of the pixels that have very large reconstruction errors, and run the above robust alignment again on the remaining region. [sent-285, score-0.277]
</p><p>51 Note that, the area does not contain any occlusions or corruptions if the values of the elements in e are all very close to zero. [sent-324, score-0.311]
</p><p>52 The procedure of aligning a new frame is summarized in Algorithm 2. [sent-328, score-0.135]
</p><p>53 Foreground Separation So far, we have the recovered temporally aligned and spatially rectified area and the associated error residual. [sent-332, score-0.417]
</p><p>54 If our goal is only to recover a clean texture of the region, remove the occluding foreground, and replace the region with another texture throughout the sequence, then results from the above two steps would be sufficient. [sent-333, score-0.342]
</p><p>55 However, if we want to super-impose the occluding foreground back to the edited region to achieve more realistic visual effect, we have to precisely segment out the foreground. [sent-334, score-0.444]
</p><p>56 No-  tice that the error residual e that we have obtained from the above robust alignment is not the foreground per se: it is the difference between the original images and the background. [sent-336, score-0.5]
</p><p>57 Nevertheless, the error residual is very informative about where the occluding foreground is: in general, we 2In this paper, we discuss the and foreground are opaque. [sent-337, score-0.463]
</p><p>58 cases  that the objects of both background  get large residual errors around occluding pixels; and small ones around background pixels. [sent-338, score-0.291]
</p><p>59 Therefore, we can initially assign a pixel to foreground if the error is above certain threshold and to background if the error is small enough. [sent-339, score-0.15]
</p><p>60 In this way, we obtain an initial trimap for foreground segmentation. [sent-341, score-0.306]
</p><p>61 We then employ existing alpha matting methods [9][2] [15] to obtain a more precise segmentation of the foreground from the image frames. [sent-342, score-0.396]
</p><p>62 Unless otherwise stated, the two paramete√rs are fixed through our experiments: ω = 5/n and λ = 3/√m, where n is the number of the images used to construct the area basis and m is the amount of pixels in the area of interest. [sent-347, score-0.281]
</p><p>63 We first verify the ability of the batch frame alignment  ×  Algorithm 1 to cope with varying levels of corruptions on a (rectified) checker-board pattern with white, gray and black blocks. [sent-348, score-0.603]
</p><p>64 Translation, rotation, skew, occlusion rate and the number of images are the factors to be tested. [sent-349, score-0.105]
</p><p>65 The task is to align and rectify the images to a 100 100 pixel sca ntoon ailcigaln nw ainnddow rec. [sent-350, score-0.164]
</p><p>66 Qualita ive results of the proposed single frame alignment method (Algorithm 2) with respect o translation, rota ion, skew and  occlusion. [sent-361, score-0.571]
</p><p>67 Every three pictures of the rest are in one group, which represent the deformed and polluted  version of the reference, the transformed result by our proposed and the corresponding residual, respectively. [sent-363, score-0.151]
</p><p>68 To take into account the skew factor, we constantly set both x0 and y0 to be 5%, and test θ0 and s0 with different values. [sent-368, score-0.235]
</p><p>69 2 skew level, expect for the one of 10◦ rotation and 0. [sent-372, score-0.292]
</p><p>70 We repeat the above procedure to further verify the performance of the proposed algorithm with respect to different levels of occlusion and the number of images. [sent-374, score-0.105]
</p><p>71 Figure 4 (d) shows the robustness of the proposed algorithm to random occlusion and skew level with x0 = 5%, y0 = 10% and θ0 = 10◦ . [sent-375, score-0.34]
</p><p>72 It indicates that our algorithm can converge well up to 15% random occlusion and 0. [sent-376, score-0.105]
</p><p>73 1 skew and 15% occlusion does not succeed in every trial, it has 90% successful rate. [sent-379, score-0.382]
</p><p>74 The second is the result of aligned and rectified windows without removing occlusions by our method. [sent-387, score-0.272]
</p><p>75 The third is the recovered clean regions B by our method. [sent-388, score-0.146]
</p><p>76 Note that due to the limit of space, we do not quantitatively analyze the performance of the single frame alignment Algorithm 2 here as it is designed in a similar spirit as Algorithm 1. [sent-393, score-0.336]
</p><p>77 We synthesize deformed and polluted images by controlling parameters (x-translation, ytranslation, angle of rotation, skew, occlusion fraction) based on the reference, i. [sent-396, score-0.256]
</p><p>78 Figure 7 demonstrates improved robust alignment and recovery results with occlusion detection (as discussed at the end of Section 2. [sent-404, score-0.401]
</p><p>79 The top row is the recovery result by using all the pixels in the region, and the middle row is the result with occlusion detection and masking. [sent-406, score-0.235]
</p><p>80 Another merit of our method is that it preserves all global illumination changes in the original frames, which can be seen from the comparison with the median facade shown in the middle of the bottom row. [sent-407, score-0.309]
</p><p>81 This property ensures that visual realism of the original sequence can be maximally preserved. [sent-408, score-0.106]
</p><p>82 Figure 8 shows an example of foreground separation 222222888866  Figure7. [sent-411, score-0.118]
</p><p>83 Top and Middle rows: the recovered results without and with occlusion detection. [sent-413, score-0.189]
</p><p>84 Bottom row: the left is the result by feature matching and RANSAC, the middle is the median area obtained from the area basis, and the right is their residual. [sent-414, score-0.294]
</p><p>85 from the recovered background and error residuals (as in Section 2. [sent-415, score-0.116]
</p><p>86 It contains four images (from left to right: the recovered residual, the trimap, the foreground mask, and the foreground cutout respectively). [sent-417, score-0.32]
</p><p>87 With the help of texture cue, we can determine that the pixels in the original image having similar textures with their corresponding pixels in the recovered image are background. [sent-419, score-0.2]
</p><p>88 Hence, the trimap is automatically  constructed and used as the input to alpha matting. [sent-421, score-0.34]
</p><p>89 The alpha matting result is computed by using the technique introduced in [9]. [sent-423, score-0.278]
</p><p>90 Readers can adopt other cues to determine the background and unknown, and choose other texture similarity measurements and alpha matting methods for the task of foreground separation. [sent-424, score-0.478]
</p><p>91 In each of the three cases, the first row displays sample images from the same sequences, and the second gives the edited results by the proposed method. [sent-427, score-0.147]
</p><p>92 The top case changes the building facade, the middle one changes the monitor background of the laptop, and the bottom one repairs the building facade texture as well as adding a new advertisement banner. [sent-428, score-0.443]
</p><p>93 The virtual reality achieved by  our method is also rather striking, not only the pose and geometry of the edited area are consistent throughout the sequence, but also all the occlusions in the original sequence are correctly synthesized on the newly edited regions (e. [sent-432, score-0.516]
</p><p>94 Using our system, the only human intervention needed to achieve these tasks is to specify the edited area in the first frame and provide the replacement texture. [sent-435, score-0.335]
</p><p>95 Conclusion We have presented a new framework that simultaneously align and rectify image regions in a video sequence, as well as automatically construct trimaps and segment foregrounds. [sent-437, score-0.235]
</p><p>96 Our framework leverages the recent advances in robust recovery of a high-dimensional low-rank matrix despite gross sparse errors. [sent-438, score-0.196]
</p><p>97 The system can significantly reduce the interaction from users for editing certain areas in the video. [sent-439, score-0.265]
</p><p>98 The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. [sent-511, score-0.21]
</p><p>99 RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images. [sent-527, score-0.283]
</p><p>100 Toward a practical face recognition system: Robust alignment and illumination by sparse representation. [sent-563, score-0.283]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rasl', 0.294), ('alignment', 0.244), ('skew', 0.235), ('facade', 0.207), ('bk', 0.164), ('trimap', 0.154), ('alpha', 0.154), ('rectified', 0.151), ('edited', 0.147), ('corruptions', 0.141), ('residual', 0.138), ('editing', 0.136), ('misaligned', 0.136), ('batch', 0.126), ('tilt', 0.124), ('matting', 0.124), ('vec', 0.119), ('foreground', 0.118), ('bx', 0.113), ('ek', 0.113), ('occlusion', 0.105), ('congealing', 0.099), ('rectify', 0.098), ('area', 0.096), ('frame', 0.092), ('rectification', 0.091), ('occluding', 0.089), ('adm', 0.088), ('bxk', 0.088), ('deformed', 0.085), ('recovered', 0.084), ('ji', 0.084), ('tianjin', 0.082), ('logo', 0.077), ('occlusions', 0.074), ('rank', 0.07), ('ganesh', 0.068), ('gross', 0.068), ('edit', 0.068), ('polluted', 0.066), ('sridharan', 0.066), ('align', 0.066), ('yk', 0.064), ('clean', 0.062), ('lagrange', 0.061), ('advertisement', 0.059), ('china', 0.059), ('region', 0.057), ('median', 0.057), ('rotation', 0.057), ('basis', 0.056), ('realism', 0.054), ('areas', 0.054), ('ransac', 0.054), ('multiplier', 0.054), ('sequence', 0.052), ('recovery', 0.052), ('texture', 0.05), ('monitor', 0.05), ('aligned', 0.047), ('alm', 0.047), ('middle', 0.045), ('transformations', 0.044), ('beijing', 0.044), ('lucey', 0.044), ('cox', 0.044), ('simulated', 0.043), ('corrupted', 0.043), ('aligning', 0.043), ('temporal', 0.043), ('loop', 0.042), ('successful', 0.042), ('qk', 0.041), ('deformation', 0.041), ('transformation', 0.041), ('users', 0.04), ('linearized', 0.039), ('sk', 0.039), ('temporally', 0.039), ('sparse', 0.039), ('simultaneously', 0.039), ('translation', 0.038), ('tpami', 0.038), ('matrix', 0.037), ('shrinkage', 0.036), ('laptop', 0.036), ('surrogate', 0.036), ('interaction', 0.035), ('inner', 0.035), ('initial', 0.034), ('replace', 0.034), ('peng', 0.034), ('precisely', 0.033), ('pixels', 0.033), ('real', 0.033), ('cn', 0.033), ('poisson', 0.032), ('pages', 0.032), ('automatically', 0.032), ('background', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="453-tfidf-1" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>Author: Xiaojie Guo, Xiaochun Cao, Xiaowu Chen, Yi Ma</p><p>Abstract: Given an area of interest in a video sequence, one may want to manipulate or edit the area, e.g. remove occlusions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency, spatial pose, and visual realism. The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment, pose rectification, as well as precise recovery of the occlusion. To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames, and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust alignment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. Experimental results on both simulated and real data demonstrate the accurate and robust performance of our method.</p><p>2 0.22055773 <a title="453-tfidf-2" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>Author: Ehsan Shahrian, Deepu Rajan, Brian Price, Scott Cohen</p><p>Abstract: In this paper, we present a new image matting algorithm that achieves state-of-the-art performance on a benchmark dataset of images. This is achieved by solving two major problems encountered by current sampling based algorithms. The first is that the range in which the foreground and background are sampled is often limited to such an extent that the true foreground and background colors are not present. Here, we describe a method by which a more comprehensive and representative set of samples is collected so as not to miss out on the true samples. This is accomplished by expanding the sampling range for pixels farther from the foreground or background boundary and ensuring that samples from each color distribution are included. The second problem is the overlap in color distributions of foreground and background regions. This causes sampling based methods to fail to pick the correct samples for foreground and background. Our design of an objective function forces those foreground and background samples to be picked that are generated from well-separated distributions. Comparison on the dataset at and evaluation by www.alphamatting.com shows that the proposed method ranks first in terms of error measures used in the website.</p><p>3 0.21128826 <a title="453-tfidf-3" href="./cvpr-2013-Image_Matting_with_Local_and_Nonlocal_Smooth_Priors.html">211 cvpr-2013-Image Matting with Local and Nonlocal Smooth Priors</a></p>
<p>Author: Xiaowu Chen, Dongqing Zou, Steven Zhiying Zhou, Qinping Zhao, Ping Tan</p><p>Abstract: In this paper we propose a novel alpha matting method with local and nonlocal smooth priors. We observe that the manifold preserving editing propagation [4] essentially introduced a nonlocal smooth prior on the alpha matte. This nonlocal smooth prior and the well known local smooth priorfrom matting Laplacian complement each other. So we combine them with a simple data term from color sampling in a graph model for nature image matting. Our method has a closed-form solution and can be solved efficiently. Compared with the state-of-the-art methods, our method produces more accurate results according to the evaluation on standard benchmark datasets.</p><p>4 0.13371186 <a title="453-tfidf-4" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>5 0.12556089 <a title="453-tfidf-5" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>6 0.10930958 <a title="453-tfidf-6" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>7 0.097538009 <a title="453-tfidf-7" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>8 0.08850684 <a title="453-tfidf-8" href="./cvpr-2013-Procrustean_Normal_Distribution_for_Non-rigid_Structure_from_Motion.html">341 cvpr-2013-Procrustean Normal Distribution for Non-rigid Structure from Motion</a></p>
<p>9 0.08798676 <a title="453-tfidf-9" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>10 0.085810013 <a title="453-tfidf-10" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>11 0.084224105 <a title="453-tfidf-11" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>12 0.082539059 <a title="453-tfidf-12" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>13 0.081426136 <a title="453-tfidf-13" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>14 0.080293462 <a title="453-tfidf-14" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>15 0.080285452 <a title="453-tfidf-15" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>16 0.080018967 <a title="453-tfidf-16" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>17 0.078531988 <a title="453-tfidf-17" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>18 0.077982292 <a title="453-tfidf-18" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>19 0.076605819 <a title="453-tfidf-19" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>20 0.076059118 <a title="453-tfidf-20" href="./cvpr-2013-Articulated_and_Restricted_Motion_Subspaces_and_Their_Signatures.html">46 cvpr-2013-Articulated and Restricted Motion Subspaces and Their Signatures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.212), (1, 0.045), (2, -0.02), (3, 0.035), (4, 0.004), (5, -0.041), (6, 0.013), (7, -0.056), (8, 0.03), (9, -0.011), (10, 0.036), (11, 0.012), (12, 0.002), (13, -0.027), (14, 0.053), (15, -0.042), (16, 0.028), (17, -0.024), (18, 0.03), (19, 0.044), (20, -0.034), (21, 0.071), (22, -0.054), (23, -0.186), (24, 0.092), (25, -0.162), (26, 0.084), (27, 0.066), (28, 0.003), (29, -0.016), (30, 0.085), (31, -0.04), (32, -0.12), (33, -0.083), (34, -0.097), (35, -0.069), (36, -0.093), (37, -0.079), (38, 0.126), (39, 0.039), (40, -0.061), (41, -0.017), (42, -0.023), (43, -0.024), (44, -0.011), (45, 0.02), (46, -0.04), (47, -0.04), (48, -0.095), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93427974 <a title="453-lsi-1" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>Author: Xiaojie Guo, Xiaochun Cao, Xiaowu Chen, Yi Ma</p><p>Abstract: Given an area of interest in a video sequence, one may want to manipulate or edit the area, e.g. remove occlusions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency, spatial pose, and visual realism. The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment, pose rectification, as well as precise recovery of the occlusion. To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames, and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust alignment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. Experimental results on both simulated and real data demonstrate the accurate and robust performance of our method.</p><p>2 0.76212472 <a title="453-lsi-2" href="./cvpr-2013-Image_Matting_with_Local_and_Nonlocal_Smooth_Priors.html">211 cvpr-2013-Image Matting with Local and Nonlocal Smooth Priors</a></p>
<p>Author: Xiaowu Chen, Dongqing Zou, Steven Zhiying Zhou, Qinping Zhao, Ping Tan</p><p>Abstract: In this paper we propose a novel alpha matting method with local and nonlocal smooth priors. We observe that the manifold preserving editing propagation [4] essentially introduced a nonlocal smooth prior on the alpha matte. This nonlocal smooth prior and the well known local smooth priorfrom matting Laplacian complement each other. So we combine them with a simple data term from color sampling in a graph model for nature image matting. Our method has a closed-form solution and can be solved efficiently. Compared with the state-of-the-art methods, our method produces more accurate results according to the evaluation on standard benchmark datasets.</p><p>3 0.74846381 <a title="453-lsi-3" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>Author: Ehsan Shahrian, Deepu Rajan, Brian Price, Scott Cohen</p><p>Abstract: In this paper, we present a new image matting algorithm that achieves state-of-the-art performance on a benchmark dataset of images. This is achieved by solving two major problems encountered by current sampling based algorithms. The first is that the range in which the foreground and background are sampled is often limited to such an extent that the true foreground and background colors are not present. Here, we describe a method by which a more comprehensive and representative set of samples is collected so as not to miss out on the true samples. This is accomplished by expanding the sampling range for pixels farther from the foreground or background boundary and ensuring that samples from each color distribution are included. The second problem is the overlap in color distributions of foreground and background regions. This causes sampling based methods to fail to pick the correct samples for foreground and background. Our design of an objective function forces those foreground and background samples to be picked that are generated from well-separated distributions. Comparison on the dataset at and evaluation by www.alphamatting.com shows that the proposed method ranks first in terms of error measures used in the website.</p><p>4 0.68492335 <a title="453-lsi-4" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>5 0.63515735 <a title="453-lsi-5" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>Author: Gunhee Kim, Eric P. Xing</p><p>Abstract: With an explosion of popularity of online photo sharing, we can trivially collect a huge number of photo streams for any interesting topics such as scuba diving as an outdoor recreational activity class. Obviously, the retrieved photo streams are neither aligned nor calibrated since they are taken in different temporal, spatial, and personal perspectives. However, at the same time, they are likely to share common storylines that consist of sequences of events and activities frequently recurred within the topic. In this paper, as a first technical step to detect such collective storylines, we propose an approach to jointly aligning and segmenting uncalibrated multiple photo streams. The alignment task discovers the matched images between different photo streams, and the image segmentation task parses each image into multiple meaningful regions to facilitate the image understanding. We close a loop between the two tasks so that solving one task helps enhance the performance of the other in a mutually rewarding way. To this end, we design a scalable message-passing based optimization framework to jointly achieve both tasks for the whole input image set at once. With evaluation on the new Flickr dataset of 15 outdoor activities that consist of 1.5 millions of images of 13 thousands of photo streams, our empirical results show that the proposed algorithms are more successful than other candidate methods for both tasks.</p><p>6 0.6012826 <a title="453-lsi-6" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>7 0.58558702 <a title="453-lsi-7" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>8 0.57524049 <a title="453-lsi-8" href="./cvpr-2013-A_Non-parametric_Framework_for_Document_Bleed-through_Removal.html">22 cvpr-2013-A Non-parametric Framework for Document Bleed-through Removal</a></p>
<p>9 0.54978222 <a title="453-lsi-9" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>10 0.54537785 <a title="453-lsi-10" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>11 0.54481781 <a title="453-lsi-11" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>12 0.51075864 <a title="453-lsi-12" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>13 0.51048493 <a title="453-lsi-13" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>14 0.48430794 <a title="453-lsi-14" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>15 0.48192951 <a title="453-lsi-15" href="./cvpr-2013-FasT-Match%3A_Fast_Affine_Template_Matching.html">162 cvpr-2013-FasT-Match: Fast Affine Template Matching</a></p>
<p>16 0.46837908 <a title="453-lsi-16" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>17 0.4677698 <a title="453-lsi-17" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>18 0.46735993 <a title="453-lsi-18" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>19 0.4659076 <a title="453-lsi-19" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>20 0.46359614 <a title="453-lsi-20" href="./cvpr-2013-Fast_Trust_Region_for_Segmentation.html">171 cvpr-2013-Fast Trust Region for Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.128), (16, 0.036), (26, 0.077), (28, 0.016), (33, 0.288), (39, 0.015), (53, 0.213), (67, 0.052), (69, 0.039), (77, 0.018), (87, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90420425 <a title="453-lda-1" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>Author: Luping Zhou, Lei Wang, Lingqiao Liu, Philip Ogunbona, Dinggang Shen</p><p>Abstract: Analyzing brain networks from neuroimages is becoming a promising approach in identifying novel connectivitybased biomarkers for the Alzheimer’s disease (AD). In this regard, brain “effective connectivity ” analysis, which studies the causal relationship among brain regions, is highly challenging and of many research opportunities. Most of the existing works in this field use generative methods. Despite their success in data representation and other important merits, generative methods are not necessarily discriminative, which may cause the ignorance of subtle but critical disease-induced changes. In this paper, we propose a learning-based approach that integrates the benefits of generative and discriminative methods to recover effective connectivity. In particular, we employ Fisher kernel to bridge the generative models of sparse Bayesian networks (SBN) and the discriminative classifiers of SVMs, and convert the SBN parameter learning to Fisher kernel learning via minimizing a generalization error bound of SVMs. Our method is able to simultaneously boost the discriminative power of both the generative SBN models and the SBN-induced SVM classifiers via Fisher kernel. The proposed method is tested on analyzing brain effective connectivity for AD from ADNI data, and demonstrates significant improvements over the state-of-the-art work.</p><p>same-paper 2 0.88324088 <a title="453-lda-2" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>Author: Xiaojie Guo, Xiaochun Cao, Xiaowu Chen, Yi Ma</p><p>Abstract: Given an area of interest in a video sequence, one may want to manipulate or edit the area, e.g. remove occlusions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency, spatial pose, and visual realism. The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment, pose rectification, as well as precise recovery of the occlusion. To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames, and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust alignment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. Experimental results on both simulated and real data demonstrate the accurate and robust performance of our method.</p><p>3 0.87262797 <a title="453-lda-3" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xingyu Zeng, Xiaogang Wang</p><p>Abstract: Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. The difficulty is added when several pedestrians overlap in images and occlude each other. We observe, however, that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the CaltechTrain dataset, the Caltech-Test dataset and the ETHdataset. Including mutual visibility leads to 4% −8% improvements on mluudlitnipglem ubteunaclh vmiasibrki ditayta lesaedtss.</p><p>4 0.86506414 <a title="453-lda-4" href="./cvpr-2013-Image_Matting_with_Local_and_Nonlocal_Smooth_Priors.html">211 cvpr-2013-Image Matting with Local and Nonlocal Smooth Priors</a></p>
<p>Author: Xiaowu Chen, Dongqing Zou, Steven Zhiying Zhou, Qinping Zhao, Ping Tan</p><p>Abstract: In this paper we propose a novel alpha matting method with local and nonlocal smooth priors. We observe that the manifold preserving editing propagation [4] essentially introduced a nonlocal smooth prior on the alpha matte. This nonlocal smooth prior and the well known local smooth priorfrom matting Laplacian complement each other. So we combine them with a simple data term from color sampling in a graph model for nature image matting. Our method has a closed-form solution and can be solved efficiently. Compared with the state-of-the-art methods, our method produces more accurate results according to the evaluation on standard benchmark datasets.</p><p>5 0.85940236 <a title="453-lda-5" href="./cvpr-2013-Improving_Image_Matting_Using_Comprehensive_Sampling_Sets.html">216 cvpr-2013-Improving Image Matting Using Comprehensive Sampling Sets</a></p>
<p>Author: Ehsan Shahrian, Deepu Rajan, Brian Price, Scott Cohen</p><p>Abstract: In this paper, we present a new image matting algorithm that achieves state-of-the-art performance on a benchmark dataset of images. This is achieved by solving two major problems encountered by current sampling based algorithms. The first is that the range in which the foreground and background are sampled is often limited to such an extent that the true foreground and background colors are not present. Here, we describe a method by which a more comprehensive and representative set of samples is collected so as not to miss out on the true samples. This is accomplished by expanding the sampling range for pixels farther from the foreground or background boundary and ensuring that samples from each color distribution are included. The second problem is the overlap in color distributions of foreground and background regions. This causes sampling based methods to fail to pick the correct samples for foreground and background. Our design of an objective function forces those foreground and background samples to be picked that are generated from well-separated distributions. Comparison on the dataset at and evaluation by www.alphamatting.com shows that the proposed method ranks first in terms of error measures used in the website.</p><p>6 0.85036856 <a title="453-lda-6" href="./cvpr-2013-Area_Preserving_Brain_Mapping.html">44 cvpr-2013-Area Preserving Brain Mapping</a></p>
<p>7 0.83724457 <a title="453-lda-7" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>8 0.8343538 <a title="453-lda-8" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>9 0.83267337 <a title="453-lda-9" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>10 0.83134657 <a title="453-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.83082348 <a title="453-lda-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.83075857 <a title="453-lda-12" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>13 0.83068436 <a title="453-lda-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.83033448 <a title="453-lda-14" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>15 0.82966805 <a title="453-lda-15" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>16 0.82902622 <a title="453-lda-16" href="./cvpr-2013-Groupwise_Registration_via_Graph_Shrinkage_on_the_Image_Manifold.html">194 cvpr-2013-Groupwise Registration via Graph Shrinkage on the Image Manifold</a></p>
<p>17 0.82894862 <a title="453-lda-17" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>18 0.8287093 <a title="453-lda-18" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>19 0.82863593 <a title="453-lda-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.82798588 <a title="453-lda-20" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
