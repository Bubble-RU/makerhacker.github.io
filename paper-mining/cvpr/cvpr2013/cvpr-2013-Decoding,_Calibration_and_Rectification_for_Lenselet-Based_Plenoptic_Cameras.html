<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-102" href="#">cvpr2013-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</h1>
<br/><p>Source: <a title="cvpr-2013-102-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Dansereau_Decoding_Calibration_and_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>Reference: <a title="cvpr-2013-102-reference" href="../cvpr2013_reference/cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. [sent-7, score-0.367]
</p><p>2 We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. [sent-8, score-0.317]
</p><p>3 We further propose a radial distortion model and a practical objective function based on ray reprojection. [sent-9, score-0.461]
</p><p>4 Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. [sent-10, score-0.35]
</p><p>5 Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. [sent-11, score-0.623]
</p><p>6 With increased depth of field and light gathering relative to conventional cameras, and post-capture capabilities ranging from refocus to occlusion removal and closed-form visual odometry [1, 16, 4, 9, 19, 6], plenoptic cameras are poised to play a significant role in computer vision applications. [sent-24, score-0.796]
</p><p>7 As such, accurate plenoptic calibration and rectification will become increasingly important. [sent-25, score-0.754]
</p><p>8 Prior work in this area has largely dealt with camera arrays [20, 18], with very little work going toward the calibration of lenselet-based cameras. [sent-26, score-0.365]
</p><p>9 By exploiting the physical characteristics of a lenselet-based plenoptic camera, we impose significant constraints beyond those present in a multiple-camera scenario. [sent-27, score-0.519]
</p><p>10 In this work we present a novel 15-parameter plenoptic camera model relating pixels to rays in 3D space, including a 4D intrinsic matrix based on a projective pinhole and thin-lens model, and a radial direction-dependent distortion model. [sent-33, score-0.978]
</p><p>11 We present a practical method for decoding a camera’s 2D lenselet images into 4D light fields without prior knowledge ofits physical parameters, and describe an efficient projected-ray objective function and calibration scheme. [sent-34, score-1.187]
</p><p>12 We use these to accurately calibrate and rectify im-  ages from a commercially available Lytro plenoptic camera. [sent-35, score-0.503]
</p><p>13 The latter include the “original” plenoptic camera as described by Ng et al. [sent-39, score-0.584]
</p><p>14 [17], with which the present work is concerned, and the “focused” plenoptic camera described by Lumsdaine and Georgiev [14]. [sent-40, score-0.584]
</p><p>15 Each camera has unique characteristics, and so the optimal model and calibration approach for each will differ. [sent-41, score-0.336]
</p><p>16 Previous work has addressed calibration of grids or freeform collections of multiple cameras [20, 18]. [sent-42, score-0.308]
</p><p>17 Similar to this is the case of a moving camera in a static scene, for which structure-from-motion can be extended for plenoptic modelling [12]. [sent-43, score-0.584]
</p><p>18 These approaches introduce more degrees of freedom in their models than are necessary to describe the lenselet-based plenoptic camera. [sent-44, score-0.444]
</p><p>19 [7] derive a plenoptic camera model using ray transfer matrix analysis. [sent-47, score-0.799]
</p><p>20 Our model is more detailed, accurately describing a real-world camera by including the effects of lens distortion and projection through the lenticular array. [sent-48, score-0.548]
</p><p>21 However, their piecewise-continuous pixel-ray mapping does  not apply to the plenoptic camera, and so our camera model and calibration procedure differ significantly from theirs. [sent-51, score-0.78]
</p><p>22 Decoding to an Unrectified Light Field Light fields are conventionally represented and processed in 4D, and so we begin by presenting a practical scheme for decoding raw 2D lenselet images to a 4D light field representation. [sent-53, score-1.006]
</p><p>23 Note that we do not address the question of demosaicing Bayer-pattern plenoptic images we instead refer the reader to [23] and related work. [sent-54, score-0.529]
</p><p>24 For the purposes of this work, we employ conventional linear demosaicing applied directly to the raw 2D lenselet image. [sent-55, score-0.734]
</p><p>25 This yields undesired effects in pixels near lenselet edges, and we therefore ignore edge pixels during calibration. [sent-56, score-0.66]
</p><p>26 In general the exact placement of the lenselet array is unknown, with lenselet spacing being a non-integer multiple of pixel pitch, and unknown translational and rotational offsets further complicating the decode process. [sent-57, score-1.426]
</p><p>27 A crop of a typical raw lenselet image is shown in Fig. [sent-58, score-0.644]
</p><p>28 1 note that the lenselet grid is hexagonally packed, further complicating the decoding process. [sent-59, score-0.854]
</p><p>29 To locate lenselet image centers we employ an image taken through a white diffuser, or of a white scene. [sent-60, score-0.718]
</p><p>30 Because of vignetting, the brightest spot in each white lenselet image approximates its center. [sent-61, score-0.631]
</p><p>31 A low-pass filter is applied to reduce sensor noise prior to finding the local maximum within each lenselet image. [sent-64, score-0.59]
</p><p>32 Grid parameters are estimated by traversing lenselet image centers, finding the mean horizontal and vertical spacing and offset, and performing line fits to estimate rotation. [sent-67, score-0.706]
</p><p>33 From the estimated grid parameters there are many potential methods for decoding the lenselet image to a 4D light field. [sent-69, score-0.972]
</p><p>34 The process begins by demosaicing the raw lenselet image, then correcting vignetting by dividing by the white image. [sent-71, score-0.824]
</p><p>35 At this point the lenselet images, depicted in blue in Fig. [sent-72, score-0.652]
</p><p>36 We therefore resample the image, rotating and scaling so all lenselet centers fall on pixel centers, as depicted in the second frame of the figure. [sent-74, score-0.725]
</p><p>37 Aligning the lenselet images to an integer pixel grid allows a very simple slicing scheme: the light field is broken into identically sized, overlapping rectangles centered on the lenselet images, as depicted in the top-right and bottomleft frames of Fig. [sent-76, score-1.538]
</p><p>38 The spacing in the bottom-left frame represents the hexagonal sampling in the lenselet indices k, l, as well as non-square pixels in the pixel indices i,j. [sent-78, score-0.837]
</p><p>39 For rectangular lenselet arrays, this interpolation step is omitted. [sent-82, score-0.59]
</p><p>40 The main lens is modelled as a thin lens and the lenselets as an array of pinholes; gray lines depict lenselet image centers rects for the rectangular pixels in i,j through a 1D interpolation along i. [sent-85, score-1.181]
</p><p>41 The final step, not shown, is to mask off pixels that fall outside the hexagonal lenselet image. [sent-87, score-0.678]
</p><p>42 Though each pixel of a plenoptic camera integrates light from a volume, we approximate each as integrating along a single ray [8]. [sent-91, score-0.977]
</p><p>43 We model the main lens as a thin lens, and the lenselets as an array of pinholes, as depicted in Fig. [sent-92, score-0.412]
</p><p>44 For lenselet images of N N pixels, iand j neasechle range fer. [sent-95, score-0.613]
</p><p>45 The conversion from relative to absolute indices, is straightforwardly found from the number of pixels per lenselet N and a translational pixel offset cpix (below). [sent-100, score-0.735]
</p><p>46 We next convert from absolute coordinates to a light field ray, with the imaging and lenselet planes as the reference planes. [sent-101, score-0.83]
</p><p>47 Next we express the ray as position and direction via HφΦ (below), and propagate to the main lens using HT: HφΦ=  ? [sent-107, score-0.387]
</p><p>48 Note that in the conventional plenoptic camera, dμ = fμ, the lenselet focal length. [sent-113, score-1.065]
</p><p>49 Next we apply the main lens using a thin lens and small angle approximation (below), and convert back to a light field ray representation, with the main lens as the s, t plane, and the u, v plane at an arbitrary plane separation D: HM=  ? [sent-114, score-1.048]
</p><p>50 (5)  In a model with pixel or lenselet skew we would expect more non-zero terms. [sent-130, score-0.617]
</p><p>51 Projection Through the Lenselets  We have hidden some complexity in deriving the 4D intrinsic matrix by assuming prior knowledge of the lenselet associated with each pixel. [sent-134, score-0.665]
</p><p>52 3, the projected image centers will deviate from the lenselet centers, and as a result a pixel will not necessarily associate with its nearest lenselet. [sent-136, score-0.663]
</p><p>53 By resizing, rotating, interpolating, and centering on the projected lenselet images, we have created a virtual light field camera with its own parameters. [sent-138, score-0.983]
</p><p>54 Lenselet-based plenoptic cameras are constructed with careful attention to the coplanarity of the lenselet array and image plane [17]. [sent-140, score-1.199]
</p><p>55 Lens Distortion Model The physical alignment and characteristics ofthe lenselet array as well as all the elements of the main lens potentially contribute to lens distortion. [sent-148, score-1.079]
</p><p>56 In the results section we show that the consumer plenoptic camera we employ suffers primarily from directionally dependent radial distortion, θd  =  (1+ k1r2 + k2r4 +  ·  ·  ·  ) ? [sent-149, score-0.621]
</p><p>57 ial distortion coefficients, and θu and θd are the undistorted and distorted 2D ray directions, respectively. [sent-154, score-0.401]
</p><p>58 Calibration and Rectification The plenoptic camera gathers enough information to perform calibration from unstructured and unknown environments. [sent-160, score-0.78]
</p><p>59 Plenoptic calibration is complicated by the fact that a single feature will appear in the imaging plane multiple times, as depicted in Fig. [sent-164, score-0.323]
</p><p>60 The problem arises that the observed and expected features do not generally appear in the same lenselet images indeed the number of expected and observed features is not generally equal. [sent-167, score-0.59]
</p><p>61 In conventional projective calibration (a) a 3D feature P has one projected image, and a convenient error metric is the 2D distance between the expected and observed image locations | nˆ n|. [sent-175, score-0.295]
</p><p>62 In the plenoptic camera (b) each feature has multiple expected |an. [sent-176, score-0.584]
</p><p>63 dIn o tbhseer pvleedno images nˆ j , ni ∈ aRch4, weahtuicrhe generally pdleo nexotappear beneath the same lenselets; we propose the per-observation ray reprojection metric |Ei | taken as the 3D distance between the reprojected ray φˆi eatrndic t |hEe f|ea tatukreen la osc tahteion 3D DP d. [sent-177, score-0.542]
</p><p>64 The observed feature locations are extracted by treating the decoded light field from Section 3 as an array of Ni Nj 2D images in k and l, applying a conventional feature d×eteNction scheme [11] to each. [sent-182, score-0.399]
</p><p>65 If the plenoptic camera takes on M poses in the calibration dataset and there are nc features on the calibration target, the total feature set over which we optimize is of size ncMNiNj . [sent-183, score-1.017]
</p><p>66 Our goal is to find the intrin-  “ray  sic matrix H, camera poses T, and distortion parameters d which minimize the error across all features, ? [sent-184, score-0.419]
</p><p>67 =1||φˆcs,t(H,Tm,d),Pc||pt-ray,  (8)  where | | · | |pt-ray is the ray reprojection error described above. [sent-192, score-0.33]
</p><p>68 The number of parameters over which we optimize is now reduced to 10 for intrinsics, 5 for lens distortion, and 6 for each of the M camera poses, for a total of 6M + 15. [sent-199, score-0.335]
</p><p>69 The Jacobian sparsity pattern is easy to derive: each of the M pose estimates will only influence that pose’s ncNiNj error terms, while all of the 15 intrinsic and distortion parameters will affect every error term. [sent-204, score-0.342]
</p><p>70 Initialization  The calibration process proceeds in stages: first initial pose and intrinsic estimates are formed, then an optimization is carried out with no distortion parameters, and finally a full optimization is carried out with distortion parameters. [sent-209, score-0.643]
</p><p>71 To form initial pose estimates, we again treat the decoded light fields across M poses each as an array of Ni Nj 2D images. [sent-210, score-0.347]
</p><p>72 By passing all the images through a conv×eNntional  ×  camera calibration process, for example that proposed by Heikkil a¨ [10], we obtain a per-image pose estimate. [sent-211, score-0.336]
</p><p>73 Note that distortion parameters are excluded from this process, and the camera intrinsics that it yields are ignored. [sent-213, score-0.419]
</p><p>74 In Section 4 we derived a closed-form expression for the intrinsic matrix H based on the plenoptic camera’s physical parameters and the parameters of the decoding process (1), (6). [sent-214, score-0.792]
</p><p>75 Rectification We wish to rectify the light field imagery, reversing the effects of lens distortion and yielding square pixels in i,j and k, l. [sent-220, score-0.717]
</p><p>76 Our approach is to interpolate from the decoded light field LA at a set of continuous-domain indices n˜ A such that the interpolated light field approximates a distortionfree rectified light field LR. [sent-221, score-0.838]
</p><p>77 To find n˜ A we begin with the indices of the rectified light field nR, and project through the ideal optical system by applying HR, yielding the ideal ray φR. [sent-242, score-0.6]
</p><p>78 Referring to the distortion model (7), the desired ray φR is arrived at by applying the forward model to some unknown undistorted ray φA. [sent-243, score-0.616]
</p><p>79 applying the inverse of the estimated intrinsic matrix There is no closed-form solution to the problem of reversing the distortion model (7), and so we propose an iterative approach similar to that of Melen [15]. [sent-245, score-0.308]
</p><p>80 Results  We carried out calibration on five datasets collected with the commercially available Lytro plenoptic camera. [sent-249, score-0.675]
</p><p>81 The decoding process requires a white image for locating lenselet image centers and correcting for vignetting. [sent-354, score-0.857]
</p><p>82 Table 2 shows the estimated parameters for Dataset B at the three stages of the calibration process: initial estimate, intrinsics without distortion, and intrinsics with distortion. [sent-372, score-0.359]
</p><p>83 Table 3 summarizes the root mean square (RMS) ray reprojection error, as described in Section 5, at the three calibration stages and across the five datasets. [sent-373, score-0.497]
</p><p>84 The first represents the plenoptic camera as an array of projective sub-cameras with independent relative poses and identical intrinsics and distortion parameters, while the second also includes per-sub-camera intrinsic and distortion parameters. [sent-375, score-1.251]
</p><p>85 These represent the dependence of a ray’s position on the lenselet through which it passes, and a consequence of these nonzero values is that rays take on a wide variety of  rational-valued positions in the s, t plane. [sent-379, score-0.624]
</p><p>86 6 depicts typical ray reprojection error in our proposed model as a function of direction and position. [sent-385, score-0.33]
</p><p>87 This shows the proposed distortion model to account for most lens distortion for this camera. [sent-388, score-0.544]
</p><p>88 ray position; top: no distortion model; bottom: the proposed five-parameter distortion model note the order of magnitude difference in the error scale. [sent-399, score-0.616]
</p><p>89 The proposed model has accounted for most lens distortion for this camera. [sent-400, score-0.358]
</p><p>90 –  Examples of decoded and rectified light fields are shown in Figs. [sent-401, score-0.3]
</p><p>91 The straight –  –  red rulings aid visual confirmation that rectification has significantly reduced the effects of lens distortion. [sent-406, score-0.37]
</p><p>92 Conclusions and Future Work We have presented a 15-parameter camera model and method for calibrating a lenselet-based plenoptic camera. [sent-411, score-0.584]
</p><p>93 This included derivation of a novel physically based 4D intrinsic matrix and distortion model which relate the indices of a pixel to its corresponding spatial ray. [sent-412, score-0.338]
</p><p>94 We also presented a method for decoding hexagonal lenselet-based plenoptic images without prior knowledge of the camera’s parameters, and related the resulting images to the camera model. [sent-414, score-0.801]
</p><p>95 Finally, we showed a method for rectifying the decoded images, reversing the effects of lens distortion and yielding square pixels in i,j and k, l. [sent-415, score-0.565]
</p><p>96 In the rectified images, the ray corresponding to each pixel is easily found through a single matrix multiplication (5). [sent-416, score-0.306]
</p><p>97 (a)(b)  grid, and b) the demosaiced and vignetting-corrected raw checkerboard image; c–h) examples of (left) unrectified and (right) rectified light fields; red rulings aid confirmation that rectification has significantly reduced the effect of lens distortion. [sent-417, score-0.677]
</p><p>98 Validation included five datasets captured with a com-  mercially available plenoptic tion grid sizes. [sent-418, score-0.5]
</p><p>99 Focusing on everything – light field cameras promise an imaging revolution. [sent-497, score-0.297]
</p><p>100 A four-step camera calibration procedure with implicit image correction. [sent-502, score-0.336]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lenselet', 0.59), ('plenoptic', 0.444), ('ray', 0.215), ('calibration', 0.196), ('distortion', 0.186), ('lens', 0.172), ('decoding', 0.152), ('light', 0.151), ('camera', 0.14), ('rectification', 0.114), ('reprojection', 0.086), ('decoded', 0.085), ('demosaicing', 0.085), ('lenselets', 0.08), ('physical', 0.075), ('intrinsic', 0.075), ('intrinsics', 0.07), ('array', 0.07), ('hexagonal', 0.065), ('dansereau', 0.064), ('rectified', 0.064), ('field', 0.062), ('depicted', 0.062), ('cameras', 0.057), ('grid', 0.056), ('vignetting', 0.052), ('lumsdaine', 0.052), ('mm', 0.051), ('indices', 0.05), ('lorikeet', 0.048), ('mproj', 0.048), ('unrectified', 0.048), ('reversing', 0.047), ('centers', 0.046), ('georgiev', 0.042), ('lytro', 0.042), ('white', 0.041), ('poses', 0.041), ('virtual', 0.04), ('checkerboard', 0.04), ('vaish', 0.039), ('projective', 0.039), ('plane', 0.038), ('radial', 0.037), ('pizarro', 0.037), ('rms', 0.036), ('australian', 0.036), ('nj', 0.036), ('commercially', 0.035), ('horizontal', 0.035), ('rays', 0.034), ('decode', 0.034), ('translational', 0.033), ('acfr', 0.032), ('cpix', 0.032), ('hexagonally', 0.032), ('hrabels', 0.032), ('mhex', 0.032), ('ncmninj', 0.032), ('rulings', 0.032), ('unrealistically', 0.032), ('usyd', 0.032), ('spacing', 0.032), ('conventional', 0.031), ('optical', 0.03), ('conversion', 0.03), ('grids', 0.03), ('arrays', 0.029), ('error', 0.029), ('converting', 0.028), ('gathering', 0.028), ('correcting', 0.028), ('thin', 0.028), ('pinholes', 0.028), ('confirmation', 0.028), ('yielding', 0.028), ('raw', 0.028), ('imaging', 0.027), ('slices', 0.027), ('pixel', 0.027), ('photography', 0.026), ('lenticular', 0.026), ('sydney', 0.026), ('ni', 0.026), ('exceed', 0.026), ('crop', 0.026), ('offsets', 0.026), ('vertical', 0.026), ('freeform', 0.025), ('effects', 0.024), ('conference', 0.024), ('complicating', 0.024), ('rectify', 0.024), ('horowitz', 0.024), ('pixels', 0.023), ('practical', 0.023), ('iand', 0.023), ('parameters', 0.023), ('heikkil', 0.023), ('odometry', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="102-tfidf-1" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>2 0.25027436 <a title="102-tfidf-2" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>3 0.25003913 <a title="102-tfidf-3" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>4 0.24438307 <a title="102-tfidf-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.2027481 <a title="102-tfidf-5" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>6 0.19092524 <a title="102-tfidf-6" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>7 0.13933113 <a title="102-tfidf-7" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>8 0.13731226 <a title="102-tfidf-8" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>9 0.1199027 <a title="102-tfidf-9" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>10 0.11605849 <a title="102-tfidf-10" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>11 0.10815147 <a title="102-tfidf-11" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>12 0.1076827 <a title="102-tfidf-12" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>13 0.092075117 <a title="102-tfidf-13" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>14 0.078025848 <a title="102-tfidf-14" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>15 0.077783369 <a title="102-tfidf-15" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>16 0.075621039 <a title="102-tfidf-16" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>17 0.073736683 <a title="102-tfidf-17" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>18 0.073613822 <a title="102-tfidf-18" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>19 0.07290516 <a title="102-tfidf-19" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>20 0.066695489 <a title="102-tfidf-20" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, 0.162), (2, -0.003), (3, 0.033), (4, -0.015), (5, -0.066), (6, -0.058), (7, -0.018), (8, 0.042), (9, 0.044), (10, -0.069), (11, 0.113), (12, 0.195), (13, -0.081), (14, -0.262), (15, 0.071), (16, 0.082), (17, 0.081), (18, -0.034), (19, 0.092), (20, 0.091), (21, 0.048), (22, -0.058), (23, -0.08), (24, -0.028), (25, 0.063), (26, 0.02), (27, 0.019), (28, -0.017), (29, -0.0), (30, 0.036), (31, -0.016), (32, 0.027), (33, 0.034), (34, -0.035), (35, 0.024), (36, -0.031), (37, -0.07), (38, -0.004), (39, 0.036), (40, -0.049), (41, -0.038), (42, -0.01), (43, -0.024), (44, 0.044), (45, -0.147), (46, -0.048), (47, -0.057), (48, 0.051), (49, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96149778 <a title="102-lsi-1" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>2 0.94272411 <a title="102-lsi-2" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>3 0.86127108 <a title="102-lsi-3" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>4 0.84845924 <a title="102-lsi-4" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>Author: José Henrique Brito, Roland Angst, Kevin Köser, Marc Pollefeys</p><p>Abstract: In cameras with radial distortion, straight lines in space are in general mapped to curves in the image. Although epipolar geometry also gets distorted, there is a set of special epipolar lines that remain straight, namely those that go through the distortion center. By finding these straight epipolar lines in camera pairs we can obtain constraints on the distortion center(s) without any calibration object or plumbline assumptions in the scene. Although this holds for all radial distortion models we conceptually prove this idea using the division distortion model and the radial fundamental matrix which allow for a very simple closed form solution of the distortion center from two views (same distortion) or three views (different distortions). The non-iterative nature of our approach makes it immune to local minima and allows finding the distortion center also for cropped images or those where no good prior exists. Besides this, we give comprehensive relations between different undistortion models and discuss advantages and drawbacks.</p><p>5 0.83269715 <a title="102-lsi-5" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>Author: Timothy Yau, Minglun Gong, Yee-Hong Yang</p><p>Abstract: In underwater imagery, the image formation process includes refractions that occur when light passes from water into the camera housing, typically through a flat glass port. We extend the existing work on physical refraction models by considering the dispersion of light, and derive new constraints on the model parameters for use in calibration. This leads to a novel calibration method that achieves improved accuracy compared to existing work. We describe how to construct a novel calibration device for our method and evaluate the accuracy of the method through synthetic and real experiments.</p><p>6 0.74683994 <a title="102-lsi-6" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>7 0.74586451 <a title="102-lsi-7" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>8 0.70035106 <a title="102-lsi-8" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>9 0.68863213 <a title="102-lsi-9" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>10 0.67931718 <a title="102-lsi-10" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>11 0.6327318 <a title="102-lsi-11" href="./cvpr-2013-Rolling_Shutter_Camera_Calibration.html">368 cvpr-2013-Rolling Shutter Camera Calibration</a></p>
<p>12 0.62461746 <a title="102-lsi-12" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>13 0.62194222 <a title="102-lsi-13" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>14 0.61152756 <a title="102-lsi-14" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>15 0.56302786 <a title="102-lsi-15" href="./cvpr-2013-Cloud_Motion_as_a_Calibration_Cue.html">84 cvpr-2013-Cloud Motion as a Calibration Cue</a></p>
<p>16 0.51952928 <a title="102-lsi-16" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>17 0.4835864 <a title="102-lsi-17" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>18 0.44146115 <a title="102-lsi-18" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>19 0.43761352 <a title="102-lsi-19" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>20 0.39043424 <a title="102-lsi-20" href="./cvpr-2013-The_Episolar_Constraint%3A_Monocular_Shape_from_Shadow_Correspondence.html">428 cvpr-2013-The Episolar Constraint: Monocular Shape from Shadow Correspondence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.128), (16, 0.045), (26, 0.052), (28, 0.018), (33, 0.231), (52, 0.218), (67, 0.034), (69, 0.044), (87, 0.097), (96, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84968048 <a title="102-lda-1" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>Author: Donald G. Dansereau, Oscar Pizarro, Stefan B. Williams</p><p>Abstract: Plenoptic cameras are gaining attention for their unique light gathering and post-capture processing capabilities. We describe a decoding, calibration and rectification procedurefor lenselet-basedplenoptic cameras appropriatefor a range of computer vision applications. We derive a novel physically based 4D intrinsic matrix relating each recorded pixel to its corresponding ray in 3D space. We further propose a radial distortion model and a practical objective function based on ray reprojection. Our 15-parameter camera model is of much lower dimensionality than camera array models, and more closely represents the physics of lenselet-based cameras. Results include calibration of a commercially available camera using three calibration grid sizes over five datasets. Typical RMS ray reprojection errors are 0.0628, 0.105 and 0.363 mm for 3.61, 7.22 and 35.1 mm calibration grids, respectively. Rectification examples include calibration targets and real-world imagery.</p><p>2 0.84180355 <a title="102-lda-2" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>Author: Zheng Lu, Kristen Grauman</p><p>Abstract: We present a video summarization approach that discovers the story of an egocentric video. Given a long input video, our method selects a short chain of video subshots depicting the essential events. Inspired by work in text analysis that links news articles over time, we define a randomwalk based metric of influence between subshots that reflects how visual objects contribute to the progression of events. Using this influence metric, we define an objective for the optimal k-subshot summary. Whereas traditional methods optimize a summary ’s diversity or representativeness, ours explicitly accounts for how one sub-event “leads to ” another—which, critically, captures event connectivity beyond simple object co-occurrence. As a result, our summaries provide a better sense of story. We apply our approach to over 12 hours of daily activity video taken from 23 unique camera wearers, and systematically evaluate its quality compared to multiple baselines with 34 human subjects.</p><p>3 0.81610435 <a title="102-lda-3" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan</p><p>Abstract: This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) “switch” variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the andnodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC2007and UIUCPeople, and it achieves state-of-the-arts performance.</p><p>4 0.79485476 <a title="102-lda-4" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>Author: Lei Zhang, Yongdong Zhang, Jinhu Tang, Ke Lu, Qi Tian</p><p>Abstract: Binary hashing has been widely used for efficient similarity search due to its query and storage efficiency. In most existing binary hashing methods, the high-dimensional data are embedded into Hamming space and the distance or similarity of two points are approximated by the Hamming distance between their binary codes. The Hamming distance calculation is efficient, however, in practice, there are often lots of results sharing the same Hamming distance to a query, which makes this distance measure ambiguous and poses a critical issue for similarity search where ranking is important. In this paper, we propose a weighted Hamming distance ranking algorithm (WhRank) to rank the binary codes of hashing methods. By assigning different bit-level weights to different hash bits, the returned binary codes are ranked at a finer-grained binary code level. We give an algorithm to learn the data-adaptive and query-sensitive weight for each hash bit. Evaluations on two large-scale image data sets demonstrate the efficacy of our weighted Hamming distance for binary code ranking.</p><p>5 0.79170996 <a title="102-lda-5" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>Author: Raviteja Vemulapalli, Jaishanker K. Pillai, Rama Chellappa</p><p>Abstract: In computer vision applications, features often lie on Riemannian manifolds with known geometry. Popular learning algorithms such as discriminant analysis, partial least squares, support vector machines, etc., are not directly applicable to such features due to the non-Euclidean nature of the underlying spaces. Hence, classification is often performed in an extrinsic manner by mapping the manifolds to Euclidean spaces using kernels. However, for kernel based approaches, poor choice of kernel often results in reduced performance. In this paper, we address the issue of kernelselection for the classification of features that lie on Riemannian manifolds using the kernel learning approach. We propose two criteria for jointly learning the kernel and the classifier using a single optimization problem. Specifically, for the SVM classifier, we formulate the problem of learning a good kernel-classifier combination as a convex optimization problem and solve it efficiently following the multiple kernel learning approach. Experimental results on image set-based classification and activity recognition clearly demonstrate the superiority of the proposed approach over existing methods for classification of manifold features.</p><p>6 0.78431088 <a title="102-lda-6" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>7 0.78246057 <a title="102-lda-7" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>8 0.77968407 <a title="102-lda-8" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>9 0.77888906 <a title="102-lda-9" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>10 0.7787354 <a title="102-lda-10" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>11 0.7787267 <a title="102-lda-11" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>12 0.7783196 <a title="102-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.77810735 <a title="102-lda-13" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>14 0.77789438 <a title="102-lda-14" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>15 0.77761644 <a title="102-lda-15" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>16 0.77737856 <a title="102-lda-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.77653515 <a title="102-lda-17" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>18 0.77615404 <a title="102-lda-18" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>19 0.77592313 <a title="102-lda-19" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<p>20 0.77548927 <a title="102-lda-20" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
