<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>271 cvpr-2013-Locally Aligned Feature Transforms across Views</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-271" href="#">cvpr2013-271</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>271 cvpr-2013-Locally Aligned Feature Transforms across Views</h1>
<br/><p>Source: <a title="cvpr-2013-271-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Locally_Aligned_Feature_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Wei Li, Xiaogang Wang</p><p>Abstract: In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person reidentification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsityinducing norm and information theoretical regularization. . cuhk . edu .hk (a) Camera view A (b) Camera view B This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.</p><p>Reference: <a title="cvpr-2013-271-reference" href="../cvpr2013_reference/cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person reidentification. [sent-2, score-0.755]
</p><p>2 It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. [sent-3, score-0.565]
</p><p>3 The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. [sent-4, score-0.742]
</p><p>4 hk  (a) Camera view A (b) Camera view B This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. [sent-10, score-0.364]
</p><p>5 Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach. [sent-12, score-0.333]
</p><p>6 Introduction Person re-identification is to match the snapshots of pedestrians observed in non-overlapping camera views with  visual features. [sent-14, score-0.481]
</p><p>7 However, this problem is extremely challenging, because it is difficult to match the visual features of pedestrians captured in different camera views due to the large variations of lightings, poses, viewpoints, image resolutions, photometric settings of cameras, and backgrounds. [sent-16, score-0.547]
</p><p>8 Accurate human parsing[18] will benefit person re-identification, but it is a hard problems to solve. [sent-17, score-0.168]
</p><p>9 Existing works solve this challenge in two possible ways: (1) learning the photometric or geometric transforms between two camera views, if the photometric/geometric Figure 1. [sent-18, score-0.436]
</p><p>10 Examples of pedestrians captured in two camera views in the VIPeR dataset[10]. [sent-19, score-0.446]
</p><p>11 Images have different poses, lightings and background even if they are captured in the same camera view. [sent-21, score-0.254]
</p><p>12 models can be assumed [24]; (2) learning a distance metric or projecting visual features from different views into a common feature space for matching in order to suppress inter-camera variations. [sent-22, score-0.47]
</p><p>13 The approaches from both categories assume two fixed camera views with a uni-model inter-camera transform and labeled training samples from the two views are available. [sent-23, score-0.767]
</p><p>14 However, in practice the configurations (which are the combinations of view points, poses, image resolutions, lightings and photometric settings) of pedestrian images are multi-modal even if they are observed  in the same camera views. [sent-24, score-0.426]
</p><p>15 Moreover, given a large camera network in video surveillance, it is impossible to label training samples for every pair of camera views. [sent-27, score-0.602]
</p><p>16 It is highly desirable to develop an algorithm which can match images from two new camera views given training samples collected from other camera views. [sent-28, score-0.745]
</p><p>17 We propose a new approach of learning locally aligned feature transforms across multiple views and apply it to person re-identification. [sent-29, score-0.776]
</p><p>18 The image spaces of two camera views are jointly partitioned based on the similarity of cross-view transforms. [sent-32, score-0.498]
</p><p>19 Sample  pairs with similar transforms are projected to a common feature space for matching. [sent-33, score-0.309]
</p><p>20 (1) As illustrated in Figure 2, the proposed approach automatically partitions the image spaces of two camera views into subregions which correspond to different configurations, and learns a different feature transform for a pair of configurations. [sent-35, score-0.588]
</p><p>21 Given a pair of images to be matched, they are softly assigned to configuration types with a gating network and their visual features are projected to a common feature space and matched by a local expert. [sent-36, score-0.611]
</p><p>22 (3) The image spaces of the two camera views are jointly partitioned instead of separately, to avoid some combinations of configurations rarely appearing in the two views. [sent-40, score-0.535]
</p><p>23 The local experts of these rare combinations cannot be well learned given few, if any, training samples. [sent-41, score-0.267]
</p><p>24 (4) Besides suppressing cross-view variations, the discriminative power of local experts is further increased by locally magnifying inter-person variations. [sent-42, score-0.261]
</p><p>25 (5) This approach is extended to the case when test images are from new camera views not existing in the training set. [sent-43, score-0.47]
</p><p>26 Related Work Metric learning and feature selection have been widely used to reduce cross-view variations and to increase the discriminative power in person re-identification. [sent-46, score-0.325]
</p><p>27 Some approaches [26, 23] assume that all the persons to be identified have samples in the training set. [sent-47, score-0.312]
</p><p>28 Lin and Davis [23] assumed that a feature optimal for distinguishing a pair of persons might not be effective for others, and therefore learned the dissimilarity profiles under a pairwise scheme. [sent-49, score-0.341]
</p><p>29 In order to identify persons outside the training set, Zheng et al. [sent-50, score-0.245]
</p><p>30 [32] formulated person re-identification as a distance learning problem by maximizing the probability that a pair of true match has a smaller distance than a wrong match. [sent-51, score-0.303]
</p><p>31 learned a metric specially designed for identification tasks under pairwise constraints and further kernelized it to overcome the linearity. [sent-54, score-0.235]
</p><p>32 In [11, 25] boosting and RankSVM were used to select features to compute the distance between images observed across camera views. [sent-55, score-0.173]
</p><p>33 proposed a transferred metric learning framework for learning specific metric for different query-candidate combinations. [sent-57, score-0.33]
</p><p>34 Although not being widely applied to person re-identification yet, Canonical Correlation Analysis (CCA)[14] has been used to match data from different views or in different modalities in the applications of face recognition [21, 33] and image-to-text matching [13]. [sent-59, score-0.433]
</p><p>35 All the approaches discussed above assume a single global model or a generic metric, which cannot well handle multiple types of transforms between two views. [sent-60, score-0.184]
</p><p>36 It is also hard for these learning-based approaches to be generalized to new views without re-labeling training data. [sent-61, score-0.297]
</p><p>37 [28] extended their metric learning framework named Large Margin Nearest Neighbor (LMNN) to learn multiple localized metrics for different image clusters. [sent-65, score-0.246]
</p><p>38 In mixture of experts [15], a gating network classified test samples into different clusters and samples within one cluster were classified with the same local expert. [sent-66, score-0.605]
</p><p>39 [30] learned a different metric for each training sample. [sent-70, score-0.219]
</p><p>40 In [6, 7], each training sample had a different metric and all the metrics were aligned with global constraints. [sent-71, score-0.275]
</p><p>41 Differently, we jointly partition the image spaces of two views based on the similarity of cross-view transform. [sent-74, score-0.325]
</p><p>42 Since the possible transforms is much less than the total visual diversity, it leads to a smaller number of local experts which can be well learned 333555999533  Figure 3. [sent-75, score-0.384]
</p><p>43 Our approach automatically separates the two types of features with a proposed sparse gating network. [sent-81, score-0.236]
</p><p>44 (x, y) ∈ Rm Rm are the visual feature vectors of a pair o ∈f images oRbserved in two camera views. [sent-84, score-0.265]
</p><p>45 A pair of test samples to be matched are input to a gating network to choose local experts in a soft way, and matched with the selected experts. [sent-93, score-0.753]
</p><p>46 One way of designing the gating network by following traditional approaches with a single image space is to partition each of the two image spaces separately into K regions, and then learn K2 experts for all the combinations. [sent-96, score-0.534]
</p><p>47 The gating functions in two image spaces are independent, i. [sent-97, score-0.299]
</p><p>48 Since some configurations ( tosx , sy) rarely cboe-rex oifs te xinboth views, not enough training samples can be found to train the experts. [sent-101, score-0.171]
</p><p>49 Instead, we assume the two image samples are correlated and compute the gating function as1  p(s = k|x,y) =? [sent-102, score-0.303]
</p><p>50 Priors The gating function parameters {φk , ψk}kK=1 and expert  parameters n{gW fukn , cVtiokn}K pk=ar1a are etros b {eφ learne}d from training pdaartaa. [sent-121, score-0.368]
</p><p>51 Objective function The objective function on the training set {(xi, yi)}iN=1, wheTrhee ( oxbi,je eycti)iv ies a pair nof o samples nwinithg tsehet same identity but observed in different views, is written as following ? [sent-149, score-0.243]
</p><p>52 Multi-Shot extension All the descriptions so far assume single-shot person reidentification, i. [sent-231, score-0.168]
</p><p>53 for a query sample xi in view A, there is only one sample yi with the same identity in the gallery of view B. [sent-233, score-0.46]
</p><p>54 Multi-shot person re-identification occurs when there are more than one samples Yi with the same identity as xi airne ev mieowr eB t. [sent-234, score-0.291]
</p><p>55 333555999755  Our multi-shot extension makes training easier, since it does not have to match every training pair when learning the  cross-view transforms. [sent-249, score-0.269]
</p><p>56 It only needs to minimize the distance ofbest matched pairs and effectively reduces the number of cross-view transforms in consideration. [sent-250, score-0.304]
</p><p>57 Discriminative metric learning The proposed locally aligned feature transform only reduces the cross-view variations without considering how to discriminate different persons. [sent-253, score-0.346]
</p><p>58 Ti is the set of top ranked samples in view B with xi as query and under the distance | |Wkx Vky| |22. [sent-275, score-0.189]
</p><p>59 50 persons appear in both views and 22 persons appear only in one view. [sent-289, score-0.586]
</p><p>60 These two datasets are used to evaluate person re-identification given two fixed camera views. [sent-291, score-0.341]
</p><p>61 CUHK02 contains 1, 816 persons and five pairs of camera views (P1-P5, ten camera views). [sent-292, score-0.793]
</p><p>62 They have 971, 306, 107, 193 and 239 persons respectively. [sent-293, score-0.178]
</p><p>63 This dataset is used to evaluate the performance when camera views in test are different than those in training. [sent-295, score-0.403]
</p><p>64 (a) The five columns on the left show five images of the same person in each of the two camera views. [sent-304, score-0.341]
</p><p>65 Only two images in different views are shown for each person. [sent-306, score-0.23]
</p><p>66 (b) CUHK02 has five pairs of camera views denoted with P1-P5. [sent-308, score-0.442]
</p><p>67 Two exemplar persons are shown for each pair of views. [sent-309, score-0.231]
</p><p>68 Many state-of-the-art person re-identification methods have published their results on VIPeR and CAVIER. [sent-313, score-0.239]
</p><p>69 The dimensionality of projected common feature space in local experts (i. [sent-326, score-0.252]
</p><p>70 Identification with two fixed camera views It is assumed that all the training and test samples come from the same pair of camera views. [sent-332, score-0.763]
</p><p>71 Singleshot assumes each person has one image in the gallery, while multi-shot assumes M gallery images per person. [sent-335, score-0.339]
</p><p>72 Two protocols on VIPeR were used in the past: randomly splitting the whole dataset into 316 persons for training and the remaining 316 for test; and randomly splitting into 100 persons for training and 532 for test. [sent-340, score-0.522]
</p><p>73 If a person has images in both camera views, we randomly select two pairs of images in different views for training. [sent-349, score-0.61]
</p><p>74 One  query image and one gallery image are randomly selected from the remaining images per person. [sent-350, score-0.231]
</p><p>75 CCA does not work very well since it assumes the feature transforms to be uni-modal while the three datasets are much more complicated. [sent-353, score-0.223]
</p><p>76 For our metho−d,y we also compare with the case when discriminative metric learning described in Section 4. [sent-357, score-0.202]
</p><p>77 Compare rank-n identification rates (%) with other published single-shot results on VIPeR. [sent-369, score-0.212]
</p><p>78 the gating network softly splits the feature space and output is the weighted sum of all experts. [sent-371, score-0.43]
</p><p>79 Figure 7 shows an exemplar pair for each local expert, according to the largest responses of the gating network. [sent-373, score-0.289]
</p><p>80 These pairs show different transforms caused by poses, lightings and backgrounds. [sent-374, score-0.304]
</p><p>81 On CAVIAR, each person has M = 3 gallery images following the protocol in [2]. [sent-381, score-0.339]
</p><p>82 On CUHK02 P1, each person has M = 2 gallery images. [sent-382, score-0.339]
</p><p>83 Its success is also due to the fact that at the training stage it does not try to reduce cross-view transforms for every pair of images, which is difficult, but instead uses a smoothed max function to select the best matches from multi-shots for learning the feature transforms. [sent-385, score-0.39]
</p><p>84 More general camera settings Our method can be easily extended to more general settings when camera views in test are not the same as those in training. [sent-389, score-0.576]
</p><p>85 But when learning the discriminative metric in  4Notice that PS [2] and SDALF  [5] are the only published results on CAVIAR. [sent-390, score-0.273]
</p><p>86 But they both rely on features specially design for person identification according to prior knowledge but without any learning methods. [sent-391, score-0.298]
</p><p>87 Ours NoM denotes our method but without discriminative  metric learning in Section 4. [sent-398, score-0.202]
</p><p>88 Compare rank-n identification rates (%) with other published multi-shot results (M=3) on CAVIAR. [sent-407, score-0.212]
</p><p>89 5, we have to assume each view in the training set could be a query view or a gallery view. [sent-413, score-0.422]
</p><p>90 If the training set contain multiple view pairs, we simply put their training samples together. [sent-414, score-0.263]
</p><p>91 To make results stable, we randomly select a gallery set of 100 persons for 100 times. [sent-417, score-0.349]
</p><p>92 Our method is still effective, because it has the ability to find the best crossview transforms from a complicated training set with combined view pairs. [sent-419, score-0.377]
</p><p>93 Table 5 reports the rank-1 rates when P4 is in test and the training set has different combination of view pairs. [sent-420, score-0.187]
</p><p>94 In CUHK02, the cross-view transforms in P3 have larger difference than those in P4. [sent-421, score-0.184]
</p><p>95 When P3 is added to the training set, the performance of other learning methods (LDM, LMNN and ITML) drops significantly, because it makes the feature transforms in the training set more complicate to learn and there is a larger mismatch between the training set and camera views in test. [sent-422, score-0.874]
</p><p>96 Conclusions We propose locally aligned feature transforms for matching pedestrians across camera views with complex crossview variations. [sent-438, score-0.841]
</p><p>97 Images to be matched are softly assigned to different local experts according to the similarity of crossview transforms, then they are projected to a common feature space and matched with a locally learned discriminative metric. [sent-439, score-0.693]
</p><p>98 It outperforms the state-of-the-art under the setting when two fixed camera views are given. [sent-440, score-0.403]
</p><p>99 Experiments on a small camera network with five pairs of camera views show its good potential of being generalized to generic camera  settings. [sent-441, score-0.857]
</p><p>100 In the future, we will further explore its general-  ization capability by creating a much larger camera network with more diversified cross-view variations. [sent-442, score-0.242]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('viper', 0.257), ('wk', 0.243), ('gating', 0.236), ('vk', 0.234), ('views', 0.23), ('cca', 0.193), ('caviar', 0.185), ('transforms', 0.184), ('persons', 0.178), ('camera', 0.173), ('gallery', 0.171), ('person', 0.168), ('experts', 0.166), ('vky', 0.157), ('wkx', 0.157), ('dld', 0.131), ('eq', 0.128), ('mk', 0.122), ('metric', 0.118), ('ldm', 0.104), ('methodstop', 0.104), ('mlmnn', 0.104), ('softly', 0.086), ('identification', 0.083), ('matched', 0.081), ('lightings', 0.081), ('published', 0.071), ('network', 0.069), ('samples', 0.067), ('training', 0.067), ('cuhk', 0.065), ('expert', 0.065), ('wkt', 0.064), ('crossview', 0.064), ('xepx', 0.064), ('spaces', 0.063), ('view', 0.062), ('query', 0.06), ('rates', 0.058), ('locally', 0.058), ('itml', 0.058), ('identity', 0.056), ('kj', 0.054), ('kt', 0.053), ('pair', 0.053), ('exepxp', 0.052), ('ktktx', 0.052), ('ktykti', 0.052), ('kvyik', 0.052), ('kyx', 0.052), ('pkt', 0.052), ('psosdu', 0.052), ('wkwkt', 0.052), ('wkxki', 0.052), ('xpkt', 0.052), ('xvi', 0.052), ('aligned', 0.05), ('yi', 0.049), ('kk', 0.049), ('projected', 0.047), ('learning', 0.047), ('lished', 0.046), ('yei', 0.046), ('yyjj', 0.046), ('pedestrians', 0.043), ('logdet', 0.043), ('prosser', 0.043), ('reidentification', 0.043), ('zheng', 0.042), ('pedestrian', 0.041), ('canonical', 0.041), ('localized', 0.041), ('sy', 0.04), ('metrics', 0.04), ('feature', 0.039), ('bazzani', 0.039), ('pairs', 0.039), ('profiles', 0.037), ('hong', 0.037), ('configurations', 0.037), ('discriminative', 0.037), ('kong', 0.036), ('projecting', 0.036), ('zhan', 0.036), ('frome', 0.036), ('match', 0.035), ('zij', 0.035), ('learned', 0.034), ('variations', 0.034), ('lmnn', 0.034), ('public', 0.032), ('jointly', 0.032), ('photometric', 0.032), ('protocols', 0.032), ('descent', 0.031), ('bse', 0.031), ('weinberger', 0.03), ('partitions', 0.03), ('poses', 0.03), ('sx', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="271-tfidf-1" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>Author: Wei Li, Xiaogang Wang</p><p>Abstract: In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person reidentification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsityinducing norm and information theoretical regularization. . cuhk . edu .hk (a) Camera view A (b) Camera view B This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.</p><p>2 0.19551922 <a title="271-tfidf-2" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>Author: Sateesh Pedagadi, James Orwell, Sergio Velastin, Boghos Boghossian</p><p>Abstract: Metric learning methods, , forperson re-identification, estimate a scaling for distances in a vector space that is optimized for picking out observations of the same individual. This paper presents a novel approach to the pedestrian re-identification problem that uses metric learning to improve the state-of-the-art performance on standard public datasets. Very high dimensional features are extracted from the source color image. A first processing stage performs unsupervised PCA dimensionality reduction, constrained to maintain the redundancy in color-space representation. A second stage further reduces the dimensionality, using a Local Fisher Discriminant Analysis defined by a training set. A regularization step is introduced to avoid singular matrices during this stage. The experiments conducted on three publicly available datasets confirm that the proposed method outperforms the state-of-the-art performance, including all other known metric learning methods. Furthermore, the method is an effective way to process observations comprising multiple shots, and is non-iterative: the computation times are relatively modest. Finally, a novel statistic is derived to characterize the Match Characteris- tic: the normalized entropy reduction can be used to define the ’Proportion of Uncertainty Removed’ (PUR). This measure is invariant to test set size and provides an intuitive indication of performance.</p><p>3 0.18323776 <a title="271-tfidf-3" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.</p><p>4 0.10135079 <a title="271-tfidf-4" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>5 0.097349085 <a title="271-tfidf-5" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>Author: Zhen Li, Shiyu Chang, Feng Liang, Thomas S. Huang, Liangliang Cao, John R. Smith</p><p>Abstract: This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community.</p><p>6 0.087119147 <a title="271-tfidf-6" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>7 0.085207306 <a title="271-tfidf-7" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>8 0.084309332 <a title="271-tfidf-8" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>9 0.081221744 <a title="271-tfidf-9" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>10 0.081071891 <a title="271-tfidf-10" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>11 0.080160625 <a title="271-tfidf-11" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<p>12 0.079514518 <a title="271-tfidf-12" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>13 0.07895539 <a title="271-tfidf-13" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>14 0.076687925 <a title="271-tfidf-14" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>15 0.076632053 <a title="271-tfidf-15" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>16 0.075998381 <a title="271-tfidf-16" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>17 0.075088978 <a title="271-tfidf-17" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>18 0.073930494 <a title="271-tfidf-18" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>19 0.073356733 <a title="271-tfidf-19" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>20 0.071828708 <a title="271-tfidf-20" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, -0.002), (2, -0.045), (3, 0.002), (4, 0.038), (5, 0.005), (6, -0.02), (7, -0.056), (8, 0.049), (9, -0.044), (10, -0.022), (11, -0.004), (12, 0.063), (13, -0.052), (14, -0.036), (15, -0.03), (16, -0.038), (17, 0.026), (18, -0.003), (19, -0.057), (20, 0.013), (21, 0.022), (22, -0.116), (23, 0.037), (24, -0.025), (25, -0.059), (26, -0.037), (27, 0.083), (28, -0.019), (29, -0.029), (30, -0.029), (31, -0.021), (32, 0.015), (33, -0.01), (34, 0.041), (35, 0.01), (36, -0.002), (37, 0.036), (38, -0.065), (39, -0.105), (40, 0.036), (41, 0.025), (42, 0.116), (43, 0.033), (44, 0.029), (45, -0.076), (46, -0.01), (47, -0.07), (48, 0.009), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91716206 <a title="271-lsi-1" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>Author: Wei Li, Xiaogang Wang</p><p>Abstract: In this paper, we propose a new approach for matching images observed in different camera views with complex cross-view transforms and apply it to person reidentification. It jointly partitions the image spaces of two camera views into different configurations according to the similarity of cross-view transforms. The visual features of an image pair from different views are first locally aligned by being projected to a common feature space and then matched with softly assigned metrics which are locally optimized. The features optimal for recognizing identities are different from those for clustering cross-view transforms. They are jointly learned by utilizing sparsityinducing norm and information theoretical regularization. . cuhk . edu .hk (a) Camera view A (b) Camera view B This approach can be generalized to the settings where test images are from new camera views, not the same as those in the training set. Extensive experiments are conducted on public datasets and our own dataset. Comparisons with the state-of-the-art metric learning and person re-identification methods show the superior performance of our approach.</p><p>2 0.80433595 <a title="271-lsi-2" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.</p><p>3 0.76561338 <a title="271-lsi-3" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>Author: Zhen Li, Shiyu Chang, Feng Liang, Thomas S. Huang, Liangliang Cao, John R. Smith</p><p>Abstract: This paper considers the person verification problem in modern surveillance and video retrieval systems. The problem is to identify whether a pair of face or human body images is about the same person, even if the person is not seen before. Traditional methods usually look for a distance (or similarity) measure between images (e.g., by metric learning algorithms), and make decisions based on a fixed threshold. We show that this is nevertheless insufficient and sub-optimal for the verification problem. This paper proposes to learn a decision function for verification that can be viewed as a joint model of a distance metric and a locally adaptive thresholding rule. We further formulate the inference on our decision function as a second-order large-margin regularization problem, and provide an efficient algorithm in its dual from. We evaluate our algorithm on both human body verification and face verification problems. Our method outperforms not only the classical metric learning algorithm including LMNN and ITML, but also the state-of-the-art in the computer vision community.</p><p>4 0.76538044 <a title="271-lsi-4" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>Author: Sateesh Pedagadi, James Orwell, Sergio Velastin, Boghos Boghossian</p><p>Abstract: Metric learning methods, , forperson re-identification, estimate a scaling for distances in a vector space that is optimized for picking out observations of the same individual. This paper presents a novel approach to the pedestrian re-identification problem that uses metric learning to improve the state-of-the-art performance on standard public datasets. Very high dimensional features are extracted from the source color image. A first processing stage performs unsupervised PCA dimensionality reduction, constrained to maintain the redundancy in color-space representation. A second stage further reduces the dimensionality, using a Local Fisher Discriminant Analysis defined by a training set. A regularization step is introduced to avoid singular matrices during this stage. The experiments conducted on three publicly available datasets confirm that the proposed method outperforms the state-of-the-art performance, including all other known metric learning methods. Furthermore, the method is an effective way to process observations comprising multiple shots, and is non-iterative: the computation times are relatively modest. Finally, a novel statistic is derived to characterize the Match Characteris- tic: the normalized entropy reduction can be used to define the ’Proportion of Uncertainty Removed’ (PUR). This measure is invariant to test set size and provides an intuitive indication of performance.</p><p>5 0.64850831 <a title="271-lsi-5" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>6 0.60022032 <a title="271-lsi-6" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>7 0.58625895 <a title="271-lsi-7" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>8 0.58335698 <a title="271-lsi-8" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>9 0.56134802 <a title="271-lsi-9" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>10 0.55659616 <a title="271-lsi-10" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<p>11 0.55356795 <a title="271-lsi-11" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>12 0.553101 <a title="271-lsi-12" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>13 0.54913878 <a title="271-lsi-13" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>14 0.5442363 <a title="271-lsi-14" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>15 0.53767759 <a title="271-lsi-15" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>16 0.52869207 <a title="271-lsi-16" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>17 0.52013814 <a title="271-lsi-17" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>18 0.51065367 <a title="271-lsi-18" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>19 0.50544828 <a title="271-lsi-19" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>20 0.4878386 <a title="271-lsi-20" href="./cvpr-2013-Pedestrian_Detection_with_Unsupervised_Multi-stage_Feature_Learning.html">328 cvpr-2013-Pedestrian Detection with Unsupervised Multi-stage Feature Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.074), (16, 0.41), (26, 0.046), (33, 0.216), (67, 0.075), (69, 0.044), (87, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92027974 <a title="271-lda-1" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>Author: Hyeongwoo Kim, Hailin Jin, Sunil Hadap, Inso Kweon</p><p>Abstract: We present a novel method to separate specular reflection from a single image. Separating an image into diffuse and specular components is an ill-posed problem due to lack of observations. Existing methods rely on a specularfree image to detect and estimate specularity, which however may confuse diffuse pixels with the same hue but a different saturation value as specular pixels. Our method is based on a novel observation that for most natural images the dark channel can provide an approximate specular-free image. We also propose a maximum a posteriori formulation which robustly recovers the specular reflection and chromaticity despite of the hue-saturation ambiguity. We demonstrate the effectiveness of the proposed algorithm on real and synthetic examples. Experimental results show that our method significantly outperforms the state-of-theart methods in separating specular reflection.</p><p>2 0.8192398 <a title="271-lda-2" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>Author: Guha Balakrishnan, Fredo Durand, John Guttag</p><p>Abstract: We extract heart rate and beat lengths from videos by measuring subtle head motion caused by the Newtonian reaction to the influx of blood at each beat. Our method tracks features on the head and performs principal component analysis (PCA) to decompose their trajectories into a set of component motions. It then chooses the component that best corresponds to heartbeats based on its temporal frequency spectrum. Finally, we analyze the motion projected to this component and identify peaks of the trajectories, which correspond to heartbeats. When evaluated on 18 subjects, our approach reported heart rates nearly identical to an electrocardiogram device. Additionally we were able to capture clinically relevant information about heart rate variability.</p><p>3 0.78598142 <a title="271-lda-3" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>4 0.76979464 <a title="271-lda-4" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>Author: Ahmed T. Kamal, Jay A. Farrell, Amit K. Roy-Chowdhury</p><p>Abstract: Due to their high fault-tolerance, ease of installation and scalability to large networks, distributed algorithms have recently gained immense popularity in the sensor networks community, especially in computer vision. Multitarget tracking in a camera network is one of the fundamental problems in this domain. Distributed estimation algorithms work by exchanging information between sensors that are communication neighbors. Since most cameras are directional sensors, it is often the case that neighboring sensors may not be sensing the same target. Such sensors that do not have information about a target are termed as “naive ” with respect to that target. In this paper, we propose consensus-based distributed multi-target tracking algorithms in a camera network that are designed to address this issue of naivety. The estimation errors in tracking and data association, as well as the effect of naivety, are jointly addressed leading to the development of an informationweighted consensus algorithm, which we term as the Multitarget Information Consensus (MTIC) algorithm. The incorporation of the probabilistic data association mecha- nism makes the MTIC algorithm very robust to false measurements/clutter. Experimental analysis is provided to support the theoretical results.</p><p>5 0.75933886 <a title="271-lda-5" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>Author: Junjie Yan, Xucong Zhang, Zhen Lei, Shengcai Liao, Stan Z. Li</p><p>Abstract: The serious performance decline with decreasing resolution is the major bottleneck for current pedestrian detection techniques [14, 23]. In this paper, we take pedestrian detection in different resolutions as different but related problems, and propose a Multi-Task model to jointly consider their commonness and differences. The model contains resolution aware transformations to map pedestrians in different resolutions to a common space, where a shared detector is constructed to distinguish pedestrians from background. For model learning, we present a coordinate descent procedure to learn the resolution aware transformations and deformable part model (DPM) based detector iteratively. In traffic scenes, there are many false positives located around vehicles, therefore, we further build a context model to suppress them according to the pedestrian-vehicle relationship. The context model can be learned automatically even when the vehicle annotations are not available. Our method reduces the mean miss rate to 60% for pedestrians taller than 30 pixels on the Caltech Pedestrian Benchmark, which noticeably outperforms previous state-of-the-art (71%).</p><p>same-paper 6 0.75657368 <a title="271-lda-6" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>7 0.74909461 <a title="271-lda-7" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>8 0.71488637 <a title="271-lda-8" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>9 0.69587386 <a title="271-lda-9" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>10 0.64076155 <a title="271-lda-10" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>11 0.63108164 <a title="271-lda-11" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>12 0.62998736 <a title="271-lda-12" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>13 0.62527275 <a title="271-lda-13" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>14 0.62328064 <a title="271-lda-14" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>15 0.61465049 <a title="271-lda-15" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>16 0.61252129 <a title="271-lda-16" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>17 0.61226892 <a title="271-lda-17" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>18 0.60654938 <a title="271-lda-18" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>19 0.60592532 <a title="271-lda-19" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>20 0.6016193 <a title="271-lda-20" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
