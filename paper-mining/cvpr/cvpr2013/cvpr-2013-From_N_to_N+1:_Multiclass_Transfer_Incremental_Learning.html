<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-179" href="#">cvpr2013-179</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</h1>
<br/><p>Source: <a title="cvpr-2013-179-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Kuzborskij_From_N_to_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>Reference: <a title="cvpr-2013-179-reference" href="../cvpr2013_reference/cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. [sent-7, score-0.591]
</p><p>2 Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. [sent-8, score-0.37]
</p><p>3 But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. [sent-9, score-1.176]
</p><p>4 This is an open challenge for existing transfer learning algorithms. [sent-10, score-0.526]
</p><p>5 Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. [sent-12, score-0.457]
</p><p>6 They also share the need to update their knowledge over time, by learning new category models whenever faced with unknown objects. [sent-16, score-0.154]
</p><p>7 Can you add effectively the new target N + 1-th class model to the known N source models by leveraging over them, while at the same time preserving their classification abilities? [sent-23, score-0.581]
</p><p>8 The problem of how to learn a new object category from few annotated samples by exploiting prior knowledge has been extensively studied [20, 11, 7]. [sent-25, score-0.132]
</p><p>9 binary classification) rather than the multiclass case [1, 19, 18]. [sent-28, score-0.212]
</p><p>10 In addition, learning from scratch and preserving training sets from all the source tasks might be infeasible due to the large number oftasks or when acquiring tasks incrementally, especially for large datasets [15]. [sent-31, score-0.452]
</p><p>11 In object categorization case this might come as training source classifiers from large scale visual datasets, in abundance of data. [sent-32, score-0.39]
</p><p>12 Consider the following example: a transfer learning task of learning a dog detector, given that the system has already 333333555866  Figure 1: Binary (left) versus N −→ N + 1transfer learning (right). [sent-33, score-0.698]
</p><p>13 iInna r byo t(hle cases, sturasn Nsfer − learning implies rth laeta trhnetarget class is learned close to where informative sources models are. [sent-34, score-0.191]
</p><p>14 This is likely to affect negatively performance in the N −→ N + 1case, where one aims for optimal accuracy on −th→e sources acnasde target cela osnsees a simultaneously. [sent-35, score-0.217]
</p><p>15 Success in this setting is defined as op-  timizing the accuracy of the dog detector, with a minimal number of annotated training samples (Figure 1, left). [sent-38, score-0.192]
</p><p>16 But if we consider the multiclass case, the different tasks now “overlap”. [sent-39, score-0.18]
</p><p>17 [18], a transfer learning method based on the multiclass extension of Least-Squares Support Vector Machine (LSSVM) [16]. [sent-44, score-0.75]
</p><p>18 Thanks to the linear nature of LSSVM, we cast transfer learning as a constraint for the classifier of the N + 1target class to be close to a subset of the N source classifiers. [sent-45, score-0.956]
</p><p>19 At the same time, we impose a stability to the system, biasing the formulation towards solutions close to the hyperplanes of the N source classes. [sent-46, score-0.423]
</p><p>20 In practice, given N source models, we require that these models would not change much when going from N to N + 1. [sent-47, score-0.325]
</p><p>21 As in [18], we learn how much to transfer from each of the source classifiers, by minimizing the Leave-One-Out (LOO) error, which is an unbiased estimator of the generalization error for a classifier [4]. [sent-48, score-0.857]
</p><p>22 Experiments on various subsets of the Caltech-256 [9] and Animals with Attributes (AwA) datasets [13] show that our algorithm outperforms the One-Versus-All (OVA) extension of [18], as well as other baselines [11, 20, 1]. [sent-50, score-0.256]
</p><p>23 Moreover, its performance often is comparable to what it would be obtained by re-training the whole N + 1classifier from all data, without the need to store the source training data. [sent-51, score-0.356]
</p><p>24 Related Work Prior work in transfer learning addresses mostly the binary classification problem (object detection). [sent-55, score-0.558]
</p><p>25 Some approaches transfer information through samples belonging to both source and target domains during the training process, as in [14] for reinforcement learning. [sent-56, score-1.092]
</p><p>26 Feature space approaches consider transferring or sharing feature space representations between source and target domains. [sent-57, score-0.492]
</p><p>27 Typically, in this setting source and target domain samples are available to the learner. [sent-58, score-0.609]
</p><p>28 Yao and Doretto [20] proposed an AdaBoost-based method  using multiple source domains for the object detection task. [sent-62, score-0.351]
</p><p>29 Another research line favors model-transfer (or parameter-transfer) methods, where the only knowledge available to the learner is “condensed” within a model trained on the source domain. [sent-63, score-0.353]
</p><p>30 Model-transfer is theoretically sound as was shown by Kuzborskij and Orabona [12], since relatedness of the source and target tasks enables quick convergence of the empirical error estimate to the true error. [sent-65, score-0.452]
</p><p>31 2 regularization, the goal of the algorithm is to keep the target domain classifier “close” to the one trained on the source domain. [sent-69, score-0.534]
</p><p>32 [18] proposed a multi-source transfer model with a similar regularizer, where each source classifier was weighted by learned coefficients. [sent-71, score-0.83]
</p><p>33 The method obtained strong results on the visual object detection task, using only a small amount of samples from the target domain. [sent-72, score-0.202]
</p><p>34 Both methods rely on weighted source classifiers, which is crucial when attempting to avoid negative transfer. [sent-74, score-0.354]
</p><p>35 Several Multiple Kernel Learning (MKL) methods were proposed for solving transfer learning problems. [sent-75, score-0.526]
</p><p>36 [11] suggested to use MKL kernel weights as source classifier  weights, proposing one of the few truly multiclass transfer learning models. [sent-77, score-1.107]
</p><p>37 There, kernel weights affect both the source classifiers and the representation of the target domain. [sent-80, score-0.543]
</p><p>38 As in related literature, we define a set of M training samples consisting of a feature vector xi ∈ Rd and the corresponding label yi ∈ Y = {1, . [sent-91, score-0.151]
</p><p>39 A common way to find the set of hyperplanes W is by solving a regularized problem with a convex loss function,  +  Wn? [sent-113, score-0.171]
</p><p>40 Defining the label matrix Y such that Yin is equal to 1 if yi = n and −1 otherwise, we obtain the multiclass LSSVM objective f −u1nct oiothne  Wmi,nb12? [sent-117, score-0.253]
</p><p>41 First, we have a set of models that were obtained from the source N class problem. [sent-126, score-0.386]
</p><p>42 These source models are encoded as a set of N hyperplanes, that we again represent in matrix form as W? [sent-127, score-0.325]
</p><p>43 Note that we assume no access to the samples used to train the source classifiers. [sent-134, score-0.4]
</p><p>44 Second, we have a small training set composed from samples belonging to all the N + 1classes, target and source classes. [sent-135, score-0.587]
</p><p>45 , WN] , wN+1, such that i) performance on the target N + 1-th class improves by transferring from the source models, and ii) performance on the source N classes should not deteriorate or even improve compared to the former. [sent-140, score-0.945]
</p><p>46 The first objective can be recognized as the transfer learning problem. [sent-143, score-0.554]
</p><p>47 lose to a linear combination of the source models, while negative transfer is prevented by weighing the amount of transfer of each source model using the coefficientvectorβ = [β1 . [sent-150, score-1.572]
</p><p>48 However, as explained before, adding a target class may affect the performance of the source models and it is therefore useful to transfer the novel information back to the N source models. [sent-157, score-1.324]
</p><p>49 To prevent negative transfer, we enforce the new hyperplanes W to remain close to the source hyperplanes W? [sent-158, score-0.521]
</p><p>50 Self-tuning of Transfer Parameters We want to set the transfer coefficients β to improve the performance by exploiting only relevant source models while preventing negative transfer. [sent-227, score-0.786]
</p><p>51 We now need a convex multiclass loss to measure the LOO errors. [sent-240, score-0.253]
</p><p>52 We could choose the the convex multiclass loss presented in [5], which keeps samples of different classes at the unit marginal distance:  L(β,i) = mr? [sent-241, score-0.395]
</p><p>53 However, from (1) and (2) it iws possible to see that by changing β we only change the scores of the target N + 1-th class. [sent-243, score-0.127]
</p><p>54 =ayxi|1 + Yir(β) − Yiyi(β)|+  = N+ 1 : yi == NN ++ 11 : yi  The rationale behind this loss is to enforce a margin of 1 between the target N + 1-th class and the correct one, even when the N + 1-th class has not the highest score. [sent-248, score-0.381]
</p><p>55 Given the LOO errors and the multiclass loss function, we can obtain β by solving the convex problem ? [sent-250, score-0.253]
</p><p>56 The source code of MULTIpLE is available online1 . [sent-310, score-0.325]
</p><p>57 Experiments We present here a series of experiments designed to investigate the behavior of our algorithm when (a) the source classes and the target class are related/unrelated, and when (b) the overall number of classes increases. [sent-312, score-0.705]
</p><p>58 1), then we describe the chosen baselines (section 5. [sent-315, score-0.192]
</p><p>59 These were then split in three disjoint sets: 30 samples for the source classifier, 20 samples for training and 30 samples for test. [sent-328, score-0.581]
</p><p>60 The samples of the source classifier were used for training the N models W? [sent-329, score-0.475]
</p><p>61 2) was evaluated using progressively {5, 10, 15, 20} training samples ufoatre dea uchsi nofg th preo Ngre+s s1i cvlealysse {s5. [sent-332, score-0.134]
</p><p>62 Furthermore, to get a reliable estimate of the performance of transfer with respect to different classes, we used a leave-one-class-out approach, considering in turn each class as the N + 1target class, and the other N as source classifiers. [sent-334, score-0.847]
</p><p>63 NTh −e no Ntra +ns 1fe prr boablseemlin beys are the following: No transfer corresponds to LSSVM trained only on the new training data. [sent-340, score-0.492]
</p><p>64 assuming to have access to all the data used to build the source models plus the new training data. [sent-343, score-0.356]
</p><p>65 de / an important reference for assessing the results obtained by transfer learning methods. [sent-351, score-0.526]
</p><p>66 Source+1 corresponds to a binary LSSVM trained to discriminate between the target class vs the source classes given the training data. [sent-354, score-0.643]
</p><p>67 x As transfer baselines, we chose the following methods: MKTL We compared against Multi Kernel Transfer Learning (MKTL) [11], which is one of the few existing discriminative transfer learning algorithm in multiclass formulation. [sent-360, score-1.167]
</p><p>68 MultiKT-OVA We implemented an OVA multiclass extension of the binary transfer learning method by Tommasi et al. [sent-361, score-0.813]
</p><p>69 At the same time we use Source as the source classifier. [sent-363, score-0.325]
</p><p>70 PMT-SVM-OVA We also implemented an OVA multiclass extension of the binary transfer learning method by Aytar and Zisserman [1], as done for MultiKT-OVA. [sent-365, score-0.813]
</p><p>71 MultisourceTrAdaBoost-OVA As a final transfer learning baseline, we implemented an OVA extension of MultisourceTrAdaBoost [20], where each source corresponds to a subset of samples designated for the source classifier, while belonging to a specific class. [sent-366, score-1.355]
</p><p>72 e dIn b case oolfd dm cordosesl-transfer algorithms, source model’s C value was reused. [sent-377, score-0.325]
</p><p>73 Bottom row: transfer and competitive no transfer baselines, average of RBF kernels over all features. [sent-386, score-0.982]
</p><p>74 [18], we performed experiments on different groups of related, unrelated and mixed categories for both databases. [sent-392, score-0.29]
</p><p>75 For the Caltech-256 database, the related classes were chosen from the “quadruped animals” subset; the unrelated classes were chosen randomly from the whole dataset, and the mixed classes were taken from the “quadruped animals” and the “ground transportation” subsets, sampled in equal proportions. [sent-393, score-0.555]
</p><p>76 For the AwA database, the related classes were chosen from the “quadruped animals” subset; the unrelated classes were randomly chosen from the whole dataset, and the mixed classes were sampled in equal proportions from the subsets “quadruped animals” and “aquatic animals”. [sent-394, score-0.607]
</p><p>77 This setting allows us to evaluate how MULTIpLE, and the chosen baselines, are able to exploit the source knowledge in different situations, while considering the overall accuracy. [sent-395, score-0.455]
</p><p>78 To assess the performance of all methods as the overall number of classes grows, we repeated all experiments in333333666311  Figure 3: Results for N + 1= 20, AwA, transfer and competitive no transfer baselines, average of RBF kernels, all features. [sent-396, score-1.041]
</p><p>79 The left column shows the results for the unrelated setting; the center column shows the results for the mixed setting, and the right column shows the results for the related setting. [sent-402, score-0.371]
</p><p>80 The first row compares the results obtained by MULTIpLE with those of the no transfer baselines (Section 5. [sent-403, score-0.621]
</p><p>81 This is a remarkable result, as the Batch method constitutes an important reference for the behavior of transfer learning algorithms in this setting (Section 5. [sent-408, score-0.602]
</p><p>82 Figure 2, middle row, reports results obtained for MULTIpLE and all transfer learning baselines, as defined in Section 5. [sent-410, score-0.526]
</p><p>83 We mark these cases with a star on the plots (Figure 2, middle and bottom  3All experimental results and the source code http : / /www . [sent-417, score-0.325]
</p><p>84 With respect to the transfer baselines, the related setting seems to be the one more favorable to our approach. [sent-421, score-0.533]
</p><p>85 With respect to the no transfer baselines, MULTIpLE seems to perform better in the unrelated case. [sent-422, score-0.684]
</p><p>86 The performance of PMT-SVM-OVA and MultisourceTrAdaBoost-OVA is disappointing, compared with what achieved by the other two transfer learning baselines, i. [sent-423, score-0.526]
</p><p>87 This is true for all settings (related, unrelated and mixed). [sent-426, score-0.195]
</p><p>88 Figure 3 shows results for N + 1 = 20 classes on the AwA dataset, for the unrelated (left), mixed (center) and related (right) settings, all features (averaged RBF kernels). [sent-429, score-0.357]
</p><p>89 For sake of readability, we report here only the baselines which were competitive with, or better than, MULTIpLE in the N + 1= 5 case, in at least one setting. [sent-430, score-0.186]
</p><p>90 We see that here our algorithm consistently outperforms all transfer learning baselines, especially with a small training set, while obtaining a performance remarkably similar to Batch, in terms of accuracy and behavior. [sent-431, score-0.557]
</p><p>91 These results  suggest that, as the number of sources grows, our method gets closer to the Batch performance while using only a considerably smaller amount of data the ultimate goal of any effective transfer learning method. [sent-434, score-0.591]
</p><p>92 Discussion and Conclusions All results confirm our claim that the mere extension to multiclass of existing binary transfer learning algorithms is not sufficient to address the N −→ N + 1problem. [sent-437, score-0.782]
</p><p>93 One might argue that the worse performance of the transfer learning baselines depends on how we implemented the OVA extension for such binary methods. [sent-443, score-0.793]
</p><p>94 Still, the results obtained by MKTL, the only transfer learning baseline with a multiclass formulation, clearly indicate that the ability to handle multiple sources by itself is not the solution. [sent-444, score-0.797]
</p><p>95 To gain a better understanding on how MULTIpLE balances the need to preserve performance over the sources, and the learning of the target class, we show the accuracy plots for the AWA experiments, N + 1 = 20, unrelated, for the N sources and for the +1 target separately (Figure 4). [sent-445, score-0.409]
</p><p>96 Both methods do not aggressively leverage over sources for learning the target class, as done by MultiKT-OVA and MKTL (to a lesser extent), although MULTIpLE seems to be able to do so better than Batch. [sent-447, score-0.285]
</p><p>97 As opposed to this, the OVA extensions of existing binary transfer learning algorithms are more biased towards a strong exploitation of source knowledge when learning the target class, at the expenses of the overall performance. [sent-450, score-1.129]
</p><p>98 How to combine these two aspects, namely how to design principled methods able to obtain an overall accuracy comparable to that of the Batch method while at the same time boosting the learning of the target class, remains the open challenge of the N −→ N 1transfer learning problem. [sent-451, score-0.283]
</p><p>99 On the algorithmic implementation of multiclass kernel-based vector machines. [sent-486, score-0.217]
</p><p>100 Is learning the n-th thing any easier than learning the first? [sent-563, score-0.13]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transfer', 0.461), ('source', 0.325), ('awa', 0.248), ('ova', 0.219), ('unrelated', 0.195), ('lssvm', 0.186), ('multiclass', 0.18), ('tommasi', 0.166), ('baselines', 0.16), ('wn', 0.153), ('loo', 0.138), ('yiyi', 0.134), ('target', 0.127), ('batch', 0.115), ('mktl', 0.107), ('multikt', 0.107), ('quadruped', 0.107), ('hyperplanes', 0.098), ('mixed', 0.095), ('orabona', 0.08), ('yir', 0.08), ('animals', 0.077), ('samples', 0.075), ('idiap', 0.071), ('classes', 0.067), ('sources', 0.065), ('learning', 0.065), ('aytar', 0.062), ('phog', 0.062), ('class', 0.061), ('rbf', 0.058), ('ayxi', 0.053), ('kuzborskij', 0.053), ('tranfer', 0.053), ('subsets', 0.052), ('yin', 0.05), ('hinge', 0.048), ('iku', 0.047), ('mkl', 0.047), ('yi', 0.045), ('extension', 0.044), ('setting', 0.044), ('reinforcement', 0.044), ('stars', 0.044), ('barbara', 0.044), ('wilcoxon', 0.044), ('classifier', 0.044), ('dog', 0.042), ('loss', 0.042), ('jie', 0.041), ('addressing', 0.041), ('transferring', 0.04), ('food', 0.039), ('domain', 0.038), ('significance', 0.038), ('leveraging', 0.037), ('algorithmic', 0.037), ('robot', 0.036), ('blitzer', 0.035), ('classifiers', 0.034), ('kernels', 0.034), ('daum', 0.033), ('chosen', 0.032), ('behavior', 0.032), ('binary', 0.032), ('faced', 0.032), ('kernel', 0.032), ('implemented', 0.031), ('preserving', 0.031), ('convex', 0.031), ('acl', 0.031), ('training', 0.031), ('adaptation', 0.03), ('category', 0.029), ('subgradient', 0.029), ('switzerland', 0.029), ('belonging', 0.029), ('achievable', 0.029), ('attempting', 0.029), ('kernelized', 0.029), ('progressively', 0.028), ('knowledge', 0.028), ('seems', 0.028), ('par', 0.028), ('objective', 0.028), ('unbiased', 0.027), ('hyperplane', 0.027), ('multi', 0.027), ('duan', 0.027), ('column', 0.027), ('bn', 0.026), ('multiple', 0.026), ('overall', 0.026), ('competitive', 0.026), ('equipped', 0.025), ('constraining', 0.025), ('preserve', 0.025), ('affect', 0.025), ('regularizer', 0.024), ('animal', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="179-tfidf-1" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>2 0.22553833 <a title="179-tfidf-2" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>Author: Lin Chen, Lixin Duan, Dong Xu</p><p>Abstract: In this work, we propose to leverage a large number of loosely labeled web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which the samples from different source domains can be represented by different types of features with different dimensions (e.g., the SIFTfeaturesfrom web images and space-time (ST) features from web videos) while the target domain samples have all types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier, in which we simultaneously seek the optimal weights for different source domains with different types of features as well as infer the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of MDA-HS for event recognition in consumer videos.</p><p>3 0.20305352 <a title="179-tfidf-3" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>4 0.15463108 <a title="179-tfidf-4" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>5 0.15388286 <a title="179-tfidf-5" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>6 0.13240816 <a title="179-tfidf-6" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<p>7 0.12904067 <a title="179-tfidf-7" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>8 0.10862404 <a title="179-tfidf-8" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>9 0.10356988 <a title="179-tfidf-9" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>10 0.10153499 <a title="179-tfidf-10" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>11 0.08934246 <a title="179-tfidf-11" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>12 0.086682349 <a title="179-tfidf-12" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>13 0.085830435 <a title="179-tfidf-13" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>14 0.085362948 <a title="179-tfidf-14" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>15 0.083666407 <a title="179-tfidf-15" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>16 0.083350338 <a title="179-tfidf-16" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>17 0.07867384 <a title="179-tfidf-17" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>18 0.078610137 <a title="179-tfidf-18" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>19 0.074704088 <a title="179-tfidf-19" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<p>20 0.073552929 <a title="179-tfidf-20" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.178), (1, -0.067), (2, -0.065), (3, 0.019), (4, 0.043), (5, 0.035), (6, -0.05), (7, 0.002), (8, 0.001), (9, 0.065), (10, -0.012), (11, -0.045), (12, -0.02), (13, -0.062), (14, -0.13), (15, -0.097), (16, -0.009), (17, -0.104), (18, -0.033), (19, -0.036), (20, -0.106), (21, -0.141), (22, -0.055), (23, -0.067), (24, 0.018), (25, 0.035), (26, 0.053), (27, -0.034), (28, -0.021), (29, -0.035), (30, -0.056), (31, -0.066), (32, -0.009), (33, 0.006), (34, -0.049), (35, -0.074), (36, -0.005), (37, 0.049), (38, 0.011), (39, 0.048), (40, -0.059), (41, 0.043), (42, -0.028), (43, -0.001), (44, 0.043), (45, -0.011), (46, 0.059), (47, 0.053), (48, 0.008), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95681012 <a title="179-lsi-1" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>2 0.88907272 <a title="179-lsi-2" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>Author: Lin Chen, Lixin Duan, Dong Xu</p><p>Abstract: In this work, we propose to leverage a large number of loosely labeled web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which the samples from different source domains can be represented by different types of features with different dimensions (e.g., the SIFTfeaturesfrom web images and space-time (ST) features from web videos) while the target domain samples have all types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier, in which we simultaneously seek the optimal weights for different source domains with different types of features as well as infer the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of MDA-HS for event recognition in consumer videos.</p><p>3 0.86933213 <a title="179-lsi-3" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>Author: Jeff Donahue, Judy Hoffman, Erik Rodner, Kate Saenko, Trevor Darrell</p><p>Abstract: Most successful object classification and detection methods rely on classifiers trained on large labeled datasets. However, for domains where labels are limited, simply borrowing labeled data from existing datasets can hurt performance, a phenomenon known as “dataset bias.” We propose a general framework for adapting classifiers from “borrowed” data to the target domain using a combination of available labeled and unlabeled examples. Specifically, we show that imposing smoothness constraints on the classifier scores over the unlabeled data can lead to improved adaptation results. Such constraints are often available in the form of instance correspondences, e.g. when the same object or individual is observed simultaneously from multiple views, or tracked between video frames. In these cases, the object labels are unknown but can be constrained to be the same or similar. We propose techniques that build on existing domain adaptation methods by explicitly modeling these relationships, and demonstrate empirically that they improve recognition accuracy in two scenarios, multicategory image classification and object detection in video.</p><p>4 0.78160131 <a title="179-lsi-4" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>Author: Jie Ni, Qiang Qiu, Rama Chellappa</p><p>Abstract: Domain adaptation addresses the problem where data instances of a source domain have different distributions from that of a target domain, which occurs frequently in many real life scenarios. This work focuses on unsupervised domain adaptation, where labeled data are only available in the source domain. We propose to interpolate subspaces through dictionary learning to link the source and target domains. These subspaces are able to capture the intrinsic domain shift and form a shared feature representation for cross domain recognition. Further, we introduce a quantitative measure to characterize the shift between two domains, which enables us to select the optimal domain to adapt to the given multiple source domains. We present exumd .edu , rama@umiacs .umd .edu training and testing data are captured from the same underlying distribution. Yet this assumption is often violated in many real life applications. For instance, images collected from an internet search engine are compared with those captured from real life [28, 4]. Face recognition systems trained on frontal and high resolution images, are applied to probe images with non-frontal poses and low resolution [6]. Human actions are recognized from an unseen target view using training data taken from source views [21, 20]. We show some examples of dataset shifts in Figure 1. In these scenarios, magnitudes of variations of innate characteristics, which distinguish one class from another, are oftentimes smaller than the variations caused by distribution shift between training and testing dataset. Directly applying the classifier from the training set to testing set periments on face recognition across pose, illumination and blur variations, cross dataset object recognition, and report improved performance over the state of the art.</p><p>5 0.64755195 <a title="179-lsi-5" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>Author: Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu, Cunzhao Shi</p><p>Abstract: In this paper, we propose a novel method for cross-view action recognition via a continuous virtual path which connects the source view and the target view. Each point on this virtual path is a virtual view which is obtained by a linear transformation of the action descriptor. All the virtual views are concatenated into an infinite-dimensional feature to characterize continuous changes from the source to the target view. However, these infinite-dimensional features cannot be used directly. Thus, we propose a virtual view kernel to compute the value of similarity between two infinite-dimensional features, which can be readily used to construct any kernelized classifiers. In addition, there are a lot of unlabeled samples from the target view, which can be utilized to improve the performance of classifiers. Thus, we present a constraint strategy to explore the information contained in the unlabeled samples. The rationality behind the constraint is that any action video belongs to only one class. Our method is verified on the IXMAS dataset, and the experimental results demonstrate that our method achieves better performance than the state-of-the-art methods.</p><p>6 0.61992162 <a title="179-lsi-6" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>7 0.58065385 <a title="179-lsi-7" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>8 0.57093918 <a title="179-lsi-8" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>9 0.5634262 <a title="179-lsi-9" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>10 0.55138391 <a title="179-lsi-10" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>11 0.5406478 <a title="179-lsi-11" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>12 0.52958852 <a title="179-lsi-12" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>13 0.52807826 <a title="179-lsi-13" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>14 0.51708406 <a title="179-lsi-14" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>15 0.49952427 <a title="179-lsi-15" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>16 0.49193954 <a title="179-lsi-16" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>17 0.48931977 <a title="179-lsi-17" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>18 0.48215473 <a title="179-lsi-18" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>19 0.47978938 <a title="179-lsi-19" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>20 0.47198638 <a title="179-lsi-20" href="./cvpr-2013-Learning_Cross-Domain_Information_Transfer_for_Location_Recognition_and_Clustering.html">250 cvpr-2013-Learning Cross-Domain Information Transfer for Location Recognition and Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.127), (16, 0.016), (26, 0.049), (33, 0.264), (36, 0.241), (67, 0.061), (69, 0.049), (76, 0.011), (87, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88659942 <a title="179-lda-1" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>Author: Kaiming He, Fang Wen, Jian Sun</p><p>Abstract: In computer vision there has been increasing interest in learning hashing codes whose Hamming distance approximates the data similarity. The hashing functions play roles in both quantizing the vector space and generating similarity-preserving codes. Most existing hashing methods use hyper-planes (or kernelized hyper-planes) to quantize and encode. In this paper, we present a hashing method adopting the k-means quantization. We propose a novel Affinity-Preserving K-means algorithm which simultaneously performs k-means clustering and learns the binary indices of the quantized cells. The distance between the cells is approximated by the Hamming distance of the cell indices. We further generalize our algorithm to a product space for learning longer codes. Experiments show our method, named as K-means Hashing (KMH), outperforms various state-of-the-art hashing encoding methods.</p><p>2 0.86670399 <a title="179-lda-2" href="./cvpr-2013-Optimized_Product_Quantization_for_Approximate_Nearest_Neighbor_Search.html">319 cvpr-2013-Optimized Product Quantization for Approximate Nearest Neighbor Search</a></p>
<p>Author: Tiezheng Ge, Kaiming He, Qifa Ke, Jian Sun</p><p>Abstract: Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a nonparametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.</p><p>3 0.85080051 <a title="179-lda-3" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>Author: Sid Yingze Bao, Manmohan Chandraker, Yuanqing Lin, Silvio Savarese</p><p>Abstract: We present a dense reconstruction approach that overcomes the drawbacks of traditional multiview stereo by incorporating semantic information in the form of learned category-level shape priors and object detection. Given training data comprised of 3D scans and images of objects from various viewpoints, we learn a prior comprised of a mean shape and a set of weighted anchor points. The former captures the commonality of shapes across the category, while the latter encodes similarities between instances in the form of appearance and spatial consistency. We propose robust algorithms to match anchor points across instances that enable learning a mean shape for the category, even with large shape variations across instances. We model the shape of an object instance as a warped version of the category mean, along with instance-specific details. Given multiple images of an unseen instance, we collate information from 2D object detectors to align the structure from motion point cloud with the mean shape, which is subsequently warped and refined to approach the actual shape. Extensive experiments demonstrate that our model is general enough to learn semantic priors for different object categories, yet powerful enough to reconstruct individual shapes with large variations. Qualitative and quantitative evaluations show that our framework can produce more accurate reconstructions than alternative state-of-the-art multiview stereo systems.</p><p>same-paper 4 0.84201986 <a title="179-lda-4" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>5 0.83743215 <a title="179-lda-5" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>Author: Dong Wang, Huchuan Lu, Ming-Hsuan Yang</p><p>Abstract: In this paper, we propose a generative tracking method based on a novel robust linear regression algorithm. In contrast to existing methods, the proposed Least Soft-thresold Squares (LSS) algorithm models the error term with the Gaussian-Laplacian distribution, which can be solved efficiently. Based on maximum joint likelihood of parameters, we derive a LSS distance to measure the difference between an observation sample and the dictionary. Compared with the distance derived from ordinary least squares methods, the proposed metric is more effective in dealing with outliers. In addition, we present an update scheme to capture the appearance change of the tracked target and ensure that the model is properly updated. Experimental results on several challenging image sequences demonstrate that the proposed tracker achieves more favorable performance than the state-of-the-art methods.</p><p>6 0.82895476 <a title="179-lda-6" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>7 0.81738532 <a title="179-lda-7" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>8 0.80390346 <a title="179-lda-8" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<p>9 0.79430836 <a title="179-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.79325449 <a title="179-lda-10" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>11 0.79142022 <a title="179-lda-11" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>12 0.78833359 <a title="179-lda-12" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>13 0.78774983 <a title="179-lda-13" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>14 0.78722614 <a title="179-lda-14" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>15 0.78713512 <a title="179-lda-15" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>16 0.78706717 <a title="179-lda-16" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>17 0.7870068 <a title="179-lda-17" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>18 0.78652102 <a title="179-lda-18" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>19 0.78642738 <a title="179-lda-19" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>20 0.785891 <a title="179-lda-20" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
