<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-375" href="#">cvpr2013-375</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</h1>
<br/><p>Source: <a title="cvpr-2013-375-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Saliency_Detection_via_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>Reference: <a title="cvpr-2013-375-reference" href="../cvpr2013_reference/cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. [sent-2, score-0.539]
</p><p>2 The saliency of the image elements is defined based on their relevances to the given seeds or queries. [sent-4, score-0.932]
</p><p>3 Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. [sent-7, score-0.558]
</p><p>4 We also create a more difficult bench-  mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field. [sent-9, score-1.462]
</p><p>5 Introduction The task of saliency detection is to identify the most important and informative part of a scene. [sent-11, score-0.77]
</p><p>6 We note that saliency models have been developed for eye fixation prediction [6, 14, 15, 17, 19, 25, 33] and salient object detection [1, 2, 7, 9, 23, 24, 32]. [sent-15, score-1.147]
</p><p>7 Salient object detection algorithms usually generate bounding boxes [7, 10], binary foreground and background segmentation [12, 23, 24, 32], or saliency maps which in-  dicate the saliency likelihood of each pixel. [sent-19, score-1.789]
</p><p>8 [23] propose a binary saliency estimation model by training a conditional random field to combine a set of novel features. [sent-21, score-0.755]
</p><p>9 [32] analyze multiple cues in a unified energy minimization framework and use a graph-based saliency model [14] to detect salient objects. [sent-23, score-1.09]
</p><p>10 develop a hierarchical graph model and utilize concavity context to compute weights between nodes, from which the graph is bi-partitioned for salient object detection. [sent-25, score-0.52]
</p><p>11 [1] compute the saliency likelihood of each pixel based on its color contrast to the entire image. [sent-27, score-0.79]
</p><p>12 [9] consider the global region contrast with respect to the entire image and spatial relationships across the regions to extract saliency map. [sent-29, score-0.818]
</p><p>13 propose a context-aware saliency algorithm to detect the image regions that represent the scene based on four principles of human visual attention. [sent-31, score-0.789]
</p><p>14 [35] propose a novel model for bottom-up saliency within the Bayesian framework by exploiting low and mid level cues. [sent-34, score-0.752]
</p><p>15 [27] show that the complete contrast and saliency estimation can be formulated in a unified way using high-dimensional Gaussian filters. [sent-38, score-0.75]
</p><p>16 In this work, we generate a full-resolution saliency map for each input image. [sent-39, score-0.757]
</p><p>17 Most above-mentioned methods measure saliency by measuring local center-surround contrast and rarity of features over the entire image. [sent-40, score-0.766]
</p><p>18 Recently,  a method that exploits background priors is proposed for saliency detection [34]. [sent-47, score-0.87]
</p><p>19 The main observation is that the distance between a pair of background regions is shorter than that of a region from the salient object and a region from the background. [sent-48, score-0.482]
</p><p>20 The node labelling task (either salient object or background) is formulated as an energy minimization problem based on this criteria. [sent-49, score-0.482]
</p><p>21 In this work, we exploit these cues to compute pixel saliency based on the ranking of superpixels. [sent-51, score-1.068]
</p><p>22 We model saliency detection as a manifold ranking problem and propose a two-stage scheme for graph labelling. [sent-53, score-1.183]
</p><p>23 From each labelled result, we compute the saliency ofnodes based on their relevances (i. [sent-56, score-0.956]
</p><p>24 The four labelled maps are then integrated to generate a saliency map. [sent-58, score-0.901]
</p><p>25 In the second stage, we apply binary segmentation on the resulted saliency map from the first stage, and take the labelled foreground nodes as salient queries. [sent-59, score-1.454]
</p><p>26 The saliency of each node is computed based on its relevance to foreground queries for the final map. [sent-60, score-1.202]
</p><p>27 To fully capture intrinsic graph structure information and  incorporate local grouping cues in graph labelling, we use manifold ranking techniques to learn a ranking function, which is essential to learn an optimal affinity matrix [20]. [sent-61, score-0.89]
</p><p>28 Different from [12], the proposed saliency detection algorithm with manifold ranking requires only seeds from one class, which are initialized with either the boundary priors or foreground cues. [sent-62, score-1.382]
</p><p>29 Furthermore, it is difficult to determine the number and locations of salient seeds as they are generated by random walks, especially for the scenes with different salient objects. [sent-66, score-0.725]
</p><p>30 In this work, all the background and foreground seeds can be easily generated via background priors and ranking background queries (or seeds). [sent-68, score-0.916]
</p><p>31 As our model incorporates local grouping cues extracted from the entire image, the proposed algorithm generates well-defined boundaries of salient objects and uniformly highlights the whole salient regions. [sent-69, score-0.776]
</p><p>32 Experimental results using large benchmark data sets show that the proposed algorithm performs efficiently and favorably against the state-of-the-art saliency detection methods. [sent-70, score-0.787]
</p><p>33 Graph-Based Manifold Ranking The graph-based ranking problem is described as follows: given a node as a query, the remaining nodes are ranked based on their relevances to the given query. [sent-72, score-0.66]
</p><p>34 The goal is to learn a ranking function, which defines the relevance between unlabelled nodes and queries. [sent-73, score-0.576]
</p><p>35 Manifold Ranking In [39], a ranking method that exploits the intrinsic manifold structure of data (such as image) for graph labelling is proposed. [sent-76, score-0.485]
</p><p>36 Let f: X → Rn denote a ranking functtioon th we qhuicehr assigns a ranking vRalue fi to each point xi, and f can be viewed as a vector f = [f1, . [sent-86, score-0.558]
</p><p>37 Similar itos Dthe = PageRank and spectral clustering algorithms [5, 26], the optimal ranking of queries are computed by solving the following optimization problem:  333 111666557  Figure2. [sent-100, score-0.53]
</p><p>38 Saliency Measure Given an input image represented as a graph and some salient query nodes, the saliency of each node is defined as its ranking score computed by Eq. [sent-125, score-1.518]
</p><p>39 In the conventional ranking problems, the queries are manually labelled with the ground-truth. [sent-130, score-0.59]
</p><p>40 queries for saliency detection are selected by the proposed algorithm, some of them may be incorrect. [sent-136, score-1.001]
</p><p>41 , the saliency value) for each query, which is defined as its ranking score ranked by the other queries (except itself). [sent-139, score-1.271]
</p><p>42 Lastly, we  measure the saliency of nodes using the normalized ranking score when salient queries are given, and using 1− f∗ when background queries are given. [sent-144, score-2.044]
</p><p>43 As neighboring nodes are likely to share similar appearance and saliency values, we use a k-regular graph to exploit the spatial relationship. [sent-148, score-0.974]
</p><p>44 First, each node is not only connected to those nodes neighboring it, but also connected to the nodes sharing common boundaries with its neighboring node (See Figure 2). [sent-149, score-0.517]
</p><p>45 From left to right: input image, result of using all the boundary nodes together as queries, result of integrating four maps from each side, result of ranking with foreground queries. [sent-159, score-0.706]
</p><p>46 The weights are computed based on the distance in the color space as it has been shown to be effective in saliency detection [2, 4]. [sent-166, score-0.77]
</p><p>47 By ranking the nodes on the constructed graph, the inverse matrix (D − αW)−1 in Eq. [sent-167, score-0.485]
</p><p>48 That is, the relevance between nodes is increased when their spatial distance is decreased, which is an important cue for saliency detection [9]. [sent-172, score-1.027]
</p><p>49 Two-Stage Saliency Detection In this section, we detail the proposed two-stage scheme for bottom-up saliency detection using ranking with background and foreground queries. [sent-174, score-1.21]
</p><p>50 Ranking with Background Queries Based on the attention theories of early works for visual saliency [17], we use the nodes on the image boundary as background seeds, i. [sent-177, score-1.074]
</p><p>51 Specifically, we construct four saliency maps using boundary priors and then integrate them for the final map, which is referred as the separation/combination (SC) approach. [sent-180, score-0.92]
</p><p>52 Taking top image boundary as an example, we use the nodes on this side as the queries and other nodes as the unlabelled data. [sent-181, score-0.694]
</p><p>53 From left to right: input images, saliency maps using all the boundary nodes together as queries, four side-specific maps, integration of four saliency maps, the final saliency map after the second stage. [sent-190, score-2.61]
</p><p>54 We normalize this vector to the range between 0 and 1, and the saliency map using the top boundary prior, St can be written as: St(i) = 1  − f∗(i)  i= 1, 2, . [sent-192, score-0.826]
</p><p>55 We note that the saliency maps are computed with different indicator vector y while the weight matrix W and the degree matrix D are fixed. [sent-197, score-0.908]
</p><p>56 The four saliency maps are integrated by the following process: Sbq (i) = St (i)  Sb(i)  Sl (i)  Sr (i) . [sent-202, score-0.821]
</p><p>57 (6)  There are two reasons for using the SC approach to generate saliency maps. [sent-203, score-0.731]
</p><p>58 , the ground-truth salient nodes are inadvertently selected as background queries. [sent-215, score-0.572]
</p><p>59 As shown in the second column of Figure 5, the saliency maps generated using all the boundary nodes are poor. [sent-216, score-1.042]
</p><p>60 Due to the imprecise labelling results, the pixels with the salient objects have low saliency values. [sent-217, score-1.169]
</p><p>61 The example in which imprecise salient queries are selected in the second stage. [sent-219, score-0.597]
</p><p>62 From left to right: input image, saliency map of the first stage, binary segmentation, the final saliency map. [sent-220, score-1.512]
</p><p>63 “stuff” (such as grass or sky) and therefore they rarely occupy three or all sides of image, the proposed SC approach ensures at least two saliency maps are effective (third column of Figure 5). [sent-221, score-0.832]
</p><p>64 By integration of four saliency maps, some salient parts of object can be identified (although the whole object is not uniformly highlighted), which provides sufficient cues for the second stage detection process. [sent-222, score-1.265]
</p><p>65 While most regions of the salient objects are highlighted in the first stage, some background nodes may not be adequately suppressed (See Figure 4 and Figure 5). [sent-223, score-0.642]
</p><p>66 To alleviate this problem and improve the results especially when objects appear near the image boundaries, the saliency maps are further improved via ranking with foreground queries. [sent-224, score-1.166]
</p><p>67 Ranking with Foreground Queries The saliency map of the first stage is binary segmented (i. [sent-227, score-0.83]
</p><p>68 , salient foreground and background) using an adaptive threshold, which facilitates selecting the nodes of the foreground salient objects as queries. [sent-229, score-1.03]
</p><p>69 We expect that the selected queries cover the salient object regions as much as  possible (i. [sent-230, score-0.605]
</p><p>70 Thus, the threshold is set as the mean saliency over the entire saliency map. [sent-233, score-1.478]
</p><p>71 Once the salient queries are given, an indicator vector y is formed to compute the ranking vector f∗ using Eq. [sent-234, score-0.893]
</p><p>72 As is carried out in the first stage, the ranking vector f∗ is normalized between the range of 0 and 1to form the final saliency map by Sfq(i) =  f∗(i)  i= 1, 2, . [sent-236, score-1.036]
</p><p>73 We note that there are cases where nodes may be incorrectly selected as foreground queries in this stage. [sent-240, score-0.499]
</p><p>74 The salient object regions are usually relatively compact (in terms of spatial distribution) and homogeneous in appearance (in terms of feature distribution), while background regions are the opposite. [sent-243, score-0.477]
</p><p>75 , two nodes of the salient objects) is statistically much larger than that of object-background and intra-background relevance, which can be inferred from the affinity matrix A. [sent-246, score-0.582]
</p><p>76 Therefore, the sum of the relevance values of object nodes to the ground-truth salient queries is considerably larger than that of background nodes to all the queries. [sent-250, score-1.076]
</p><p>77 That is, background saliency can be suppressed effectively (fourth column of Figure 6). [sent-251, score-0.822]
</p><p>78 Similarly, in spite of the saliency maps after the first stage of Figure 5 are not precise, salient object can be well detected by the saliency maps after the foreground queries in the second stage. [sent-252, score-2.304]
</p><p>79 4: Bi-segment Sbq to form salient foreground queries and an indicator vector y. [sent-262, score-0.681]
</p><p>80 Output: a saliency map Sfq representing the saliency value of each superpixel. [sent-266, score-1.488]
</p><p>81 We compare our method with fourteen state-of-the-art saliency detection algorithms: the IT [17], GB [14], MZ [25], SR [15], AC [1], Gof [11], FT [2], LC [37], RC [9], SVO [7], SF [27], CB [18], GS SP [34] and XIE [35] methods. [sent-302, score-0.819]
</p><p>82 The parameter σ controls the strength of weight between a pair of nodes and the parameter α balances the smooth and fitting constraints in the regularization function of manifold ranking algorithm. [sent-307, score-0.542]
</p><p>83 The precision value corresponds to the ratio of salient pixels correctly assigned to all the pixels of extracted regions, while the recall value is defined as  the percentage of detected salient pixels in relation to the ground-truth number. [sent-312, score-0.72]
</p><p>84 Similar as prior works, the precisionrecall curves are obtained by binarizing the saliency map using thresholds in the range of 0 and 255. [sent-313, score-0.781]
</p><p>85 Figure 8 (c) shows that our approach  using the integration of saliency maps generated from different boundary priors performs better in the first stage. [sent-327, score-0.942]
</p><p>86 Figure 8 (d) demonstrates that the second stage using the foreground queries further improve the performance of the first stage with background queries. [sent-329, score-0.49]
</p><p>87 We evaluate the performance of the proposed method against fourteen state-of-the-art bottom-up saliency detection methods. [sent-330, score-0.819]
</p><p>88 We note that the proposed methods outperforms the SVO [7], Gof [11], CB [18], and RC [9] which are top-performance methods for saliency detection in a recent benchmark study [4]. [sent-332, score-0.77]
</p><p>89 We also compute the precision, recall and F-measure with an adaptive threshold proposed in [2], defined as twice the mean saliency of the image. [sent-334, score-0.812]
</p><p>90 Figure 10 shows a few saliency maps of the evaluated methods. [sent-337, score-0.796]
</p><p>91 To compute precision and recall values, we first fit a rectangle to the binary saliency map and then use the output bounding box for 333 111676991  All results  are computed on  the MSRA-1000  dataset. [sent-342, score-0.897]
</p><p>92 The proposed algorithm consistently generates saliency maps close to the ground  truth. [sent-378, score-0.813]
</p><p>93 Similar to the experiments on the MSRA1000 database, we also binarize saliency maps using the threshold of twice the mean saliency to compute precision,  recall and F-measure bars. [sent-386, score-1.587]
</p><p>94 Similar to the experiments on the MSRA database, we also compute a rectangle of the binary saliency map and then evaluate our model by the fixed thresholding and the adaptive thresholding ways. [sent-392, score-0.826]
</p><p>95 Our run time is much faster than that of the other saliency models. [sent-398, score-0.731]
</p><p>96 165 s (about 64%), and the actual saliency computation spends 0. [sent-400, score-0.761]
</p><p>97 Conclusion We propose a bottom-up method to detect salient regions in images through manifold ranking on a graph, which incorporates local grouping cues and boundary priors. [sent-411, score-0.826]
</p><p>98 ground queries for ranking to generate the saliency maps. [sent-436, score-1.241]
</p><p>99 Fusing generic objectness and visual saliency for salient object detection. [sent-497, score-1.072]
</p><p>100 Top-down visual saliency via joint crf and dictionary learning. [sent-702, score-0.731]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('saliency', 0.731), ('salient', 0.325), ('ranking', 0.279), ('queries', 0.231), ('nodes', 0.177), ('relevances', 0.105), ('foreground', 0.091), ('msra', 0.081), ('labelled', 0.08), ('relevance', 0.08), ('superpixels', 0.078), ('seeds', 0.075), ('labelling', 0.072), ('background', 0.07), ('node', 0.069), ('boundary', 0.069), ('manifold', 0.068), ('graph', 0.066), ('maps', 0.065), ('sbq', 0.055), ('sfq', 0.055), ('affinity', 0.051), ('xie', 0.05), ('stage', 0.049), ('fourteen', 0.049), ('query', 0.048), ('imprecise', 0.041), ('unlabelled', 0.04), ('achanta', 0.039), ('detection', 0.039), ('unnormalized', 0.039), ('gopalakrishnan', 0.037), ('iindexes', 0.037), ('fixation', 0.036), ('geodesic', 0.036), ('sides', 0.036), ('recall', 0.036), ('gof', 0.035), ('lu', 0.035), ('cues', 0.034), ('indicator', 0.034), ('precision', 0.034), ('superpixel', 0.033), ('regions', 0.033), ('laplacian', 0.033), ('slic', 0.031), ('sc', 0.031), ('walks', 0.031), ('priors', 0.03), ('koch', 0.03), ('integration', 0.03), ('spends', 0.03), ('ranked', 0.03), ('matrix', 0.029), ('svo', 0.028), ('attention', 0.027), ('perazzi', 0.026), ('constraint', 0.026), ('map', 0.026), ('boundaries', 0.025), ('bousquet', 0.025), ('wij', 0.025), ('four', 0.025), ('compute', 0.024), ('binary', 0.024), ('goferman', 0.024), ('cheng', 0.024), ('curves', 0.024), ('estrada', 0.023), ('concavity', 0.023), ('tip', 0.023), ('cb', 0.022), ('zheng', 0.022), ('bounding', 0.022), ('mid', 0.021), ('elements', 0.021), ('suppressed', 0.021), ('adaptive', 0.021), ('sr', 0.02), ('options', 0.02), ('weston', 0.02), ('degree', 0.02), ('spectral', 0.02), ('region', 0.019), ('gs', 0.019), ('contrast', 0.019), ('xl', 0.018), ('controls', 0.018), ('grouping', 0.018), ('generates', 0.017), ('performs', 0.017), ('yuan', 0.017), ('object', 0.016), ('entire', 0.016), ('highlighted', 0.016), ('highlights', 0.016), ('ofnodes', 0.016), ('tiso', 0.016), ('brin', 0.016), ('wils', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="375-tfidf-1" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>2 0.66030276 <a title="375-tfidf-2" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>3 0.65686935 <a title="375-tfidf-3" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>4 0.62859434 <a title="375-tfidf-4" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>5 0.59421742 <a title="375-tfidf-5" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>Author: Parthipan Siva, Chris Russell, Tao Xiang, Lourdes Agapito</p><p>Abstract: We propose a principled probabilistic formulation of object saliency as a sampling problem. This novel formulation allows us to learn, from a large corpus of unlabelled images, which patches of an image are of the greatest interest and most likely to correspond to an object. We then sample the object saliency map to propose object locations. We show that using only a single object location proposal per image, we are able to correctly select an object in over 42% of the images in the PASCAL VOC 2007 dataset, substantially outperforming existing approaches. Furthermore, we show that our object proposal can be used as a simple unsupervised approach to the weakly supervised annotation problem. Our simple unsupervised approach to annotating objects of interest in images achieves a higher annotation accuracy than most weakly supervised approaches.</p><p>6 0.59191203 <a title="375-tfidf-6" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>7 0.45525673 <a title="375-tfidf-7" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>8 0.36559552 <a title="375-tfidf-8" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>9 0.34025839 <a title="375-tfidf-9" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>10 0.3167831 <a title="375-tfidf-10" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>11 0.23912385 <a title="375-tfidf-11" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>12 0.23672937 <a title="375-tfidf-12" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>13 0.1787582 <a title="375-tfidf-13" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>14 0.16436969 <a title="375-tfidf-14" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>15 0.14179152 <a title="375-tfidf-15" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>16 0.10672303 <a title="375-tfidf-16" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>17 0.10667264 <a title="375-tfidf-17" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>18 0.090131 <a title="375-tfidf-18" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>19 0.086656749 <a title="375-tfidf-19" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>20 0.084249072 <a title="375-tfidf-20" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.218), (1, -0.254), (2, 0.659), (3, 0.347), (4, -0.1), (5, -0.034), (6, -0.009), (7, -0.089), (8, 0.032), (9, 0.034), (10, 0.013), (11, 0.049), (12, -0.065), (13, 0.029), (14, -0.036), (15, 0.01), (16, 0.003), (17, -0.0), (18, -0.051), (19, -0.016), (20, 0.003), (21, 0.01), (22, -0.05), (23, 0.057), (24, -0.032), (25, -0.012), (26, 0.026), (27, -0.003), (28, 0.008), (29, 0.076), (30, 0.016), (31, -0.027), (32, 0.026), (33, -0.03), (34, -0.002), (35, -0.003), (36, 0.016), (37, -0.032), (38, 0.023), (39, -0.023), (40, -0.002), (41, -0.015), (42, 0.009), (43, -0.001), (44, -0.002), (45, -0.029), (46, -0.016), (47, 0.019), (48, 0.015), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96869892 <a title="375-lsi-1" href="./cvpr-2013-Saliency_Aggregation%3A_A_Data-Driven_Approach.html">374 cvpr-2013-Saliency Aggregation: A Data-Driven Approach</a></p>
<p>Author: Long Mai, Yuzhen Niu, Feng Liu</p><p>Abstract: A variety of methods have been developed for visual saliency analysis. These methods often complement each other. This paper addresses the problem of aggregating various saliency analysis methods such that the aggregation result outperforms each individual one. We have two major observations. First, different methods perform differently in saliency analysis. Second, the performance of a saliency analysis method varies with individual images. Our idea is to use data-driven approaches to saliency aggregation that appropriately consider the performance gaps among individual methods and the performance dependence of each method on individual images. This paper discusses various data-driven approaches and finds that the image-dependent aggregation method works best. Specifically, our method uses a Conditional Random Field (CRF) framework for saliency aggregation that not only models the contribution from individual saliency map but also the interaction between neighboringpixels. To account for the dependence of aggregation on an individual image, our approach selects a subset of images similar to the input image from a training data set and trains the CRF aggregation model only using this subset instead of the whole training set. Our experiments on public saliency benchmarks show that our aggregation method outperforms each individual saliency method and is robust with the selection of aggregated methods.</p><p>same-paper 2 0.96273851 <a title="375-lsi-2" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>Author: Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, Ming-Hsuan Yang</p><p>Abstract: Most existing bottom-up methods measure the foreground saliency of a pixel or region based on its contrast within a local context or the entire image, whereas a few methods focus on segmenting out background regions and thereby salient objects. Instead of considering the contrast between the salient objects and their surrounding regions, we consider both foreground and background cues in a different way. We rank the similarity of the image elements (pixels or regions) with foreground cues or background cues via graph-based manifold ranking. The saliency of the image elements is defined based on their relevances to the given seeds or queries. We represent the image as a close-loop graph with superpixels as nodes. These nodes are ranked based on the similarity to background and foreground queries, based on affinity matrices. Saliency detection is carried out in a two-stage scheme to extract background regions and foreground salient objects efficiently. Experimental results on two large benchmark databases demonstrate the proposed method performs well when against the state-of-the-art methods in terms of accuracy and speed. We also create a more difficult bench- mark database containing 5,172 images to test the proposed saliency model and make this database publicly available with this paper for further studies in the saliency field.</p><p>3 0.94771338 <a title="375-lsi-3" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>Author: Keyang Shi, Keze Wang, Jiangbo Lu, Liang Lin</p><p>Abstract: Driven by recent vision and graphics applications such as image segmentation and object recognition, assigning pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly critical. More often, such fine-grained saliency detection is also desired to have a fast runtime. Motivated by these, we propose a generic and fast computational framework called PISA Pixelwise Image Saliency Aggregating complementary saliency cues based on color and structure contrasts with spatial priors holistically. Overcoming the limitations of previous methods often using homogeneous superpixel-based and color contrast-only treatment, our PISA approach directly performs saliency modeling for each individual pixel and makes use of densely overlapping, feature-adaptive observations for saliency measure computation. We further impose a spatial prior term on each of the two contrast measures, which constrains pixels rendered salient to be compact and also centered in image domain. By fusing complementary contrast measures in such a pixelwise adaptive manner, the detection effectiveness is significantly boosted. Without requiring reliable region segmentation or post– relaxation, PISA exploits an efficient edge-aware image representation and filtering technique and produces spatially coherent yet detail-preserving saliency maps. Extensive experiments on three public datasets demonstrate PISA’s superior detection accuracy and competitive runtime speed over the state-of-the-arts approaches.</p><p>4 0.9403879 <a title="375-lsi-4" href="./cvpr-2013-Salient_Object_Detection%3A_A_Discriminative_Regional_Feature_Integration_Approach.html">376 cvpr-2013-Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></p>
<p>Author: Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, Shipeng Li</p><p>Abstract: Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional backgroundness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, backgroundness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.</p><p>5 0.90581197 <a title="375-lsi-5" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>Author: Qiong Yan, Li Xu, Jianping Shi, Jiaya Jia</p><p>Abstract: When dealing with objects with complex structures, saliency detection confronts a critical problem namely that detection accuracy could be adversely affected if salient foreground or background in an image contains small-scale high-contrast patterns. This issue is common in natural images and forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. The final saliency map is produced in a hierarchical model. Different from varying patch sizes or downsizing images, our scale-based region handling is by finding saliency values optimally in a tree model. Our approach improves saliency detection on many images that cannot be handled well traditionally. A new dataset is also constructed. –</p><p>6 0.88547254 <a title="375-lsi-6" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>7 0.84466547 <a title="375-lsi-7" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>8 0.78594851 <a title="375-lsi-8" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>9 0.76849061 <a title="375-lsi-9" href="./cvpr-2013-Learning_Video_Saliency_from_Human_Gaze_Using_Candidate_Selection.html">258 cvpr-2013-Learning Video Saliency from Human Gaze Using Candidate Selection</a></p>
<p>10 0.59396875 <a title="375-lsi-10" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>11 0.53693759 <a title="375-lsi-11" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>12 0.51172006 <a title="375-lsi-12" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>13 0.37191477 <a title="375-lsi-13" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>14 0.35248664 <a title="375-lsi-14" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>15 0.33500952 <a title="375-lsi-15" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>16 0.33099464 <a title="375-lsi-16" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>17 0.27728811 <a title="375-lsi-17" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>18 0.25680551 <a title="375-lsi-18" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>19 0.24845375 <a title="375-lsi-19" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>20 0.24639642 <a title="375-lsi-20" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.101), (16, 0.012), (26, 0.033), (33, 0.247), (67, 0.396), (69, 0.033), (87, 0.055), (89, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96158046 <a title="375-lda-1" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>Author: Pramod Sharma, Ram Nevatia</p><p>Abstract: In this work, we present a novel and efficient detector adaptation method which improves the performance of an offline trained classifier (baseline classifier) by adapting it to new test datasets. We address two critical aspects of adaptation methods: generalizability and computational efficiency. We propose an adaptation method, which can be applied to various baseline classifiers and is computationally efficient also. For a given test video, we collect online samples in an unsupervised manner and train a randomfern adaptive classifier . The adaptive classifier improves precision of the baseline classifier by validating the obtained detection responses from baseline classifier as correct detections or false alarms. Experiments demonstrate generalizability, computational efficiency and effectiveness of our method, as we compare our method with state of the art approaches for the problem of human detection and show good performance with high computational efficiency on two different baseline classifiers.</p><p>2 0.95776659 <a title="375-lda-2" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>Author: James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</p><p>Abstract: We introduce a new problem domain for activity recognition: the analysis of children ’s social and communicative behaviors based on video and audio data. We specifically target interactions between children aged 1–2 years and an adult. Such interactions arise naturally in the diagnosis and treatment of developmental disorders such as autism. We introduce a new publicly-available dataset containing over 160 sessions of a 3–5 minute child-adult interaction. In each session, the adult examiner followed a semistructured play interaction protocol which was designed to elicit a broad range of social behaviors. We identify the key technical challenges in analyzing these behaviors, and describe methods for decoding the interactions. We present experimental results that demonstrate the potential of the dataset to drive interesting research questions, and show preliminary results for multi-modal activity recognition.</p><p>3 0.93574375 <a title="375-lda-3" href="./cvpr-2013-Single-Pedestrian_Detection_Aided_by_Multi-pedestrian_Detection.html">398 cvpr-2013-Single-Pedestrian Detection Aided by Multi-pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xiaogang Wang</p><p>Abstract: In this paper, we address the challenging problem of detecting pedestrians who appear in groups and have interaction. A new approach is proposed for single-pedestrian detection aided by multi-pedestrian detection. A mixture model of multi-pedestrian detectors is designed to capture the unique visual cues which are formed by nearby multiple pedestrians but cannot be captured by single-pedestrian detectors. A probabilistic framework is proposed to model the relationship between the configurations estimated by single- and multi-pedestrian detectors, and to refine the single-pedestrian detection result with multi-pedestrian detection. It can integrate with any single-pedestrian detector without significantly increasing the computation load. 15 state-of-the-art single-pedestrian detection approaches are investigated on three widely used public datasets: Caltech, TUD-Brussels andETH. Experimental results show that our framework significantly improves all these approaches. The average improvement is 9% on the Caltech-Test dataset, 11% on the TUD-Brussels dataset and 17% on the ETH dataset in terms of average miss rate. The lowest average miss rate is reduced from 48% to 43% on the Caltech-Test dataset, from 55% to 50% on the TUD-Brussels dataset and from 51% to 41% on the ETH dataset.</p><p>4 0.8938992 <a title="375-lda-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.88899279 <a title="375-lda-5" href="./cvpr-2013-Articulated_Pose_Estimation_Using_Discriminative_Armlet_Classifiers.html">45 cvpr-2013-Articulated Pose Estimation Using Discriminative Armlet Classifiers</a></p>
<p>Author: Georgia Gkioxari, Pablo Arbeláez, Lubomir Bourdev, Jitendra Malik</p><p>Abstract: We propose a novel approach for human pose estimation in real-world cluttered scenes, and focus on the challenging problem of predicting the pose of both arms for each person in the image. For this purpose, we build on the notion of poselets [4] and train highly discriminative classifiers to differentiate among arm configurations, which we call armlets. We propose a rich representation which, in addition to standardHOGfeatures, integrates the information of strong contours, skin color and contextual cues in a principled manner. Unlike existing methods, we evaluate our approach on a large subset of images from the PASCAL VOC detection dataset, where critical visual phenomena, such as occlusion, truncation, multiple instances and clutter are the norm. Our approach outperforms Yang and Ramanan [26], the state-of-the-art technique, with an improvement from 29.0% to 37.5% PCP accuracy on the arm keypoint prediction task, on this new pose estimation dataset.</p><p>6 0.88140613 <a title="375-lda-6" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>7 0.87687612 <a title="375-lda-7" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>8 0.86898893 <a title="375-lda-8" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>same-paper 9 0.85896605 <a title="375-lda-9" href="./cvpr-2013-Saliency_Detection_via_Graph-Based_Manifold_Ranking.html">375 cvpr-2013-Saliency Detection via Graph-Based Manifold Ranking</a></p>
<p>10 0.84495765 <a title="375-lda-10" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>11 0.8212142 <a title="375-lda-11" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>12 0.81285149 <a title="375-lda-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.80875444 <a title="375-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.79316121 <a title="375-lda-14" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>15 0.78657132 <a title="375-lda-15" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>16 0.78594989 <a title="375-lda-16" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>17 0.76225626 <a title="375-lda-17" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>18 0.761531 <a title="375-lda-18" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>19 0.75654411 <a title="375-lda-19" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>20 0.75503546 <a title="375-lda-20" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
