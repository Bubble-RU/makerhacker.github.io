<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-249" href="#">cvpr2013-249</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</h1>
<br/><p>Source: <a title="cvpr-2013-249-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Learning_Compact_Binary_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Xi Li, Chunhua Shen, Anthony Dick, Anton van_den_Hengel</p><p>Abstract: A key problem in visual tracking is to represent the appearance of an object in a way that is robust to visual changes. To attain this robustness, increasingly complex models are used to capture appearance variations. However, such models can be difficult to maintain accurately and efficiently. In this paper, we propose a visual tracker in which objects are represented by compact and discriminative binary codes. This representation can be processed very efficiently, and is capable of effectively fusing information from multiple cues. An incremental discriminative learner is then used to construct an appearance model that optimally separates the object from its surrounds. Furthermore, we design a hypergraph propagation method to capture the contextual information on samples, which further improves the tracking accuracy. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>Reference: <a title="cvpr-2013-249-reference" href="../cvpr2013_reference/cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose a visual tracker in which objects are represented by compact and discriminative binary codes. [sent-4, score-0.607]
</p><p>2 Furthermore, we design a hypergraph propagation method to capture the contextual information on samples, which further improves the tracking accuracy. [sent-7, score-0.693]
</p><p>3 Most state of the art trackers use a sampling approach, in which the object location is selected from a pool of candidate samples at each frame. [sent-11, score-0.229]
</p><p>4 Ideally, the highest scoring sample should be the one which best aligns with the object, and sample scores should decrease with the amount of object overlap, while all background samples should be scored lower than any sample containing at least part of the object. [sent-13, score-0.358]
</p><p>5 , linear regression [16, 19, 21, 34], principal component analysis [15, 24], discrete cosine transform [17], random forest [25], support vector machine [3, 11], and boosting [2, 9]), which is in turn based on a robust image feature (e. [sent-17, score-0.288]
</p><p>6 Many state of the art trackers construct appearance models from a collection of different feature types to cope with object appearance variations. [sent-20, score-0.283]
</p><p>7 We propose a hashing method to perform feature fusion by reducing multiple feature descriptors to a single binary code vector. [sent-24, score-0.674]
</p><p>8 The proposed hash function is based on randomized decision trees, each of which is efficiently built by a sequence of simple operations on samples and their associated features. [sent-25, score-0.456]
</p><p>9 As a result, the problem of feature fusion is converted to that of randomized decision tree growing. [sent-26, score-0.343]
</p><p>10 Using the learned hash function, we can explicitly formulate the binary code corresponding to a sample by aggregating the posterior distributions of the leaf nodes reached in all randomized decision trees. [sent-27, score-0.645]
</p><p>11 These binary codes capture hierarchical discriminative information from different feature modalities in a decision-tree-growing manner, leading to a discriminative image representation. [sent-28, score-0.366]
</p><p>12 Since they are individually learned over randomly sampled subsets, the learned hash functions are almost uncorrelated with each other. [sent-29, score-0.215]
</p><p>13 Given the compact and discriminative binary codes representing samples, an object appearance model is typically required to maximize the separation of foreground and background samples. [sent-32, score-0.474]
</p><p>14 Learning the LS-SVM on the binary codes ensures a  max-margin hyperplane separating the foreground from the background. [sent-40, score-0.268]
</p><p>15 To further refine the scoring function, we note that sample confidence scores are not only determined by their own appearance features but also constrained by their contextual dependencies. [sent-44, score-0.321]
</p><p>16 In other words, if two test image regions have a similar binary code, their confidence scores ought to be close; otherwise, their confidence scores may greatly differ from each other. [sent-45, score-0.289]
</p><p>17 In order to model such a dependency, we use hypergraph analysis, which is a useful tool for capturing the contextual interaction between graph vertices [13,35]. [sent-46, score-0.44]
</p><p>18 In hypergraph analysis, the problem of dependency modeling is converted to that of building a set of hyperedges, which correspond to vertex communities. [sent-47, score-0.467]
</p><p>19 , the same weak labels obtained by compact binary code learning in our case), and pass support messages to each other. [sent-50, score-0.287]
</p><p>20 Therefore, the hypergraph propagation method can refine the sample scores to be consistent with their binary codes, leading to more accurate object localization. [sent-51, score-0.755]
</p><p>21 In summary, we propose a robust visual tracker that incorporates three measures to improve the accuracy and robustness of the sample scoring function while maintaining its required computational efficiency. [sent-52, score-0.506]
</p><p>22 We propose a novel compact binary code learning method based on random forest hashing, which learns to produce compact and discriminative binary codes representing samples. [sent-55, score-0.871]
</p><p>23 To our knowledge, it is the first time that the compact binary code learning method is proposed to build a robust image representation for visual tracking. [sent-56, score-0.35]
</p><p>24 We build an appearance model based on these binary codes using an incremental closed-form LS-SVM, which can online learn a hyperplane that separates the foreground samples from the background samples. [sent-58, score-0.538]
</p><p>25 We present a hypergraph propagation method that further refines the appearance model by capturing contextual similarity information from samples. [sent-60, score-0.601]
</p><p>26 The proposed visual tracker The workflow of our tracking system is summarized in Algorithm 1. [sent-63, score-0.521]
</p><p>27 Like most sampling based trackers, at each frame the method generates a sample set, scores each sample, and updates its estimated target location based on the highest scoring sample. [sent-64, score-0.213]
</p><p>28 This is based on three techniques: i) compact binary code learning; ii) incremental LS-SVM learning; and iii) hypergraph propagation. [sent-66, score-0.781]
</p><p>29 For i), we focus on learning a set of random forest hash functions for feature fusion, as described in Sec. [sent-67, score-0.475]
</p><p>30 For iii), a weakly supervised hypergraph is created by exploring a set of hashing-bit-specific communities, as shown in Sec. [sent-75, score-0.447]
</p><p>31 (14), hypergraph propagation is performed to diffuse the LS-SVM classification scores on the weakly supervised hypergraph, resulting in more accurate object localization. [sent-79, score-0.602]
</p><p>32 These training samples are used at regular intervals to update the random forest and and LS-SVM classifier, as explained in Secs. [sent-81, score-0.31]
</p><p>33 Compact binary code learning Given multiple types of visual features, we design a hashing method to form a compact and discriminative fused feature. [sent-87, score-0.683]
</p><p>34 In principle, the hashing method needs to satisfy the following two conditions: i) each individual hash function is balanced such that:  ? [sent-88, score-0.524]
</p><p>35 uTon catciohnie;v eii )th tehese htwasoh goals, 222444112088  Algorithm 1 Compact binary code learning based tracker Input: New video frame. [sent-91, score-0.588]
</p><p>36 Compute the corresponding binary codes {xi}iK=1 by compact binary cCoodme plueatren thineg c o(Srrece. [sent-94, score-0.425]
</p><p>37 Perform LS-SVM classification on the binary codes to produce the initial confidence score vector s0 (Sec. [sent-97, score-0.311]
</p><p>38 Update the tracker location to {uk |k = arg maxi si }. [sent-103, score-0.317]
</p><p>39 Update the random forest hash functions (Algorithm 2) and the LSSVM classifier (Sec. [sent-110, score-0.435]
</p><p>40 the training samples for each hash function are randomly selected from the entire training dataset, and equally distributed between positive and negative samples. [sent-113, score-0.277]
</p><p>41 To capture the discriminative information from inter-class samples, the hash function is typically formulated as a binary classifier:  ? [sent-114, score-0.38]
</p><p>42 The process of growing each randomized tree enables our hashing method to effectively combine the discriminative information from different feature modalities in a top-down manner, as shown in Fig. [sent-121, score-0.555]
</p><p>43 Each internal node of these randomized trees contains a binary test that best splits the space of a randomly sampled subset of training data along a randomly chosen feature dimension:  sp(ui) =? [sent-123, score-0.294]
</p><p>44 Such random sample selection and random internal node split ensure the high efficiency of our hashing method. [sent-125, score-0.463]
</p><p>45 Using the aforementioned tree growing scheme (3), we construct a random forest T comprising a set of randomized trees. [sent-126, score-0.411]
</p><p>46 Mathematically, Prt,l (c) is calculated as  |Q|Qt , l,lc| ,  the ratio where Qt,l is the set of training samples reaching th|eQ le|af node lin the randomized tree t, and Qt,l,c is the set of class-c training samples in Qt,l. [sent-130, score-0.278]
</p><p>47 Algorithm 2 Learning random forest hash functions  Input: Training sample set {ui,yi}iN=1with yi∈ {1,−1}, binary code length n ? [sent-131, score-0.688]
</p><p>48 u1catlo repagrnedsoermnatiez aodntrafeinomt gpsboaymirtpavlne-  end • Obtain random forest Tj = {t1, . [sent-138, score-0.22]
</p><p>49 , tM} and output the hash func•tio Onb hTj ( r·)a by Eq. [sent-141, score-0.215]
</p><p>50 end Based on the random forest T, we define the sim? [sent-143, score-0.22]
</p><p>51 the random forest T is obtained by the following ,th−r1ee} steps. [sent-153, score-0.22]
</p><p>52 First, pass the test sample down each randomized tree until reaching a leaf node; second, aggregate all the corresponding posterior probabilities of the reached leaf nodes from T; finally, output the binary code such that hT(u) =  sgn(? [sent-154, score-0.471]
</p><p>53 t∈T  Algorithm 2 shows the detailed workflow of learning the random forest hash functions. [sent-159, score-0.474]
</p><p>54 Incremental LS-SVM To classify binary features as object/non-object, we build an online discriminative appearance model based on incremental LS-SVM learning. [sent-173, score-0.347]
</p><p>55 , xN) be the data matrix, N+ (N−) be the positive (negative) sample size such that N+ +N− = N, μ+ (μ−) be the sample mean of the foreground (background) class, and μ be the mean of all the training samples such that μ = NN+μ+ NN−μ−. [sent-190, score-0.22]
</p><p>56 pagation The goal of hypergraph propagation is to refine the confidence score si0 obtained from the LS-SVM for each sample, taking into account contextual information from surrounding samples. [sent-268, score-0.682]
</p><p>57 This is necessary because the score is based on binary codes which have sufficient precision to distinguish foreground from background, but not to locate the object reliably among foreground samples. [sent-269, score-0.347]
</p><p>58 cBoadseesd on }{xi}iK=1, we create a weakly supervised hypergraph eGd o=n ( {Vx, E}, w), where V = {vi}iK=1 is the vertex set corresponding to {ui}iK=1, E is Algorithm  3  Hypergraph  Algorithm 3 Hypergraph propagation  propagation  ×  Input: Binary codes {xi}iK=1and maximum iteration number τ. [sent-276, score-0.838]
</p><p>59 •  n ←  0;  Cno ←mp 0u;te the LS-SVM classification score vector s0 for {xi}iK=1; repeat • Construct the hypergraph G = (V,E,w) in Sec. [sent-278, score-0.434]
</p><p>60 The hypergraph G is represented by a |V| |E| incidence matTrihxe hHy p=e (gHra(pvhi, G ej)) |V| |E| :  H(vi,ej) =? [sent-291, score-0.401]
</p><p>61 The process of random walk on the hypergraph G [35] is governed by the following transition probability matrix P = (pij)K×K whose entry is defined as:  pij=e? [sent-306, score-0.45]
</p><p>62 (13)  Based on this transition probability matrix, the process of hypergraph propagation is formulated as follows:  sn+1 = αPsn where  α  + (1 − α)s0,  is a trade-off control factor such that 0 <  (14) α  < 1,  s0 = (s01, s20, . [sent-310, score-0.524]
</p><p>63 , si0 = f(xi)) and sn is the confidence score vector after propagation at the n-th iteration. [sent-316, score-0.245]
</p><p>64 The complete procedure of hypergraph propagation is summarized in Algorithm 3. [sent-338, score-0.524]
</p><p>65 Experimental setup In the experiments, eighteen publicly available video sequences are used for tracking performance evaluation. [sent-342, score-0.3]
</p><p>66 Like the multiinstance tracker [2], the proposed tracker performs object localization using a sliding-window-search scheme with a search radius of 30 pixels. [sent-344, score-0.634]
</p><p>67 The Haar-like feature is extracted in the same way as the CT tracker [33], resulting in a 100-dimensional feature vector. [sent-352, score-0.397]
</p><p>68 After random forest hashing, the binary code length ? [sent-354, score-0.417]
</p><p>69 Each random forest hash function is associated with a random forest comprising 10 randomized decision trees (i. [sent-357, score-0.864]
</p><p>70 The random forest hash functions in Algorithm 2 are updated every 10 frames. [sent-360, score-0.461]
</p><p>71 Empirical comparison of trackers We compare the proposed tracker with several state-ofthe-art trackers both qualitatively and quantitatively. [sent-394, score-0.651]
</p><p>72 We evaluate the CLE and VOR performance of all the nine trackers on eighteen video sequences. [sent-398, score-0.376]
</p><p>73 2 shows the qualitative tracking results of the nine trackers over several representative frames of eight video sequences. [sent-400, score-0.41]
</p><p>74 3 plots the frame-by-frame CLEs (highlighted in different colors) obtained by the nine trackers for the fifteen video sequences. [sent-402, score-0.28]
</p><p>75 1-2 report the average CLEs and VORs of the nine trackers on each of all the eighteen video sequences. [sent-404, score-0.376]
</p><p>76 1-2, we observe that the proposed tracker achieves the best tracking performance on most video sequences. [sent-407, score-0.521]
</p><p>77 In particular, the proposed tracker obtains the more robust tracking results in the presence of complicated appearance changes (caused by occlusion, drastic pose variation, background clutter, image distortion and blurring, etc. [sent-408, score-0.574]
</p><p>78 The tracked pedestrian, moving right to left, is lost by all other trackers at frame 78 as he overlaps 222444222311  “FJiugmurpe r2”:,Q“PueadlCitraoisve”,tra ncdk“inSgylrve”s)ultshaotfarteh rensip e ct iravcekly arsliogvne drsferovemralerft proesriegnhta nivedf raom euspotf hdeoweing. [sent-412, score-0.248]
</p><p>79 In contrast, both Struck and our tracker are robust to the body pose variations and partial occlusions encountered throughout the entire video sequence. [sent-423, score-0.419]
</p><p>80 Both our tracker and Struck successfully track the target across the whole video sequence, although our tracker locates the 222444222422  arEotnc1LieC24608 10C 2o d e L en3=0512F0r4amCye5c0liInsdt6x7809iorntacLeCE876543210 C o d 2e 0L en = 32150 FrJa4um0epIn5drx60789  FigatvOelpVRrCo0u. [sent-428, score-0.736]
</p><p>81 3, both Struck and our tracker obtain more accurate tracking results than the other trackers. [sent-439, score-0.447]
</p><p>82 Discussion and analysis In this section, we evaluate each component to show its contribution to the overall performance of the tracker and its sensitivity to parameter settings. [sent-442, score-0.317]
</p><p>83 Binary code length We quantitatively evaluate the performance of the proposed tracker for five different binary code lengths. [sent-444, score-0.598]
</p><p>84 4 that the CLE (VOR) performance improves as code length increases, and plateaus with approximately more than 100 hashing bits. [sent-448, score-0.393]
</p><p>85 This is a desirable property because we do not need a high-dimensional binary feature to achieve promising tracking performance. [sent-449, score-0.283]
</p><p>86 6543219870LStan2dV0arMSFV3Ma0meIn4d0x56  Figure 7: Quantitative evaluation of the proposed tracker with different SVMs in CLE and VOR on the “Cyclist” and “animal” video sequences. [sent-456, score-0.391]
</p><p>87 5, our RFH used in the proposed tracker achieves the better CLE and VOR performance than the other hashing methods in most cases. [sent-460, score-0.626]
</p><p>88 Evaluation of feature fusion methods In order to verify the effectiveness of our feature fusion method, we compare it to a direct concatenation of normalized feature vectors into a unified feature vector. [sent-461, score-0.336]
</p><p>89 6 displays the quantitative CLE and VOR performance of the proposed tracker with different feature fusion methods. [sent-463, score-0.476]
</p><p>90 Clearly, we see that our feature fusion method outperforms the standard feature fusion method in most cases. [sent-464, score-0.256]
</p><p>91 7 shows the quantitative CLE and VOR tracking results on two video sequences. [sent-467, score-0.235]
</p><p>92 in CLE and VOR on the “animal”,  “Jumper”,  and “Trellis”  video se-  tracker using the LS-SVM achieves close but slightly superior tracking performance to the standard SVM. [sent-473, score-0.521]
</p><p>93 Performance with and without hypergraph propagation The task of hypergraph propagation is to refine the confidence scores by random walk on the object/non-object community hypergraph. [sent-474, score-1.246]
</p><p>94 8 exhibits the quantitative CLE and VOR tracking results of the proposed tracker with and without hypergraph propagation on three video sequences. [sent-476, score-1.076]
</p><p>95 8 that hypergraph propagation gives rise to performance gain. [sent-478, score-0.524]
</p><p>96 Conclusion In this paper, we have proposed a robust visual tracker that learns compact and discriminative binary codes for an effective image representation. [sent-480, score-0.744]
</p><p>97 To obtain this representation, we develop a random forest hashing method, which efficiently constructs a set of hash functions by learning several randomized decision trees. [sent-481, score-0.889]
</p><p>98 To further improve the accuracy of object localization, we present a hypergraph propagation method to capture the interaction information from samples and their contexts. [sent-484, score-0.586]
</p><p>99 Compared with several state-of-the-art trackers on eighteen challenging sequences, we empirically show  that our tracker is able to achieve more accurate and robust tracking results in challenging conditions. [sent-485, score-0.738]
</p><p>100 Non-sparse linear representations for visual tracking with online reservoir metric learning. [sent-689, score-0.216]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hypergraph', 0.401), ('tracker', 0.317), ('hashing', 0.309), ('vor', 0.276), ('hash', 0.215), ('cle', 0.186), ('forest', 0.171), ('cyclist', 0.168), ('trackers', 0.167), ('tracking', 0.13), ('propagation', 0.123), ('randomized', 0.114), ('binary', 0.113), ('codes', 0.109), ('eighteen', 0.096), ('struck', 0.095), ('incremental', 0.093), ('compact', 0.09), ('oab', 0.089), ('fusion', 0.088), ('ui', 0.087), ('code', 0.084), ('nn', 0.082), ('ik', 0.075), ('frag', 0.074), ('video', 0.074), ('jumper', 0.072), ('ivt', 0.071), ('scoring', 0.07), ('cles', 0.064), ('samples', 0.062), ('dick', 0.059), ('nci', 0.059), ('cni', 0.059), ('sample', 0.056), ('confidence', 0.056), ('orf', 0.055), ('ct', 0.054), ('discriminative', 0.052), ('pages', 0.052), ('grabner', 0.052), ('online', 0.051), ('random', 0.049), ('walker', 0.049), ('vors', 0.048), ('nc', 0.047), ('weakly', 0.046), ('foreground', 0.046), ('covariance', 0.045), ('animal', 0.044), ('rfh', 0.043), ('incrementally', 0.041), ('shen', 0.041), ('pij', 0.04), ('sgn', 0.04), ('tree', 0.04), ('feature', 0.04), ('compressive', 0.04), ('htj', 0.039), ('hyperedges', 0.039), ('workflow', 0.039), ('nine', 0.039), ('contextual', 0.039), ('xi', 0.038), ('appearance', 0.038), ('lssvm', 0.037), ('buffer', 0.037), ('comprising', 0.037), ('ieee', 0.036), ('den', 0.036), ('vertex', 0.036), ('hengel', 0.035), ('drastic', 0.035), ('visual', 0.035), ('trellis', 0.034), ('sequence', 0.034), ('sn', 0.033), ('girl', 0.033), ('score', 0.033), ('scores', 0.032), ('leaf', 0.032), ('decision', 0.031), ('ht', 0.031), ('community', 0.031), ('quantitative', 0.031), ('dv', 0.03), ('refine', 0.03), ('converted', 0.03), ('mit', 0.03), ('lim', 0.029), ('update', 0.028), ('target', 0.028), ('bronstein', 0.028), ('robust', 0.028), ('uj', 0.027), ('saffari', 0.027), ('frame', 0.027), ('trees', 0.027), ('background', 0.026), ('updated', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="249-tfidf-1" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>Author: Xi Li, Chunhua Shen, Anthony Dick, Anton van_den_Hengel</p><p>Abstract: A key problem in visual tracking is to represent the appearance of an object in a way that is robust to visual changes. To attain this robustness, increasingly complex models are used to capture appearance variations. However, such models can be difficult to maintain accurately and efficiently. In this paper, we propose a visual tracker in which objects are represented by compact and discriminative binary codes. This representation can be processed very efficiently, and is capable of effectively fusing information from multiple cues. An incremental discriminative learner is then used to construct an appearance model that optimally separates the object from its surrounds. Furthermore, we design a hypergraph propagation method to capture the contextual information on samples, which further improves the tracking accuracy. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>2 0.3182371 <a title="249-tfidf-2" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>3 0.3025983 <a title="249-tfidf-3" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>4 0.28725407 <a title="249-tfidf-4" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>Author: Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van_den_Hengel, Zhenmin Tang</p><p>Abstract: Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original highdimensional data. The complexity of these models, and the problems with out-of-sample data, havepreviously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE [29] outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths.</p><p>5 0.28349823 <a title="249-tfidf-5" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>Author: Yue Lin, Rong Jin, Deng Cai, Shuicheng Yan, Xuelong Li</p><p>Abstract: Recent studies have shown that hashing methods are effective for high dimensional nearest neighbor search. A common problem shared by many existing hashing methods is that in order to achieve a satisfied performance, a large number of hash tables (i.e., long codewords) are required. To address this challenge, in this paper we propose a novel approach called Compressed Hashing by exploring the techniques of sparse coding and compressed sensing. In particular, we introduce a sparse coding scheme, based on the approximation theory of integral operator, that generate sparse representation for high dimensional vectors. We then project sparse codes into a low dimensional space by effectively exploring the Restricted Isometry Property (RIP), a key property in compressed sensing theory. Both of the theoretical analysis and the empirical studies on two large data sets show that the proposed approach is more effective than the state-of-the-art hashing algorithms.</p><p>6 0.27872801 <a title="249-tfidf-6" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>7 0.26031339 <a title="249-tfidf-7" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>8 0.23995966 <a title="249-tfidf-8" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>9 0.2316862 <a title="249-tfidf-9" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>10 0.22730094 <a title="249-tfidf-10" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>11 0.20297819 <a title="249-tfidf-11" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>12 0.15040988 <a title="249-tfidf-12" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>13 0.14474605 <a title="249-tfidf-13" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>14 0.12213787 <a title="249-tfidf-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.11150665 <a title="249-tfidf-15" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>16 0.11029947 <a title="249-tfidf-16" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>17 0.10923866 <a title="249-tfidf-17" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>18 0.10912701 <a title="249-tfidf-18" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>19 0.10711996 <a title="249-tfidf-19" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>20 0.10478119 <a title="249-tfidf-20" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.227), (1, -0.047), (2, -0.04), (3, -0.044), (4, 0.076), (5, -0.025), (6, 0.112), (7, -0.256), (8, -0.09), (9, 0.123), (10, -0.296), (11, -0.122), (12, -0.06), (13, 0.257), (14, -0.054), (15, 0.07), (16, 0.015), (17, -0.025), (18, 0.013), (19, 0.071), (20, -0.108), (21, 0.047), (22, 0.012), (23, -0.081), (24, -0.064), (25, -0.05), (26, -0.022), (27, 0.077), (28, 0.077), (29, 0.072), (30, -0.094), (31, 0.021), (32, -0.014), (33, 0.004), (34, 0.057), (35, 0.017), (36, -0.028), (37, -0.039), (38, 0.014), (39, 0.045), (40, -0.015), (41, -0.018), (42, -0.074), (43, 0.001), (44, 0.096), (45, 0.022), (46, 0.013), (47, 0.134), (48, -0.021), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91996598 <a title="249-lsi-1" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>Author: Xi Li, Chunhua Shen, Anthony Dick, Anton van_den_Hengel</p><p>Abstract: A key problem in visual tracking is to represent the appearance of an object in a way that is robust to visual changes. To attain this robustness, increasingly complex models are used to capture appearance variations. However, such models can be difficult to maintain accurately and efficiently. In this paper, we propose a visual tracker in which objects are represented by compact and discriminative binary codes. This representation can be processed very efficiently, and is capable of effectively fusing information from multiple cues. An incremental discriminative learner is then used to construct an appearance model that optimally separates the object from its surrounds. Furthermore, we design a hypergraph propagation method to capture the contextual information on samples, which further improves the tracking accuracy. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>2 0.68247771 <a title="249-lsi-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.66882724 <a title="249-lsi-3" href="./cvpr-2013-Compressed_Hashing.html">87 cvpr-2013-Compressed Hashing</a></p>
<p>Author: Yue Lin, Rong Jin, Deng Cai, Shuicheng Yan, Xuelong Li</p><p>Abstract: Recent studies have shown that hashing methods are effective for high dimensional nearest neighbor search. A common problem shared by many existing hashing methods is that in order to achieve a satisfied performance, a large number of hash tables (i.e., long codewords) are required. To address this challenge, in this paper we propose a novel approach called Compressed Hashing by exploring the techniques of sparse coding and compressed sensing. In particular, we introduce a sparse coding scheme, based on the approximation theory of integral operator, that generate sparse representation for high dimensional vectors. We then project sparse codes into a low dimensional space by effectively exploring the Restricted Isometry Property (RIP), a key property in compressed sensing theory. Both of the theoretical analysis and the empirical studies on two large data sets show that the proposed approach is more effective than the state-of-the-art hashing algorithms.</p><p>4 0.65357602 <a title="249-lsi-4" href="./cvpr-2013-Inductive_Hashing_on_Manifolds.html">223 cvpr-2013-Inductive Hashing on Manifolds</a></p>
<p>Author: Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van_den_Hengel, Zhenmin Tang</p><p>Abstract: Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes that preserve the Euclidean distance in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original highdimensional data. The complexity of these models, and the problems with out-of-sample data, havepreviously rendered them unsuitable for application to large-scale embedding, however. In this work, we consider how to learn compact binary embeddings on their intrinsic manifolds. In order to address the above-mentioned difficulties, we describe an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method. Our proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. We particularly show that hashing on the basis of t-SNE [29] outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths.</p><p>5 0.65330762 <a title="249-lsi-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.64942902 <a title="249-lsi-6" href="./cvpr-2013-Binary_Code_Ranking_with_Weighted_Hamming_Distance.html">63 cvpr-2013-Binary Code Ranking with Weighted Hamming Distance</a></p>
<p>7 0.63159454 <a title="249-lsi-7" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>8 0.6268878 <a title="249-lsi-8" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>9 0.60594404 <a title="249-lsi-9" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>10 0.5830375 <a title="249-lsi-10" href="./cvpr-2013-K-Means_Hashing%3A_An_Affinity-Preserving_Quantization_Method_for_Learning_Binary_Compact_Codes.html">236 cvpr-2013-K-Means Hashing: An Affinity-Preserving Quantization Method for Learning Binary Compact Codes</a></p>
<p>11 0.5782854 <a title="249-lsi-11" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>12 0.51607811 <a title="249-lsi-12" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>13 0.50831628 <a title="249-lsi-13" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>14 0.46546268 <a title="249-lsi-14" href="./cvpr-2013-Fast%2C_Accurate_Detection_of_100%2C000_Object_Classes_on_a_Single_Machine.html">163 cvpr-2013-Fast, Accurate Detection of 100,000 Object Classes on a Single Machine</a></p>
<p>15 0.4538025 <a title="249-lsi-15" href="./cvpr-2013-Boosting_Binary_Keypoint_Descriptors.html">69 cvpr-2013-Boosting Binary Keypoint Descriptors</a></p>
<p>16 0.44025937 <a title="249-lsi-16" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>17 0.41295186 <a title="249-lsi-17" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>18 0.40557873 <a title="249-lsi-18" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>19 0.40487006 <a title="249-lsi-19" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>20 0.39928642 <a title="249-lsi-20" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.151), (16, 0.021), (25, 0.186), (26, 0.025), (27, 0.027), (33, 0.238), (36, 0.032), (67, 0.111), (69, 0.041), (76, 0.011), (87, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86536026 <a title="249-lda-1" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>Author: Xi Li, Chunhua Shen, Anthony Dick, Anton van_den_Hengel</p><p>Abstract: A key problem in visual tracking is to represent the appearance of an object in a way that is robust to visual changes. To attain this robustness, increasingly complex models are used to capture appearance variations. However, such models can be difficult to maintain accurately and efficiently. In this paper, we propose a visual tracker in which objects are represented by compact and discriminative binary codes. This representation can be processed very efficiently, and is capable of effectively fusing information from multiple cues. An incremental discriminative learner is then used to construct an appearance model that optimally separates the object from its surrounds. Furthermore, we design a hypergraph propagation method to capture the contextual information on samples, which further improves the tracking accuracy. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.</p><p>2 0.83854795 <a title="249-lda-2" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>3 0.83554739 <a title="249-lda-3" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>4 0.83529603 <a title="249-lda-4" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>5 0.83427465 <a title="249-lda-5" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>6 0.83215714 <a title="249-lda-6" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>7 0.83191192 <a title="249-lda-7" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>8 0.8311801 <a title="249-lda-8" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>9 0.82882905 <a title="249-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.82845503 <a title="249-lda-10" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>11 0.82780355 <a title="249-lda-11" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>12 0.82774425 <a title="249-lda-12" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>13 0.82763553 <a title="249-lda-13" href="./cvpr-2013-3D_Pictorial_Structures_for_Multiple_View_Articulated_Pose_Estimation.html">2 cvpr-2013-3D Pictorial Structures for Multiple View Articulated Pose Estimation</a></p>
<p>14 0.82545674 <a title="249-lda-14" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>15 0.82374644 <a title="249-lda-15" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>16 0.82259804 <a title="249-lda-16" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>17 0.82215041 <a title="249-lda-17" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>18 0.82136816 <a title="249-lda-18" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>19 0.81989545 <a title="249-lda-19" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>20 0.81981122 <a title="249-lda-20" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
