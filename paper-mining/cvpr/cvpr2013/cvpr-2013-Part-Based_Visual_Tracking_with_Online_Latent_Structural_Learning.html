<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-324" href="#">cvpr2013-324</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</h1>
<br/><p>Source: <a title="cvpr-2013-324-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yao_Part-Based_Visual_Tracking_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>Reference: <a title="cvpr-2013-324-reference" href="../cvpr2013_reference/cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. [sent-10, score-0.494]
</p><p>2 We thus propose a method which models the unknown parts using latent variables. [sent-11, score-0.295]
</p><p>3 In doing so we extend the online algorithm pegasos to the structured prediction case (i. [sent-12, score-0.481]
</p><p>4 , predicting the location of the bounding boxes) with latent part variables. [sent-14, score-0.399]
</p><p>5 Introduction  Visual tracking is a goal in itself, but is also a preprocessing step before further analysis of the activities, behaviours, interactions and relationships between objects of interest. [sent-18, score-0.313]
</p><p>6 Recent progress in object tracking has yielded a steady increase in performance, but designing a robust algorithm to track generic objects in the presence of partially occluded and deformable targets is still a major challenge (as shown in Fig. [sent-19, score-0.502]
</p><p>7 This additional difficulty falls on top of those that regularly challenge visual tracking including significant variation in appearance due to factors such as changing pose, viewpoint, and illumination. [sent-21, score-0.362]
</p><p>8 One solution is to build a robust object appearance model and find the best candidate image patch to match the model, see, for example, incremental subspace learning [18], integral histogram tracking [1], visual tracking decomposition [13],  ? [sent-23, score-0.739]
</p><p>9 Figure 1: Qualitative tracking results of our tracker and competing trackers over representative frames of three sequences. [sent-45, score-0.651]
</p><p>10 [10] applied structured learning to (whole) visual object tracking, which builds upon its previous successful application to object detection [5]. [sent-54, score-0.27]
</p><p>11 All of these methods, however, delineate the tracked object by  a single regular bounding box (e. [sent-56, score-0.312]
</p><p>12 The difficulty in extending part-based appearance models to visual tracking is that adding parts dramatically increases the complexity of the model, and the model needs to be updated online. [sent-62, score-0.446]
</p><p>13 In addition, explicitly tracking each part would require individual training and initialisation for each part, which would significantly limit the practicality of an online method. [sent-63, score-0.671]
</p><p>14 We avoid this problem here by instead using latent variables in the representation of each part, thereby avoiding the need for additional interaction. [sent-64, score-0.254]
</p><p>15 The difficulty with using latent variables in a structured SVM, however, is that the underlying optimisation problem ceases to be convex. [sent-66, score-0.446]
</p><p>16 This is  a problem because structured SVM based tracking methods have thus far optimised the dual form of the underlying problem in order to reduce computational cost. [sent-67, score-0.538]
</p><p>17 Developing a practical, online, structured SVM visual tracking method which uses a part-based appearance model thus requires solving the primal form of the underlying optimisation problem. [sent-69, score-0.617]
</p><p>18 We employ an online structured output learning with latent variables to learn the weight parameters for an object and its parts, and distinguish the target object from the background using the weight parameters consequently. [sent-71, score-0.758]
</p><p>19 During tracking, the target object location is estimated by searching for the maximum classification score in the vicinity around the estimate from the previous frame, where the classification score is composed of the score of object and each parts. [sent-72, score-0.331]
</p><p>20 1 shows some tracking results obtained by our algorithm. [sent-75, score-0.313]
</p><p>21 Contributions (1) Unlike existing offline latent SVMs in object detection [7, 27, 29], we propose an online latent structured SVM for visual tracking. [sent-76, score-0.759]
</p><p>22 (2) We propose twostage training: the stage 1tracks the parts and estimates the  part parameter, and the stage 2 tracks the object and estimate the object parameter and correlation parameter. [sent-77, score-0.402]
</p><p>23 (3) We design a part-based linear kernel tracker that has competitive and often better tracking results than nonlinear kernel trackers. [sent-79, score-0.544]
</p><p>24 Our tracker adapts quickly to appearance changes such as partial occlusions and deformations. [sent-80, score-0.334]
</p><p>25 Discriminative learning with latent variables Motived by object detection with parts, Felzenszwalb et al. [sent-92, score-0.318]
</p><p>26 [7] introduce latent variables into discriminative training models for binary classification. [sent-93, score-0.294]
</p><p>27 They propose a latent binary SVM to handle object parts, which are not labelled during training. [sent-94, score-0.275]
</p><p>28 [27] extend this latent ap-  proach to use structured prediction, and solve the ensuing non-convex optimisation problem using Concave-Convex Procedure (CCCP). [sent-96, score-0.441]
</p><p>29 In [29] the viewpoint and the positions of object parts are treated as structural latent variables, and the latent structural SVM optimised in its dual form using an incremental CCCP algorithm. [sent-97, score-0.737]
</p><p>30 [23] also use a latent SVM framework, but develop a structured output model for detection with partial truncation. [sent-99, score-0.445]
</p><p>31 Although all of these part-based object detection frameworks use latent variables as part of a structured SVM, they rely on the availability of a model at training time, and thus cannot easily be used for online object tracking. [sent-100, score-0.766]
</p><p>32 Object tracking with parts In [11], each part is tracked independently, and the results treated as multiple measurements. [sent-101, score-0.468]
</p><p>33 In [26], a  weighted online structural learning approach is used to deal with the inevitable changes in target appearance over time. [sent-114, score-0.287]
</p><p>34 In comparison, the method we propose adds a part-based 222333666422  model to a discriminative training framework, and solves the ensuing linear structured SVM optimisation problem in primal form. [sent-115, score-0.333]
</p><p>35 Representation The method proceeds as follows: a bounding box on the target object is given in the 1st frame of the video, and the tracker is then required to track the object (by predicting a bounding box containing the object) from the 2nd frame to the end of the video. [sent-124, score-1.099]
</p><p>36 At the t-th frame, the bounding box of the target object is represented by Bt = (ct, rt, wt, ht), where ct, rt are the column and row coordinates of the upper-left corner, and wt, ht are the width and height. [sent-125, score-0.469]
</p><p>37 We consider a part, which can contain a piece of object, or a piece of background or both, to be represented by a smaller bounding box btj = (cjt , rtj , wtj , htj ), which we call part box from now on. [sent-131, score-0.512]
</p><p>38 Denote by M the number of parts, we use zt = (z1t , · · · , ztM) ∈ Z to represent the offsets of M part boxes at, ·t·he· tz-th )fr ∈am Ze. [sent-132, score-0.25]
</p><p>39 oH reeprer eesaecnht = , , 0, 0) ,j ∈ {1, ·· · , M} is the offset of j-th part box btj , i. [sent-133, score-0.342]
</p><p>40 (4) Here φ1 (xt, zj) represents the appearance of the j-th part box, φ2 (xt, y) represents the appearance of the bounding box, and φ3 (y, zj) reflects the compatibility between bounding box and the j-th part box. [sent-144, score-0.605]
</p><p>41 Latent Pegasos for Training Online As in existing tracking applications [10, 26], we would like the tracker to track the object from the 2nd frame on the basis of a bounding box of the target object provided in the 1st frame. [sent-162, score-1.092]
</p><p>42 In order to allow the use  of latent variables in an online SVM for visual tracking, we propose what can be seen an extension of online pegasos [21] (originally for binary or multi-class problems, but not latent variables) to structured output. [sent-167, score-1.077]
</p><p>43 We then use this w to predict y2 to obtain tracking result for the 2nd frame i. [sent-173, score-0.385]
</p><p>44 Denote wt the parameter to predict yt for the t-th frame. [sent-177, score-0.501]
</p><p>45 After predicting yt via f(xt, y, z; wt), we sample offsets {yt,i yt}iN=1, and estimate wt+1 via  =  =  =  wt+1  = argmin g(w, t) ,  (7)  w  where we consider the following objective  g(w;t) =2λ||w| 2+N1i? [sent-178, score-0.29]
</p><p>46 The label 222333666533  cost Δ(yt, y) measures dis-similarity between the true output yt and a candidate output y. [sent-192, score-0.303]
</p><p>47 Here we use  Δ(yt,y) = 1 −((BBtt−−11++ y ytt)) ∩ ∪ ((BBtt−−11++ yy)),  (9)  which was introduced in [6] to measure the VOC (Visual Object Classes) bounding box overlap ratio. [sent-193, score-0.248]
</p><p>48 We then update wt+1 = wt −ηt∇t with step size ηt = 1/(λt), it can be written as:  wt+1← (1 − ηtλ)wt+Nηti? [sent-205, score-0.311]
</p><p>49 (11)  Given the parameters wt and an image xt, the inference is to find the offset of bounding box of the target object with the best score,  (yt∗,zt∗) = ay∈rgYm,z∈aZxf(xt,y,z;wt)  (12)  ×  = argym∈Yax? [sent-213, score-0.729]
</p><p>50 Features  Zj  We use Haar-like features [24] to represent the appearances of the bounding box of object and object parts. [sent-235, score-0.376]
</p><p>51 Tlh ime integral image requires computing the sum of rectangular region, the tracking performance will be reduced when the shape of the target cannot be well fitted by rectangle. [sent-239, score-0.412]
</p><p>52 The spatial relationship between bounding box and part box is  defined as φ3 (y, zj) = (a, a2) ∈ R4 where a = (Bt−1 + y) [1 : 2] − (bt−1 + zj) [1 : 2] ∈ R2 . [sent-246, score-0.45]
</p><p>53 Two-stage Training The latent pegasos proposed in Section 2. [sent-259, score-0.419]
</p><p>54 3 (Left), a good y with red highlighted frame boundary does not receive the highest score in latent pegasos, and the predicted y via the parameter learnt by latent pegasos is obviously not a good one. [sent-270, score-0.83]
</p><p>55 In the first stage, given t-th frame xt and predicted ztj , we sample offsets of part box {zjt,k ztj }kN=1 . [sent-273, score-0.794]
</p><p>56 Learning utj+1 is effectively tracking part boxes, thus the learnt utj+1 gives pretty good estimate of where the parts should be. [sent-284, score-0.468]
</p><p>57 s the  bounding box of object, and four small rectangles  show the bounding box of parts. [sent-309, score-0.496]
</p><p>58 The parameter wt+1 can be used to predict the best offset yt∗+1 relative to the bounding box Bt+1 for tracking task, and used as the starting point in next iteration. [sent-315, score-0.639]
</p><p>59 The tracker adapts to deal with partial occlusion quickly: u1 and u2 (not occluded) adapt to have higher weights than u3 and u4 (occluded), thus the tracking result will rely more on the not occluded parts. [sent-318, score-0.598]
</p><p>60 So far, we have introduced the part-based appearance model and online updated discriminatively structured output classifier to identify the target object from candidates. [sent-320, score-0.489]
</p><p>61 The overall procedure of the proposed tracking algorithm we implemented in this work is shown in Algorithm 2. [sent-321, score-0.313]
</p><p>62 Experiments We evaluated our tracking system on thirteen challenging sequences, which include eight tracking by detection benchmark sequences (coke, david, faceocc1, faceocc2, girl, sylvester, tiger1, tiger2) and additional five sequences (board, fskater, threemen, trellis, dollar). [sent-323, score-0.881]
</p><p>63 we sample offsets y and z within a search radius s = 30 exhaustively in tracking stage, while on a polar grid within a search radius s = 60 in training stage. [sent-341, score-0.416]
</p><p>64 The search radius of object tracker and part trackers are all the same for all sequences. [sent-342, score-0.436]
</p><p>65 with the highest score in Latent Pegasos has much worse tracking result than the red highlighted one. [sent-352, score-0.401]
</p><p>66 The target object of three sequences marked with star (∗) wasn’t decomposed into parts in experiment since tThaeb olbe j1ec:t C iso mtopoa rsemdal avl. [sent-356, score-0.282]
</p><p>67 Otherwise, the tracking object is divided into three parts (in vertical direction if width is larger than height, or in horizontal direction). [sent-362, score-0.461]
</p><p>68 To examine the performance of the proposed tracking algorithm, we run nine state-of-the-art algorithms with the same initial position of the tracking object. [sent-363, score-0.626]
</p><p>69 For BHMC, only partial frames can be processed by using the binary code released by authors, we report the tracking results of those frames. [sent-368, score-0.367]
</p><p>70 Because our tracker use latent structured SVM with linear kernel, we also compared Struck with linear kernel (named as Struck Linear or StruckL). [sent-369, score-0.584]
</p><p>71 Secondly, we report the Pascal VOC overlap ratio, which is defined as Roverlap = Area(BT ∩ BGT)/Area(BT ∪ BGT), where  BT is the tracking bounding box and BGT t∪he B ground truth bounding box. [sent-372, score-0.678]
</p><p>72 5, it is considered to be successful in tracking for each frame. [sent-374, score-0.313]
</p><p>73 Comparison of different latent structured learning schema In order to evaluate the performance of different latent learning schema, we conduct an experiment on david sequence using different form of latent structural SVM. [sent-375, score-0.93]
</p><p>74 We sort the score of candidate offsets ys at frame 112 with different latent learning schema, and report the score of maximum 100 samples in Fig. [sent-376, score-0.476]
</p><p>75 From the experimental results, we can see that the part boxes and object boxes with la222333666866  SequenceOursStruck [10]StruckL [10]CT [28]MIL [4]OAB [8]BHMC [12]Frag [1]? [sent-378, score-0.259]
</p><p>76 Figure 4: The centre location error plots of our tracker with/without parts and struck with linear kernel on four sequences. [sent-490, score-0.5]
</p><p>77 On the contrary, the tracker with two-stage training maintains tracking the object and the parts robustly shown in Fig. [sent-493, score-0.732]
</p><p>78 Note that the output score of two-stage training is higher than latent pegasos due to different parameter scales. [sent-495, score-0.543]
</p><p>79 3) doesn’t not get the highest score using the latent pegasos. [sent-499, score-0.257]
</p><p>80 Evaluation of our tracking algorithm with and without part To justify the effect of object parts, we run Struck with linear kernel, ours tracking algorithm with and without part. [sent-503, score-0.761]
</p><p>81 Note that our algorithm without part is different with Struck with linear kernel, the structured SVM of Struck is solved in dual, but for us, it’s optimised based on primal form. [sent-504, score-0.321]
</p><p>82 evaluation of our tracker with different part initialisation on  Left: two kinds of initialisation (automatically  and manually). [sent-553, score-0.534]
</p><p>83 Evaluation of different part initialisations In [29], the authors point out that the traditional latent SVM using a gradient decent algorithm requires careful part initialisation. [sent-556, score-0.4]
</p><p>84 5, we initialise part with the same number in sequence fskater by using our automatic heuristic method and dividing meaningful object parts manually, respectively. [sent-560, score-0.325]
</p><p>85 The results indicate that our tracking algorithm is robust to part initialisation. [sent-565, score-0.384]
</p><p>86 Comparison of competing tracking algorithms We compare the proposed tracking algorithm with nine state-of222333666977  SequenceOursStruckStruckLCTMILOABBHMCFrag? [sent-567, score-0.663]
</p><p>87 1 shows some qualitative tracking results of our tracker and another several competing trackers on three sequences (for clarity, we just show the results of five trackers with better performance. [sent-572, score-0.79]
</p><p>88 ) More tracking results can be found in the supplemental material. [sent-573, score-0.313]
</p><p>89 Table 3 summarises the average centre location error performance of the compared tracking algorithms over the thirteen sequences. [sent-575, score-0.43]
</p><p>90 From the experimental results, we can see that our tracking algorithm obtains the best result on ten sequences. [sent-577, score-0.313]
</p><p>91 As for sequence girl, the tracking performance of our tracker is slightly lower than Stuck, it’s partly due to the part will not help when each part appears similarly (e. [sent-578, score-0.686]
</p><p>92 As mentioned in Table 1, there are not many practical implications to divide the target object of sequence coke, tiger1 and tiger2 into parts due to their small object re-  gion. [sent-581, score-0.277]
</p><p>93 But the results of our tracker on the above mentioned four sequences are better than Struck with linear kernel. [sent-583, score-0.3]
</p><p>94 In conclusion, the proposed tracker performs well on rigid objects such as board and faces as well as deformable objects such as persons. [sent-584, score-0.318]
</p><p>95 Conclusion We have introduced a part-based tracking algorithm with online latent structured learning. [sent-585, score-0.797]
</p><p>96 We use a global object box and a small number of part boxes to approximate the irregular object, and the model can be initialised easily. [sent-586, score-0.328]
</p><p>97 The new online learning schema overcame the over-fitting caused by the complexity of part model, thus improved the tracking accuracy of object parts. [sent-589, score-0.656]
</p><p>98 We performed object tracking using the proposed algorithm on thirteen challenging sequences comparable with a few of state-of-the-art methods, the results demonstrated the effectiveness and robustness of the proposed tracker. [sent-591, score-0.563]
</p><p>99 Robust visual tracking and vehicle classification via sparse representation. [sent-764, score-0.313]
</p><p>100 Automatic discovery of meaningful object parts with latent crfs. [sent-799, score-0.359]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tracking', 0.313), ('wt', 0.274), ('xt', 0.271), ('tracker', 0.231), ('yt', 0.227), ('latent', 0.211), ('pegasos', 0.208), ('struck', 0.185), ('bt', 0.184), ('zj', 0.15), ('structured', 0.142), ('box', 0.131), ('online', 0.131), ('bounding', 0.117), ('thirteen', 0.117), ('utj', 0.117), ('initialisation', 0.116), ('bhmc', 0.093), ('mza', 0.093), ('ztj', 0.093), ('cle', 0.09), ('vt', 0.084), ('parts', 0.084), ('offset', 0.078), ('schema', 0.077), ('vor', 0.077), ('frame', 0.072), ('part', 0.071), ('fskater', 0.07), ('mzaxf', 0.07), ('struckl', 0.07), ('trackers', 0.07), ('sequences', 0.069), ('target', 0.065), ('object', 0.064), ('offsets', 0.063), ('primal', 0.063), ('boxes', 0.062), ('btj', 0.062), ('bgt', 0.062), ('coke', 0.062), ('ct', 0.06), ('svm', 0.059), ('maxz', 0.058), ('mil', 0.056), ('zt', 0.054), ('frag', 0.054), ('vtd', 0.054), ('voc', 0.054), ('partial', 0.054), ('uj', 0.053), ('oab', 0.052), ('ivt', 0.052), ('optimisation', 0.05), ('ht', 0.05), ('deformable', 0.05), ('appearance', 0.049), ('girl', 0.048), ('azxf', 0.047), ('initialisations', 0.047), ('sequenceoursstruck', 0.047), ('threemen', 0.047), ('hare', 0.046), ('score', 0.046), ('optimised', 0.045), ('kwon', 0.045), ('zm', 0.044), ('variables', 0.043), ('rt', 0.042), ('structural', 0.042), ('xf', 0.042), ('highlighted', 0.042), ('btm', 0.041), ('motived', 0.041), ('twostage', 0.041), ('argym', 0.041), ('yax', 0.041), ('bbtt', 0.041), ('targets', 0.04), ('training', 0.04), ('receive', 0.04), ('stage', 0.039), ('pattern', 0.039), ('ys', 0.038), ('compressive', 0.038), ('dual', 0.038), ('ensuing', 0.038), ('sylvester', 0.038), ('grabner', 0.038), ('output', 0.038), ('competing', 0.037), ('board', 0.037), ('update', 0.037), ('initialise', 0.036), ('david', 0.036), ('track', 0.035), ('chunhua', 0.034), ('basin', 0.034), ('hopping', 0.034), ('rectangular', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="324-tfidf-1" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>2 0.35532752 <a title="324-tfidf-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.32227427 <a title="324-tfidf-3" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>4 0.32006297 <a title="324-tfidf-4" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>5 0.31239283 <a title="324-tfidf-5" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>Author: unkown-author</p><p>Abstract: We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the “right” frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.</p><p>6 0.31168634 <a title="324-tfidf-6" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>7 0.27872801 <a title="324-tfidf-7" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>8 0.22663726 <a title="324-tfidf-8" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>9 0.20887761 <a title="324-tfidf-9" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>10 0.16064373 <a title="324-tfidf-10" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>11 0.15994151 <a title="324-tfidf-11" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>12 0.15615398 <a title="324-tfidf-12" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>13 0.15603344 <a title="324-tfidf-13" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>14 0.14849906 <a title="324-tfidf-14" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>15 0.14053892 <a title="324-tfidf-15" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>16 0.14020123 <a title="324-tfidf-16" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>17 0.12569386 <a title="324-tfidf-17" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>18 0.12423992 <a title="324-tfidf-18" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>19 0.11841133 <a title="324-tfidf-19" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>20 0.11804405 <a title="324-tfidf-20" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.247), (1, -0.041), (2, -0.017), (3, -0.118), (4, 0.064), (5, -0.031), (6, 0.269), (7, -0.136), (8, 0.14), (9, 0.235), (10, -0.194), (11, -0.166), (12, -0.155), (13, 0.11), (14, -0.084), (15, -0.048), (16, 0.04), (17, -0.02), (18, 0.051), (19, 0.044), (20, 0.052), (21, -0.003), (22, 0.164), (23, -0.09), (24, -0.007), (25, 0.031), (26, -0.055), (27, 0.051), (28, 0.07), (29, 0.046), (30, 0.048), (31, 0.013), (32, 0.016), (33, -0.0), (34, -0.058), (35, -0.048), (36, 0.004), (37, -0.048), (38, -0.07), (39, 0.001), (40, 0.061), (41, 0.021), (42, -0.052), (43, 0.05), (44, 0.002), (45, -0.078), (46, 0.01), (47, 0.042), (48, -0.012), (49, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95727706 <a title="324-lsi-1" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>2 0.91702086 <a title="324-lsi-2" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>3 0.89829975 <a title="324-lsi-3" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>4 0.84648287 <a title="324-lsi-4" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>5 0.83191878 <a title="324-lsi-5" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><p>6 0.829943 <a title="324-lsi-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.80669045 <a title="324-lsi-7" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>8 0.70877773 <a title="324-lsi-8" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>9 0.69774961 <a title="324-lsi-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.61069363 <a title="324-lsi-10" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>11 0.60977018 <a title="324-lsi-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.5744319 <a title="324-lsi-12" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>13 0.54745275 <a title="324-lsi-13" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>14 0.52421588 <a title="324-lsi-14" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>15 0.50659782 <a title="324-lsi-15" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>16 0.49538863 <a title="324-lsi-16" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>17 0.48455238 <a title="324-lsi-17" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>18 0.48037907 <a title="324-lsi-18" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>19 0.47571981 <a title="324-lsi-19" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>20 0.47379443 <a title="324-lsi-20" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.177), (16, 0.034), (26, 0.048), (27, 0.248), (28, 0.01), (33, 0.211), (67, 0.087), (69, 0.042), (87, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82628793 <a title="324-lda-1" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>2 0.77786899 <a title="324-lda-2" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>Author: Hilton Bristow, Anders Eriksson, Simon Lucey</p><p>Abstract: Sparse coding has become an increasingly popular method in learning and vision for a variety of classification, reconstruction and coding tasks. The canonical approach intrinsically assumes independence between observations during learning. For many natural signals however, sparse coding is applied to sub-elements (i.e. patches) of the signal, where such an assumption is invalid. Convolutional sparse coding explicitly models local interactions through the convolution operator, however the resulting optimization problem is considerably more complex than traditional sparse coding. In this paper, we draw upon ideas from signal processing and Augmented Lagrange Methods (ALMs) to produce a fast algorithm with globally optimal subproblems and super-linear convergence.</p><p>3 0.75957257 <a title="324-lda-3" href="./cvpr-2013-Unsupervised_Salience_Learning_for_Person_Re-identification.html">451 cvpr-2013-Unsupervised Salience Learning for Person Re-identification</a></p>
<p>Author: Rui Zhao, Wanli Ouyang, Xiaogang Wang</p><p>Abstract: Human eyes can recognize person identities based on some small salient regions. However, such valuable salient information is often hidden when computing similarities of images with existing approaches. Moreover, many existing approaches learn discriminative features and handle drastic viewpoint change in a supervised way and require labeling new training data for a different pair of camera views. In this paper, we propose a novel perspective for person re-identification based on unsupervised salience learning. Distinctive features are extracted without requiring identity labels in the training procedure. First, we apply adjacency constrained patch matching to build dense correspondence between image pairs, which shows effectiveness in handling misalignment caused by large viewpoint and pose variations. Second, we learn human salience in an unsupervised manner. To improve the performance of person re-identification, human salience is incorporated in patch matching to find reliable and discriminative matched patches. The effectiveness of our approach is validated on the widely used VIPeR dataset and ETHZ dataset.</p><p>4 0.74610567 <a title="324-lda-4" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>5 0.74544245 <a title="324-lda-5" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>6 0.74495918 <a title="324-lda-6" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>7 0.74482244 <a title="324-lda-7" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>8 0.74224454 <a title="324-lda-8" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>9 0.7410928 <a title="324-lda-9" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>10 0.74049693 <a title="324-lda-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.73897231 <a title="324-lda-11" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>12 0.73852998 <a title="324-lda-12" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>13 0.73828894 <a title="324-lda-13" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>14 0.7370308 <a title="324-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.73646355 <a title="324-lda-15" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>16 0.7363891 <a title="324-lda-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.73577106 <a title="324-lda-17" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>18 0.73569548 <a title="324-lda-18" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>19 0.731996 <a title="324-lda-19" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>20 0.73196292 <a title="324-lda-20" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
