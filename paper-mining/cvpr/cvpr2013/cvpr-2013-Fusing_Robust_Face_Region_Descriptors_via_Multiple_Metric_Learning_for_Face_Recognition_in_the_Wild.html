<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-182" href="#">cvpr2013-182</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</h1>
<br/><p>Source: <a title="cvpr-2013-182-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Cui_Fusing_Robust_Face_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>Reference: <a title="cvpr-2013-182-reference" href="../cvpr2013_reference/cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn ct 1 ct  Abstract In many real-world face recognition  scenarios,  face  images can hardly be aligned accurately due to complex appearance variations or low-quality images. [sent-12, score-0.799]
</p><p>2 To address this issue, we propose a new approach to extract robust face region descriptors. [sent-13, score-0.475]
</p><p>3 volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. [sent-17, score-0.28]
</p><p>4 Moreover,  we develop  a new distance  method for face verification  metric  learning  called Pairwise-constrained  Multiple Metric Learning (PMML) to effectively integrate the face region descriptors  of all blocks (resp. [sent-22, score-1.234]
</p><p>5 Introduction Many face recognition systems have demonstrated promising results under well-controlled conditions with cooperative users. [sent-27, score-0.417]
</p><p>6 However, face recognition “in the wild” is still a challenging problem due to dramatic intra-class variations caused by pose, lighting and expression. [sent-28, score-0.417]
</p><p>7 Moreover, the faces in surveillance or internet videos (e. [sent-29, score-0.104]
</p><p>8 YouTube videos) are commonly with low-resolution and may be even blurred, which brings additional challenges for face recognition systems. [sent-31, score-0.417]
</p><p>9 According to the reported results, most face recognition algorithms degrade heavily on two real-world datasets: Labeled Faces in the Wild (LFW) [16] and YouTube Faces (YTF) [37]. [sent-32, score-0.417]
</p><p>10 Therefore, it is crucial to develop robust features and machine learning algorithms to improve the face recognition performances on these realworld datasets. [sent-33, score-0.509]
</p><p>11 According to the features, face recognition methods can be roughly divided into two categories: global feature based approaches and local feature based approaches. [sent-34, score-0.475]
</p><p>12 Although some of these methods have shown promising performances on public face datasets collected in the controlled environment, these hand-crafted descriptors cannot work well for face recognition in the wild (See the results on the LFW dataset1). [sent-42, score-1.003]
</p><p>13 Moreover, it is also unclear how to effectively employ these features for the more challenging Video-based Face Recognition (VFR) task, in which the face images are usually of low-resolution and even low-quality. [sent-44, score-0.382]
</p><p>14 If we directly apply the classical BoF model to face recognition, many facial details will be discarded, which might degrade the performance. [sent-47, score-0.461]
</p><p>15 edu/lfw/ 333555555422  BoF-based methods, such as Random Projection Tree [4] and Local Quantized Patterns (LQP) [32], have attempted  to apply BoF to face images, but it still lacks of a systematic study of BoF for face recognition with clear performance improvements. [sent-51, score-0.799]
</p><p>16 In this work, we propose a new approach to extract robust features for real-world face recognition tasks. [sent-52, score-0.446]
</p><p>17 One major challenge is that in these scenarios the face images are only roughly aligned, thus considerable spatial misalignment exists in the cropped face images. [sent-53, score-0.83]
</p><p>18 To address this issue, in this work, we partition each image or one video keyframe into a set of blocks and only compare the features extracted from corresponding blocks. [sent-54, score-0.168]
</p><p>19 On the other hand, we represent each block by a set of position-free patches without enforcing spatial constraints for the patches within the block. [sent-55, score-0.225]
</p><p>20 This simple strategy in combination with the BoF framework leads to an effective feature, which is robust to face misalignment. [sent-56, score-0.382]
</p><p>21 Specifically, we first adopt the nonnegative sparse coding to quantize each patch according to a set of visual words in a pre-constructed visual vocabulary from k-means clustering. [sent-57, score-0.185]
</p><p>22 video) by sum-pooling the reconstruction coefficients over the patches within each block (resp. [sent-59, score-0.149]
</p><p>23 We also develop a new distance metric learning method called Pairwise-constrained Multiple Metric Learning (PMML) for face verification by integrating the SFRDs (resp. [sent-64, score-0.646]
</p><p>24 In contrast to the existing approaches which can only learn one distance metric for one type of feature, our method simultaneously learns multiple metrics for different descriptors, which better utilizes the correlations of these descriptors. [sent-67, score-0.195]
</p><p>25 We conduct the experiments on two real-world face datasets, LFW [16] and YTF [37]. [sent-69, score-0.382]
</p><p>26 Extensive experiments demonstrate the effectiveness of our new descriptors, SFRD and STFRD, as well as our PMML metric learning method for face verification under unconstrained conditions. [sent-70, score-0.67]
</p><p>27 However, only few BoF methods were used for face recognition, because face images are different only on very subtle textons. [sent-75, score-0.796]
</p><p>28 Most of the existing work on metric learning focuses on the Mahalanobis distance learning [12, 14, 21, 26, 36]. [sent-81, score-0.211]
</p><p>29 However, they only optimize one metric without considering how to jointly learn multiple metrics. [sent-83, score-0.11]
</p><p>30 Video-based face recognition underwent explosive developments in recent years [6, 7, 10, 15, 20, 34, 35, 37]. [sent-84, score-0.417]
</p><p>31 Such approaches usually consider face images in a video as an image set. [sent-85, score-0.422]
</p><p>32 Overview In this section, we take video based face verification as an example to briefly introduce our proposed method, and  the image based face verification can be considered a special case by assuming each video only contains one frame. [sent-90, score-1.012]
</p><p>33 For the input of our system, we first crop out face images by using Viola-Jones face detector [33] and then roughly align them by fixing the coordinates ofautomatically detected facial feature points [37]2. [sent-91, score-0.872]
</p><p>34 1, to address the misalignment problem, we partition each video into several spatial-temporal volumes and then represent each volume by using a set of position-free patches sampled from each keyframe within the volume without considering the spatial positions of these patches. [sent-93, score-0.388]
</p><p>35 we use  the roughly aligned faces provided in the LFW 333555555533  based face verification. [sent-98, score-0.433]
</p><p>36 The image based face verification is a special case by treating each video as one frame. [sent-99, score-0.506]
</p><p>37 patches are used as the input for the sparse coding method to obtain their reconstruction coefficients. [sent-100, score-0.125]
</p><p>38 We then apply sumpooling over the reconstruction coefficients in each volume to extract the token-frequency feature. [sent-101, score-0.116]
</p><p>39 After that, whitened principal component analysis is utilized to further reduce  the feature dimension, which leads to our spatial-temporal face region descriptor. [sent-102, score-0.554]
</p><p>40 For still images, spatial face region descriptors are extracted by a similar process without considering the temporal dimension. [sent-103, score-0.569]
</p><p>41 Moreover, we further propose a pairwise-constrained multiple metric learning method to integrate the descriptors from all the volumes in each video. [sent-105, score-0.301]
</p><p>42 Different from the existing methods which can only learn one distance metric for one type of descriptor, we simultaneously learn multiple distance metrics, which can better exploit the correlations among the descriptors from different volumes. [sent-106, score-0.283]
</p><p>43 Spatial Face Region Descriptor  To enhance the facial textons and suppress Gaussian noises, we first apply a DoG filter on each face image. [sent-116, score-0.461]
</p><p>44 Then we uniformly partition each face image into K spatial blocks. [sent-117, score-0.445]
</p><p>45 Moreover, we denote each spatial block as Sk for k = 1, . [sent-121, score-0.129]
</p><p>46 Nonnegative sparse coding: Since each patch only captures subtle facial textons, many patches are visually similar to each other. [sent-125, score-0.23]
</p><p>47 Directly using the intensity feature for face recognition may not be favorable, because even a tiny perturbation(e. [sent-126, score-0.48]
</p><p>48 To improve the robustness and enhance the discriminability, we employ the sparse coding method to map the patches from the low dimension feature space to a high dimension feature space. [sent-129, score-0.279]
</p><p>49 we employ the sparse coding method with nonnegative constraints, which can be formulated as follows:  2  mcin ? [sent-138, score-0.146]
</p><p>50 To effectively describe the statistical properties of these position-free codes in each block, we extract the TF feature for each block by using the sumpooling method. [sent-150, score-0.232]
</p><p>51 Specifically, the TF feature of the k-th block can be obtained as,  xk = ? [sent-151, score-0.13]
</p><p>52 To re-  duce the feature redundancy for efficient face recognition, we therefore seek a compact representation. [sent-159, score-0.411]
</p><p>53 Especially, for face images, the same visual word usually recurs many times in smooth facial areas, e. [sent-162, score-0.461]
</p><p>54 Then we obtain the SFRD zk for the k-th block by using WPCA as follows:  zk=? [sent-170, score-0.155]
</p><p>55 Therefore, instead of using pixelwise sampling, we adopt the sparse sampling along the spatial dimension with a fixed step because it is sufficient to capture the texton information. [sent-179, score-0.135]
</p><p>56 We then perform the nonnegative sparse coding as similarly used in SFRD. [sent-180, score-0.146]
</p><p>57 In the pooling step, the sum pooling is applied to all the patches within the entire spatial-temporal volume along both the spatial and temporal dimensions. [sent-181, score-0.216]
</p><p>58 Such a spatial-temporal pooling strategy essentially characterizes the statistics of a certain region of the face in the video. [sent-182, score-0.494]
</p><p>59 Pairwise-constrained Multiple Metric Learning With above descriptors extracted for each block, we need to further consider distance metric to compare them, as well as the method fusing these block-wise descriptors. [sent-188, score-0.244]
</p><p>60 For this purpose, we propose a new multiple metric learning  method to jointly learn a discriminative distance. [sent-189, score-0.141]
</p><p>61 , zK} where zlekt |k uKs=1 d eisn othtee feaacceh region descriptor a =nd { Kz is the tot}al w number| of blocks/volumns for each image/video. [sent-193, score-0.126]
</p><p>62 =K1(zik− zjk)TWk(zik− zjk),  (5)  where zi and zj are respectively the i-th and j-th samples (i. [sent-197, score-0.107]
</p><p>63 , two sets of face region descriptors from two images/videos), zik and zjk are respectively the k-th face region descriptors for the i-th and j-th samples, and Wk is the Mahalanobis matrix. [sent-199, score-1.472]
</p><p>64 Intuitively, given two descriptors zi and zj of two images/videos, their learnt distance d(zi, zj) should be smaller than a certain threshold ρ when the two images/videos are from the same person, and it must be larger than the threshold ρ when the two images/videos are from different subjects. [sent-200, score-0.285]
</p><p>65 An alternative way is to separately learn a distance metrics Wk for each face region descriptor, and then fuse the K distances as the final distance. [sent-205, score-0.531]
</p><p>66 However, considering that these K face region descriptors are from the same face image, jointly learning these distance metrics is more favorable by implicitly taking global information into account. [sent-206, score-1.039]
</p><p>67 Another possible way is to concatenate the K descriptors together into a single feature vector for each sample, however, which results in a very large distance metric (Km×Km). [sent-207, score-0.273]
</p><p>68 Formulation In distance metric learning methods, a regularizer is usually imposed on the Mahalanobis matrix to prevent overfitting due to the small training set and high model complexity. [sent-211, score-0.18]
</p><p>69 333555555755  With the regularizer in (6), we formulate the multiple metric learning problem as follows:  W1m,. [sent-217, score-0.141]
</p><p>70 k=K1dWk(zik,zjk) ≤ δijρ − τ,  (7) (8)  where dWk = (zik − zjk)TWk(zik − zjk) is the distance between the two face− region descriptors, ρ is the threshold  for distance comparison, τ > 0 is the margin, δij = 1 if the two samples belong to the same subject, and δij = −1 otherwise. [sent-224, score-0.142]
</p><p>71 (11)  Compared with the original LogDet divergence loss in [12], this hinge loss not only avoids unnecessary penalties on those pairs that already satisfy the constraint, but also accelerates the algorithm when only considering the violated constraints in each cyclic projection (See Section 5. [sent-238, score-0.208]
</p><p>72 Experiments We evaluate our method for the unconstrained face verification task by using two real-world face datasets: LFW [16] and YTF [37] datasets. [sent-279, score-0.911]
</p><p>73 Experimental Setup LFW is an image dataset for unconstrained face verification. [sent-282, score-0.445]
</p><p>74 It contains more than 13,000 face images collected from the web with large variations in pose, age, expression, illumination, etc. [sent-283, score-0.382]
</p><p>75 We use the 5,000 video pairs randomly selected in [37] for unconstrained face verification, in which one half of the pairs of videos are from the same subject, while the remaining half of the pairs are from different subjects. [sent-290, score-0.661]
</p><p>76 We directly crop the face images according to the provided data and then resize them into 110×60 pixels for LFW aendd d 4a0ta× a2n4d pixels feosirz Ye tThFe. [sent-292, score-0.382]
</p><p>77 Face Region Descriptor and PMML Our approach consists of two key components: face re-  ×  gion descriptors and the learning algorithm PMML. [sent-314, score-0.508]
</p><p>78 72%) while “Single LE + comp” requires more accurate face alignment technique for cropping the facial components. [sent-334, score-0.461]
</p><p>79 Moreover, we also observe that the learnt descriptors (i. [sent-335, score-0.139]
</p><p>80 intensity feature, LBP and Gabor), which indicates that it is beneficial to learn the descriptors for unconstrained face recognition. [sent-339, score-0.574]
</p><p>81 For the distance metric learning method, we compare our PMML with three baselines: SFRD, SFRD+SVM and SFRD+ITML. [sent-340, score-0.18]
</p><p>82 SFRD uses the Euclidean distance based on the original spatial face region descriptor; SFRD+SVM uses the SVM classifier for multiple SFRDs; SFRD+ITML is the average of distance metrics which are individually learnt by using ITML [12] for each face region descriptor. [sent-341, score-1.088]
</p><p>83 on Osturarte SsF tRhDat+ PPMMMMLL can better integrate the descriptors from different facial regions. [sent-349, score-0.174]
</p><p>84 The above results clearly demonstrate the effectiveness of the proposed SFRD descriptor and the pairwise-constrained multiple metric learning method. [sent-380, score-0.203]
</p><p>85 Experimental Comparisons on YTF For video-based face verification on the YTF dataset, we compare our proposed method with the several existing VFR methods, including MSM [39], DCC(pair) [20], MMD [35], AHISD [6], CHISD [6] and SANP [15]. [sent-383, score-0.466]
</p><p>86 Note that LBP(Min dist) and MBGS+LBP are generally better than other baselines, possibly because higher resolution face images were used in their methods. [sent-406, score-0.382]
</p><p>87 Conclusion In this paper, we have proposed two robust face region descriptors SFRD and STFRD for image-based and videobased face recognition, respectively. [sent-412, score-0.923]
</p><p>88 spatial-temporal volumes), and then apply the BoF model and the sum pooling method in each block (resp. [sent-419, score-0.149]
</p><p>89 WPCA is finally adopted to generate robust face region descriptors. [sent-421, score-0.446]
</p><p>90 Furthermore, we develop a new pairwiseconstrained multiple metric learning (PMML) method to integrate the face region descriptors from different regions. [sent-422, score-0.682]
</p><p>91 Face description with local binary patterns: Application to face recognition. [sent-432, score-0.382]
</p><p>92 Beyond simple features: A large-scale feature search approach to unconstrained face recognition. [sent-472, score-0.474]
</p><p>93 Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. [sent-526, score-0.531]
</p><p>94 Patch distribution compatible semisupervised dimension reduction for face and human gait recognition. [sent-539, score-0.463]
</p><p>95 Robust face recognition using block-  [26] [27] [28] [29] [30] [3 1] [32] [33] [34] [35]  [36] [37] [38] [39] [40]  [41] [42]  based bag of words. [sent-584, score-0.417]
</p><p>96 How far can you get with a modern face recognition test set using only simple features? [sent-600, score-0.417]
</p><p>97 Manifold-manifold distance with application to face recognition based on image set. [sent-640, score-0.456]
</p><p>98 Distance metric learning for large margin nearest neighbor classification. [sent-646, score-0.141]
</p><p>99 Face recognition in unconstrained videos with matched background similarity. [sent-652, score-0.151]
</p><p>100 Dynamic texture recognition using local binary patterns with an application to facial expressions. [sent-689, score-0.114]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sfrd', 0.429), ('face', 0.382), ('pmml', 0.257), ('ytf', 0.236), ('lfw', 0.23), ('zjk', 0.214), ('zik', 0.176), ('stfrd', 0.15), ('wpca', 0.15), ('ij', 0.125), ('bof', 0.124), ('metric', 0.11), ('block', 0.101), ('descriptors', 0.095), ('wk', 0.095), ('tf', 0.091), ('wkt', 0.088), ('lbp', 0.084), ('verification', 0.084), ('comp', 0.08), ('whitened', 0.079), ('facial', 0.079), ('nonnegative', 0.069), ('zj', 0.069), ('volumes', 0.065), ('region', 0.064), ('sfrds', 0.064), ('stfrds', 0.064), ('vfr', 0.064), ('unconstrained', 0.063), ('descriptor', 0.062), ('performances', 0.061), ('mahalanobis', 0.06), ('cyclic', 0.055), ('zk', 0.054), ('videos', 0.053), ('logdet', 0.053), ('faces', 0.051), ('gabor', 0.05), ('dimension', 0.048), ('patches', 0.048), ('pooling', 0.048), ('wild', 0.048), ('itml', 0.047), ('blocks', 0.047), ('metrics', 0.046), ('keyframe', 0.046), ('coding', 0.045), ('volume', 0.044), ('learnt', 0.044), ('cui', 0.044), ('fisherface', 0.043), ('sumpooling', 0.043), ('twk', 0.043), ('pca', 0.043), ('pairs', 0.041), ('video', 0.04), ('youtube', 0.04), ('distance', 0.039), ('patch', 0.039), ('misalignment', 0.038), ('zi', 0.038), ('lqp', 0.038), ('ldml', 0.038), ('noises', 0.037), ('restricted', 0.037), ('dictionary', 0.035), ('le', 0.035), ('partition', 0.035), ('recognition', 0.035), ('shan', 0.035), ('intensity', 0.034), ('gait', 0.033), ('itj', 0.033), ('magic', 0.033), ('pcca', 0.033), ('zhen', 0.033), ('wolf', 0.032), ('sparse', 0.032), ('mbgs', 0.032), ('fg', 0.032), ('subtle', 0.032), ('learning', 0.031), ('overcomplete', 0.031), ('nanyang', 0.03), ('codes', 0.03), ('extract', 0.029), ('eigenfaces', 0.029), ('ntu', 0.029), ('violated', 0.029), ('hinge', 0.029), ('feature', 0.029), ('kij', 0.028), ('moreover', 0.028), ('spatial', 0.028), ('sampling', 0.027), ('divergence', 0.027), ('penalties', 0.027), ('scatter', 0.027), ('technological', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="182-tfidf-1" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>2 0.31419411 <a title="182-tfidf-2" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>3 0.26211655 <a title="182-tfidf-3" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>Author: Dong Chen, Xudong Cao, Fang Wen, Jian Sun</p><p>Abstract: Making a high-dimensional (e.g., 100K-dim) feature for face recognition seems not a good idea because it will bring difficulties on consequent training, computation, and storage. This prevents further exploration of the use of a highdimensional feature. In this paper, we study the performance of a highdimensional feature. We first empirically show that high dimensionality is critical to high performance. A 100K-dim feature, based on a single-type Local Binary Pattern (LBP) descriptor, can achieve significant improvements over both its low-dimensional version and the state-of-the-art. We also make the high-dimensional feature practical. With our proposed sparse projection method, named rotated sparse regression, both computation and model storage can be reduced by over 100 times without sacrificing accuracy quality.</p><p>4 0.25244373 <a title="182-tfidf-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.25027129 <a title="182-tfidf-5" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>Author: Dong Yi, Zhen Lei, Stan Z. Li</p><p>Abstract: Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper, we propose a novel method for pose robust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-ofthe-art methods significantly, meanwhile, the method works well on LFW.</p><p>6 0.23948054 <a title="182-tfidf-6" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>7 0.19973545 <a title="182-tfidf-7" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>8 0.17102529 <a title="182-tfidf-8" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>9 0.16742574 <a title="182-tfidf-9" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>10 0.16097617 <a title="182-tfidf-10" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>11 0.15366393 <a title="182-tfidf-11" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>12 0.11601079 <a title="182-tfidf-12" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>13 0.10545578 <a title="182-tfidf-13" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>14 0.10544121 <a title="182-tfidf-14" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>15 0.10333371 <a title="182-tfidf-15" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>16 0.10234493 <a title="182-tfidf-16" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>17 0.10135079 <a title="182-tfidf-17" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>18 0.095993772 <a title="182-tfidf-18" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>19 0.0959002 <a title="182-tfidf-19" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>20 0.094212897 <a title="182-tfidf-20" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, -0.107), (2, -0.121), (3, 0.069), (4, 0.018), (5, -0.023), (6, -0.057), (7, -0.141), (8, 0.226), (9, -0.206), (10, 0.119), (11, -0.067), (12, 0.124), (13, 0.106), (14, 0.028), (15, 0.037), (16, 0.015), (17, -0.013), (18, -0.001), (19, 0.035), (20, -0.044), (21, 0.039), (22, 0.016), (23, 0.035), (24, 0.027), (25, 0.053), (26, -0.054), (27, 0.1), (28, -0.055), (29, -0.12), (30, -0.012), (31, 0.041), (32, 0.098), (33, 0.015), (34, 0.035), (35, 0.005), (36, -0.023), (37, 0.064), (38, 0.02), (39, -0.037), (40, 0.073), (41, 0.0), (42, 0.005), (43, -0.103), (44, -0.049), (45, -0.034), (46, -0.007), (47, 0.036), (48, -0.022), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97525382 <a title="182-lsi-1" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>2 0.92264366 <a title="182-lsi-2" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>Author: Haoxiang Li, Gang Hua, Zhe Lin, Jonathan Brandt, Jianchao Yang</p><p>Abstract: Pose variation remains to be a major challenge for realworld face recognition. We approach this problem through a probabilistic elastic matching method. We take a part based representation by extracting local features (e.g., LBP or SIFT) from densely sampled multi-scale image patches. By augmenting each feature with its location, a Gaussian mixture model (GMM) is trained to capture the spatialappearance distribution of all face images in the training corpus. Each mixture component of the GMM is confined to be a spherical Gaussian to balance the influence of the appearance and the location terms. Each Gaussian component builds correspondence of a pair of features to be matched between two faces/face tracks. For face verification, we train an SVM on the vector concatenating the difference vectors of all the feature pairs to decide if a pair of faces/face tracks is matched or not. We further propose a joint Bayesian adaptation algorithm to adapt the universally trained GMM to better model the pose variations between the target pair of faces/face tracks, which consistently improves face verification accuracy. Our experiments show that our method outperforms the state-ofthe-art in the most restricted protocol on Labeled Face in the Wild (LFW) and the YouTube video face database by a significant margin.</p><p>3 0.89631826 <a title="182-lsi-3" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>4 0.82283968 <a title="182-lsi-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.81485516 <a title="182-lsi-5" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>Author: Dong Yi, Zhen Lei, Stan Z. Li</p><p>Abstract: Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper, we propose a novel method for pose robust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-ofthe-art methods significantly, meanwhile, the method works well on LFW.</p><p>6 0.77976251 <a title="182-lsi-6" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>7 0.77836913 <a title="182-lsi-7" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>8 0.72592497 <a title="182-lsi-8" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>9 0.72518468 <a title="182-lsi-9" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<p>10 0.72493619 <a title="182-lsi-10" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>11 0.72046542 <a title="182-lsi-11" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>12 0.69570535 <a title="182-lsi-12" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>13 0.68411368 <a title="182-lsi-13" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>14 0.65662694 <a title="182-lsi-14" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>15 0.63786227 <a title="182-lsi-15" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>16 0.59002584 <a title="182-lsi-16" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>17 0.58642572 <a title="182-lsi-17" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>18 0.58289093 <a title="182-lsi-18" href="./cvpr-2013-Learning_Locally-Adaptive_Decision_Functions_for_Person_Verification.html">252 cvpr-2013-Learning Locally-Adaptive Decision Functions for Person Verification</a></p>
<p>19 0.58188283 <a title="182-lsi-19" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>20 0.53573757 <a title="182-lsi-20" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.197), (10, 0.095), (16, 0.055), (19, 0.035), (26, 0.042), (28, 0.011), (29, 0.012), (33, 0.267), (39, 0.012), (67, 0.096), (69, 0.042), (87, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93293595 <a title="182-lda-1" href="./cvpr-2013-Measuring_Crowd_Collectiveness.html">282 cvpr-2013-Measuring Crowd Collectiveness</a></p>
<p>Author: Bolei Zhou, Xiaoou Tang, Xiaogang Wang</p><p>Abstract: Collective motions are common in crowd systems and have attracted a great deal of attention in a variety of multidisciplinary fields. Collectiveness, which indicates the degree of individuals acting as a union in collective motion, is a fundamental and universal measurement for various crowd systems. By integrating path similarities among crowds on collective manifold, this paper proposes a descriptor of collectiveness and an efficient computation for the crowd and its constituent individuals. The algorithm of the Collective Merging is then proposed to detect collective motions from random motions. We validate the effectiveness and robustness of the proposed collectiveness descriptor on the system of self-driven particles. We then compare the collectiveness descriptor to human perception for collective motion and show high consistency. Our experiments regarding the detection of collective motions and the measurement of collectiveness in videos of pedestrian crowds and bacteria colony demonstrate a wide range of applications of the collectiveness descriptor1.</p><p>2 0.87679827 <a title="182-lda-2" href="./cvpr-2013-Prostate_Segmentation_in_CT_Images_via_Spatial-Constrained_Transductive_Lasso.html">342 cvpr-2013-Prostate Segmentation in CT Images via Spatial-Constrained Transductive Lasso</a></p>
<p>Author: Yinghuan Shi, Shu Liao, Yaozong Gao, Daoqiang Zhang, Yang Gao, Dinggang Shen</p><p>Abstract: Accurate prostate segmentation in CT images is a significant yet challenging task for image guided radiotherapy. In this paper, a novel semi-automated prostate segmentation method is presented. Specifically, to segment the prostate in the current treatment image, the physician first takes a few seconds to manually specify the first and last slices of the prostate in the image space. Then, the prostate is segmented automatically by theproposed two steps: (i) Thefirst step of prostate-likelihood estimation to predict the prostate likelihood for each voxel in the current treatment image, aiming to generate the 3-D prostate-likelihood map by the proposed Spatial-COnstrained Transductive LassO (SCOTO); (ii) The second step of multi-atlases based label fusion to generate the final segmentation result by using the prostate shape information obtained from the planning and previous treatment images. The experimental result shows that the proposed method outperforms several state-of-the-art methods on prostate segmentation in a real prostate CT dataset, consisting of 24 patients with 330 images. Moreover, it is also clinically feasible since our method just requires the physician to spend a few seconds on manual specification of the first and last slices of the prostate.</p><p>3 0.86854297 <a title="182-lda-3" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>Author: Ning Zhu, Albert C.S. Chung</p><p>Abstract: In this paper, we propose a graph-based method for 3D vessel tree structure segmentation based on a new tubularity Markov tree model ( TMT), which works as both new energy function and graph construction method. With the help of power-watershed implementation [7], a global optimal segmentation can be obtained with low computational cost. Different with other graph-based vessel segmentation methods, the proposed method does not depend on any skeleton and ROI extraction method. The classical issues of the graph-based methods, such as shrinking bias and sensitivity to seed point location, can be solved with the proposed method thanks to vessel data fidelity obtained with TMT. The proposed method is compared with some classical graph-based image segmentation methods and two up-to-date 3D vessel segmentation methods, and is demonstrated to be more accurate than these methods for 3D vessel tree segmentation. Although the segmentation is done without ROI extraction, the computational cost for the proposed method is low (within 20 seconds for 256*256*144 image).</p><p>4 0.8654834 <a title="182-lda-4" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<p>Author: Dan Levi, Shai Silberstein, Aharon Bar-Hillel</p><p>Abstract: In this work we present a new part-based object detection algorithm with hundreds of parts performing realtime detection. Part-based models are currently state-ofthe-art for object detection due to their ability to represent large appearance variations. However, due to their high computational demands such methods are limited to several parts only and are too slow for practical real-time implementation. Our algorithm is an accelerated version of the “Feature Synthesis ” (FS) method [1], which uses multiple object parts for detection and is among state-of-theart methods on human detection benchmarks, but also suffers from a high computational cost. The proposed Accelerated Feature Synthesis (AFS) uses several strategies for reducing the number of locations searched for each part. The first strategy uses a novel algorithm for approximate nearest neighbor search which we developed, termed “KDFerns ”, to compare each image location to only a subset of the model parts. Candidate part locations for a specific part are further reduced using spatial inhibition, and using an object-level “coarse-to-fine ” strategy. In our empirical evaluation on pedestrian detection benchmarks, AFS main- × tains almost fully the accuracy performance of the original FS, while running more than 4 faster than existing partbased methods which use only several parts. AFS is to our best knowledge the first part-based object detection method achieving real-time running performance: nearly 10 frames per-second on 640 480 images on a regular CPU.</p><p>same-paper 5 0.85931373 <a title="182-lda-5" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>Author: Zhen Cui, Wen Li, Dong Xu, Shiguang Shan, Xilin Chen</p><p>Abstract: In many real-world face recognition scenarios, face images can hardly be aligned accurately due to complex appearance variations or low-quality images. To address this issue, we propose a new approach to extract robust face region descriptors. Specifically, we divide each image (resp. video) into several spatial blocks (resp. spatial-temporal volumes) and then represent each block (resp. volume) by sum-pooling the nonnegative sparse codes of position-free patches sampled within the block (resp. volume). Whitened Principal Component Analysis (WPCA) is further utilized to reduce the feature dimension, which leads to our Spatial Face Region Descriptor (SFRD) (resp. Spatial-Temporal Face Region Descriptor, STFRD) for images (resp. videos). Moreover, we develop a new distance method for face verification metric learning called Pairwise-constrained Multiple Metric Learning (PMML) to effectively integrate the face region descriptors of all blocks (resp. volumes) from an image (resp. a video). Our work achieves the state- of-the-art performances on two real-world datasets LFW and YouTube Faces (YTF) according to the restricted protocol.</p><p>6 0.83485013 <a title="182-lda-6" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>7 0.83049661 <a title="182-lda-7" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>8 0.82981813 <a title="182-lda-8" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>9 0.82931018 <a title="182-lda-9" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>10 0.82828939 <a title="182-lda-10" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>11 0.82810211 <a title="182-lda-11" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>12 0.82767099 <a title="182-lda-12" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>13 0.82691634 <a title="182-lda-13" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>14 0.82648551 <a title="182-lda-14" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>15 0.82616562 <a title="182-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.8258999 <a title="182-lda-16" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>17 0.82585001 <a title="182-lda-17" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>18 0.82553399 <a title="182-lda-18" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>19 0.82547295 <a title="182-lda-19" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>20 0.8249867 <a title="182-lda-20" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
