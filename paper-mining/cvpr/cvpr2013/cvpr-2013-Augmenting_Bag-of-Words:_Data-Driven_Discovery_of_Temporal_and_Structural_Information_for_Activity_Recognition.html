<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-49" href="#">cvpr2013-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-49-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bettadapura_Augmenting_Bag-of-Words_Data-Driven_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>Reference: <a title="cvpr-2013-49-reference" href="../cvpr2013_reference/cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu/cpl/projects/abow  Abstract We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. [sent-11, score-0.246]
</p><p>2 Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. [sent-12, score-0.489]
</p><p>3 Extracting activity information from time-varying data has applications in domains such as video understanding, activity monitor-  ing for healthcare and surveillance. [sent-17, score-0.444]
</p><p>4 Traditionally, sequential models like Hidden Markov Models (HMMs) and Dynamic Bayesian Networks have been used to address activity recognition as a time-series analysis problem. [sent-18, score-0.31]
</p><p>5 However, the assumption of Markovian dynamics restricts the application of such sequential models to relatively simple problems with known spatial and temporal structure of the data to be analyzed [22]. [sent-19, score-0.317]
</p><p>6 As a promising alternative, research in activity recognition from videos and other time-series data has moved towards bag-of-words (BoW) approaches and away from the traditional sequential and syntactic models. [sent-21, score-0.354]
</p><p>7 Furthermore, standard BoW approaches do not account for the fact that different types of activities have different temporal signatures. [sent-23, score-0.407]
</p><p>8 Recent activity recognition approaches such as [19] have extended the BoW approach  with topic models [23] using probabilistic Latent Semantic Analysis [10] and Latent Dirichlet Allocation [1], leading to more complex classification methods built on top of standard BoW representations. [sent-25, score-0.289]
</p><p>9 In contrast, we increase the richness of the features in the BoW representation and with the use of standard classification backends (like k-NN, HMM and SVM), we demonstrate that our augmented BoW techniques lead to better recognition of complex activities. [sent-26, score-0.249]
</p><p>10 Contributions: We describe a method to represent temporal information by quantizing time and defining new temporal events in a data-driven manner. [sent-27, score-0.836]
</p><p>11 We propose three encoding schemes that use n-grams to augment BoW with the discovered temporal events in a way that preserves the local structural information (relative word positions) in the activity. [sent-28, score-0.827]
</p><p>12 We evaluate our approach in comparison to standard BoW representations on four diverse classification tasks: i) Vehicle activity recognition from surveillance videos (Section 4. [sent-32, score-0.4]
</p><p>13 1); ii) Surgical skill assessment from surgery videos 222666111977  (Section 4. [sent-33, score-0.331]
</p><p>14 However, when activities are represented as bags of words, the underlying sequential information provided by the ordering of the words is typically lost. [sent-45, score-0.315]
</p><p>15 More recently, variants of the n-gram approach have been used to represent activities in terms of their local event sub-sequences [9]. [sent-47, score-0.346]
</p><p>16 While this preserves local sequential information and causal ordering, adding absolute and relative temporal information results in more powerful representations as we demonstrate in this paper. [sent-48, score-0.351]
</p><p>17 Activity Recognition with Augmented BoW We define an activity as a finite sequence of events over  a finite period of time where each event in the activity is an occurrence. [sent-59, score-1.102]
</p><p>18 For example, if “start”, “turn”, “straight” and “stop” are four individual events, then a vehicle driving activity will be a finite sequence of those events over some finite time (e. [sent-60, score-0.769]
</p><p>19 Recent methods for activity recognition try to detect such observable events and build BoW upon it. [sent-66, score-0.806]
</p><p>20 However, the temporal structure underlying the activities that shall be recognized is typically neglected. [sent-67, score-0.407]
</p><p>21 The time taken by each observable event and the time elapsed between two subsequent events are two important properties that contribute to the temporal signature of an activity that is being performed. [sent-68, score-1.401]
</p><p>22 For example, a car at a traffic light will have a shorter time gap between the “stop” and “start” events than a delivery vehicle that has to stop for a much longer time (until its contents are loaded/unloaded) before it can start again. [sent-69, score-0.537]
</p><p>23 Discovering Temporal Information We represent activities as sequences of discrete, observable events. [sent-72, score-0.379]
</p><p>24 Each activity ai iost a sequence of elements from Each event type can occur multiple times at different positions in ai. [sent-81, score-0.493]
</p><p>25 poral event defined as the time elapsed between the end of observable event ej and the start of observable event ek, where k > j. [sent-84, score-1.303]
</p><p>26 Also, let πj,k be the temporal event defined as the time elapsed between the start of observable event ej and the end of observable event ek, where k ≥ j. [sent-86, score-1.532]
</p><p>27 ts T hwuhs,er τeas πj,k measure the time elapsed between any two events including the time taken by those two events. [sent-88, score-0.581]
</p><p>28 We posit that these two types of temporal events, τj,k and πj,k, can model all the temporal properties of an activity. [sent-90, score-0.458]
</p><p>29 τj,j+1  : Time elapsed between any two consecutive events ej and ej+1  2. [sent-92, score-0.735]
</p><p>30 πj,k:  : Time elapsed between any two events where k > j : Time taken by a single event Time taken by set of events 222666112088  ej  and  ek,  ej  ej  to  ek,  where k ≥ j  Figure 1. [sent-95, score-1.559]
</p><p>31 Histogram of event durations for Ocean City dataset (left) and data-driven creation of temporal bins (right; N = 5). [sent-96, score-0.461]
</p><p>32 To work with these temporal events, we will have to quantize them into a finite number of N bins. [sent-97, score-0.257]
</p><p>33 As illustrated by the temporal event duration histograms of τj,j+1 for the Ocean-City dataset (see Section 4. [sent-100, score-0.494]
</p><p>34 1) in Figure 1, short and medium duration temporal events occur much more frequently than longer duration temporal events. [sent-101, score-1.005]
</p><p>35 Similar temporal distributions are observed in the other datasets we have analyzed. [sent-102, score-0.229]
</p><p>36 To ensure that we capture the most useful temporal information, we pursue a data-driven approach for binning. [sent-103, score-0.229]
</p><p>37 If there are S temporal events, then we divide the temporal space into N bins such that each of the N bins contains an equal proportion S/N of the temporal events (illustrated in Figure 1 for N = 5). [sent-105, score-1.168]
</p><p>38 Note that, if the time-line had been naively divided into 5 equally sized bins, then most of the temporal events would have been placed in the first bin while the other 4 bins would have been almost empty. [sent-106, score-0.646]
</p><p>39 Example 1: Say, temporal event τj,k is of 4 second duration and temporal event τl,m is of 20 second duration, then from Figure 1, we see that τj,k will be assigned to bin D and τl,m will be assigned to bin E. [sent-109, score-0.891]
</p><p>40 Let ψ denote the function that maps the temporal events to their respective temporal bins. [sent-110, score-0.811]
</p><p>41 There are many possible ways by which we can encode these new temporal events along with the observable events to build augmented BoW representations. [sent-112, score-1.247]
</p><p>42 The simplest way would be to just add the quantized temporal events to the BoW, i. [sent-113, score-0.623]
</p><p>43 , if the BoW contained x observable events and we extracted y new quantized temporal events, then the augmented BoW will now contain x + y number of elements. [sent-115, score-0.935]
</p><p>44 Encoding Local Structure In the following we describe three encoding schemes we have developed that merge the temporal events with the observable events in a way that captures local structure. [sent-119, score-1.279]
</p><p>45 1  Interspersed Encoding  In interspersed encoding, the main focus is on the time elapsed between every pair of consecutive events. [sent-123, score-0.434]
</p><p>46 Let τj,j+1 be a temporal event defined as the time elapsed between any two consecutive observable events ej and ej+1 in activity ai. [sent-124, score-1.58]
</p><p>47 Once the quantized temporal events ψ(τj,j+1 ) are computed for all event pairs ej , ej+1 ∈ ai, they are then inserted into ai at their appropriate positions between events ej and ej+1 . [sent-125, score-1.529]
</p><p>48 Let this new sequence of interspersed events for activity ai be denoted by Ti. [sent-126, score-0.874]
</p><p>49 In general, if activity ai has d events, then after the inclusion of the quantized temporal events, Ti will have 2d −1 events (the original d observable events plus tihlel new 2dd −11 temporal heeve onrtisgi)n. [sent-127, score-1.675]
</p><p>50 If temporal event τ1,2 is of 4 second duration and τ2,3 is of 20 second duration, then the quantized temporal events will be ψ(τ1,2) = D and ψ(τ2,3) = E. [sent-129, score-1.117]
</p><p>51 So, the interspersed sequence of events for activity a1, will be T1= (e1, D, e2 , E, e3). [sent-130, score-0.827]
</p><p>52 This is particularly adverse in the context of activity recognition because activities correspond to causal chains of observable and temporal events. [sent-134, score-0.898]
</p><p>53 We employ n-grams in order to retain ordering of events [6]. [sent-136, score-0.403]
</p><p>54 Using this approach, for every activity ai, the event sequence Ti is transformed into an n-gram sequence TiI (where the superscript I stands for interspersed). [sent-139, score-0.502]
</p><p>55 This TiI feature vector representing activity ai is the final result of interspersed encoding. [sent-140, score-0.465]
</p><p>56 2  Cumulative Encoding  In cumulative encoding, the main focus is on the cumulative time taken by a subsequence of observable events. [sent-144, score-0.36]
</p><p>57 Let ψ(πj,j+n−1) be a quantized temporal event defined as the total time taken by n consecutive events ej to ej+n−1 in ac222666112199  tivity  ai. [sent-145, score-1.02]
</p><p>58 Once the quantized temporal event ψ(πj,j+n−1 ) is  computed for the consecutive sequence ofobservable events ej . [sent-146, score-1.051]
</p><p>59 Le∈t t hais new sequence of “cumulative” observable and temporal events for activity ai be denoted by TiC (where the superscript C stands for “cumulative”). [sent-150, score-1.108]
</p><p>60 So, the new sequence of events for activity a2, will be T2C = (e1e2e3D, e2e3e4E, e3e4e5A) or in histogram form T2C = {e1e2e3D ⇒ 1, e2e3e4E ⇒ 1, e3e4e5A ⇒ 1 }. [sent-156, score-0.631]
</p><p>61 Interspersed encoding ⇒foc 1u,sees on tAhe ⇒ ⇒tim 1e} elapsed between events whereas cumulative encoding focuses on the time taken by the events. [sent-157, score-0.847]
</p><p>62 3  Pyramid Encoding  Given the choice of encoding scheme —either interspersed or cumulative— in pyramid encoding all l-grams of length l,∀l ∈ [1, n] are generated. [sent-160, score-0.42]
</p><p>63 We denote BoW representations for activity ai generated through pyramid encoding by TiP. [sent-162, score-0.407]
</p><p>64 , TiI, TiC and TiP is the augmented BoW model containing the observable and temporal events, encoded in a way that cap-  tures the local structure. [sent-165, score-0.541]
</p><p>65 Obviously, it is computationally intractable to enumerate all possible regular expressions for a given vocabulary of observable and temporal events. [sent-170, score-0.622]
</p><p>66 The results of our experimental evaluation suggest that this combination of PPSSRS sampling of the regular expression subspace strikes the right balance between discovering global patterns across activities and discovering the anomalous activities. [sent-188, score-0.277]
</p><p>67 Accepted regular expressions are treated as new words and added to our augmented BoW representation. [sent-190, score-0.332]
</p><p>68 This final representation now contains automatically discovered temporal information and both local and global structural information of the activities. [sent-191, score-0.293]
</p><p>69 Our experiments show that increasing the number of words in BoW through randomly generated regular expressions by just 20% boosts the activity recognition and anomaly detection results significantly (Section 4). [sent-192, score-0.473]
</p><p>70 Given videos or time-series data of activities, temporal information is discovered using the histogram method described in Section 3. [sent-198, score-0.301]
</p><p>71 Using n-grams, the temporal information is then merged with the extracted BoW thereby preserving local ordering of the words. [sent-200, score-0.279]
</p><p>72 The new BoW model is then further augmented by adding new words created using randomly sampled regular expressions (to capture global patterns in the data), and then processed by the statistical modeling backend for actual activity recognition. [sent-201, score-0.59]
</p><p>73 Optimization of the estimation procedure for augmented BoW representations involves the two main parameters in our system: N, the number of temporal bins used for quantization and n, the size of the n-gram used for encoding. [sent-207, score-0.43]
</p><p>74 Low values of N and n result in the loss of temporal and structural information whereas high values can lead to large BoW with very high dimensionality. [sent-208, score-0.265]
</p><p>75 The main evaluation criterion for all activity recognition experiments is classification accuracy, which we report as absolute percentages and, for more detailed analysis, in confusion matrices. [sent-213, score-0.289]
</p><p>76 Vector Space Models (VSM), treat the derived BoW vectors of activities as document vectors and allow for automatic analysis in terms of querying, classifying, and clustering the activities [16]. [sent-220, score-0.356]
</p><p>77 Our encoding schemes outperform the BoW baseline on three classification backends: VSM, sequential models (HMMs) and SVMs. [sent-227, score-0.267]
</p><p>78 An event detector analyzed the tracks, detected changes in structure over time and represented each track by a sequence of observable events. [sent-237, score-0.48]
</p><p>79 The types of events detected in each track were “start”, “stop”, “turn” and “u-turn”. [sent-238, score-0.353]
</p><p>80 Long-range(left)andclose-up(right)stilsofvideo  footage from training sessions for surgical skill assessment. [sent-248, score-0.282]
</p><p>81 As part of a larger case-study, 16 medical students were recruited to perform typical suturing activities (stitching, knot tying, etc. [sent-259, score-0.231]
</p><p>82 It can be seen that augmented BoW based approaches outperform the BoW baseline in all 7 skill metrics with an overall accuracy of72. [sent-267, score-0.292]
</p><p>83 Since our augmented BoW representations capture time and co-occurrence of words, we hypothesized that an automated analysis procedure using augmented BoW should perform particularly well in assessing the “time and motion” and “knowledge of procedure” skills. [sent-269, score-0.273]
</p><p>84 Our encoding (Interspersed encoding with 3-grams, 3 time bins and 20 random regular expressions) gives better cluster quality than the BoW baseline. [sent-289, score-0.412]
</p><p>85 Learning Player Activities from Soccer Videos Automatic detection, tracking and labeling of the players in soccer videos is critical for analyzing team tac-  tics and player activities. [sent-295, score-0.231]
</p><p>86 In our experiments, we consider the problem of unsupervised learning of long-range activities and roles the various players take on the field. [sent-297, score-0.276]
</p><p>87 The tracks were given to an event detector that divided the field into 4 zones (Figure 6) and detected 10 types of events: “Enter-Zone-A”, “Leave-Zone-A”, “EnterZone-B”, “Leave-Zone-B”, “Enter-Zone-C”, “Leave-ZoneC”, “Enter-Zone-D”, “Leave-Zone-D”, “Receive-Ball” and “Send-Ball”. [sent-301, score-0.236]
</p><p>88 Our encoding (Interspersed encoding with 3-grams, 5 time bins and with 20 random regular expressions) outperforms the BoW  baseline on all 7 metrics. [sent-337, score-0.441]
</p><p>89 Results on WAAS dataset: Left: BoW baseline; Middle: BoW + Time; Right: Our encoding (5-grams, 5 time bins and with 1, 000 random regular expressions). [sent-339, score-0.3]
</p><p>90 In order to aid research in this area, the WAAS dataset has been released, which contains Monte Carlo simulation of the activities of 4, 623 individuals for a total duration of 46. [sent-349, score-0.328]
</p><p>91 There are a total of 180 events (like “Eat Lunch”, “Enter Vehicle”, “Exit Vehicle”, “Move”, “Wait”, etc) with a total of 544, 777 event sequences spread across 28, 682 buildings. [sent-351, score-0.521]
</p><p>92 Given this large database, we show that our augmented BoW can successfully classify people’s professions and detect some of the suspect individuals based on the temporal and structural similarities in their activities. [sent-354, score-0.438]
</p><p>93 This successful identification of suspicious behavior is especially remarkable since those suspects aim for imitating "normal" behavior and thus their activities are very similar to harmless activities. [sent-357, score-0.232]
</p><p>94 For the surgery dataset, though all the 7 skill classifications were statistically significant, due to space constraints, only results on “knowledge of procedure” classification is presented. [sent-361, score-0.283]
</p><p>95 Conclusion BoW models are a promising approach to real-world activity recognition problems where only little is known a-priori about the underlying structure of the data to 222666222533  M1 vs M2M1 vs M3M1 vs M2M1 vs M3  p-χva2lue<1 06. [sent-368, score-0.356]
</p><p>96 Left: Comparing the methods in Figure 7 for the WAAS dataset; Right: Comparing the methods in Table 1 for the “knowledge of procedure” skill in the surgery dataset (the other 6 skill classifications were also statistically significant, but are not shown due to space constraints). [sent-379, score-0.398]
</p><p>97 We presented a significant extension to BoW-based activity recognition, where we augment BoW with temporal information and with both local and global structural information, using temporal encoding, n-grams and randomly sampled regular expressions, respectively. [sent-381, score-0.853]
</p><p>98 In addition to generally improved activity recognition, our approach also detects anomalies in the data, which is important, for example in human behavior analysis applications. [sent-382, score-0.277]
</p><p>99 Exploiting multi-level parallelism for low-latency activity recognition in streaming video. [sent-411, score-0.252]
</p><p>100 Objective structured assessment of technical skill (osats) for surgical residents. [sent-504, score-0.313]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bow', 0.506), ('events', 0.353), ('temporal', 0.229), ('activity', 0.222), ('observable', 0.201), ('interspersed', 0.196), ('activities', 0.178), ('elapsed', 0.178), ('ej', 0.169), ('event', 0.168), ('skill', 0.152), ('waas', 0.125), ('encoding', 0.112), ('augmented', 0.111), ('regular', 0.099), ('duration', 0.097), ('surgical', 0.095), ('expressions', 0.093), ('backends', 0.071), ('players', 0.071), ('ocean', 0.069), ('surgery', 0.069), ('cumulative', 0.067), ('assessment', 0.066), ('hmms', 0.066), ('bins', 0.064), ('soccer', 0.064), ('airborne', 0.063), ('sequential', 0.058), ('vehicle', 0.057), ('sequence', 0.056), ('osats', 0.053), ('referee', 0.053), ('suturing', 0.053), ('city', 0.053), ('stop', 0.052), ('player', 0.052), ('ordering', 0.05), ('ai', 0.047), ('videos', 0.044), ('symbols', 0.042), ('significance', 0.042), ('quantized', 0.041), ('hmm', 0.041), ('surveillance', 0.041), ('augment', 0.038), ('causal', 0.038), ('classification', 0.037), ('gat', 0.036), ('tii', 0.036), ('structural', 0.036), ('backend', 0.036), ('bbooww', 0.036), ('mcnemar', 0.036), ('pps', 0.036), ('professions', 0.036), ('suture', 0.036), ('vinay', 0.036), ('vsm', 0.036), ('zones', 0.036), ('sessions', 0.035), ('consecutive', 0.035), ('tracks', 0.032), ('actom', 0.032), ('srs', 0.032), ('surgeon', 0.032), ('action', 0.031), ('schemes', 0.031), ('participants', 0.03), ('ek', 0.03), ('analyzed', 0.03), ('preceding', 0.03), ('recognition', 0.03), ('symbol', 0.029), ('competence', 0.029), ('disney', 0.029), ('irfan', 0.029), ('words', 0.029), ('baseline', 0.029), ('finite', 0.028), ('discovered', 0.028), ('anomalies', 0.028), ('instruments', 0.028), ('parking', 0.028), ('skills', 0.028), ('simulation', 0.027), ('vehicles', 0.027), ('unsupervised', 0.027), ('massive', 0.027), ('rand', 0.027), ('behavior', 0.027), ('expert', 0.027), ('vs', 0.026), ('essa', 0.026), ('individuals', 0.026), ('representations', 0.026), ('start', 0.025), ('hamid', 0.025), ('time', 0.025), ('statistically', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="49-tfidf-1" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>2 0.29193956 <a title="49-tfidf-2" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>Author: Yingying Zhu, Nandita M. Nayak, Amit K. Roy-Chowdhury</p><p>Abstract: In thispaper, rather than modeling activities in videos individually, we propose a hierarchical framework that jointly models and recognizes related activities using motion and various context features. This is motivated from the observations that the activities related in space and time rarely occur independently and can serve as the context for each other. Given a video, action segments are automatically detected using motion segmentation based on a nonlinear dynamical model. We aim to merge these segments into activities of interest and generate optimum labels for the activities. Towards this goal, we utilize a structural model in a max-margin framework that jointly models the underlying activities which are related in space and time. The model explicitly learns the duration, motion and context patterns for each activity class, as well as the spatio-temporal relationships for groups of them. The learned model is then used to optimally label the activities in the testing videos using a greedy search method. We show promising results on the VIRAT Ground Dataset demonstrating the benefit of joint modeling and recognizing activities in a wide-area scene.</p><p>3 0.2343953 <a title="49-tfidf-3" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>Author: Michael S. Ryoo, Larry Matthies</p><p>Abstract: This paper discusses the problem of recognizing interaction-level human activities from a first-person viewpoint. The goal is to enable an observer (e.g., a robot or a wearable camera) to understand ‘what activity others are performing to it’ from continuous video inputs. These include friendly interactions such as ‘a person hugging the observer’ as well as hostile interactions like ‘punching the observer’ or ‘throwing objects to the observer’, whose videos involve a large amount of camera ego-motion caused by physical interactions. The paper investigates multichannel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. In our experiments, we not only show classification results with segmented videos, but also confirm that our new approach is able to detect activities from continuous videos reliably.</p><p>4 0.19868016 <a title="49-tfidf-4" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>Author: Yu Cao, Daniel Barrett, Andrei Barbu, Siddharth Narayanaswamy, Haonan Yu, Aaron Michaux, Yuewei Lin, Sven Dickinson, Jeffrey Mark Siskind, Song Wang</p><p>Abstract: Recognizing human activities in partially observed videos is a challengingproblem and has many practical applications. When the unobserved subsequence is at the end of the video, the problem is reduced to activity prediction from unfinished activity streaming, which has been studied by many researchers. However, in the general case, an unobserved subsequence may occur at any time by yielding a temporal gap in the video. In this paper, we propose a new method that can recognize human activities from partially observed videos in the general case. Specifically, we formulate the problem into a probabilistic framework: 1) dividing each activity into multiple ordered temporal segments, 2) using spatiotemporal features of the training video samples in each segment as bases and applying sparse coding (SC) to derive the activity likelihood of the test video sample at each segment, and 3) finally combining the likelihood at each segment to achieve a global posterior for the activities. We further extend the proposed method to include more bases that correspond to a mixture of segments with different temporal lengths (MSSC), which can better rep- resent the activities with large intra-class variations. We evaluate the proposed methods (SC and MSSC) on various real videos. We also evaluate the proposed methods on two special cases: 1) activity prediction where the unobserved subsequence is at the end of the video, and 2) human activity recognition on fully observed videos. Experimental results show that the proposed methods outperform existing state-of-the-art comparison methods.</p><p>5 0.19491798 <a title="49-tfidf-5" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied, we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers, we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components, across other videos in the system, to perform subsequent video analysis. We demonstrate the efficacy of our approach for activity classification and clustering by reporting competitive results on standard datasets such as, HMDB, UCF-50, Olympic Sports and KTH.</p><p>6 0.19006832 <a title="49-tfidf-6" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>7 0.18568018 <a title="49-tfidf-7" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>8 0.18473162 <a title="49-tfidf-8" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>9 0.17262657 <a title="49-tfidf-9" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>10 0.1581071 <a title="49-tfidf-10" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>11 0.14240903 <a title="49-tfidf-11" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>12 0.13804938 <a title="49-tfidf-12" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>13 0.13109152 <a title="49-tfidf-13" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>14 0.13044885 <a title="49-tfidf-14" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>15 0.1272417 <a title="49-tfidf-15" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>16 0.12614408 <a title="49-tfidf-16" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>17 0.1116079 <a title="49-tfidf-17" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>18 0.10710362 <a title="49-tfidf-18" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>19 0.10637016 <a title="49-tfidf-19" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>20 0.099578075 <a title="49-tfidf-20" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, -0.085), (2, -0.038), (3, -0.14), (4, -0.138), (5, 0.008), (6, -0.125), (7, -0.058), (8, -0.054), (9, 0.055), (10, 0.095), (11, -0.097), (12, 0.068), (13, -0.029), (14, 0.025), (15, 0.034), (16, 0.05), (17, 0.161), (18, 0.026), (19, -0.232), (20, -0.064), (21, 0.051), (22, 0.047), (23, -0.067), (24, -0.048), (25, 0.075), (26, -0.029), (27, -0.065), (28, 0.022), (29, 0.11), (30, 0.075), (31, 0.009), (32, -0.02), (33, -0.13), (34, -0.014), (35, -0.06), (36, 0.029), (37, -0.082), (38, -0.005), (39, 0.046), (40, 0.004), (41, -0.014), (42, 0.056), (43, -0.007), (44, 0.014), (45, -0.047), (46, -0.015), (47, -0.011), (48, -0.034), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.969284 <a title="49-lsi-1" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>2 0.82838887 <a title="49-lsi-2" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>Author: Yu Cao, Daniel Barrett, Andrei Barbu, Siddharth Narayanaswamy, Haonan Yu, Aaron Michaux, Yuewei Lin, Sven Dickinson, Jeffrey Mark Siskind, Song Wang</p><p>Abstract: Recognizing human activities in partially observed videos is a challengingproblem and has many practical applications. When the unobserved subsequence is at the end of the video, the problem is reduced to activity prediction from unfinished activity streaming, which has been studied by many researchers. However, in the general case, an unobserved subsequence may occur at any time by yielding a temporal gap in the video. In this paper, we propose a new method that can recognize human activities from partially observed videos in the general case. Specifically, we formulate the problem into a probabilistic framework: 1) dividing each activity into multiple ordered temporal segments, 2) using spatiotemporal features of the training video samples in each segment as bases and applying sparse coding (SC) to derive the activity likelihood of the test video sample at each segment, and 3) finally combining the likelihood at each segment to achieve a global posterior for the activities. We further extend the proposed method to include more bases that correspond to a mixture of segments with different temporal lengths (MSSC), which can better rep- resent the activities with large intra-class variations. We evaluate the proposed methods (SC and MSSC) on various real videos. We also evaluate the proposed methods on two special cases: 1) activity prediction where the unobserved subsequence is at the end of the video, and 2) human activity recognition on fully observed videos. Experimental results show that the proposed methods outperform existing state-of-the-art comparison methods.</p><p>3 0.77467352 <a title="49-lsi-3" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>Author: Yingying Zhu, Nandita M. Nayak, Amit K. Roy-Chowdhury</p><p>Abstract: In thispaper, rather than modeling activities in videos individually, we propose a hierarchical framework that jointly models and recognizes related activities using motion and various context features. This is motivated from the observations that the activities related in space and time rarely occur independently and can serve as the context for each other. Given a video, action segments are automatically detected using motion segmentation based on a nonlinear dynamical model. We aim to merge these segments into activities of interest and generate optimum labels for the activities. Towards this goal, we utilize a structural model in a max-margin framework that jointly models the underlying activities which are related in space and time. The model explicitly learns the duration, motion and context patterns for each activity class, as well as the spatio-temporal relationships for groups of them. The learned model is then used to optimally label the activities in the testing videos using a greedy search method. We show promising results on the VIRAT Ground Dataset demonstrating the benefit of joint modeling and recognizing activities in a wide-area scene.</p><p>4 0.77411425 <a title="49-lsi-4" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>Author: Michael S. Ryoo, Larry Matthies</p><p>Abstract: This paper discusses the problem of recognizing interaction-level human activities from a first-person viewpoint. The goal is to enable an observer (e.g., a robot or a wearable camera) to understand ‘what activity others are performing to it’ from continuous video inputs. These include friendly interactions such as ‘a person hugging the observer’ as well as hostile interactions like ‘punching the observer’ or ‘throwing objects to the observer’, whose videos involve a large amount of camera ego-motion caused by physical interactions. The paper investigates multichannel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. In our experiments, we not only show classification results with segmented videos, but also confirm that our new approach is able to detect activities from continuous videos reliably.</p><p>5 0.71678203 <a title="49-lsi-5" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied, we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers, we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components, across other videos in the system, to perform subsequent video analysis. We demonstrate the efficacy of our approach for activity classification and clustering by reporting competitive results on standard datasets such as, HMDB, UCF-50, Olympic Sports and KTH.</p><p>6 0.68077338 <a title="49-lsi-6" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>7 0.65277904 <a title="49-lsi-7" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>8 0.62201005 <a title="49-lsi-8" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>9 0.58203578 <a title="49-lsi-9" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>10 0.56393439 <a title="49-lsi-10" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>11 0.55633545 <a title="49-lsi-11" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>12 0.54995823 <a title="49-lsi-12" href="./cvpr-2013-Bilinear_Programming_for_Human_Activity_Recognition_with_Unknown_MRF_Graphs.html">62 cvpr-2013-Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</a></p>
<p>13 0.52704674 <a title="49-lsi-13" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>14 0.51645893 <a title="49-lsi-14" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>15 0.51147449 <a title="49-lsi-15" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>16 0.50505108 <a title="49-lsi-16" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>17 0.49945447 <a title="49-lsi-17" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>18 0.49796173 <a title="49-lsi-18" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>19 0.45473227 <a title="49-lsi-19" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>20 0.44332397 <a title="49-lsi-20" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.088), (16, 0.022), (19, 0.017), (26, 0.103), (28, 0.015), (33, 0.299), (61, 0.135), (67, 0.09), (69, 0.054), (76, 0.01), (87, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95409685 <a title="49-lda-1" href="./cvpr-2013-As-Projective-As-Possible_Image_Stitching_with_Moving_DLT.html">47 cvpr-2013-As-Projective-As-Possible Image Stitching with Moving DLT</a></p>
<p>Author: Julio Zaragoza, Tat-Jun Chin, Michael S. Brown, David Suter</p><p>Abstract: We investigate projective estimation under model inadequacies, i.e., when the underpinning assumptions oftheprojective model are not fully satisfied by the data. We focus on the task of image stitching which is customarily solved by estimating a projective warp — a model that is justified when the scene is planar or when the views differ purely by rotation. Such conditions are easily violated in practice, and this yields stitching results with ghosting artefacts that necessitate the usage of deghosting algorithms. To this end we propose as-projective-as-possible warps, i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for violations to the assumed imaging conditions. Based on a novel estimation technique called Moving Direct Linear Transformation (Moving DLT), our method seamlessly bridges image regions that are inconsistent with the projective model. The result is highly accurate image stitching, with significantly reduced ghosting effects, thus lowering the dependency on post hoc deghosting.</p><p>2 0.93089461 <a title="49-lda-2" href="./cvpr-2013-A_Linear_Approach_to_Matching_Cuboids_in_RGBD_Images.html">16 cvpr-2013-A Linear Approach to Matching Cuboids in RGBD Images</a></p>
<p>Author: Hao Jiang, Jianxiong Xiao</p><p>Abstract: We propose a novel linear method to match cuboids in indoor scenes using RGBD images from Kinect. Beyond depth maps, these cuboids reveal important structures of a scene. Instead of directly fitting cuboids to 3D data, we first construct cuboid candidates using superpixel pairs on a RGBD image, and then we optimize the configuration of the cuboids to satisfy the global structure constraints. The optimal configuration has low local matching costs, small object intersection and occlusion, and the cuboids tend to project to a large region in the image; the number of cuboids is optimized simultaneously. We formulate the multiple cuboid matching problem as a mixed integer linear program and solve the optimization efficiently with a branch and bound method. The optimization guarantees the global optimal solution. Our experiments on the Kinect RGBD images of a variety of indoor scenes show that our proposed method is efficient, accurate and robust against object appearance variations, occlusions and strong clutter.</p><p>3 0.92429864 <a title="49-lda-3" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>4 0.92118531 <a title="49-lda-4" href="./cvpr-2013-Crossing_the_Line%3A_Crowd_Counting_by_Integer_Programming_with_Local_Features.html">100 cvpr-2013-Crossing the Line: Crowd Counting by Integer Programming with Local Features</a></p>
<p>Author: Zheng Ma, Antoni B. Chan</p><p>Abstract: We propose an integer programming method for estimating the instantaneous count of pedestrians crossing a line of interest in a video sequence. Through a line sampling process, the video is first converted into a temporal slice image. Next, the number of people is estimated in a set of overlapping sliding windows on the temporal slice image, using a regression function that maps from local features to a count. Given that count in a sliding window is the sum of the instantaneous counts in the corresponding time interval, an integer programming method is proposed to recover the number of pedestrians crossing the line of interest in each frame. Integrating over a specific time interval yields the cumulative count of pedestrian crossing the line. Compared with current methods for line counting, our proposed approach achieves state-of-the-art performance on several challenging crowd video datasets.</p><p>same-paper 5 0.92067379 <a title="49-lda-5" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>6 0.92026281 <a title="49-lda-6" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>7 0.91873962 <a title="49-lda-7" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>8 0.91681284 <a title="49-lda-8" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>9 0.91335392 <a title="49-lda-9" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<p>10 0.91277581 <a title="49-lda-10" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>11 0.91218483 <a title="49-lda-11" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>12 0.91206336 <a title="49-lda-12" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>13 0.91043365 <a title="49-lda-13" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>14 0.90875077 <a title="49-lda-14" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>15 0.90681422 <a title="49-lda-15" href="./cvpr-2013-Analyzing_Semantic_Segmentation_Using_Hybrid_Human-Machine_CRFs.html">43 cvpr-2013-Analyzing Semantic Segmentation Using Hybrid Human-Machine CRFs</a></p>
<p>16 0.90613449 <a title="49-lda-16" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>17 0.90579808 <a title="49-lda-17" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>18 0.90552872 <a title="49-lda-18" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>19 0.9054938 <a title="49-lda-19" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>20 0.9036811 <a title="49-lda-20" href="./cvpr-2013-Fast_Multiple-Part_Based_Object_Detection_Using_KD-Ferns.html">167 cvpr-2013-Fast Multiple-Part Based Object Detection Using KD-Ferns</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
