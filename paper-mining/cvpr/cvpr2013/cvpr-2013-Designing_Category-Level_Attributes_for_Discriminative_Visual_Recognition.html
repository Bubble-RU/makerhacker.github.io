<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-116" href="#">cvpr2013-116</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-116-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yu_Designing_Category-Level_Attributes_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>Reference: <a title="cvpr-2013-116-reference" href="../cvpr2013_reference/cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. [sent-7, score-0.466]
</p><p>2 In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. [sent-8, score-0.968]
</p><p>3 The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1. [sent-10, score-1.2]
</p><p>4 Introduction Visual attributes have received renewed attention by the  computer vision community in the past few years. [sent-14, score-0.756]
</p><p>5 , furry, striped, black) that are shared across categories, thereby enabling applications of leveraging knowledge learned from known categories to recognize novel categories, a. [sent-17, score-0.357]
</p><p>6 Designing attributes usually involves manually picking a set of words that are descriptive for the images under consideration, either heuristically [8] or through knowledge bases provided by domain specialists [13]. [sent-38, score-0.856]
</p><p>7 After deciding the set of attributes, additional human efforts are needed to label the attributes, in order to train attribute classifiers. [sent-39, score-0.35]
</p><p>8 More importantly, a manually defined set of attributes (and the corresponding attribute classifiers) may be intuitive but not discriminative for the visual recognition task. [sent-41, score-1.239]
</p><p>9 In this paper, we propose a scalable approach of automatically designing category-level attributes for discriminative visual recognition. [sent-43, score-0.988]
</p><p>10 Our approach is motivated by [13,18], in which the attributes are defined by concise semantics, and then manually related to the categories as a category-attribute matrix (Figure 1). [sent-44, score-1.142]
</p><p>11 This matrix characterizes each category (row) in terms of the pre-defined attributes (columns). [sent-45, score-0.952]
</p><p>12 Similar to characterizing categories as a list of attributes, attributes can also be expressed as how they relate to the known categories. [sent-48, score-1.013]
</p><p>13 For example, we can say the second attribute of Figure 1 characterizes the property that has high association of polar bear, and low association of walrus, lion, etc. [sent-49, score-0.42]
</p><p>14 The designed attributes will not have concise names as the manually specified attributes, but they can be loosely interpreted as relative associations of the known categories. [sent-52, score-1.063]
</p><p>15 This matrix is obtained from human judgments on the “relative strength of association” between attributes and animal categories. [sent-55, score-0.927]
</p><p>16 • We theoretically demonstrate that discriminative  •  •  category-level attributes should have the properties of category-separability and learnability (Section 3. [sent-59, score-0.913]
</p><p>17 Designing Semantic Attributes Traditionally, the semantic attributes are designed by manually picking a set of words that are descriptive for the images under consideration [8,13,16]. [sent-67, score-1.007]
</p><p>18 To alleviate the human burdens, [2] proposes to automatically discover attributes by mining the text and images on the web, and [23] explores the “semantic relatedness” through online knowledge source to relate the attributes to the categories. [sent-70, score-1.608]
</p><p>19 In order to incorporate discriminativeness for the semantic attributes, [6,19] propose to build nameable and discriminative attributes with humanin-the-loop. [sent-71, score-0.965]
</p><p>20 Compared to the above manually designed semantic attributes, our designed attributes cannot be used to  describe images with concise semantic terms, and they may not capture subtle non-discriminative visual patterns of individual images. [sent-72, score-1.265]
</p><p>21 However, the category-level attributes can be automatically and efficiently designed for discriminative visual recognition, leading to effective solutions and even state-of-the-art performance on the tasks that traditionally achieved with semantic attributes. [sent-73, score-1.05]
</p><p>22 Designing Data-Driven Attributes Non-semantic “data-driven attributes” have been explored to complement semantic attributes with various forms. [sent-76, score-0.822]
</p><p>23 [12] combines semantic attributes with “simile classifiers” for face verification. [sent-77, score-0.822]
</p><p>24 [15] extends a set of manually specified attributes with data-driven attributes for improved action recognition. [sent-79, score-1.564]
</p><p>25 [24] extends a semantic attribute representation with extra non-interpretable dimensions for enhanced discrimination. [sent-80, score-0.371]
</p><p>26 [3,10,22] use the large-margin framework to model attributes for objective recognition. [sent-81, score-0.756]
</p><p>27 The category-level attribute definition can be seen as a generalization of the discriminative attributes used in [8]. [sent-84, score-1.161]
</p><p>28 The Framework We propose a framework of using attributes as mid-level cues for multi-class classification on known categories. [sent-89, score-0.842]
</p><p>29 Category Decoding: Choose the closest category (row of A) in the attribute space (column space of A):  − f(x)T  argmiin ? [sent-100, score-0.397]
</p><p>30 Figure 2 illustrates using two attributes to discriminate cats and dogs. [sent-104, score-0.792]
</p><p>31 Designing discriminative category-level attributes is to find a category-attribute matrix A, as well as the attribute classifiers f(·) to minimize the multi-class classification error. [sent-106, score-1.314]
</p><p>32 The above framework is motivated by two previous studies: learning attributes based on category-attribute matrix [13], and Error Correcting Output Code (ECOC) [1,5]. [sent-108, score-0.862]
</p><p>33 For example, imn tthriex previous nst mudoideesl,i nAg was ascect as a manually xdaemfinpeled, matrix [13], a random matrix (discriminative attributes [8]), or a k-dimensional square matrix with diagonal elements as 1and others as −1. [sent-110, score-1.051]
</p><p>34 When applied for recognizing novel categories, such attributes are termed as category-level semantic features, or, classemes [27,3 1]. [sent-112, score-1.045]
</p><p>35 Unlike the manual attributes and classemes, the designed attributes are without concise semantics. [sent-113, score-1.73]
</p><p>36 However the category-level attributes are more intuitive than mid-level representations defined on low-level features, reviewed in Section 2. [sent-114, score-0.786]
</p><p>37 In fact, our attributes can be seen as soft groupings of categories, with analogy to the idea of building taxonomy or concept hierarchy in the library science. [sent-116, score-0.756]
</p><p>38 In addition, by defining attributes based on a set of known categories, we are able to develop a highly efficient algorithm to design the attributes (Section 4). [sent-118, score-1.637]
</p><p>39 Theoretical Analysis In this section, we theoretically show the properties of good attributes in a more explicit form. [sent-123, score-0.781]
</p><p>40 Specifically, we bound the empirical multi-class classification error in terms of attribute encoding error and a property of the categoryattribute matrix, as illustrated in Figure 2. [sent-124, score-0.458]
</p><p>41 as the average encoding error of the attribute classifiers f(·), with respect to the categorytahtteri abtuttreib mutaetr cixla Assi. [sent-127, score-0.407]
</p><p>42 Each category (row of A) is a template vector in the attribute space (column space of A). [sent-143, score-0.397]
</p><p>43 A new image of dog can be represented as an attribute vector through attribute encoding, and ? [sent-145, score-0.61]
</p><p>44 It tells us discriminative attributes should have the following properties, illustrated in Figure 2: • Category-separability. [sent-151, score-0.823]
</p><p>45 In addition, we also want the attributes to be non-redundant, otherwise we may get a large amount of identical attributes. [sent-161, score-0.804]
</p><p>46 The Attribute Design Algorithm Based on the above analysis, we propose an efficient and scalable algorithm to design the category-attribute matrix A, and to learn the attribute classifiers f(·). [sent-168, score-0.53]
</p><p>47 To benefit the algorithm, we set J1(A) as sum of all distances between every two rows of A, encouraging every two categories to be separable in the attribute space. [sent-174, score-0.506]
</p><p>48 The intuition is that if two categories are visually similar, we expect them to share more attributes, otherwise the attribute classifiers will be hard to learn. [sent-186, score-0.555]
</p><p>49 the designed attributes to be strictly orthogonal to each other, i. [sent-191, score-0.868]
</p><p>50 Learning the Attribute Classifiers After getting the real-valued category-attribute matrix A, the next step is to learn the attribute classifiers f(·). [sent-216, score-0.455]
</p><p>51 m  in which the binarized category-attribute matrix element sign(Ayj ,i) defines the presence/non-presence of the ith attribute for xj . [sent-228, score-0.386]
</p><p>52 t Teh visis muaaltr pixro xisi key yto mwaatrridxs making the attributes learnable, and sharable across categories. [sent-235, score-0.756]
</p><p>53 D is built dependent on type of kernel used for learning the attribute classifiers f(·) (Section 4. [sent-238, score-0.399]
</p><p>54 The attribute design algorithm requires no expensive iterations on the image features. [sent-246, score-0.38]
</p><p>55 The computational complexity of designing an attribute (a column of A) is as efficient as finding the eigenvector with the largest eigenvalue of matrix R in Equation 10 (quadratic to # categories). [sent-247, score-0.523]
</p><p>56 5 GHz 2The visual proximity matrix S is only dependent on the kernel type, not the learned attribute classifiers. [sent-249, score-0.499]
</p><p>57 7 7 7 7 7 7 42 242  workstation, it just takes about 1 hour to design 2,000 attributes based on 950 categories on the large-scale ILSVRC2010 dataset (Section 5. [sent-253, score-1.012]
</p><p>58 Though the above algorithm is for designing attributes to discriminate known categories, the application of the designed attributes  is for recognizing novel categories, the categories that are not used in the attribute designing process. [sent-257, score-2.475]
</p><p>59 In specific, we will show by experiment: • The designed attributes are discriminative for novel, yet rTehleate dde categories b(Suetcetsi aonre e5 d. [sent-258, score-1.116]
</p><p>60 • The designed attributes are discriminative for general nTohveel d categories, provided we can design a large a gmenoeurantl of attributes based on a diverse set of known categories (Section 5. [sent-260, score-1.997]
</p><p>61 • The attributes are effective for the task of zero-shot learning (aStetrcitbiuonte 5s. [sent-262, score-0.781]
</p><p>62 We evaluate the performance of the designed attributes on Animal with Attributes (AwA) [13], and ILSVRC2010 datasets3. [sent-266, score-0.868]
</p><p>63 Associated with the images, there is a manually designed category-attribute matrix of 85 attributes shown in Figure 1. [sent-268, score-1.001]
</p><p>64 We first demonstrate that our designed categorylevel attributes are more discriminative than other categorylevel representations. [sent-273, score-1.061]
</p><p>65 2 −, we use the extracted attributes as features to perform classification on the images of novel categories. [sent-277, score-0.842]
</p><p>66 Our approach is compared with the manual attributes, random attributes, classemes [27] (one-vs-all classifiers learned on the known categories), and low-level features (one-vs-all classification scheme based on low-level features of the novel categories). [sent-278, score-0.359]
</p><p>67 The attributes are designed based on 40 training categories defined in [13]. [sent-306, score-1.072]
</p><p>68 • The designed attributes perform significantly better than tThhee mdeasnigunale a atttrtriibbuutetess apnerdf rrmand soigmn category-level aatntributes (Figure 3 left). [sent-331, score-0.868]
</p><p>69 • The designed attributes is competitive to, if not bettTehr eth daens gthnee dlo awtt-rliebvuetel sfe iastu croesm paired w toit,h fon neo-vts b-aeltlχ2 classifiers (Figure 3 right). [sent-332, score-0.937]
</p><p>70 The designed attributes significantly outperform the the one-vs-all classifiers (known as classemes) for the task of recognizing novel  categories (Section 5. [sent-333, score-1.201]
</p><p>71 • The designed attributes have smaller encoding error, larger row separation atensd hsamvaelle srm redundancy. [sent-335, score-0.937]
</p><p>72 The random attributes therefore outperform the manual attributes (Figure 3 left). [sent-339, score-1.546]
</p><p>73 Discriminating Novel Categories We show that the designed attributes are also discriminative for novel categories. [sent-342, score-0.985]
</p><p>74 Efficient linear SVM classifiers are trained based on different kinds of attribute features to perform classification on the images of 10 novel categories. [sent-347, score-0.46]
</p><p>75 This means that attributes are discriminative representation for the novel categories, by leveraging knowledge learned from known categories. [sent-368, score-0.979]
</p><p>76 Note that the manual attributes and classemes  are with fixed dimensions, not extendable due to definition, whereas the dimension of the designed category-level attributes is scalable. [sent-370, score-1.778]
</p><p>77 In the previous experiments on AwA, we show that the designed attributes are discriminative for novel, yet related categories. [sent-372, score-0.935]
</p><p>78 We now demonstrate that the designed attributes can be discriminative for general novel categories, provided that we can design a large amount of attributes based on a diverse set of known categories. [sent-373, score-1.892]
</p><p>79 950 categories are used as known categories to design attributes. [sent-376, score-0.487]
</p><p>80 We test the performance of using attribute features for category-level retrieval and classification on the remaining 50 disjoint categories. [sent-377, score-0.364]
</p><p>81 The attributes are trained by linear weighted SVM models. [sent-381, score-0.756]
</p><p>82 We first test the performance of the designed attributes 7 7 7 7 7 7 64 464  for category-level image retrieval. [sent-383, score-0.868]
</p><p>83 The designed attributes outperform low-level features and classemes, even with 500 dimensions. [sent-386, score-0.868]
</p><p>84 Next, we use the attribute feature, combined with linear SVM classifiers to classify the images of the 50 novel categories. [sent-390, score-0.424]
</p><p>85 Similar to the experiments on AwA dataset, attribute representation outperforms the baselines, especially when training with small amount of examples. [sent-393, score-0.354]
</p><p>86 It means attributes are effective for leveraging information of the known categories to recognize novel categories. [sent-394, score-1.086]
</p><p>87 In such case, human knowledge [13,18] is required to build a new categoryattribute matrix ∈ Rp×l, to relate the p novel categories to the l designed at ∈tri Rbutes. [sent-400, score-0.584]
</p><p>88 However, for each designed attribute in our approach, there  A˜  is no guarantee that it possesses a coherent human interpretation. [sent-403, score-0.44]
</p><p>89 Motivated by the fact that the visual proximity matrix S in Equation 7 is central to the attribute design process, we propose a fairly straightforward solution: similar to [29], given each novel category, and k known categories, we ask the user to find the top-M visually similar categories. [sent-408, score-0.674]
</p><p>90 The novel categories are related to the designed attributes by the simple weighted sum:  S˜  S˜ij  A˜ = S˜A  (12)  The amount of human interaction is minimal for the above approach, independent on the number of attributes. [sent-411, score-1.148]
</p><p>91 animal categories for training and 10 categories for testing). [sent-414, score-0.452]
</p><p>92 Empirically, fewer categories cannot fully characterize the visual appearance of the animals, and more categories will lead to more human burdens. [sent-416, score-0.414]
</p><p>93 In the experiments above, the attributes are designed to be discriminative for the known categorizes. [sent-430, score-0.985]
</p><p>94 As a refinement for zero-shot learning, we can modify the algorithm to design attributes adaptively for discriminating the novel categories. [sent-431, score-0.932]
</p><p>95 The last two rows of Table 4 demonstrate the performance of adaptive attribute design. [sent-436, score-0.325]
</p><p>96 The  drawback for the adaptive attribute design is that we need to redesign the attributes for different tasks. [sent-439, score-1.136]
</p><p>97 Because the proposed attribute design algorithm is highly efficient, the drawback can be alleviated. [sent-440, score-0.38]
</p><p>98 Such attributes can be effectively used for tasks of cross-category knowledge transfer. [sent-443, score-0.783]
</p><p>99 A joint learning framework for attribute models and object descriptions. [sent-552, score-0.33]
</p><p>100 Additional remarks on designing category-level attributes for discriminative visual recognition. [sent-662, score-0.968]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attributes', 0.756), ('attribute', 0.305), ('categories', 0.181), ('awa', 0.13), ('classemes', 0.12), ('designing', 0.116), ('designed', 0.112), ('category', 0.092), ('proximity', 0.084), ('bracket', 0.084), ('categoryattribute', 0.084), ('matrix', 0.081), ('design', 0.075), ('concise', 0.072), ('classifiers', 0.069), ('animal', 0.067), ('discriminative', 0.067), ('semantic', 0.066), ('learnability', 0.065), ('categorylevel', 0.063), ('aat', 0.054), ('ata', 0.052), ('manually', 0.052), ('discriminating', 0.051), ('animals', 0.051), ('novel', 0.05), ('known', 0.05), ('columbia', 0.047), ('striped', 0.046), ('feris', 0.043), ('atqa', 0.042), ('maax', 0.042), ('sij', 0.042), ('nameable', 0.042), ('separation', 0.036), ('cats', 0.036), ('classification', 0.036), ('torresani', 0.035), ('discriminativeness', 0.034), ('manual', 0.034), ('recognizing', 0.033), ('equation', 0.033), ('encoding', 0.033), ('definition', 0.033), ('published', 0.032), ('bandwidth', 0.032), ('ontology', 0.031), ('eil', 0.031), ('kennedy', 0.031), ('aj', 0.031), ('intuitive', 0.03), ('rk', 0.03), ('farhadi', 0.03), ('relatedness', 0.03), ('leveraging', 0.029), ('visual', 0.029), ('smith', 0.028), ('semantics', 0.028), ('knowledge', 0.027), ('relate', 0.026), ('amount', 0.026), ('bear', 0.026), ('polar', 0.026), ('iarpa', 0.026), ('simile', 0.026), ('tuned', 0.025), ('learning', 0.025), ('properties', 0.025), ('margins', 0.024), ('concepts', 0.024), ('principled', 0.024), ('human', 0.023), ('training', 0.023), ('retrieval', 0.023), ('characterizes', 0.023), ('tsai', 0.023), ('dogs', 0.023), ('efforts', 0.022), ('ai', 0.022), ('say', 0.022), ('tthhee', 0.022), ('want', 0.022), ('association', 0.022), ('induces', 0.022), ('settings', 0.022), ('svm', 0.021), ('berg', 0.021), ('eigenvector', 0.021), ('names', 0.021), ('descriptive', 0.021), ('endres', 0.02), ('redundancy', 0.02), ('perronnin', 0.02), ('termed', 0.02), ('deviation', 0.02), ('automatically', 0.02), ('centers', 0.02), ('yu', 0.02), ('rows', 0.02), ('recognize', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="116-tfidf-1" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>2 0.4639644 <a title="116-tfidf-2" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>3 0.45530802 <a title="116-tfidf-3" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>4 0.43792245 <a title="116-tfidf-4" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>5 0.41533962 <a title="116-tfidf-5" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>Author: Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</p><p>Abstract: Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently, and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query, which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions without explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discrimina- tive binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improveperformance significantly over several strong baselines, while being an order of magnitude faster than exhaustively searching through all possible conjunctions.</p><p>6 0.36487374 <a title="116-tfidf-6" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>7 0.33102846 <a title="116-tfidf-7" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>8 0.31327307 <a title="116-tfidf-8" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>9 0.26557559 <a title="116-tfidf-9" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>10 0.26345918 <a title="116-tfidf-10" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>11 0.25749663 <a title="116-tfidf-11" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>12 0.24527211 <a title="116-tfidf-12" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>13 0.21883543 <a title="116-tfidf-13" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>14 0.21410008 <a title="116-tfidf-14" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>15 0.18814695 <a title="116-tfidf-15" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>16 0.16578723 <a title="116-tfidf-16" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>17 0.14134277 <a title="116-tfidf-17" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>18 0.1381623 <a title="116-tfidf-18" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>19 0.13704009 <a title="116-tfidf-19" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>20 0.11415735 <a title="116-tfidf-20" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.193), (1, -0.203), (2, -0.074), (3, -0.052), (4, 0.171), (5, 0.188), (6, -0.478), (7, 0.1), (8, 0.144), (9, 0.345), (10, -0.082), (11, 0.157), (12, -0.099), (13, -0.006), (14, 0.097), (15, 0.048), (16, -0.072), (17, -0.05), (18, -0.03), (19, 0.159), (20, 0.006), (21, 0.01), (22, -0.004), (23, 0.004), (24, 0.023), (25, 0.004), (26, -0.051), (27, 0.024), (28, -0.009), (29, 0.018), (30, 0.035), (31, 0.017), (32, 0.049), (33, -0.044), (34, -0.017), (35, 0.022), (36, 0.007), (37, 0.025), (38, 0.018), (39, 0.036), (40, 0.017), (41, -0.012), (42, 0.035), (43, -0.003), (44, 0.012), (45, 0.004), (46, -0.015), (47, -0.014), (48, -0.013), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97864902 <a title="116-lsi-1" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>2 0.90456575 <a title="116-lsi-2" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>3 0.90011555 <a title="116-lsi-3" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>4 0.88904613 <a title="116-lsi-4" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>5 0.88266093 <a title="116-lsi-5" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>6 0.86089164 <a title="116-lsi-6" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>7 0.83091635 <a title="116-lsi-7" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>8 0.79041582 <a title="116-lsi-8" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>9 0.75870192 <a title="116-lsi-9" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>10 0.74879932 <a title="116-lsi-10" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>11 0.72122401 <a title="116-lsi-11" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>12 0.67052984 <a title="116-lsi-12" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>13 0.60936028 <a title="116-lsi-13" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>14 0.55230772 <a title="116-lsi-14" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>15 0.51005387 <a title="116-lsi-15" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>16 0.48601618 <a title="116-lsi-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.4400484 <a title="116-lsi-17" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>18 0.41312906 <a title="116-lsi-18" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>19 0.37738723 <a title="116-lsi-19" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>20 0.3574523 <a title="116-lsi-20" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.102), (16, 0.033), (26, 0.031), (27, 0.01), (28, 0.013), (33, 0.315), (36, 0.018), (45, 0.015), (67, 0.071), (69, 0.084), (77, 0.022), (80, 0.015), (87, 0.048), (95, 0.143)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9345575 <a title="116-lda-1" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>Author: Ran Margolin, Ayellet Tal, Lihi Zelnik-Manor</p><p>Abstract: What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns, others on the colors, and several add high-level cues and priors. We propose a simple, yet powerful, algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.</p><p>2 0.93356878 <a title="116-lda-2" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>Author: Weixin Li, Qian Yu, Harpreet Sawhney, Nuno Vasconcelos</p><p>Abstract: In this work, we propose a novel video representation for activity recognition that models video dynamics with attributes of activities. A video sequence is decomposed into short-term segments, which are characterized by the dynamics of their attributes. These segments are modeled by a dictionary of attribute dynamics templates, which are implemented by a recently introduced generative model, the binary dynamic system (BDS). We propose methods for learning a dictionary of BDSs from a training corpus, and for quantizing attribute sequences extracted from videos into these BDS codewords. This procedure produces a representation of the video as a histogram of BDS codewords, which is denoted the bag-of-words for attribute dynamics (BoWAD). An extensive experimental evaluation reveals that this representation outperforms other state-of-the-art approaches in temporal structure modeling for complex ac- tivity recognition.</p><p>same-paper 3 0.9286769 <a title="116-lda-3" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>4 0.91635406 <a title="116-lda-4" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>Author: Xin Guo, Dong Liu, Brendan Jou, Mojun Zhu, Anni Cai, Shih-Fu Chang</p><p>Abstract: Object co-detection aims at simultaneous detection of objects of the same category from a pool of related images by exploiting consistent visual patterns present in candidate objects in the images. The related image set may contain a mixture of annotated objects and candidate objects generated by automatic detectors. Co-detection differs from the conventional object detection paradigm in which detection over each test image is determined one-by-one independently without taking advantage of common patterns in the data pool. In this paper, we propose a novel, robust approach to dramatically enhance co-detection by extracting a shared low-rank representation of the object instances in multiple feature spaces. The idea is analogous to that of the well-known Robust PCA [28], but has not been explored in object co-detection so far. The representation is based on a linear reconstruction over the entire data set and the low-rank approach enables effective removal of noisy and outlier samples. The extracted low-rank representation can be used to detect the target objects by spectral clustering. Extensive experiments over diverse benchmark datasets demonstrate consistent and significant performance gains of the proposed method over the state-of-the-art object codetection method and the generic object detection methods without co-detection formulations.</p><p>5 0.91560555 <a title="116-lda-5" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>Author: Sanja Fidler, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun</p><p>Abstract: In this paper we are interested in how semantic segmentation can help object detection. Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional HOG filters as well as a set of novel segmentation features. Thus our model “blends ” between the detector and segmentation models. Since our features can be computed very efficiently given the segments, we maintain the same complexity as the original DPM [14]. We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector [12] on all classes, achieving 13% higher average AP. When employing the parts, we outperform the original DPM [14] in 19 out of 20 classes, achieving an improvement of 8% AP. Furthermore, we outperform the previous state-of-the-art on VOC’10 test by 4%.</p><p>6 0.9144125 <a title="116-lda-6" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>7 0.91419512 <a title="116-lda-7" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>8 0.91352159 <a title="116-lda-8" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>9 0.91335243 <a title="116-lda-9" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>10 0.91255909 <a title="116-lda-10" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>11 0.91125268 <a title="116-lda-11" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>12 0.91113693 <a title="116-lda-12" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>13 0.9107641 <a title="116-lda-13" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>14 0.910662 <a title="116-lda-14" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>15 0.91037357 <a title="116-lda-15" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>16 0.9103477 <a title="116-lda-16" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>17 0.9103052 <a title="116-lda-17" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>18 0.91027951 <a title="116-lda-18" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>19 0.90966499 <a title="116-lda-19" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>20 0.90942317 <a title="116-lda-20" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
