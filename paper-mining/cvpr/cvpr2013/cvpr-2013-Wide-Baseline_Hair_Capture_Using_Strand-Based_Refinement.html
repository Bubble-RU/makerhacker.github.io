<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-467" href="#">cvpr2013-467</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</h1>
<br/><p>Source: <a title="cvpr-2013-467-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Luo_Wide-Baseline_Hair_Capture_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>Reference: <a title="cvpr-2013-467-reference" href="../cvpr2013_reference/cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our result is robust to the wide-baseline setup and reveals detailed hair structures. [sent-2, score-0.685]
</p><p>2 The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. [sent-5, score-1.282]
</p><p>3 The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. [sent-6, score-1.661]
</p><p>4 Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. [sent-7, score-1.398]
</p><p>5 We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll-  as hair in motion with as few as 8 cameras. [sent-8, score-0.747]
</p><p>6 However, hair reconstruction remains one of the most challenging tasks due to many unique hair characteristics. [sent-10, score-1.381]
</p><p>7 For instance, omni-present occlusions and complex strand geometry preclude general surface-based smoothness priors [26] for hair reconstruction. [sent-11, score-1.219]
</p><p>8 The highly specular nature of hair [17] also violates the Lambertian surface assumption employed in most multi-view stereo methods. [sent-12, score-0.749]
</p><p>9 Consequently, many practical systems have either completely avoided hair reconstruction during facial capture (e. [sent-13, score-0.805]
</p><p>10 Researchers have explored specialized hardware to facilitate hair capture, such as a fixed camera with moving light sources [21], a stage-mounted camera with macro lens [8], thermal imaging [12], etc. [sent-16, score-0.795]
</p><p>11 In this work, we study hair capture with a wide-baseline  camera setup. [sent-21, score-0.734]
</p><p>12 Merely 8 cameras are used to capture the complete hair geometry, with each adjacent pair of cameras having a large 45-degree wide angular baseline. [sent-22, score-0.788]
</p><p>13 Instead, we propose that 3D strand is a better “aggregation unit” for stereo matching in hair reconstruction because it models hair’s characteristic “strand-like” structural continuity and thus yields improved robustness against matching ambiguities in wide-baseline setups. [sent-24, score-1.236]
</p><p>14 The 3D strands are first generated separately from a 2D strand extraction step in each view and then jointly optimized in a strand-based refinement step. [sent-25, score-0.97]
</p><p>15 We also introduce a novel formulation of smoothness energy that regularizes the optimization at the strand, wisp and global levels to better account for real hair dynamics, hair wisp structures and 222666555  Figure 2: The overview of our reconstruction method. [sent-26, score-1.927]
</p><p>16 Using the visual hull constructed from the segmented images, we extract strands on the orientation maps and project them from each view onto the visual hull for strand initialization. [sent-28, score-1.402]
</p><p>17 Finally we perform strand-based refinement to obtain the final strand positions. [sent-29, score-0.544]
</p><p>18 The hair surface can then be reconstructed from the refined strands using [10]. [sent-30, score-1.137]
</p><p>19 2 Related Work In this section, we review existing technologies for hair capture, including those using dedicated setups and dense camera arrays. [sent-35, score-0.741]
</p><p>20 1 Hair Capture A few dedicated systems in the literature have been designed for hair capture. [sent-38, score-0.668]
</p><p>21 [20] proposed to estimate the hair orientation in images and analyze the highlights on the hair. [sent-40, score-0.77]
</p><p>22 [21] presented Hair Photobooth, a complex system made of several light sources, projectors, and video cameras that capture a rich set of data to extract the hair geometry and appearance. [sent-43, score-0.78]
</p><p>23 [8] showed how to capture individual hair strands using focal sweeps with a macro-lens equipped camera controlled by a robotic gantry. [sent-45, score-1.125]
</p><p>24 Recently, thermal imaging has been applied for hair reconstruction to avoid shadowing and anisotropic reflectance [12]. [sent-46, score-0.763]
</p><p>25 Work has also been done to capture hair with more flexible setups. [sent-48, score-0.701]
</p><p>26 Their approach uses a coarse visual hull as the approximate bounding geometry for hair growing constrained with orientation consistency. [sent-51, score-0.982]
</p><p>27 [27] used an array of 12 cameras to capture partial geometry of straight hair in moderate motion. [sent-53, score-0.782]
</p><p>28 [2] used a high resolution dense camera array to reconstruct facial hair strand geometry by matching distinctive strands. [sent-59, score-1.256]
</p><p>29 In contrast with these approaches, our method is capable of reconstructing accurate hair geometry from a wide-baseline sparse camera array. [sent-60, score-0.715]
</p><p>30 And the consistency is measured in novel ways in order to handle the wide-baseline and challenging hair characteristics. [sent-66, score-0.671]
</p><p>31 Inspired by the active contour method [9], many reconstruction methods iteratively refine a rough initial  shape (usually the visual hull) to obtain the final reconstruction by optimizing cross-view photo-consistency and surface smoothness. [sent-68, score-0.232]
</p><p>32 For instance, Hernandez and Schmitt [5] proposed a visual hull refinement method by iteratively minimizing the texture, silhouette and surface smoothness energies. [sent-69, score-0.464]
</p><p>33 Furukawa and Ponce [6] segmented the initial visual hull into surface areas between the rims and refined each via graph cuts. [sent-70, score-0.274]
</p><p>34 [23] extended the idea of SIFT to find dense correspondences 222666666  Figure 3: Our hair capture setup and a few sample images. [sent-74, score-0.749]
</p><p>35 We use 8 cameras (outlined in red) in wide-baseline to capture the complete hair styles. [sent-75, score-0.754]
</p><p>36 However, due to the lack of reliable texture and corner-like features on hair, it is difficult to apply these methods on hair reconstruction. [sent-79, score-0.652]
</p><p>37 3 for some sample images), our goal is to compute a shape that best approximates the captured hair volume. [sent-81, score-0.652]
</p><p>38 We achieve this by refining the positions of a dense set of representative 3D hair strands derived from each camera view. [sent-82, score-1.106]
</p><p>39 2 gives an overview of the various steps involved in our hair capture algorithm. [sent-84, score-0.701]
</p><p>40 To create the initial 3D strands for refinement, we first compute the hair orientation map for  each input image, and extract the 2D strands by tracking the confident ridges on the orientation map. [sent-87, score-1.71]
</p><p>41 The 2D strands are then back-projected onto the visual hull constructed from the segmented foreground of all input images to form the initial 3D strands. [sent-88, score-0.612]
</p><p>42 An iterative strand refinement algorithm is then applied to optimize the orientation consistency of the projected strands on all the orientation maps. [sent-89, score-1.224]
</p><p>43 The final hair shape is obtained using Poisson surface reconstruction [10] from the refined 3D strands. [sent-91, score-0.805]
</p><p>44 In the rest of the paper, we will describe strand initialization in Sec. [sent-92, score-0.469]
</p><p>45 (a)(b)(c) Figure 4: The steps of strand initialization in Sec. [sent-95, score-0.469]
</p><p>46 Then we extract strands on the orientation map and project them onto the visual hull for initialization (c). [sent-98, score-0.746]
</p><p>47 4 Strand initialization We first compute an orientation map for each image using the method proposed in [15], which uses a bank of rotated filters to detect the dominant orientation at each pixel. [sent-99, score-0.268]
</p><p>48 The orientation map is enhanced with 3 passes of iterative refinement to improve the signal-to-noise ratio as in [4]. [sent-100, score-0.225]
</p><p>49 The result is a set of poly-line 2D strands consisting of densely sampled vertices in about 1-pixel steps. [sent-104, score-0.432]
</p><p>50 We backproject each vertex of the resulting 2D strands onto the visual hull to determine the initial position of the 3D strands, as shown in Fig. [sent-105, score-0.645]
</p><p>51 Note that the 3D strands are generally over-sampled after back-projection from 2D strands. [sent-107, score-0.391]
</p><p>52 Thus we down-sample each 3D strand by uniformly decimating the vertices to 20% of the original vertex count in order to reduce the computation cost in the following steps. [sent-108, score-0.545]
</p><p>53 The total energy is defined as the weighted sum of a few specific energies, such as orientation energy, silhouette energy and smoothness energy: E = ∑α? [sent-112, score-0.43]
</p><p>54 5), the strands (first row) are refined over the iterations with their reconstructed surfaces (second row) revealing more hair details. [sent-138, score-1.092]
</p><p>55 1  Notations and Definitions  Let p denote a strand vertex on a 3D strand S. [sent-140, score-0.959]
</p><p>56 The strand direction d(p) at p is defined as p+1 − p−1 . [sent-144, score-0.455]
</p><p>57 Since strand visibility is difficult to define exactly during strand refinement, we approximate V(p) by the visibility of its closest point h(p) on the visual hull H during the refinement. [sent-146, score-1.144]
</p><p>58 Vertices on the same strand as p are excluded from N+ (p). [sent-151, score-0.455]
</p><p>59 (2)  where σe controls the influence radius around the strand vertices and is set to 0. [sent-158, score-0.496]
</p><p>60 The different-view neighbor q is located on the strands from a different reference view (in blue). [sent-164, score-0.46]
</p><p>61 (3)  where σo controls the influence between strand vertices with similar orientations and is set to 0. [sent-171, score-0.515]
</p><p>62 We also define a “surface” normal n(p) at each strand vertex p, which can be computed by finding the eigenvector with the smallest eigenvalue of the covariance matrix  w+(p, q)(q − p)(q − p)? [sent-200, score-0.504]
</p><p>63 2  Orientation Energy  The orientation energy Eorient is designed to make sure that when a 3D strand is projected onto its visible views, the projected orientations are consistent with those indicated by the orientation maps of those views. [sent-207, score-0.898]
</p><p>64 4 to the orientation map OV of view V, an orientation vector OV (pV) is defined at any point pV in the hair region , otherwise we set OV (pV) = 0 (Fig. [sent-209, score-0.941]
</p><p>65 We then define an orientation energy term eoVrient (pV) for each segment (p, p+1 ) on 222666888  pVpV+0. [sent-211, score-0.2]
</p><p>66 A strand is projected on the orientation map in similar color coding as in Fig. [sent-214, score-0.625]
</p><p>67 3 Silhouette Energy We also enforce the 3D strands to be within and near the visual hull H using silhouette energy. [sent-235, score-0.639]
</p><p>68 w1out ( p − − h ( p ) ) · n h >≤ 0 ,  (7)  where wout is a large penalty (104) against the case where the vertex is outside the visual hull H. [sent-242, score-0.231]
</p><p>69 4  Smoothness Energy  Smoothness energy is formulated at three different levels to better control the smoothness granularity: the strand level, the wisp level and the global level. [sent-246, score-0.81]
</p><p>70 The formulation for strand level smoothness Estrand stems from the discrete elastic rod model [3] often used in hair simulation that minimizes the squared curvature along hair strands. [sent-247, score-1.916]
</p><p>71 Further inspired by [4], we take into account the orientation similarity in the bilateral same-view weight so that the  w+  poutnhh(p)Hpinh(p)nh  Figure 8: The illustration of silhouette energy. [sent-248, score-0.227]
</p><p>72 wisp smoothness energy Ewisp can better adapt to the local wisp structures and hair’s depth discontinuities. [sent-250, score-0.546]
</p><p>73 Finally, the global smoothness energy Eglobal ensures the global consistency of strand geometry across different views. [sent-251, score-0.668]
</p><p>74 Strand smoothness energy Inspired by [3], we define the strand smoothness energy as the summation of squared curvature for each vertex along all the strands: Estrand = D2 ∑curv2(p) p where curvature is computed as:  (8)  ×  curv(p) =l+1+2 l−1? [sent-252, score-0.887]
</p><p>75 Wisp smoothness energy We use wisp smoothness energy to enforce a strand vertex and its small same-view neighborhood N+ (p) within the same wisp to lie on a local plane. [sent-268, score-1.233]
</p><p>76 We use the orientation similarity to estimate the likelihood of being in the same wisp and encode it in the same-view weight The wisp smoothness energy is thus defined as:  w+. [sent-269, score-0.686]
</p><p>77 2  (10)  Global smoothness energy Finally, the global smoothness energy is defined similarly to the wisp smoothness energy to enforce global refinement consistency through local planar resemblance across different views:  Eglobal=D12∑p? [sent-274, score-0.791]
</p><p>78 Fewer camera views will push the reconstruction quality towards visual hull due to smaller overlap between views. [sent-284, score-0.33]
</p><p>79 We also use the hair datasets from [13], but 222666999  ×  Figure 9: We evaluate the reconstruction accuracy on three synthetic hair styles (straight, wavy and wavythin, first row). [sent-285, score-1.475]
</p><p>80 Note that a relatively small αsilh is used to de-emphasize the importance of the visual hull on the reconstruction once the shape is inside the visual hull: αorient = 2 10−2 αstrand = 10−4 αglobal = 0. [sent-291, score-0.278]
</p><p>81 Note that we use [7] to reconstruct each subject’s facial area and then merge our hair reconstruction using Poisson surface reconstruction [10]. [sent-295, score-0.922]
</p><p>82 Our method can accurately reconstruct a variety of hair styles from short to long, from smooth to messy and from unconstrained to  constrained. [sent-296, score-0.742]
</p><p>83 Also, our method is able to faithfully reveal interesting hair structures like wisps and curls. [sent-297, score-0.652]
</p><p>84 In contrast, general visual hull refinement on color texture [5] loses details (Fig. [sent-298, score-0.271]
</p><p>85 Quantitative evaluation To quantitatively evaluate our method, we use the hair models by [28] and render them using the hair appearance model in [17], as shown in Fig. [sent-302, score-1.304]
</p><p>86 Figure10:Sampleframes(firt ow)andtherconstruced  surfaces (second row) from the dynamic hair capture setup. [sent-304, score-0.701]
</p><p>87 The three hair models (straight, wavy, wavythin) in the evaluation are representative for a variety of common hair types. [sent-305, score-1.304]
</p><p>88 Using the rendered images from viewpoints similar to our real capture setup, we are able to reconstruct the surface for the synthetic hair models. [sent-306, score-0.827]
</p><p>89 Since hair is volumetric, average closest point distance is not a good error measure. [sent-307, score-0.652]
</p><p>90 We therefore evaluate the reconstruction accuracy by comparing the depth maps of the hair model and the reconstructed surface on a specific view and visualize the differences in coded color (Fig. [sent-308, score-0.841]
</p><p>91 Dynamic hair capture Compared to previous methods [21, 15], our method is able to capture complete moving hair with only 8 cameras. [sent-312, score-1.421]
</p><p>92 7 Conclusion and Future Work We have proposed a novel algorithm to reconstruct complete full-head hair styles with strand-based refinement us-  ing only 8 views. [sent-320, score-0.85]
</p><p>93 Compared to previous methods, our method is able to capture hair accurately with faithful hair structures even with a wide baseline setup. [sent-321, score-1.353]
</p><p>94 The reconstruction results are evaluated on a set of synthetic hair models and achieve ∼3mm reconstruction error on average. [sent-322, score-0.825]
</p><p>95 The falnexdib alceh requirement f roerc input acltlioowns e us otor capture complete hair in motion with an inexpensive camera rig. [sent-323, score-0.753]
</p><p>96 The strand-based refinement relies on reasonably long strands to provide good regularization in the optimization. [sent-325, score-0.48]
</p><p>97 For certain extreme hair styles, like very short hair and fluffy hair, long continuous strands are scarce, which can adversely affect our reconstruction result. [sent-326, score-1.772]
</p><p>98 Also, because segmentation of hairy objects is still a very challenging problem in compute vision, the visual hull we used to reconstruct the hair is often too smooth, which causes our method to easily miss interesting stray hairs in the reconstruction. [sent-327, score-0.904]
</p><p>99 Coupled 3d reconstruction of sparse facial hair and skin. [sent-349, score-0.756]
</p><p>100 Lighting hair from the inside: A thermal approach to hair reconstruction. [sent-429, score-1.338]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hair', 0.652), ('strand', 0.455), ('strands', 0.391), ('wisp', 0.191), ('hull', 0.163), ('orientation', 0.118), ('refinement', 0.089), ('pv', 0.084), ('smoothness', 0.082), ('energy', 0.082), ('reconstruction', 0.077), ('silhouette', 0.066), ('eovrient', 0.059), ('stereo', 0.052), ('capture', 0.049), ('vertex', 0.049), ('styles', 0.046), ('ov', 0.045), ('surface', 0.045), ('eorient', 0.044), ('hairstyles', 0.044), ('reconstruct', 0.044), ('vertices', 0.041), ('beeler', 0.039), ('views', 0.038), ('view', 0.035), ('thermal', 0.034), ('paris', 0.034), ('projected', 0.034), ('cameras', 0.034), ('reference', 0.034), ('setup', 0.033), ('camera', 0.033), ('nh', 0.032), ('refined', 0.031), ('geometry', 0.03), ('eglobal', 0.029), ('esilh', 0.029), ('estrand', 0.029), ('ewisp', 0.029), ('photobooth', 0.029), ('torient', 0.029), ('wavy', 0.029), ('wavythin', 0.029), ('facial', 0.027), ('furukawa', 0.026), ('silh', 0.026), ('beardsley', 0.026), ('hairs', 0.026), ('affinely', 0.026), ('august', 0.026), ('visibility', 0.026), ('setups', 0.025), ('bickel', 0.024), ('marschner', 0.024), ('jakob', 0.024), ('onto', 0.023), ('tola', 0.023), ('sumner', 0.023), ('acm', 0.023), ('simulation', 0.023), ('ridges', 0.022), ('poisson', 0.022), ('weight', 0.022), ('bilateral', 0.021), ('extremal', 0.02), ('wv', 0.02), ('curvature', 0.02), ('pw', 0.019), ('dynamics', 0.019), ('synthetic', 0.019), ('neighborhood', 0.019), ('consistency', 0.019), ('visual', 0.019), ('acquisition', 0.019), ('july', 0.019), ('complete', 0.019), ('orientations', 0.019), ('princeton', 0.018), ('reconstructed', 0.018), ('multiview', 0.018), ('yamaguchi', 0.018), ('map', 0.018), ('viewpoints', 0.018), ('chai', 0.017), ('straight', 0.017), ('elastic', 0.017), ('dedicated', 0.016), ('segmented', 0.016), ('light', 0.015), ('dense', 0.015), ('refining', 0.015), ('squared', 0.015), ('visible', 0.015), ('hardware', 0.014), ('initialization', 0.014), ('optimizing', 0.014), ('coded', 0.014), ('specialized', 0.014), ('outlined', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="467-tfidf-1" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>2 0.312565 <a title="467-tfidf-2" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>3 0.19637004 <a title="467-tfidf-3" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>4 0.067561522 <a title="467-tfidf-4" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>5 0.06489791 <a title="467-tfidf-5" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>Author: Zhenglong Zhou, Zhe Wu, Ping Tan</p><p>Abstract: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple, which consists of only a digital camera and a handheld light source. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distributionfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments, the captured shapes are accurate to 0.3 millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9%.</p><p>6 0.063035011 <a title="467-tfidf-6" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>7 0.058597252 <a title="467-tfidf-7" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<p>8 0.057580557 <a title="467-tfidf-8" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>9 0.056708872 <a title="467-tfidf-9" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>10 0.055947624 <a title="467-tfidf-10" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>11 0.054248095 <a title="467-tfidf-11" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>12 0.051940549 <a title="467-tfidf-12" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>13 0.050854277 <a title="467-tfidf-13" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>14 0.04832508 <a title="467-tfidf-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.047510814 <a title="467-tfidf-15" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>16 0.046685044 <a title="467-tfidf-16" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>17 0.046168879 <a title="467-tfidf-17" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>18 0.044533651 <a title="467-tfidf-18" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>19 0.044458807 <a title="467-tfidf-19" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>20 0.04347609 <a title="467-tfidf-20" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.103), (1, 0.082), (2, 0.004), (3, 0.02), (4, 0.023), (5, -0.033), (6, -0.031), (7, 0.01), (8, 0.062), (9, -0.031), (10, 0.035), (11, -0.013), (12, 0.017), (13, 0.033), (14, 0.031), (15, 0.041), (16, 0.006), (17, 0.034), (18, 0.038), (19, 0.072), (20, 0.009), (21, -0.067), (22, 0.021), (23, -0.006), (24, -0.041), (25, -0.059), (26, 0.105), (27, -0.047), (28, 0.012), (29, 0.009), (30, 0.019), (31, 0.038), (32, -0.024), (33, 0.089), (34, 0.054), (35, -0.05), (36, 0.01), (37, -0.078), (38, -0.049), (39, -0.048), (40, 0.129), (41, -0.027), (42, 0.042), (43, -0.021), (44, 0.075), (45, -0.09), (46, 0.052), (47, 0.007), (48, -0.091), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91408437 <a title="467-lsi-1" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>2 0.64295095 <a title="467-lsi-2" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<p>Author: Andrew Kae, Kihyuk Sohn, Honglak Lee, Erik Learned-Miller</p><p>Abstract: Conditional random fields (CRFs) provide powerful tools for building models to label image segments. They are particularly well-suited to modeling local interactions among adjacent regions (e.g., superpixels). However, CRFs are limited in dealing with complex, global (long-range) interactions between regions. Complementary to this, restricted Boltzmann machines (RBMs) can be used to model global shapes produced by segmentation models. In this work, we present a new model that uses the combined power of these two network types to build a state-of-the-art labeler. Although the CRF is a good baseline labeler, we show how an RBM can be added to the architecture to provide a global shape bias that complements the local modeling provided by the CRF. We demonstrate its labeling performance for the parts of complex face images from the Labeled Faces in the Wild data set. This hybrid model produces results that are both quantitatively and qualitatively better than the CRF alone. In addition to high-quality labeling results, we demonstrate that the hidden units in the RBM portion of our model can be interpreted as face attributes that have been learned without any attribute-level supervision.</p><p>3 0.4750351 <a title="467-lsi-3" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>4 0.46576369 <a title="467-lsi-4" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>5 0.44463316 <a title="467-lsi-5" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<p>Author: Henrik Zimmer, Marcel Campen, Leif Kobbelt</p><p>Abstract: In the context of shape segmentation and retrieval object-wide distributions of measures are needed to accurately evaluate and compare local regions ofshapes. Lien et al. [16] proposed two point-wise concavity measures in the context of Approximate Convex Decompositions of polygons measuring the distance from a point to the polygon ’s convex hull: an accurate Shortest Path-Concavity (SPC) measure and a Straight Line-Concavity (SLC) approximation of the same. While both are practicable on 2D shapes, the exponential costs of SPC in 3D makes it inhibitively expensive for a generalization to meshes [14]. In this paper we propose an efficient and straight forward approximation of the Shortest Path-Concavity measure to 3D meshes. Our approximation is based on discretizing the space between mesh and convex hull, thereby reducing the continuous Shortest Path search to an efficiently solvable graph problem. Our approach works outof-the-box on complex mesh topologies and requires no complicated handling of genus. Besides presenting a rigorous evaluation of our method on a variety of input meshes, we also define an SPC-based Shape Descriptor and show its superior retrieval and runtime performance compared with the recently presented results on the Convexity Distribution by Lian et al. [12].</p><p>6 0.43851924 <a title="467-lsi-6" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>7 0.43096817 <a title="467-lsi-7" href="./cvpr-2013-Learning_Multiple_Non-linear_Sub-spaces_Using_K-RBMs.html">253 cvpr-2013-Learning Multiple Non-linear Sub-spaces Using K-RBMs</a></p>
<p>8 0.4249756 <a title="467-lsi-8" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>9 0.41013744 <a title="467-lsi-9" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>10 0.40756321 <a title="467-lsi-10" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>11 0.38971069 <a title="467-lsi-11" href="./cvpr-2013-City-Scale_Change_Detection_in_Cadastral_3D_Models_Using_Images.html">81 cvpr-2013-City-Scale Change Detection in Cadastral 3D Models Using Images</a></p>
<p>12 0.37801868 <a title="467-lsi-12" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>13 0.37720427 <a title="467-lsi-13" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>14 0.37693849 <a title="467-lsi-14" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>15 0.37545061 <a title="467-lsi-15" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>16 0.37423012 <a title="467-lsi-16" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>17 0.36946857 <a title="467-lsi-17" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>18 0.36003667 <a title="467-lsi-18" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>19 0.35848609 <a title="467-lsi-19" href="./cvpr-2013-Monocular_Template-Based_3D_Reconstruction_of_Extensible_Surfaces_with_Local_Linear_Elasticity.html">289 cvpr-2013-Monocular Template-Based 3D Reconstruction of Extensible Surfaces with Local Linear Elasticity</a></p>
<p>20 0.35784876 <a title="467-lsi-20" href="./cvpr-2013-Exploring_Compositional_High_Order_Pattern_Potentials_for_Structured_Output_Learning.html">156 cvpr-2013-Exploring Compositional High Order Pattern Potentials for Structured Output Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.094), (16, 0.025), (26, 0.049), (28, 0.012), (33, 0.172), (59, 0.011), (63, 0.304), (67, 0.033), (69, 0.075), (87, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74982584 <a title="467-lda-1" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>Author: Linjie Luo, Cha Zhang, Zhengyou Zhang, Szymon Rusinkiewicz</p><p>Abstract: We propose a novel algorithm to reconstruct the 3D geometry of human hairs in wide-baseline setups using strand-based refinement. The hair strands arefirst extracted in each 2D view, and projected onto the 3D visual hull for initialization. The 3D positions of these strands are then refined by optimizing an objective function that takes into account cross-view hair orientation consistency, the visual hull constraint and smoothness constraints defined at the strand, wisp and global levels. Based on the refined strands, the algorithm can reconstruct an approximate hair surface: experiments with synthetic hair models achieve an accuracy of ∼3mm. We also show real-world examples to demonsotfra ∼te3 mthme capability t soh capture full-head hamairp styles as mwoenll- as hair in motion with as few as 8 cameras.</p><p>2 0.74263942 <a title="467-lda-2" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>Author: Kilho Son, Eduardo B. Almeida, David B. Cooper</p><p>Abstract: Thispaper introduces a novel approachfor reassembling pot sherds found at archaeological excavation sites, for the purpose ofreconstructing claypots that had been made on a wheel. These pots and the sherds into which they have broken are axially symmetric. The reassembly process can be viewed as 3D puzzle solving or generalized cylinder learning from broken fragments. The estimation exploits both local and semi-global geometric structure, thus making it a fundamental problem of geometry estimation from noisy fragments in computer vision and pattern recognition. The data used are densely digitized 3D laser scans of each fragment’s outer surface. The proposed reassembly system is automatic and functions when the pile of available fragments is from one or multiple pots, and even when pieces are missing from any pot. The geometric structure used are curves on the pot along which the surface had broken and the silhouette of a pot with respect to an axis, called axisprofile curve (APC). For reassembling multiple pots with or without missing pieces, our algorithm estimates the APC from each fragment, then reassembles into configurations the ones having distinctive APC. Further growth of configurations is based on adding remaining fragments such that their APC and break curves are consistent with those of a configuration. The method is novel, more robust and handles the largest numbers of fragments to date.</p><p>3 0.67843986 <a title="467-lda-3" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>Author: Jonathan Balzer, Stefano Soatto</p><p>Abstract: We describe a method to efficiently generate a model (map) of small-scale objects from video. The map encodes sparse geometry as well as coarse photometry, and could be used to initialize dense reconstruction schemes as well as to support recognition and localization of three-dimensional objects. Self-occlusions and the predominance of outliers present a challenge to existing online Structure From Motion and Simultaneous Localization and Mapping systems. We propose a unified inference criterion that encompasses map building and localization (object detection) relative to the map in a coupled fashion. We establish correspondence in a computationally efficient way without resorting to combinatorial matching or random-sampling techniques. Instead, we use a simpler M-estimator that exploits putative correspondence from tracking after photometric and topological validation. We have collected a new dataset to benchmark model building in the small scale, which we test our algorithm on in comparison to others. Although our system is significantly leaner than previous ones, it compares favorably to the state of the art in terms of accuracy and robustness.</p><p>4 0.66730094 <a title="467-lda-4" href="./cvpr-2013-Nonlinearly_Constrained_MRFs%3A_Exploring_the_Intrinsic_Dimensions_of_Higher-Order_Cliques.html">308 cvpr-2013-Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques</a></p>
<p>Author: Yun Zeng, Chaohui Wang, Stefano Soatto, Shing-Tung Yau</p><p>Abstract: This paper introduces an efficient approach to integrating non-local statistics into the higher-order Markov Random Fields (MRFs) framework. Motivated by the observation that many non-local statistics (e.g., shape priors, color distributions) can usually be represented by a small number of parameters, we reformulate the higher-order MRF model by introducing additional latent variables to represent the intrinsic dimensions of the higher-order cliques. The resulting new model, called NC-MRF, not only provides the flexibility in representing the configurations of higher-order cliques, but also automatically decomposes the energy function into less coupled terms, allowing us to design an efficient algorithmic framework for maximum a posteriori (MAP) inference. Based on this novel modeling/inference framework, we achieve state-of-the-art solutions to the challenging problems of class-specific image segmentation and template-based 3D facial expression tracking, which demonstrate the potential of our approach.</p><p>5 0.6216442 <a title="467-lda-5" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>6 0.6077823 <a title="467-lda-6" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>7 0.59442973 <a title="467-lda-7" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>8 0.59211826 <a title="467-lda-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.59142596 <a title="467-lda-9" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>10 0.58852488 <a title="467-lda-10" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>11 0.58824569 <a title="467-lda-11" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>12 0.58634025 <a title="467-lda-12" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>13 0.58491033 <a title="467-lda-13" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>14 0.58416516 <a title="467-lda-14" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>15 0.58415473 <a title="467-lda-15" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>16 0.58310723 <a title="467-lda-16" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>17 0.58250844 <a title="467-lda-17" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>18 0.58210385 <a title="467-lda-18" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>19 0.58153236 <a title="467-lda-19" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>20 0.58138573 <a title="467-lda-20" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
