<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>330 cvpr-2013-Photometric Ambient Occlusion</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-330" href="#">cvpr2013-330</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>330 cvpr-2013-Photometric Ambient Occlusion</h1>
<br/><p>Source: <a title="cvpr-2013-330-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hauagge_Photometric_Ambient_Occlusion_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>Reference: <a title="cvpr-2013-330-reference" href="../cvpr2013_reference/cvpr-2013-Photometric_Ambient_Occlusion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Photometric Ambient Occlusion Daniel Hauagge  Scott Wehrwein Kavita Bala Cornell University  {hauagge  Noah Snavely  ,  swehrwe in  ,  kb  Abstract We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. [sent-1, score-0.898]
</p><p>2 Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. [sent-2, score-0.569]
</p><p>3 Some  ,  notable examples include recovering surface normals using photometric stereo [6, 25, 2], recovering diffuse reflectance and illumination as intrinsic images [27, 15], and computing low-dimensional models of appearance of objects and scenes [26, 9]. [sent-8, score-0.506]
</p><p>4 However, these methods typically disregard the effect of the local visibility of illumination in determining shading. [sent-9, score-0.351]
</p><p>5 The appearance of a point over such an image stack depends on many factors, such as the point’s albedo and the distribution of illuminations. [sent-18, score-0.475]
</p><p>6 However, a key observation is that the local visibility of a point—i. [sent-19, score-0.265]
</p><p>7 Our method takes as input a stack of images captured from varying, unknown illumination (a) and computes a per-pixel statistic, κ, over this stack (b). [sent-24, score-0.363]
</p><p>8 We infer both per-pixel ambient occlusion, a measure of local visibility (c), and albedo (d) for the scene by relating κ to a simple image formation model. [sent-25, score-1.304]
</p><p>9 tions, often modeled as ambient occlusion (AO) in computer graphics—is also an important property in determining its appearance in images. [sent-26, score-0.735]
</p><p>10 We show that we can estimate ambient occlusion directly from image observations, by introducing a simple, aggregate statistic (κ in Fig. [sent-27, score-0.894]
</p><p>11 To do so, we consider a physical model of a point with a cone of visibility to the hemisphere, lit by a moving point light and constant ambient light over the image stack. [sent-29, score-1.684]
</p><p>12 We then combine this model with  our statistic to infer ambient occlusion for each scene point (Fig. [sent-30, score-0.936]
</p><p>13 This kind of lighting visibility is often treated as a nuisance in computer vision methods, and in many cases is simply ignored. [sent-32, score-0.311]
</p><p>14 In contrast, we explicitly model such visibility for each scene point, and use it to aid in estimating other physical parameters, such as surface albedo (Fig. [sent-33, score-0.734]
</p><p>15 The result is a photometric approach to estimating ambient occlusion and albedo. [sent-35, score-0.822]
</p><p>16 Our method has several key properties: we do not require knowledge of light positions, explicit scene geometry, or surface normals. [sent-36, score-0.31]
</p><p>17 The setup for acquisition is simple, requiring a point light source and a camera. [sent-37, score-0.316]
</p><p>18 However, we do assume 222555 111533  that light source positions vary uniformly over the full hemisphere, although in practice we achieve good results even when this assumption does not hold. [sent-38, score-0.278]
</p><p>19 Our contributions are: •  •  A per-pixel, image-space approach to estimating ambient occlusion that does not require information about the underlying geometry. [sent-41, score-0.735]
</p><p>20 A new method for intrinsic image decomposition using our model of ambient occlusion, accounting for local visibility at each point. [sent-42, score-0.982]
</p><p>21 Ambient Occlusion Ambient occlusion [16] is a measure of light accessibility commonly used in computer graphics to properly account for ambient illumination. [sent-50, score-1.019]
</p><p>22 Formally, for a single scene point x, AO is the integral over the hemisphere  AO(x) =π1ZΩV (x, ω~)h n~, ω~idω  (1)  of the local visibility function V (x, ω) (i. [sent-51, score-0.446]
</p><p>23 If the albedo at x is ρ, the measured radiance due to ambient illumination with intensity la can be expressed as: Ia = ρπlaAO  (2)  Note that this only considers the first bounce of light (direct illumination), and does not account for inter-reflections. [sent-59, score-1.39]
</p><p>24 In contrast, we are interested in estimating AO from a set of images illuminated by a varying, unknown light source. [sent-62, score-0.306]
</p><p>25 require accurate estimates of the albedo for a sparse set of 3D scene points [14]. [sent-73, score-0.35]
</p><p>26 This yields a very simple approach that could be used as a pre-process to account for light visibility in other vision algorithms. [sent-76, score-0.49]
</p><p>27 recover albedo and normals by directly tracking the intensity of pixel values over time [24]. [sent-83, score-0.429]
</p><p>28 Our approach is especially related to uncalibrated photometric stereo, in which the light source directions are unknown. [sent-88, score-0.365]
</p><p>29 A key challenge in photometric stereo is dealing with shadows, either by detecting them in some manner [8, 25] (a nontrivial problem with surfaces of varying albedo or complex self-occlusions), or treating them as a source of noise [30]. [sent-89, score-0.543]
</p><p>30 reason about lighting visibility of surface points, by clustering them into “visibility subspaces” that see a common set of lights [25]. [sent-91, score-0.358]
</p><p>31 However, they use an implicit model of lighting visibility that grows in complexity as the number of lighting conditions increases. [sent-92, score-0.357]
</p><p>32 In contrast, our method relies on a simple per-pixel measure of ambient occlusion that becomes more robust as more images are added. [sent-93, score-0.735]
</p><p>33 In addition, our model incorporates ambient illumination as well as directional lighting. [sent-94, score-0.798]
</p><p>34 A Model for AO in Image Stacks We now describe how to obtain a simple approximation to ambient occlusion (AO) by observing pixel intensities in multiple images under varying directional lighting. [sent-97, score-0.977]
</p><p>35 We introduce an physically-based image formation model for our measure of AO, then use this model to derive AO and albedo from image sequences. [sent-98, score-0.365]
</p><p>36 The scene is lit by an unknown, directional light source that changes from image to image, together with a constant ambient light source, both of which are of constant intensity over time. [sent-105, score-1.481]
</p><p>37 We assume that the distribution of directional light sources is uniform over the hemisphere. [sent-106, score-0.327]
</p><p>38 This distribution of pixel intensities is related to the distribution of illuminations over the image stack, as well as to the albedo of that point and to the surrounding geometry. [sent-111, score-0.423]
</p><p>39 concavity will very often appear dark, because light rarely reaches it (only when the light is shining straight down into the hole). [sent-120, score-0.45]
</p><p>40 ) The intuition then, is that the samples we record give us information about a pixel’s PID, which in turns reveals information about surface albedo and ambient occlusion. [sent-124, score-0.969]
</p><p>41 We show that this simple ratio of statistics over recorded intensities yields an approximation to ambient occlusion; to understand this relationship between κ and ambient occlusion, we first describe our image formation model, then relate this to a physical model of local scene geometry. [sent-130, score-1.421]
</p><p>42 [ LT]his behavior suggests tEh[aLt κ c=ou Eld[L b]e useful as a measure of ambient occlusion at a point. [sent-142, score-0.735]
</p><p>43 A physical image formation model for  κ  So far we have shown that κ is independent of albedo and is bounded. [sent-145, score-0.437]
</p><p>44 As a statistic, κ relates to the geometry and visibility at a point; to show this, we introduce a simplified geometry and lighting model to connect κ to a physical measure of local visibility. [sent-147, score-0.463]
</p><p>45 Our model assumes that the visibility at a point can be approximated by a cone of angle α (Fig. [sent-148, score-0.433]
</p><p>46 A point x, on a  Lambertian surface, is observed by camera c while illuminated by two light sources: a directional light with intensity ld, and a background ambient illumination with constant intensity la. [sent-150, score-1.48]
</p><p>47 Surface geometry around the point blocks all light outside the cone with angle α from reaching x. [sent-152, score-0.433]
</p><p>48 We refer to this angle α(x) as the local visibility angle for point x. [sent-153, score-0.431]
</p><p>49 Further, across our input images, we assume that the directional light uniformly samples the full hemisphere, so each measure of the radiance of x captured by the camera corresponds to a different (unknown) position for the light ld. [sent-154, score-0.613]
</p><p>50 Given these assumptions, κ(x) only depends on the local visibility angle α(x). [sent-155, score-0.329]
</p><p>51 To begin, each image I the sum ofthe contributions is from both light sources: I Id + Ia =  (6)  The directional component Id varies from image to image and depends on the angle θd(t) between the light source direction ω~d(t) and the point normal and whether the light is blocked by other geometry. [sent-157, score-0.973]
</p><p>52 It is given by:  n,  Id(t)  = =  ρldVα(~ n, ωd(t))h n~, ωd(t)i ρldVα (θ(t)) cos θd(t)  where Vα is the visibility term (i. [sent-158, score-0.265]
</p><p>53 The ambient component is coθn ≤stan αt and proportional to the projected solid angle of the local visibility angle. [sent-161, score-0.939]
</p><p>54 A point x on a Lambertian surface is observed by camera c and illuminated by a distant, moving light source with intensity ld, and a constant ambient term of intensity la. [sent-164, score-1.167]
</p><p>55 The local visibility is approximated by a cone with angle α. [sent-165, score-0.395]
</p><p>56 If the light source angle with the surface normal θ is larger than α, light is blocked and does not reach point x at the bottom of the valley. [sent-166, score-0.693]
</p><p>57 ambient illumination over the visible hemisphere at the point:  Ia= ρZϕ2=π0Zθ=α0lacos(θ)sin(θ)dθdϕ = ρlaπ sin2α (7) Given this model for Id and Ia, to relate κ to our physical parameter α, we compute the expectations in Eq. [sent-167, score-0.895]
</p><p>58 (5) over light source positions: E[I] = E[Id]  + E[Ia]  = E[Id]  + Ia  E[I2] = E[(Id + Ia)2] = E[Id2] + 2IaE[Id] + Ia2 where we use the linearity of expectation, E[·] , and the assumption that Ia does not change over the image stack. [sent-168, score-0.278]
</p><p>59 For the direct component, we integrate over the visible cone of angles at the point, assuming the point light is uniformly distributed over the hemisphere for the image stack:  E[Id] =21πZϕ=2π0Zθ=α0Idsinθdθdϕ =21ρldsin2(α)  (8)  E[Id2] =21πZϕ2=π0Zθ=α0Id2sinθdθdϕ = −31ρ2ld2? [sent-169, score-0.461]
</p><p>60 κ(α) for different ratios of ambient to direct light f. [sent-182, score-0.862]
</p><p>61 In summary, we have derived a relation between the statistic κ, and the ambient occlusion at a point, using a physical model of a crevice (with a cone of visibility characterized by α) lit by a varying directional light, and a constant ambient light over a stack of images. [sent-192, score-2.618]
</p><p>62 As we show later, this physical model, though simple and an approximation of real scenarios, works surprisingly well in characterizing the visibility at points in a scene. [sent-194, score-0.337]
</p><p>63 Algorithm  ×  In this section we use our model to compute a per-pixel local visibility angle α(x) and albedo ρ(x) given a stack of images of the same scene under varying illumination. [sent-196, score-0.845]
</p><p>64 , ambient lighting is negligible) to derive an initial α0 using Eq. [sent-201, score-0.656]
</p><p>65 In total we have nc np equations, where nc is the number of color channels a×nd n np the number of pixels, and np + nc variables, one α per pixel and nc variables corresponding to the direct to ambient illumination ratios f. [sent-205, score-0.874]
</p><p>66 Given our final estimates α1 and f1, we compute estimates for the albedo ρ(x) at each point from Eqs. [sent-208, score-0.35]
</p><p>67 2 we use an object with known geometry to measure the error in our estimate of ambient occlusion. [sent-216, score-0.684]
</p><p>68 3 we evaluate our estimate of albedo by comparing our algorithm with others using the MIT Intrinsic Images benchmark [11]. [sent-218, score-0.346]
</p><p>69 For each dataset we show κ, ambient occlusion, ρ, and the illumination. [sent-225, score-0.61]
</p><p>70 The light source position in TENTACLE was precisely controlled by a  mechanical gantry allowing us to sample uniformly random positions over the full hemisphere. [sent-229, score-0.278]
</p><p>71 Here the main challenge is that there are only 10 images of each object lit by a point light source. [sent-239, score-0.382]
</p><p>72 Figure 5 shows that the recovered AO seem to match our expectation of local visibility for these scenes. [sent-241, score-0.294]
</p><p>73 The recovered albedos are mostly free of shading and the ambient occlusion map is mostly free of albedo (e. [sent-243, score-1.187]
</p><p>74 For a white directional light source, color casts in κ reveal the color of the ambient term, since f has a different value per channel. [sent-248, score-1.008]
</p><p>75 The cause is likely to be subsurface scattering, where light arriving after multiple subsurface scattering events can be thought of as acting like a local ambient light term. [sent-253, score-1.299]
</p><p>76 The selected material (sandstone) was the most diffuse of the available materials, but was not perfectly diffuse, and exhibited a fair amount of subsurface scattering (see the red ray gun of TENTACLE). [sent-263, score-0.263]
</p><p>77 9 (a)  the average error for α at the center of the crevice for LIGHTWELL compared to ground truth, as a function of the local visibility angle α. [sent-271, score-0.436]
</p><p>78 We suspect that this is caused by the increase in light interreflections for higher albedos. [sent-275, score-0.265]
</p><p>79 Estimated Albedo We ran our algorithm on the MIT Intrinsic Images benchmark [11] to measure the quality of our albedo estimates. [sent-285, score-0.312]
</p><p>80 , Retinex) operate on a single image, usually by imposing priors on the illumination and albedo images. [sent-289, score-0.423]
</p><p>81 We obtain the shading image for each of the input images by simply dividing the input image by our estimated albedo (see Eq. [sent-291, score-0.355]
</p><p>82 We believe that this is a result of the setup, which indeed does not contain ambient illumination, and the fact that most objects have a very high albedo, resulting in a larger contribution due to inter reflections, which is not modeled by our algorithm. [sent-301, score-0.61]
</p><p>83 Rate of convergence We now consider the impact of the number of images and the visibility angle in estimating ambient occlusion. [sent-305, score-0.939]
</p><p>84 Figure 9 (b) shows the root mean squared error (RMSE) of our ambient occlusion estimate as a function of the number of  GR-RET 00. [sent-306, score-0.769]
</p><p>85 Compared algorithms are: Grayscale Retinex (GR-RET), Color Retinex (COL-RET), Weiss (W), Weiss+Retinex (W+RET), ours with only direct term (κ-D) and our second estimate containing direct and ambient terms (κDA). [sent-326, score-0.698]
</p><p>86 Results are for our first estimate of the albedo (i. [sent-329, score-0.346]
</p><p>87 , ambient illumination is assumed to be zero) as this gave us the best results on the benchmark. [sent-331, score-0.696]
</p><p>88 (a) Error in the estimated local visibility angle α vs. [sent-334, score-0.329]
</p><p>89 the true local visibility angle for the LIGHTWELL object printed in different colors (shown in the left). [sent-335, score-0.398]
</p><p>90 (b) Average Root Mean Square Error (RMSE) for our estimate of ambient occlusion vs. [sent-336, score-0.769]
</p><p>91 Conclusions Ambient occlusion, a measure of local visibility at a point, plays an important role in the shading of surfaces. [sent-344, score-0.308]
</p><p>92 We introduce an image-space approach to estimating ambient occlusion from a set of images under varying, unknown illumination. [sent-345, score-0.762]
</p><p>93 Our method analyzes the scene in terms of a physical model of a visibility cone, lit by a varying point light over the image stack. [sent-346, score-0.798]
</p><p>94 We propose a simple, per-pixel statistic, κ, based on observed intensities over the set of images; from κ, we recover per-pixel ambient occlusion and albedo values by relating our physical model to this measured statistic. [sent-347, score-1.183]
</p><p>95 We assume that input images are illuminated by a point light source that moves over the entire hemisphere visible to any given point. [sent-351, score-0.475]
</p><p>96 For outdoor scenes, where the directional light is from the sun, this assumption  is violated; we need improved models to account for more general distributions of lighting directions. [sent-352, score-0.373]
</p><p>97 However, in the presence of specularities, subsurface scattering, or significant interreflections, our albedo estimates are less accurate. [sent-354, score-0.4]
</p><p>98 Our crevice model assumes a conical visibility model; in the future, we could extend this to include anisotropy so as to better match more general visibility scenarios. [sent-356, score-0.637]
</p><p>99 High-frequency shape and albedo from shading using natural image statistics. [sent-383, score-0.355]
</p><p>100 Improved reconstruction of deforming surfaces by cancelling ambient occlusion. [sent-402, score-0.61]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ambient', 0.61), ('albedo', 0.312), ('visibility', 0.265), ('ao', 0.245), ('light', 0.225), ('stack', 0.125), ('statistic', 0.125), ('occlusion', 0.125), ('lit', 0.119), ('tentacle', 0.111), ('crevice', 0.107), ('hemisphere', 0.105), ('directional', 0.102), ('ld', 0.095), ('subsurface', 0.088), ('photometric', 0.087), ('illumination', 0.086), ('intrinsic', 0.084), ('retinex', 0.079), ('pid', 0.073), ('physical', 0.072), ('lightwell', 0.071), ('printed', 0.069), ('cone', 0.066), ('angle', 0.064), ('hole', 0.064), ('ret', 0.063), ('scattering', 0.063), ('id', 0.061), ('radiance', 0.061), ('ia', 0.059), ('sunkavalli', 0.059), ('intensity', 0.057), ('mit', 0.055), ('diffuse', 0.055), ('illuminated', 0.054), ('albedos', 0.053), ('formation', 0.053), ('source', 0.053), ('stereo', 0.05), ('reflectance', 0.048), ('renderer', 0.048), ('surface', 0.047), ('lighting', 0.046), ('lambertian', 0.045), ('shading', 0.043), ('blocked', 0.041), ('varying', 0.041), ('geometry', 0.04), ('interreflections', 0.04), ('la', 0.039), ('scene', 0.038), ('point', 0.038), ('intensities', 0.038), ('assumptions', 0.038), ('accessibility', 0.036), ('aldrian', 0.036), ('laffont', 0.036), ('lmse', 0.036), ('pixel', 0.035), ('estimate', 0.034), ('ldv', 0.032), ('gun', 0.032), ('printing', 0.032), ('beeler', 0.032), ('cloudy', 0.032), ('shadows', 0.032), ('barron', 0.031), ('expectation', 0.029), ('weiss', 0.028), ('obs', 0.028), ('hauagge', 0.028), ('darker', 0.027), ('direct', 0.027), ('unknown', 0.027), ('observing', 0.026), ('webpage', 0.026), ('frog', 0.026), ('relating', 0.026), ('constant', 0.026), ('priors', 0.025), ('yale', 0.025), ('var', 0.025), ('material', 0.025), ('normals', 0.025), ('color', 0.024), ('scenes', 0.024), ('brighter', 0.024), ('nc', 0.023), ('graphics', 0.023), ('decomposition', 0.023), ('casts', 0.023), ('quotient', 0.023), ('smoothness', 0.023), ('expectations', 0.022), ('rmse', 0.022), ('neck', 0.022), ('grosse', 0.022), ('mostly', 0.022), ('face', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="330-tfidf-1" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>2 0.24128385 <a title="330-tfidf-2" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>3 0.1913923 <a title="330-tfidf-3" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>4 0.16261484 <a title="330-tfidf-4" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>Author: Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials, the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals, and that the intensity distribution of an intensity profile conveys information about the reflectance properties, when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations, we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.</p><p>5 0.15020686 <a title="330-tfidf-5" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>Author: Zhenglong Zhou, Zhe Wu, Ping Tan</p><p>Abstract: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple, which consists of only a digital camera and a handheld light source. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distributionfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments, the captured shapes are accurate to 0.3 millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9%.</p><p>6 0.14398029 <a title="330-tfidf-6" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>7 0.14358376 <a title="330-tfidf-7" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>8 0.13021536 <a title="330-tfidf-8" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>9 0.12057102 <a title="330-tfidf-9" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>10 0.11677346 <a title="330-tfidf-10" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>11 0.11666376 <a title="330-tfidf-11" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>12 0.10345348 <a title="330-tfidf-12" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>13 0.096918836 <a title="330-tfidf-13" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>14 0.089416392 <a title="330-tfidf-14" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>15 0.088940062 <a title="330-tfidf-15" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>16 0.083810702 <a title="330-tfidf-16" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>17 0.082975186 <a title="330-tfidf-17" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>18 0.082258388 <a title="330-tfidf-18" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>19 0.078506075 <a title="330-tfidf-19" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>20 0.077157229 <a title="330-tfidf-20" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.16), (2, -0.002), (3, 0.063), (4, 0.007), (5, -0.093), (6, -0.073), (7, 0.062), (8, 0.074), (9, -0.005), (10, -0.085), (11, -0.125), (12, 0.01), (13, -0.054), (14, 0.005), (15, 0.098), (16, 0.051), (17, -0.052), (18, -0.001), (19, -0.039), (20, 0.051), (21, 0.011), (22, 0.019), (23, 0.014), (24, 0.051), (25, -0.062), (26, 0.04), (27, -0.063), (28, -0.022), (29, 0.053), (30, 0.11), (31, -0.127), (32, 0.027), (33, 0.057), (34, -0.018), (35, 0.026), (36, -0.087), (37, -0.021), (38, 0.077), (39, -0.015), (40, -0.205), (41, 0.014), (42, 0.019), (43, -0.108), (44, 0.094), (45, 0.056), (46, -0.048), (47, 0.036), (48, 0.011), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93729049 <a title="330-lsi-1" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>2 0.74686396 <a title="330-lsi-2" href="./cvpr-2013-Non-parametric_Filtering_for_Geometric_Detail_Extraction_and_Material_Representation.html">305 cvpr-2013-Non-parametric Filtering for Geometric Detail Extraction and Material Representation</a></p>
<p>Author: Zicheng Liao, Jason Rock, Yang Wang, David Forsyth</p><p>Abstract: Geometric detail is a universal phenomenon in real world objects. It is an important component in object modeling, but not accounted for in current intrinsic image works. In this work, we explore using a non-parametric method to separate geometric detail from intrinsic image components. We further decompose an image as albedo ∗ (ccoomarpsoen-escnatsle. shading +e shading pdoestaeil a).n Oaugre decomposition offers quantitative improvement in albedo recovery and material classification.Our method also enables interesting image editing activities, including bump removal, geometric detail smoothing/enhancement and material transfer.</p><p>3 0.72650272 <a title="330-lsi-3" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>Author: Antony Lam, Imari Sato</p><p>Abstract: Hyperspectral reflectance data allows for highly accurate spectral relighting under arbitrary illumination, which is invaluable to applications ranging from archiving cultural e-heritage to consumer product design. Past methods for capturing the spectral reflectance of scenes has proven successful in relighting but they all share a common assumption. All the methods do not consider the effects of fluorescence despite fluorescence being found in many everyday objects. In this paper, we describe the very different ways that reflectance and fluorescence interact with illuminants and show the need to explicitly consider fluorescence in the relighting problem. We then propose a robust method based on well established theories of reflectance and fluorescence for imaging each of these components. Finally, we show that we can relight real scenes of reflective-fluorescent surfaces with much higher accuracy in comparison to only considering the reflective component.</p><p>4 0.63245261 <a title="330-lsi-4" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>5 0.59170568 <a title="330-lsi-5" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>Author: Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials, the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals, and that the intensity distribution of an intensity profile conveys information about the reflectance properties, when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations, we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.</p><p>6 0.55914849 <a title="330-lsi-6" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>7 0.55293876 <a title="330-lsi-7" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>8 0.54904044 <a title="330-lsi-8" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>9 0.53068715 <a title="330-lsi-9" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>10 0.5052067 <a title="330-lsi-10" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>11 0.49947608 <a title="330-lsi-11" href="./cvpr-2013-Whitened_Expectation_Propagation%3A_Non-Lambertian_Shape_from_Shading_and_Shadow.html">466 cvpr-2013-Whitened Expectation Propagation: Non-Lambertian Shape from Shading and Shadow</a></p>
<p>12 0.48685116 <a title="330-lsi-12" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>13 0.46728945 <a title="330-lsi-13" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>14 0.46456563 <a title="330-lsi-14" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>15 0.45083362 <a title="330-lsi-15" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>16 0.43892029 <a title="330-lsi-16" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>17 0.43618411 <a title="330-lsi-17" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>18 0.42157412 <a title="330-lsi-18" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>19 0.42121243 <a title="330-lsi-19" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>20 0.40761065 <a title="330-lsi-20" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.183), (10, 0.128), (16, 0.07), (26, 0.058), (28, 0.016), (33, 0.219), (66, 0.051), (67, 0.037), (69, 0.063), (87, 0.068), (99, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84660476 <a title="330-lda-1" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>2 0.840832 <a title="330-lda-2" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>3 0.81159759 <a title="330-lda-3" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>Author: Zhenglong Zhou, Zhe Wu, Ping Tan</p><p>Abstract: We present a method to capture both 3D shape and spatially varying reflectance with a multi-view photometric stereo technique that works for general isotropic materials. Our data capture setup is simple, which consists of only a digital camera and a handheld light source. From a single viewpoint, we use a set of photometric stereo images to identify surface points with the same distance to the camera. We collect this information from multiple viewpoints and combine it with structure-from-motion to obtain a precise reconstruction of the complete 3D shape. The spatially varying isotropic bidirectional reflectance distributionfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs and their mixing weights at each surface point. According to our experiments, the captured shapes are accurate to 0.3 millimeters. The captured reflectance has relative root-mean-square error (RMSE) of 9%.</p><p>4 0.8095454 <a title="330-lda-4" href="./cvpr-2013-Calibrating_Photometric_Stereo_by_Holistic_Reflectance_Symmetry_Analysis.html">75 cvpr-2013-Calibrating Photometric Stereo by Holistic Reflectance Symmetry Analysis</a></p>
<p>Author: Zhe Wu, Ping Tan</p><p>Abstract: Under unknown directional lighting, the uncalibrated Lambertian photometric stereo algorithm recovers the shape of a smooth surface up to the generalized bas-relief (GBR) ambiguity. We resolve this ambiguity from the halfvector symmetry, which is observed in many isotropic materials. Under this symmetry, a 2D BRDF slice with low-rank structure can be obtained from an image, if the surface normals and light directions are correctly recovered. In general, this structure is destroyed by the GBR ambiguity. As a result, we can resolve the ambiguity by restoring this structure. We develop a simple algorithm of auto-calibration from separable homogeneous specular reflection of real images. Compared with previous methods, this method takes a holistic approach to exploiting reflectance symmetry and produces superior results.</p><p>5 0.80839497 <a title="330-lda-5" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>Author: Ilya Reshetouski, Alkhazur Manakov, Ayush Bandhari, Ramesh Raskar, Hans-Peter Seidel, Ivo Ihrke</p><p>Abstract: We investigate the problem of identifying the position of a viewer inside a room of planar mirrors with unknown geometry in conjunction with the room’s shape parameters. We consider the observations to consist of angularly resolved depth measurements of a single scene point that is being observed via many multi-bounce interactions with the specular room geometry. Applications of this problem statement include areas such as calibration, acoustic echo cancelation and time-of-flight imaging. We theoretically analyze the problem and derive sufficient conditions for a combination of convex room geometry, observer, and scene point to be reconstructable. The resulting constructive algorithm is exponential in nature and, therefore, not directly applicable to practical scenarios. To counter the situation, we propose theoretically devised geometric constraints that enable an efficient pruning of the solution space and develop a heuristic randomized search algorithm that uses these constraints to obtain an effective solution. We demonstrate the effectiveness of our algorithm on extensive simulations as well as in a challenging real-world calibration scenario.</p><p>6 0.80807835 <a title="330-lda-6" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>7 0.80427462 <a title="330-lda-7" href="./cvpr-2013-Robust_Estimation_of_Nonrigid_Transformation_for_Point_Set_Registration.html">360 cvpr-2013-Robust Estimation of Nonrigid Transformation for Point Set Registration</a></p>
<p>8 0.80304372 <a title="330-lda-8" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>9 0.79880542 <a title="330-lda-9" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>10 0.79837435 <a title="330-lda-10" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>11 0.79721665 <a title="330-lda-11" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>12 0.79664516 <a title="330-lda-12" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<p>13 0.79577941 <a title="330-lda-13" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>14 0.79558414 <a title="330-lda-14" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>15 0.7953546 <a title="330-lda-15" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>16 0.79517686 <a title="330-lda-16" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>17 0.79479021 <a title="330-lda-17" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>18 0.79441524 <a title="330-lda-18" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>19 0.79423058 <a title="330-lda-19" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>20 0.7935524 <a title="330-lda-20" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
