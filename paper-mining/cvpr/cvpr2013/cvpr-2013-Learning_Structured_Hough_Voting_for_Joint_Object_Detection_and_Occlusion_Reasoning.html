<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-256" href="#">cvpr2013-256</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</h1>
<br/><p>Source: <a title="cvpr-2013-256-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Learning_Structured_Hough_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Tao Wang, Xuming He, Nick Barnes</p><p>Abstract: Wepropose a structuredHough voting methodfor detecting objects with heavy occlusion in indoor environments. First, we extend the Hough hypothesis space to include both object location and its visibility pattern, and design a new score function that accumulates votes for object detection and occlusion prediction. In addition, we explore the correlation between objects and their environment, building a depth-encoded object-context model based on RGB-D data. Particularly, we design a layered context representation and .barne s }@ nict a . com .au (a)(b)(c) (d)(e)(f) allow image patches from both objects and backgrounds voting for the object hypotheses. We demonstrate that using a data-driven 2.1D representation we can learn visual codebooks with better quality, and more interpretable detection results in terms of spatial relationship between objects and viewer. We test our algorithm on two challenging RGB-D datasets with significant occlusion and intraclass variation, and demonstrate the superior performance of our method.</p><p>Reference: <a title="cvpr-2013-256-reference" href="../cvpr2013_reference/cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 he  ,  nick  Abstract Wepropose a structuredHough voting methodfor detecting objects with heavy occlusion in indoor environments. [sent-3, score-0.62]
</p><p>2 First, we extend the Hough hypothesis space to include both object location and its visibility pattern, and design a new score function that accumulates votes for object detection and occlusion prediction. [sent-4, score-0.828]
</p><p>3 au  (a)(b)(c)  (d)(e)(f)  allow image patches from both objects and backgrounds voting for the object hypotheses. [sent-9, score-0.651]
</p><p>4 The partial objects being observed usually provide limited information on the object position and pose, so many previous object detection approaches are prone to failure as they solely rely on image cues from objects themselves. [sent-15, score-0.316]
</p><p>5 (a) RGB frame with object bounding box (red) and visible part bounding box (green). [sent-22, score-0.345]
</p><p>6 In particular, occlusion can be viewed as a special type of contextual relationship in 3D, which would become an intrinsic component of object and scene models. [sent-33, score-0.286]
</p><p>7 Finally, joint modeling of an object class and its 3D context may provide effective constraints on the object’s scope on image plane and lead to a coarse-level object segmentation. [sent-34, score-0.407]
</p><p>8 Our work aims to utilize RGB-D datasets to learn a context-aware object detection model which encodes depth cues and a coarse level of 3D relationships. [sent-37, score-0.35]
</p><p>9 The learned depth-encoded object 111777889088  and context model is then applied to 2D images during test so it can be used to facilitate generic object detection [24]. [sent-39, score-0.406]
</p><p>10 Specifically, we propose a structured Hough voting method that incorporates depth-dependent contexts into a code-book based object detection model. [sent-40, score-0.688]
</p><p>11 Our model generalizes the traditional Hough voting detection methods in three ways. [sent-41, score-0.515]
</p><p>12 An image region contributes to each object hypothesis in a different manner based on its depth layer. [sent-43, score-0.351]
</p><p>13 Secondly, we define a new object hypothesis space in which both the object’s center and its visibility mask will be predicted. [sent-44, score-0.79]
</p><p>14 Each image patch will generate a weighted vote to a joint score of the object center and its support mask in the image. [sent-45, score-0.664]
</p><p>15 Finally, we view occlusion as special contextual information, which could provide cues for object localization and help with reasoning about visibility of object parts. [sent-46, score-0.693]
</p><p>16 Our detection and segmentation are achieved by maximizing the joint score of object center and visibility mask. [sent-48, score-0.604]
</p><p>17 We derive an efficient alternating ascent method to search modes of the Hough voting score maps. [sent-49, score-0.537]
</p><p>18 However, the majority of Hough voting methods focus on improving the target object model and few have studied context and occlusion reasoning. [sent-59, score-0.752]
</p><p>19 Joint detection and segmentation with Hough voting based methods has been investigated in [11], which only represents the object parts with additional masks and generates segmentation in two separate stages. [sent-60, score-0.828]
</p><p>20 Unlike those methods, our inference iteratively optimizes a well-defined objective function of object center and visibility mask. [sent-62, score-0.43]
</p><p>21 Many works have incorporated object-level context and rely on semantic contextual information for object segmentation (e. [sent-65, score-0.406]
</p><p>22 1D layered object representation in a scene can positively impact object localization. [sent-69, score-0.312]
</p><p>23 Our work, however, explores depth encoded image context for improving object detection. [sent-70, score-0.333]
</p><p>24 However, [24] uses the depth only to prune out patches of incorrect scales, and to create a generative depth model. [sent-76, score-0.345]
</p><p>25 In [27], we solely focused on object detection with a single layer context model. [sent-77, score-0.429]
</p><p>26 Our work seeks a unified model that can encode object and context information simultaneously at the object level. [sent-79, score-0.339]
</p><p>27 Structured Hough voting We first briefly review the original Hough voting based object detection method and introduce notation. [sent-89, score-1.071]
</p><p>28 , [11, 6]) generally use object poses as their hypothesis, accumulate scores from each image patch into a confidence map for the hypothesis space, and search for the highest voting scores from the map [1]. [sent-92, score-0.822]
</p><p>29 Let the object hypothesis be x ∈ X, jwechetr cel Xss oisf ft ihnet object pose space. [sent-94, score-0.357]
</p><p>30 Hough voting methods define a scoring function S(x) for each valid location x on the image plane, which is a summation of weighted votes from every local image patch. [sent-97, score-0.504]
</p><p>31 To compute the voting weights, an appearance-based codebook is usually learned from the image patches in object class o, denoted by C = {Ci}iK=1 . [sent-98, score-0.796]
</p><p>32 Each codebook entry  Cjeci tc colnassisst os, dofe a typical patch descriptor fci and geometric 111777889199  3D object dataset. [sent-99, score-0.455]
</p><p>33 Right panel: Illustration of multiple layered object centroid and mask voting. [sent-100, score-0.566]
</p><p>34 L1 corresponds to the object layer, and L2, L3, L4 correspond to far-away context, close-up context and occluder layers, respectively. [sent-101, score-0.28]
</p><p>35 For mask voting, brighter regions indicate a  higher response, while darker regions indicate a lower response. [sent-102, score-0.284]
</p><p>36 Notice that the object hypothesis x essentially specifies a bounding box. [sent-120, score-0.335]
</p><p>37 However, the bounding box hypothesis space is limited in its representation power as it is incapable of describing partial objects or its visibility pattern. [sent-121, score-0.46]
</p><p>38 We propose to extend the object hypothesis space from a single centroid x to a joint space (x, v) and define a new score function S(x, v). [sent-122, score-0.429]
</p><p>39 Here x specifies the object center (or equivalently its bounding box), and v is a visibility mask indicating which part of object is visible, as shown in Fig. [sent-123, score-0.843]
</p><p>40 The mask v has the same size as the image I, and v(y) = 1 if the image patch at y belongs to the object o, and 0 otherwise. [sent-125, score-0.517]
</p><p>41 1, we introduce a class of voting masks that are capable of representing the relative positions as well as the object visibility pattern. [sent-128, score-0.833]
</p><p>42 2, we include a local mask and a global mask for each codebook entry. [sent-130, score-0.713]
</p><p>43 The local mask predicts if a local patch itself is part of the object, and the global mask casts a vote for the spatial extent of the whole object on the image plane based on the relative geometric feature d. [sent-131, score-0.827]
</p><p>44 Formally, each codebook entry Ci includes a new set of geometric features = = (d, mdL, mdG)}, where mdL is the local mask feature= a n{d mdG is the global }m,a wskh efereat mure. [sent-132, score-0.506]
</p><p>45 The local mask features describe local visibility of object regions, which is similar to the ISM [11]. [sent-133, score-0.604]
</p><p>46 The global mask features limit the scope of each object in the image plane. [sent-134, score-0.421]
</p><p>47 Note that by choosing a different family of mask features, our model allows for finer description of the object shape and/or visibility pattern. [sent-138, score-0.604]
</p><p>48 D˜i  {d˜  For an image patch at Iy and object center hypothesis x, we can compute two average voting masks from the i-th codebook entry as follows: miG(x,y)  ∝  ? [sent-139, score-1.128]
</p><p>49 ∈D˜i where mG and mL are the average global and local voting mask, respectively; m(x) represents the mask with its center shifted to x, G(·) is the Gaussian kernel, and ∗ is the cceonnvteorl usthiiofnte operator. [sent-143, score-0.777]
</p><p>50 d We define the new score function as a matching score between the visibility mask hypothesis v and a weighted sum of the voting mask values,  S(x,v) =? [sent-146, score-1.495]
</p><p>51 (4)  where wb is a global bias to the mask voting score, and μ is the relative weight of the local mask. [sent-155, score-0.781]
</p><p>52 It can be shown that when v = 1, μ = 0 and the global voting mask has the shape of object bounding box, the new score function is equivalent to the Hough voting score in Eqn. [sent-159, score-1.479]
</p><p>53 Depth-encoded context The structured Hough voting model can easily incorporate image contextual information by extending the codebook and including votes from both object and context patches. [sent-163, score-1.173]
</p><p>54 We associate each layer with its own specific parameters as they contribute to object detection and occlusion reasoning in different ways. [sent-166, score-0.408]
</p><p>55 We first learn a separate codebookbased appearance model for each layer using object labels and depth cues. [sent-167, score-0.341]
</p><p>56 Denote the i-th codebook entry of layer l as Cil, we define a context-aware structured Hough voting model by including the votes from all the layers:  Sc(x,v) =? [sent-168, score-0.896]
</p><p>57 Note that each layer has its own Gaussian kernel width σdl in the voting masks. [sent-181, score-0.579]
</p><p>58 We use HOG features [5] for image patches on the target object and Texton like [21] features for patches from context layers. [sent-185, score-0.421]
</p><p>59 We design the global mask feature mdG and local mask feature mdL according to  (a)  (b)  (c)  (d)  (e)  Figure 3. [sent-196, score-0.568]
</p><p>60 Illustration of the impact of patch pair terms on hypothesis scoring. [sent-197, score-0.307]
</p><p>61 (b) Object centroid voting results without patch pair terms. [sent-199, score-0.692]
</p><p>62 (c) Object centroid voting results with patch pair terms added. [sent-200, score-0.692]
</p><p>63 (e) Shape voting results with patch pair terms added. [sent-202, score-0.614]
</p><p>64 For the local masks, the object layer has a positive 2D stump with 1/10th of the object size, while other layers have a negative 2D stump with the same size. [sent-209, score-0.486]
</p><p>65 Intuitively,  the active image patches from context layers help localize the object center but also indicate the local patches that do not belong to the object. [sent-210, score-0.533]
</p><p>66 In addition, we set the Gaussian blur parameter σdl such that the far away context layer has larger variances in terms of center prediction (3 times). [sent-211, score-0.299]
</p><p>67 We incorporate the object-context pair features into our structured Hough voting model by adding a second-order term to the score function: S(x, v) = Sc(x, v)+αSp(x, v), where α is the relative weight, and Sp is the object-context feature pair term. [sent-217, score-0.658]
</p><p>68 We also use depth information to prune out geometrically unstable or inconsistent codebook pairs as in the previous subsection. [sent-252, score-0.293]
</p><p>69 Joint inference for object localization Once the structured Hough voting model is trained with depth-augmented image data, we can apply it to 2D images for object detection and occlusion prediction. [sent-256, score-0.959]
</p><p>70 Our method infers the object center hypothesis and its visibility mask by maximizing the Hough score function S(x, v). [sent-257, score-0.853]
</p><p>71 However, due to the large hypothesis space of (x, v), it is difficult to use the original Hough voting approach, or conduct bruteforce search. [sent-258, score-0.589]
</p><p>72 Afrnaimleustraion fhowiterativeinfer nceupdatestheob-  ject centroid and supporting mask hypotheses. [sent-263, score-0.41]
</p><p>73 The first row on the right shows object centroid voting, with the corresponding supporting mask estimation in the second row. [sent-264, score-0.493]
</p><p>74 weighted Hough voting step and the local maxima xi∗ can be retrieved from the Hough map. [sent-265, score-0.517]
</p><p>75 It initializes the object center hypothesis with the original Hough voting method, and search object hypotheses at multiple scales. [sent-278, score-0.85]
</p><p>76 Dataset and setup We evaluate the proposed structured Hough voting method on two challenging RGB-D object datasets: the 111777999422  Berkeley 3D Object (B3DO) Dataset (Version 1) [8] and a subset of object classes on the NYU Depth Dataset (Version 2) [15]. [sent-295, score-0.729]
</p><p>77 As labeling of visibility masks is expensive to obtain, we assume only coarse-level labels for our masks. [sent-303, score-0.3]
</p><p>78 For evaluation of segmentation accuracy we also manually label the visibility ground-truth using polygons on the B3DO dataset. [sent-307, score-0.282]
</p><p>79 Model details For codebook generation, we randomly sample 200 patches per image from the visible part bounding box and generate 400 clusters for non-object patches using Kmeans, then rank them according to the patches’ offset variance. [sent-311, score-0.534]
</p><p>80 After object location and the corresponding visibility mask is inferred, we run GrabCut [19] in the bounding box specified by x to generate a final segmentation mask to utilize bottom-up image cues and examine segmentation performance. [sent-320, score-1.208]
</p><p>81 Based on the shape voting results, we set regions with highest responses as foreground seeds and regions with 1The modified labeling can be downloaded from ce c s . [sent-321, score-0.495]
</p><p>82 , single layer context (Single), 2D geometric context (2D), patch pair term off (P Off), and segmentation off (S Off). [sent-341, score-0.639]
</p><p>83 Simultaneously voting for local feature position and whole object hypothesis for v yields best segmentation results. [sent-358, score-0.767]
</p><p>84 In addition, we include a comparison with Hough voting using additional 2D geometric context, which uses 2D offset only in generating a single-layered contextual codebook. [sent-369, score-0.618]
</p><p>85 For modelling the object itself with a depth-encoded codebook, we also tried M2HT with a codebook learned with 3D offset, which did not work well due to noisy labels of object 111777999533  Figure7. [sent-370, score-0.361]
</p><p>86 With the depth-encoded contextual cues, the performance of our structured Hough voting model is improved significantly. [sent-376, score-0.618]
</p><p>87 All variants of our model which utilize depth supervision and contextual information achieved a minimum of 10% to 15% average precision increase, with further improvement by refining the  contextual model. [sent-377, score-0.373]
</p><p>88 Segmentation performance analysis Finally, we present a segmentation performance analysis with different mask terms enabled. [sent-386, score-0.354]
</p><p>89 We present the precision-recall of visibility mask at the point of 50% recall in object detection. [sent-387, score-0.645]
</p><p>90 Red boxes indicate correct detections, with segmentation mask overlaid. [sent-392, score-0.385]
</p><p>91 see that both local and global mask features help improve the segmentation performance. [sent-394, score-0.354]
</p><p>92 It is also clear that simultaneously voting for the local mask position and the whole object mask yields best segmentation performance. [sent-395, score-1.194]
</p><p>93 Conclusion In this paper, we have presented a novel structured  Hough voting model for indoor object detection and occlusion reasoning. [sent-397, score-0.814]
</p><p>94 We extend the original Hough voting based detection model by introducing a joint Hough space of object location and visibility pattern. [sent-398, score-0.874]
</p><p>95 The structured Hough model can naturally incorporate both the object and its context information, which is especially important for cluttered indoor scenes. [sent-399, score-0.349]
</p><p>96 In addition, we utilize depth information at the training stage to build a multilayer contextual model so that a better visual codebook is learned and more detailed object-context relationships can be captured. [sent-400, score-0.417]
</p><p>97 Fast prism: Branch and bound hough transform for object class detection. [sent-502, score-0.676]
</p><p>98 Efficient object detection and segmentation with a cascaded hough forest ism. [sent-549, score-0.803]
</p><p>99 Depth-encoded hough voting for joint object detection and shape recovery. [sent-588, score-1.194]
</p><p>100 Learning hough forest with depth-encoded context for object detection. [sent-603, score-0.789]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hough', 0.532), ('voting', 0.448), ('mask', 0.284), ('visibility', 0.212), ('codebook', 0.145), ('hypothesis', 0.141), ('layer', 0.131), ('patch', 0.125), ('context', 0.123), ('nyu', 0.113), ('object', 0.108), ('contextual', 0.105), ('depth', 0.102), ('layered', 0.096), ('patches', 0.095), ('mdg', 0.082), ('centroid', 0.078), ('occlusion', 0.073), ('codebooks', 0.071), ('segmentation', 0.07), ('maxima', 0.069), ('mdl', 0.068), ('layers', 0.067), ('detection', 0.067), ('ilj', 0.065), ('masks', 0.065), ('inference', 0.065), ('structured', 0.065), ('bounding', 0.065), ('score', 0.063), ('alternate', 0.057), ('votes', 0.056), ('berkeley', 0.055), ('indoor', 0.053), ('entry', 0.051), ('occluder', 0.049), ('bionic', 0.046), ('mig', 0.046), ('nick', 0.046), ('precsiion', 0.046), ('prune', 0.046), ('center', 0.045), ('ci', 0.042), ('box', 0.042), ('pair', 0.041), ('cil', 0.041), ('cjl', 0.041), ('xuming', 0.041), ('recall', 0.041), ('panel', 0.04), ('utilize', 0.04), ('joint', 0.039), ('australian', 0.039), ('grabcut', 0.039), ('offset', 0.039), ('gall', 0.037), ('kl', 0.037), ('leibe', 0.037), ('transform', 0.036), ('stump', 0.036), ('cues', 0.033), ('sp', 0.033), ('dl', 0.032), ('razavi', 0.032), ('boxes', 0.031), ('nicta', 0.031), ('clusters', 0.03), ('dpm', 0.029), ('reasoning', 0.029), ('scope', 0.029), ('iy', 0.029), ('workshops', 0.028), ('mil', 0.028), ('wb', 0.027), ('geometric', 0.026), ('ay', 0.026), ('forest', 0.026), ('alternating', 0.026), ('australia', 0.025), ('jl', 0.025), ('relationships', 0.025), ('localization', 0.025), ('readers', 0.025), ('texton', 0.025), ('ject', 0.025), ('curves', 0.025), ('bao', 0.024), ('seeds', 0.024), ('arc', 0.023), ('labeling', 0.023), ('visible', 0.023), ('di', 0.023), ('supporting', 0.023), ('poselet', 0.022), ('version', 0.022), ('weight', 0.022), ('precision', 0.021), ('quadratic', 0.021), ('brox', 0.021), ('specifies', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="256-tfidf-1" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>Author: Tao Wang, Xuming He, Nick Barnes</p><p>Abstract: Wepropose a structuredHough voting methodfor detecting objects with heavy occlusion in indoor environments. First, we extend the Hough hypothesis space to include both object location and its visibility pattern, and design a new score function that accumulates votes for object detection and occlusion prediction. In addition, we explore the correlation between objects and their environment, building a depth-encoded object-context model based on RGB-D data. Particularly, we design a layered context representation and .barne s }@ nict a . com .au (a)(b)(c) (d)(e)(f) allow image patches from both objects and backgrounds voting for the object hypotheses. We demonstrate that using a data-driven 2.1D representation we can learn visual codebooks with better quality, and more interpretable detection results in terms of spatial relationship between objects and viewer. We test our algorithm on two challenging RGB-D datasets with significant occlusion and intraclass variation, and demonstrate the superior performance of our method.</p><p>2 0.34294868 <a title="256-tfidf-2" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>3 0.20944169 <a title="256-tfidf-3" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>4 0.17784101 <a title="256-tfidf-4" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>Author: Byung-soo Kim, Shili Xu, Silvio Savarese</p><p>Abstract: In this paper we focus on the problem of detecting objects in 3D from RGB-D images. We propose a novel framework that explores the compatibility between segmentation hypotheses of the object in the image and the corresponding 3D map. Our framework allows to discover the optimal location of the object using a generalization of the structural latent SVM formulation in 3D as well as the definition of a new loss function defined over the 3D space in training. We evaluate our method using two existing RGB-D datasets. Extensive quantitative and qualitative experimental results show that our proposed approach outperforms state-of-theart as methods well as a number of baseline approaches for both 3D and 2D object recognition tasks.</p><p>5 0.16852716 <a title="256-tfidf-5" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>6 0.16331227 <a title="256-tfidf-6" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>7 0.14619794 <a title="256-tfidf-7" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>8 0.13243948 <a title="256-tfidf-8" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>9 0.12733766 <a title="256-tfidf-9" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>10 0.11741316 <a title="256-tfidf-10" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>11 0.11721116 <a title="256-tfidf-11" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>12 0.11302427 <a title="256-tfidf-12" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>13 0.11169723 <a title="256-tfidf-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.10865331 <a title="256-tfidf-14" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>15 0.10693184 <a title="256-tfidf-15" href="./cvpr-2013-A_Fully-Connected_Layered_Model_of_Foreground_and_Background_Flow.html">10 cvpr-2013-A Fully-Connected Layered Model of Foreground and Background Flow</a></p>
<p>16 0.10672682 <a title="256-tfidf-16" href="./cvpr-2013-Leveraging_Structure_from_Motion_to_Learn_Discriminative_Codebooks_for_Scalable_Landmark_Classification.html">268 cvpr-2013-Leveraging Structure from Motion to Learn Discriminative Codebooks for Scalable Landmark Classification</a></p>
<p>17 0.10506181 <a title="256-tfidf-17" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>18 0.10135339 <a title="256-tfidf-18" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>19 0.10108402 <a title="256-tfidf-19" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>20 0.098713398 <a title="256-tfidf-20" href="./cvpr-2013-Mesh_Based_Semantic_Modelling_for_Indoor_and_Outdoor_Scenes.html">284 cvpr-2013-Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.227), (1, 0.014), (2, 0.056), (3, -0.026), (4, 0.072), (5, 0.018), (6, 0.067), (7, 0.129), (8, -0.001), (9, -0.069), (10, -0.065), (11, -0.07), (12, 0.082), (13, 0.014), (14, 0.072), (15, -0.083), (16, -0.082), (17, 0.09), (18, -0.01), (19, 0.014), (20, 0.052), (21, -0.031), (22, 0.074), (23, -0.059), (24, 0.102), (25, -0.071), (26, -0.048), (27, -0.081), (28, 0.002), (29, -0.029), (30, -0.025), (31, -0.097), (32, 0.083), (33, 0.024), (34, 0.023), (35, -0.046), (36, 0.049), (37, -0.06), (38, 0.169), (39, 0.1), (40, -0.034), (41, 0.082), (42, 0.133), (43, 0.013), (44, 0.043), (45, 0.156), (46, -0.052), (47, 0.115), (48, 0.101), (49, 0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92469746 <a title="256-lsi-1" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>Author: Tao Wang, Xuming He, Nick Barnes</p><p>Abstract: Wepropose a structuredHough voting methodfor detecting objects with heavy occlusion in indoor environments. First, we extend the Hough hypothesis space to include both object location and its visibility pattern, and design a new score function that accumulates votes for object detection and occlusion prediction. In addition, we explore the correlation between objects and their environment, building a depth-encoded object-context model based on RGB-D data. Particularly, we design a layered context representation and .barne s }@ nict a . com .au (a)(b)(c) (d)(e)(f) allow image patches from both objects and backgrounds voting for the object hypotheses. We demonstrate that using a data-driven 2.1D representation we can learn visual codebooks with better quality, and more interpretable detection results in terms of spatial relationship between objects and viewer. We test our algorithm on two challenging RGB-D datasets with significant occlusion and intraclass variation, and demonstrate the superior performance of our method.</p><p>2 0.67196429 <a title="256-lsi-2" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>3 0.64078248 <a title="256-lsi-3" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>Author: Byung-soo Kim, Shili Xu, Silvio Savarese</p><p>Abstract: In this paper we focus on the problem of detecting objects in 3D from RGB-D images. We propose a novel framework that explores the compatibility between segmentation hypotheses of the object in the image and the corresponding 3D map. Our framework allows to discover the optimal location of the object using a generalization of the structural latent SVM formulation in 3D as well as the definition of a new loss function defined over the 3D space in training. We evaluate our method using two existing RGB-D datasets. Extensive quantitative and qualitative experimental results show that our proposed approach outperforms state-of-theart as methods well as a number of baseline approaches for both 3D and 2D object recognition tasks.</p><p>4 0.63919878 <a title="256-lsi-4" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>Author: Bojan Pepikj, Michael Stark, Peter Gehler, Bernt Schiele</p><p>Abstract: Despite the success of recent object class recognition systems, the long-standing problem of partial occlusion remains a major challenge, and a principled solution is yet to be found. In this paper we leave the beaten path of methods that treat occlusion as just another source of noise instead, we include the occluder itself into the modelling, by mining distinctive, reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular, we evaluate and compare models that range from standard object class detectors to hierarchical, part-based representations of occluder/occludee pairs. In an extensive evaluation we derive insights that can aid further developments in tackling the occlusion challenge. –</p><p>5 0.59543973 <a title="256-lsi-5" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>Author: Hsin-Yi Chen, Yen-Yu Lin, Bing-Yu Chen</p><p>Abstract: We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.</p><p>6 0.54032397 <a title="256-lsi-6" href="./cvpr-2013-3D-Based_Reasoning_with_Blocks%2C_Support%2C_and_Stability.html">1 cvpr-2013-3D-Based Reasoning with Blocks, Support, and Stability</a></p>
<p>7 0.5237329 <a title="256-lsi-7" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>8 0.52017409 <a title="256-lsi-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.51997578 <a title="256-lsi-9" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>10 0.51660383 <a title="256-lsi-10" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>11 0.51021475 <a title="256-lsi-11" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>12 0.501248 <a title="256-lsi-12" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>13 0.49854472 <a title="256-lsi-13" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>14 0.48694462 <a title="256-lsi-14" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>15 0.48213986 <a title="256-lsi-15" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>16 0.47534171 <a title="256-lsi-16" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>17 0.46728957 <a title="256-lsi-17" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>18 0.46437949 <a title="256-lsi-18" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>19 0.46392938 <a title="256-lsi-19" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>20 0.46335104 <a title="256-lsi-20" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.159), (16, 0.017), (26, 0.037), (28, 0.024), (30, 0.069), (33, 0.269), (39, 0.014), (57, 0.015), (67, 0.082), (69, 0.104), (80, 0.018), (87, 0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95548904 <a title="256-lda-1" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>2 0.9454428 <a title="256-lda-2" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>3 0.94429338 <a title="256-lda-3" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>4 0.94386661 <a title="256-lda-4" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>Author: Bo Zheng, Yibiao Zhao, Joey C. Yu, Katsushi Ikeuchi, Song-Chun Zhu</p><p>Abstract: In this paper, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.</p><p>5 0.9407537 <a title="256-lda-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.93799651 <a title="256-lda-6" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>same-paper 7 0.93797147 <a title="256-lda-7" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>8 0.93772668 <a title="256-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.93715489 <a title="256-lda-9" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>10 0.93623316 <a title="256-lda-10" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>11 0.93595082 <a title="256-lda-11" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>12 0.93593764 <a title="256-lda-12" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>13 0.9359256 <a title="256-lda-13" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>14 0.93583125 <a title="256-lda-14" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>15 0.93464977 <a title="256-lda-15" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>16 0.93463367 <a title="256-lda-16" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>17 0.93376541 <a title="256-lda-17" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>18 0.93313068 <a title="256-lda-18" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>19 0.93277675 <a title="256-lda-19" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>20 0.93187577 <a title="256-lda-20" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
