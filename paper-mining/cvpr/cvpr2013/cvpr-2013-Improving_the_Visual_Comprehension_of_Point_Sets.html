<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>218 cvpr-2013-Improving the Visual Comprehension of Point Sets</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-218" href="#">cvpr2013-218</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>218 cvpr-2013-Improving the Visual Comprehension of Point Sets</h1>
<br/><p>Source: <a title="cvpr-2013-218-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Katz_Improving_the_Visual_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Sagi Katz, Ayellet Tal</p><p>Abstract: Point sets are the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way.</p><p>Reference: <a title="cvpr-2013-218-reference" href="../cvpr2013_reference/cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. [sent-4, score-0.402]
</p><p>2 Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. [sent-5, score-0.461]
</p><p>3 This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. [sent-6, score-0.313]
</p><p>4 This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. [sent-7, score-0.255]
</p><p>5 In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way. [sent-8, score-0.385]
</p><p>6 These devices inherently produce 3D point sets from which, in some cases, 3D surfaces are reconstructed. [sent-11, score-0.173]
</p><p>7 Obviously, one reason is that when drawing all the points, both the fore and the back points are visible, which is confusing. [sent-17, score-0.264]
</p><p>8 Solving this problem by removing the back points is helpful, but some problems still remain (Figure 1(b)). [sent-18, score-0.232]
</p><p>9 After removing the hidden points from (a), only the visible points are presented in (b). [sent-28, score-0.63]
</p><p>10 Drawing every 5th visible point (c)  results in loss of the features. [sent-30, score-0.282]
</p><p>11 In (d), the result of our solution is shown: points on the silhouette and on subtle features, such as on the leaf-shaped relief on the horse’s back, are maintained. [sent-31, score-0.343]
</p><p>12 Our goal is to perform reduction of points, as viewed from a given viewpoint, such that the object’s fine features become apparent, thus improving the visual comprehension of the entire object, as demonstrated in Figure 1(d). [sent-32, score-0.286]
</p><p>13 The regions that are desired to appear bright, should have a lower density than regions that are desired to appear dark. [sent-33, score-0.3]
</p><p>14 The question is how we can characterize these regions and determine the appropriate subset of points. [sent-34, score-0.116]
</p><p>15 Given a point set P (193,252 points), the visible set of points PV is extracted. [sent-37, score-0.483]
</p><p>16 Then, silhouette points PSIL and pit points PPIT are extracted as subsets of PV. [sent-38, score-0.688]
</p><p>17 Some aim at accelerating surface reconstruction, by first reducing the number of input points, and then performing reconstruction on the reduced set [3, 6]. [sent-43, score-0.131]
</p><p>18 In [8], in addition to the visible points [5], silhouette points, extracted by local reconstruction, are rendered. [sent-46, score-0.514]
</p><p>19 In both cases, we are given a set of points and one specific point. [sent-52, score-0.201]
</p><p>20 HPR aims at detecting the visible (or hidden) points, seen from the viewpoint. [sent-53, score-0.201]
</p><p>21 Conversely, in TPO the goal is to find the subset of points that are visible to outside observers, positioned along the lines that connect these points with a given target point. [sent-54, score-0.773]
</p><p>22 Second, we show that, interestingly, the false negatives of the operators that approximate HPR and TPO, are the subset of points we wish to preserve for achieving good visual comprehension of the sampled shape. [sent-57, score-0.654]
</p><p>23 In particular, we  show how to extract points that belong to locally-concave regions by determining false negatives of the HPR operator. [sent-58, score-0.56]
</p><p>24 Similarly, we extract silhouette points by using false negatives of the TPO operator. [sent-59, score-0.534]
</p><p>25 First and foremost, we present a novel algorithm for meaningful view-dependent data reduction of point sets. [sent-61, score-0.117]
</p><p>26 Our algorithm is inherently simple and easy to implement, and is flexible in its ability to handle points in various dimensions. [sent-62, score-0.225]
</p><p>27 Second, we introduce the problem of targetpoint occlusion and present an operator that solves it in the limit and approximates it in the finite case. [sent-64, score-0.307]
</p><p>28 Finally, we suggest an algorithm for detecting the silhouettes of an object directly from the point cloud. [sent-65, score-0.163]
</p><p>29 Algorithm Outline Given a point set P and a viewpoint C, our algorithm aims at selecting a subset of P, which preserves the features and the patterns of the object (Figure 2). [sent-67, score-0.198]
</p><p>30 The naive approach of reducing the number of points by taking every kth point, blurs the silhouettes and the fine features, as it treats all the points similarly (Figure 1(c)). [sent-68, score-0.573]
</p><p>31 Dense regions appear dark, whereas sparse regions appear bright. [sent-70, score-0.198]
</p><p>32 Formally, we seek a visually-comprehensible subset PC of the visible points PV , PC ⊆ PV ⊆ P. [sent-71, score-0.452]
</p><p>33 hInav oer tdheer f tooll porwo-ing two properties: First, in order to highlight the outline of the object, the likelihood of a point pi ∈ PV to belong to PC should be proportional to the angle b∈etw Peen the surface normal and the line-of-sight. [sent-73, score-0.321]
</p><p>34 For instance, points on the silhouette should belong to PC. [sent-74, score-0.351]
</p><p>35 Second, to enhance the shading effect on the viewed point set, the likelihood of a point pi ∈ PV to belong to PC should be large if it belongs to a deep concavity and small if it belongs to a locally-convex region. [sent-75, score-0.379]
</p><p>36 This is based on the observation that deep concavi111222222  ties tend to be less illuminated, compared to convex regions, due to a reduced amount of light that reaches them. [sent-76, score-0.107]
</p><p>37 The first operator is the HPR operator of [5] (Section 3), used for approximating the set of visible points, PV, from a viewpoint. [sent-78, score-0.675]
</p><p>38 The second operator is used for approximating the set of occluding points from infinity (Section 4). [sent-79, score-0.689]
</p><p>39 Both operators have a curvature-related limitation, which is leveraged for extracting the following two subsets (Section 5): PSIL ⊂ PV is the set of points that are located near silhouette regions, as viewed from the given viewpoint. [sent-80, score-0.513]
</p><p>40 PPIT ⊂ PV is the set of points that are located near concave regions, as viewed from the viewpoint. [sent-81, score-0.353]
</p><p>41 Hence, PC contains points that are either near silhouette regions or near concave regions, as desired: PC  = PPIT  ∪ PSIL. [sent-83, score-0.492]
</p><p>42 The Hidden-Point Removal (HPR) Operator Given a set of points P and a viewpoint C, the goal is to find the set of points PV, which would be visible if the surface from which the points were sampled, were known. [sent-85, score-0.976]
</p><p>43 Determining the visibility of a point set is an intriguing problem, as points cannot occlude each other. [sent-86, score-0.488]
</p><p>44 The most common way to compute visibility is therefore to reconstruct the surface [2] and determine visibility on it. [sent-87, score-0.379]
</p><p>45 Since both rays and points are singular primitives, the algorithms assume either “thick” rays [13] or “finitearea” points [4, 12]. [sent-90, score-0.534]
</p><p>46 Unfortunately, assumptions need to be made regarding the thickness of the rays or the area of the points and normals must be estimated. [sent-91, score-0.295]
</p><p>47 Inversion: An inversion function maps every point pi ∈ P to an inverted domain. [sent-96, score-0.301]
</p><p>48 Many inversion functions  are possible ann idn vine e[5d] dFo(mpia)i =. [sent-97, score-0.12]
</p><p>49 Assuming, without loss of generality, that C is at the origin, the inversion function is defined as −rs  Fγ(pi) =? [sent-100, score-0.12]
</p><p>50 Convex hull construction: The convex hull of the transformed set of points and the viewpoint is calculated. [sent-106, score-0.48]
</p><p>51 The main result of [5] is that the points that reside on the convex hull of Step 2 are the images of the visible points. [sent-107, score-0.532]
</p><p>52 Intuitively, “how much” a point is visible, depends on the size of the empty region that lies between the point and the viewpoint. [sent-108, score-0.19]
</p><p>53 It is proved that points that fall on the convex hull in the inverted domain are those that are associated with large empty regions. [sent-110, score-0.392]
</p><p>54 While the HPR operator is powerful, deep concavities in the object result in false negatives (i. [sent-111, score-0.504]
</p><p>55 , points that should be visible but are not detected as such by the operator). [sent-113, score-0.402]
</p><p>56 This can be explained by the following lemma, which relates between the correct detection of the visible points to the curvature κ, the distance between the surface and the viewpoint d, and the angle β between the surface normal and the line of sight. [sent-114, score-0.7]
</p><p>57 1 Point p ∈ {HPRγ (P)} if p is visible and the Lcuermvamtuare 3 κ 1a tP p nsatt pisf ∈ies {:  κ <γ(1 −d γ()γs2isni(nβ2()β(c)o +s2 c(oβs)2 −(β γ)s)i23n2(β) . [sent-116, score-0.201]
</p><p>58 This lemma, whose proof for the inversion function that we use, is given in the appendix, implies that the HPR operator cannot resolve visibility in regions of high curvature. [sent-117, score-0.559]
</p><p>59 It is guaranteed to detect the points correctly only if the curvature κ is below a threshold, i. [sent-118, score-0.27]
</p><p>60 , the local surface is convex or the local surface is concave and the curvature is small enough. [sent-120, score-0.322]
</p><p>61 Given a set of points P and a target point C, the goal is to find the subset of P, which would occlude C from outside observers positioned at ∞, if the surface from which the points were sampled, dex aitst ∞ed,. [sent-124, score-0.868]
</p><p>62 In the HPR problem, the rays coming out ofviewpoint C, intersect the visible points. [sent-126, score-0.326]
</p><p>63 In the TPO problem, on the other hand, the rays come towards C from infinity and intersect the occluding points before reaching the target point. [sent-127, score-0.579]
</p><p>64 Hence, each of these points occludes C from an observer, which is located in the direction from which the ray is coming, but farther than the occluding point. [sent-128, score-0.508]
</p><p>65 Obviously, when a ray reaches C, the sample point on the ray is not occluded by any other point in this direction. [sent-129, score-0.317]
</p><p>66 1 Occluding point: Given a point set P and a target point C, a point pi ∈ P is an occluding point to C, 111222333  (a) From-point visibility (b) Target-point occlusion Figure 3. [sent-131, score-0.805]
</p><p>67 We are given a point set P (sampled from the gray surface) and a point C (blue). [sent-134, score-0.162]
</p><p>68 The black points are the visible points from C in (a) and the points occluding C from observers at infinity in (b). [sent-135, score-1.116]
</p><p>69 if pi is visible to some observer located in O where t >  suppj∈P  ? [sent-136, score-0.363]
</p><p>70 To approximate the solution to the target-point occlusion problem, we present an operator which, similarly to the HPR operator, is composed of two stages: (1) point transformation and (2) convex hull computation. [sent-139, score-0.517]
</p><p>71 In particular, in Equation (1), we use γ < 0 for from-point visibility and 0 < γ < 1for target-point occlusion; see Figure 4. [sent-141, score-0.149]
</p><p>72 The correctness of this operator is expressed in the following lemmas, whose proofs are provided in the supplementary material. [sent-142, score-0.224]
</p><p>73 Specifically, we show that in the limit, when the point set is an infinite sampling of the surface, the operator extracts the occluding subset accurately. [sent-143, score-0.547]
</p><p>74 Let P be an infinite sampling of a surface S, O ⊆ S be tLheet s Pet bofe points itnhiatet o scacmlupdlein tghe o target point SC, (O i. [sent-144, score-0.432]
</p><p>75 ⊆, th Se ground truth), and TPOγ (P) ⊆ S be the set of points extracted by our operator. [sent-146, score-0.201]
</p><p>76 , every point p marked occluding by the operator ⊆is Oind,e ie. [sent-150, score-0.471]
</p><p>77 2 When γ → +0, the set of points marked by the TLPeOm operator, hise equal to the set of occluding points. [sent-154, score-0.367]
</p><p>78 1 suggests that in the limit, the number of false positives of the operator is 0. [sent-156, score-0.311]
</p><p>79 2 adds that the number of false negatives is also 0 when γ → +0. [sent-158, score-0.221]
</p><p>80 ) We are particularly interested in understanding the locations where false negatives appear. [sent-160, score-0.221]
</p><p>81 0or1 from-point visibility & target-point occlusion for a 2D point set. [sent-167, score-0.286]
</p><p>82 Blue points were detected visible (occluding), while green points were detected as invisible (nonoccluding). [sent-168, score-0.603]
</p><p>83 3 p ∈ {TPOγ (P)} if p is visible to an observer positioned on t h∈e ray pO-C(P, farther away bthlean to any point einr  P, and the curvature  κ  at p satisfies  κ >γ(1 −d γ()γs2isni(nβ2()β(c)o +s2 c(oβs)2 −(β γ)s)i23n2(β) . [sent-171, score-0.558]
</p><p>84 Therefore, while the HPR operator is limited in its ability to correctly identify visible points around locally-concave regions, in respect to the viewpoint, the TPO is limited in its ability to correctly identify occluding points around locallyconvex regions. [sent-172, score-0.993]
</p><p>85 These dual limitations are the basis for our point reduction method. [sent-173, score-0.168]
</p><p>86 Controlled Reduction of Points Our goal is to extract a subset of the visible points, which satisfies the two requirements discussed in Section 2. [sent-175, score-0.275]
</p><p>87 Recall that we look for (1) points at locally-concave regions, which create a natural shading effect, and (2) points around the silhouettes regions, which by definition, have normals perpendicular to the line of sight. [sent-176, score-0.557]
</p><p>88 1, which states that when applying the HPR operator with γ < 0, the operator will produce false negatives around locally-concave regions. [sent-179, score-0.669]
</p><p>89 But these are precisely the points we seek after. [sent-180, score-0.201]
</p><p>90 We term these false negatives as pit points and define them as: Ppits = PV \ HPRγp (P). [sent-181, score-0.566]
</p><p>91 3, we learn that the TPO op-  erator will produce false negatives around locally-convex regions. [sent-183, score-0.221]
</p><p>92 These false negatives are the tip points, defined as: Ptips = PV \ TPOγt (PV). [sent-184, score-0.286]
</p><p>93 (3)  Since, as illustrated in Figure 4, the tip points need not necessarily be a subset of the visible points (e. [sent-185, score-0.718]
</p><p>94 of the bunny), we apply the TPO operator on the set of visible points, rather than on the entire point set. [sent-190, score-0.506]
</p><p>95 Since we wish to extract points with a small β (i. [sent-193, score-0.201]
</p><p>96 Due to the fact that this data consists of samples that are seen from a given viewpoint (scanner), it provides the visible point set from the scanner location. [sent-201, score-0.39]
</p><p>97 Therefore, in this case PV = P and the pit points are simply Ppits = P \ HPRγp (P) . [sent-202, score-0.345]
</p><p>98 Similarly, the set of silhouette points is defined as Psils = P \ TPOγt (P). [sent-203, score-0.313]
</p><p>99 Figure 5 shows the pit points and the tip points for a range data set, for γt = −γp, which are points on locallyconcave regions and on locally-convex regions, respectively. [sent-204, score-0.878]
</p><p>100 While the details of the facade of the church cannot be identified in the input, they are clearly perceived when rendering only the tip or only the pit points. [sent-205, score-0.209]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hpr', 0.495), ('tpo', 0.33), ('pv', 0.248), ('lemma', 0.235), ('operator', 0.224), ('visible', 0.201), ('points', 0.201), ('occluding', 0.166), ('comprehension', 0.155), ('visibility', 0.149), ('pit', 0.144), ('negatives', 0.134), ('ppit', 0.124), ('inversion', 0.12), ('silhouette', 0.112), ('psil', 0.093), ('false', 0.087), ('hull', 0.082), ('silhouettes', 0.082), ('point', 0.081), ('surface', 0.081), ('pc', 0.08), ('ppii', 0.076), ('observers', 0.074), ('infinity', 0.072), ('curvature', 0.069), ('pi', 0.067), ('viewpoint', 0.067), ('rays', 0.066), ('regions', 0.066), ('ray', 0.065), ('tip', 0.065), ('ppits', 0.062), ('psils', 0.062), ('occlusion', 0.056), ('katz', 0.055), ('observer', 0.054), ('positioned', 0.053), ('dual', 0.051), ('subset', 0.05), ('convex', 0.048), ('hide', 0.046), ('shading', 0.045), ('concave', 0.043), ('target', 0.043), ('devices', 0.043), ('technion', 0.042), ('located', 0.041), ('scanner', 0.041), ('removal', 0.038), ('fine', 0.038), ('belong', 0.038), ('bright', 0.036), ('density', 0.036), ('limitation', 0.036), ('reduction', 0.036), ('farther', 0.035), ('near', 0.035), ('determining', 0.034), ('leveraged', 0.034), ('deep', 0.034), ('desired', 0.033), ('inverted', 0.033), ('viewed', 0.033), ('appear', 0.033), ('occlude', 0.032), ('drawing', 0.032), ('intersect', 0.031), ('back', 0.031), ('conversely', 0.03), ('cc', 0.03), ('subtle', 0.03), ('subsets', 0.03), ('scanning', 0.029), ('normals', 0.028), ('coming', 0.028), ('empty', 0.028), ('controlling', 0.028), ('aitst', 0.028), ('agi', 0.028), ('chnion', 0.028), ('lemmas', 0.028), ('pni', 0.028), ('tdheer', 0.028), ('operators', 0.027), ('hidden', 0.027), ('limit', 0.027), ('outline', 0.026), ('approximating', 0.026), ('similarly', 0.026), ('infinite', 0.026), ('bunny', 0.025), ('intriguing', 0.025), ('concavities', 0.025), ('reconstruction', 0.025), ('reducing', 0.025), ('reaches', 0.025), ('sets', 0.025), ('goal', 0.024), ('inherently', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="218-tfidf-1" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>Author: Sagi Katz, Ayellet Tal</p><p>Abstract: Point sets are the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way.</p><p>2 0.10033117 <a title="218-tfidf-2" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>Author: Amy Tabb</p><p>Abstract: This paper considers the problem of reconstructing the shape ofthin, texture-less objects such as leafless trees when there is noise or deterministic error in the silhouette extraction step or there are small errors in camera calibration. Traditional intersection-based techniques such as the visual hull are not robust to error because they penalize false negative and false positive error unequally. We provide a voxel-based formalism that penalizes false negative and positive error equally, by casting the reconstruction problem as a pseudo-Boolean minimization problem, where voxels are the variables of a pseudo-Boolean function and are labeled occupied or empty. Since the pseudo-Boolean minimization problem is NP-Hard for nonsubmodular functions, we developed an algorithm for an approximate solution using local minimum search. Our algorithm treats input binary probability maps (in other words, silhouettes) or continuously-valued probability maps identically, and places no constraints on camera placement. The algorithm was tested on three different leafless trees and one metal object where the number of voxels is 54.4 million (voxel sides measure 3.6 mm). Results show that our . usda .gov (a)Orignalimage(b)SilhoueteProbabiltyMap approach reconstructs the complicated branching structure of thin, texture-less objects in the presence of error where intersection-based approaches currently fail. 1</p><p>3 0.093432888 <a title="218-tfidf-3" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>Author: Wanli Ouyang, Xingyu Zeng, Xiaogang Wang</p><p>Abstract: Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. The difficulty is added when several pedestrians overlap in images and occlude each other. We observe, however, that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the CaltechTrain dataset, the Caltech-Test dataset and the ETHdataset. Including mutual visibility leads to 4% −8% improvements on mluudlitnipglem ubteunaclh vmiasibrki ditayta lesaedtss.</p><p>4 0.079033084 <a title="218-tfidf-4" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>5 0.078865342 <a title="218-tfidf-5" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>Author: Michael Kolomenkin, Ilan Shimshoni, Ayellet Tal</p><p>Abstract: This paper extends to surfaces the multi-scale approach of edge detection on images. The common practice for detecting curves on surfaces requires the user to first select the scale of the features, apply an appropriate smoothing, and detect the edges on the smoothed surface. This approach suffers from two drawbacks. First, it relies on a hidden assumption that all the features on the surface are of the same scale. Second, manual user intervention is required. In this paper, we propose a general framework for automatically detecting the optimal scale for each point on the surface. We smooth the surface at each point according to this optimal scale and run the curve detection algorithm on the resulting surface. Our multi-scale algorithm solves the two disadvantages of the single-scale approach mentioned above. We demonstrate how to realize our approach on two commonly-used special cases: ridges & valleys and relief edges. In each case, the optimal scale is found in accordance with the mathematical definition of the curve.</p><p>6 0.077157229 <a title="218-tfidf-6" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>7 0.071465991 <a title="218-tfidf-7" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>8 0.07081835 <a title="218-tfidf-8" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>9 0.070747875 <a title="218-tfidf-9" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>10 0.069809608 <a title="218-tfidf-10" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>11 0.065513954 <a title="218-tfidf-11" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>12 0.063035011 <a title="218-tfidf-12" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>13 0.061866101 <a title="218-tfidf-13" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>14 0.061505616 <a title="218-tfidf-14" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<p>15 0.06100709 <a title="218-tfidf-15" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>16 0.059307877 <a title="218-tfidf-16" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>17 0.05781056 <a title="218-tfidf-17" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>18 0.057378851 <a title="218-tfidf-18" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>19 0.056938477 <a title="218-tfidf-19" href="./cvpr-2013-Nonlinearly_Constrained_MRFs%3A_Exploring_the_Intrinsic_Dimensions_of_Higher-Order_Cliques.html">308 cvpr-2013-Nonlinearly Constrained MRFs: Exploring the Intrinsic Dimensions of Higher-Order Cliques</a></p>
<p>20 0.054370496 <a title="218-tfidf-20" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, 0.087), (2, 0.008), (3, 0.019), (4, 0.017), (5, -0.054), (6, -0.025), (7, 0.01), (8, 0.018), (9, -0.009), (10, -0.034), (11, -0.03), (12, -0.026), (13, -0.055), (14, -0.032), (15, 0.004), (16, 0.008), (17, 0.058), (18, 0.032), (19, 0.045), (20, -0.026), (21, -0.037), (22, -0.01), (23, 0.001), (24, 0.019), (25, -0.025), (26, 0.034), (27, -0.04), (28, 0.02), (29, -0.005), (30, 0.074), (31, -0.002), (32, -0.012), (33, 0.063), (34, 0.017), (35, 0.007), (36, -0.015), (37, -0.047), (38, 0.02), (39, -0.042), (40, -0.036), (41, 0.013), (42, 0.051), (43, 0.012), (44, -0.002), (45, 0.06), (46, 0.02), (47, -0.01), (48, 0.003), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91333723 <a title="218-lsi-1" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>Author: Sagi Katz, Ayellet Tal</p><p>Abstract: Point sets are the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way.</p><p>2 0.65972352 <a title="218-lsi-2" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>Author: Kilho Son, Eduardo B. Almeida, David B. Cooper</p><p>Abstract: Thispaper introduces a novel approachfor reassembling pot sherds found at archaeological excavation sites, for the purpose ofreconstructing claypots that had been made on a wheel. These pots and the sherds into which they have broken are axially symmetric. The reassembly process can be viewed as 3D puzzle solving or generalized cylinder learning from broken fragments. The estimation exploits both local and semi-global geometric structure, thus making it a fundamental problem of geometry estimation from noisy fragments in computer vision and pattern recognition. The data used are densely digitized 3D laser scans of each fragment’s outer surface. The proposed reassembly system is automatic and functions when the pile of available fragments is from one or multiple pots, and even when pieces are missing from any pot. The geometric structure used are curves on the pot along which the surface had broken and the silhouette of a pot with respect to an axis, called axisprofile curve (APC). For reassembling multiple pots with or without missing pieces, our algorithm estimates the APC from each fragment, then reassembles into configurations the ones having distinctive APC. Further growth of configurations is based on adding remaining fragments such that their APC and break curves are consistent with those of a configuration. The method is novel, more robust and handles the largest numbers of fragments to date.</p><p>3 0.65454108 <a title="218-lsi-3" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>Author: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem</p><p>Abstract: Early work in computer vision considered a host of geometric cues for both shape reconstruction [11] and recognition [14]. However, since then, the vision community has focused heavily on shading cues for reconstruction [1], and moved towards data-driven approaches for recognition [6]. In this paper, we reconsider these perhaps overlooked “boundary” cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).</p><p>4 0.64885527 <a title="218-lsi-4" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>Author: Daniel Hauagge, Scott Wehrwein, Kavita Bala, Noah Snavely</p><p>Abstract: We present a method for computing ambient occlusion (AO) for a stack of images of a scene from a fixed viewpoint. Ambient occlusion, a concept common in computer graphics, characterizes the local visibility at a point: it approximates how much light can reach that point from different directions without getting blocked by other geometry. While AO has received surprisingly little attention in vision, we show that it can be approximated using simple, per-pixel statistics over image stacks, based on a simplified image formation model. We use our derived AO measure to compute reflectance and illumination for objects without relying on additional smoothness priors, and demonstrate state-of-the art performance on the MIT Intrinsic Images benchmark. We also demonstrate our method on several synthetic and real scenes, including 3D printed objects with known ground truth geometry.</p><p>5 0.60424531 <a title="218-lsi-5" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>Author: Yen H. Le, Uday Kurkure, Ioannis A. Kakadiaris</p><p>Abstract: Statistical shape models, such as Active Shape Models (ASMs), sufferfrom their inability to represent a large range of variations of a complex shape and to account for the large errors in detection of model points. We propose a novel method (dubbed PDM-ENLOR) that overcomes these limitations by locating each shape model point individually using an ensemble of local regression models and appearance cues from selected model points. Our method first detects a set of reference points which were selected based on their saliency during training. For each model point, an ensemble of regressors is built. From the locations of the detected reference points, each regressor infers a candidate location for that model point using local geometric constraints, encoded by a point distribution model (PDM). The final location of that point is determined as a weighted linear combination, whose coefficients are learnt from the training data, of candidates proposed from its ensemble ’s component regressors. We use different subsets of reference points as explanatory variables for the component regressors to provide varying degrees of locality for the models in each ensemble. This helps our ensemble model to capture a larger range of shape variations as compared to a single PDM. We demonstrate the advantages of our method on the challenging problem of segmenting gene expression images of mouse brain.</p><p>6 0.60382038 <a title="218-lsi-6" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>7 0.59893775 <a title="218-lsi-7" href="./cvpr-2013-Monocular_Template-Based_3D_Reconstruction_of_Extensible_Surfaces_with_Local_Linear_Elasticity.html">289 cvpr-2013-Monocular Template-Based 3D Reconstruction of Extensible Surfaces with Local Linear Elasticity</a></p>
<p>8 0.59755278 <a title="218-lsi-8" href="./cvpr-2013-Area_Preserving_Brain_Mapping.html">44 cvpr-2013-Area Preserving Brain Mapping</a></p>
<p>9 0.58598518 <a title="218-lsi-9" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>10 0.58262587 <a title="218-lsi-10" href="./cvpr-2013-Shape_from_Silhouette_Probability_Maps%3A_Reconstruction_of_Thin_Objects_in_the_Presence_of_Silhouette_Extraction_and_Calibration_Error.html">395 cvpr-2013-Shape from Silhouette Probability Maps: Reconstruction of Thin Objects in the Presence of Silhouette Extraction and Calibration Error</a></p>
<p>11 0.58148217 <a title="218-lsi-11" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>12 0.57991385 <a title="218-lsi-12" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<p>13 0.57991171 <a title="218-lsi-13" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>14 0.57035726 <a title="218-lsi-14" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>15 0.5637536 <a title="218-lsi-15" href="./cvpr-2013-Correspondence-Less_Non-rigid_Registration_of_Triangular_Surface_Meshes.html">97 cvpr-2013-Correspondence-Less Non-rigid Registration of Triangular Surface Meshes</a></p>
<p>16 0.55752188 <a title="218-lsi-16" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>17 0.55209237 <a title="218-lsi-17" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>18 0.55204356 <a title="218-lsi-18" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>19 0.55048877 <a title="218-lsi-19" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>20 0.54714632 <a title="218-lsi-20" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.138), (16, 0.022), (26, 0.031), (33, 0.246), (59, 0.012), (67, 0.03), (69, 0.078), (87, 0.09), (96, 0.264)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89605427 <a title="218-lda-1" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>Author: Stefan Harmeling, Michael Hirsch, Bernhard Schölkopf</p><p>Abstract: We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition. A real world experiment shows that we can super-resolve beyond the Rayleigh limit.</p><p>2 0.85051304 <a title="218-lda-2" href="./cvpr-2013-Is_There_a_Procedural_Logic_to_Architecture%3F.html">228 cvpr-2013-Is There a Procedural Logic to Architecture?</a></p>
<p>Author: Julien Weissenberg, Hayko Riemenschneider, Mukta Prasad, Luc Van_Gool</p><p>Abstract: Urban models are key to navigation, architecture and entertainment. Apart from visualizing fa ¸cades, a number of tedious tasks remain largely manual (e.g. compression, generating new fac ¸ade designs and structurally comparing fa c¸ades for classification, retrieval and clustering). We propose a novel procedural modelling method to automatically learn a grammar from a set of fa c¸ades, generate new fa ¸cade instances and compare fa ¸cades. To deal with the difficulty of grammatical inference, we reformulate the problem. Instead of inferring a compromising, onesize-fits-all, single grammar for all tasks, we infer a model whose successive refinements are production rules tailored for each task. We demonstrate our automatic rule inference on datasets of two different architectural styles. Our method supercedes manual expert work and cuts the time required to build a procedural model of a fa ¸cade from several days to a few milliseconds.</p><p>same-paper 3 0.83705604 <a title="218-lda-3" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>Author: Sagi Katz, Ayellet Tal</p><p>Abstract: Point sets are the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way.</p><p>4 0.82355481 <a title="218-lda-4" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>5 0.77063549 <a title="218-lda-5" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>Author: Paul Bodesheim, Alexander Freytag, Erik Rodner, Michael Kemmler, Joachim Denzler</p><p>Abstract: Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in com- prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.</p><p>6 0.76856124 <a title="218-lda-6" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>7 0.74797177 <a title="218-lda-7" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>8 0.74621814 <a title="218-lda-8" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>9 0.74427783 <a title="218-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.74327314 <a title="218-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>11 0.74206364 <a title="218-lda-11" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>12 0.74003208 <a title="218-lda-12" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>13 0.73931962 <a title="218-lda-13" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>14 0.73915136 <a title="218-lda-14" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>15 0.73844624 <a title="218-lda-15" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>16 0.73813587 <a title="218-lda-16" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>17 0.73734015 <a title="218-lda-17" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>18 0.73731208 <a title="218-lda-18" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>19 0.73720747 <a title="218-lda-19" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>20 0.73643547 <a title="218-lda-20" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
