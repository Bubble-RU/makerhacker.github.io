<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>314 cvpr-2013-Online Object Tracking: A Benchmark</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-314" href="#">cvpr2013-314</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>314 cvpr-2013-Online Object Tracking: A Benchmark</h1>
<br/><p>Source: <a title="cvpr-2013-314-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wu_Online_Object_Tracking_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>Reference: <a title="cvpr-2013-314-reference" href="../cvpr2013_reference/cvpr-2013-Online_Object_Tracking%3A_A_Benchmark_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Object tracking is one of the most important components in numerous applications of computer vision. [sent-5, score-0.439]
</p><p>2 By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field. [sent-9, score-0.438]
</p><p>3 Introduction Object tracking is one of the most important components in a wide range of applications in computer vision, such as surveillance, human computer interaction, and medical imaging [60, 12]. [sent-11, score-0.397]
</p><p>4 , position and size) of a target object in a frame of a video, the goal  of tracking is to estimate the states of the target in the subsequent frames. [sent-14, score-0.627]
</p><p>5 Although object tracking has been studied for several decades, and much progress has been made in recent years [28, 16, 47, 5, 40, 26, 19], it remains a very challenging problem. [sent-15, score-0.424]
</p><p>6 Numerous factors affect the performance of a tracking algorithm, such as illumination variation, occlusion, as well as background clutters, and there exists no single tracking approach that can successfully handle all scenarios. [sent-16, score-0.821]
</p><p>7 Therefore, it is crucial to evaluate the performance of state-of-the-art trackers to demonstrate their strength and weakness and help identify future research directions in this field for designing more robust algorithms. [sent-17, score-0.51]
</p><p>8 There exist several datasets for visual tracking in the surveillance scenarios, such as the VIVID [13], CAVIAR [21], and PETS databases. [sent-19, score-0.464]
</p><p>9 Although some tracking datasets [47, 5, 33] for generic scenes are annotated with bounding box, most of them are not. [sent-21, score-0.524]
</p><p>10 For sequences without labeled ground truth, it is difficult to evaluate tracking algorithms as the reported results are based on inconsistently annotated object locations. [sent-22, score-0.537]
</p><p>11 Recently, more tracking source codes have been made publicly available, e. [sent-23, score-0.397]
</p><p>12 However, the input and output formats of most trackers are different and thus it is inconve-  nient for large scale performance evaluation. [sent-26, score-0.526]
</p><p>13 In this work, we build a code library that includes most publicly available trackers and a test dataset with ground-truth annotations to facilitate the evaluation task. [sent-27, score-0.594]
</p><p>14 Additionally each sequence in the dataset is annotated with attributes that often affect tracking performance, such as occlusion, fast motion, and illumination variation. [sent-28, score-0.554]
</p><p>15 One common issue in assessing tracking algorithms is that the results are reported based on just a few sequences with different initial conditions or parameters. [sent-29, score-0.479]
</p><p>16 We use the precision plots based on location error metric and the success plots based on the overlap metric, to analyze the performance of each algorithm. [sent-34, score-0.562]
</p><p>17 We build a tracking dataset with 50 fully annotated sequences to facilitate tracking evaluation. [sent-36, score-0.969]
</p><p>18 We integrate most publicly available trackers in our code library with uniform input and output formats to facilitate large scale performance evaluation. [sent-38, score-0.635]
</p><p>19 The initial bounding boxes for tracking are sampled spatially and temporally to evaluate the robustness and characteristics of trackers. [sent-41, score-0.548]
</p><p>20 This work mainly focuses on the online1 tracking of single target. [sent-43, score-0.397]
</p><p>21 The code library, annotated dataset and all the tracking results are available on the website http://visualtracking. [sent-44, score-0.493]
</p><p>22 Related Work In this section, we review recent algorithms for object tracking in terms of several main modules: target representation scheme, search mechanism, and model update. [sent-47, score-0.494]
</p><p>23 In addition, some methods have been proposed that build on combing some trackers or mining context information. [sent-48, score-0.438]
</p><p>24 Object representation is one of major components in any visual tracker and numerous schemes have been presented [35]. [sent-50, score-0.332]
</p><p>25 Since the pioneering work of Lucas and Kanade [37, 8], holistic templates (raw intensity values) have been widely used for tracking [25, 39, 2]. [sent-51, score-0.489]
</p><p>26 Subsequently, subspace-based tracking approaches [11, 47] have been proposed to better account for appearance changes. [sent-52, score-0.438]
</p><p>27 Furthermore, Mei and Ling [40] proposed a tracking approach based on sparse representation to handle the corrupted appearance and recently it has  been further improved [41, 57, 64, 10, 55, 42]. [sent-53, score-0.474]
</p><p>28 In addition to template, many other visual features have been adopted in tracking algorithms, such as color histograms [16], histograms of oriented gradients (HOG) [17, 52], covariance region descriptor [53, 46, 56] and Haar-like features [54, 22]. [sent-54, score-0.494]
</p><p>29 Recently, the discriminative model has been widely adopted in tracking [15, 4], where a binary classifier is learned online to discriminate the target from the background. [sent-55, score-0.638]
</p><p>30 Numerous learning methods have been adapted to the tracking problem, such as SVM [3], structured output SVM [26], ranking SVM [7], boosting [4, 22], semiboosting [23] and multi-instance boosting [5]. [sent-56, score-0.502]
</p><p>31 To make trackers more robust to pose variation and partial occlusion, an object can be represented by parts where each one is represented by descriptors or histograms. [sent-57, score-0.479]
</p><p>32 When the tracking problem is posed within an optimization framework, assuming the objective function is differentiable with respect to the motion parameters, gradient descent methods can be used to locate the target efficiently [37, 16, 20, 49]. [sent-63, score-0.494]
</p><p>33 However, these objective functions are  1Here, the word online means during tracking only the information of previous few frames is used for inference at any time instance. [sent-64, score-0.539]
</p><p>34 [39] address the template update problem for the Lucas-Kanade algorithm [37] where the template is updated with the combination of the fixed reference template extracted from the first frame and the result from the most recent frame. [sent-71, score-0.29]
</p><p>35 Effective update algorithms have also been proposed via online mixture model [29], online boosting [22], and incremental subspace update [47]. [sent-72, score-0.329]
</p><p>36 To improve the tracking performance, some tracker fusion methods have been proposed recently. [sent-79, score-0.66]
</p><p>37 [48] proposed an approach that combines static, moderately adaptive and highly adaptive trackers to account for appearance changes. [sent-81, score-0.545]
</p><p>38 Even multiple trackers [34] or multiple feature sets [61] are maintained and selected in a Bayesian framework to better account for appearance changes. [sent-82, score-0.479]
</p><p>39 Evaluated Algorithms and Datasets For fair evaluation, we test the tracking algorithms whose original source or binary codes are publicly available as all implementations inevitably involve technical details and specific parameter settings2. [sent-84, score-0.427]
</p><p>40 Table 1shows the list of the evaluated tracking algorithms. [sent-85, score-0.397]
</p><p>41 We also evaluate the trackers in the VIVID testbed [13] including the mean shift (MS-V), template matching (TM-V), ratio shift (RS-V) and peak difference (PD-V) methods. [sent-86, score-0.543]
</p><p>42 There exist some datasets for the tracking in the surveillance scenario, such as the VIVID [13] and CAVIAR [21] datasets. [sent-88, score-0.437]
</p><p>43 To facilitate fair performance evaluation, we have collected and annotated most commonly used tracking sequences. [sent-97, score-0.52]
</p><p>44 Figure 1shows the first frame of each sequence where the target object is initialized with a bounding box. [sent-98, score-0.278]
</p><p>45 Evaluating trackers is difficult because many factors can affect the tracking performance. [sent-100, score-0.835]
</p><p>46 In addition, we evaluate the robustness of tracking algorithms in two aspects. [sent-115, score-0.45]
</p><p>47 One widely used evaluation metric on tracking precision is the center location error, which is defined as the average Euclidean distance between the center locations of the tracked targets and the manually labeled ground truths. [sent-117, score-0.53]
</p><p>48 However, when the tracker loses the target, the output location can be random and the average error value may not measure the tracking performance correctly [6]. [sent-119, score-0.66]
</p><p>49 Recently the precision plot [6, 27] has been adopted to measure the overall tracking performance. [sent-120, score-0.529]
</p><p>50 As the representative precision score for each tracker we use the score for the threshold = 20 pixels [6]. [sent-122, score-0.35]
</p><p>51 The  sequences are ordered based on our ranking results (See supplementary material): the ones on the top left are more difficult for tracking  than the ones on the bottom right. [sent-128, score-0.556]
</p><p>52 5) for tracker evaluation may not be fair or representative. [sent-136, score-0.34]
</p><p>53 Instead we use the area under curve (AUC) of each success plot to rank the tracking algorithms. [sent-137, score-0.536]
</p><p>54 The conventional way to evaluate trackers is to run them throughout a test sequence with initialization from the ground truth position in the first frame and report the average precision or success rate. [sent-139, score-0.745]
</p><p>55 However a tracker may be sensitive to the initialization, and its performance  with different initialization at a different start frame may become much worse or better. [sent-141, score-0.377]
</p><p>56 The proposed test scenarios happen a lot in the realworld applications as a tracker is often initialized by an object detector, which is likely to introduce initialization errors in terms of position and scale. [sent-148, score-0.379]
</p><p>57 In addition, an object detector may be used to re-initialize a tracker at different time instances. [sent-149, score-0.263]
</p><p>58 By investigating a tracker’s characteristic in the robustness evaluation, more thorough understanding and analysis of the tracking algorithm can be carried out. [sent-150, score-0.45]
</p><p>59 Given one initial frame together with the ground-truth bounding box of target, one tracker is initialized and runs to the end of the sequence, i. [sent-152, score-0.455]
</p><p>60 The tracker is evaluated on each segment, and the overall statistics are tallied. [sent-155, score-0.263]
</p><p>61 Table 1 lists the average FPS of each tracker in OPE running on a PC with Intel i7 3770 CPU (3. [sent-167, score-0.263]
</p><p>62 For SRE, each tracker is evaluated 12 times on each sequence, where more than 350,000 bounding box results are generated. [sent-171, score-0.381]
</p><p>63 For TRE, each sequence is partitioned into 20 segments and thus each tracker is performed on around 3 10,000 frames. [sent-172, score-0.301]
</p><p>64 Overall Performance The overall performance for all the trackers is summarized by the success and precision plots as shown in Fig222444111422  are presented for clarity and complete plots are in the supplementary material (best viewed on high-resolution display). [sent-177, score-1.009]
</p><p>65 For success plots, we use AUC scores to summarize and rank the trackers, while for precision plots we use the results at error threshold of 20 for ranking. [sent-179, score-0.356]
</p><p>66 In the precision plots, the rankings of some trackers are slightly different from the rankings in the success plots in that they are based on different metrics which measure different characteristics of trackers. [sent-180, score-0.847]
</p><p>67 Because the AUC score of success plot measures the overall performance which is more accurate than the score at one threshold of the plot, in the following we mainly analyze the rankings based on success plots but use the precision plots as auxiliary. [sent-181, score-0.744]
</p><p>68 As the trackers tend to perform well in shorter sequences, the average of all the results in TRE tend to be higher. [sent-183, score-0.438]
</p><p>69 The initialization errors tend to cause trackers to update with imprecise appearance information, thereby causing gradual drifts. [sent-185, score-0.601]
</p><p>70 In the success plots, the top ranked tracker SCM in OPE outperforms Struck by 2. [sent-186, score-0.364]
</p><p>71 The success plots of Struck in TRE and SRE show that the success rate of Struck is higher than SCM and ALSA when the overlap threshold is small, but less than SCM and ALSA when the overlap threshold is large. [sent-192, score-0.502]
</p><p>72 These trackers perform well in SRE and TRE, which suggests sparse representations are effective models to account for appearance change (e. [sent-195, score-0.515]
</p><p>73 The AUC score of ASLA deceases less than the other top 5 trackers from OPE to SRE and the ranking of ASLA also increases. [sent-200, score-0.477]
</p><p>74 The VTD and VTS methods adopt mixture models to improve the tracking performance. [sent-203, score-0.397]
</p><p>75 Attribute-based Performance Analysis By annotating the attributes of each sequence, we construct subsets with different dominant attributes which facilitates analyzing the performance of trackers for each challenging factor. [sent-208, score-0.586]
</p><p>76 Due to space limitations, we only illustrate and analyze the success plots and precision plots of SRE for attributes OCC, SV, and FM as shown in Figure 4, and more results are presented in the supplementary material. [sent-209, score-0.628]
</p><p>77 When an object moves fast, dense sampling based trackers (e. [sent-210, score-0.438]
</p><p>78 However, the stochastic search based trackers with high overall performance (e. [sent-214, score-0.438]
</p><p>79 These trackers can be further improved with dynam-  ×  ic models with more effective particle filters. [sent-218, score-0.493]
</p><p>80 The results show that trackers with affine motion models (e. [sent-222, score-0.438]
</p><p>81 Initialization with Different Scale It has been known that trackers are often sensitive to initialization variations. [sent-227, score-0.516]
</p><p>82 Figure 5 and Figure 6 show the summarized tracking performance with initialization at different scales. [sent-228, score-0.475]
</p><p>83 When computing the overlap score, we rescale the tracking results so that the performance summary could be comparable with the original scale, i. [sent-229, score-0.43]
</p><p>84 Figure 6 illustrates the average performance of all trackers for each scale which shows the performance often decreases significantly when the scale factor is large (e. [sent-232, score-0.516]
</p><p>85 This indicates these trackers are more sensitive to background clutters. [sent-240, score-0.465]
</p><p>86 Some trackers perform better when the scale factor is smaller, such as L1APG, MTT, LOT and CPF. [sent-241, score-0.477]
</p><p>87 On the other hand, some trackers perform well or even better when the initial bounding box is enlarged, such as Struck, OAB, SemiT, and BSBT. [sent-243, score-0.556]
</p><p>88 Concluding Remarks In this paper, we carry out large scale experiments to evaluate the performance of recent online tracking algorithms. [sent-247, score-0.54]
</p><p>89 Based on our evaluation results and observations, we highlight some tracking components which are essential for improving tracking performance. [sent-248, score-0.841]
</p><p>90 , Struck), or serving as the tracking context explicitly (e. [sent-252, score-0.397]
</p><p>91 Second, local models are important for tracking as shown in the performance improvement of local sparse representation (e. [sent-255, score-0.433]
</p><p>92 However, most of our evaluated trackers do not focus on this component. [sent-262, score-0.438]
</p><p>93 Good location prediction based on the dynamic model could reduce the search range and thus improve the tracking efficiency and robustness. [sent-263, score-0.397]
</p><p>94 The evaluation results show that significant progress in the field of object tracking has been made in the last decade. [sent-265, score-0.471]
</p><p>95 We propose and demonstrate evaluation metrics for in-depth analysis of tracking algorithms from several perspectives. [sent-266, score-0.444]
</p><p>96 This large scale performance evaluation facilitates better understanding of the state-of-the-art online object tracking approaches, and provides a platform for gauging new algorithms. [sent-267, score-0.587]
</p><p>97 Only the top 10 trackers are presented for clarity and complete plots are in the supplementary material (best viewed on high-resolution display). [sent-296, score-0.686]
</p><p>98 figure, the top 10 trackers are presented for clarity and complete plots are in the supplementary material. [sent-297, score-0.686]
</p><p>99 Performance summary for the trackers initialized with different size of bounding box. [sent-299, score-0.545]
</p><p>100 AVG (the last one) illustrates the average performance over all trackers for each scale. [sent-300, score-0.438]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trackers', 0.438), ('tracking', 0.397), ('tracker', 0.263), ('asla', 0.245), ('scm', 0.217), ('struck', 0.194), ('ope', 0.19), ('plots', 0.168), ('sre', 0.138), ('tre', 0.133), ('occ', 0.108), ('online', 0.104), ('success', 0.101), ('target', 0.097), ('mtt', 0.09), ('tld', 0.087), ('sequences', 0.082), ('ling', 0.081), ('initialization', 0.078), ('lsk', 0.073), ('vivid', 0.073), ('template', 0.07), ('bounding', 0.069), ('grabner', 0.066), ('mei', 0.066), ('kwon', 0.063), ('attributes', 0.061), ('cxt', 0.06), ('blasch', 0.06), ('wu', 0.06), ('annotated', 0.058), ('particle', 0.055), ('precision', 0.054), ('holistic', 0.054), ('robustness', 0.053), ('pami', 0.052), ('auc', 0.051), ('box', 0.049), ('alsa', 0.049), ('formats', 0.049), ('hanyang', 0.049), ('evaluation', 0.047), ('pets', 0.046), ('occlusion', 0.044), ('update', 0.044), ('circulant', 0.043), ('rankings', 0.043), ('clarity', 0.042), ('numerous', 0.042), ('appearance', 0.041), ('robust', 0.041), ('adopted', 0.04), ('ucme', 0.04), ('surveillance', 0.04), ('ranking', 0.039), ('scale', 0.039), ('frames', 0.038), ('sequence', 0.038), ('code', 0.038), ('rced', 0.038), ('clutters', 0.038), ('supplementary', 0.038), ('plot', 0.038), ('templates', 0.038), ('analyze', 0.038), ('initialized', 0.038), ('sparse', 0.036), ('oab', 0.036), ('feret', 0.036), ('library', 0.036), ('frame', 0.036), ('testbed', 0.035), ('caviar', 0.035), ('merced', 0.035), ('facilitate', 0.035), ('santner', 0.033), ('adaptive', 0.033), ('boosting', 0.033), ('sv', 0.033), ('overlap', 0.033), ('threshold', 0.033), ('shifts', 0.032), ('targets', 0.032), ('weakness', 0.031), ('lu', 0.031), ('shen', 0.031), ('tip', 0.031), ('covariance', 0.03), ('fair', 0.03), ('lucas', 0.029), ('porikli', 0.029), ('temporally', 0.029), ('saffari', 0.028), ('tuzel', 0.027), ('progress', 0.027), ('methodology', 0.027), ('visual', 0.027), ('background', 0.027), ('annotating', 0.026), ('leistner', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="314-tfidf-1" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>2 0.5098958 <a title="314-tfidf-2" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>3 0.44316724 <a title="314-tfidf-3" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>4 0.36811602 <a title="314-tfidf-4" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>Author: unkown-author</p><p>Abstract: We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the “right” frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.</p><p>5 0.35532752 <a title="314-tfidf-5" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>6 0.3025983 <a title="314-tfidf-6" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>7 0.23203981 <a title="314-tfidf-7" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>8 0.22764461 <a title="314-tfidf-8" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>9 0.21676174 <a title="314-tfidf-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.19901234 <a title="314-tfidf-10" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>11 0.18566667 <a title="314-tfidf-11" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>12 0.18272386 <a title="314-tfidf-12" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>13 0.15875451 <a title="314-tfidf-13" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>14 0.14208491 <a title="314-tfidf-14" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>15 0.13975856 <a title="314-tfidf-15" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>16 0.13559061 <a title="314-tfidf-16" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>17 0.12454033 <a title="314-tfidf-17" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>18 0.11255006 <a title="314-tfidf-18" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>19 0.10675165 <a title="314-tfidf-19" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>20 0.10379584 <a title="314-tfidf-20" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.232), (1, -0.027), (2, -0.023), (3, -0.123), (4, 0.037), (5, -0.056), (6, 0.226), (7, -0.219), (8, 0.174), (9, 0.337), (10, -0.178), (11, -0.186), (12, -0.153), (13, 0.187), (14, -0.054), (15, -0.047), (16, 0.03), (17, -0.049), (18, 0.081), (19, 0.075), (20, 0.081), (21, 0.027), (22, 0.119), (23, -0.089), (24, -0.084), (25, -0.003), (26, -0.064), (27, 0.057), (28, 0.077), (29, 0.065), (30, 0.028), (31, 0.011), (32, 0.052), (33, 0.036), (34, -0.028), (35, -0.034), (36, -0.004), (37, 0.044), (38, -0.01), (39, 0.083), (40, -0.007), (41, 0.027), (42, 0.012), (43, 0.029), (44, 0.064), (45, -0.016), (46, -0.058), (47, 0.071), (48, -0.012), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97166127 <a title="314-lsi-1" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>2 0.9155032 <a title="314-lsi-2" href="./cvpr-2013-Visual_Tracking_via_Locality_Sensitive_Histograms.html">457 cvpr-2013-Visual Tracking via Locality Sensitive Histograms</a></p>
<p>Author: Shengfeng He, Qingxiong Yang, Rynson W.H. Lau, Jiang Wang, Ming-Hsuan Yang</p><p>Abstract: This paper presents a novel locality sensitive histogram algorithm for visual tracking. Unlike the conventional image histogram that counts the frequency of occurrences of each intensity value by adding ones to the corresponding bin, a locality sensitive histogram is computed at each pixel location and a floating-point value is added to the corresponding bin for each occurrence of an intensity value. The floating-point value declines exponentially with respect to the distance to the pixel location where the histogram is computed; thus every pixel is considered but those that are far away can be neglected due to the very small weights assigned. An efficient algorithm is proposed that enables the locality sensitive histograms to be computed in time linear in the image size and the number of bins. A robust tracking framework based on the locality sensitive histograms is proposed, which consists of two main components: a new feature for tracking that is robust to illumination changes and a novel multi-region tracking algorithm that runs in realtime even with hundreds of regions. Extensive experiments demonstrate that the proposed tracking framework outper- , forms the state-of-the-art methods in challenging scenarios, especially when the illumination changes dramatically.</p><p>3 0.87192684 <a title="314-lsi-3" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>4 0.84000605 <a title="314-lsi-4" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>Author: Rui Yao, Qinfeng Shi, Chunhua Shen, Yanning Zhang, Anton van_den_Hengel</p><p>Abstract: Despite many advances made in the area, deformable targets and partial occlusions continue to represent key problems in visual tracking. Structured learning has shown good results when applied to tracking whole targets, but applying this approach to a part-based target model is complicated by the need to model the relationships between parts, and to avoid lengthy initialisation processes. We thus propose a method which models the unknown parts using latent variables. In doing so we extend the online algorithm pegasos to the structured prediction case (i.e., predicting the location of the bounding boxes) with latent part variables. To better estimate the parts, and to avoid over-fitting caused by the extra model complexity/capacity introduced by theparts, wepropose a two-stage trainingprocess, based on the primal rather than the dual form. We then show that the method outperforms the state-of-the-art (linear and non-linear kernel) trackers.</p><p>5 0.81267959 <a title="314-lsi-5" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>Author: Junseok Kwon, Kyoung Mu Lee</p><p>Abstract: We propose a novel tracking algorithm that robustly tracks the target by finding the state which minimizes uncertainty of the likelihood at current state. The uncertainty of the likelihood is estimated by obtaining the gap between the lower and upper bounds of the likelihood. By minimizing the gap between the two bounds, our method finds the confident and reliable state of the target. In the paper, the state that gives the Minimum Uncertainty Gap (MUG) between likelihood bounds is shown to be more reliable than the state which gives the maximum likelihood only, especially when there are severe illumination changes, occlusions, and pose variations. A rigorous derivation of the lower and upper bounds of the likelihood for the visual tracking problem is provided to address this issue. Additionally, an efficient inference algorithm using Interacting Markov Chain Monte Carlo is presented to find the best state that maximizes the average of the lower and upper bounds of the likelihood and minimizes the gap between two bounds simultaneously. Experimental results demonstrate that our method successfully tracks the target in realistic videos and outperforms conventional tracking methods.</p><p>6 0.81080598 <a title="314-lsi-6" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>7 0.79528564 <a title="314-lsi-7" href="./cvpr-2013-Least_Soft-Threshold_Squares_Tracking.html">267 cvpr-2013-Least Soft-Threshold Squares Tracking</a></p>
<p>8 0.7335611 <a title="314-lsi-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.72233403 <a title="314-lsi-9" href="./cvpr-2013-Learning_Compact_Binary_Codes_for_Visual_Tracking.html">249 cvpr-2013-Learning Compact Binary Codes for Visual Tracking</a></p>
<p>10 0.62980849 <a title="314-lsi-10" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>11 0.62388587 <a title="314-lsi-11" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>12 0.55224198 <a title="314-lsi-12" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>13 0.54698569 <a title="314-lsi-13" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>14 0.5258652 <a title="314-lsi-14" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>15 0.4828774 <a title="314-lsi-15" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>16 0.47557321 <a title="314-lsi-16" href="./cvpr-2013-Real-Time_Model-Based_Rigid_Object_Pose_Estimation_and_Tracking_Combining_Dense_and_Sparse_Visual_Cues.html">345 cvpr-2013-Real-Time Model-Based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues</a></p>
<p>17 0.4630805 <a title="314-lsi-17" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>18 0.43544775 <a title="314-lsi-18" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>19 0.42374852 <a title="314-lsi-19" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>20 0.37836894 <a title="314-lsi-20" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.235), (16, 0.013), (26, 0.043), (27, 0.011), (30, 0.169), (33, 0.215), (67, 0.114), (69, 0.038), (87, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87604719 <a title="314-lda-1" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>Author: Yi Wu, Jongwoo Lim, Ming-Hsuan Yang</p><p>Abstract: Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.</p><p>2 0.87080306 <a title="314-lda-2" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>Author: M. Zeeshan Zia, Michael Stark, Konrad Schindler</p><p>Abstract: Despite the success of current state-of-the-art object class detectors, severe occlusion remains a major challenge. This is particularly true for more geometrically expressive 3D object class representations. While these representations have attracted renewed interest for precise object pose estimation, the focus has mostly been on rather clean datasets, where occlusion is not an issue. In this paper, we tackle the challenge of modeling occlusion in the context of a 3D geometric object class model that is capable of fine-grained, part-level 3D object reconstruction. Following the intuition that 3D modeling should facilitate occlusion reasoning, we design an explicit representation of likely geometric occlusion patterns. Robustness is achieved by pooling image evidence from of a set of fixed part detectors as well as a non-parametric representation of part configurations in the spirit of poselets. We confirm the potential of our method on cars in a newly collected data set of inner-city street scenes with varying levels of occlusion, and demonstrate superior performance in occlusion estimation and part localization, compared to baselines that are unaware of occlusions.</p><p>3 0.86262637 <a title="314-lda-3" href="./cvpr-2013-Self-Paced_Learning_for_Long-Term_Tracking.html">386 cvpr-2013-Self-Paced Learning for Long-Term Tracking</a></p>
<p>Author: unkown-author</p><p>Abstract: We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the “right” frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time online (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.</p><p>4 0.85896492 <a title="314-lda-4" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>Author: Filippo Bergamasco, Andrea Albarelli, Emanuele Rodolà, Andrea Torsello</p><p>Abstract: Traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process. These considerations led to the definition of several ad hoc models that best adapt to different imaging devices, ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics. In this paper we dai s .unive . it ence points in the scene with their projections on the image plane [5]. Unfortunately, no real camera behaves exactly like an ideal pinhole. In fact, in most cases, at least the distortion effects introduced by the lens should be accounted for [19]. Any pinhole-based model, regardless of its level of sophistication, is geometrically unable to properly describe cameras exhibiting a frustum angle that is near or above 180 degrees. For wide-angle cameras, several different para- metric models have been proposed. Some of them try to modify the captured image in order to follow the original propose the use of an unconstrained model even in standard central camera settings dominated by the pinhole model, and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with it, resulting in a higher precision calibration than what is possible with the standard pinhole model with correction for radial distortion. This effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations. The benefit of such an unconstrained model to quasipinhole central cameras is supported by an extensive experimental validation.</p><p>5 0.85853833 <a title="314-lda-5" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>Author: Dohyung Seo, Jeffrey Ho, Baba C. Vemuri</p><p>Abstract: In this paper, we introduce a novel framework for computing a path of diffeomorphisms between a pair of input diffeomorphisms. Direct computation of a geodesic path on the space of diffeomorphisms Diff(Ω) is difficult, and it can be attributed mainly to the infinite dimensionality of Diff(Ω). Our proposed framework, to some degree, bypasses this difficulty using the quotient map of Diff(Ω) to the quotient space Diff(M)/Diff(M)μ obtained by quotienting out the subgroup of volume-preserving diffeomorphisms Diff(M)μ. This quotient space was recently identified as the unit sphere in a Hilbert space in mathematics literature, a space with well-known geometric properties. Our framework leverages this recent result by computing the diffeomorphic path in two stages. First, we project the given diffeomorphism pair onto this sphere and then compute the geodesic path between these projected points. Sec- ond, we lift the geodesic on the sphere back to the space of diffeomerphisms, by solving a quadratic programming problem with bilinear constraints using the augmented Lagrangian technique with penalty terms. In this way, we can estimate the path of diffeomorphisms, first, staying in the space of diffeomorphisms, and second, preserving shapes/volumes in the deformed images along the path as much as possible. We have applied our framework to interpolate intermediate frames of frame-sub-sampled video sequences. In the reported experiments, our approach compares favorably with the popular Large Deformation Diffeomorphic Metric Mapping framework (LDDMM).</p><p>6 0.85583526 <a title="314-lda-6" href="./cvpr-2013-GeoF%3A_Geodesic_Forests_for_Learning_Coupled_Predictors.html">186 cvpr-2013-GeoF: Geodesic Forests for Learning Coupled Predictors</a></p>
<p>7 0.84943044 <a title="314-lda-7" href="./cvpr-2013-Multi-image_Blind_Deblurring_Using_a_Coupled_Adaptive_Sparse_Prior.html">295 cvpr-2013-Multi-image Blind Deblurring Using a Coupled Adaptive Sparse Prior</a></p>
<p>8 0.84920472 <a title="314-lda-8" href="./cvpr-2013-Non-uniform_Motion_Deblurring_for_Bilayer_Scenes.html">307 cvpr-2013-Non-uniform Motion Deblurring for Bilayer Scenes</a></p>
<p>9 0.84624392 <a title="314-lda-9" href="./cvpr-2013-Part-Based_Visual_Tracking_with_Online_Latent_Structural_Learning.html">324 cvpr-2013-Part-Based Visual Tracking with Online Latent Structural Learning</a></p>
<p>10 0.84516162 <a title="314-lda-10" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>11 0.84194875 <a title="314-lda-11" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>12 0.8408649 <a title="314-lda-12" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>13 0.83826405 <a title="314-lda-13" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>14 0.83685255 <a title="314-lda-14" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>15 0.83421868 <a title="314-lda-15" href="./cvpr-2013-Handling_Noise_in_Single_Image_Deblurring_Using_Directional_Filters.html">198 cvpr-2013-Handling Noise in Single Image Deblurring Using Directional Filters</a></p>
<p>16 0.82992715 <a title="314-lda-16" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>17 0.82628232 <a title="314-lda-17" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>18 0.82594019 <a title="314-lda-18" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>19 0.823533 <a title="314-lda-19" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>20 0.82222289 <a title="314-lda-20" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
