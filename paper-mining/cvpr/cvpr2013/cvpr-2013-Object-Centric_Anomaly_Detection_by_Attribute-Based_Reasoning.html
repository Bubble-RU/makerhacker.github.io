<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-310" href="#">cvpr2013-310</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</h1>
<br/><p>Source: <a title="cvpr-2013-310-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Saleh_Object-Centric_Anomaly_Detection_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>Reference: <a title="cvpr-2013-310-reference" href="../cvpr2013_reference/cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. [sent-5, score-0.331]
</p><p>2 In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. [sent-6, score-0.638]
</p><p>3 We also show that abnormality predictions can help image categorization. [sent-8, score-0.518]
</p><p>4 We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities. [sent-9, score-0.554]
</p><p>5 Inspired by infant category learning, we propose to learn the structure of typical images using their attributes and then recognize abnormalities as special deviations from prototypical examples. [sent-18, score-0.622]
</p><p>6 Similar to infants’ learning, we want to reason about abnormalities by only observing typical instances. [sent-19, score-0.301]
</p><p>7 We believe that any reasoning about abnormalities should be based on understandings of normalities and should not require any observations about abnormal instances. [sent-22, score-0.832]
</p><p>8 We want to form category structures in terms of common attributes in categories and reason  about deviations from categories using attributes. [sent-30, score-0.441]
</p><p>9 An object can be abnormal due to the absence of typical attributes (a car without wheels) or the presence of atypical attributes (a car with wings). [sent-32, score-1.252]
</p><p>10 Also, abnormality can be caused by deviations from the extent by which an attribute varies inside a category (a furry dog). [sent-33, score-0.859]
</p><p>11 Furthermore, contextual irregularities and semantical peculiarities can also cause abnormalities such as an elephant in the room [32, 3 1]. [sent-34, score-0.321]
</p><p>12 What does studying abnormality in images tell us about object recognition? [sent-36, score-0.561]
</p><p>13 While being slower, humans seem to be able to recognize abnormalities and reason about category memberships of atypical instances without learning on any atypical instance [20]. [sent-37, score-0.617]
</p><p>14 Certain types of abnormality in images can be an indication of abnormal event. [sent-41, score-1.113]
</p><p>15 1) This is the first in-depth study of objects abnormalities that are stemmed from the object itself; 2) This paper provides an abnormality dataset for qualitative and quantitative analysis. [sent-43, score-0.755]
</p><p>16 Quantitative evaluation is tricky as the notion of abnormality is subjective. [sent-44, score-0.518]
</p><p>17 4) This paper shows a model to recognize abnormal images, reason about the category memberships for abnormal objects and also provide evidence beyond each ab777778888877555  normality prediction. [sent-46, score-1.457]
</p><p>18 The exact mechanism by which human learners determine typicality, or determine category membership as a function of typicality within a given category, is the main focus of most prominent theories of human categorization. [sent-51, score-0.298]
</p><p>19 Abnormality Detection: The problem of abnormality detection for single images is not really well explored. [sent-61, score-0.518]
</p><p>20 In this paper  we only focus on abnormality predictions. [sent-70, score-0.518]
</p><p>21 In terms of using abnormal instance, several methods need to observe abnormal examples to model abnormality. [sent-81, score-1.19]
</p><p>22 Our model predicts abnormality by reasoning about normality in terms of attributes. [sent-84, score-0.642]
</p><p>23 In [7] abnormalities due to the absence of typical attributes and presence of atypical attributes are explored. [sent-96, score-0.772]
</p><p>24 In this paper, we argue that the SVM scores are not the best typicality scores and show that by modeling the interaction between attributes and categories of typical objects one can compute a better normality score. [sent-98, score-0.612]
</p><p>25 Abnormality Dataset For the purpose of our study, we needed to collect an exploratory dataset of abnormal images. [sent-102, score-0.615]
</p><p>26 There are  datasets for studying abnormal activities in videos, however our goal is to study abnormalities in images. [sent-104, score-0.837]
</p><p>27 To collect the abnormal images in our dataset, we used image search engines, in particular Google images and Yahoo images where we searched for keywords like “Abnormal”, “Strange”, “Weird” and “Unusual” in combination with class labels like cars, airplanes, etc. [sent-106, score-0.647]
</p><p>28 Unlike typical images, it is not that easy to find abundance of abnormal images. [sent-108, score-0.62]
</p><p>29 Human Subject Experiments The subject of abnormality is rooted in people’s opinion, so any work on detecting strange images without any comparison to the human decision is not informative. [sent-114, score-0.625]
</p><p>30 2) Providing ground truth 3) Providing some insight about how people judge about the abnormality of images. [sent-117, score-0.518]
</p><p>31 2) Whether abnormality is because of the object itself or its relation to the scene. [sent-122, score-0.537]
</p><p>32 3) Rate the importance of each of the attributes in affecting their decision about normality (Color, Texture/Material, Shape/Part configuration, Object pose/viewing direction) 4) Also the subjects were asked to comment about context abnormality  if it is the case. [sent-123, score-0.907]
</p><p>33 3-top shows the subjects’ average rating for the different causes of abnormality for each category. [sent-125, score-0.575]
</p><p>34 This is for the images that subjects decide that the abnormality stems from the object itself. [sent-126, score-0.582]
</p><p>35 As figure 2 shows except for the airplane category, the variances in the ratings for each cause of abnormality is relatively small. [sent-128, score-0.575]
</p><p>36 Top: Subject’s rating of different sources of abnormality  variance which might indicate that the real reason for abnormality is not one of the four reasons given. [sent-564, score-1.141]
</p><p>37 This means normality affects the class distribution and consequently attribute distributions through classes. [sent-573, score-0.387]
</p><p>38 At inference, our task is to figure out if a given image contains an abnormal object or not. [sent-578, score-0.614]
</p><p>39 The attribute value given each category typically looks like a normal distribution. [sent-594, score-0.389]
</p><p>40 Therefore, we use a Gaussian distribution to model the distribution of each attribute classifier response given each class. [sent-595, score-0.33]
</p><p>41 On the other hand, the performance of attribute classifiers is not consistent across different attributes; some attributes are harder to learn than others. [sent-611, score-0.462]
</p><p>42 To measure attribute reliability, we compute the accuracy of attribute classifiers evaluated on a validation set. [sent-613, score-0.482]
</p><p>43 A measure of reliability can be defined as reliability(Ai) = acc(Ai), where acc(Ai) is the accuracy of the classifier for attribute Ai, which ranges between 0. [sent-614, score-0.317]
</p><p>44 The relevance factor, based on the conditional entropies, is computed during the training time on normal images and will appear as a fixed term for each combination of attributes and object classes. [sent-618, score-0.368]
</p><p>45 |Cj)  Attributes responsible for Abnormalities: Each abnormality prediction for an image can be supported by a set of abnormality causes in terms of attributes. [sent-620, score-1.101]
</p><p>46 However there are two possible reasons that can cause a given attribute to be surprising: either the attribute is typical within the class and is missing in the image, or the attribute is not typical to the class and exist in the image. [sent-623, score-0.868]
</p><p>47 Both cases will results in low attribute likelihood given the category and therefore, high surprise value. [sent-624, score-0.433]
</p><p>48 However it is very useful to discriminate between the two cases for abnormal attribute reporting. [sent-625, score-0.825]
</p><p>49 This function encodes absence of expected attributes and presence of unexpected attributes by projecting scores to the range −∞ to +∞ respectively . [sent-628, score-0.481]
</p><p>50 |Cj)  Abnormality Detection helps Object Categorization: Knowing that an object is abnormal along with the list of attributes that cause the abnormality should help categorizing that object. [sent-631, score-1.374]
</p><p>51 The normal category models are trained on the attributes of normal images. [sent-632, score-0.463]
</p><p>52 By discounting the abnormal attributes in category models, one can improve the categorization of abnormal images. [sent-633, score-1.535]
</p><p>53 More specifically, assume we train a linear classifier for each category of normal objects in the attribute space. [sent-634, score-0.42]
</p><p>54 Test set is a combination of normal images and abnormal images. [sent-648, score-0.689]
</p><p>55 For each class of objects we used an equal number of normal and abnormal images. [sent-650, score-0.721]
</p><p>56 The task of abnormality prediction is to label images in the test set as either normal or abnormal. [sent-652, score-0.657]
</p><p>57 The complement of this probability can be used as an abnormality score, denoted as ”Graphical model”. [sent-654, score-0.518]
</p><p>58 Evaluation of Abnormal Detection approaches (AUC) score is used to compute a robust version of P(Ai |Cj , N), taking the relevance and reliability of attribute into| Cconsideration. [sent-658, score-0.349]
</p><p>59 We compare our abnormality prediction with that of oneclass SVM, which is widely used for abnormality prediction [3]. [sent-660, score-1.126]
</p><p>60 We train a one-class SVM using attributes of positive examples from each object classes (in the normal image dataset). [sent-661, score-0.342]
</p><p>61 We used the confidence of these one-class SVM as scores of normality and measured its accuracy for abnormality prediction by AUC (normal vs abnormal classification). [sent-662, score-1.285]
</p><p>62 Adding the relevance term and attribute classifier reliability improves our original model. [sent-666, score-0.362]
</p><p>63 We also compared our method with an abnormality classifier trained on both normal and abnormal images. [sent-667, score-1.238]
</p><p>64 For this classifier(second row in table 1), we learn a two class SVM  on top of visual attributes to learn a boundary between normal and abnormal images. [sent-668, score-0.931]
</p><p>65 Normal images are selected from PASCAL train dataset and equal number of abnormal images have been chosen from abnormal dataset. [sent-669, score-1.19]
</p><p>66 Our model, without observing any instance of abnormal images, outperforms this baseline that is learned on both abnormal and normal images. [sent-670, score-1.306]
</p><p>67 Our abnormality score can also impose a ranking on abnormal images. [sent-673, score-1.131]
</p><p>68 Figure 6 shows ranked abnormal images for cars and boats. [sent-674, score-0.595]
</p><p>69 From left to right the abnormality of images increases. [sent-675, score-0.518]
</p><p>70 Abnormal Attribute Reporting After detecting an image as abnormal, we recognize its abnormality causes in terms of visual attributes. [sent-678, score-0.584]
</p><p>71 Our proposed graphical model assigns a surprise score for each attribute in an abnormal image. [sent-679, score-1.006]
</p><p>72 In the first step, we predict top categories for each abnormal image as its object class. [sent-681, score-0.656]
</p><p>73 As we discussed in Section 4, assuming an image belongs to a specific class, each attribute will have a surprise factor. [sent-682, score-0.368]
</p><p>74 Evaluation of abnormal attribute reporting gence from ground truth  -  KL diver-  prise factor with a negative sign for missing attributes and positive sign for unexpected ones. [sent-705, score-1.099]
</p><p>75 Figure 5 shows some abnormal images and their corresponding output of our model for the task of abnormal attribute reporting. [sent-706, score-1.42]
</p><p>76 We use ground truth rating from the MTurk responses to quantitatively evaluate our abnormal attribute reporting. [sent-708, score-0.889]
</p><p>77 1each abnormal image in our dataset, has a user score for four different causes of abnormality (Shape, Color, Texture and Pose). [sent-710, score-1.151]
</p><p>78 Since our model evaluates strangeness of attributes individually for an image, we grouped the attributes together based on their relatedness to each of these four cases. [sent-711, score-0.42]
</p><p>79 With this grouping, we can aggregate and normalize the scores for each abnormality cause. [sent-712, score-0.54]
</p><p>80 These surprising scores for each category of attributes can be compared to those we have in MTurk annotation. [sent-713, score-0.328]
</p><p>81 Table 2 reports Kullback-Leibler divergence between distribution of surprising scores for each abnormality cause made by our approach and the ground truth MTurk annotation. [sent-714, score-0.642]
</p><p>82 [7] finds an attribute abnormal, if its value goes beyond a range around the mean of that attribute value. [sent-718, score-0.46]
</p><p>83 In the first row of Table 2, an attribute is considered abnormal if its value is more than one standard deviation away  from the mean. [sent-719, score-0.825]
</p><p>84 In the the second row, an attribute is considered abnormal if its response is two standard deviations away from the mean. [sent-720, score-0.9]
</p><p>85 Abnormal Image Categorization Our task here is to evaluate how well different models can categorize abnormal images when they have only been trained on normal images. [sent-723, score-0.689]
</p><p>86 Just to provide a sense on how difficult these tasks are we used deformable part-based detectors of [10] as classifiers and check their performance on our test set of abnormal images. [sent-724, score-0.617]
</p><p>87 The knowledge of abnormality prediction can enhance the problem of object categorization for abnormal images. [sent-821, score-1.223]
</p><p>88 As indicated in Section 4 after the first run of object classification on abnormal images and predicting how normal sample of a specific class this image is. [sent-822, score-0.74]
</p><p>89 We detect abnormal attributes for the best possible class and adjust the value of its abnormal attributes by their average value given. [sent-823, score-1.642]
</p><p>90 For a given class of object, we get the mean response for an attribute by averaging over normal samples in PASCAL dataset. [sent-824, score-0.385]
</p><p>91 We re-run the same SVM classifier on abnormal images, but this time the effect of abnormal attributes for classification has been adjusted. [sent-826, score-1.431]
</p><p>92 Second row of Table 3 shows that by this refinement the distribution over different object classes for abnormal images gets more similar to what people have guessed about it. [sent-827, score-0.653]
</p><p>93 Last row in Table 3 refers to the case that each class has a surprising score given a set of attribute responses in an image, inverse of these surprising factors  for each object category shows the class-membership confidence. [sent-829, score-0.453]
</p><p>94 Evaluation of abnormal object categorization - KL divergence from ground truth 6. [sent-832, score-0.679]
</p><p>95 Conclusions In this paper we presented results of our investigation on the subject of abnormality in images. [sent-833, score-0.545]
</p><p>96 We introduced a dataset for abnormal images for quantitative evaluation along with human subjects’ ground truth. [sent-834, score-0.621]
</p><p>97 We also introduced a model to predict abnormality by reasonings in terms of attributes. [sent-835, score-0.518]
</p><p>98 We show improvements over standard baselines on abnormality prediction. [sent-836, score-0.518]
</p><p>99 For each abnormality prediction our model can also report its reasoning in terms of abnormal attributes. [sent-837, score-1.177]
</p><p>100 We show that we can improve abnormal image categorization by discounting for abnormal attributes. [sent-839, score-1.26]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('abnormal', 0.595), ('abnormality', 0.518), ('attribute', 0.23), ('abnormalities', 0.218), ('attributes', 0.21), ('typicality', 0.163), ('surprise', 0.138), ('cj', 0.128), ('atypical', 0.109), ('normality', 0.105), ('normal', 0.094), ('ai', 0.091), ('category', 0.065), ('reliability', 0.056), ('farhadi', 0.056), ('psychology', 0.055), ('irregularities', 0.052), ('mturk', 0.051), ('antonio', 0.051), ('categorization', 0.046), ('deviations', 0.046), ('relevance', 0.045), ('subjects', 0.045), ('prediction', 0.045), ('rareness', 0.044), ('typicalities', 0.044), ('categories', 0.042), ('car', 0.042), ('infants', 0.039), ('unexpected', 0.039), ('rating', 0.037), ('strange', 0.036), ('reason', 0.036), ('memberships', 0.033), ('class', 0.032), ('cause', 0.032), ('reasons', 0.032), ('classifier', 0.031), ('surprising', 0.031), ('prototypical', 0.03), ('babak', 0.03), ('jessica', 0.03), ('memorability', 0.03), ('response', 0.029), ('asked', 0.029), ('pascal', 0.029), ('recognize', 0.028), ('anomaly', 0.028), ('responses', 0.027), ('subject', 0.027), ('rutgers', 0.026), ('rosch', 0.026), ('human', 0.026), ('typical', 0.025), ('airplane', 0.025), ('svm', 0.025), ('graphical', 0.025), ('reporting', 0.025), ('tells', 0.024), ('discounting', 0.024), ('rut', 0.024), ('ivanov', 0.024), ('peaky', 0.024), ('studying', 0.024), ('auc', 0.023), ('wings', 0.023), ('surprised', 0.023), ('argue', 0.023), ('scores', 0.022), ('members', 0.022), ('observing', 0.022), ('stemming', 0.022), ('classifiers', 0.022), ('signed', 0.022), ('eric', 0.021), ('cognitive', 0.02), ('collect', 0.02), ('causes', 0.02), ('wheels', 0.02), ('ali', 0.02), ('distribution', 0.02), ('cognition', 0.02), ('classes', 0.019), ('humans', 0.019), ('contextual', 0.019), ('object', 0.019), ('motorbike', 0.019), ('prototypes', 0.019), ('divergence', 0.019), ('reasoning', 0.019), ('acc', 0.018), ('saliency', 0.018), ('detecting', 0.018), ('salient', 0.018), ('activity', 0.018), ('kl', 0.018), ('score', 0.018), ('theories', 0.018), ('engines', 0.018), ('mark', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000002 <a title="310-tfidf-1" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>2 0.26930487 <a title="310-tfidf-2" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>Author: Mehrsan Javan Roshtkhari, Martin D. Levine</p><p>Abstract: We present a novel approach for video parsing and simultaneous online learning of dominant and anomalous behaviors in surveillance videos. Dominant behaviors are those occurring frequently in videos and hence, usually do not attract much attention. They can be characterized by different complexities in space and time, ranging from a scene background to human activities. In contrast, an anomalous behavior is defined as having a low likelihood of occurrence. We do not employ any models of the entities in the scene in order to detect these two kinds of behaviors. In this paper, video events are learnt at each pixel without supervision using densely constructed spatio-temporal video volumes. Furthermore, the volumes are organized into large contextual graphs. These compositions are employed to construct a hierarchical codebook model for the dominant behaviors. By decomposing spatio-temporal contextual information into unique spatial and temporal contexts, the proposed framework learns the models of the dominant spatial and temporal events. Thus, it is ultimately capable of simultaneously modeling high-level behaviors as well as low-level spatial, temporal and spatio-temporal pixel level changes.</p><p>3 0.26345918 <a title="310-tfidf-3" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>4 0.19476843 <a title="310-tfidf-4" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>5 0.18133025 <a title="310-tfidf-5" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>6 0.16654772 <a title="310-tfidf-6" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>7 0.1594779 <a title="310-tfidf-7" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>8 0.1548934 <a title="310-tfidf-8" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>9 0.14786592 <a title="310-tfidf-9" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>10 0.14281867 <a title="310-tfidf-10" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>11 0.13368827 <a title="310-tfidf-11" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>12 0.12629497 <a title="310-tfidf-12" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>13 0.12537092 <a title="310-tfidf-13" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>14 0.12216728 <a title="310-tfidf-14" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>15 0.090099886 <a title="310-tfidf-15" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>16 0.086736478 <a title="310-tfidf-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.086578496 <a title="310-tfidf-17" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>18 0.083945967 <a title="310-tfidf-18" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>19 0.073082402 <a title="310-tfidf-19" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>20 0.068558328 <a title="310-tfidf-20" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, -0.105), (2, 0.0), (3, -0.032), (4, 0.078), (5, 0.087), (6, -0.24), (7, 0.062), (8, 0.089), (9, 0.184), (10, -0.029), (11, 0.063), (12, -0.029), (13, -0.021), (14, 0.07), (15, 0.048), (16, -0.016), (17, 0.017), (18, -0.034), (19, 0.069), (20, -0.006), (21, 0.019), (22, 0.001), (23, 0.006), (24, 0.026), (25, -0.014), (26, -0.026), (27, 0.034), (28, -0.0), (29, 0.0), (30, 0.012), (31, 0.029), (32, 0.014), (33, -0.024), (34, -0.013), (35, 0.026), (36, -0.009), (37, 0.008), (38, -0.043), (39, 0.018), (40, -0.023), (41, 0.001), (42, -0.01), (43, -0.005), (44, 0.021), (45, 0.055), (46, -0.021), (47, -0.034), (48, 0.031), (49, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94726926 <a title="310-lsi-1" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>2 0.92253232 <a title="310-lsi-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.90293729 <a title="310-lsi-3" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>4 0.89249057 <a title="310-lsi-4" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>5 0.84039485 <a title="310-lsi-5" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>Author: Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</p><p>Abstract: Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently, and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query, which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions without explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discrimina- tive binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improveperformance significantly over several strong baselines, while being an order of magnitude faster than exhaustively searching through all possible conjunctions.</p><p>6 0.83997738 <a title="310-lsi-6" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>7 0.81964707 <a title="310-lsi-7" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>8 0.79951537 <a title="310-lsi-8" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>9 0.7558738 <a title="310-lsi-9" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>10 0.71879071 <a title="310-lsi-10" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>11 0.69588387 <a title="310-lsi-11" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>12 0.68582726 <a title="310-lsi-12" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>13 0.60409099 <a title="310-lsi-13" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>14 0.55728424 <a title="310-lsi-14" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>15 0.51633924 <a title="310-lsi-15" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>16 0.50970948 <a title="310-lsi-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.48301592 <a title="310-lsi-17" href="./cvpr-2013-Bringing_Semantics_into_Focus_Using_Visual_Abstraction.html">73 cvpr-2013-Bringing Semantics into Focus Using Visual Abstraction</a></p>
<p>18 0.44791865 <a title="310-lsi-18" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>19 0.4360612 <a title="310-lsi-19" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>20 0.42546752 <a title="310-lsi-20" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.085), (16, 0.023), (26, 0.059), (28, 0.011), (33, 0.215), (65, 0.256), (67, 0.073), (69, 0.054), (77, 0.017), (87, 0.068), (99, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87940037 <a title="310-lda-1" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>Author: Ajay Kumar, Cyril Kwong</p><p>Abstract: In order to avail the benefits of higher user convenience, hygiene, and improved accuracy, contactless 3D fingerprint recognition techniques have recently been introduced. One of the key limitations of these emerging 3D fingerprint technologies to replace the conventional 2D fingerprint system is their bulk and high cost, which mainly results from the use of multiple imaging cameras or structured lighting employed in these systems. This paper details the development of a contactless 3D fingerprint identification system that uses only single camera. We develop a new representation of 3D finger surface features using Finger Surface Codes and illustrate its effectiveness in matching 3D fingerprints. Conventional minutiae representation is extended in 3D space to accurately match the recovered 3D minutiae. Multiple 2D fingerprint images (with varying illumination profile) acquired to build 3D fingerprints can themselves be used recover 2D features for further improving 3D fingerprint identification and has been illustrated in this paper. The experimental results are shown on a database of 240 client fingerprints and confirm the advantages of the single camera based 3D fingerprint identification.</p><p>2 0.83168548 <a title="310-lda-2" href="./cvpr-2013-Five_Shades_of_Grey_for_Fast_and_Reliable_Camera_Pose_Estimation.html">176 cvpr-2013-Five Shades of Grey for Fast and Reliable Camera Pose Estimation</a></p>
<p>Author: Adam Herout, István Szentandrási, Michal Zachariáš, Markéta Dubská, Rudolf Kajan</p><p>Abstract: We introduce here an improved design of the Uniform Marker Fields and an algorithm for their fast and reliable detection. Our concept of the marker field is designed so that it can be detected and recognized for camera pose estimation: in various lighting conditions, under a severe perspective, while heavily occluded, and under a strong motion blur. Our marker field detection harnesses the fact that the edges within the marker field meet at two vanishing points and that the projected planar grid of squares can be defined by a detectable mathematical formalism. The modules of the grid are greyscale and the locations within the marker field are defined by the edges between the modules. The assumption that the marker field is planar allows for a very cheap and reliable camera pose estimation in the captured scene. The detection rates and accuracy are slightly better compared to state-of-the-art marker-based solutions. At the same time, and more importantly, our detector of the marker field is several times faster and the reliable real-time detection can be thus achieved on mobile and low-power devices. We show three targeted applications where theplanarity is assured and where thepresented marker field design and detection algorithm provide a reliable and extremely fast solution.</p><p>3 0.8272866 <a title="310-lda-3" href="./cvpr-2013-Efficient_3D_Endfiring_TRUS_Prostate_Segmentation_with_Globally_Optimized_Rotational_Symmetry.html">139 cvpr-2013-Efficient 3D Endfiring TRUS Prostate Segmentation with Globally Optimized Rotational Symmetry</a></p>
<p>Author: Jing Yuan, Wu Qiu, Eranga Ukwatta, Martin Rajchl, Xue-Cheng Tai, Aaron Fenster</p><p>Abstract: Segmenting 3D endfiring transrectal ultrasound (TRUS) prostate images efficiently and accurately is of utmost importance for the planning and guiding 3D TRUS guided prostate biopsy. Poor image quality and imaging artifacts of 3D TRUS images often introduce a challenging task in computation to directly extract the 3D prostate surface. In this work, we propose a novel global optimization approach to delineate 3D prostate boundaries using its rotational resliced images around a specified axis, which properly enforces the inherent rotational symmetry of prostate shapes to jointly adjust a series of 2D slicewise segmentations in the global 3D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard, we propose a novel coupled continuous max-flow model, which not only provides a powerful mathematical tool to analyze the proposed optimization problem but also amounts to a new and efficient duality-basedalgorithm. Ex- tensive experiments demonstrate that the proposed method significantly outperforms the state-of-art methods in terms ofefficiency, accuracy, reliability and less user-interactions, and reduces the execution time by a factor of 100.</p><p>4 0.81108594 <a title="310-lda-4" href="./cvpr-2013-FrameBreak%3A_Dramatic_Image_Extrapolation_by_Guided_Shift-Maps.html">177 cvpr-2013-FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps</a></p>
<p>Author: Yinda Zhang, Jianxiong Xiao, James Hays, Ping Tan</p><p>Abstract: We significantly extrapolate the field of view of a photograph by learning from a roughly aligned, wide-angle guide image of the same scene category. Our method can extrapolate typical photos into complete panoramas. The extrapolation problem is formulated in the shift-map image synthesis framework. We analyze the self-similarity of the guide image to generate a set of allowable local transformations and apply them to the input image. Our guided shift-map method preserves to the scene layout of the guide image when extrapolating a photograph. While conventional shiftmap methods only support translations, this is not expressive enough to characterize the self-similarity of complex scenes. Therefore we additionally allow image transformations of rotation, scaling and reflection. To handle this in- crease in complexity, we introduce a hierarchical graph optimization method to choose the optimal transformation at each output pixel. We demonstrate our approach on a variety of indoor, outdoor, natural, and man-made scenes.</p><p>same-paper 5 0.80624056 <a title="310-lda-5" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>Author: Babak Saleh, Ali Farhadi, Ahmed Elgammal</p><p>Abstract: When describing images, humans tend not to talk about the obvious, but rather mention what they find interesting. We argue that abnormalities and deviations from typicalities are among the most important components that form what is worth mentioning. In this paper we introduce the abnormality detection as a recognition problem and show how to model typicalities and, consequently, meaningful deviations from prototypical properties of categories. Our model can recognize abnormalities and report the main reasons of any recognized abnormality. We also show that abnormality predictions can help image categorization. We introduce the abnormality detection dataset and show interesting results on how to reason about abnormalities.</p><p>6 0.77831566 <a title="310-lda-6" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>7 0.77185392 <a title="310-lda-7" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>8 0.76127708 <a title="310-lda-8" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>9 0.70451981 <a title="310-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.70431894 <a title="310-lda-10" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>11 0.70178443 <a title="310-lda-11" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>12 0.70002651 <a title="310-lda-12" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>13 0.69973356 <a title="310-lda-13" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>14 0.69943172 <a title="310-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.69922113 <a title="310-lda-15" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>16 0.6989935 <a title="310-lda-16" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>17 0.69828856 <a title="310-lda-17" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>18 0.6978628 <a title="310-lda-18" href="./cvpr-2013-Studying_Relationships_between_Human_Gaze%2C_Description%2C_and_Computer_Vision.html">416 cvpr-2013-Studying Relationships between Human Gaze, Description, and Computer Vision</a></p>
<p>19 0.6968022 <a title="310-lda-19" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>20 0.69650859 <a title="310-lda-20" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
