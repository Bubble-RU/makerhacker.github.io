<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-399" href="#">cvpr2013-399</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</h1>
<br/><p>Source: <a title="cvpr-2013-399-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhuang_Single-Sample_Face_Recognition_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>Reference: <a title="cvpr-2013-399-reference" href="../cvpr2013_reference/cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uedu  ,  ,  Abstract Single-sample face recognition is one of the most challenging problems in face recognition. [sent-10, score-0.657]
</p><p>2 We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. [sent-11, score-0.447]
</p><p>3 To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. [sent-13, score-1.008]
</p><p>4 The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. [sent-14, score-1.162]
</p><p>5 By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. [sent-15, score-1.149]
</p><p>6 In particular, the face alignment accuracy is  comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization. [sent-17, score-1.329]
</p><p>7 Hence, to develop a face recognition system whose performance can be comparable to or even exceed that of human vision, the computer system needs to address at least the following three closely related problems: First, it needs to effectively model the change of illumination on the human face. [sent-23, score-0.75]
</p><p>8 Third, it needs to tolerance the corruption of facial features that leads to potential gross pixel error against the training images. [sent-26, score-0.47]
</p><p>9 The framework is built on a subspace illumination model characterizing the distribution of a corruption-free face image sample (stacked in vector form) under a fixed pose, one subspace model per subject class [2, 1]. [sent-29, score-0.946]
</p><p>10 In the case of image corruption, since the corruption typically only affects a sparse set of pixel values, one can concurrently optimize a sparse error term in the image space to compensate for the corrupted pixel values. [sent-32, score-0.472]
</p><p>11 Hence, a face detection and registration step is typically first used to detect the face image. [sent-34, score-0.623]
</p><p>12 Most of the methods in face detection would learn 333555444644  a class of local image features/patches that are sensitive to the appearance of key facial features [27, 23, 17]. [sent-35, score-0.473]
</p><p>13 Using  either an active shape model [5] or an active appearance model [4], the location of the face can be detected even when the expression of the face is not neutral or some facial features are occluded [21, 12]. [sent-36, score-0.82]
</p><p>14 However, using these face registration algorithms alone is not sufficient to align a query image to training images for SRC. [sent-37, score-0.648]
</p><p>15 Following the sparse representation framework in [26, 24], we propose a novel algorithm to effectively extend SRC for face alignment and recognition in the small sample set scenario. [sent-40, score-0.729]
</p><p>16 We observe that in addition to the wellunderstood image nuisances aforementioned, one of the remaining challenges in face recognition is indeed the small sample set problem. [sent-41, score-0.403]
</p><p>17 For instance, in many biometric, surveillance, and Internet applications, there may be only a few training examples per subject that are collected in the wild, and the subjects of interest may not be able to undergo an extended image collection session in a laboratory. [sent-42, score-0.49]
</p><p>18 For starters, the original SRC algorithm [26] assumes a plurality of training samples from each class must sufficiently span its illumination subspace. [sent-44, score-0.506]
</p><p>19 In [24], in order to guarantee the training images contain sufficient illumination patterns, the test subjects must further go through a nontrivial passport-style image collection process in a dark room in order to be entered into the training database. [sent-46, score-0.608]
</p><p>20 More recently, another development in the SRC framework is simultaneous face alignment and recognition methods [28, 15, 30]. [sent-47, score-0.659]
</p><p>21 Nevertheless, these methods did not go beyond the basic assumption used in SRC and other prior art that the face illumination model is measured by a plurality of training samples for each class. [sent-48, score-0.756]
</p><p>22 These more severe conditions can be addressed in the face detection stage using more sophisticated face models as we mentioned above. [sent-51, score-0.659]
</p><p>23 Therefore, simultaneous face alignment and recognition could make the already expensive sparse optimization problem even more difficult to solve. [sent-53, score-0.724]
</p><p>24 Contributions Single-sample face alignment and recognition represents an important step towards practical face recognition solutions using images collected in the wild or on the Internet. [sent-56, score-1.039]
</p><p>25 The key observation is that one sample per class mainly deprives the algorithm of an illumination subspace model for each individual class. [sent-58, score-0.478]
</p><p>26 We show that a sparse illumination transfer (SIT) dictionary can be constructed to compensate the lack of the illumination information in the training set. [sent-59, score-1.197]
</p><p>27 Due to the fact that most human faces have similar shapes, only one subject is often sufficient to provide images of different illumination patterns, although adding more subjects may further improve the accuracy. [sent-60, score-0.596]
</p><p>28 The subject(s) for illumination transfer can be selected outside the set of training subjects for recognition. [sent-61, score-0.601]
</p><p>29 Finally, we show that the other image nuisances, including pose variation and image corruption, can be readily corrected by a single reference image ofarbitrary illumination condition per class combined with the SIT dictionary. [sent-62, score-0.525]
</p><p>30 The SIT dictionary also does not need to know the information of any possible facial corruption for the algorithm to be robust. [sent-63, score-0.519]
</p><p>31 To the best of our knowledge, this work is the first to propose a solution to perform facial illumination compensation in the alignment stage and illumination and pose transfer in the recognition stage. [sent-64, score-1.359]
</p><p>32 In terms of the algorithm complexity, the construction of  the SIT dictionary is extremely simple when the illumination data of the SIT subject(s) are provided, and it does not necessarily involve any dictionary learning algorithm. [sent-65, score-0.728]
</p><p>33 The algorithm is also fast to execute in the alignment and recognition stages compared to the other SRC-type algorithms because a sparse optimization solver such as those in [29] is now faced with much smaller linear systems. [sent-66, score-0.444]
</p><p>34 Our work differs from [6] in that the proposed SIT dictionary can be constructed from a selection of independent subject(s) only for the purpose of illumination transfer. [sent-68, score-0.559]
</p><p>35 Furthermore, by transferring both the pose and illumination from the alignment stage to the recognition stage, our algorithm can handle insufficient illumination and misalignment at the same time, and allows for the single reference images to have arbitrary illumination conditions. [sent-70, score-1.637]
</p><p>36 Finally, our algorithm is also robust to moderate amounts of image pixel corruption, even though we do not need to include any image corruption examples in the SIT 333555444755  dictionary, while in [6] the intraclass variant dictionary uses both normal and corrupted face samples. [sent-71, score-0.867]
</p><p>37 In the training stage, given L training subject classes, assume ni well-aligned training images Ai = [ai,1, ai,2 , · · · , ai,ni] ∈ Rd×ni of the same dimension as b are sampled ·f·o·r ,thae i-th] ∈cla Rss under the frontal position and various illumination conditions. [sent-76, score-0.745]
</p><p>38 Based on the illumination subspace assumption, if b belongs to the i-th class, then b lies in the low-dimensional subspace spanned by the training images in Ai, namely, b = Aixi . [sent-82, score-0.537]
</p><p>39 (1)  In the query stage, the query image b may contain an unknown 3D pose that is different from the neutral position. [sent-83, score-0.392]
</p><p>40 The goal of the alignment is to recover the transformation τ, such that an unwarped query image b0 of the same subject  =. [sent-85, score-0.631]
</p><p>41 In robust face alignment, the issue is oft=en b fu◦rτth =er exacerbated by the cascade of complex illumination patterns and moderate image pixel corruption and occlusion. [sent-87, score-0.958]
</p><p>42 to  b ◦ τi = Aixi + e, (2)  where the alignment is achieved on a per-class basis for each Ai, and e ∈ Rd is the sparse alignment error as the objective afnudnc etio ∈n. [sent-91, score-0.629]
</p><p>43 In [24], it was shown that the alignment based on (2) can tolerate translation shift up to 20% of the between-eye distance and up to 30◦ in-plane rotation, which is typically sufficient to compensate moderate misalignment caused by a good face detector. [sent-94, score-0.809]
</p><p>44 Once the optimal transformation τi is recovered for each class i, the transformation is carried over to the recognition algorithm, where the training images in each Ai are transformed by τi−1 to align with the query image b. [sent-95, score-0.468]
</p><p>45 Single-Sample Alignment In this section, we first propose a novel face alignment algorithm that is effective even when a very small number of training images are provided per class. [sent-116, score-0.72]
</p><p>46 In the extreme case, we specifically consider the single-sample face alignment problem where only one training image ai of arbitrary illumination is available from Class i. [sent-117, score-1.088]
</p><p>47 To mitigate the scarcity of the training images, something has to give to recover the missing illumination model under which the image appearance of a human face can be affected. [sent-119, score-0.764]
</p><p>48 Motivated by the idea of transfer learning [7, 20, 16], we stipulate that one can obtain the illumination information for both alignment and recognition from a set of additional subject classes, called the illumination dictionary. [sent-120, score-1.277]
</p><p>49 The additional face images have the same frontal pose as the training images, and can be collected offline and can be different from the query classes A = [A1, · · · , AL] . [sent-121, score-0.658]
</p><p>50 In other words, no matter how scarce the training images of the query classes are, one can always obtain a potentially  large set of additional face images of unrelated subjects who may have similar face shapes as the query subjects and may provide sufficient illumination examples. [sent-122, score-1.588]
</p><p>51 The illumination dictionary for an additional class L + 1 is defined as follows. [sent-123, score-0.588]
</p><p>52 Assume face images of sufficient illumination patterns (aL+1,1 , aL+1,2 , · · · , aL+1,n) (c1, c2 , · · · , cn) are samples from the cla,s··s,· f ,uarther assume all images ·in , vcector form are normalized to have unit length. [sent-124, score-0.748]
</p><p>53 Then the illumination dictionary by the (L + 1)-th subject can be written as the difference of two face images of the same shape:  =. [sent-125, score-0.981]
</p><p>54 (4)  The multiplication of C1y by vector y can further generate more complex illumination patterns that involve multiple images in the columns of C1. [sent-127, score-0.392]
</p><p>55 The focus of this paper is not on the illumination transfer function per se, but how its application on face images can enable single-sample alignment and recognition under the SRC framework. [sent-130, score-1.159]
</p><p>56 In addition, the illumination transfer shown later in (5) can be solved by efficient ? [sent-131, score-0.466]
</p><p>57 Another issue with the illumination dictionary is that, if additional subject classes beyond L + 1 are provided, one can continue to construct additional dictionaries C = [C1, C2 , · · · ] . [sent-136, score-0.723]
</p><p>58 ve H doiwsecvoevre,re ad s during our experiment aisl tohbaste rifthe first dictionary C1 is carefully chosen, a single additional subject class is sufficient to achieve extremely good performance for face alignment and recognition. [sent-138, score-0.995]
</p><p>59 In Section 4, we will show that using a single illumination class, our alignment accuracy using only one reference image is comparable to that of [24] using multiple reference images, and the subsequent recognition accuracy further exceeds those using manual alignment results. [sent-139, score-1.15]
</p><p>60 Clearly, this singular subject needs to have the facial appearance that is close to the “mean face,” which has been used in face recognition to refer to the average appearance of faces over a population [2]. [sent-140, score-0.643]
</p><p>61 4, we will examine the efficacy of designing different illumination dictionaries with more subjects. [sent-144, score-0.406]
</p><p>62 Nevertheless, given the limited number of training images in practice, the illumination dictionary itself also cannot be arbitrarily large. [sent-147, score-0.628]
</p><p>63 Therefore, an effective solution should be able to achieve accurate alignment while only relying on a few illumination samples. [sent-148, score-0.632]
</p><p>64 Our solution is called sparse illumination transfer (SIT):  subj. [sent-149, score-0.531]
</p><p>65 Thesolid  red boxes are the initial face locations provided by a face detector. [sent-161, score-0.618]
</p><p>66 Right: The subject image has 30% of the face pixels corrupted by random noise. [sent-164, score-0.454]
</p><p>67 Similar to the above alignment algorithm, the algorithm also applies trivially when multiple training samples per class are available. [sent-168, score-0.427]
</p><p>68 Given the same reference image ai as in (5), again we assume ai is sampled from a random illumination condition. [sent-169, score-0.553]
</p><p>69 The key idea of our algorithm is to transfer and apply the estimated image transformation τi and the SIT compensation Cyi directly from the alignment step (5) to the recognition step. [sent-170, score-0.529]
</p><p>70 (6)  The modified reference image a˜i aligns the orientation of ai towards the query image, and at the same time adjusts the appearance of ai to take into account the transferred illumination model Cyi. [sent-173, score-0.726]
</p><p>71 Right: Warped reference a˜i has closer pose and illumination to b than the original image ai. [sent-193, score-0.446]
</p><p>72 The illumination dictionary is constructed from YaleB face database [10]. [sent-216, score-0.858]
</p><p>73 YaleB contains 5760 single light source image of 10 subjects under 9 poses and 64 illumination conditions. [sent-217, score-0.419]
</p><p>74 For every subject in a particular pose, an image with ambient (background) illumination was also captured. [sent-218, score-0.52]
</p><p>75 In our experiments, we only use the first subject with its 65 aligned frontal images (64 illuminations + 1 ambient) to construct our illumination dictionary. [sent-219, score-0.571]
</p><p>76 The dictionary C is constructed by subtracting the ambient image from the other 64 illumination image. [sent-220, score-0.609]
</p><p>77 All the training face images are manually cropped into 60 60 pixels based on the locations of eyes out-corner points, ×an6d0 th piex edlisst banasceed b oentw theeen lo tchaeti townos oofu etyeer eye corners is normalized to be 50 pixels for each person. [sent-228, score-0.45]
</p><p>78 We again emphasize that our experimental setting is more practical than those used in some other publications, as we allow the training images to have arbitrary illumination and not necessarily just the ambient illumination. [sent-229, score-0.509]
</p><p>79 We compare our algorithms with several state-of-the-art face alignment and recognition algorithms under the SRC framework. [sent-230, score-0.68]
</p><p>80 333555445088  For the recognition benchmark, we compare with DSRC, MRR based on the above automatic alignment results to find face regions. [sent-232, score-0.64]
</p><p>81 the alignment error obtained by aligning a query image from the manually labeled position to the training images. [sent-243, score-0.502]
</p><p>82 SIT slightly outperforms DSRC-7, where DSRC-7 has access to seven training images of different illumination conditions. [sent-264, score-0.439]
</p><p>83 It validates that illumination examples of a well-chosen subject are sufficient for SIT alignment. [sent-266, score-0.504]
</p><p>84 In particular, the illumination transfer (6) can be easily adopted by the other algorithms to improve the illumination condition of the training images, especially in the single-sample setting. [sent-273, score-0.902]
</p><p>85 Since both algorithms do not address the alignment problem, manual labels of the face location are assumed to be the aligned face location. [sent-275, score-0.933]
</p><p>86 The comparison further shows adding the illumination transfer information to the SRC and ESRC algorithms meaningfully improves their performance by 3% – 4%. [sent-282, score-0.486]
</p><p>87 SIT that relies on a SIT dictionary to automatically alignment the testing images achieves 65. [sent-296, score-0.494]
</p><p>88 For each subject, we randomly select one frontal image with arbitrary illumination for testing. [sent-307, score-0.424]
</p><p>89 Various levels of image corruption from 10% to 40% are randomly generated in the face region. [sent-308, score-0.523]
</p><p>90 In the above alignment and recognition comparison, we have seen that SIT is compa-  rable to or outperforms the existing face recognition algorithms using just a one-subject illumination dictionary. [sent-330, score-1.087]
</p><p>91 In this experiment, we provide some empirical observations to investigate the change of its alignment accuracy from using one subject to 10 subjects. [sent-331, score-0.402]
</p><p>92 Figure 6 again shows the alignment success rates when the face bounding box undergoes x-axis and y-axis translation, respectively, between [-12, 12] pixels. [sent-332, score-0.609]
</p><p>93 We observe that adjusting the size of the illumination dictionary does affect the alignment performance. [sent-335, score-0.821]
</p><p>94 Conclusion and Discussion In this paper, we have presented a novel face recognition algorithm specifically designed for single-sample alignment and recognition. [sent-340, score-0.64]
</p><p>95 Although we have provided some exciting results that represent a meaningful step forward towards a real-world face recognition system, there remain several open problems that warrant further investigation. [sent-341, score-0.396]
</p><p>96 First, although the current way of constructing the illumination dictionary is efficient, the method is not able to separate the effect of surface albedo, shape, and illumination completely on face images. [sent-342, score-1.188]
</p><p>97 Therefore, a more sophisticated illumination transfer algorithm could lead to better overall performance. [sent-343, score-0.493]
</p><p>98 4 that including more subjects in the illumination dictionary may not necessarily lead to better performance, one could study whether a better dictionary learning algorithm could be applied to formulate the illumination dictionary that might represent more face shapes and illumination patterns. [sent-345, score-1.985]
</p><p>99 Extended SRC: Undersampled face recognition via intraclass variant dictionary. [sent-381, score-0.43]
</p><p>100 Toward a practical face recognition: Robust pose and illumination via sparse representation. [sent-498, score-0.749]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sit', 0.41), ('illumination', 0.35), ('src', 0.321), ('face', 0.299), ('alignment', 0.282), ('corruption', 0.224), ('session', 0.205), ('dsrc', 0.205), ('dictionary', 0.189), ('query', 0.154), ('mrr', 0.146), ('subject', 0.12), ('transfer', 0.116), ('facial', 0.106), ('esrc', 0.091), ('yaleb', 0.084), ('misalignment', 0.073), ('ai', 0.071), ('subjects', 0.069), ('training', 0.066), ('sparse', 0.065), ('reference', 0.061), ('recognition', 0.059), ('cyi', 0.055), ('frontal', 0.054), ('translation', 0.053), ('aixi', 0.051), ('ambient', 0.05), ('class', 0.049), ('neutral', 0.049), ('subspace', 0.049), ('beard', 0.048), ('intraclass', 0.047), ('nuisances', 0.045), ('transformation', 0.045), ('ganesh', 0.042), ('methodsession', 0.041), ('plurality', 0.041), ('compensate', 0.041), ('wright', 0.039), ('eye', 0.038), ('dictionaries', 0.037), ('corrupted', 0.035), ('pose', 0.035), ('stage', 0.034), ('sufficient', 0.034), ('manual', 0.033), ('gross', 0.032), ('per', 0.03), ('recover', 0.03), ('regime', 0.029), ('wagner', 0.029), ('rates', 0.028), ('align', 0.027), ('moderate', 0.027), ('compensation', 0.027), ('classes', 0.027), ('warped', 0.027), ('sophisticated', 0.027), ('pami', 0.027), ('quotient', 0.026), ('al', 0.026), ('registration', 0.025), ('variant', 0.025), ('illuminations', 0.024), ('sastry', 0.024), ('active', 0.024), ('corners', 0.024), ('representation', 0.024), ('arxiv', 0.023), ('images', 0.023), ('glasses', 0.022), ('cootes', 0.022), ('exceeds', 0.022), ('experiment', 0.022), ('unrelated', 0.021), ('pixel', 0.021), ('needs', 0.021), ('coefficients', 0.021), ('yang', 0.021), ('arbitrary', 0.02), ('constructed', 0.02), ('algorithms', 0.02), ('provided', 0.02), ('simultaneous', 0.019), ('appearance', 0.019), ('efficacy', 0.019), ('patterns', 0.019), ('subsection', 0.019), ('wild', 0.018), ('reflectance', 0.018), ('exacerbated', 0.018), ('warrant', 0.018), ('abr', 0.018), ('abrg', 0.018), ('execute', 0.018), ('rable', 0.018), ('tamura', 0.018), ('yyi', 0.018), ('shashua', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="399-tfidf-1" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>2 0.36077049 <a title="399-tfidf-2" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>Author: Weihong Deng, Jiani Hu, Jun Guo</p><p>Abstract: The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. The experiments results on AR, FERET and FRGC databases validate that, if the proposed prototype plus variation representation model is applied, sparse coding plays a crucial role in face recognition, and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.</p><p>3 0.28300208 <a title="399-tfidf-3" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>4 0.22902383 <a title="399-tfidf-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.21339045 <a title="399-tfidf-5" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>6 0.19955602 <a title="399-tfidf-6" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>7 0.19457446 <a title="399-tfidf-7" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>8 0.17318188 <a title="399-tfidf-8" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>9 0.17211604 <a title="399-tfidf-9" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>10 0.16742574 <a title="399-tfidf-10" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>11 0.160228 <a title="399-tfidf-11" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>12 0.15527481 <a title="399-tfidf-12" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>13 0.15520079 <a title="399-tfidf-13" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>14 0.14475954 <a title="399-tfidf-14" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>15 0.14446653 <a title="399-tfidf-15" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>16 0.14210974 <a title="399-tfidf-16" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>17 0.14154665 <a title="399-tfidf-17" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>18 0.14030109 <a title="399-tfidf-18" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>19 0.1364347 <a title="399-tfidf-19" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>20 0.13371186 <a title="399-tfidf-20" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, -0.103), (2, -0.207), (3, 0.187), (4, -0.008), (5, -0.102), (6, -0.014), (7, -0.049), (8, 0.244), (9, -0.137), (10, 0.088), (11, -0.018), (12, 0.064), (13, 0.109), (14, 0.047), (15, -0.002), (16, 0.093), (17, -0.015), (18, -0.049), (19, 0.031), (20, 0.003), (21, 0.024), (22, 0.005), (23, 0.055), (24, 0.031), (25, -0.102), (26, -0.019), (27, -0.004), (28, -0.035), (29, 0.056), (30, 0.053), (31, -0.007), (32, 0.006), (33, -0.07), (34, -0.035), (35, -0.043), (36, -0.075), (37, 0.011), (38, -0.059), (39, 0.043), (40, -0.147), (41, 0.01), (42, -0.017), (43, -0.04), (44, -0.006), (45, 0.03), (46, -0.023), (47, 0.024), (48, -0.073), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96677911 <a title="399-lsi-1" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>2 0.89386249 <a title="399-lsi-2" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>Author: Weihong Deng, Jiani Hu, Jun Guo</p><p>Abstract: The success of sparse representation based classification (SRC) has largely boosted the research of sparsity based face recognition in recent years. A prevailing view is that the sparsity based face recognition performs well only when the training images have been carefully controlled and the number of samples per class is sufficiently large. This paper challenges the prevailing view by proposing a “prototype plus variation ” representation model for sparsity based face recognition. Based on the new model, a Superposed SRC (SSRC), in which the dictionary is assembled by the class centroids and the sample-to-centroid differences, leads to a substantial improvement on SRC. The experiments results on AR, FERET and FRGC databases validate that, if the proposed prototype plus variation representation model is applied, sparse coding plays a crucial role in face recognition, and performs well even when the dictionary bases are collected under uncontrolled conditions and only a single sample per classes is available.</p><p>3 0.7212072 <a title="399-lsi-3" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>Author: Dong Yi, Zhen Lei, Stan Z. Li</p><p>Abstract: Most existing pose robust methods are too computational complex to meet practical applications and their performance under unconstrained environments are rarely evaluated. In this paper, we propose a novel method for pose robust face recognition towards practical applications, which is fast, pose robust and can work well under unconstrained environments. Firstly, a 3D deformable model is built and a fast 3D model fitting algorithm is proposed to estimate the pose of face image. Secondly, a group of Gabor filters are transformed according to the pose and shape of face image for feature extraction. Finally, PCA is applied on the pose adaptive Gabor features to remove the redundances and Cosine metric is used to evaluate the similarity. The proposed method has three advantages: (1) The pose correction is applied in the filter space rather than image space, which makes our method less affected by the precision of the 3D model; (2) By combining the holistic pose transformation and local Gabor filtering, the final feature is robust to pose and other negative factors in face recognition; (3) The 3D structure and facial symmetry are successfully used to deal with self-occlusion. Extensive experiments on FERET and PIE show the proposed method outperforms state-ofthe-art methods significantly, meanwhile, the method works well on LFW.</p><p>4 0.71953773 <a title="399-lsi-4" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>Author: Enrique G. Ortiz, Alan Wright, Mubarak Shah</p><p>Abstract: This paper presents an end-to-end video face recognition system, addressing the difficult problem of identifying a video face track using a large dictionary of still face images of a few hundred people, while rejecting unknown individuals. A straightforward application of the popular ?1minimization for face recognition on a frame-by-frame basis is prohibitively expensive, so we propose a novel algorithm Mean Sequence SRC (MSSRC) that performs video face recognition using a joint optimization leveraging all of the available video data and the knowledge that the face track frames belong to the same individual. By adding a strict temporal constraint to the ?1-minimization that forces individual frames in a face track to all reconstruct a single identity, we show the optimization reduces to a single minimization over the mean of the face track. We also introduce a new Movie Trailer Face Dataset collected from 101 movie trailers on YouTube. Finally, we show that our methodmatches or outperforms the state-of-the-art on three existing datasets (YouTube Celebrities, YouTube Faces, and Buffy) and our unconstrained Movie Trailer Face Dataset. More importantly, our method excels at rejecting unknown identities by at least 8% in average precision.</p><p>5 0.69306505 <a title="399-lsi-5" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>6 0.67886442 <a title="399-lsi-6" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>7 0.67654651 <a title="399-lsi-7" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>8 0.67487228 <a title="399-lsi-8" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>9 0.65330631 <a title="399-lsi-9" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>10 0.64356834 <a title="399-lsi-10" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>11 0.62564552 <a title="399-lsi-11" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>12 0.59309477 <a title="399-lsi-12" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>13 0.57767701 <a title="399-lsi-13" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>14 0.57184798 <a title="399-lsi-14" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>15 0.56503963 <a title="399-lsi-15" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>16 0.54559976 <a title="399-lsi-16" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>17 0.53940743 <a title="399-lsi-17" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>18 0.53722197 <a title="399-lsi-18" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>19 0.53560978 <a title="399-lsi-19" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>20 0.53366548 <a title="399-lsi-20" href="./cvpr-2013-Semi-supervised_Learning_with_Constraints_for_Person_Identification_in_Multimedia_Data.html">389 cvpr-2013-Semi-supervised Learning with Constraints for Person Identification in Multimedia Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.106), (16, 0.016), (26, 0.077), (33, 0.273), (39, 0.218), (67, 0.087), (69, 0.044), (87, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93075085 <a title="399-lda-1" href="./cvpr-2013-Scene_Parsing_by_Integrating_Function%2C_Geometry_and_Appearance_Models.html">381 cvpr-2013-Scene Parsing by Integrating Function, Geometry and Appearance Models</a></p>
<p>Author: Yibiao Zhao, Song-Chun Zhu</p><p>Abstract: Indoor functional objects exhibit large view and appearance variations, thus are difficult to be recognized by the traditional appearance-based classification paradigm. In this paper, we present an algorithm to parse indoor images based on two observations: i) The functionality is the most essentialproperty to define an indoor object, e.g. “a chair to sit on ”; ii) The geometry (3D shape) ofan object is designed to serve its function. We formulate the nature of the object function into a stochastic grammar model. This model characterizes a joint distribution over the function-geometryappearance (FGA) hierarchy. The hierarchical structure includes a scene category, , functional groups, , functional objects, functional parts and 3D geometric shapes. We use a simulated annealing MCMC algorithm to find the maximum a posteriori (MAP) solution, i.e. a parse tree. We design four data-driven steps to accelerate the search in the FGA space: i) group the line segments into 3D primitive shapes, ii) assign functional labels to these 3D primitive shapes, iii) fill in missing objects/parts according to the functional labels, and iv) synthesize 2D segmentation maps and verify the current parse tree by the Metropolis-Hastings acceptance probability. The experimental results on several challenging indoor datasets demonstrate theproposed approach not only significantly widens the scope ofindoor sceneparsing algorithm from the segmentation and the 3D recovery to the functional object recognition, but also yields improved overall performance.</p><p>2 0.91641045 <a title="399-lda-2" href="./cvpr-2013-Discriminatively_Trained_And-Or_Tree_Models_for_Object_Detection.html">136 cvpr-2013-Discriminatively Trained And-Or Tree Models for Object Detection</a></p>
<p>Author: Xi Song, Tianfu Wu, Yunde Jia, Song-Chun Zhu</p><p>Abstract: This paper presents a method of learning reconfigurable And-Or Tree (AOT) models discriminatively from weakly annotated data for object detection. To explore the appearance and geometry space of latent structures effectively, we first quantize the image lattice using an overcomplete set of shape primitives, and then organize them into a directed acyclic And-Or Graph (AOG) by exploiting their compositional relations. We allow overlaps between child nodes when combining them into a parent node, which is equivalent to introducing an appearance Or-node implicitly for the overlapped portion. The learning of an AOT model consists of three components: (i) Unsupervised sub-category learning (i.e., branches of an object Or-node) with the latent structures in AOG being integrated out. (ii) Weaklysupervised part configuration learning (i.e., seeking the globally optimal parse trees in AOG for each sub-category). To search the globally optimal parse tree in AOG efficiently, we propose a dynamic programming (DP) algorithm. (iii) Joint appearance and structural parameters training under latent structural SVM framework. In experiments, our method is tested on PASCAL VOC 2007 and 2010 detection , benchmarks of 20 object classes and outperforms comparable state-of-the-art methods.</p><p>3 0.91638464 <a title="399-lda-3" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>Author: Samuele Salti, Alessandro Lanza, Luigi Di_Stefano</p><p>Abstract: The paper conjectures and demonstrates that repeatable keypoints based on salient symmetries at different scales can be detected by a novel analysis grounded on the wave equation rather than the heat equation underlying traditional Gaussian scale–space theory. While the image structures found by most state-of-the-art detectors, such as blobs and corners, occur typically on planar highly textured surfaces, salient symmetries are widespread in diverse kinds of images, including those related to untextured objects, which are hardly dealt with by current feature-based recognition pipelines. We provide experimental results on standard datasets and also contribute with a new dataset focused on untextured objects. Based on the positive experimental results, we hope to foster further research on the promising topic ofscale invariant analysis through the wave equation.</p><p>4 0.87734878 <a title="399-lda-4" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>Author: Byung-soo Kim, Shili Xu, Silvio Savarese</p><p>Abstract: In this paper we focus on the problem of detecting objects in 3D from RGB-D images. We propose a novel framework that explores the compatibility between segmentation hypotheses of the object in the image and the corresponding 3D map. Our framework allows to discover the optimal location of the object using a generalization of the structural latent SVM formulation in 3D as well as the definition of a new loss function defined over the 3D space in training. We evaluate our method using two existing RGB-D datasets. Extensive quantitative and qualitative experimental results show that our proposed approach outperforms state-of-theart as methods well as a number of baseline approaches for both 3D and 2D object recognition tasks.</p><p>5 0.85966527 <a title="399-lda-5" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>Author: Akshay Asthana, Stefanos Zafeiriou, Shiyang Cheng, Maja Pantic</p><p>Abstract: We present a novel discriminative regression based approach for the Constrained Local Models (CLMs) framework, referred to as the Discriminative Response Map Fitting (DRMF) method, which shows impressive performance in the generic face fitting scenario. The motivation behind this approach is that, unlike the holistic texture based features used in the discriminative AAM approaches, the response map can be represented by a small set of parameters and these parameters can be very efficiently used for reconstructing unseen response maps. Furthermore, we show that by adopting very simple off-the-shelf regression techniques, it is possible to learn robust functions from response maps to the shape parameters updates. The experiments, conducted on Multi-PIE, XM2VTS and LFPW database, show that the proposed DRMF method outperforms stateof-the-art algorithms for the task of generic face fitting. Moreover, the DRMF method is computationally very efficient and is real-time capable. The current MATLAB implementation takes 1second per image. To facilitate future comparisons, we release the MATLAB code1 and the pretrained models for research purposes.</p><p>same-paper 6 0.85759366 <a title="399-lda-6" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>7 0.84551436 <a title="399-lda-7" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>8 0.83727854 <a title="399-lda-8" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>9 0.83512926 <a title="399-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.82058758 <a title="399-lda-10" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>11 0.81905991 <a title="399-lda-11" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>12 0.81868631 <a title="399-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.81714725 <a title="399-lda-13" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>14 0.81703627 <a title="399-lda-14" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>15 0.81586158 <a title="399-lda-15" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>16 0.81451684 <a title="399-lda-16" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>17 0.8138532 <a title="399-lda-17" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>18 0.81383348 <a title="399-lda-18" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>19 0.81355178 <a title="399-lda-19" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>20 0.81279224 <a title="399-lda-20" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
