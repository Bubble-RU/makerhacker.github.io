<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-269" href="#">cvpr2013-269</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</h1>
<br/><p>Source: <a title="cvpr-2013-269-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Maeno_Light_Field_Distortion_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Kazuki Maeno, Hajime Nagahara, Atsushi Shimada, Rin-Ichiro Taniguchi</p><p>Abstract: Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance ofsuch objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light?eld image as an input and model the distortion of the light ?eld caused by the refractive property of a transparent object. We propose a new feature, called the light ?eld distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.</p><p>Reference: <a title="cvpr-2013-269-reference" href="../cvpr2013_reference/cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance ofsuch objects dramatically varies with changes in scene background. [sent-2, score-0.57]
</p><p>2 Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. [sent-3, score-0.454]
</p><p>3 eld image as an input and model the distortion of the light ? [sent-5, score-0.567]
</p><p>4 eld caused by the refractive property of a transparent object. [sent-6, score-0.754]
</p><p>5 The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. [sent-9, score-0.376]
</p><p>6 However, these features and learning algorithms cannot apply to transparent objects. [sent-15, score-0.368]
</p><p>7 lled with many transparent objects, such as glasses, bottles, bowls, jars, vases, and windows, to name a few. [sent-18, score-0.348]
</p><p>8 Depending on backgrounds, whether stationary or moving, the appearance ofa transparent object drastically changes, as such objects do not have features entirely of their own, but rather transmitted background images. [sent-19, score-0.505]
</p><p>9 Using the standard approaches requires modeling local features not of the transparent object but the background. [sent-20, score-0.399]
</p><p>10 In this paper, we propose a novel object feature, called the light ? [sent-21, score-0.204]
</p><p>11 We discuss how the LFD feature models and visually recognizes transparent objects, which to date have been ignored as exceptions in applications of visual object recognition or annotation. [sent-23, score-0.436]
</p><p>12 Transparent objects are made of refractive materials, such as glass or plastics, and distort rays emanating from the scene background. [sent-24, score-0.14]
</p><p>13 Different objects produce different distortions, each carrying intrinsic characteristics of the transparent object, namely the refractive index of material and the object’s shape, both of which in? [sent-25, score-0.449]
</p><p>14 Compared with conventional cameras, which capture 2D photos from a single perspective, light ? [sent-31, score-0.185]
</p><p>15 eld cameras obtain richer 4D images that include multiple 2D viewpoints (position resolution) as well as standard 2D image coordinates (angular resolutions). [sent-32, score-0.418]
</p><p>16 The LFD feature models the distortion from differences in corresponding points between viewpoints including the 4D light ? [sent-33, score-0.348]
</p><p>17 This is an entirely original concept for feature description with the advantage that LFD is less affected by background changes, as it uses patterns of ray distortions caused by the object, not patterns from the object’s appearance . [sent-35, score-0.208]
</p><p>18 eld camera was originally proposed for image-based rendering for use in the graphics community, and has been used for a variety of different visualization applications, such as generating free-view images, 3D graphics, and digital refocusing. [sent-37, score-0.382]
</p><p>19 eld cameras consisting of a micro-lens array between the sensor and main lens are becoming inexpensive and compact [2, 3] . [sent-43, score-0.375]
</p><p>20 , some of these are available in the product market [4, 5, 6] Hence, we believe that the light ? [sent-44, score-0.173]
</p><p>21 eld camera is becoming a popular input device in computer vision applications. [sent-45, score-0.37]
</p><p>22 cult computer vision problem, transparent object recognition with a single-shot image, 2) in proposing a new feature 222777888644  for transparent object recognition, called the LFD feature, and 3) in applying a light ? [sent-48, score-1.022]
</p><p>23 ed transparent objects without explicit physics-based refraction analysis and refraction models. [sent-52, score-0.664]
</p><p>24 Therefore, if local features are drawn from a more dominant background than an object’s surface, existing learning and recognition methods perform poorly. [sent-58, score-0.125]
</p><p>25 A transparent object yields less information about its appearr. [sent-59, score-0.379]
</p><p>26 In consequence, extracting scene-independent local features from a transparent object area is dif? [sent-62, score-0.399]
</p><p>27 In other directions, there has been much research on measuring refraction responses in transparent objects using cameras to obtain physical parameters, such as surface curvature or refractive index. [sent-66, score-0.638]
</p><p>28 measured light intensities from transparent objects through polarizing ? [sent-69, score-0.551]
</p><p>29 This method visualizes the refraction response in a scene as a gray-scale or color image by using special optics, although it requires high-quality optics and precise alignment. [sent-74, score-0.157]
</p><p>30 eld background-oriented Schlieren photography that obtains Schlieren photos using a common hand-held camera and a special-purpose optical sheet. [sent-78, score-0.429]
</p><p>31 Although this technique recovers the transparent surface [16] it also has restricted practical use as the special sheet is always required t. [sent-79, score-0.365]
</p><p>32 Background distortion from changing viewpoints as a background object. [sent-83, score-0.223]
</p><p>33 [19] used two calibrated cameras to estimate the refractive indices over time-varying liquid surfaces from distortions of known grid patterns at the bottom of a tank. [sent-91, score-0.145]
</p><p>34 In contrast to these approaches, the novelty of our work is to apply refraction to transparent object recognition, realized from a single shot image, using a light ? [sent-92, score-0.723]
</p><p>35 Unlike previous methods, there are no constraints on background texture, camera motion or known parameters. [sent-94, score-0.128]
</p><p>36 Light Field Distortion Feature By refraction, a transparent object deforms the background scene. [sent-96, score-0.455]
</p><p>37 Different objects produce different images of the same scene (Figure 1), because refraction by objects is affected by shape and refractive index. [sent-97, score-0.274]
</p><p>38 Using the background distortion caused by refraction is our means to recognize transparent objects. [sent-98, score-0.672]
</p><p>39 In fact, we modeled the background distortion to the appearance difference from different perspectives (Figure 2). [sent-99, score-0.152]
</p><p>40 In theory, the modeled distortion itself is independent of background texture, although the background determines image appearance, the distortion for corresponding points from different viewpoints is maintained. [sent-100, score-0.375]
</p><p>41 Therefore, our proposal is to model the object’s refraction as a distortion of multiple viewpoints cap222777888755  (a)Undsi toredlight? [sent-101, score-0.29]
</p><p>42 ne the LFD feature and outline its use in transparent object recognition. [sent-108, score-0.407]
</p><p>43 eld is a function that describes the amount of light emitting in every direction from every point in a scene. [sent-110, score-0.491]
</p><p>44 Conventional cameras only record that part of the light ? [sent-111, score-0.202]
</p><p>45 eld passing through a single viewpoint of a 2D image. [sent-112, score-0.318]
</p><p>46 Here, we use the 4D-ray representation of the light ? [sent-118, score-0.173]
</p><p>47 eld L(s, t, u, v) determined by the intersection of a plane (s, t) and a slant of ray (u, v) (see Figure 3). [sent-119, score-0.339]
</p><p>48 Figure 4(a) illustrates the functioning of a camera array and shows the relation between light ? [sent-120, score-0.253]
</p><p>49 Figure 4(a) depicts a scene where there is no object be-  tween background and camera; i. [sent-124, score-0.107]
</p><p>50 As illustrated, if rays emitted from a point in the background are straight, the observed light ? [sent-128, score-0.271]
</p><p>51 eld has constant disparities over the images for the different viewpoints. [sent-129, score-0.357]
</p><p>52 In fact, these rays are distributed on a hyperplane in stuv-space because the actual light ? [sent-131, score-0.214]
</p><p>53 In contrast, if a transparent object intervenes between background and camera, the ray distribution deviates from the line or the hyperplane (Figure 4(b)). [sent-133, score-0.495]
</p><p>54 This LFD is caused by refraction occurring within the object, which is characterized by the material (refractive index) and shape of the transparent object. [sent-134, score-0.508]
</p><p>55 We call this the LDF feature that is to be used as a feature in transparent object recognition. [sent-135, score-0.435]
</p><p>56 Transparent object recognition In this section, we describe an algorithm of our transparent object recognition. [sent-149, score-0.439]
</p><p>57 This camera system has 25 VGA resolution (640 ×480 pixels) cameras and can simultaneously capture images f8r0om pi x25e viewpoints ( a5n dho criazno snitmalu viewpoints 5 vertical viewpoints). [sent-154, score-0.223]
</p><p>58 eld image we uses the colors for indicating LFD patterns in this paper. [sent-165, score-0.339]
</p><p>59 We estimated the disparities between the center and the  ×  other 24 viewpoints by the optical ? [sent-166, score-0.122]
</p><p>60 The LFD is also an example of 3 3 case; these images are actually taken from a 25-viewpoint light c? [sent-175, score-0.173]
</p><p>61 coming from the transparent object has a larger distortion than these from background, since the disparities contain refraction effect and deviate from hyperplane assumed as Lambertian re? [sent-178, score-0.656]
</p><p>62 We represent classes of transparent objects as patterns of histograms of the visual words. [sent-188, score-0.399]
</p><p>63 cation of transparent objects in a laboratory setting and real environments under following assumptions; •  •  •  •  There is one transparent object as a recognition target iTnh a scene . [sent-196, score-0.88]
</p><p>64 The target object appears in all of the viewpoints of the light a? [sent-197, score-0.288]
</p><p>65 Relative positions and poses of the camera and target object are aoslmitoiosnt same pbeotsweese onf a training raan dan testings. [sent-199, score-0.108]
</p><p>66 ve background patterns  We performed some experiments in a laboratory and real setting to evaluate robustness and limitations of our proposed method. [sent-206, score-0.143]
</p><p>67 We used as a reference position for LFD learning a setting where camera position was 40 cm in front and background was 150 cm behind the object position. [sent-212, score-0.352]
</p><p>68 Our task is classifying seven various shapes of the objects (Figure 8) into the seven classes under the various background textures. [sent-214, score-0.148]
</p><p>69 Also the LFDs came from the different regions of the ob-  Object A Object B Object C (a) Different objects with same background  Background A  Background B  Background C (b) Same object with different backgrounds  ×  Figure 10. [sent-219, score-0.174]
</p><p>70 rmed that LFD feature is irrespective of the background difference, since the feature models not intensity pattern but geometrical distortion cased by object refraction. [sent-232, score-0.286]
</p><p>71 cation accuracy over the 7 object classes in front of 5 different backgrounds, although it realized transparent object recognition from a single-shot image. [sent-234, score-0.489]
</p><p>72 Recognition ratios for camera position changes  octReoiraongi0 0 0. [sent-243, score-0.156]
</p><p>73 Recognition ratios for background position changes the camera against real backgrounds of structures at various depths, i. [sent-246, score-0.283]
</p><p>74 This result shows that local features are unsuitable for transparent object recognition. [sent-254, score-0.399]
</p><p>75 We changed camera and background positions, object poses and lighting conditions for the evaluation. [sent-262, score-0.215]
</p><p>76 Recognition ratios for object pose changes  Lighting angle [degree]  Figure 15. [sent-267, score-0.105]
</p><p>77 Recognition ratios for illumination changes 12-15 show decreases ofthe recognition accuracies by these changes from the reference setting on the learning step. [sent-268, score-0.137]
</p><p>78 We moved the camera over a range ±10 cm from the refeWreen cmeo pvoesditi tohne 4ca0m cm. [sent-269, score-0.12]
</p><p>79 Fovigeurre a 1ra2n sgheo w±s1 0th catm mth fer recognition ratios are decreased when the displacement form the  reference position is increased, because the LFD feature are changed from the learned pattern related to the distance between the camera and object. [sent-270, score-0.27]
</p><p>80 5 cm if we accept 20% decrease in the recognition ratio. [sent-272, score-0.106]
</p><p>81 We also moved the background position over the range of 50 cm to 250 cm from the object, while the reference position of the background is 150 cm. [sent-273, score-0.345]
</p><p>82 Figure 13 shows the recognition ratio decreased when the background displaced from the reference position. [sent-274, score-0.173]
</p><p>83 The ratio is not so changed when the background is away from the object, while it is steeply decreased when the background position is approaching to the object. [sent-277, score-0.263]
</p><p>84 The background position did not affect much about the recognition ratio and we can apply this method to more realistic no planer scene background, if we can assume that the background objects of the scene are places reasonably far positions, e. [sent-279, score-0.261]
</p><p>85 The ratio of asymmetric group was decreased gradually and the limitation on object pose variation is within 10 degree if we accept a 20% degradation in recognition ratios. [sent-286, score-0.145]
</p><p>86 We placed an additional point light source to the global illumination that was used in the all of the experiments. [sent-288, score-0.185]
</p><p>87 We changed the direction of the light source from above (0 degree) to the side (90 degree) with respect to the target object. [sent-289, score-0.214]
</p><p>88 ections from the light source and these effects were changed as we moved the light source. [sent-292, score-0.417]
</p><p>89 gure shows that the internal and specular from the light source contaminated the LFDs and decreases averagely 20% of the recognition. [sent-296, score-0.241]
</p><p>90 Analysis for Texture Density The background patterns used in the experiment have complex textures (see Figure 9) from which correspondence detection can be easily performed. [sent-300, score-0.112]
</p><p>91 In Figure 16(a), the LFD features were extracted from only the edges of the transparent object, with no LFD feature taken interior to the object. [sent-305, score-0.396]
</p><p>92 For another background (Figure 16(b)), LFD features were wrongly extracted exterior to the transparent object (see the top-left part of the ? [sent-306, score-0.475]
</p><p>93 First, to obtain ideal feature points, a dot pattern was displayed as a background to the transparent object for easy to detect the correspondence of the LFDs. [sent-310, score-0.498]
</p><p>94 Therefore, our proposed LFD  feature is considered effective in transparent object recognition. [sent-337, score-0.407]
</p><p>95 The recognition ratios across different standard deviation of noise (Figure 18) show that ratios was decreased when error levels was increased. [sent-341, score-0.172]
</p><p>96 Conclusion This paper proposed a novel feature termed the LFD feature, which models refraction in objects as distortions between multiple views captured by a light ? [sent-346, score-0.398]
</p><p>97 Our method using the LFD feature achieved on average 70% accuracy with seven objects against different backgrounds as assessed by leave222777999200  one-out cross-validation in real environments, hence verifying the effectiveness of our LFD feature. [sent-350, score-0.13]
</p><p>98 We also evaluated the robustness and limitation of the proposed method under various conditions such as: camera and background  positions, object poses, and lighting conditions. [sent-351, score-0.189]
</p><p>99 In conclusion, we have been successful in: 1) producing a transparent object recognition approach based on a single-shot image, 2) employing a novel feature, namely refraction, for transparent object recognition, and 3) introducing the light ? [sent-353, score-0.96]
</p><p>100 Nevertheless, the recognition accuracy did not exceed 70% and it is not so high in light of applications. [sent-358, score-0.202]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lfd', 0.786), ('transparent', 0.348), ('eld', 0.318), ('light', 0.173), ('refraction', 0.143), ('lfds', 0.089), ('background', 0.076), ('distortion', 0.076), ('refractive', 0.071), ('viewpoints', 0.071), ('schlieren', 0.067), ('gure', 0.056), ('ratios', 0.055), ('camera', 0.052), ('cm', 0.05), ('classi', 0.042), ('disparities', 0.039), ('backgrounds', 0.037), ('photography', 0.035), ('cult', 0.034), ('decreased', 0.033), ('bof', 0.033), ('laboratory', 0.032), ('dif', 0.032), ('object', 0.031), ('miyazaki', 0.031), ('wetzstein', 0.031), ('position', 0.03), ('objects', 0.03), ('recognition', 0.029), ('cameras', 0.029), ('phase', 0.028), ('changed', 0.028), ('array', 0.028), ('feature', 0.028), ('cased', 0.025), ('ections', 0.025), ('maeno', 0.025), ('shadowgraph', 0.025), ('toredlight', 0.025), ('distortions', 0.024), ('taniguchi', 0.022), ('rmed', 0.022), ('farneback', 0.022), ('ection', 0.022), ('suf', 0.022), ('rays', 0.022), ('acquisition', 0.021), ('ray', 0.021), ('patterns', 0.021), ('seven', 0.021), ('nagahara', 0.021), ('ratio', 0.02), ('features', 0.02), ('settles', 0.02), ('hyperplane', 0.019), ('changes', 0.019), ('horowitz', 0.019), ('front', 0.018), ('environments', 0.018), ('sift', 0.018), ('moved', 0.018), ('ikeuchi', 0.017), ('morris', 0.017), ('cation', 0.017), ('raskar', 0.017), ('caused', 0.017), ('frequent', 0.017), ('primal', 0.017), ('disparity', 0.017), ('degree', 0.017), ('glass', 0.017), ('heidrich', 0.017), ('polarization', 0.017), ('surface', 0.017), ('lighting', 0.016), ('ned', 0.016), ('accept', 0.015), ('correspondence', 0.015), ('hue', 0.015), ('saturation', 0.015), ('realized', 0.015), ('reference', 0.015), ('real', 0.014), ('evaluated', 0.014), ('optics', 0.014), ('simulation', 0.013), ('shot', 0.013), ('target', 0.013), ('con', 0.012), ('surf', 0.012), ('optical', 0.012), ('recognize', 0.012), ('decrease', 0.012), ('photos', 0.012), ('placed', 0.012), ('tracking', 0.012), ('poses', 0.012), ('visualization', 0.012), ('specular', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="269-tfidf-1" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>Author: Kazuki Maeno, Hajime Nagahara, Atsushi Shimada, Rin-Ichiro Taniguchi</p><p>Abstract: Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance ofsuch objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light?eld image as an input and model the distortion of the light ?eld caused by the refractive property of a transparent object. We propose a new feature, called the light ?eld distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.</p><p>2 0.28579244 <a title="269-tfidf-2" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>3 0.15162888 <a title="269-tfidf-3" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>Author: Timothy Yau, Minglun Gong, Yee-Hong Yang</p><p>Abstract: In underwater imagery, the image formation process includes refractions that occur when light passes from water into the camera housing, typically through a flat glass port. We extend the existing work on physical refraction models by considering the dispersion of light, and derive new constraints on the model parameters for use in calibration. This leads to a novel calibration method that achieves improved accuracy compared to existing work. We describe how to construct a novel calibration device for our method and evaluate the accuracy of the method through synthetic and real experiments.</p><p>4 0.13215564 <a title="269-tfidf-4" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>Author: Yu Ji, Jinwei Ye, Jingyi Yu</p><p>Abstract: Transparent gas flows are difficult to reconstruct: the refractive index field (RIF) within the gas volume is uneven and rapidly evolving, and correspondence matching under distortions is challenging. We present a novel computational imaging solution by exploiting the light field probe (LFProbe). A LF-probe resembles a view-dependent pattern where each pixel on the pattern maps to a unique ray. By . ude l. edu observing the LF-probe through the gas flow, we acquire a dense set of ray-ray correspondences and then reconstruct their light paths. To recover the RIF, we use Fermat’s Principle to correlate each light path with the RIF via a Partial Differential Equation (PDE). We then develop an iterative optimization scheme to solve for all light-path PDEs in conjunction. Specifically, we initialize the light paths by fitting Hermite splines to ray-ray correspondences, discretize their PDEs onto voxels, and solve a large, over-determined PDE system for the RIF. The RIF can then be used to refine the light paths. Finally, we alternate the RIF and light-path estimations to improve the reconstruction. Experiments on synthetic and real data show that our approach can reliably reconstruct small to medium scale gas flows. In particular, when the flow is acquired by a small number of cameras, the use of ray-ray correspondences can greatly improve the reconstruction.</p><p>5 0.097363688 <a title="269-tfidf-5" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>6 0.093653813 <a title="269-tfidf-6" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>7 0.090111859 <a title="269-tfidf-7" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>8 0.07789816 <a title="269-tfidf-8" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>9 0.077783369 <a title="269-tfidf-9" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>10 0.061053153 <a title="269-tfidf-10" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>11 0.060451653 <a title="269-tfidf-11" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>12 0.059309445 <a title="269-tfidf-12" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>13 0.057364389 <a title="269-tfidf-13" href="./cvpr-2013-Background_Modeling_Based_on_Bidirectional_Analysis.html">55 cvpr-2013-Background Modeling Based on Bidirectional Analysis</a></p>
<p>14 0.05367646 <a title="269-tfidf-14" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>15 0.052844044 <a title="269-tfidf-15" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>16 0.04130403 <a title="269-tfidf-16" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>17 0.040471453 <a title="269-tfidf-17" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>18 0.038138963 <a title="269-tfidf-18" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>19 0.035046745 <a title="269-tfidf-19" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>20 0.034869157 <a title="269-tfidf-20" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (1, 0.093), (2, 0.006), (3, 0.019), (4, -0.01), (5, -0.055), (6, -0.041), (7, 0.008), (8, 0.031), (9, 0.028), (10, -0.053), (11, 0.029), (12, 0.104), (13, -0.054), (14, -0.152), (15, 0.043), (16, 0.077), (17, 0.038), (18, 0.006), (19, 0.056), (20, 0.078), (21, 0.029), (22, -0.023), (23, -0.072), (24, -0.027), (25, 0.015), (26, 0.058), (27, 0.063), (28, -0.035), (29, -0.017), (30, 0.018), (31, -0.054), (32, 0.082), (33, -0.058), (34, 0.061), (35, -0.03), (36, -0.006), (37, 0.002), (38, -0.006), (39, -0.032), (40, -0.098), (41, 0.027), (42, -0.02), (43, 0.018), (44, 0.01), (45, -0.031), (46, -0.062), (47, 0.063), (48, 0.01), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88165909 <a title="269-lsi-1" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>Author: Kazuki Maeno, Hajime Nagahara, Atsushi Shimada, Rin-Ichiro Taniguchi</p><p>Abstract: Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance ofsuch objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light?eld image as an input and model the distortion of the light ?eld caused by the refractive property of a transparent object. We propose a new feature, called the light ?eld distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.</p><p>2 0.82966018 <a title="269-lsi-2" href="./cvpr-2013-Underwater_Camera_Calibration_Using_Wavelength_Triangulation.html">447 cvpr-2013-Underwater Camera Calibration Using Wavelength Triangulation</a></p>
<p>Author: Timothy Yau, Minglun Gong, Yee-Hong Yang</p><p>Abstract: In underwater imagery, the image formation process includes refractions that occur when light passes from water into the camera housing, typically through a flat glass port. We extend the existing work on physical refraction models by considering the dispersion of light, and derive new constraints on the model parameters for use in calibration. This leads to a novel calibration method that achieves improved accuracy compared to existing work. We describe how to construct a novel calibration device for our method and evaluate the accuracy of the method through synthetic and real experiments.</p><p>3 0.80537492 <a title="269-lsi-3" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>Author: Yu Ji, Jinwei Ye, Jingyi Yu</p><p>Abstract: Transparent gas flows are difficult to reconstruct: the refractive index field (RIF) within the gas volume is uneven and rapidly evolving, and correspondence matching under distortions is challenging. We present a novel computational imaging solution by exploiting the light field probe (LFProbe). A LF-probe resembles a view-dependent pattern where each pixel on the pattern maps to a unique ray. By . ude l. edu observing the LF-probe through the gas flow, we acquire a dense set of ray-ray correspondences and then reconstruct their light paths. To recover the RIF, we use Fermat’s Principle to correlate each light path with the RIF via a Partial Differential Equation (PDE). We then develop an iterative optimization scheme to solve for all light-path PDEs in conjunction. Specifically, we initialize the light paths by fitting Hermite splines to ray-ray correspondences, discretize their PDEs onto voxels, and solve a large, over-determined PDE system for the RIF. The RIF can then be used to refine the light paths. Finally, we alternate the RIF and light-path estimations to improve the reconstruction. Experiments on synthetic and real data show that our approach can reliably reconstruct small to medium scale gas flows. In particular, when the flow is acquired by a small number of cameras, the use of ray-ray correspondences can greatly improve the reconstruction.</p><p>4 0.78017795 <a title="269-lsi-4" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>5 0.7730481 <a title="269-lsi-5" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>Author: Weiming Li, Haitao Wang, Mingcai Zhou, Shandong Wang, Shaohui Jiao, Xing Mei, Tao Hong, Hoyoung Lee, Jiyeun Kim</p><p>Abstract: Integral imaging display (IID) is a promising technology to provide realistic 3D image without glasses. To achieve a large screen IID with a reasonable fabrication cost, a potential solution is a tiled-lens-array IID (TLA-IID). However, TLA-IIDs are subject to 3D image artifacts when there are even slight misalignments between the lens arrays. This work aims at compensating these artifacts by calibrating the lens array poses with a camera and including them in a ray model used for rendering the 3D image. Since the lens arrays are transparent, this task is challenging for traditional calibration methods. In this paper, we propose a novel calibration method based on defining a set of principle observation rays that pass lens centers of the TLA and the camera ’s optical center. The method is able to determine the lens array poses with only one camera at an arbitrary unknown position without using any additional markers. The principle observation rays are automatically extracted using a structured light based method from a dense correspondence map between the displayed and captured . pixels. . com, Experiments show that lens array misalignments xme i nlpr . ia . ac . cn @ can be estimated with a standard deviation smaller than 0.4 pixels. Based on this, 3D image artifacts are shown to be effectively removed in a test TLA-IID with challenging misalignments.</p><p>6 0.74999911 <a title="269-lsi-6" href="./cvpr-2013-Decoding%2C_Calibration_and_Rectification_for_Lenselet-Based_Plenoptic_Cameras.html">102 cvpr-2013-Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras</a></p>
<p>7 0.71194166 <a title="269-lsi-7" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>8 0.69398099 <a title="269-lsi-8" href="./cvpr-2013-Can_a_Fully_Unconstrained_Imaging_Model_Be_Applied_Effectively_to_Central_Cameras%3F.html">76 cvpr-2013-Can a Fully Unconstrained Imaging Model Be Applied Effectively to Central Cameras?</a></p>
<p>9 0.63420421 <a title="269-lsi-9" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>10 0.59282488 <a title="269-lsi-10" href="./cvpr-2013-Spectral_Modeling_and_Relighting_of_Reflective-Fluorescent_Scenes.html">409 cvpr-2013-Spectral Modeling and Relighting of Reflective-Fluorescent Scenes</a></p>
<p>11 0.56169254 <a title="269-lsi-11" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>12 0.51027453 <a title="269-lsi-12" href="./cvpr-2013-Adherent_Raindrop_Detection_and_Removal_in_Video.html">37 cvpr-2013-Adherent Raindrop Detection and Removal in Video</a></p>
<p>13 0.50058389 <a title="269-lsi-13" href="./cvpr-2013-Photometric_Ambient_Occlusion.html">330 cvpr-2013-Photometric Ambient Occlusion</a></p>
<p>14 0.4914287 <a title="269-lsi-14" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>15 0.46635947 <a title="269-lsi-15" href="./cvpr-2013-Megastereo%3A_Constructing_High-Resolution_Stereo_Panoramas.html">283 cvpr-2013-Megastereo: Constructing High-Resolution Stereo Panoramas</a></p>
<p>16 0.45771387 <a title="269-lsi-16" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>17 0.43883768 <a title="269-lsi-17" href="./cvpr-2013-Radial_Distortion_Self-Calibration.html">344 cvpr-2013-Radial Distortion Self-Calibration</a></p>
<p>18 0.43272996 <a title="269-lsi-18" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>19 0.39786068 <a title="269-lsi-19" href="./cvpr-2013-Discovering_the_Structure_of_a_Planar_Mirror_System_from_Multiple_Observations_of_a_Single_Point.html">127 cvpr-2013-Discovering the Structure of a Planar Mirror System from Multiple Observations of a Single Point</a></p>
<p>20 0.37597016 <a title="269-lsi-20" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.109), (16, 0.125), (26, 0.025), (33, 0.225), (39, 0.011), (67, 0.029), (69, 0.036), (74, 0.223), (80, 0.01), (87, 0.073), (96, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79029495 <a title="269-lda-1" href="./cvpr-2013-Light_Field_Distortion_Feature_for_Transparent_Object_Recognition.html">269 cvpr-2013-Light Field Distortion Feature for Transparent Object Recognition</a></p>
<p>Author: Kazuki Maeno, Hajime Nagahara, Atsushi Shimada, Rin-Ichiro Taniguchi</p><p>Abstract: Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance ofsuch objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light?eld image as an input and model the distortion of the light ?eld caused by the refractive property of a transparent object. We propose a new feature, called the light ?eld distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.</p><p>2 0.78985381 <a title="269-lda-2" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>Author: Bo Zheng, Yibiao Zhao, Joey C. Yu, Katsushi Ikeuchi, Song-Chun Zhu</p><p>Abstract: In this paper, we present an approach for scene understanding by reasoning physical stability of objects from point cloud. We utilize a simple observation that, by human design, objects in static scenes should be stable with respect to gravity. This assumption is applicable to all scene categories and poses useful constraints for the plausible interpretations (parses) in scene understanding. Our method consists of two major steps: 1) geometric reasoning: recovering solid 3D volumetric primitives from defective point cloud; and 2) physical reasoning: grouping the unstable primitives to physically stable objects by optimizing the stability and the scene prior. We propose to use a novel disconnectivity graph (DG) to represent the energy landscape and use a Swendsen-Wang Cut (MCMC) method for optimization. In experiments, we demonstrate that the algorithm achieves substantially better performance for i) object segmentation, ii) 3D volumetric recovery of the scene, and iii) better parsing result for scene understanding in comparison to state-of-the-art methods in both public dataset and our own new dataset.</p><p>3 0.78656119 <a title="269-lda-3" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>Author: Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</p><p>Abstract: The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.</p><p>4 0.78483886 <a title="269-lda-4" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>Author: Victor Fragoso, Matthew Turk</p><p>Abstract: We present SWIGS, a Swift and efficient Guided Sampling method for robust model estimation from image feature correspondences. Our method leverages the accuracy of our new confidence measure (MR-Rayleigh), which assigns a correctness-confidence to a putative correspondence in an online fashion. MR-Rayleigh is inspired by Meta-Recognition (MR), an algorithm that aims to predict when a classifier’s outcome is correct. We demonstrate that by using a Rayleigh distribution, the prediction accuracy of MR can be improved considerably. Our experiments show that MR-Rayleigh tends to predict better than the often-used Lowe ’s ratio, Brown’s ratio, and the standard MR under a range of imaging conditions. Furthermore, our homography estimation experiment demonstrates that SWIGS performs similarly or better than other guided sampling methods while requiring fewer iterations, leading to fast and accurate model estimates.</p><p>5 0.78342706 <a title="269-lda-5" href="./cvpr-2013-A_Theory_of_Refractive_Photo-Light-Path_Triangulation.html">27 cvpr-2013-A Theory of Refractive Photo-Light-Path Triangulation</a></p>
<p>Author: Visesh Chari, Peter Sturm</p><p>Abstract: 3D reconstruction of transparent refractive objects like a plastic bottle is challenging: they lack appearance related visual cues and merely reflect and refract light from the surrounding environment. Amongst several approaches to reconstruct such objects, the seminal work of Light-Path triangulation [17] is highly popular because of its general applicability and analysis of minimal scenarios. A lightpath is defined as the piece-wise linear path taken by a ray of light as it passes from source, through the object and into the camera. Transparent refractive objects not only affect the geometric configuration of light-paths but also their radiometric properties. In this paper, we describe a method that combines both geometric and radiometric information to do reconstruction. We show two major consequences of the addition of radiometric cues to the light-path setup. Firstly, we extend the case of scenarios in which reconstruction is plausible while reducing the minimal re- quirements for a unique reconstruction. This happens as a consequence of the fact that radiometric cues add an additional known variable to the already existing system of equations. Secondly, we present a simple algorithm for reconstruction, owing to the nature of the radiometric cue. We present several synthetic experiments to validate our theories, and show high quality reconstructions in challenging scenarios.</p><p>6 0.77809393 <a title="269-lda-6" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>7 0.76871049 <a title="269-lda-7" href="./cvpr-2013-Information_Consensus_for_Distributed_Multi-target_Tracking.html">224 cvpr-2013-Information Consensus for Distributed Multi-target Tracking</a></p>
<p>8 0.76800466 <a title="269-lda-8" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>9 0.76601154 <a title="269-lda-9" href="./cvpr-2013-Specular_Reflection_Separation_Using_Dark_Channel_Prior.html">410 cvpr-2013-Specular Reflection Separation Using Dark Channel Prior</a></p>
<p>10 0.76216078 <a title="269-lda-10" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>11 0.75789672 <a title="269-lda-11" href="./cvpr-2013-Robust_Multi-resolution_Pedestrian_Detection_in_Traffic_Scenes.html">363 cvpr-2013-Robust Multi-resolution Pedestrian Detection in Traffic Scenes</a></p>
<p>12 0.75756925 <a title="269-lda-12" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>13 0.75641328 <a title="269-lda-13" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>14 0.75210273 <a title="269-lda-14" href="./cvpr-2013-Reconstructing_Gas_Flows_Using_Light-Path_Approximation.html">349 cvpr-2013-Reconstructing Gas Flows Using Light-Path Approximation</a></p>
<p>15 0.74919158 <a title="269-lda-15" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>16 0.74556798 <a title="269-lda-16" href="./cvpr-2013-Robust_Feature_Matching_with_Alternate_Hough_and_Inverted_Hough_Transforms.html">361 cvpr-2013-Robust Feature Matching with Alternate Hough and Inverted Hough Transforms</a></p>
<p>17 0.74320352 <a title="269-lda-17" href="./cvpr-2013-Video_Enhancement_of_People_Wearing_Polarized_Glasses%3A_Darkening_Reversal_and_Reflection_Reduction.html">454 cvpr-2013-Video Enhancement of People Wearing Polarized Glasses: Darkening Reversal and Reflection Reduction</a></p>
<p>18 0.74291873 <a title="269-lda-18" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>19 0.74142796 <a title="269-lda-19" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>20 0.73871905 <a title="269-lda-20" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
