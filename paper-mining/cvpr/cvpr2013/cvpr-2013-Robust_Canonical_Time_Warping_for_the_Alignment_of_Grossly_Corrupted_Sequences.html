<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-358" href="#">cvpr2013-358</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</h1>
<br/><p>Source: <a title="cvpr-2013-358-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Panagakis_Robust_Canonical_Time_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>Reference: <a title="cvpr-2013-358-reference" href="../cvpr2013_reference/cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk c}  Abstract Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. [sent-14, score-0.517]
</p><p>2 Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. [sent-15, score-0.052]
</p><p>3 In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. [sent-17, score-0.514]
</p><p>4 The  projections are obtained by minimizing the weighted sum of nuclear and ? [sent-19, score-0.08]
</p><p>5 1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. [sent-20, score-0.404]
</p><p>6 The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets. [sent-21, score-0.481]
</p><p>7 Introduction Accurate temporal alignment of data sequences is a challenging problem raised in bioinformatics [14], speech processing [12, 19], and computer vision [8, 11, 24, 27, 25, 26], among many scientific disciplines. [sent-23, score-0.459]
</p><p>8 The problem is defined as finding the temporal coordinate transformation that Original Sequences  (a)  Error-free common low-rank latent space  (b)  Gross Errors  (c)  Figure 1. [sent-24, score-0.142]
</p><p>9 (b) Alignment of the two data sequences onto an error-free common low-rank latent subspace which has been robustly estimated by the RCTW. [sent-27, score-0.212]
</p><p>10 brings two given data sequences into alignment in time. [sent-29, score-0.344]
</p><p>11 Some particular applications in computer vision include the alignment and the temporal segmentation of human motion [27, 26], the alignment of facial and motion capture data [25], the alignment of Kinect data [24], and view invariant action recognition [8, 11]. [sent-30, score-0.835]
</p><p>12 The dynamic time warping (DTW) [19] aligns two sequences by minimizing the pairwise squared Euclidean distance via dynamic programming. [sent-31, score-0.379]
</p><p>13 Although, the DTW has been widely used for temporal alignment of data sequences, it has two main drawbacks: 1) the DTW fails under arbitrary affine transformations of one or both sequences and 2) cannot handle sequences with different dimensions. [sent-32, score-0.56]
</p><p>14 The CTW aligns two sequences in a common low-dimensional (or low-rank) latent subspace found by the canonical correlation analysis (CCA) [9]. [sent-35, score-0.296]
</p><p>15 The main limitation of the CTW is that, it is unable to handle sequences that lie on different manifolds. [sent-36, score-0.122]
</p><p>16 To this end, the dynamic manifold temporal warping (DMTW) [8] and the manifold warping (MW) [24] extend the CTW to handle more complex spatial transformations through man555343880  ifold learning. [sent-37, score-0.512]
</p><p>17 Since these methods rely on the DTW to find the temporal alignment, it is unclear how to adaptively  constrain the temporal warping [26]. [sent-38, score-0.363]
</p><p>18 This drawback of the aforementioned DTW-based warping methods is addressed by the isotonic cca (ICCA) [21] and the generalized time warping (GTW) [26], where alternative constraints are imposed in order to guarantee monotonicity in the alignment space. [sent-39, score-0.749]
</p><p>19 Despite the success of these methods in practise they are unable to uncover a common low-rank latent space for temporal alignment when high-dimensional data sequences are corrupted by gross non-Gaussian errors. [sent-40, score-0.771]
</p><p>20 Such errors are often occur in real video and motion capture data due to inaccurate tracking, illumination variations, partial occlusions, and gross pixel/angle corruptions. [sent-41, score-0.171]
</p><p>21 Indeed, it is known that the CCA-based alignment methods, discussed in the previous paragraph, are extremely fragile to the presence of gross corruptions [5]. [sent-42, score-0.463]
</p><p>22 In this paper, the robust canonical time warping (RCTW) is proposed for accurate temporal alignment of grossly corrupted high-dimensional data sequences. [sent-44, score-0.727]
</p><p>23 In other words, the RCTW aligns the corrupted sequences in a error-free common low-rank latent subspace which is robustly esti-  mated, even in the presence of gross errors. [sent-46, score-0.529]
</p><p>24 The projections are obtained by minimizing the weighted sum of nuclear and ? [sent-47, score-0.08]
</p><p>25 1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. [sent-48, score-0.404]
</p><p>26 Unlike the small Gaussian noise assumed in the CTW, the GTW, and the ICCA (due to the involvement of the CCA) [10], the RCTW can handle adequately the gross corruptions of large magnitude [1], provided that the corruptions are sparse enough (i. [sent-51, score-0.295]
</p><p>27 The RCTW model is mainly motivated by the success of robust principal component analysis (RPCA) [5] and inductive RPCA (IRPCA) [1] in gross error correction, and especially from the successful combination ofrank minimization principles with spatial alignment [18]. [sent-54, score-0.39]
</p><p>28 , the RCTW, is proposed for accurAat en temporal alignment RofC high-dimensional odar taac se-  quences despite large occlusions and corruptions. [sent-58, score-0.316]
</p><p>29 •  •  An efficient algorithm for the RCTW is derived by solving a sequence homf convex problems. [sent-59, score-0.045]
</p><p>30 Three different sets of experiments on synthetic and rTeharle evid deioff edreatnat v sealtisda otfe ethxpate rtihme proposed nmthetehtoicd accurately aligns grossly corrupted data sequences compared to state-of-the-art alignment methods, namely the CTW [25] and the GTW [26]. [sent-61, score-0.61]
</p><p>31 Notations Throughout the paper, matrices are denoted by uppercase boldface letters (e. [sent-69, score-0.072]
</p><p>32 , X, Y), vectors are denoted by lowercase boldface letters (e. [sent-71, score-0.044]
</p><p>33 , x, y), and scalars appear as either uppercase or lowercase letters (e. [sent-73, score-0.044]
</p><p>34 Dynamic Time Warping Given two data sequences X = [x1|x2 | . [sent-124, score-0.122]
</p><p>35 h|ex DT]W ∈ aligns the sequences by solving y[19]]: aΔrgxm,Δiny  21? [sent-132, score-0.182]
</p><p>36 Δx  ∈  {0, 1}Tx ×T, Δy  (1)  ∈  {0, 1}Ty ×T,  555343991  where Δx and Δy are binary selection matrices encoding the alignment path. [sent-136, score-0.25]
</p><p>37 Although the number of possible alignments is exponential in TxTy, the DTW is able to recover the optimal alignment path in O(TxTy) by employing dynthaem oipc programming. [sent-137, score-0.253]
</p><p>38 Canonical Time Warping  The CTW [25] incorporates CCA into the DTW, allowing the alignment of data sequences of different dimensions by projecting them into a common latent subspace found by CCA [9]. [sent-140, score-0.434]
</p><p>39 Furthermore, the CCA-based projections perform feature selection by reducing the dimensionality of the data to that of the common latent subspace, handling the irrelevant or possibly noisy attributes. [sent-141, score-0.113]
</p><p>40 |bxe tw]o ∈dat Ra sequences of differen|ty dimensionality R(i. [sent-150, score-0.122]
</p><p>41 , dx dy), the CCA is incorporated into the DTW by solving =[25 d]:  =  Vxa,Vrgy,mΔxin,Δy  12? [sent-152, score-0.049]
</p><p>42 (2) Vx ∈ project X and Y, respectively onto∈ a common latent subspace of d? [sent-157, score-0.09]
</p><p>43 ≤ min(dx , dy) dimensions, where the correlation between≤ ≤th me idna(tda sequences is maximized. [sent-158, score-0.122]
</p><p>44 ×dy  The solution of (2) is obtained by solving CCA and DTW in an alternating fashion. [sent-163, score-0.065]
</p><p>45 Robust Canonical Time Warping In this section, the alignment of high-dimensional data sequences in the presence of noise is investigated. [sent-165, score-0.396]
</p><p>46 Provided that the errors in the data sequences follow Gaussian distribution with small variance, the CCA is still able to uncover the common low-rank latent subspace and thus the CTW will accurately align two such noisy data sequences. [sent-166, score-0.29]
</p><p>47 However, in real world conditions, the performance of the CCA and thus that of the CCA-based time warping methods (i. [sent-167, score-0.175]
</p><p>48 , CTW and GTW) is limited since the CCA is not robust to gross corruptions. [sent-169, score-0.149]
</p><p>49 That is, the estimation of the common low-rank latent subspace found by the CCA could be far away from the underlying true common subspace in the presence of gross corruptions [1]. [sent-170, score-0.373]
</p><p>50 To this end, the RCTW is proposed as a robust to gross errors extension of the CTW. [sent-171, score-0.171]
</p><p>51 Consequently, the main aim of the RCTW is to learn two low-rank projections, which are able to uncover a common error-free low-rank subspace for the temporal alignment. [sent-172, score-0.172]
</p><p>52 h|ex highdimensional grossly corrupted ydata] sequences to be aligned, Px ∈ Rd×d and Py ∈ Rd×d are the low-rank projections matr∈ice Rs, and Δx ∈ {0∈, R1}Tx×T, Δy ∈ {0, 1}Ty×T encode the alignment path. [sent-182, score-0.579]
</p><p>53 {B0a,s1e}d on the des∈ire {d0 ,lo1}w-rankness of the projections and the sparsity of the noise, the unknown  matrices Px , Py, Δx , Δy as well as the sparse distortion terms (i. [sent-183, score-0.073]
</p><p>54 0 norm by their convex envelopes as follows, namely by the nuclear norm [7] and the ? [sent-198, score-0.093]
</p><p>55 Problem (4) can be solved iteratively by employing the linearized alternating directions method (LADM) [13], which is a variant of the alternating direction augmented Lagrange multiplier method (ADM) [3]. [sent-212, score-0.117]
</p><p>56 By employing the LADM, (5) is minimized with respect to each variable in an alternating fashion and finally the Lagrange multipliers are updated at each iteration as outlined in Algorithm 1. [sent-234, score-0.096]
</p><p>57 (11) Although the standard procedure for solving nuclear norm regularized least squares problems is the singular value thresholding operator [4], it cannot be directly applied in case of (11), due to the existence of the second term (i. [sent-247, score-0.168]
</p><p>58 Subproblem (10) is solved by applying the DTW on the clean latent spaces defined by Px[t+1]X, Py[t+1]Y. [sent-301, score-0.048]
</p><p>59 Thus the warping matrices are obtained as follows:  [Δx[t+1] , Δy[t+1]] = DTW(Px[t+1]X, Py[t+1]Y). [sent-302, score-0.203]
</p><p>60 7:  8:  Fix the other variables, and update the warping paths Δx[t+1] , Δy[t+1] by: [Δx[t+1] , Δy[t+1]] ← DTW(Px[t+1]X, Py[t+1]Y). [sent-329, score-0.198]
</p><p>61 (21) The dominant cost of each iteration in Algorithm 1is the computation the singular value thresholding operator (i. [sent-366, score-0.082]
</p><p>62 , X ∈ Rdx×Tx and Y ∈ Rdy×Ty with dy dx, then the dimensionality onfd t hYe largest sequence can =be d reduced to that of the smallest by a random projection matrix drawn from a normal zero-mean distribution. [sent-382, score-0.051]
</p><p>63 Furthermore, if both data sequences are highdimensional such as videos, random projections could be  =  applied to both of the for computational tractability. [sent-384, score-0.167]
</p><p>64 Experimental Evaluation In this section, the performance of the RCTW in temporal alignment is assessed by conducting experiments on both synthetic (Subsection 5. [sent-386, score-0.41]
</p><p>65 Performance comparisons are made against the state-of-the-art temporal alignment methods, namely the CTW [25] and the GTW [26]. [sent-390, score-0.316]
</p><p>66 The alignment error is evaluated by employing the following metric [26]:  Err =dist(Π∗,Πmˆ)∗ ++ d miˆ st(Πˆ,Π∗), dist(Π1,Π2)  =? [sent-391, score-0.253]
</p><p>67 That is, a set of 3D spirals data sequences were generated as follows: X = SxZTx ∈ R3×Tx, Y = SyZTy ∈ R3×Ty, where Z ∈ R3×T is the∈ ∈true R latent data sequence. [sent-400, score-0.17]
</p><p>68 S Rx, Sy ∈ w hRer3e×3 Z a ∈nd R Tx ∈ RTx ×T, Ty ∈ RTy×T are random spatial Rand temporal warping matrices∈, respectively. [sent-401, score-0.269]
</p><p>69 Next, both X and Y are corrupted by adding  gross non-gaussian noise to a percentage of samples (i. [sent-402, score-0.271]
</p><p>70 An example of the noisy synthetic data alignment obtained by the CTW, the GTW, and the proposed RCTW is depicted in Fig. [sent-405, score-0.28]
</p><p>71 Clearly, the RCTW smooths the noise of 555444224  Original sequences  (a)  CTW  (b)  GTW  (c)  RCTW  (d)  Figure 2. [sent-407, score-0.144]
</p><p>72 Alignment of synthetic data; 30% of the samples of each sequence have been contaminated by sequences. [sent-408, score-0.061]
</p><p>73 The alignment achieved by (b) the CTW, (c) the GTW, and (d) the RCTW. [sent-409, score-0.222]
</p><p>74 (a) Initial noisy data  Error of alignment  1 40682 40 0 0 501 50RGCT ruTW t horE1086420 CTWG RCTWD Figure 3. [sent-411, score-0.242]
</p><p>75 The mean alignment path (left) and the mean alignment error (right) obtained by the CTW, the GTW, and the RCTW (left) by applying 50 different random spatial and temporal transformations on the latent data sequence Z. [sent-413, score-0.609]
</p><p>76 Mean alignment error obtained by the CTW, the GTW, and the RCTW, as a function of the percentage of corrupted samples on synthetic data sequences. [sent-415, score-0.36]
</p><p>77 the initial data sequencers, yielding a better alignment than the CTW and the GTW. [sent-416, score-0.222]
</p><p>78 3 we present averaged results on 50 data sequences, where the latent data sequence Z is perturbed by 50 different random spatial and temporal transformations. [sent-418, score-0.165]
</p><p>79 The mean alignment error of the compared techniques is presented in Fig. [sent-419, score-0.222]
</p><p>80 It is clear from both figures that the RCTW outperforms the compared approaches, exhibiting a stable and low path alignment error. [sent-421, score-0.222]
</p><p>81 Mean alignment error obtained by the CTW, the GTW and the RCTW on human walking sequences by the KTH. [sent-423, score-0.39]
</p><p>82 Real Data I: Temporal Alignment of Human Walking In this set of experiments, the performance of the RCTW in alignment ofhuman actions is assessed by conducting experiments on the KTH database [20]. [sent-426, score-0.278]
</p><p>83 To this end, 25 pairs of sequences consisting of videos performing the same action (walking) were randomly selected. [sent-427, score-0.16]
</p><p>84 5 the mean alignment error obtained by the CTW, the GTW and the RCTW on corrupted human walking sequences is depicted. [sent-431, score-0.49]
</p><p>85 Clearly, the RCTW outper-  forms the CTW and the GTW with respect to alignment error. [sent-432, score-0.222]
</p><p>86 An illustrative example of aligning occluded human walking sequences with the RCTW is depicted in Fig. [sent-433, score-0.168]
</p><p>87 Real Data II: Temporal Action Unit Alignment The MMI dataset [16] has been employed in order to assess the performance of the RCTW on the temporal alignment of facial expressions. [sent-438, score-0.353]
</p><p>88 In particular, each video contains frame-by-frame annotations of each action unit activated covering all temporal phases (i. [sent-440, score-0.132]
</p><p>89 Alignment of occluded human walking sequences obtained by the RCTW. [sent-444, score-0.168]
</p><p>90 (b) Aligned sequences onto the error-free latent common space which has been robustly estimated by the RCTW. [sent-448, score-0.17]
</p><p>91 Subsequently, we corrupt the facial features with sparse spike noise in order to evaluate the robustness of the compared algorithms. [sent-454, score-0.059]
</p><p>92 In the presented results, the number of features corrupted by noise increases to 4 out of 8 (which essentially means that 50% of our features are corrupted by noise). [sent-465, score-0.222]
</p><p>93 Conclusions By exploiting recent advances on matrix rank minimization and compressive sensing we proposed the first method which simultaneously discovers a subspace, in which two sequences maximally correlate, and in the same time removes possibly gross errors from the data. [sent-468, score-0.34]
</p><p>94 , the RCTW) outperforms the state-of-the-art techniques in temporal alignment of data sequences in the presence of gross errors. [sent-471, score-0.617]
</p><p>95 Linearized alternating direction method with adaptive penalty for low-rank representation. [sent-563, score-0.043]
</p><p>96 555444446  Noisy F eatures (a)  Noisy F eatures (b)  (c)  Figure 7. [sent-564, score-0.044]
</p><p>97 Action Unit alignment comparing the RCTW, the CTW, and the GTW. [sent-565, score-0.222]
</p><p>98 RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images. [sent-608, score-0.222]
</p><p>99 Generalized time warping for multi-modal alignment of human motion. [sent-659, score-0.397]
</p><p>100 Aligned cluster analysis for temporal segmentation of human motion. [sent-668, score-0.094]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rctw', 0.436), ('ctw', 0.363), ('px', 0.339), ('py', 0.251), ('pxx', 0.232), ('alignment', 0.222), ('gtw', 0.193), ('dtw', 0.18), ('warping', 0.175), ('ey', 0.164), ('ex', 0.16), ('cca', 0.155), ('gross', 0.149), ('pyy', 0.145), ('sequences', 0.122), ('corrupted', 0.1), ('tx', 0.094), ('temporal', 0.094), ('grossly', 0.09), ('ty', 0.086), ('corruptions', 0.062), ('ladm', 0.058), ('yx', 0.055), ('latent', 0.048), ('walking', 0.046), ('canonical', 0.046), ('projections', 0.045), ('vxx', 0.044), ('alternating', 0.043), ('xtx', 0.043), ('subspace', 0.042), ('aligns', 0.038), ('action', 0.038), ('singular', 0.038), ('synthetic', 0.038), ('inl', 0.037), ('facial', 0.037), ('uncover', 0.036), ('yty', 0.036), ('rd', 0.036), ('nuclear', 0.035), ('lagrange', 0.033), ('tr', 0.032), ('employing', 0.031), ('xx', 0.031), ('presence', 0.03), ('apex', 0.03), ('variables', 0.03), ('norm', 0.029), ('aregxm', 0.029), ('argpmxin', 0.029), ('arpgxm', 0.029), ('icca', 0.029), ('mihalis', 0.029), ('nicolaou', 0.029), ('txty', 0.029), ('tyytvyt', 0.029), ('vyy', 0.029), ('conducting', 0.029), ('matrices', 0.028), ('dy', 0.028), ('dx', 0.027), ('assessed', 0.027), ('rank', 0.027), ('correlate', 0.026), ('rdx', 0.026), ('adm', 0.026), ('norms', 0.025), ('subproblem', 0.025), ('rx', 0.024), ('pantic', 0.024), ('update', 0.023), ('operator', 0.023), ('sequence', 0.023), ('manifold', 0.023), ('isotonic', 0.022), ('rpca', 0.022), ('lowercase', 0.022), ('uppercase', 0.022), ('mmi', 0.022), ('eatures', 0.022), ('boldface', 0.022), ('solving', 0.022), ('dynamic', 0.022), ('xt', 0.022), ('multipliers', 0.022), ('noise', 0.022), ('errors', 0.022), ('xxt', 0.021), ('imperial', 0.021), ('netherlands', 0.021), ('speech', 0.021), ('thresholding', 0.021), ('fix', 0.021), ('maximally', 0.02), ('noisy', 0.02), ('inductive', 0.019), ('candes', 0.019), ('torre', 0.019), ('acoustics', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="358-tfidf-1" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>2 0.12556089 <a title="358-tfidf-2" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>Author: Xiaojie Guo, Xiaochun Cao, Xiaowu Chen, Yi Ma</p><p>Abstract: Given an area of interest in a video sequence, one may want to manipulate or edit the area, e.g. remove occlusions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency, spatial pose, and visual realism. The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment, pose rectification, as well as precise recovery of the occlusion. To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames, and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust alignment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. Experimental results on both simulated and real data demonstrate the accurate and robust performance of our method.</p><p>3 0.092743464 <a title="358-tfidf-3" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>Author: Liansheng Zhuang, Allen Y. Yang, Zihan Zhou, S. Shankar Sastry, Yi Ma</p><p>Abstract: Single-sample face recognition is one of the most challenging problems in face recognition. We propose a novel face recognition algorithm to address this problem based on a sparse representation based classification (SRC) framework. The new algorithm is robust to image misalignment and pixel corruption, and is able to reduce required training images to one sample per class. To compensate the missing illumination information typically provided by multiple training images, a sparse illumination transfer (SIT) technique is introduced. The SIT algorithms seek additional illumination examples of face images from one or more additional subject classes, and form an illumination dictionary. By enforcing a sparse representation of the query image, the method can recover and transfer the pose and illumination information from the alignment stage to the recognition stage. Our extensive experiments have demonstrated that the new algorithms significantly outperform the existing algorithms in the single-sample regime and with less restrictions. In particular, the face alignment accuracy is comparable to that of the well-known Deformable SRC algorithm using multiple training images; and the face recognition accuracy exceeds those ofthe SRC andExtended SRC algorithms using hand labeled alignment initialization.</p><p>4 0.082017764 <a title="358-tfidf-4" href="./cvpr-2013-What_Makes_a_Patch_Distinct%3F.html">464 cvpr-2013-What Makes a Patch Distinct?</a></p>
<p>Author: Ran Margolin, Ayellet Tal, Lihi Zelnik-Manor</p><p>Abstract: What makes an object salient? Most previous work assert that distinctness is the dominating factor. The difference between the various algorithms is in the way they compute distinctness. Some focus on the patterns, others on the colors, and several add high-level cues and priors. We propose a simple, yet powerful, algorithm that integrates these three factors. Our key contribution is a novel and fast approach to compute pattern distinctness. We rely on the inner statistics of the patches in the image for identifying unique patterns. We provide an extensive evaluation and show that our approach outperforms all state-of-the-art methods on the five most commonly-used datasets.</p><p>5 0.05645873 <a title="358-tfidf-5" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>6 0.055857521 <a title="358-tfidf-6" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>7 0.055669602 <a title="358-tfidf-7" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>8 0.051536333 <a title="358-tfidf-8" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>9 0.049798705 <a title="358-tfidf-9" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>10 0.049306046 <a title="358-tfidf-10" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>11 0.049064487 <a title="358-tfidf-11" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>12 0.048424311 <a title="358-tfidf-12" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>13 0.048021715 <a title="358-tfidf-13" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>14 0.047126774 <a title="358-tfidf-14" href="./cvpr-2013-Dense_Non-rigid_Point-Matching_Using_Random_Projections.html">109 cvpr-2013-Dense Non-rigid Point-Matching Using Random Projections</a></p>
<p>15 0.047080193 <a title="358-tfidf-15" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>16 0.046240583 <a title="358-tfidf-16" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>17 0.0460632 <a title="358-tfidf-17" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>18 0.045960825 <a title="358-tfidf-18" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>19 0.045814455 <a title="358-tfidf-19" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>20 0.045790009 <a title="358-tfidf-20" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.108), (1, 0.006), (2, -0.032), (3, -0.001), (4, -0.048), (5, -0.017), (6, -0.008), (7, -0.063), (8, 0.023), (9, -0.025), (10, 0.026), (11, -0.005), (12, -0.029), (13, -0.007), (14, 0.025), (15, 0.002), (16, -0.014), (17, 0.025), (18, 0.013), (19, 0.03), (20, -0.027), (21, 0.017), (22, 0.018), (23, -0.063), (24, 0.03), (25, -0.021), (26, -0.009), (27, -0.03), (28, 0.023), (29, 0.018), (30, 0.016), (31, -0.044), (32, -0.056), (33, -0.043), (34, -0.055), (35, -0.002), (36, -0.039), (37, -0.039), (38, -0.024), (39, -0.005), (40, -0.035), (41, 0.02), (42, -0.017), (43, 0.011), (44, -0.025), (45, 0.011), (46, -0.039), (47, -0.027), (48, -0.087), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9484719 <a title="358-lsi-1" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>2 0.72535318 <a title="358-lsi-2" href="./cvpr-2013-Video_Editing_with_Temporal%2C_Spatial_and_Appearance_Consistency.html">453 cvpr-2013-Video Editing with Temporal, Spatial and Appearance Consistency</a></p>
<p>Author: Xiaojie Guo, Xiaochun Cao, Xiaowu Chen, Yi Ma</p><p>Abstract: Given an area of interest in a video sequence, one may want to manipulate or edit the area, e.g. remove occlusions from or replace with an advertisement on it. Such a task involves three main challenges including temporal consistency, spatial pose, and visual realism. The proposed method effectively seeks an optimal solution to simultaneously deal with temporal alignment, pose rectification, as well as precise recovery of the occlusion. To make our method applicable to long video sequences, we propose a batch alignment method for automatically aligning and rectifying a small number of initial frames, and then show how to align the remaining frames incrementally to the aligned base images. From the error residual of the robust alignment process, we automatically construct a trimap of the region for each frame, which is used as the input to alpha matting methods to extract the occluding foreground. Experimental results on both simulated and real data demonstrate the accurate and robust performance of our method.</p><p>3 0.63713151 <a title="358-lsi-3" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>Author: Vishnu Naresh Boddeti, Takeo Kanade, B.V.K. Vijaya Kumar</p><p>Abstract: Alignment of 3D objects from 2D images is one of the most important and well studied problems in computer vision. A typical object alignment system consists of a landmark appearance model which is used to obtain an initial shape and a shape model which refines this initial shape by correcting the initialization errors. Since errors in landmark initialization from the appearance model propagate through the shape model, it is critical to have a robust landmark appearance model. While there has been much progress in designing sophisticated and robust shape models, there has been relatively less progress in designing robust landmark detection models. In thispaper wepresent an efficient and robust landmark detection model which is designed specifically to minimize localization errors thereby leading to state-of-the-art object alignment performance. We demonstrate the efficacy and speed of the proposed approach on the challenging task of multi-view car alignment.</p><p>4 0.63213843 <a title="358-lsi-4" href="./cvpr-2013-Procrustean_Normal_Distribution_for_Non-rigid_Structure_from_Motion.html">341 cvpr-2013-Procrustean Normal Distribution for Non-rigid Structure from Motion</a></p>
<p>Author: Minsik Lee, Jungchan Cho, Chong-Ho Choi, Songhwai Oh</p><p>Abstract: Non-rigid structure from motion is a fundamental problem in computer vision, which is yet to be solved satisfactorily. The main difficulty of the problem lies in choosing the right constraints for the solution. In this paper, we propose new constraints that are more effective for non-rigid shape recovery. Unlike the other proposals which have mainly focused on restricting the deformation space using rank constraints, our proposal constrains the motion parameters so that the 3D shapes are most closely aligned to each other, which makes the rank constraints unnecessary. Based on these constraints, we define a new class ofprobability distribution called the Procrustean normal distribution and propose a new NRSfM algorithm, EM-PND. The experimental results show that the proposed method outperforms the existing methods, and it works well even if there is no temporal dependence between the observed samples.</p><p>5 0.62556303 <a title="358-lsi-5" href="./cvpr-2013-Dense_Variational_Reconstruction_of_Non-rigid_Surfaces_from_Monocular_Video.html">113 cvpr-2013-Dense Variational Reconstruction of Non-rigid Surfaces from Monocular Video</a></p>
<p>Author: Ravi Garg, Anastasios Roussos, Lourdes Agapito</p><p>Abstract: This paper offers the first variational approach to the problem of dense 3D reconstruction of non-rigid surfaces from a monocular video sequence. We formulate nonrigid structure from motion (NRSfM) as a global variational energy minimization problem to estimate dense low-rank smooth 3D shapes for every frame along with the camera motion matrices, given dense 2D correspondences. Unlike traditional factorization based approaches to NRSfM, which model the low-rank non-rigid shape using a fixed number of basis shapes and corresponding coefficients, we minimize the rank of the matrix of time-varying shapes directly via trace norm minimization. In conjunction with this low-rank constraint, we use an edge preserving total-variation regularization term to obtain spatially smooth shapes for every frame. Thanks to proximal splitting techniques the optimization problem can be decomposed into many point-wise sub-problems and simple linear systems which can be easily solved on GPU hardware. We show results on real sequences of different objects (face, torso, beating heart) where, despite challenges in tracking, illumination changes and occlusions, our method reconstructs highly deforming smooth surfaces densely and accurately directly from video, without the need for any prior models or shape templates.</p><p>6 0.60139114 <a title="358-lsi-6" href="./cvpr-2013-PDM-ENLOR%3A_Learning_Ensemble_of_Local_PDM-Based_Regressions.html">321 cvpr-2013-PDM-ENLOR: Learning Ensemble of Local PDM-Based Regressions</a></p>
<p>7 0.59583795 <a title="358-lsi-7" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>8 0.58734846 <a title="358-lsi-8" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>9 0.58655179 <a title="358-lsi-9" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>10 0.56471938 <a title="358-lsi-10" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>11 0.55772245 <a title="358-lsi-11" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>12 0.5505451 <a title="358-lsi-12" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>13 0.54432005 <a title="358-lsi-13" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>14 0.53756481 <a title="358-lsi-14" href="./cvpr-2013-Computing_Diffeomorphic_Paths_for_Large_Motion_Interpolation.html">90 cvpr-2013-Computing Diffeomorphic Paths for Large Motion Interpolation</a></p>
<p>15 0.53107375 <a title="358-lsi-15" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>16 0.52964175 <a title="358-lsi-16" href="./cvpr-2013-Dense_Object_Reconstruction_with_Semantic_Priors.html">110 cvpr-2013-Dense Object Reconstruction with Semantic Priors</a></p>
<p>17 0.51578957 <a title="358-lsi-17" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>18 0.51221216 <a title="358-lsi-18" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>19 0.50935084 <a title="358-lsi-19" href="./cvpr-2013-HDR_Deghosting%3A_How_to_Deal_with_Saturation%3F.html">195 cvpr-2013-HDR Deghosting: How to Deal with Saturation?</a></p>
<p>20 0.50712025 <a title="358-lsi-20" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.091), (16, 0.015), (22, 0.01), (26, 0.066), (33, 0.201), (39, 0.015), (67, 0.037), (69, 0.041), (77, 0.356), (87, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75685138 <a title="358-lda-1" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>2 0.75357968 <a title="358-lda-2" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>3 0.74324411 <a title="358-lda-3" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>4 0.67918676 <a title="358-lda-4" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>Author: Tim Matthews, Mark S. Nixon, Mahesan Niranjan</p><p>Abstract: We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pairwise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence ofattributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.</p><p>5 0.67878324 <a title="358-lda-5" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>6 0.63115829 <a title="358-lda-6" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>7 0.62728745 <a title="358-lda-7" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>8 0.58701396 <a title="358-lda-8" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>9 0.57787758 <a title="358-lda-9" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>10 0.57117212 <a title="358-lda-10" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>11 0.56890231 <a title="358-lda-11" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>12 0.56270486 <a title="358-lda-12" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>13 0.56251705 <a title="358-lda-13" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>14 0.56027991 <a title="358-lda-14" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>15 0.55969667 <a title="358-lda-15" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>16 0.55934006 <a title="358-lda-16" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>17 0.55891967 <a title="358-lda-17" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>18 0.55669647 <a title="358-lda-18" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>19 0.55585307 <a title="358-lda-19" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>20 0.55471796 <a title="358-lda-20" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
