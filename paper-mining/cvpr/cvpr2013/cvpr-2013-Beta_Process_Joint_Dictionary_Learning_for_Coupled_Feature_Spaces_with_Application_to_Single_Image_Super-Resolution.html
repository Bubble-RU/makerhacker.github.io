<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-58" href="#">cvpr2013-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</h1>
<br/><p>Source: <a title="cvpr-2013-58-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/He_Beta_Process_Joint_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>Reference: <a title="cvpr-2013-58-reference" href="../cvpr2013_reference/cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. [sent-2, score-1.153]
</p><p>2 A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. [sent-3, score-0.364]
</p><p>3 Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. [sent-4, score-1.423]
</p><p>4 This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. [sent-5, score-1.115]
</p><p>5 The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. [sent-6, score-1.476]
</p><p>6 Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. [sent-7, score-0.685]
</p><p>7 We compare the proposed approach to several state-of-the-art dictionary  learning methods super-resolution. [sent-8, score-0.617]
</p><p>8 Introduction The use of over-complete dictionaries for sparse representation has been the subject of extensive research over the last decade. [sent-11, score-0.42]
</p><p>9 There have been numerous methods proposed to design such overcomplete dictionaries [1, 9, 12, 14, 17, 19, 21]. [sent-14, score-0.384]
</p><p>10 In many signal processing problems, we have coupled feature spaces, e. [sent-16, score-0.352]
</p><p>11 , the image patch space and sketch patch space for photo-sketch abstraction, the original and compressed signal spaces in compressive sensing, and the highresolution patch space and low-resolution patch space in  patch-based image super-resolution. [sent-18, score-0.568]
</p><p>12 The intuitive method to learn dictionaries for coupled feature spaces is using single sparse coding model to learn the coupled dictionaries in concatenated spaces [25]. [sent-19, score-1.76]
</p><p>13 However, dictionaries learned this way usually cannot capture the complex, spatial-variant and nonlinear relationship between the two feature spaces. [sent-20, score-0.496]
</p><p>14 proposed a twostep learning algorithm, where one dictionary is learned by KSVD [1] and the other is generated via least-square. [sent-23, score-0.683]
</p><p>15 Although the dictionaries are learned individually, same coefficients are still used for the two feature spaces, limiting the dictionaries from being customized to both spaces. [sent-24, score-0.948]
</p><p>16 Although the learned dictionaries can better minimize the error in both spaces than those learned in concatenated spaces, the corresponding relationship of dictionaries in the two feature spaces are not captured during the learning process. [sent-26, score-1.314]
</p><p>17 Instead of solving the two optimization problems in two feature spaces together [26], the bilevel method moves one of the optimization problem to the regularization term of the other problem. [sent-28, score-0.375]
</p><p>18 Although the learned sparse representation of bilevel method has less learning errors, the same sparse coding is still required for both feature spaces. [sent-29, score-0.532]
</p><p>19 In this paper, a beta process joint dictionary learning 333444555  (BP-JDL) algorithm is proposed for dictionary learning problems in coupled feature spaces. [sent-30, score-1.86]
</p><p>20 Recent research on using non-parametric Bayesian approach [6, 17] to learn an over-complete dictionary offers several advantages not found in earlier approaches and shows significant improvement in applications such as image denoising, inpainting and compressive sensing [28]. [sent-31, score-0.621]
</p><p>21 However, those approaches are only suitable for dictionary learning in single feature space. [sent-32, score-0.679]
</p><p>22 We propose a new beta process model which is customized for the problem of learning dictionaries in coupled feature spaces. [sent-33, score-1.076]
</p><p>23 Our model, together with [22, 24], provides dictionary learning methods that customized to each feature space, however, our method adds more consistent and accurate mapping between the two feature spaces. [sent-34, score-0.847]
</p><p>24 This is due to the unique property of the beta process model [17] that the sparse representations can be decomposed to values and dictionary atom indicators. [sent-35, score-1.139]
</p><p>25 We use the same beta process prior for dictionary atom indicators but different priors for values in two feature spaces. [sent-36, score-1.122]
</p><p>26 In this way, the proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. [sent-37, score-1.476]
</p><p>27 In addition, in previous over-complete dictionary learning methods, the dictionary size is an unknown parameter and a large-size dictionary is necessary to produce good recovery accuracy. [sent-38, score-1.803]
</p><p>28 BP-JDL may infer dictionary size non-  parametrically and produce the same or better learning accuracies with much smaller dictionary size. [sent-39, score-1.226]
</p><p>29 In order to compare BP-JDL with state-of-the-art coupled feature space dictionary learning methods, we tailor BP-JDL to the dictionary learning problem of the patchbased single image super-resolution. [sent-40, score-1.524]
</p><p>30 Section 3 describes the beta process joint dictionary learning for coupled feature spaces. [sent-44, score-1.243]
</p><p>31 Related Works Many image analysis problems use coupled feature spaces [7, 10, 11, 25]. [sent-48, score-0.465]
</p><p>32 In this paper, we focus on the problem of patch-based single image super-resolution, since several dictionary learning algorithms have been proposed for this application. [sent-49, score-0.617]
</p><p>33 Many methods [4, 20, 22, 24, 25, 27] have been proposed trying to capture the concurrent prior between the low- and high-resolution patches using dictionary learning techniques. [sent-53, score-0.638]
</p><p>34 In these methods, a high-res patch is normally recovered using the high-res dictionary and sparse coefficients calculated using the low-res feature patch and low-res feature dictionary. [sent-54, score-1.022]
</p><p>35 Therefore, we need to learn these two dictionaries in both high-res and low-res feature spaces. [sent-55, score-0.429]
</p><p>36 This is a typical dictionary learning problem in coupled feature spaces. [sent-56, score-0.907]
</p><p>37 The first approach that generated the state-of-the-art SISR result concatenates the two feature spaces together, thus converting the problem to dictionary learning in single feature space. [sent-57, score-0.916]
</p><p>38 Since the learning of an over-complete dictionary is often an NP-hard problem, many approximation algorithms have been proposed, such as RVM [21], KSVD [1], online dictionary learning [12], efficient sparse coding [9], and beta process [17]. [sent-58, score-1.71]
</p><p>39 All these methods are able to generate the over-complete dictionary and sparse coefficients. [sent-59, score-0.653]
</p><p>40 Once the dictionaries are learned, we can use one dictionary to calculate the sparse coefficients and the other dictionary to recover the desired signal. [sent-60, score-1.672]
</p><p>41 However, because the sparse coefficients are shared between the two dictionaries, the algorithm normally finds it difficult to fit the dictionary and coefficients to both feature spaces. [sent-61, score-0.914]
</p><p>42 Therefore, a further learning model is necessary to adapt the dictionary learning algorithm to coupled feature spaces. [sent-62, score-0.952]
</p><p>43 The second approach is to learn dictionary from one  space first then generate the other dictionary via least square. [sent-63, score-1.172]
</p><p>44 Zeyde [27] used this approach for the SISR problem, where the low-res dictionary is learned and the highres dictionary is generated via least square. [sent-64, score-1.247]
</p><p>45 Although this method largely decreases the computational cost because only one dictionary is learned and the dictionary is wellfitted in the low-res patch space, the same is not true in the high-res patch space. [sent-65, score-1.354]
</p><p>46 A simultaneous dictionary learning algorithm is thus essential to balance the learning errors in both feature spaces. [sent-66, score-0.724]
</p><p>47 The most recent approaches, also referred to as the semicoupled approaches [22, 24], seek to improve the learning result by letting the dictionaries fit the two feature spaces better. [sent-67, score-0.643]
</p><p>48 Yang’s method still shares the coefficients between the two feature spaces and both methods did not enforce the corresponding relationship between the learned dictionaries. [sent-69, score-0.408]
</p><p>49 We resolve these two issues by taking advantage of the beta process prior model. [sent-70, score-0.336]
</p><p>50 Recent non-parametric Bayesian approaches such as the Indian Buffet Process (IBP) [6] and the beta process 333444666  (BP) [17] for latent factor analysis have been extensively studied. [sent-71, score-0.365]
</p><p>51 BP is more suitable for dictionary learning compared to IBP because it has more flexibility. [sent-72, score-0.617]
</p><p>52 However, BP  is developed to learn dictionary in single feature space and may not be suitable to learn dictionaries in coupled feature spaces. [sent-73, score-1.319]
</p><p>53 Nevertheless, the truncated beta process allows the sparse coefficients to be expressed as an element-wise multiplication of a binary latent factor indicator and a normal coefficient value. [sent-74, score-0.607]
</p><p>54 We can take advantage of this property in the dictionary learning problem of coupled feature spaces by restraining the coefficients in coupled feature spaces to use the same dictionary atom indicator but different coefficient values. [sent-75, score-2.376]
</p><p>55 Finally, in many applications, the dictionary size and the desired sparsity level need to be manually set [1,26]. [sent-77, score-0.628]
</p><p>56 There have been recent interests in applying non-parametric Bayesian methods [8, 18] to infer the number of dictionary atoms based on the observed data. [sent-79, score-0.679]
</p><p>57 Beta Process Joint Dictionary  Learning for  Coupled Feature Space Suppose we have two coupled feature spaces Y ∈ RPy andS uXp o∈s Rw ePx h , wveh tewreo tchoeu pfeleadtu freeast are sparse Yin ∈ ∈te Rrms  aofn dce Xrtain∈ d Rictionaries. [sent-82, score-0.546]
</p><p>58 , are dictionaries learned in each space and both dictionaries have K atoms. [sent-97, score-0.744]
</p><p>59 In order to learn two dictionaries at the same time, previous algorithms [24, 26] use the same coefficients for both dictionaries, i. [sent-101, score-0.443]
</p><p>60 In this way, one might concatenate two feature spaces and convert the dictionary learning problem of coupled feature spaces to the dictionary learning problem of single feature space. [sent-104, score-1.998]
</p><p>61 However, allowing different coefficients in two feature spaces provides a better fitting of learning and the learned dictionaries are more customized to individual feature space. [sent-105, score-0.891]
</p><p>62 i(x)  αi(x)  αi(y)  αi(y)  multiplication of dictionary atom indicators and coefficient values, providing the much needed flexibility to fit each feature space better while still maintaining the correspondence between the two dictionaries. [sent-108, score-0.859]
</p><p>63 We develop a new beta process based on [28] to tackle the dictionary learning problem in coupled feature spaces. [sent-109, score-1.243]
</p><p>64 The new two-parameter beta process with parameters a, b > 0 and base measure H0, is represented as BP(a, b, H0) and may be written in set function form as ? [sent-110, score-0.336]
</p><p>65 (2)  πk  ∼  −  ∼  where We use a single beta process prior and the same dictionary atom indicator to connect the two feature spaces. [sent-115, score-1.13]
</p><p>66 Following the general structure of beta process described in [28], the beta process joint dictionary learning model for the coupled feature spaces may be expressed as  d(ky)  d(ky)  d(kx) d(kx))  ? [sent-119, score-1.754]
</p><p>67 (y)  ∼  Γ(e, f)  (3) In order to constrain that xi uses the same corresponding dictionary atom as that used by yi, we choose the same dictionary atom indicator zi for both and . [sent-131, score-1.544]
</p><p>68 Because ◦αs(y) ,a wndh eαre(x ◦) use t ehlee same dictionary atom indicator zi, they have the same number of non-zero elements and the corresponding relationship of dictionary atoms in the two feature spaces are enforced during the learning process. [sent-134, score-1.722]
</p><p>69 These N binary column vectors are us∼ed B Btoe rcnoonusltiltiu(πte the dictionary atom indicator matrix Z ∈ {0, 1}K×N, with the ith column corresponding to zi and the kth row associated with both and . [sent-139, score-0.819]
</p><p>70 For the purpose of building a fully conjugate model, the dictionary atoms d(x) k are drawn from a multivariate zero-mean Gaussian (H0) with variance Px−1IPx and the error vectors ? [sent-147, score-0.737]
</p><p>71 Instead of reversing the process directly, Yang [25] suggested that we can use learned dictionaries of high-res feature space and low-res feature space to reconstruct the high-res image. [sent-165, score-0.592]
</p><p>72 The two feature spaces are constructed as: xi  = h; yi = [F1l; F2l; F3l; F4l]  where h is a high-res patch and l is a low-res F1 . [sent-166, score-0.371]
</p><p>73 Once the dictionaries are learned, we can use them for super-resolution reconstruction. [sent-172, score-0.339]
</p><p>74 The first step calculates the sparse coding of observed lowres feature using learned low-res feature dictionary. [sent-174, score-0.33]
</p><p>75 In order to compare our dictionary with dictionaries learned by [22,24,26], we use the standard ? [sent-175, score-0.977]
</p><p>76 The second step maps the sparse coding of the low-res feature to sparse coding of the high-res feature using the learned matrix M. [sent-177, score-0.47]
</p><p>77 All dictionaries are trained from 100,000 patch pairs sampled from 10 category representative and texture rich images. [sent-191, score-0.411]
</p><p>78 We set the initial dictionary size K of BP-JDL as 1024, 2048 and 4096 to test the capability of BP-JDL’s K inference. [sent-193, score-0.591]
</p><p>79 We use 10000 Gibbs samples for BP-JDL, where the burn-in is 9500 samples and the dictionary is averaged using the rest 500 samples. [sent-194, score-0.572]
</p><p>80 1  (6)  Step 2 Map the sparse coefficients α(y) to α(x) using the  learned M:  αi(x) = ziMα(iy)  (7)  if α(iyk)  where zi is a binary vector that zik = 1 = 0. [sent-202, score-0.418]
</p><p>81 1  Dictionary Learning  Firstly, dictionaries in coupled feature spaces are learned using the proposed BP-JDL algorithm. [sent-228, score-0.87]
</p><p>82 Compared to the dictionaries learned in concatenated spaces [26], the dictionaries learned by BP-JDL are able to reduce the learning root-mean-square (RMS) errors of high-res feature space X raonodt -lmowe-arne-ss qfeuaatruere (R space rYr by o2f7 . [sent-229, score-1.11]
</p><p>83 Secondly, the dictionary size inferred by BP-JDL is shown in Figure 1. [sent-236, score-0.615]
</p><p>84 During the Gibbs sampling process, we search the unused dictionary atoms and delete them. [sent-237, score-0.661]
</p><p>85 Because BP-JDL has the non-parametric advantage, with different initial Ks, the dictionary size decreases rapidly during the first 1000 samples and gradually converges to similar values, confirming that BP-JDL can infer appropriate dictionary size no matter what the initial value is. [sent-238, score-1.225]
</p><p>86 With the initial size of 1024, the BP-JDL inferred that K = 771 is an appropriate dictionary size. [sent-239, score-0.615]
</p><p>87 If we fix the dictionary size to 1024 for BP-JDL, the learning RMS errors and sparsity level of the 1024-size dictionaries stay the same as the 771-size dictionaries, indicating that 771 is the appropriate dictionary size for the training data. [sent-240, score-1.603]
</p><p>88 If the dictionary size is unknown, normally we need exhaustively search for the optimal size. [sent-241, score-0.616]
</p><p>89 Yang [26] found that the the 1024-size dictionary is optimal, however, the 771-size dictionary may have the same super-resolution performance as the 1024size dictionary. [sent-242, score-1.144]
</p><p>90 Besides, since super-resolution using a smaller size dictionary needs less computational power, it may significantly affect the speed and energy consumption of super-resolution applications in resource-constrained environments. [sent-243, score-0.591]
</p><p>91 , bicubic), because the over-complete dictionaries can recover high-frequency details of images more accurately. [sent-254, score-0.371]
</p><p>92 Although BP-JDL benefits from using a smaller dictionary compared to ScSR, the extra operation of Eq. [sent-275, score-0.572]
</p><p>93 SCDL is the slowest method because it needs 32 dictionaries (clusters) for each feature space instead of single dictionary, thus consuming much more time than other methods. [sent-280, score-0.401]
</p><p>94 Conclusion In this paper, a beta process joint dictionary learning (BP-JDL) method was proposed for solving the dictionary learning problem in coupled feature spaces. [sent-282, score-1.86]
</p><p>95 Four state-of-the-art dictionary learning based SISR methods were compared with BP-JDL in terms of the quality of dictionary generated and the quality of the superresolution images. [sent-285, score-1.246]
</p><p>96 The experimental results showed that the  BP-JDL method is able to learn dictionaries that fit the coupled feature spaces better than previous methods. [sent-286, score-0.854]
</p><p>97 In addition, BP-JDL was able to infer an appropriate dictionary size non-parametrically. [sent-295, score-0.609]
</p><p>98 K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. [sent-334, score-0.465]
</p><p>99 Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis. [sent-496, score-0.617]
</p><p>100 Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images. [sent-549, score-0.654]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dictionary', 0.572), ('dictionaries', 0.339), ('beta', 0.301), ('kx', 0.252), ('coupled', 0.228), ('spaces', 0.175), ('ky', 0.168), ('sisr', 0.15), ('bilevel', 0.138), ('atom', 0.126), ('zeyde', 0.112), ('zik', 0.108), ('sr', 0.098), ('scdl', 0.094), ('scsr', 0.094), ('atoms', 0.089), ('zi', 0.087), ('sparse', 0.081), ('si', 0.077), ('coefficients', 0.076), ('patch', 0.072), ('customized', 0.066), ('learned', 0.066), ('ssim', 0.064), ('feature', 0.062), ('coding', 0.059), ('iy', 0.059), ('gamma', 0.057), ('superresolution', 0.057), ('ikx', 0.056), ('gibbs', 0.056), ('drawn', 0.051), ('paisley', 0.05), ('psnr', 0.05), ('bh', 0.046), ('learning', 0.045), ('hi', 0.045), ('overcomplete', 0.045), ('signal', 0.042), ('mapping', 0.04), ('infinite', 0.039), ('highres', 0.037), ('sparsity', 0.037), ('bayesian', 0.037), ('dk', 0.036), ('process', 0.035), ('yi', 0.035), ('indicator', 0.034), ('overlap', 0.034), ('ibp', 0.033), ('bp', 0.033), ('recover', 0.032), ('asks', 0.031), ('downsample', 0.031), ('multiplication', 0.03), ('relationship', 0.029), ('ksvd', 0.029), ('buffet', 0.029), ('factor', 0.029), ('yang', 0.028), ('learn', 0.028), ('reversing', 0.028), ('ix', 0.027), ('xi', 0.027), ('bicubic', 0.027), ('px', 0.026), ('indicators', 0.026), ('olshausen', 0.026), ('normally', 0.025), ('conjugate', 0.025), ('confirming', 0.025), ('bringing', 0.025), ('indian', 0.025), ('inferred', 0.024), ('representations', 0.024), ('ik', 0.023), ('highresolution', 0.023), ('recovery', 0.023), ('luminance', 0.023), ('abstraction', 0.023), ('bernoulli', 0.023), ('py', 0.022), ('fit', 0.022), ('coefficient', 0.021), ('patches', 0.021), ('rms', 0.021), ('denoising', 0.021), ('compressive', 0.021), ('processing', 0.02), ('elad', 0.02), ('sketch', 0.019), ('super', 0.019), ('size', 0.019), ('ks', 0.019), ('reconstructed', 0.018), ('reconstruction', 0.018), ('enforced', 0.018), ('infer', 0.018), ('concatenated', 0.018), ('wright', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="58-tfidf-1" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>2 0.52501142 <a title="58-tfidf-2" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>Author: Li Shen, Shuhui Wang, Gang Sun, Shuqiang Jiang, Qingming Huang</p><p>Abstract: For the task of visual categorization, the learning model is expected to be endowed with discriminative visual feature representation and flexibilities in processing many categories. Many existing approaches are designed based on a flat category structure, or rely on a set of pre-computed visual features, hence may not be appreciated for dealing with large numbers of categories. In this paper, we propose a novel dictionary learning method by taking advantage of hierarchical category correlation. For each internode of the hierarchical category structure, a discriminative dictionary and a set of classification models are learnt for visual categorization, and the dictionaries in different layers are learnt to exploit the discriminative visual properties of different granularity. Moreover, the dictionaries in lower levels also inherit the dictionary of ancestor nodes, so that categories in lower levels are described with multi-scale visual information using our dictionary learning approach. Experiments on ImageNet object data subset and SUN397 scene dataset demonstrate that our approach achieves promising performance on data with large numbers of classes compared with some state-of-the-art methods, and is more efficient in processing large numbers of categories.</p><p>3 0.52426177 <a title="58-tfidf-3" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>4 0.4260197 <a title="58-tfidf-4" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>Author: Sumit Shekhar, Vishal M. Patel, Hien V. Nguyen, Rama Chellappa</p><p>Abstract: Data-driven dictionaries have produced state-of-the-art results in various classification tasks. However, when the target data has a different distribution than the source data, the learned sparse representation may not be optimal. In this paper, we investigate if it is possible to optimally represent both source and target by a common dictionary. Specifically, we describe a technique which jointly learns projections of data in the two domains, and a latent dictionary which can succinctly represent both the domains in the projected low-dimensional space. An efficient optimization technique is presented, which can be easily kernelized and extended to multiple domains. The algorithm is modified to learn a common discriminative dictionary, which can be further used for classification. The proposed approach does not require any explicit correspondence between the source and target domains, and shows good results even when there are only a few labels available in the target domain. Various recognition experiments show that the methodperforms onparor better than competitive stateof-the-art methods.</p><p>5 0.40931466 <a title="58-tfidf-5" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>6 0.39154169 <a title="58-tfidf-6" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>7 0.38379276 <a title="58-tfidf-7" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>8 0.32130119 <a title="58-tfidf-8" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>9 0.30019444 <a title="58-tfidf-9" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>10 0.28997645 <a title="58-tfidf-10" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>11 0.26503402 <a title="58-tfidf-11" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>12 0.22956061 <a title="58-tfidf-12" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>13 0.18855542 <a title="58-tfidf-13" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>14 0.1464929 <a title="58-tfidf-14" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>15 0.14030109 <a title="58-tfidf-15" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>16 0.11388407 <a title="58-tfidf-16" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>17 0.10277064 <a title="58-tfidf-17" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>18 0.10174884 <a title="58-tfidf-18" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>19 0.094830021 <a title="58-tfidf-19" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>20 0.094529726 <a title="58-tfidf-20" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, -0.203), (2, -0.348), (3, 0.418), (4, -0.174), (5, -0.15), (6, 0.149), (7, 0.184), (8, -0.036), (9, 0.117), (10, 0.01), (11, 0.075), (12, 0.008), (13, 0.042), (14, 0.044), (15, 0.006), (16, 0.02), (17, 0.051), (18, 0.004), (19, 0.013), (20, 0.012), (21, 0.047), (22, -0.002), (23, 0.021), (24, -0.01), (25, -0.044), (26, -0.051), (27, -0.036), (28, -0.05), (29, 0.026), (30, 0.018), (31, -0.035), (32, -0.011), (33, 0.022), (34, 0.022), (35, -0.01), (36, 0.04), (37, -0.032), (38, -0.046), (39, -0.011), (40, 0.062), (41, 0.011), (42, 0.009), (43, 0.016), (44, 0.043), (45, 0.017), (46, 0.008), (47, 0.002), (48, 0.006), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97936565 <a title="58-lsi-1" href="./cvpr-2013-Separable_Dictionary_Learning.html">392 cvpr-2013-Separable Dictionary Learning</a></p>
<p>Author: Simon Hawe, Matthias Seibert, Martin Kleinsteuber</p><p>Abstract: Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary proper- , ties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.</p><p>same-paper 2 0.97009844 <a title="58-lsi-2" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>3 0.93051374 <a title="58-lsi-3" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>4 0.9241271 <a title="58-lsi-4" href="./cvpr-2013-Online_Robust_Dictionary_Learning.html">315 cvpr-2013-Online Robust Dictionary Learning</a></p>
<p>Author: Cewu Lu, Jiaping Shi, Jiaya Jia</p><p>Abstract: Online dictionary learning is particularly useful for processing large-scale and dynamic data in computer vision. It, however, faces the major difficulty to incorporate robust functions, rather than the square data fitting term, to handle outliers in training data. In thispaper, wepropose a new online framework enabling the use of ?1 sparse data fitting term in robust dictionary learning, notably enhancing the usability and practicality of this important technique. Extensive experiments have been carried out to validate our new framework.</p><p>5 0.88264304 <a title="58-lsi-5" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>6 0.87217879 <a title="58-lsi-6" href="./cvpr-2013-Multi-level_Discriminative_Dictionary_Learning_towards_Hierarchical_Visual_Categorization.html">296 cvpr-2013-Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</a></p>
<p>7 0.81036043 <a title="58-lsi-7" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>8 0.78466928 <a title="58-lsi-8" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>9 0.69838375 <a title="58-lsi-9" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>10 0.65650439 <a title="58-lsi-10" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>11 0.59187192 <a title="58-lsi-11" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>12 0.58921081 <a title="58-lsi-12" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<p>13 0.54295695 <a title="58-lsi-13" href="./cvpr-2013-Histograms_of_Sparse_Codes_for_Object_Detection.html">204 cvpr-2013-Histograms of Sparse Codes for Object Detection</a></p>
<p>14 0.48782328 <a title="58-lsi-14" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>15 0.44255474 <a title="58-lsi-15" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>16 0.41140515 <a title="58-lsi-16" href="./cvpr-2013-Subspace_Interpolation_via_Dictionary_Learning_for_Unsupervised_Domain_Adaptation.html">419 cvpr-2013-Subspace Interpolation via Dictionary Learning for Unsupervised Domain Adaptation</a></p>
<p>17 0.38797322 <a title="58-lsi-17" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>18 0.38636273 <a title="58-lsi-18" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>19 0.35697308 <a title="58-lsi-19" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>20 0.34540939 <a title="58-lsi-20" href="./cvpr-2013-Multipath_Sparse_Coding_Using_Hierarchical_Matching_Pursuit.html">304 cvpr-2013-Multipath Sparse Coding Using Hierarchical Matching Pursuit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.106), (16, 0.024), (19, 0.012), (26, 0.027), (28, 0.015), (33, 0.278), (39, 0.021), (67, 0.046), (69, 0.074), (71, 0.237), (87, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87088811 <a title="58-lda-1" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>Author: Wen-Sheng Chu, Fernando De La Torre, Jeffery F. Cohn</p><p>Abstract: Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+ [20], GEMEP-FERA [32] and RU-FACS [2]. STM outperformed generic classifiers in all.</p><p>2 0.84604448 <a title="58-lda-2" href="./cvpr-2013-Universality_of_the_Local_Marginal_Polytope.html">448 cvpr-2013-Universality of the Local Marginal Polytope</a></p>
<p>Author: unkown-author</p><p>Abstract: We show that solving the LP relaxation of the MAP inference problem in graphical models (also known as the minsum problem, energy minimization, or weighted constraint satisfaction) is not easier than solving any LP. More precisely, any polytope is linear-time representable by a local marginal polytope and any LP can be reduced in linear time to a linear optimization (allowing infinite weights) over a local marginal polytope.</p><p>same-paper 3 0.83625919 <a title="58-lda-3" href="./cvpr-2013-Beta_Process_Joint_Dictionary_Learning_for_Coupled_Feature_Spaces_with_Application_to_Single_Image_Super-Resolution.html">58 cvpr-2013-Beta Process Joint Dictionary Learning for Coupled Feature Spaces with Application to Single Image Super-Resolution</a></p>
<p>Author: Li He, Hairong Qi, Russell Zaretzki</p><p>Abstract: This paper addresses the problem of learning overcomplete dictionaries for the coupled feature spaces, where the learned dictionaries also reflect the relationship between the two spaces. A Bayesian method using a beta process prior is applied to learn the over-complete dictionaries. Compared to previous couple feature spaces dictionary learning algorithms, our algorithm not only provides dictionaries that customized to each feature space, but also adds more consistent and accurate mapping between the two feature spaces. This is due to the unique property of the beta process model that the sparse representation can be decomposed to values and dictionary atom indicators. The proposed algorithm is able to learn sparse representations that correspond to the same dictionary atoms with the same sparsity but different values in coupled feature spaces, thus bringing consistent and accurate mapping between coupled feature spaces. Another advantage of the proposed method is that the number of dictionary atoms and their relative importance may be inferred non-parametrically. We compare the proposed approach to several state-of-the-art dictionary learning methods super-resolution. tionaries learned resolution results ods. by applying this method to single image The experimental results show that dicby our method produces the best supercompared to other state-of-the-art meth-</p><p>4 0.80864543 <a title="58-lda-4" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>Author: Fang Wang, Yi Li</p><p>Abstract: Simple tree models for articulated objects prevails in the last decade. However, it is also believed that these simple tree models are not capable of capturing large variations in many scenarios, such as human pose estimation. This paper attempts to address three questions: 1) are simple tree models sufficient? more specifically, 2) how to use tree models effectively in human pose estimation? and 3) how shall we use combined parts together with single parts efficiently? Assuming we have a set of single parts and combined parts, and the goal is to estimate a joint distribution of their locations. We surprisingly find that no latent variables are introduced in the Leeds Sport Dataset (LSP) during learning latent trees for deformable model, which aims at approximating the joint distributions of body part locations using minimal tree structure. This suggests one can straightforwardly use a mixed representation of single and combined parts to approximate their joint distribution in a simple tree model. As such, one only needs to build Visual Categories of the combined parts, and then perform inference on the learned latent tree. Our method outperformed the state of the art on the LSP, both in the scenarios when the training images are from the same dataset and from the PARSE dataset. Experiments on animal images from the VOC challenge further support our findings.</p><p>5 0.80691218 <a title="58-lda-5" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>Author: Byung-Woo Hong, Zhaojin Lu, Ganesh Sundaramoorthi</p><p>Abstract: In this work, we address the multi-label Mumford-Shah problem, i.e., the problem of jointly estimating a partitioning of the domain of the image, and functions defined within regions of the partition. We create algorithms that are efficient, robust to undesirable local minima, and are easy-toimplement. Our algorithms are formulated by slightly modifying the underlying statistical model from which the multilabel Mumford-Shah functional is derived. The advantage of this statistical model is that the underlying variables: the labels and thefunctions are less coupled than in the original formulation, and the labels can be computed from the functions with more global updates. The resulting algorithms can be tuned to the desired level of locality of the solution: from fully global updates to more local updates. We demonstrate our algorithm on two applications: joint multi-label segmentation and denoising, and joint multi-label motion segmentation and flow estimation. We compare to the stateof-the-art in multi-label Mumford-Shah problems and show that we achieve more promising results.</p><p>6 0.79703754 <a title="58-lda-6" href="./cvpr-2013-Probabilistic_Label_Trees_for_Efficient_Large_Scale_Image_Classification.html">340 cvpr-2013-Probabilistic Label Trees for Efficient Large Scale Image Classification</a></p>
<p>7 0.79673278 <a title="58-lda-7" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>8 0.79609609 <a title="58-lda-8" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>9 0.79538852 <a title="58-lda-9" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>10 0.79345083 <a title="58-lda-10" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>11 0.79284894 <a title="58-lda-11" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<p>12 0.79272503 <a title="58-lda-12" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>13 0.79226553 <a title="58-lda-13" href="./cvpr-2013-Discriminative_Re-ranking_of_Diverse_Segmentations.html">132 cvpr-2013-Discriminative Re-ranking of Diverse Segmentations</a></p>
<p>14 0.79098803 <a title="58-lda-14" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>15 0.79091561 <a title="58-lda-15" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>16 0.79057574 <a title="58-lda-16" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>17 0.79055887 <a title="58-lda-17" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>18 0.7904405 <a title="58-lda-18" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>19 0.79004151 <a title="58-lda-19" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>20 0.78985935 <a title="58-lda-20" href="./cvpr-2013-Tensor-Based_High-Order_Semantic_Relation_Transfer_for_Semantic_Scene_Segmentation.html">425 cvpr-2013-Tensor-Based High-Order Semantic Relation Transfer for Semantic Scene Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
