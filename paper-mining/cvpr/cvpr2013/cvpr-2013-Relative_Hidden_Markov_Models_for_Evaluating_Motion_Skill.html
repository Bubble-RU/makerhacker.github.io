<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-353" href="#">cvpr2013-353</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</h1>
<br/><p>Source: <a title="cvpr-2013-353-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_Relative_Hidden_Markov_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Qiang Zhang, Baoxin Li</p><p>Abstract: This paper is concerned with a novel problem: learning temporal models using only relative information. Such a problem arises naturally in many applications involving motion or video data. Our focus in this paper is on videobased surgical training, in which a key task is to rate the performance of a trainee based on a video capturing his motion. Compared with the conventional method of relying on ratings from senior surgeons, an automatic approach to this problem is desirable for its potential lower cost, better objectiveness, and real-time availability. To this end, we propose a novel formulation termed Relative Hidden Markov Model and develop an algorithm for obtaining a solution under this model. The proposed method utilizes only a relative ranking (based on an attribute of interest) between pairs of the inputs, which is easier to obtain and often more consistent, especially for the chosen application domain. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration is linked to the likelihood of the inputs under the learned model. Hence the model can be used to compare new sequences. Synthetic data is first used to systematically evaluate the model and the algorithm, and then we experiment with real data from a surgical training system. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video.</p><p>Reference: <a title="cvpr-2013-353-reference" href="../cvpr2013_reference/cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu i  Abstract This paper is concerned with a novel problem: learning temporal models using only relative information. [sent-3, score-0.203]
</p><p>2 Our focus in this paper is on videobased surgical training, in which a key task is to rate the performance of a trainee based on a video capturing his motion. [sent-5, score-0.493]
</p><p>3 The proposed method utilizes only a relative ranking (based on an attribute of interest) between pairs of the inputs, which is easier to obtain and often more consistent, especially for the chosen application domain. [sent-8, score-0.344]
</p><p>4 The proposed algorithm effectively learns a model from the training data so that the attribute under consideration is linked to the likelihood of the inputs under the learned model. [sent-9, score-0.465]
</p><p>5 Synthetic data is first used to systematically evaluate the model and the algorithm, and then we experiment with real data from a surgical training system. [sent-11, score-0.585]
</p><p>6 The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video. [sent-12, score-0.542]
</p><p>7 Sensory data that capture such motion may be analyzed to provide a computational understanding of such differences, which may in turn be used to facilitate tasks such as skill evaluation and training. [sent-17, score-0.573]
</p><p>8 Among those fields, surgery is one domain where motion expertise is of the primary concern. [sent-19, score-0.213]
</p><p>9 Often a surgeon has to go through lengthy training programs that aim at improving his/her motion skills. [sent-20, score-0.209]
</p><p>10 As a result, simulation-based training platforms have been developed  and widely adopted in surgical education. [sent-21, score-0.491]
</p><p>11 Accordingly, computational approaches have been developed for motion skill analysis on such training platforms. [sent-26, score-0.638]
</p><p>12 For example, [14] provided an HMM-based method to evaluate surgical residents’ learning curve. [sent-28, score-0.423]
</p><p>13 HMM was also adopted in [7] to measure motion skills in surgical tasks, where the video is first segmented into basic gestures based on velocity and angle of movement, with segments of the gestures corresponding to the states of an HMM. [sent-31, score-0.721]
</p><p>14 One practical difficulty in these approaches is that they require the skill labels for the training data since the HMMs are typically learned from data of each skill level. [sent-32, score-1.172]
</p><p>15 Labeling the skill of a trainee is currently done by senior surgeons, which is not only a costly practice but also one that is subjective and less quantifiable. [sent-33, score-0.595]
</p><p>16 Thus it is difficult, if not impossible, to obtain sufficient and consistent skill labels for a large amount of data for reliable HMM training. [sent-34, score-0.502]
</p><p>17 For example, in [12], it was argued that using 555444668  binary label to describe the image is not only too restrictive but also unnatural and thus relative visual attributes were used and classifiers were trained based on such features. [sent-36, score-0.168]
</p><p>18 The proposed method utilizes only a relative ranking (based on an attribute of interest, or motion skill in the surgical training application) between pairs of the inputs, which is easier to obtain and often more consistent. [sent-41, score-1.377]
</p><p>19 The proposed algorithm effectively learns a model from the training data so that the attribute under consideration (i. [sent-43, score-0.204]
</p><p>20 , the motion skill in our application) is linked to the likelihood of the inputs under the learned model. [sent-45, score-0.803]
</p><p>21 For evaluation, we first design synthetic experiments to systematically evaluate the model and the algorithm, and then experiment with real data captured  on a commonly-used surgical training platform. [sent-47, score-0.603]
</p><p>22 The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video. [sent-48, score-0.542]
</p><p>23 The key contribution of the work lies in the novel formulation of learning temporal models using only relative information and the proposed algorithm for obtaining solutions under the formulation. [sent-49, score-0.203]
</p><p>24 Additional contributions include the specific application of the proposed method to the problem of video-based motion skill evaluation in surgical training, which has seen increasing importance in recent years. [sent-50, score-0.937]
</p><p>25 Related Work In this section, we review two categories of existing work, discriminative learning for hidden Markov models and learning based on relative information, which are most related to our effort. [sent-52, score-0.291]
</p><p>26 These methods are “supervised” in nature, and thus the labeling of the state sequence is required for the training data, which limits their practical use. [sent-61, score-0.182]
</p><p>27 Instead, only a relative ranking of the training data is used, and the resultant model is a valid HMM. [sent-66, score-0.396]
</p><p>28 Learning with relative information: Several methods for learning with relative information have been proposed recently. [sent-67, score-0.234]
</p><p>29 In [16], a distance metric is learned from relative comparisons. [sent-68, score-0.175]
</p><p>30 Considering the limited training examples for object recognition, [19] proposes an approach based on comparative objective similarities, where the learned model scores high for objects of similar categories and low for objects of dissimilar categories. [sent-69, score-0.232]
</p><p>31 In [9], comparative facial attributes were learned for face verification. [sent-70, score-0.169]
</p><p>32 The method of [12] learns relative attributes for image classification and the problem is formulated as a variation of SVM. [sent-71, score-0.168]
</p><p>33 An HMM can be defined by a set of parameters: the initial transition probabilities π ∈ RK×1, the state transition probabilities A ∈ RK×K and the observation model {φk}kK=1, where K is the number of states. [sent-83, score-0.165]
</p><p>34 There are two central problems in HMM: 1) learning a model from the given training data; and 2) evaluating the probability of a sequence under a given model, i. [sent-84, score-0.215]
</p><p>35 In the learning problem, one learns the model (θ) by 555444779  maximizing the likelihood of the training data (X):  θ∗ : mθax  ? [sent-87, score-0.311]
</p><p>36 When the training data include sequences of multiple categories, multiple models would be learned and each model will be learned from data of each category independently. [sent-95, score-0.4]
</p><p>37 In the decoding problem, given a hidden Markov model, one needs to determine the probability of a given sequence X being generated by the model. [sent-96, score-0.187]
</p><p>38 Proposed Method Based on the previous discussion, we are concerned with a new problem of learning temporal models using only relative information. [sent-103, score-0.203]
</p><p>39 In the case of video-based surgical training, the focus is on learning to rate/compare the performance of the trainees from recorded videos capturing their motion. [sent-105, score-0.61]
</p><p>40 :  (2)  F(Xi, θ) > F(Xj , θ), ∀(i, j) ∈ E  where F(X, θ) is a score function for data X given by model θ, which is introduced to maintain the relative ranking of the pair Xi and Xj, and E is the set of given pairs with prior ranking constraint. [sent-111, score-0.528]
</p><p>41 In an existing HMM-based method, a set of models is trained using the training data of each category independently. [sent-114, score-0.159]
</p><p>42 The model explicitly considers the ranking constraint between given data pairs, whereas independentlytrained HMMs in existing methods can’t guarantee it. [sent-118, score-0.197]
</p><p>43 (3)  : p(Xi|θ) > p(Xj |θ), ∀(i, j) ∈ E  It has been proved in [11] that, the marginal likelihood is dominated by the likelihood with the optimal path and their difference decreases exponentially with regarding to the length (number of frames) of sequence. [sent-143, score-0.3]
</p><p>44 This idea was used in segmental K-means algorithm and similarly we can approximate the marginal data likelihood p(X|θ) by the likelihood with optimal path p(X, z∗ |θ) (when there is no ambiguity, we will use z for z∗), which can be written as: logp(X, z|θ)  =  logp(X1 |φz1 ) + log π(z1) T  +  ? [sent-144, score-0.504]
</p><p>45 Then the log likelihood with the optimal path can be written as: logp(Xi,zi|θ)  =  ? [sent-152, score-0.279]
</p><p>46 ψTyi ≥ ψTyj + ρ ∀(i,j) ∈ E where ρ ≥ 0 defines the required margin between the logarithms of likelihood for a pair of data and Ω defines the set of valid parameters for the hidden Markov model, i. [sent-160, score-0.255]
</p><p>47 3, we assumed that every pairwise ranking constraint provided in the data is correct (or valid). [sent-167, score-0.165]
</p><p>48 Now, we are ready to describe the proposed learning algorithm: The Baseline Algorithm Input: X, E, ρ, γ Output: θ Initialization: Initialize θ (and ψ) via ordinary HMM learning algorithm; while NOT terminated Compute the optimal path z for each sequence;  Update the model ψ according to Eqn. [sent-185, score-0.192]
</p><p>49 8; end Convert ψ to θ; After the model is learned, it can be used to a testing pair: For each sequence we evaluate the data likelihood via the Viterbi algorithm and use the logarithm of the data likelihood as the score of the data. [sent-186, score-0.609]
</p><p>50 8, we compare the logarithm of the data likelihood, which is, according to Eqn. [sent-191, score-0.177]
</p><p>51 , repeating an action multiple times within a sequence, we may consider normalizing the logarithm of the data likelihood by the number of frames of the observation. [sent-197, score-0.301]
</p><p>52 Recall that in HMM, we classify a sequence based on the model with which the sequence gets the maximal likelihood, i. [sent-200, score-0.182]
</p><p>53 , it is the ratio of data likelihood with different models that decides the label of the data. [sent-202, score-0.22]
</p><p>54 ij ≥ 0 ∀ (i, j) ∈ E  (9)  where Ξ1 is the set of data associated with Model θ1 (Ξ2 for Model θ2), is the optimal path for sequence  zi  555544 919  zi  xi with Model θ1 and for optimal path with Model θ2. [sent-215, score-0.316]
</p><p>55 We may view them as the centers of two clusters, where the distances of the data to those two centers can be related to the ranking score. [sent-224, score-0.165]
</p><p>56 Here, the proposed model trains two ”sub-models” jointly with only relative ranking constraints. [sent-227, score-0.269]
</p><p>57 The dimension of this problem is K(1+K+D)+ |E| (or 2K(1+K+D)+ |E|) with 2|E| +K(1+K+D) (or 2|E| +2K(1 +K+D)) linear inequality constraints and 1+ K + D (or 2(1 + K + D)) nonlinear equality constraints for the baseline model (or the improved model). [sent-243, score-0.219]
</p><p>58 The algorithm is terminated when at least one of the following condition satisfied: the maximal number of iterations is achieved; all of the training pair get correctly ranked; the model (i. [sent-246, score-0.212]
</p><p>59 While there is no guar-  antee on the convergence, empirically it was found that after a certain number of iterations the learned model starts to deliver reasonable results (in terms of the percentage of the training pairs getting correctly-maintained ranking). [sent-255, score-0.262]
</p><p>60 Experiments In this section, we evaluate the proposed methods, including the baseline method and the improved method, using both synthetic data (Sec. [sent-257, score-0.207]
</p><p>61 1) and realistic data collected from the surgical training platform FLS box (Sec. [sent-259, score-0.559]
</p><p>62 For the sequences from each data-generating model, we randomly assign 50 of them to the training set and the remaining to the testing set. [sent-268, score-0.16]
</p><p>63 A set of pairs {(i, j) |Xi ∼ θk, Xj ∼ θk+1 , k = 1, · · · , 5} are then formed accordingly, some of which are then randomly selected as the training pairs E. [sent-271, score-0.22]
</p><p>64 The result of the methods with different number of training pairs is summarized in Fig. [sent-279, score-0.158]
</p><p>65 1, we can find that the improved method achieves the best results on both the training set and the testing set; and the HMM method gives the worse result. [sent-283, score-0.215]
</p><p>66 Normalizing the logarithm of data likelihood does not improve the performance of baseline method, which could be explained by that, all the sequences have roughly the same length, i. [sent-287, score-0.373]
</p><p>67 2 shows the logarithm of the data likelihood ratio with the models learned by the improved method, when about 1250 training pairs are provided. [sent-291, score-0.685]
</p><p>68 This clearly demonstrates that, although we formed the training pairs only with data from data-generating models of adjacent indices (i. [sent-292, score-0.221]
</p><p>69 , iand i+ 1), the learned model is able to recover the strict ranking of the original data. [sent-294, score-0.238]
</p><p>70 It is obvious from this experiment that the sequences are different from (or similar to) each other only because they are from different (or the same) data-generating models, whereas their relative ranking can be arbitrarily defined. [sent-301, score-0.271]
</p><p>71 This suggests that, as long as  we can assume there are some data-generating models for the given sequential data, we can use the proposed methods to learn a relative HMM. [sent-303, score-0.169]
</p><p>72 The results of four methods on training set (dashed curve) and testing set (solid curve) with different numbers of training pairs. [sent-306, score-0.222]
</p><p>73 The logarithm of the data likelihood ratio with the models learned by the improved method. [sent-308, score-0.527]
</p><p>74 Skill Evaluation Using Surgical Training Video We now evaluate the proposed method using real videos captured from the FLS trainer box, which has been widely used in surgical training. [sent-314, score-0.489]
</p><p>75 The data set contains 546 videos captured from 18 subjects performing the “peg transfer” operation, which is one of the standard training tasks a resident surgeon needs to perform and pass. [sent-315, score-0.311]
</p><p>76 The convergence behavior of the improved method, around 1250 training pairs were used. [sent-320, score-0.247]
</p><p>77 In the existing practice, senior surgeons rate the performance of the trainees based on such videos. [sent-323, score-0.288]
</p><p>78 The data set covers a training period offour weeks, with every trainee performing three sessions each week. [sent-325, score-0.243]
</p><p>79 , a later video is associated with a better skill) based on the reasonable assumption that the trainees improve their skills over time (which is the whole point of having the resident surgeons going through the training before taking the exam). [sent-328, score-0.523]
</p><p>80 , there is no rank information between videos of different subjects (which would be hard to obtain anyway, since there is no clearly-defined skill levels for a group of trainees with diverse background). [sent-331, score-0.715]
</p><p>81 Based on this, we randomly pick 300 pairs as the training pairs, similarly as in the experiment using synthetic data. [sent-332, score-0.207]
</p><p>82 After learning the models from the training data, we compute the score of the test data as the logarithm of data likelihood (for the baseline method) or the logarithm of the data likelihood ratio (for the improved method and the HMM). [sent-340, score-0.981]
</p><p>83 4 shows the computed scores with the learned models, where for better illustration purpose we group them by their subject ID and within each subjects’ corpus we sort the videos by their recording time. [sent-353, score-0.217]
</p><p>84 It is worth emphasizing that only one joint model is learned from ranked pairs of subjects with potentially varying skill levels. [sent-357, score-0.694]
</p><p>85 Still the learned model is able to recover the improving trend, independent of the underlying skill levels. [sent-358, score-0.575]
</p><p>86 5 depicts the two models learned by the improved method in this realdata based experiment. [sent-361, score-0.193]
</p><p>87 This may be linked to different motion patterns for data of different surgical skills. [sent-364, score-0.528]
</p><p>88 Discussions and Conclusions In this paper, we presented a new formulation for the problem of learning temporal models using only relative information. [sent-366, score-0.203]
</p><p>89 Such a setting is useful for many practical applications where relative attributes are easier to obtain while explicit labeling is difficult to get. [sent-369, score-0.168]
</p><p>90 The application of video-based surgical training was the focus of this study, and the evaluation results using realistic data suggests that the proposed method provides a promis-  ing solution to the problem of motion skill evaluation from videos. [sent-370, score-1.064]
</p><p>91 Top: the logarithm of the data likelihood ratio from two models learned by HMM. [sent-374, score-0.438]
</p><p>92 Middle: the logarithm of data likelihood with the model learned by the baseline method. [sent-375, score-0.443]
</p><p>93 Bottom: the logarithm of the data likelihood ratio with the models learned by the improved method. [sent-376, score-0.527]
</p><p>94 Model 1  Model 2  2 for Ξ2) learned by the improved method, where we only draw the edges with a transition probability larger than 0. [sent-379, score-0.214]
</p><p>95 Discriminative training methods for hidden  [4]  [5]  [6]  [7]  [8]  [9]  [10] [11]  markov models: Theory and experiments with perceptron algorithms. [sent-401, score-0.323]
</p><p>96 Analyzing human skill through control trajectories and motion capture data. [sent-411, score-0.542]
</p><p>97 The segmental¡ e1¿ k¡/e1¿-means algorithm for estimating parameters of hidden markov models. [sent-418, score-0.189]
</p><p>98 Maximum likelihood hidden markov modeling using a dominant sequence of states. [sent-458, score-0.372]
</p><p>99 Task decomposition of laparoscopic surgery for objective evaluation of surgical residents’ learning curve using hidden markov model. [sent-478, score-0.759]
</p><p>100 Support vector machine training for improved hidden markov modeling. [sent-493, score-0.374]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('skill', 0.471), ('hmm', 0.401), ('surgical', 0.395), ('hmms', 0.19), ('logarithm', 0.146), ('trainees', 0.14), ('ranking', 0.134), ('skills', 0.127), ('likelihood', 0.124), ('surgery', 0.109), ('relative', 0.103), ('hidden', 0.1), ('training', 0.096), ('zt', 0.095), ('surgeons', 0.094), ('markov', 0.089), ('improved', 0.089), ('tyi', 0.073), ('learned', 0.072), ('motion', 0.071), ('log', 0.07), ('fls', 0.07), ('segmental', 0.07), ('trainee', 0.07), ('tyj', 0.07), ('viterbi', 0.069), ('attributes', 0.065), ('ij', 0.065), ('pairs', 0.062), ('vec', 0.06), ('sequence', 0.059), ('xi', 0.057), ('subjects', 0.057), ('senior', 0.054), ('logp', 0.054), ('transition', 0.053), ('path', 0.052), ('terminated', 0.052), ('synthetic', 0.049), ('videos', 0.047), ('axx', 0.047), ('console', 0.047), ('loga', 0.047), ('trainer', 0.047), ('xyi', 0.047), ('zti', 0.047), ('likelihoods', 0.047), ('sessions', 0.046), ('attribute', 0.045), ('xt', 0.044), ('xj', 0.044), ('oi', 0.043), ('rk', 0.043), ('surgeon', 0.042), ('residents', 0.042), ('novice', 0.042), ('peg', 0.042), ('temporal', 0.04), ('perceptron', 0.038), ('laparoscopic', 0.038), ('resident', 0.038), ('sports', 0.038), ('states', 0.038), ('baseline', 0.038), ('platform', 0.037), ('accordingly', 0.037), ('logpp', 0.036), ('movement', 0.036), ('watanabe', 0.035), ('sequences', 0.034), ('corpus', 0.034), ('inputs', 0.034), ('sequential', 0.034), ('written', 0.033), ('expertise', 0.033), ('ratio', 0.033), ('comparative', 0.032), ('model', 0.032), ('recording', 0.032), ('score', 0.032), ('models', 0.032), ('subject', 0.032), ('maximal', 0.032), ('linked', 0.031), ('equality', 0.031), ('gestures', 0.031), ('data', 0.031), ('parikh', 0.03), ('converts', 0.03), ('testing', 0.03), ('ax', 0.029), ('rating', 0.029), ('nonlinear', 0.029), ('discrimination', 0.029), ('learning', 0.028), ('kovashka', 0.028), ('decoding', 0.028), ('notations', 0.028), ('video', 0.028), ('state', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="353-tfidf-1" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>Author: Qiang Zhang, Baoxin Li</p><p>Abstract: This paper is concerned with a novel problem: learning temporal models using only relative information. Such a problem arises naturally in many applications involving motion or video data. Our focus in this paper is on videobased surgical training, in which a key task is to rate the performance of a trainee based on a video capturing his motion. Compared with the conventional method of relying on ratings from senior surgeons, an automatic approach to this problem is desirable for its potential lower cost, better objectiveness, and real-time availability. To this end, we propose a novel formulation termed Relative Hidden Markov Model and develop an algorithm for obtaining a solution under this model. The proposed method utilizes only a relative ranking (based on an attribute of interest) between pairs of the inputs, which is easier to obtain and often more consistent, especially for the chosen application domain. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration is linked to the likelihood of the inputs under the learned model. Hence the model can be used to compare new sequences. Synthetic data is first used to systematically evaluate the model and the algorithm, and then we experiment with real data from a surgical training system. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video.</p><p>2 0.19006832 <a title="353-tfidf-2" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>3 0.11323808 <a title="353-tfidf-3" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>Author: Rui Li, Pengcheng Shi, Anne R. Haake</p><p>Abstract: Eliciting and representing experts ’ remarkable perceptual capability of locating, identifying and categorizing objects in images specific to their domains of expertise will benefit image understanding in terms of transferring human domain knowledge and perceptual expertise into image-based computational procedures. In this paper, we present a hierarchical probabilistic framework to summarize the stereotypical and idiosyncratic eye movement patterns shared within 11 board-certified dermatologists while they are examining and diagnosing medical images. Each inferred eye movement pattern characterizes the similar temporal and spatial properties of its corresponding seg. edu anne .haake @ rit . edu , ments of the experts ’ eye movement sequences. We further discover a subset of distinctive eye movement patterns which are commonly exhibited across multiple images. Based on the combinations of the exhibitions of these eye movement patterns, we are able to categorize the images from the perspective of experts’ viewing strategies. In each category, images share similar lesion distributions and configurations. The performance of our approach shows that modeling physicians ’ diagnostic viewing behaviors informs about medical images’ understanding to correct diagnosis.</p><p>4 0.09764193 <a title="353-tfidf-4" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>5 0.09329199 <a title="353-tfidf-5" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>Author: Ziheng Wang, Shangfei Wang, Qiang Ji</p><p>Abstract: Spatial-temporal relations among facial muscles carry crucial information about facial expressions yet have not been thoroughly exploited. One contributing factor for this is the limited ability of the current dynamic models in capturing complex spatial and temporal relations. Existing dynamic models can only capture simple local temporal relations among sequential events, or lack the ability for incorporating uncertainties. To overcome these limitations and take full advantage of the spatio-temporal information, we propose to model the facial expression as a complex activity that consists of temporally overlapping or sequential primitive facial events. We further propose the Interval Temporal Bayesian Network to capture these complex temporal relations among primitive facial events for facial expression modeling and recognition. Experimental results on benchmark databases demonstrate the feasibility of the proposed approach in recognizing facial expressions based purely on spatio-temporal relations among facial muscles, as well as its advantage over the existing methods.</p><p>6 0.085787483 <a title="353-tfidf-6" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>7 0.078251205 <a title="353-tfidf-7" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>8 0.07301335 <a title="353-tfidf-8" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>9 0.071375377 <a title="353-tfidf-9" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>10 0.070239864 <a title="353-tfidf-10" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>11 0.070166491 <a title="353-tfidf-11" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>12 0.069875151 <a title="353-tfidf-12" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>13 0.068463691 <a title="353-tfidf-13" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>14 0.068414859 <a title="353-tfidf-14" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>15 0.065638393 <a title="353-tfidf-15" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>16 0.065166131 <a title="353-tfidf-16" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>17 0.064965941 <a title="353-tfidf-17" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>18 0.063507527 <a title="353-tfidf-18" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>19 0.063140228 <a title="353-tfidf-19" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>20 0.062683299 <a title="353-tfidf-20" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, -0.041), (2, -0.031), (3, -0.039), (4, -0.001), (5, 0.028), (6, -0.073), (7, -0.037), (8, 0.009), (9, 0.068), (10, 0.048), (11, 0.009), (12, -0.031), (13, 0.002), (14, 0.013), (15, 0.053), (16, 0.01), (17, 0.051), (18, 0.002), (19, -0.037), (20, -0.028), (21, -0.04), (22, 0.001), (23, -0.009), (24, 0.003), (25, 0.026), (26, -0.022), (27, -0.008), (28, 0.005), (29, 0.057), (30, 0.014), (31, -0.038), (32, -0.047), (33, -0.001), (34, 0.002), (35, -0.025), (36, -0.033), (37, -0.026), (38, -0.087), (39, -0.02), (40, 0.005), (41, -0.003), (42, -0.004), (43, 0.062), (44, -0.08), (45, -0.013), (46, -0.034), (47, 0.012), (48, -0.004), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90196943 <a title="353-lsi-1" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>Author: Qiang Zhang, Baoxin Li</p><p>Abstract: This paper is concerned with a novel problem: learning temporal models using only relative information. Such a problem arises naturally in many applications involving motion or video data. Our focus in this paper is on videobased surgical training, in which a key task is to rate the performance of a trainee based on a video capturing his motion. Compared with the conventional method of relying on ratings from senior surgeons, an automatic approach to this problem is desirable for its potential lower cost, better objectiveness, and real-time availability. To this end, we propose a novel formulation termed Relative Hidden Markov Model and develop an algorithm for obtaining a solution under this model. The proposed method utilizes only a relative ranking (based on an attribute of interest) between pairs of the inputs, which is easier to obtain and often more consistent, especially for the chosen application domain. The proposed algorithm effectively learns a model from the training data so that the attribute under consideration is linked to the likelihood of the inputs under the learned model. Hence the model can be used to compare new sequences. Synthetic data is first used to systematically evaluate the model and the algorithm, and then we experiment with real data from a surgical training system. The experimental results suggest that the proposed approach provides a promising solution to the real-world problem of motion skill evaluation from video.</p><p>2 0.73960084 <a title="353-lsi-2" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>Author: Weixin Li, Qian Yu, Harpreet Sawhney, Nuno Vasconcelos</p><p>Abstract: In this work, we propose a novel video representation for activity recognition that models video dynamics with attributes of activities. A video sequence is decomposed into short-term segments, which are characterized by the dynamics of their attributes. These segments are modeled by a dictionary of attribute dynamics templates, which are implemented by a recently introduced generative model, the binary dynamic system (BDS). We propose methods for learning a dictionary of BDSs from a training corpus, and for quantizing attribute sequences extracted from videos into these BDS codewords. This procedure produces a representation of the video as a histogram of BDS codewords, which is denoted the bag-of-words for attribute dynamics (BoWAD). An extensive experimental evaluation reveals that this representation outperforms other state-of-the-art approaches in temporal structure modeling for complex ac- tivity recognition.</p><p>3 0.65060639 <a title="353-lsi-3" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>Author: Guha Balakrishnan, Fredo Durand, John Guttag</p><p>Abstract: We extract heart rate and beat lengths from videos by measuring subtle head motion caused by the Newtonian reaction to the influx of blood at each beat. Our method tracks features on the head and performs principal component analysis (PCA) to decompose their trajectories into a set of component motions. It then chooses the component that best corresponds to heartbeats based on its temporal frequency spectrum. Finally, we analyze the motion projected to this component and identify peaks of the trajectories, which correspond to heartbeats. When evaluated on 18 subjects, our approach reported heart rates nearly identical to an electrocardiogram device. Additionally we were able to capture clinically relevant information about heart rate variability.</p><p>4 0.62793547 <a title="353-lsi-4" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>Author: Yale Song, Louis-Philippe Morency, Randall Davis</p><p>Abstract: Recent progress has shown that learning from hierarchical feature representations leads to improvements in various computer vision tasks. Motivated by the observation that human activity data contains information at various temporal resolutions, we present a hierarchical sequence summarization approach for action recognition that learns multiple layers of discriminative feature representations at different temporal granularities. We build up a hierarchy dynamically and recursively by alternating sequence learning and sequence summarization. For sequence learning we use CRFs with latent variables to learn hidden spatiotemporal dynamics; for sequence summarization we group observations that have similar semantic meaning in the latent space. For each layer we learn an abstract feature representation through non-linear gate functions. This procedure is repeated to obtain a hierarchical sequence summary representation. We develop an efficient learning method to train our model and show that its complexity grows sublinearly with the size of the hierarchy. Experimental results show the effectiveness of our approach, achieving the best published results on the ArmGesture and Canal9 datasets.</p><p>5 0.62457907 <a title="353-lsi-5" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>6 0.61452848 <a title="353-lsi-6" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>7 0.61156946 <a title="353-lsi-7" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>8 0.59952706 <a title="353-lsi-8" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>9 0.593472 <a title="353-lsi-9" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>10 0.59044039 <a title="353-lsi-10" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>11 0.58811074 <a title="353-lsi-11" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>12 0.58600926 <a title="353-lsi-12" href="./cvpr-2013-Accurate_and_Robust_Registration_of_Nonrigid_Surface_Using_Hierarchical_Statistical_Shape_Model.html">31 cvpr-2013-Accurate and Robust Registration of Nonrigid Surface Using Hierarchical Statistical Shape Model</a></p>
<p>13 0.58283961 <a title="353-lsi-13" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>14 0.57918477 <a title="353-lsi-14" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>15 0.57628381 <a title="353-lsi-15" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>16 0.57430714 <a title="353-lsi-16" href="./cvpr-2013-Dynamic_Scene_Classification%3A_Learning_Motion_Descriptors_with_Slow_Features_Analysis.html">137 cvpr-2013-Dynamic Scene Classification: Learning Motion Descriptors with Slow Features Analysis</a></p>
<p>17 0.57092863 <a title="353-lsi-17" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>18 0.56861967 <a title="353-lsi-18" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>19 0.56225032 <a title="353-lsi-19" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>20 0.56042743 <a title="353-lsi-20" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.095), (16, 0.014), (26, 0.364), (33, 0.271), (67, 0.059), (69, 0.033), (87, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92319781 <a title="353-lda-1" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>Author: Adrien Bartoli, Toby Collins</p><p>Abstract: It has been shown that a surface deforming isometrically can be reconstructed from a single image and a template 3D shape. Methods from the literature solve this problem efficiently. However, they all assume that the camera model is calibrated, which drastically limits their applicability. We propose (i) a general variational framework that applies to (calibrated and uncalibrated) general camera models and (ii) self-calibrating 3D reconstruction algorithms for the weak-perspective and full-perspective camera models. In the former case, our algorithm returns the normal field and camera ’s scale factor. In the latter case, our algorithm returns the normal field, depth and camera ’s focal length. Our algorithms are the first to achieve deformable 3D reconstruction including camera self-calibration. They apply to much more general setups than existing methods. Experimental results on simulated and real data show that our algorithms give results with the same level of accuracy as existing methods (which use the true focal length) on perspective images, and correctly find the normal field on affine images for which the existing methods fail.</p><p>2 0.91635948 <a title="353-lda-2" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>Author: Liang Li, Wei Feng, Liang Wan, Jiawan Zhang</p><p>Abstract: This paper addresses a challenging problem of regularizing arbitrary superpixels into an optimal grid structure, which may significantly extend current low-level vision algorithms by allowing them to use superpixels (SPs) conveniently as using pixels. For this purpose, we aim at constructing maximum cohesive SP-grid, which is composed of real nodes, i.e. SPs, and dummy nodes that are meaningless in the image with only position-taking function in the grid. For a given formation of image SPs and proper number of dummy nodes, we first dynamically align them into a grid based on the centroid localities of SPs. We then define the SP-grid coherence as the sum of edge weights, with SP locality and appearance encoded, along all direct paths connecting any two nearest neighboring real nodes in the grid. We finally maximize the SP-grid coherence via cascade dynamic programming. Our approach can take the regional objectness as an optional constraint to produce more semantically reliable SP-grids. Experiments on object localization show that our approach outperforms state-of-the-art methods in terms of both detection accuracy and speed. We also find that with the same searching strategy and features, object localization at SP-level is about 100-500 times faster than pixel-level, with usually better detection accuracy.</p><p>3 0.90156788 <a title="353-lda-3" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>Author: Jordi Pont-Tuset, Ferran Marques</p><p>Abstract: This paper tackles the supervised evaluation of image segmentation algorithms. First, it surveys and structures the measures used to compare the segmentation results with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the goodness of these measures, it defines three quantitative meta-measures involving six state of the art segmentation methods. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion, this paper proposes the precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p><p>4 0.88603663 <a title="353-lda-4" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>Author: Tobias Baumgartner, Dennis Mitzel, Bastian Leibe</p><p>Abstract: Current pedestrian tracking approaches ignore important aspects of human behavior. Humans are not moving independently, but they closely interact with their environment, which includes not only other persons, but also different scene objects. Typical everyday scenarios include people moving in groups, pushing child strollers, or pulling luggage. In this paper, we propose a probabilistic approach for classifying such person-object interactions, associating objects to persons, and predicting how the interaction will most likely continue. Our approach relies on stereo depth information in order to track all scene objects in 3D, while simultaneously building up their 3D shape models. These models and their relative spatial arrangement are then fed into a probabilistic graphical model which jointly infers pairwise interactions and object classes. The inferred interactions can then be used to support tracking by recovering lost object tracks. We evaluate our approach on a novel dataset containing more than 15,000 frames of personobject interactions in 325 video sequences and demonstrate good performance in challenging real-world scenarios.</p><p>5 0.87141287 <a title="353-lda-5" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>6 0.84483021 <a title="353-lda-6" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>same-paper 7 0.80652678 <a title="353-lda-7" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>8 0.80347031 <a title="353-lda-8" href="./cvpr-2013-Compressible_Motion_Fields.html">88 cvpr-2013-Compressible Motion Fields</a></p>
<p>9 0.77226406 <a title="353-lda-9" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>10 0.75998706 <a title="353-lda-10" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>11 0.74461311 <a title="353-lda-11" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>12 0.73765969 <a title="353-lda-12" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>13 0.73526299 <a title="353-lda-13" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>14 0.73397416 <a title="353-lda-14" href="./cvpr-2013-Sparse_Subspace_Denoising_for_Image_Manifolds.html">405 cvpr-2013-Sparse Subspace Denoising for Image Manifolds</a></p>
<p>15 0.73280185 <a title="353-lda-15" href="./cvpr-2013-Templateless_Quasi-rigid_Shape_Modeling_with_Implicit_Loop-Closure.html">424 cvpr-2013-Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure</a></p>
<p>16 0.73156571 <a title="353-lda-16" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>17 0.73148501 <a title="353-lda-17" href="./cvpr-2013-Optimal_Geometric_Fitting_under_the_Truncated_L2-Norm.html">317 cvpr-2013-Optimal Geometric Fitting under the Truncated L2-Norm</a></p>
<p>18 0.73085386 <a title="353-lda-18" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>19 0.73030484 <a title="353-lda-19" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>20 0.7297734 <a title="353-lda-20" href="./cvpr-2013-Hyperbolic_Harmonic_Mapping_for_Constrained_Brain_Surface_Registration.html">208 cvpr-2013-Hyperbolic Harmonic Mapping for Constrained Brain Surface Registration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
