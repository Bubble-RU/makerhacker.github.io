<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>415 cvpr-2013-Structured Face Hallucination</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-415" href="#">cvpr2013-415</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>415 cvpr-2013-Structured Face Hallucination</h1>
<br/><p>Source: <a title="cvpr-2013-415-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Structured_Face_Hallucination_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>Reference: <a title="cvpr-2013-415-reference" href="../cvpr2013_reference/cvpr-2013-Structured_Face_Hallucination_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Structured Face Hallucination Chih-Yuan Yang Sifei Liu Ming-Hsuan Yang Electrical Engineering and Computer Science University of California at Merced  {cyang3 5  ,  sl 2 iu3  ,  Abstract The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. [sent-1, score-0.473]
</p><p>2 In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. [sent-2, score-0.196]
</p><p>3 Each face image is represented in terms of facial components, contours and smooth regions. [sent-3, score-0.425]
</p><p>4 For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. [sent-5, score-0.338]
</p><p>5 A patch matching method is utilized on the smooth regions where the image gradients are preserved. [sent-7, score-0.201]
</p><p>6 Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability. [sent-8, score-0.252]
</p><p>7 Introduction  Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) images from low-resolution (LR) inputs, which finds numerous vision applications. [sent-10, score-0.329]
</p><p>8 Since a LR image can be modeled from a HR image by a linear convolution process with downsampling, the hallucination problem can be viewed as an inverse task to reconstruct the high-frequency details. [sent-11, score-0.263]
</p><p>9 In this paper, we propose a face hallucination algorithm that exploits domain-specific image structures to generate HR results with high fidelity. [sent-13, score-0.424]
</p><p>10 A landmark detection algorithm is utilized to locate facial components and contours, and process facial alignment in both frontal faces and those at different poses. [sent-15, score-0.675]
</p><p>11 In this work, the exemplar face dataset consists both LR face images and the corresponding HR ones. [sent-16, score-0.51]
</p><p>12 The landmark points of each HR exemplar face mhyang} @ucmerced . [sent-17, score-0.463]
</p><p>13 From the set of HR exemplar images, the corresponding LR images with landmarks and labels are generated. [sent-19, score-0.333]
</p><p>14 Given a test LR image, the pose and landmark points are extracted from an intermediate HR image via bicubic  interpolation. [sent-20, score-0.214]
</p><p>15 Based on the pose and landmark points, the aligned facial components of the input images are compared with those of the training LR images. [sent-21, score-0.459]
</p><p>16 The LR exemplar images with most similar components are selected, and their gradients are preserved in reconstructing the output HR image. [sent-22, score-0.463]
</p><p>17 To preserve the structure of edges, we generate HR edges through an anisotropic interpolation and restore the sharpness of edges via statistical priors. [sent-23, score-0.431]
</p><p>18 For other remaining smooth region, we generate the HR details through a patch match method. [sent-24, score-0.22]
</p><p>19 Extensive experiments with comparisons to the state-of-the-art methods show that high-quality images with richer details can be generated by the proposed algorithm without assuming faces are well aligned, at fixed pose and without facial expression change. [sent-26, score-0.409]
</p><p>20 Related Work In contrast to generic super-resolution algorithms, recent work in face hallucination aims to learn the mapping between HR and LR patches from a set of exemplar images to recover the missing details of an input frame. [sent-28, score-0.71]
</p><p>21 In [1], the relationship between LR and HR image patches are modeled in a probabilistic framework such that high-frequency details can be transferred from exemplar images for face hallucination. [sent-29, score-0.525]
</p><p>22 For every query patch cropped from an input image, the most similar LR patch is retrieved from an exemplar set and the corresponding HR patch is transferred in terms of the first and second order derivatives. [sent-30, score-0.454]
</p><p>23 The gen-  erated face images contain significantly richer details than those by bicubic interpolation, but some artifacts also can be introduced as the transferred HR patches are not structurally consistent although their LR patches are similar to the LR test patches. [sent-31, score-0.441]
</p><p>24 (e) Based on the component masks, the corresponding HR components are found and the set of gradient maps Uc are generated. [sent-38, score-0.289]
</p><p>25 (f)  Priors are used to restore edge sharpness and generate the set of gradient maps Ue. [sent-39, score-0.432]
</p><p>26 (g) Three sets of gradient maps based on components, edges and smooth regions are generated. [sent-40, score-0.283]
</p><p>27 In [9], a face hallucination method is proposed which enforces linear constraints for HR face images using a subspace learned from a set of training images via Principal Component Analysis (PCA). [sent-45, score-0.552]
</p><p>28 The global linear constraints of subspace representations are replaced by multiple local constraints learned from exemplar patches [10]. [sent-52, score-0.337]
</p><p>29 When the exemplar and test images are precisely aligned with similar appearance, the adopted local linear constraints are effective as the mapping between HR and LR local patches can be modeled via manifold learning [3]. [sent-53, score-0.371]
</p><p>30 However, the resulting HR images may contain significant noisy artifacts along contours since the number of training patches collected along edges are relatively less than that of smooth regions and thus the sparse representation dictionary is not effective in reconstructing these regions. [sent-56, score-0.31]
</p><p>31 This method performs well when training faces are highly similar to the test face in terms ofthe identity, pose, and expression. [sent-58, score-0.209]
</p><p>32 Proposed Algorithm Given a LR test image Il, we generate a set of HR gradient maps U from exemplar images such that we can generate a HR image Ih based on matching HR gradients and LR intensity values by Ih= argImin? [sent-61, score-0.689]
</p><p>33 GWaue group image asntrduc ↓t ruerepsr eosfe a tfsa ace d ionwton tsharmeep lcinatgegories including facial components, edges, and smooth regions, whose gradients are generated by specific methods to produce the best visual quality. [sent-66, score-0.356]
</p><p>34 The gradients of facial components are transferred from the corresponding components of exemplar images to preserve the consistency of highfrequency details. [sent-67, score-0.755]
</p><p>35 In addition, we ex-  ploit the similarity between the test image and the training images to drive an efficient patch matching algorithm to reduce the computational load of retrieving exemplar patches. [sent-73, score-0.374]
</p><p>36 Gradient Maps for Facial Components In order to generate effective gradients of facial components, we prepare a dataset in which every face image is associated with a set of landmark points and two label sets indicating the pose of the face and the existence of glasses on the face. [sent-77, score-0.867]
</p><p>37 The landmark points are used to generate an aligned image while the pose and glasses labels restrict the search domains. [sent-78, score-0.406]
</p><p>38 Given a LR test image Il, we generate an intermediate HR image Ib by bicubic interpolation, localize its landmark points and estimate the pose of the test image using [17]. [sent-80, score-0.29]
</p><p>39 We use the estimated results to select a set of exemplar images in the dataset which have the same pose as Ib. [sent-81, score-0.32]
</p><p>40 Each face is annotated by several landmark points such that all the facial components and contours are known (Figure 1(b)). [sent-82, score-0.538]
</p><p>41 Suppose a facial component is annotated by n landmark points denoted as {xbi, ybi}in=1 of Ib and t{axteei,d y beiy}in n= l1a nodfm an exemplar image. [sent-83, score-0.569]
</p><p>42 We use the estimated parameters to generate an aligned exemplar image, denoted by H. [sent-93, score-0.371]
</p><p>43 Note that the alignment is carried out for each facial component individually, which is different from existing methods [9, 16, 10] in which faces are aligned based on eyes locations. [sent-94, score-0.398]
</p><p>44 The proposed alignment approach is more flexible for dealing with face images containing various expressions and shapes because they cannot be effectively aligned by eye positions only. [sent-95, score-0.216]
</p><p>45 Suppose {Hj } is a set of aligned iHniRn exemplar images fso. [sent-97, score-0.319]
</p><p>46 We generate the corresponding LR exemplar image Lj = (Hj ⊗ G) ↓,  (3)  and compare Lj and the Il to determine the best exemplar image for the component c. [sent-101, score-0.622]
</p><p>47 Based on the landmark points belonging to component c estimated from Ib, we create a HR mask map  Figure 2. [sent-103, score-0.188]
</p><p>48 3 and determine the best exemplar by  j∗= argj∈mSinp? [sent-112, score-0.248]
</p><p>49 The index set S is determined by the component c and the labels of glasses associated with the exemplar images {Hj }. [sent-116, score-0.461]
</p><p>50 As we determine the best exemplar by comparing features in LR, to prevent artifacts caused by selecting an incorrect HR exemplar image, we utilize the labels of glasses to exclude gradient maps Vj from index set S if the component c may be covered by glasses. [sent-120, score-0.897]
</p><p>51 On the contrary, if the component c is irrelevant to glasses such as a mouth, all Vj are included in S. [sent-121, score-0.189]
</p><p>52 Figure 2 shows an example of four matched images based on different facial components (the dark boundary of each LR exemplar is the result of alignment). [sent-122, score-0.55]
</p><p>53 Note that the best matched facial components are matched from images of different subjects. [sent-123, score-0.336]
</p><p>54 Once the best LR exemplar image Lj∗ is determined for a component, we transfer the gradients of the corresponding source HR image Hj∗ for the pixels whose values in the mask Mh are 1 as the gradients in the set of gradient maps Uc. [sent-125, score-0.65]
</p><p>55 The same process is carried out for each component to generate the most effective image gradients, and together they form the gradient map Uc. [sent-126, score-0.21]
</p><p>56 (c) The set of LR similarity maps are upsampled to a set of HR maps through bilinear interpolation to preserve the directions of edges. [sent-131, score-0.433]
</p><p>57 Although the generated edges are visually pleasing, the HR image may contain significant artifacts (especially along sharp edges) as they are generated by enhancing the contrast of edges from a bicubic interpolated image where edges are jaggy. [sent-138, score-0.396]
</p><p>58 In this work, we propose to preserve the structure of edges and restore their sharpness through learned statistical priors. [sent-139, score-0.245]
</p><p>59 Rather than generating sharp edges based on interpolated images, we develop a direction-preserving upsampling function that eliminates the artifacts for prior learning. [sent-140, score-0.21]
</p><p>60 We use t,h teh upsampled rdviirengcti tohneal d isriemc-ilarity maps {Tk} to regularize an under-constrained optiimlaizriattyio mn problem  Id= argImin? [sent-157, score-0.244]
</p><p>61 Edges in the upsampled image Id are clear and smooth but the not sharp enough because the sharpness is not modeled in the  regularization term of Eq. [sent-167, score-0.335]
</p><p>62 Since the structure of edges are highly symmetric with greatest magnitude of gradients along the center, we label the pixels at edge centers (Figure 4(d)) in Id by  C(p) =? [sent-173, score-0.244]
</p><p>63 We collect millions of samples from images of the exemplar dataset, and separate the domain of (mp, mc, t) into thousands of bins. [sent-179, score-0.272]
</p><p>64 Suppose Ud are the gradient maps of Id, we generate the Suppose Ud is the set of the gradient maps of Id, we generate the set of gradient maps for facial contours Ue by  Ue(p) =m m¯? [sent-190, score-0.916]
</p><p>65 (8)  According to the definition of magnitude of gradients mp = ? [sent-192, score-0.213]
</p><p>66 We generate LR exemplar images from the matched dataset using Eq. [sent-200, score-0.382]
</p><p>67 3 and utilize the PatchMatch algorithm [2] to reduce the computational load of retrieving the most similar LR exemplar patches. [sent-201, score-0.273]
</p><p>68 (h) An image generated by the restored gradients to show the effectiveness of the restored edge sharpness. [sent-214, score-0.238]
</p><p>69 We extract the gradients of the back-projected HR image as the gradients of smooth regions, denoted by Ub (Figure 1(g)). [sent-218, score-0.242]
</p><p>70 Integrating Gradient Maps In order to generate the required gradient map set U for producing the output HR image, we generate two weight maps wc and we. [sent-221, score-0.357]
</p><p>71 We set map wc as the summation of all HR mask maps Mh (Figure 1(b)), and set we (p) = min{1, α m¯e}, where m¯ e is the gradient magnitude in Eq. [sent-222, score-0.302]
</p><p>72 It is clear that the use of each gradient map facilitates generating better results for different facial structures. [sent-232, score-0.283]
</p><p>73 (a)(b) The generated gradient maps Uc ensure consistency of high-frequency details at components. [sent-236, score-0.242]
</p><p>74 (c)(d) The generated gradient maps Ue ensure clear and sharp edges. [sent-237, score-0.237]
</p><p>75 The pose labels and landmarks of each image are given in the dataset, and we manually generate the glasses labels for training images. [sent-240, score-0.324]
</p><p>76 One set with 2,184 320 240 images at upright frontal pose of 289 winidthiv 2id,u1a84ls 3is2 0us ×ed 2 as t ihme training dparitagshett f froorn tfaalce p ohsaellu ofc i2n8a9tion experiments (Figure 6, Figure 7 and Figure 8). [sent-241, score-0.197]
</p><p>77 We generate the input LR images by downsampling the original HR test images through Eq. [sent-244, score-0.192]
</p><p>78 The ground truth HR images in the test set are used for compar-  ×  isons with the generated hallucination results. [sent-247, score-0.287]
</p><p>79 For color images, we apply the proposed algorithm on grayscaling channel and the color channels are upsampled by bicubic interpolation to make fair comparisons with existing methods [9, 16, 10]. [sent-251, score-0.277]
</p><p>80 To label facial landmarks, we use the algorithm of [17] which produces the landmarks as the active appearance model [4] with 68 points as shown in Figure 1(b). [sent-254, score-0.236]
</p><p>81 As the landmarks of eyebrows and nose do not form a close polygons, we mask eyebrows as the rectangles where the landmarks are the center vertical segments. [sent-256, score-0.305]
</p><p>82 We implement several state-of-the-art face hallucination 1 1 1 1 1 10 0 03 1 1  algorithms [9, 16, 10] for comparisons. [sent-258, score-0.348]
</p><p>83 Figures 6, 7 and 8 show hallucinated faces of frontal pose where the input images are enlarged by nearest neighbor interpolation for illustration purpose. [sent-260, score-0.386]
</p><p>84 , one eye of Figure 6(e) contains glasses and the other one does not) with significant ghosty effects. [sent-268, score-0.218]
</p><p>85 While the algorithms based on sparse coding [16] and position-patch [10] generate high-frequency textures, the results do not contain fine facial details such as contours and hair (See Figure 6). [sent-269, score-0.432]
</p><p>86 The proposed algorithm reconstruct fine details of facial components such as the spots and moles in Figure 7(f) and individual tooth in Figure 6(f). [sent-271, score-0.306]
</p><p>87 In addition, the proposed algorithm also generates bet-  ter details due to the glasses label and the component-level alignment. [sent-279, score-0.207]
</p><p>88 We also compare the hallucination results on faces at different pose. [sent-281, score-0.319]
</p><p>89 As shown in Figure 9 and Figure 10, the subspace-based methods [10] and [9] do not perform well as both the subspace learning and the patch reconstruction at fix positions require precise face alignment. [sent-283, score-0.21]
</p><p>90 However, it is more difficult to align face images at different pose such that PCA subspace can be well constructed for hallucination. [sent-284, score-0.228]
</p><p>91 Conclusion A novel approach that exploits image structures for face hallucination is proposed in this paper. [sent-290, score-0.348]
</p><p>92 The image struc-  tures of a face are grouped into three categories including facial components, edges, and smooth regions. [sent-291, score-0.346]
</p><p>93 Their gradient maps are generated and integrated to produce HR results with the best visual quality. [sent-292, score-0.204]
</p><p>94 Experimental results show that the proposed algorithm generates hallucinated face images with fine and consistent details over state-of-the-art algorithms. [sent-293, score-0.314]
</p><p>95 Qualitative comparison for 4 times upsampled upright frontal faces (results best viewed  on a  high-resolution display). [sent-425, score-0.407]
</p><p>96 Qualitative comparison for 4 times upsampled upright frontal faces (results best viewed on a high-resolution display). [sent-445, score-0.407]
</p><p>97 Qualitative comparison for 4 times upsampled upright frontal faces (results best viewed on a high-resolution display). [sent-465, score-0.407]
</p><p>98 Qualitative comparison for 4 times upsampled non-frontal faces (results best viewed on a high-resolution display). [sent-485, score-0.282]
</p><p>99 Qualitative comparison for 4 times upsampled non-frontal faces (results best viewed on a high-resolution display). [sent-505, score-0.282]
</p><p>100 Qualitative comparison for 4 times upsampled upright frontal faces (results best viewed on a high-resolution display). [sent-525, score-0.407]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hr', 0.568), ('lr', 0.393), ('exemplar', 0.248), ('hallucination', 0.229), ('facial', 0.175), ('upsampled', 0.158), ('diivine', 0.141), ('glasses', 0.139), ('ssim', 0.122), ('face', 0.119), ('landmark', 0.096), ('gradients', 0.095), ('psnr', 0.095), ('sharpness', 0.092), ('faces', 0.09), ('maps', 0.086), ('gradient', 0.084), ('ghosty', 0.079), ('hallucinated', 0.079), ('contours', 0.079), ('generate', 0.076), ('bicubic', 0.07), ('frontal', 0.07), ('components', 0.069), ('downsampling', 0.068), ('display', 0.066), ('ud', 0.066), ('il', 0.066), ('mp', 0.063), ('landmarks', 0.061), ('edges', 0.061), ('restore', 0.061), ('hj', 0.056), ('magnitude', 0.055), ('upright', 0.055), ('mouth', 0.055), ('id', 0.054), ('patch', 0.054), ('patches', 0.052), ('smooth', 0.052), ('component', 0.05), ('infinite', 0.05), ('upsampling', 0.05), ('eyebrows', 0.049), ('hallucinating', 0.049), ('interpolation', 0.049), ('ue', 0.049), ('pose', 0.048), ('aligned', 0.047), ('mh', 0.046), ('transferred', 0.044), ('nose', 0.043), ('artifacts', 0.042), ('lj', 0.042), ('mask', 0.042), ('mc', 0.04), ('hair', 0.04), ('blocky', 0.04), ('fk', 0.04), ('details', 0.038), ('uc', 0.038), ('restored', 0.038), ('ub', 0.038), ('ib', 0.038), ('ih', 0.037), ('subspace', 0.037), ('qualitative', 0.037), ('superresolution', 0.036), ('eyes', 0.036), ('argimin', 0.035), ('wc', 0.035), ('vj', 0.035), ('generated', 0.034), ('matched', 0.034), ('viewed', 0.034), ('sharp', 0.033), ('edge', 0.033), ('suppose', 0.031), ('preserve', 0.031), ('generates', 0.03), ('directional', 0.028), ('polygons', 0.028), ('upsample', 0.028), ('preserved', 0.027), ('expressions', 0.026), ('enlarged', 0.026), ('patchmatch', 0.026), ('structural', 0.025), ('tip', 0.025), ('load', 0.025), ('qk', 0.025), ('highresolution', 0.025), ('tk', 0.024), ('fine', 0.024), ('masks', 0.024), ('generating', 0.024), ('sk', 0.024), ('images', 0.024), ('similarity', 0.023), ('pleasing', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="415-tfidf-1" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>2 0.25112838 <a title="415-tfidf-2" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>3 0.24165717 <a title="415-tfidf-3" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>4 0.18626465 <a title="415-tfidf-4" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>5 0.14078362 <a title="415-tfidf-5" href="./cvpr-2013-Learning_Structured_Low-Rank_Representations_for_Image_Classification.html">257 cvpr-2013-Learning Structured Low-Rank Representations for Image Classification</a></p>
<p>Author: Yangmuzi Zhang, Zhuolin Jiang, Larry S. Davis</p><p>Abstract: An approach to learn a structured low-rank representation for image classification is presented. We use a supervised learning method to construct a discriminative and reconstructive dictionary. By introducing an ideal regularization term, we perform low-rank matrix recovery for contaminated training data from all categories simultaneously without losing structural information. A discriminative low-rank representation for images with respect to the constructed dictionary is obtained. With semantic structure information and strong identification capability, this representation is good for classification tasks even using a simple linear multi-classifier. Experimental results demonstrate the effectiveness of our approach.</p><p>6 0.12905176 <a title="415-tfidf-6" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>7 0.11721221 <a title="415-tfidf-7" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>8 0.11225116 <a title="415-tfidf-8" href="./cvpr-2013-Constrained_Clustering_and_Its_Application_to_Face_Clustering_in_Videos.html">92 cvpr-2013-Constrained Clustering and Its Application to Face Clustering in Videos</a></p>
<p>9 0.10788668 <a title="415-tfidf-9" href="./cvpr-2013-Fast_Image_Super-Resolution_Based_on_In-Place_Example_Regression.html">166 cvpr-2013-Fast Image Super-Resolution Based on In-Place Example Regression</a></p>
<p>10 0.10671437 <a title="415-tfidf-10" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>11 0.10113302 <a title="415-tfidf-11" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>12 0.099975795 <a title="415-tfidf-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.093764715 <a title="415-tfidf-13" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>14 0.091795884 <a title="415-tfidf-14" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>15 0.090423696 <a title="415-tfidf-15" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>16 0.086992532 <a title="415-tfidf-16" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>17 0.086542301 <a title="415-tfidf-17" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>18 0.084843241 <a title="415-tfidf-18" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>19 0.081238829 <a title="415-tfidf-19" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>20 0.080540836 <a title="415-tfidf-20" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, -0.028), (2, -0.042), (3, 0.051), (4, 0.049), (5, 0.004), (6, -0.027), (7, -0.05), (8, 0.198), (9, -0.169), (10, 0.098), (11, -0.007), (12, 0.077), (13, 0.092), (14, 0.051), (15, 0.021), (16, 0.027), (17, -0.012), (18, 0.049), (19, 0.065), (20, -0.001), (21, 0.041), (22, 0.034), (23, -0.036), (24, 0.051), (25, 0.044), (26, 0.04), (27, -0.084), (28, 0.035), (29, 0.074), (30, -0.043), (31, -0.068), (32, 0.0), (33, 0.018), (34, -0.109), (35, -0.046), (36, 0.049), (37, -0.106), (38, -0.07), (39, 0.005), (40, 0.001), (41, -0.034), (42, -0.026), (43, 0.043), (44, 0.055), (45, -0.018), (46, -0.038), (47, -0.075), (48, -0.048), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94516462 <a title="415-lsi-1" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>2 0.78372908 <a title="415-lsi-2" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>3 0.74500114 <a title="415-lsi-3" href="./cvpr-2013-Facial_Feature_Tracking_Under_Varying_Facial_Expressions_and_Face_Poses_Based_on_Restricted_Boltzmann_Machines.html">161 cvpr-2013-Facial Feature Tracking Under Varying Facial Expressions and Face Poses Based on Restricted Boltzmann Machines</a></p>
<p>Author: Yue Wu, Zuoguan Wang, Qiang Ji</p><p>Abstract: Facial feature tracking is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, sincefaces may have varyingfacial expressions, poses or occlusions. In this paper, we address this problem by proposing a face shape prior model that is constructed based on the Restricted Boltzmann Machines (RBM) and their variants. Specifically, we first construct a model based on Deep Belief Networks to capture the face shape variations due to varying facial expressions for near-frontal view. To handle pose variations, the frontal face shape prior model is incorporated into a 3-way RBM model that could capture the relationship between frontal face shapes and non-frontal face shapes. Finally, we introduce methods to systematically combine the face shape prior models with image measurements of facial feature points. Experiments on benchmark databases show that with the proposed method, facial feature points can be tracked robustly and accurately even if faces have significant facial expressions and poses.</p><p>4 0.70087838 <a title="415-lsi-4" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>Author: Xiaohui Shen, Zhe Lin, Jonathan Brandt, Ying Wu</p><p>Abstract: Detecting faces in uncontrolled environments continues to be a challenge to traditional face detection methods[24] due to the large variation in facial appearances, as well as occlusion and clutter. In order to overcome these challenges, we present a novel and robust exemplarbased face detector that integrates image retrieval and discriminative learning. A large database of faces with bounding rectangles and facial landmark locations is collected, and simple discriminative classifiers are learned from each of them. A voting-based method is then proposed to let these classifiers cast votes on the test image through an efficient image retrieval technique. As a result, faces can be very efficiently detected by selecting the modes from the voting maps, without resorting to exhaustive sliding window-style scanning. Moreover, due to the exemplar-based framework, our approach can detect faces under challenging conditions without explicitly modeling their variations. Evaluation on two public benchmark datasets shows that our new face detection approach is accurate and efficient, and achieves the state-of-the-art performance. We further propose to use image retrieval for face validation (in order to remove false positives) and for face alignment/landmark localization. The same methodology can also be easily generalized to other facerelated tasks, such as attribute recognition, as well as general object detection.</p><p>5 0.68389589 <a title="415-lsi-5" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>Author: Xuehan Xiong, Fernando De_la_Torre</p><p>Abstract: Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization ofa general smoothfunction. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, thispaperproposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-ofthe-art performance in the problem of facial feature detec- tion. The code is available at www. .human sen sin g. . cs . cmu . edu/in t ra fa ce.</p><p>6 0.676557 <a title="415-lsi-6" href="./cvpr-2013-Robust_Discriminative_Response_Map_Fitting_with_Constrained_Local_Models.html">359 cvpr-2013-Robust Discriminative Response Map Fitting with Constrained Local Models</a></p>
<p>7 0.60589522 <a title="415-lsi-7" href="./cvpr-2013-Capturing_Complex_Spatio-temporal_Relations_among_Facial_Muscles_for_Facial_Expression_Recognition.html">77 cvpr-2013-Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition</a></p>
<p>8 0.6038515 <a title="415-lsi-8" href="./cvpr-2013-Single-Sample_Face_Recognition_with_Image_Corruption_and_Misalignment_via_Sparse_Illumination_Transfer.html">399 cvpr-2013-Single-Sample Face Recognition with Image Corruption and Misalignment via Sparse Illumination Transfer</a></p>
<p>9 0.59364647 <a title="415-lsi-9" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>10 0.58167523 <a title="415-lsi-10" href="./cvpr-2013-Towards_Pose_Robust_Face_Recognition.html">438 cvpr-2013-Towards Pose Robust Face Recognition</a></p>
<p>11 0.57006437 <a title="415-lsi-11" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>12 0.56367499 <a title="415-lsi-12" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>13 0.53674835 <a title="415-lsi-13" href="./cvpr-2013-Fusing_Robust_Face_Region_Descriptors_via_Multiple_Metric_Learning_for_Face_Recognition_in_the_Wild.html">182 cvpr-2013-Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild</a></p>
<p>14 0.53093064 <a title="415-lsi-14" href="./cvpr-2013-Selective_Transfer_Machine_for_Personalized_Facial_Action_Unit_Detection.html">385 cvpr-2013-Selective Transfer Machine for Personalized Facial Action Unit Detection</a></p>
<p>15 0.53064746 <a title="415-lsi-15" href="./cvpr-2013-Correlation_Filters_for_Object_Alignment.html">96 cvpr-2013-Correlation Filters for Object Alignment</a></p>
<p>16 0.52367032 <a title="415-lsi-16" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>17 0.50454974 <a title="415-lsi-17" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>18 0.46846747 <a title="415-lsi-18" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>19 0.46712023 <a title="415-lsi-19" href="./cvpr-2013-Face_Recognition_in_Movie_Trailers_via_Mean_Sequence_Sparse_Representation-Based_Classification.html">160 cvpr-2013-Face Recognition in Movie Trailers via Mean Sequence Sparse Representation-Based Classification</a></p>
<p>20 0.45843184 <a title="415-lsi-20" href="./cvpr-2013-In_Defense_of_Sparsity_Based_Face_Recognition.html">220 cvpr-2013-In Defense of Sparsity Based Face Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.108), (16, 0.03), (26, 0.106), (28, 0.019), (33, 0.24), (39, 0.021), (67, 0.073), (69, 0.033), (80, 0.012), (87, 0.082), (91, 0.2)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84381145 <a title="415-lda-1" href="./cvpr-2013-Structured_Face_Hallucination.html">415 cvpr-2013-Structured Face Hallucination</a></p>
<p>Author: Chih-Yuan Yang, Sifei Liu, Ming-Hsuan Yang</p><p>Abstract: The goal of face hallucination is to generate highresolution images with fidelity from low-resolution ones. In contrast to existing methods based on patch similarity or holistic constraints in the image space, we propose to exploit local image structures for face hallucination. Each face image is represented in terms of facial components, contours and smooth regions. The image structure is maintained via matching gradients in the reconstructed highresolution output. For facial components, we align input images to generate accurate exemplars and transfer the high-frequency details for preserving structural consistency. For contours, we learn statistical priors to generate salient structures in the high-resolution images. A patch matching method is utilized on the smooth regions where the image gradients are preserved. Experimental results demonstrate that the proposed algorithm generates hallucinated face images with favorable quality and adaptability.</p><p>2 0.83930111 <a title="415-lda-2" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>3 0.83836162 <a title="415-lda-3" href="./cvpr-2013-Learning_a_Manifold_as_an_Atlas.html">259 cvpr-2013-Learning a Manifold as an Atlas</a></p>
<p>Author: Nikolaos Pitelis, Chris Russell, Lourdes Agapito</p><p>Abstract: In this work, we return to the underlying mathematical definition of a manifold and directly characterise learning a manifold as finding an atlas, or a set of overlapping charts, that accurately describe local structure. We formulate the problem of learning the manifold as an optimisation that simultaneously refines the continuous parameters defining the charts, and the discrete assignment of points to charts. In contrast to existing methods, this direct formulation of a manifold does not require “unwrapping ” the manifold into a lower dimensional space and allows us to learn closed manifolds of interest to vision, such as those corresponding to gait cycles or camera pose. We report state-ofthe-art results for manifold based nearest neighbour classification on vision datasets, and show how the same techniques can be applied to the 3D reconstruction of human motion from a single image.</p><p>4 0.82149565 <a title="415-lda-4" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>Author: Bojan Pepikj, Michael Stark, Peter Gehler, Bernt Schiele</p><p>Abstract: Despite the success of recent object class recognition systems, the long-standing problem of partial occlusion remains a major challenge, and a principled solution is yet to be found. In this paper we leave the beaten path of methods that treat occlusion as just another source of noise instead, we include the occluder itself into the modelling, by mining distinctive, reoccurring occlusion patterns from annotated training data. These patterns are then used as training data for dedicated detectors of varying sophistication. In particular, we evaluate and compare models that range from standard object class detectors to hierarchical, part-based representations of occluder/occludee pairs. In an extensive evaluation we derive insights that can aid further developments in tackling the occlusion challenge. –</p><p>5 0.81741643 <a title="415-lda-5" href="./cvpr-2013-Exemplar-Based_Face_Parsing.html">152 cvpr-2013-Exemplar-Based Face Parsing</a></p>
<p>Author: Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe Lin, Jianchao Yang</p><p>Abstract: In this work, we propose an exemplar-based face image segmentation algorithm. We take inspiration from previous works on image parsing for general scenes. Our approach assumes a database of exemplar face images, each of which is associated with a hand-labeled segmentation map. Given a test image, our algorithm first selects a subset of exemplar images from the database, Our algorithm then computes a nonrigid warp for each exemplar image to align it with the test image. Finally, we propagate labels from the exemplar images to the test image in a pixel-wise manner, using trained weights to modulate and combine label maps from different exemplars. We evaluate our method on two challenging datasets and compare with two face parsing algorithms and a general scene parsing algorithm. We also compare our segmentation results with contour-based face alignment results; that is, we first run the alignment algorithms to extract contour points and then derive segments from the contours. Our algorithm compares favorably with all previous works on all datasets evaluated.</p><p>6 0.817092 <a title="415-lda-6" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>7 0.81177902 <a title="415-lda-7" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>8 0.80705011 <a title="415-lda-8" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>9 0.80552471 <a title="415-lda-9" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>10 0.80393153 <a title="415-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>11 0.80318362 <a title="415-lda-11" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<p>12 0.8020705 <a title="415-lda-12" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>13 0.80109853 <a title="415-lda-13" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>14 0.8005836 <a title="415-lda-14" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>15 0.79970205 <a title="415-lda-15" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>16 0.7987383 <a title="415-lda-16" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>17 0.79839253 <a title="415-lda-17" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>18 0.79743457 <a title="415-lda-18" href="./cvpr-2013-Accurate_Localization_of_3D_Objects_from_RGB-D_Data_Using_Segmentation_Hypotheses.html">30 cvpr-2013-Accurate Localization of 3D Objects from RGB-D Data Using Segmentation Hypotheses</a></p>
<p>19 0.79721975 <a title="415-lda-19" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>20 0.79706752 <a title="415-lda-20" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
