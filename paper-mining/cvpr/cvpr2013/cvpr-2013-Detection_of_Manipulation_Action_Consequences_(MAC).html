<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-123" href="#">cvpr2013-123</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</h1>
<br/><p>Source: <a title="cvpr-2013-123-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Detection_of_Manipulation_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos</p><p>Abstract: The problem of action recognition and human activity has been an active research area in Computer Vision and Robotics. While full-body motions can be characterized by movement and change of posture, no characterization, that holds invariance, has yet been proposed for the description of manipulation actions. We propose that a fundamental concept in understanding such actions, are the consequences of actions. There is a small set of fundamental primitive action consequences that provides a systematic high-level classification of manipulation actions. In this paper a technique is developed to recognize these action consequences. At the heart of the technique lies a novel active tracking and segmentation method that monitors the changes in appearance and topological structure of the manipulated object. These are then used in a visual semantic graph (VSG) based procedure applied to the time sequence of the monitored object to recognize the action consequence. We provide a new dataset, called Manipulation Action Consequences (MAC 1.0), which can serve as testbed for other studies on this topic. Several ex- periments on this dataset demonstrates that our method can robustly track objects and detect their deformations and division during the manipulation. Quantitative tests prove the effectiveness and efficiency of the method.</p><p>Reference: <a title="cvpr-2013-123-reference" href="../cvpr2013_reference/cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  yianni s @ c s umd  Computer Vision Lab, University of Maryland, College Park, MD 20742, USA  Abstract The problem of action recognition and human activity has been an active research area in Computer Vision and Robotics. [sent-7, score-0.626]
</p><p>2 While full-body motions can be characterized by movement and change of posture, no characterization, that holds invariance, has yet been proposed for the description of manipulation actions. [sent-8, score-0.44]
</p><p>3 We propose that a fundamental concept in understanding such actions, are the consequences of actions. [sent-9, score-0.523]
</p><p>4 There is a small set of fundamental primitive action consequences that provides a systematic high-level classification of manipulation actions. [sent-10, score-1.142]
</p><p>5 In this paper a technique is developed to recognize these action consequences. [sent-11, score-0.462]
</p><p>6 At the heart of the technique lies a novel active tracking and segmentation method that monitors the changes in appearance and topological structure of the manipulated object. [sent-12, score-0.533]
</p><p>7 These are then used in a visual semantic graph (VSG) based procedure applied to the time sequence of the monitored object to recognize the action consequence. [sent-13, score-0.575]
</p><p>8 0), which can serve as testbed for other studies on this topic. [sent-15, score-0.06]
</p><p>9 Several ex-  periments on this dataset demonstrates that our method can robustly track objects and detect their deformations and division during the manipulation. [sent-16, score-0.08]
</p><p>10 Introduction Visual recognition is the process through which intelligent agents associate a visual observation to a concept from their memory. [sent-19, score-0.158]
</p><p>11 In most cases, the concept either corresponds to a term in natural language, or an explicit definition in natural language. [sent-20, score-0.049]
</p><p>12 Most research in Computer Vision has focused on two concepts: objects and actions; humans, faces and scenes can be regarded as special cases of objects. [sent-21, score-0.042]
</p><p>13 Object and action recognition are indeed crucial since they are the fundamental building blocks for an intelligent agent to semantically understand its observations. [sent-22, score-0.672]
</p><p>14 When it comes to understanding actions ofmanipulation, the movement of the body (especially the hands) is not a very good characteristic feature. [sent-23, score-0.422]
</p><p>15 There is great variability in the way humans carry out such actions. [sent-24, score-0.07]
</p><p>16 It has been realized that such actions are better described by involving a number of quantities. [sent-25, score-0.254]
</p><p>17 Besides the motion trajectories, the objects involved, the hand pose, and the spatial relations between the body and the objects under influence, provide information about the action. [sent-26, score-0.084]
</p><p>18 In this work we want to bring attention to another concept, the action consequence. [sent-27, score-0.489]
</p><p>19 It describes the transformation of the object during the manipulation. [sent-28, score-0.035]
</p><p>20 For  example during a CUT or a SPLIT action an object is divided into segments, during a GLUE or a MERGE action two objects are combined into one, etc. [sent-29, score-0.869]
</p><p>21 The recognition and understanding of human manipulation actions recently has attracted the attention of Computer Vision and Robotics researchers because of their critical role in human behavior analysis. [sent-30, score-0.749]
</p><p>22 Moreover, they naturally relate to both, the movement involved in the action and the objects. [sent-31, score-0.566]
</p><p>23 However, so far researchers have not considered that the most crucial cue in describing manipulation actions is actually not the movement nor the specific object under influence, but the object centric action consequence. [sent-32, score-1.314]
</p><p>24 We can come up with examples, where two actions involve the same tool and same object under influence, and the motions of the hands are similar, for example in “cutting a piece of meat” vs. [sent-33, score-0.355]
</p><p>25 In such cases, the action consequence is the key in differentiating the actions. [sent-36, score-0.491]
</p><p>26 Thus, to fully understand manipulation actions, the intelligent system should be able to determine the object centric consequences. [sent-37, score-0.565]
</p><p>27 Few researchers have addressed the problem of action consequences due to the difficulties involved. [sent-38, score-0.886]
</p><p>28 The main challenge comes from the monitoring process, which calls for the ability to continuously check the topological and appearance changes of the object-under-manipulation. [sent-39, score-0.307]
</p><p>29 In this paper, for the first time, a system 222555666311  is implemented to conquer these difficulties and eventually achieve robust action consequence detection. [sent-41, score-0.609]
</p><p>30 Why Consequences and Fundamental Types Recognizing human actions has been an active research area in Computer Vision [10]. [sent-43, score-0.403]
</p><p>31 Several excellent surveys on the topic of visual recognition are available ([21], [29]). [sent-44, score-0.039]
</p><p>32 Most work on visual action analysis has been devoted to the study of movement and change of posture, such as walking, running etc. [sent-45, score-0.565]
</p><p>33 The dominant approaches to the recognition of single actions compute as descriptors statistics of spatio-temporal interest points ([16], [3 1]) and flow in video volumes, or represent short actions by stacks of silhouettes ([4], [34]). [sent-46, score-0.508]
</p><p>34 Approaches to more complex, longer actions employ parametric approaches, such as Hidden Markov Models [13], Linear Dynamical Systems [26] or Non-linear Dynamical Systems [7], which are defined on extracted features. [sent-47, score-0.254]
</p><p>35 There are a few recent studies on human manipulation actions ([30], [14], [27]), but they do not consider action consequences for the interpretation of manipulation actions. [sent-48, score-1.721]
</p><p>36 Works like [33] emphasize the role of object perception in action or pose recognition, but they focus on object labels, not object-centric consequences. [sent-49, score-0.466]
</p><p>37 How do humans understand, recognize, and even replicate manipulation actions? [sent-50, score-0.343]
</p><p>38 ) have pointed out the importance of manipulation action consequences for both understanding human cognition and intelligent system research. [sent-52, score-1.245]
</p><p>39 When we perform an action, we always have a goal in mind, and the goal affects the action. [sent-54, score-0.082]
</p><p>40 Similarly, when we try to recognize an action, we also keep a goal in mind. [sent-55, score-0.107]
</p><p>41 The close relation between the movement during the action and goal is reflected also in language. [sent-56, score-0.567]
</p><p>42 For example, the word “CUT” denotes both the action in which hands move up and down or in and out with sharp bladed tools, and the consequence of the action, namely that the object is separated. [sent-57, score-0.592]
</p><p>43 Very often, we can recognize an action purely by the goal satisfaction, and even neglect the motion or the tools used. [sent-58, score-0.569]
</p><p>44 For example, we may observe a human carry out movement with a knife, that is ”up and down”, but if the object remains as one whole, we won’t draw the conclusion that a “CUT” action has been performed. [sent-59, score-0.598]
</p><p>45 Only when the goal of the recognition process, here “DIVIDE”, is detected, the goal satisfaction is reached and a “CUT” action is confirmed. [sent-60, score-0.543]
</p><p>46 An intelligent system should have the ability to detect the consequences of manipulation actions, in order to check the goal of actions. [sent-61, score-0.852]
</p><p>47 In addition, experiments conducted in neuronscience [25] show that a monkey’s mirror neuron system fires when a hand/object interaction is observed, and it will not fire when a similar movement is observed without hand/object  interaction. [sent-62, score-0.278]
</p><p>48 Recent experiments [9] further showed that the mirror neuron regions responding to the sight of actions responded more during the observation of goal-directed actions than similar movements not directed at goals. [sent-63, score-0.653]
</p><p>49 These evidences support the idea of goal matching, as well as the crucial role of action consequence in the understanding of manipulation actions. [sent-64, score-0.915]
</p><p>50 Taking an object-centric point of view, manipulation actions can be classified into six categories according how the object is transformed during the manipulation, or in other words what consequence the action has on the object. [sent-65, score-1.127]
</p><p>51 These categories are: DIVIDE, ASSEMBLE, CREATE, CONSUME, TRANSFER, and DEFORM. [sent-66, score-0.037]
</p><p>52 DToE FdOesRcrMib:e a tnhe osbej eacctti hoans categories we en ceehad a feo. [sent-68, score-0.037]
</p><p>53 We use the visual semantic graph (VSG) inspired from the work of Aksoy et. [sent-70, score-0.078]
</p><p>54 This formalism takes as input computed object segments, their spatial relationship, and  temporal relationship over consecutive frames. [sent-72, score-0.035]
</p><p>55 To provide the symbols for the VSG, an active monitoring process (discussed in sec. [sent-73, score-0.176]
</p><p>56 4) is required for the purpose of (1) tracking the object to obtain temporal correspondence, and (2) segmenting the object to obtain its topological structure and appearance model. [sent-74, score-0.313]
</p><p>57 This active monitoring (consisting of segmentation and tracking) is related to studies on active segmentation [20], and stochastic tracking ([11] etc. [sent-75, score-0.61]
</p><p>58 Visual Semantic Graph (VSG) To define object-centric action consequences, a graph representation is used. [sent-78, score-0.396]
</p><p>59 The vertex set |V | represents the set of semantically meaningful segments, t|hVe | edge sseetn t|Es t|h represents mthaen spatial mreleaatnioinngsf ubel tsweegemne any tohfe th edeg tew soe segments. [sent-80, score-0.072]
</p><p>60 nTwtso t segments are oconnsn beecttwedee wn ahenyn they share parts of their borders, or when one of the segments is contained in the other. [sent-81, score-0.122]
</p><p>61 IVn aadredition, every node v ∈ V is associated with a set of propertdiietsio Pn(, evv)e, trhya nt oddeesc vri ∈bes V t ihse aaststoricbiuatteesd o wfi tthhe a segment. [sent-83, score-0.035]
</p><p>62 We need to compute the changes of the object over time. [sent-86, score-0.106]
</p><p>63 At any time instance t, we consider two consecutive VSGs, the VSG at time t − 1, denoted as Ga(Va, Ea, Pa) aVnSdG Gtshe, tVheSG V SatG Gti amte t itm, dee tno −te 1d, as Gnozt (eVdz a , sE Gz , Pz). [sent-88, score-0.037]
</p><p>64 We then define the following four consequences, where → is used to ddeefninoete thhee f temporal correspondence e bse,tw weheenre tw →o i sv eursteicde tso, ? [sent-89, score-0.037]
</p><p>65 d purely on tthioen b(a4-) sis of topological changes, there are no such changes for TRANSFER and DEFORM. [sent-94, score-0.197]
</p><p>66 Therefore, we have to define them through changes in property. [sent-95, score-0.071]
</p><p>67 In the following definitions, PL represents properties of location, and PS represents properties of appearance (shape, color, etc. [sent-96, score-0.04]
</p><p>68 • TRANSFER:{∃v1 ∈ Va; v2 ∈ Vz|PaL(v1) PzL(v2)} TCRonAdNitSioFnE (R5:){ • DEFORM: {∃v1 ∈ Va; v2 ∈ Vz|PaS(v1) PzS(v2)} DCEonFdOitRioMn: ( {6∃)  ∈  ∈∈  = =  Figure 1: Graphical illustration of the changes for Condition (1-6). [sent-98, score-0.071]
</p><p>69 A new active segmentation and tracking method is introduced to 1) find correspondences (→) between Va iasn din tVroz;d u2c) mdo tonit 1o)r ilnodcat cioornr property ePsL ( →and) appearance property PS in the VSG. [sent-103, score-0.369]
</p><p>70 The procedure for computing action consequences, first decides on whether there is a topological change between Ga and Gz. [sent-104, score-0.489]
</p><p>71 If yes, the system checks whether Condition (1) to Condition (4) are fulfilled and returns the corresponding consequence. [sent-105, score-0.096]
</p><p>72 If no, the system then checks whether Condition (5) or Condition (6) is fulfilled. [sent-106, score-0.096]
</p><p>73 If both of them are not met, no consequence is detected. [sent-107, score-0.095]
</p><p>74 Active Segmentation and Tracking Previously, researchers have treated segmentation and tracking as two different problems. [sent-109, score-0.242]
</p><p>75 Here we propose a new method combining the two tasks to obtain the information necessary to monitor the objects under influence. [sent-110, score-0.042]
</p><p>76 Our methods combines stochastic tracking [11] with a fixation based active segmentation [20]. [sent-111, score-0.464]
</p><p>77 The tracking module provides a number of tracked points. [sent-112, score-0.158]
</p><p>78 The locations of these points are  used to define an area of interest and a fixation point for the segmentation, and the color in their immediate surroundings are used in the data term of the segmentation module. [sent-113, score-0.343]
</p><p>79 The segmentation module segments the object, and based on the segmentation, updates the appearance model for the tracker. [sent-114, score-0.227]
</p><p>80 Figure 2: Flow chart of the proposed active segmentation and tracking method for object monitoring. [sent-122, score-0.331]
</p><p>81 The proposed method meets two challenging requirements, necessary to detect action consequences: 1) the system is able to track and segment objects when the shape  or color (appearance) changes; 2) the system is also able to track and segment objects when they are divided into pieces. [sent-123, score-0.692]
</p><p>82 1 show that our method can handle these requirements, while systems implementing independently tracking and segmentation cannot. [sent-126, score-0.188]
</p><p>83 The Attention Field The idea underlying our approach is, that first a process of visual attention selects an area of interest. [sent-129, score-0.173]
</p><p>84 Segmentation then is considered the process that separates the area selected by visual attention from background by finding closed contours that best separate the regions. [sent-130, score-0.173]
</p><p>85 The minimization uses a color model for the data term and edges in the regularization term. [sent-131, score-0.056]
</p><p>86 To achieve a minimization that is very robust to the length of the boundary, edges are weighted with their distance from the fixation center. [sent-132, score-0.168]
</p><p>87 Visual attention, the process of driving an agent’s attention to a certain area, is based on both bottom-up processes defined on low level visual features, and top-down processes influenced by the agent’s previous experience [28]. [sent-133, score-0.132]
</p><p>88 [32], instead of using a single fixation point in the active segmentation [20], here we use a weighted sample set S = {(s(n) , π(n) ) |n = 1. [sent-135, score-0.354]
</p><p>89 N} 222555666533  to represent the attention field around the fixation point (N = 500 in practice). [sent-138, score-0.261]
</p><p>90 Each sample consists of an ele-  ×  mdisecnrtet se fr woemig thhte π se wth oefre tr? [sent-139, score-0.035]
</p><p>91 ance model can be used to represent the local visual inf? [sent-142, score-0.039]
</p><p>92 We choose to use a color histogram with a dynamic sampling area defined by an ellipse. [sent-144, score-0.097]
</p><p>93 To compute the color distribution, every point is represented by an ellipse, s = {x, y, x˙ , y˙ , Hx , Hy, H˙x , H˙y, } where x and y denote the lo{caxt,ioyn,, x˙ x,˙ y˙ ,anHd y˙ the motion,, }H wx,h Hy xth aen length nooft eth teh e h laolfaxes, and H˙x, H˙y the changes in the axes. [sent-145, score-0.162]
</p><p>94 Color Distribution Model To make the color model invariant to various textures or patterns, a color distribution model is used. [sent-148, score-0.112]
</p><p>95 A function h(xi) is defined to create a color histogram, which assigns one of the m-bins to a giving color at location xi. [sent-149, score-0.153]
</p><p>96 To make the algorithm less sensitive to lighting conditions, the HSV color space is used with less sensitivity in the V channel (8 8 4 bins). [sent-150, score-0.056]
</p><p>97 The color distribution for each fixation point 8s( ×n) 4is computed as: ? [sent-151, score-0.224]
</p><p>98 the inkt(u||iyti−oxn ||t)hat not all pixels in the sampling region are equally important for describing the color model. [sent-163, score-0.056]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('action', 0.396), ('consequences', 0.391), ('manipulation', 0.31), ('actions', 0.254), ('vsg', 0.253), ('fixation', 0.168), ('va', 0.155), ('movement', 0.13), ('vz', 0.12), ('tracking', 0.11), ('active', 0.108), ('assemble', 0.098), ('consequence', 0.095), ('topological', 0.093), ('attention', 0.093), ('endition', 0.084), ('meat', 0.084), ('agent', 0.081), ('umd', 0.081), ('segmentation', 0.078), ('changes', 0.071), ('intelligent', 0.07), ('monitoring', 0.068), ('recognize', 0.066), ('hands', 0.066), ('centric', 0.065), ('mac', 0.065), ('consume', 0.065), ('satisfaction', 0.065), ('condition', 0.064), ('neuron', 0.062), ('hy', 0.062), ('segments', 0.061), ('posture', 0.06), ('studies', 0.06), ('color', 0.056), ('checks', 0.056), ('researchers', 0.054), ('deform', 0.053), ('cut', 0.051), ('divide', 0.05), ('concept', 0.049), ('module', 0.048), ('mirror', 0.046), ('fundamental', 0.045), ('difficulties', 0.045), ('understand', 0.045), ('ga', 0.045), ('objects', 0.042), ('goal', 0.041), ('dynamical', 0.041), ('create', 0.041), ('area', 0.041), ('system', 0.04), ('involved', 0.04), ('appearance', 0.04), ('semantic', 0.039), ('visual', 0.039), ('understanding', 0.038), ('transfer', 0.038), ('track', 0.038), ('pas', 0.037), ('aksoy', 0.037), ('amte', 0.037), ('disappears', 0.037), ('ferm', 0.037), ('manners', 0.037), ('responded', 0.037), ('sseetn', 0.037), ('weheenre', 0.037), ('categories', 0.037), ('language', 0.037), ('carry', 0.037), ('object', 0.035), ('merge', 0.035), ('vri', 0.035), ('zv', 0.035), ('pal', 0.035), ('innt', 0.035), ('nooft', 0.035), ('aloimonos', 0.035), ('bes', 0.035), ('calls', 0.035), ('cvo', 0.035), ('knife', 0.035), ('monkey', 0.035), ('mthaen', 0.035), ('oefre', 0.035), ('twt', 0.035), ('vde', 0.035), ('crucial', 0.035), ('ps', 0.034), ('humans', 0.033), ('purely', 0.033), ('tools', 0.033), ('ivn', 0.033), ('won', 0.033), ('mdo', 0.033), ('monitors', 0.033), ('conquer', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="123-tfidf-1" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>Author: Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos</p><p>Abstract: The problem of action recognition and human activity has been an active research area in Computer Vision and Robotics. While full-body motions can be characterized by movement and change of posture, no characterization, that holds invariance, has yet been proposed for the description of manipulation actions. We propose that a fundamental concept in understanding such actions, are the consequences of actions. There is a small set of fundamental primitive action consequences that provides a systematic high-level classification of manipulation actions. In this paper a technique is developed to recognize these action consequences. At the heart of the technique lies a novel active tracking and segmentation method that monitors the changes in appearance and topological structure of the manipulated object. These are then used in a visual semantic graph (VSG) based procedure applied to the time sequence of the monitored object to recognize the action consequence. We provide a new dataset, called Manipulation Action Consequences (MAC 1.0), which can serve as testbed for other studies on this topic. Several ex- periments on this dataset demonstrates that our method can robustly track objects and detect their deformations and division during the manipulation. Quantitative tests prove the effectiveness and efficiency of the method.</p><p>2 0.34793872 <a title="123-tfidf-2" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>3 0.26015204 <a title="123-tfidf-3" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>4 0.25304088 <a title="123-tfidf-4" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>Author: Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</p><p>Abstract: How should a video be represented? We propose a new representation for videos based on mid-level discriminative spatio-temporal patches. These spatio-temporal patches might correspond to a primitive human action, a semantic object, or perhaps a random but informative spatiotemporal patch in the video. What defines these spatiotemporal patches is their discriminative and representative properties. We automatically mine these patches from hundreds of training videos and experimentally demonstrate that these patches establish correspondence across videos and align the videos for label transfer techniques. Furthermore, these patches can be used as a discriminative vocabulary for action classification where they demonstrate stateof-the-art performance on UCF50 and Olympics datasets.</p><p>5 0.25272188 <a title="123-tfidf-5" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>Author: Chunyu Wang, Yizhou Wang, Alan L. Yuille</p><p>Abstract: We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations ofbodyparts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.</p><p>6 0.22450329 <a title="123-tfidf-6" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>7 0.21742781 <a title="123-tfidf-7" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>8 0.21475494 <a title="123-tfidf-8" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>9 0.21203615 <a title="123-tfidf-9" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>10 0.17571141 <a title="123-tfidf-10" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>11 0.17021464 <a title="123-tfidf-11" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>12 0.1641493 <a title="123-tfidf-12" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>13 0.16244099 <a title="123-tfidf-13" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>14 0.1547467 <a title="123-tfidf-14" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>15 0.13332312 <a title="123-tfidf-15" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>16 0.12197812 <a title="123-tfidf-16" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>17 0.12000628 <a title="123-tfidf-17" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>18 0.11970814 <a title="123-tfidf-18" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>19 0.11033087 <a title="123-tfidf-19" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>20 0.10955994 <a title="123-tfidf-20" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, -0.066), (2, 0.013), (3, -0.205), (4, -0.291), (5, -0.047), (6, -0.052), (7, 0.033), (8, -0.071), (9, -0.017), (10, 0.023), (11, -0.028), (12, -0.082), (13, 0.064), (14, -0.085), (15, -0.014), (16, 0.027), (17, -0.028), (18, 0.107), (19, 0.219), (20, 0.049), (21, -0.014), (22, -0.004), (23, 0.115), (24, 0.048), (25, -0.047), (26, 0.067), (27, -0.006), (28, 0.002), (29, -0.02), (30, -0.018), (31, -0.019), (32, -0.029), (33, 0.066), (34, -0.009), (35, -0.012), (36, 0.024), (37, 0.1), (38, -0.068), (39, -0.004), (40, -0.02), (41, 0.048), (42, -0.033), (43, 0.052), (44, 0.019), (45, 0.024), (46, -0.03), (47, -0.075), (48, -0.009), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95532918 <a title="123-lsi-1" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>Author: Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos</p><p>Abstract: The problem of action recognition and human activity has been an active research area in Computer Vision and Robotics. While full-body motions can be characterized by movement and change of posture, no characterization, that holds invariance, has yet been proposed for the description of manipulation actions. We propose that a fundamental concept in understanding such actions, are the consequences of actions. There is a small set of fundamental primitive action consequences that provides a systematic high-level classification of manipulation actions. In this paper a technique is developed to recognize these action consequences. At the heart of the technique lies a novel active tracking and segmentation method that monitors the changes in appearance and topological structure of the manipulated object. These are then used in a visual semantic graph (VSG) based procedure applied to the time sequence of the monitored object to recognize the action consequence. We provide a new dataset, called Manipulation Action Consequences (MAC 1.0), which can serve as testbed for other studies on this topic. Several ex- periments on this dataset demonstrates that our method can robustly track objects and detect their deformations and division during the manipulation. Quantitative tests prove the effectiveness and efficiency of the method.</p><p>2 0.89038402 <a title="123-lsi-2" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>Author: Alireza Fathi, James M. Rehg</p><p>Abstract: In this paper we present a model of action based on the change in the state of the environment. Many actions involve similar dynamics and hand-object relationships, but differ in their purpose and meaning. The key to differentiating these actions is the ability to identify how they change the state of objects and materials in the environment. We propose a weakly supervised method for learning the object and material states that are necessary for recognizing daily actions. Once these state detectors are learned, we can apply them to input videos and pool their outputs to detect actions. We further demonstrate that our method can be used to segment discrete actions from a continuous video of an activity. Our results outperform state-of-the-art action recognition and activity segmentation results.</p><p>3 0.81359476 <a title="123-lsi-3" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>Author: Michalis Raptis, Leonid Sigal</p><p>Abstract: In this paper, we develop a new model for recognizing human actions. An action is modeled as a very sparse sequence of temporally local discriminative keyframes collections of partial key-poses of the actor(s), depicting key states in the action sequence. We cast the learning of keyframes in a max-margin discriminative framework, where we treat keyframes as latent variables. This allows us to (jointly) learn a set of most discriminative keyframes while also learning the local temporal context between them. Keyframes are encoded using a spatially-localizable poselet-like representation with HoG and BoW components learned from weak annotations; we rely on structured SVM formulation to align our components and minefor hard negatives to boost localization performance. This results in a model that supports spatio-temporal localization and is insensitive to dropped frames or partial observations. We show classification performance that is competitive with the state of the art on the benchmark UT-Interaction dataset and illustrate that our model outperforms prior methods in an on-line streaming setting.</p><p>4 0.79187739 <a title="123-lsi-4" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>Author: Yicong Tian, Rahul Sukthankar, Mubarak Shah</p><p>Abstract: Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned. By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions.</p><p>5 0.68914074 <a title="123-lsi-5" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>Author: Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu, Cunzhao Shi</p><p>Abstract: In this paper, we propose a novel method for cross-view action recognition via a continuous virtual path which connects the source view and the target view. Each point on this virtual path is a virtual view which is obtained by a linear transformation of the action descriptor. All the virtual views are concatenated into an infinite-dimensional feature to characterize continuous changes from the source to the target view. However, these infinite-dimensional features cannot be used directly. Thus, we propose a virtual view kernel to compute the value of similarity between two infinite-dimensional features, which can be readily used to construct any kernelized classifiers. In addition, there are a lot of unlabeled samples from the target view, which can be utilized to improve the performance of classifiers. Thus, we present a constraint strategy to explore the information contained in the unlabeled samples. The rationality behind the constraint is that any action video belongs to only one class. Our method is verified on the IXMAS dataset, and the experimental results demonstrate that our method achieves better performance than the state-of-the-art methods.</p><p>6 0.67727435 <a title="123-lsi-6" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>7 0.64953566 <a title="123-lsi-7" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>8 0.64373463 <a title="123-lsi-8" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>9 0.6394186 <a title="123-lsi-9" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>10 0.61465412 <a title="123-lsi-10" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>11 0.60979903 <a title="123-lsi-11" href="./cvpr-2013-3D_R_Transform_on_Spatio-temporal_Interest_Points_for_Action_Recognition.html">3 cvpr-2013-3D R Transform on Spatio-temporal Interest Points for Action Recognition</a></p>
<p>12 0.59888017 <a title="123-lsi-12" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>13 0.59176481 <a title="123-lsi-13" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>14 0.58637714 <a title="123-lsi-14" href="./cvpr-2013-Multi-task_Sparse_Learning_with_Beta_Process_Prior_for_Action_Recognition.html">302 cvpr-2013-Multi-task Sparse Learning with Beta Process Prior for Action Recognition</a></p>
<p>15 0.54435849 <a title="123-lsi-15" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>16 0.48255858 <a title="123-lsi-16" href="./cvpr-2013-Action_Recognition_by_Hierarchical_Sequence_Summarization.html">32 cvpr-2013-Action Recognition by Hierarchical Sequence Summarization</a></p>
<p>17 0.45700997 <a title="123-lsi-17" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>18 0.45410022 <a title="123-lsi-18" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>19 0.44458058 <a title="123-lsi-19" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>20 0.43236789 <a title="123-lsi-20" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.125), (16, 0.04), (22, 0.225), (26, 0.034), (33, 0.291), (67, 0.095), (69, 0.048), (87, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88639259 <a title="123-lda-1" href="./cvpr-2013-Detection_of_Manipulation_Action_Consequences_%28MAC%29.html">123 cvpr-2013-Detection of Manipulation Action Consequences (MAC)</a></p>
<p>Author: Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos</p><p>Abstract: The problem of action recognition and human activity has been an active research area in Computer Vision and Robotics. While full-body motions can be characterized by movement and change of posture, no characterization, that holds invariance, has yet been proposed for the description of manipulation actions. We propose that a fundamental concept in understanding such actions, are the consequences of actions. There is a small set of fundamental primitive action consequences that provides a systematic high-level classification of manipulation actions. In this paper a technique is developed to recognize these action consequences. At the heart of the technique lies a novel active tracking and segmentation method that monitors the changes in appearance and topological structure of the manipulated object. These are then used in a visual semantic graph (VSG) based procedure applied to the time sequence of the monitored object to recognize the action consequence. We provide a new dataset, called Manipulation Action Consequences (MAC 1.0), which can serve as testbed for other studies on this topic. Several ex- periments on this dataset demonstrates that our method can robustly track objects and detect their deformations and division during the manipulation. Quantitative tests prove the effectiveness and efficiency of the method.</p><p>2 0.87423414 <a title="123-lda-2" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>Author: Yang Liu, Jing Liu, Zechao Li, Jinhui Tang, Hanqing Lu</p><p>Abstract: In this paper, we propose a novel Weakly-Supervised Dual Clustering (WSDC) approach for image semantic segmentation with image-level labels, i.e., collaboratively performing image segmentation and tag alignment with those regions. The proposed approach is motivated from the observation that superpixels belonging to an object class usually exist across multiple images and hence can be gathered via the idea of clustering. In WSDC, spectral clustering is adopted to cluster the superpixels obtained from a set of over-segmented images. At the same time, a linear transformation between features and labels as a kind of discriminative clustering is learned to select the discriminative features among different classes. The both clustering outputs should be consistent as much as possible. Besides, weakly-supervised constraints from image-level labels are imposed to restrict the labeling of superpixels. Finally, the non-convex and non-smooth objective function are efficiently optimized using an iterative CCCP procedure. Extensive experiments conducted on MSRC andLabelMe datasets demonstrate the encouraging performance of our method in comparison with some state-of-the-arts.</p><p>3 0.86153579 <a title="123-lda-3" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>Author: Steve Branson, Oscar Beijbom, Serge Belongie</p><p>Abstract: unkown-abstract</p><p>4 0.83526552 <a title="123-lda-4" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>5 0.83137733 <a title="123-lda-5" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>Author: Feng Lu, Yasuyuki Matsushita, Imari Sato, Takahiro Okabe, Yoichi Sato</p><p>Abstract: We propose an uncalibrated photometric stereo method that works with general and unknown isotropic reflectances. Our method uses a pixel intensity profile, which is a sequence of radiance intensities recorded at a pixel across multi-illuminance images. We show that for general isotropic materials, the geodesic distance between intensity profiles is linearly related to the angular difference of their surface normals, and that the intensity distribution of an intensity profile conveys information about the reflectance properties, when the intensity profile is obtained under uniformly distributed directional lightings. Based on these observations, we show that surface normals can be estimated up to a convex/concave ambiguity. A solution method based on matrix decomposition with missing data is developed for a reliable estimation. Quantitative and qualitative evaluations of our method are performed using both synthetic and real-world scenes.</p><p>6 0.82651919 <a title="123-lda-6" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>7 0.82647008 <a title="123-lda-7" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>8 0.82536358 <a title="123-lda-8" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>9 0.82533348 <a title="123-lda-9" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>10 0.8252722 <a title="123-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.8250891 <a title="123-lda-11" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>12 0.82505852 <a title="123-lda-12" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>13 0.82501739 <a title="123-lda-13" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>14 0.82500035 <a title="123-lda-14" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>15 0.8245672 <a title="123-lda-15" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>16 0.82401365 <a title="123-lda-16" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>17 0.82365209 <a title="123-lda-17" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>18 0.82356977 <a title="123-lda-18" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>19 0.82258886 <a title="123-lda-19" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>20 0.82257414 <a title="123-lda-20" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
