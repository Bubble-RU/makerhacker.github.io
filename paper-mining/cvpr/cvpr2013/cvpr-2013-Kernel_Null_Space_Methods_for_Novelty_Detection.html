<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-239" href="#">cvpr2013-239</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</h1>
<br/><p>Source: <a title="cvpr-2013-239-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bodesheim_Kernel_Null_Space_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Paul Bodesheim, Alexander Freytag, Erik Rodner, Michael Kemmler, Joachim Denzler</p><p>Abstract: Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in com- prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.</p><p>Reference: <a title="cvpr-2013-239-reference" href="../cvpr2013_reference/cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. [sent-4, score-1.513]
</p><p>2 In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. [sent-6, score-0.667]
</p><p>3 This subspace is called the null space of the training data. [sent-7, score-0.679]
</p><p>4 To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. [sent-8, score-1.258]
</p><p>5 Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. [sent-9, score-0.647]
</p><p>6 Our novelty detection approach is assessed in com-  prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. [sent-10, score-0.713]
</p><p>7 The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods. [sent-11, score-1.327]
</p><p>8 Novelty detection in the null space of a three-classexample: training samples of the known classes bonsai, lemon, and vase are projected to a single point, respectively (colored dots). [sent-17, score-1.003]
</p><p>9 Despite its importance, however, novelty detection is an often neglected part in visual recognition systems. [sent-21, score-0.713]
</p><p>10 The definition of novelty detection can be summarized as follows. [sent-22, score-0.713]
</p><p>11 Based on a fixed set of training samples from a fixed number of categories, novelty detection is a binary decision task to determine for each test sample whether it belongs to one of the known categories or not. [sent-23, score-1.053]
</p><p>12 A common assumption for novelty detection is that in feature space, sam-  ples occurring far away from the training data most likely belong to a new category. [sent-24, score-0.743]
</p><p>13 However, we assume that objects of new categories occur far away from the training data in the null space. [sent-25, score-0.668]
</p><p>14 As a consequence, a whole class is represented as a single point and we can directly use distances between the projection of a test sample and the class representations to obtain a novelty measure. [sent-27, score-1.001]
</p><p>15 An example of our null space approach using three categories of the ImageNet dataset [3] is shown in Figure 1. [sent-28, score-0.652]
</p><p>16 In 333333777422  the null space, test samples of known categories have small distances to the corresponding class representations. [sent-29, score-0.92]
</p><p>17 Related work on novelty detection mainly focuses on modeling the distribution of a single class with arbitrary complex models (see Sect. [sent-32, score-0.855]
</p><p>18 With our proposed approach, we circumvent the estimation of complex class distributions by totally removing the intra-class variances using null space projections. [sent-34, score-0.709]
</p><p>19 In contrast, our novelty detection approach yields a score obtained from a single subspace computed jointly for all known categories. [sent-37, score-0.895]
</p><p>20 We provide a method for novelty detection where we build on the Null Foley-Sammon transform (NFST) [7] due to its inherent properties explained later in this paper. [sent-39, score-0.745]
</p><p>21 With this transform, we are able to model all known training classes jointly and obtain a single novelty score for multiple classes allowing for joint multi-class novelty detection. [sent-40, score-1.737]
</p><p>22 We are not aware of any existing method that is able to perform multiclass novelty detection with a single model. [sent-41, score-0.786]
</p><p>23 Since our approach is based on the theory of null spaces which is not widely-used in our community, we give a detailed review of null space methods and a kernelization strategy in Sect. [sent-44, score-1.209]
</p><p>24 Our multi-class novelty detection approach as well as the derived one-class classification method using null space methods is explained in Sect. [sent-46, score-1.334]
</p><p>25 An overview of related work on novelty detection is given in Sect. [sent-48, score-0.713]
</p><p>26 5 showing the suitability of null space methods for multi-class novelty detection. [sent-51, score-1.24]
</p><p>27 Reviewing null space methods In the following, we review NFST in detail, since it lies at the core of our approach and is not widely-used so far. [sent-54, score-0.593]
</p><p>28 Our resulting novelty detection method based on null spaces is carried out in Sect. [sent-55, score-1.3]
</p><p>29 NFST is limited to problems with small  IRD×N  the input space (left) to the null space (right), adapted from [7]. [sent-68, score-0.626]
</p><p>30 (5)  ϕ  >0  (4)  In [7], such a ϕ is called null projection direction. [sent-93, score-0.611]
</p><p>31 To additionally guarantee (6), we first define the null spaces of the matrices St and Sw :  IRD = {z ∈ IRD  Zt = {z ∈  | St z = 0}  ,  (7)  Zw  | Swz = 0}  ,  (8)  Zt⊥  Zw⊥. [sent-105, score-0.63]
</p><p>32 , of problem (11), we can compute null projection directions ϕ(1) , . [sent-154, score-0.635]
</p><p>33 re S aibmle-  ilar to (10), we calculate C − 1 null projection directions iϕla(1r) t,o . [sent-211, score-0.658]
</p><p>34 , ϕ),( Cw−e1 c) lbcuutl using c−oe 1ff nicuileln ptsr ijne T dhierercetfioornes, the coefficients for null projection directions are:  Vt˜io . [sent-214, score-0.635]
</p><p>35 , in the null space, where k∗ contai∗ns values of∗ the kernel function calculated between x∗ and all N training samples. [sent-224, score-0.66]
</p><p>36 Related approaches Beside NFST and KNFST, there exist further null space approaches. [sent-227, score-0.593]
</p><p>37 However, it is not guaranteed that such principal components exist, especially in large-scale settings (which is in contrast to the null space of KNFST we use in our approach). [sent-230, score-0.619]
</p><p>38 There also exists a metric learning approach [5] that is closely related to null space methods. [sent-232, score-0.615]
</p><p>39 To bridge this gap, we propose a novelty detection method in the next section. [sent-240, score-0.713]
</p><p>40 Novelty detection with null space methods In the previous section, we have described NFST and its kernelization based on already existing work [7, 12, 26]. [sent-242, score-0.688]
</p><p>41 This section explains how to adapt null space methods for novelty detection in both one-class and multi-class scenarios. [sent-243, score-1.306]
</p><p>42 We therefore first show how to perform multi-class novelty detection and then apply this idea to the one-class case. [sent-246, score-0.713]
</p><p>43 Additionally, we characterize the advantages of our novelty detection approach that come from the model properties. [sent-247, score-0.713]
</p><p>44 Multi-class novelty detection using null spaces  In multi-class novelty detection, we want to calculate a novelty score indicating whether a test sample belongs to one of C known classes, no matter to which class. [sent-250, score-2.762]
</p><p>45 Throughout the rest of this paper, we refer to the classes known dur-  using projections in the joint null space: the novelty score of a test sample x∗ is the smallest distance between its projection t∗ and the class projections in the null space. [sent-251, score-2.295]
</p><p>46 We calculate a null space of dimension C −1 and determine target points one point nf oCr −eac1h a target ecrlmasisn, corresponding to the projection of class samples in the null space (see Sect. [sent-253, score-1.654]
</p><p>47 To obtain a single novelty score of a test sample x∗, we first map x∗ to t∗ by projecting x∗ into the null space. [sent-255, score-1.312]
</p><p>48 Applying a pooling step directly in the joint null space of all C classes, we use the smallest distance between t∗ and the target points . [sent-256, score-0.729]
</p><p>49 (16)  The larger the score and thus the minimum distance in the null space, the more novel is the test sample. [sent-263, score-0.612]
</p><p>50 Training a binary SVM for each known class using the samples of the other classes as negatives only leads to separations from other known classes and not from currently unknown ones. [sent-267, score-0.688]
</p><p>51 In contrast, the separation from every currently unknown class is possible with our approach due to the simple class representations in the null space. [sent-268, score-0.886]
</p><p>52 Additionally, we are able to treat all classes jointly with their true class labels, while training a binary SVM for each known class treats remaining known classes as a single negative class, which contradicts to the idea of novelty detection. [sent-269, score-1.352]
</p><p>53 The novelty detection formulation of SVM [20] is only derived for one-class settings (see Sect. [sent-270, score-0.713]
</p><p>54 Our one-class classification approaches: all samples of the target class are mapped on a single point t in a one-dimensional subspace and the novelty score of a test sample x∗ is the distance of its projection t∗ to t. [sent-286, score-1.241]
</p><p>55 One-class classification using null spaces At first glance, one-class classification is not possible with null space methods, because we only have a single target class in a one-class setting. [sent-289, score-1.458]
</p><p>56 This leads to zero null projection directions, since the number of these directions is C 1(see Sect. [sent-290, score-0.672]
</p><p>57 Using this idea, we are able to compute a single null projection direction and all class samples are mapped on a single target value t along this direction. [sent-295, score-1.039]
</p><p>58 To check whether a test sample x∗ belongs to the target class, we compute its projection on −  the null projection direction and obtain the value t∗ . [sent-296, score-0.787]
</p><p>59 As a novelty score of x∗ we propose using the absolute difference between t and t∗ : OneClassNovelty(x∗) = |t − t∗ |  (17)  similar to the multi-class case, where a large score indicates novelty. [sent-297, score-0.715]
</p><p>60 gle null projection direction by separating the class samples from “minus data”. [sent-301, score-0.855]
</p><p>61 Again, all true class samples are mapped on a single target value t and we compute the novelty score similar to our first approach using Eq. [sent-303, score-1.061]
</p><p>62 t, which is of no interest for computing the novelty score. [sent-306, score-0.647]
</p><p>63 Computing the novelty score of a new sample can be done in linear time. [sent-311, score-0.708]
</p><p>64 Advantages of our novelty detection approach Our proposed novelty detection approach benefits from the null space, ajoint subspace of all training samples where each known class is represented by a single point. [sent-316, score-2.381]
</p><p>65 In contrast to other subspace methods such as Kernel PCA, additional density estimation or clustering within the obtained subspace can be avoided and a simple distance measure can be applied to get a novelty score. [sent-317, score-0.759]
</p><p>66 , when applying the one-vsrest SVM framework, null space methods offer the possibility to treat several classes in a joint manner with a single subspace model. [sent-320, score-0.819]
</p><p>67 Additionally, our approach separates known classes from every currently unknown class without the necessity of negative samples by using simple representations of known classes in the null space. [sent-321, score-1.227]
</p><p>68 This is in contrast to binary classifiers treating samples of one class as positives and samples of remaining known classes as negatives. [sent-322, score-0.593]
</p><p>69 Using null space methods for novelty detection, we are able to calculate a single feature for each class of the target data and thus are able to compute features with zero intra-class variance. [sent-323, score-1.566]
</p><p>70 Such features are computable with  null space methods, even for multiple classes. [sent-326, score-0.593]
</p><p>71 The transformed features obtained using null space methods can therefore be treated as class-specific features, since the transformation preserves the joint characteristics within each class. [sent-329, score-0.616]
</p><p>72 As previously mentioned, such features are perfectly suited for novelty detection from a theoretical point of view [23]. [sent-330, score-0.734]
</p><p>73 Hence, additional parameter tuning beyond kernel hyperparameters is not necessary for our proposed novelty detection method. [sent-336, score-0.803]
</p><p>74 Related work on novelty detection  An overview of basic concepts for novelty detection in signal processing is provided by the review papers of Markou and Singh [15, 16]. [sent-338, score-1.426]
</p><p>75 In visual object recognition, novelty detection should not be confused with the detection of unseen classes in zero shot learning [9], where knowledge about new objects is used explicitly, e. [sent-339, score-0.963]
</p><p>76 Generally, novelty detection problems can be divided into one-class and multi-class settings depending on the number of known classes during training. [sent-342, score-0.9]
</p><p>77 Recent work on novelty detection focuses on one-class classification. [sent-343, score-0.713]
</p><p>78 In the following, we give a short overview of related work for both one-class and multiclass novelty detection scenarios. [sent-345, score-0.738]
</p><p>79 Artificial super-class A simple way to perform multiclass novelty detection is to train a single one-class classifier for all available samples of all known categories [10]. [sent-364, score-0.99]
</p><p>80 Therefore, the rejection method used in [22] can be applied to novelty detection and we compare our approach to this strategy. [sent-371, score-0.732]
</p><p>81 Multi-class classifiers Reject strategies for multi-class classification are related to novelty detection but differ in treating regions between classes. [sent-372, score-0.808]
</p><p>82 Samples could also be rejected when being close to the decision boundaries between known classes, which is obviously contradicting with the idea of novelty detection and which is a severe problem especially when classes overlap in feature space. [sent-373, score-0.918]
</p><p>83 Experiments  We evaluate our novelty detection approach1 in visual object recognition on two datasets, Caltech-256 [6] and ImageNet [3]. [sent-380, score-0.713]
</p><p>84 In the experiments, we focus on multi-class novelty detection. [sent-384, score-0.647]
</p><p>85 However, this is not the case when multiple classes are considered and we show that our null space approach outperforms all other methods in this typical scenario important for lifelong learning and automatic object discovery. [sent-387, score-0.749]
</p><p>86 Performance in multi-class novelty detection on the Caltech-256 dataset. [sent-433, score-0.713]
</p><p>87 The experiments on the ImageNet dataset are done with 100 samples per target class for training and 50 samples from each of the 1,000 classes (including the target classes) for testing. [sent-439, score-0.629]
</p><p>88 Multi-class novelty detection results The results on the Caltech-256 dataset and the ImageNet dataset are shown in Figure 5 and Figure 6, respectively. [sent-470, score-0.713]
</p><p>89 Interestingly, the binary SVM approach seems to be more suitable for the task of multi-class novelty detection in terms of higher median AUC scores compared to most approaches based on one-class classifiers. [sent-472, score-0.77]
</p><p>90 This highlights the capability and the relevance of our proposed null space approach for novelty detection. [sent-476, score-1.24]
</p><p>91 Conclusions and future work Multi-class novelty detection is a challenging problem, which needs more attention from the research community. [sent-478, score-0.713]
</p><p>92 This paper proposes a new novelty detection approach based on null space projections, which is perfectly suitable for tackling this problem. [sent-480, score-1.327]
</p><p>93 The benefit of our proposed multi-class novelty detection approach is its ability of separating a set of known 333333778088  iCAadnMU%][e86570 K6N. [sent-481, score-0.806]
</p><p>94 Performance in multi-class novelty detection on the ImageNet dataset. [sent-522, score-0.713]
</p><p>95 The approach  is able to decide about novelty in a single step using a single model, whereas other approaches need to train a model for each known class without considering them jointly. [sent-524, score-0.903]
</p><p>96 Our experimental results clearly demonstrate the advantage of the joint learning approach using the null space leading to the best performance for multi-class novelty detection compared to all other methods. [sent-525, score-1.329]
</p><p>97 Additionally, we have addressed one-class classification as a special case of novelty detection. [sent-526, score-0.675]
</p><p>98 Future work will concentrate on novelty detection in large-scale scenarios, where hundreds or thousands of categories are known to the model. [sent-528, score-0.838]
</p><p>99 Additionally, incorporating metric learning approaches related to null space methods such as [5, 17] is of special interest. [sent-529, score-0.615]
</p><p>100 A small sphere and large margin approach for novelty detection using training data with outliers. [sent-735, score-0.743]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('novelty', 0.647), ('null', 0.56), ('nfst', 0.176), ('classes', 0.121), ('class', 0.116), ('knfst', 0.105), ('samples', 0.101), ('scatter', 0.088), ('target', 0.08), ('zt', 0.072), ('ird', 0.07), ('kernel', 0.07), ('imagenet', 0.069), ('detection', 0.066), ('known', 0.066), ('fst', 0.062), ('hik', 0.062), ('categories', 0.059), ('mapped', 0.057), ('subspace', 0.056), ('tax', 0.054), ('eigenbasis', 0.053), ('svdd', 0.053), ('projection', 0.051), ('sw', 0.05), ('zw', 0.048), ('svm', 0.048), ('markou', 0.047), ('additionally', 0.043), ('treating', 0.038), ('currently', 0.038), ('unknown', 0.038), ('zero', 0.037), ('auc', 0.037), ('projections', 0.036), ('btswb', 0.035), ('icaadnmu', 0.035), ('lifelong', 0.035), ('tsb', 0.035), ('score', 0.034), ('nothing', 0.034), ('products', 0.033), ('space', 0.033), ('pooling', 0.033), ('transform', 0.032), ('acu', 0.031), ('reviewed', 0.031), ('inner', 0.031), ('training', 0.03), ('classifiers', 0.029), ('kernelization', 0.029), ('tsw', 0.029), ('fisher', 0.029), ('discriminant', 0.029), ('classification', 0.028), ('origin', 0.028), ('jena', 0.027), ('separating', 0.027), ('sample', 0.027), ('spaces', 0.027), ('principal', 0.026), ('single', 0.026), ('variance', 0.026), ('unseen', 0.026), ('beside', 0.025), ('multiclass', 0.025), ('directions', 0.024), ('sb', 0.023), ('calculate', 0.023), ('crosses', 0.023), ('joint', 0.023), ('kernels', 0.022), ('dealing', 0.022), ('able', 0.022), ('basis', 0.022), ('metric', 0.022), ('outlier', 0.022), ('binary', 0.021), ('eigenvalues', 0.021), ('perfectly', 0.021), ('hyperparameters', 0.02), ('eigenvectors', 0.02), ('ect', 0.02), ('artificial', 0.02), ('rejection', 0.019), ('pages', 0.019), ('kernelized', 0.019), ('occur', 0.019), ('separation', 0.018), ('decision', 0.018), ('predictive', 0.018), ('median', 0.018), ('test', 0.018), ('gaussian', 0.018), ('kt', 0.018), ('scores', 0.018), ('neural', 0.018), ('nf', 0.017), ('libsvm', 0.017), ('matrix', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="239-tfidf-1" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>Author: Paul Bodesheim, Alexander Freytag, Erik Rodner, Michael Kemmler, Joachim Denzler</p><p>Abstract: Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in com- prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.</p><p>2 0.13412981 <a title="239-tfidf-2" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>3 0.082709305 <a title="239-tfidf-3" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>Author: Michele Fenzi, Laura Leal-Taixé, Bodo Rosenhahn, Jörn Ostermann</p><p>Abstract: In this paper, we propose a method for learning a class representation that can return a continuous value for the pose of an unknown class instance using only 2D data and weak 3D labelling information. Our method is based on generative feature models, i.e., regression functions learnt from local descriptors of the same patch collected under different viewpoints. The individual generative models are then clustered in order to create class generative models which form the class representation. At run-time, the pose of the query image is estimated in a maximum a posteriori fashion by combining the regression functions belonging to the matching clusters. We evaluate our approach on the EPFL car dataset [17] and the Pointing’04 face dataset [8]. Experimental results show that our method outperforms by 10% the state-of-the-art in the first dataset and by 9% in the second.</p><p>4 0.069203027 <a title="239-tfidf-4" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>5 0.067192204 <a title="239-tfidf-5" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>6 0.0642948 <a title="239-tfidf-6" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>7 0.063332304 <a title="239-tfidf-7" href="./cvpr-2013-Semi-supervised_Domain_Adaptation_with_Instance_Constraints.html">387 cvpr-2013-Semi-supervised Domain Adaptation with Instance Constraints</a></p>
<p>8 0.060280278 <a title="239-tfidf-8" href="./cvpr-2013-Blocks_That_Shout%3A_Distinctive_Parts_for_Scene_Classification.html">67 cvpr-2013-Blocks That Shout: Distinctive Parts for Scene Classification</a></p>
<p>9 0.058650378 <a title="239-tfidf-9" href="./cvpr-2013-Supervised_Kernel_Descriptors_for_Visual_Recognition.html">421 cvpr-2013-Supervised Kernel Descriptors for Visual Recognition</a></p>
<p>10 0.057452708 <a title="239-tfidf-10" href="./cvpr-2013-Finding_Things%3A_Image_Parsing_with_Regions_and_Per-Exemplar_Detectors.html">173 cvpr-2013-Finding Things: Image Parsing with Regions and Per-Exemplar Detectors</a></p>
<p>11 0.05708899 <a title="239-tfidf-11" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>12 0.056786958 <a title="239-tfidf-12" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>13 0.056512877 <a title="239-tfidf-13" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>14 0.054921705 <a title="239-tfidf-14" href="./cvpr-2013-Efficient_Maximum_Appearance_Search_for_Large-Scale_Object_Detection.html">144 cvpr-2013-Efficient Maximum Appearance Search for Large-Scale Object Detection</a></p>
<p>15 0.053179473 <a title="239-tfidf-15" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>16 0.052010179 <a title="239-tfidf-16" href="./cvpr-2013-CLAM%3A_Coupled_Localization_and_Mapping_with_Efficient_Outlier_Handling.html">74 cvpr-2013-CLAM: Coupled Localization and Mapping with Efficient Outlier Handling</a></p>
<p>17 0.052008115 <a title="239-tfidf-17" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>18 0.051721405 <a title="239-tfidf-18" href="./cvpr-2013-Learning_to_Detect_Partially_Overlapping_Instances.html">264 cvpr-2013-Learning to Detect Partially Overlapping Instances</a></p>
<p>19 0.050975595 <a title="239-tfidf-19" href="./cvpr-2013-Generalized_Domain-Adaptive_Dictionaries.html">185 cvpr-2013-Generalized Domain-Adaptive Dictionaries</a></p>
<p>20 0.050301507 <a title="239-tfidf-20" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.135), (1, -0.031), (2, -0.039), (3, 0.02), (4, 0.036), (5, 0.02), (6, -0.03), (7, -0.028), (8, -0.003), (9, -0.008), (10, -0.029), (11, -0.024), (12, -0.023), (13, -0.064), (14, -0.055), (15, -0.029), (16, -0.035), (17, -0.033), (18, -0.026), (19, -0.012), (20, -0.005), (21, -0.019), (22, -0.006), (23, -0.006), (24, 0.003), (25, 0.017), (26, -0.038), (27, 0.043), (28, -0.051), (29, -0.049), (30, -0.068), (31, 0.029), (32, -0.039), (33, -0.022), (34, -0.008), (35, 0.004), (36, -0.01), (37, -0.026), (38, 0.005), (39, 0.015), (40, 0.003), (41, -0.023), (42, -0.008), (43, 0.002), (44, 0.037), (45, -0.006), (46, 0.009), (47, -0.013), (48, 0.003), (49, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94628251 <a title="239-lsi-1" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>Author: Paul Bodesheim, Alexander Freytag, Erik Rodner, Michael Kemmler, Joachim Denzler</p><p>Abstract: Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in com- prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.</p><p>2 0.77705258 <a title="239-lsi-2" href="./cvpr-2013-Optimizing_1-Nearest_Prototype_Classifiers.html">320 cvpr-2013-Optimizing 1-Nearest Prototype Classifiers</a></p>
<p>Author: Paul Wohlhart, Martin Köstinger, Michael Donoser, Peter M. Roth, Horst Bischof</p><p>Abstract: The development of complex, powerful classifiers and their constant improvement have contributed much to the progress in many fields of computer vision. However, the trend towards large scale datasets revived the interest in simpler classifiers to reduce runtime. Simple nearest neighbor classifiers have several beneficial properties, such as low complexity and inherent multi-class handling, however, they have a runtime linear in the size of the database. Recent related work represents data samples by assigning them to a set of prototypes that partition the input feature space and afterwards applies linear classifiers on top of this representation to approximate decision boundaries locally linear. In this paper, we go a step beyond these approaches and purely focus on 1-nearest prototype classification, where we propose a novel algorithm for deriving optimal prototypes in a discriminative manner from the training samples. Our method is implicitly multi-class capable, parameter free, avoids noise overfitting and, since during testing only comparisons to the derived prototypes are required, highly efficient. Experiments demonstrate that we are able to outperform related locally linear methods, while even getting close to the results of more complex classifiers.</p><p>3 0.75948584 <a title="239-lsi-3" href="./cvpr-2013-Learning_by_Associating_Ambiguously_Labeled_Images.html">261 cvpr-2013-Learning by Associating Ambiguously Labeled Images</a></p>
<p>Author: Zinan Zeng, Shijie Xiao, Kui Jia, Tsung-Han Chan, Shenghua Gao, Dong Xu, Yi Ma</p><p>Abstract: We study in this paper the problem of learning classifiers from ambiguously labeled images. For instance, in the collection of new images, each image contains some samples of interest (e.g., human faces), and its associated caption has labels with the true ones included, while the samplelabel association is unknown. The task is to learn classifiers from these ambiguously labeled images and generalize to new images. An essential consideration here is how to make use of the information embedded in the relations between samples and labels, both within each image and across the image set. To this end, we propose a novel framework to address this problem. Our framework is motivated by the observation that samples from the same class repetitively appear in the collection of ambiguously labeled training images, while they are just ambiguously labeled in each image. If we can identify samples of the same class from each image and associate them across the image set, the matrix formed by the samples from the same class would be ideally low-rank. By leveraging such a low-rank assump- tion, we can simultaneously optimize a partial permutation matrix (PPM) for each image, which is formulated in order to exploit all information between samples and labels in a principled way. The obtained PPMs can be readily used to assign labels to samples in training images, and then a standard SVM classifier can be trained and used for unseen data. Experiments on benchmark datasets show the effectiveness of our proposed method.</p><p>4 0.75325227 <a title="239-lsi-4" href="./cvpr-2013-Heterogeneous_Visual_Features_Fusion_via_Sparse_Multimodal_Machine.html">201 cvpr-2013-Heterogeneous Visual Features Fusion via Sparse Multimodal Machine</a></p>
<p>Author: Hua Wang, Feiping Nie, Heng Huang, Chris Ding</p><p>Abstract: To better understand, search, and classify image and video information, many visual feature descriptors have been proposed to describe elementary visual characteristics, such as the shape, the color, the texture, etc. How to integrate these heterogeneous visual features and identify the important ones from them for specific vision tasks has become an increasingly critical problem. In this paper, We propose a novel Sparse Multimodal Learning (SMML) approach to integrate such heterogeneous features by using the joint structured sparsity regularizations to learn the feature importance of for the vision tasks from both group-wise and individual point of views. A new optimization algorithm is also introduced to solve the non-smooth objective with rigorously proved global convergence. We applied our SMML method to five broadly used object categorization and scene understanding image data sets for both singlelabel and multi-label image classification tasks. For each data set we integrate six different types of popularly used image features. Compared to existing scene and object cat- egorization methods using either single modality or multimodalities of features, our approach always achieves better performances measured.</p><p>5 0.71168518 <a title="239-lsi-5" href="./cvpr-2013-From_N_to_N%2B1%3A_Multiclass_Transfer_Incremental_Learning.html">179 cvpr-2013-From N to N+1: Multiclass Transfer Incremental Learning</a></p>
<p>Author: Ilja Kuzborskij, Francesco Orabona, Barbara Caputo</p><p>Abstract: Since the seminal work of Thrun [17], the learning to learnparadigm has been defined as the ability ofan agent to improve its performance at each task with experience, with the number of tasks. Within the object categorization domain, the visual learning community has actively declined this paradigm in the transfer learning setting. Almost all proposed methods focus on category detection problems, addressing how to learn a new target class from few samples by leveraging over the known source. But if one thinks oflearning over multiple tasks, there is a needfor multiclass transfer learning algorithms able to exploit previous source knowledge when learning a new class, while at the same time optimizing their overall performance. This is an open challenge for existing transfer learning algorithms. The contribution of this paper is a discriminative method that addresses this issue, based on a Least-Squares Support Vector Machine formulation. Our approach is designed to balance between transferring to the new class and preserving what has already been learned on the source models. Exten- sive experiments on subsets of publicly available datasets prove the effectiveness of our approach.</p><p>6 0.7018497 <a title="239-lsi-6" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>7 0.69852018 <a title="239-lsi-7" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>8 0.6916694 <a title="239-lsi-8" href="./cvpr-2013-Fast_Object_Detection_with_Entropy-Driven_Evaluation.html">168 cvpr-2013-Fast Object Detection with Entropy-Driven Evaluation</a></p>
<p>9 0.68596274 <a title="239-lsi-9" href="./cvpr-2013-Sparse_Output_Coding_for_Large-Scale_Visual_Recognition.html">403 cvpr-2013-Sparse Output Coding for Large-Scale Visual Recognition</a></p>
<p>10 0.68224877 <a title="239-lsi-10" href="./cvpr-2013-Locally_Aligned_Feature_Transforms_across_Views.html">271 cvpr-2013-Locally Aligned Feature Transforms across Views</a></p>
<p>11 0.67516404 <a title="239-lsi-11" href="./cvpr-2013-Discriminative_Sub-categorization.html">134 cvpr-2013-Discriminative Sub-categorization</a></p>
<p>12 0.67222315 <a title="239-lsi-12" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>13 0.66871232 <a title="239-lsi-13" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>14 0.65989673 <a title="239-lsi-14" href="./cvpr-2013-Kernel_Learning_for_Extrinsic_Classification_of_Manifold_Features.html">237 cvpr-2013-Kernel Learning for Extrinsic Classification of Manifold Features</a></p>
<p>15 0.65360355 <a title="239-lsi-15" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<p>16 0.64971644 <a title="239-lsi-16" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>17 0.64816582 <a title="239-lsi-17" href="./cvpr-2013-A_Lazy_Man%27s_Approach_to_Benchmarking%3A_Semisupervised_Classifier_Evaluation_and_Recalibration.html">15 cvpr-2013-A Lazy Man's Approach to Benchmarking: Semisupervised Classifier Evaluation and Recalibration</a></p>
<p>18 0.64410591 <a title="239-lsi-18" href="./cvpr-2013-Learning_without_Human_Scores_for_Blind_Image_Quality_Assessment.html">266 cvpr-2013-Learning without Human Scores for Blind Image Quality Assessment</a></p>
<p>19 0.64194047 <a title="239-lsi-19" href="./cvpr-2013-Efficient_Detector_Adaptation_for_Object_Detection_in_a_Video.html">142 cvpr-2013-Efficient Detector Adaptation for Object Detection in a Video</a></p>
<p>20 0.64135039 <a title="239-lsi-20" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.099), (16, 0.022), (26, 0.044), (28, 0.019), (33, 0.265), (67, 0.065), (69, 0.06), (87, 0.067), (96, 0.251)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89875674 <a title="239-lda-1" href="./cvpr-2013-On_a_Link_Between_Kernel_Mean_Maps_and_Fraunhofer_Diffraction%2C_with_an_Application_to_Super-Resolution_Beyond_the_Diffraction_Limit.html">312 cvpr-2013-On a Link Between Kernel Mean Maps and Fraunhofer Diffraction, with an Application to Super-Resolution Beyond the Diffraction Limit</a></p>
<p>Author: Stefan Harmeling, Michael Hirsch, Bernhard Schölkopf</p><p>Abstract: We establish a link between Fourier optics and a recent construction from the machine learning community termed the kernel mean map. Using the Fraunhofer approximation, it identifies the kernel with the squared Fourier transform of the aperture. This allows us to use results about the invertibility of the kernel mean map to provide a statement about the invertibility of Fraunhofer diffraction, showing that imaging processes with arbitrarily small apertures can in principle be invertible, i.e., do not lose information, provided the objects to be imaged satisfy a generic condition. A real world experiment shows that we can super-resolve beyond the Rayleigh limit.</p><p>2 0.87875825 <a title="239-lda-2" href="./cvpr-2013-Is_There_a_Procedural_Logic_to_Architecture%3F.html">228 cvpr-2013-Is There a Procedural Logic to Architecture?</a></p>
<p>Author: Julien Weissenberg, Hayko Riemenschneider, Mukta Prasad, Luc Van_Gool</p><p>Abstract: Urban models are key to navigation, architecture and entertainment. Apart from visualizing fa ¸cades, a number of tedious tasks remain largely manual (e.g. compression, generating new fac ¸ade designs and structurally comparing fa c¸ades for classification, retrieval and clustering). We propose a novel procedural modelling method to automatically learn a grammar from a set of fa c¸ades, generate new fa ¸cade instances and compare fa ¸cades. To deal with the difficulty of grammatical inference, we reformulate the problem. Instead of inferring a compromising, onesize-fits-all, single grammar for all tasks, we infer a model whose successive refinements are production rules tailored for each task. We demonstrate our automatic rule inference on datasets of two different architectural styles. Our method supercedes manual expert work and cuts the time required to build a procedural model of a fa ¸cade from several days to a few milliseconds.</p><p>3 0.85006213 <a title="239-lda-3" href="./cvpr-2013-Improving_the_Visual_Comprehension_of_Point_Sets.html">218 cvpr-2013-Improving the Visual Comprehension of Point Sets</a></p>
<p>Author: Sagi Katz, Ayellet Tal</p><p>Abstract: Point sets are the standard output of many 3D scanning systems and depth cameras. Presenting the set of points as is, might “hide ” the prominent features of the object from which the points are sampled. Our goal is to reduce the number of points in a point set, for improving the visual comprehension from a given viewpoint. This is done by controlling the density of the reduced point set, so as to create bright regions (low density) and dark regions (high density), producing an effect of shading. This data reduction is achieved by leveraging a limitation of a solution to the classical problem of determining visibility from a viewpoint. In addition, we introduce a new dual problem, for determining visibility of a point from infinity, and show how a limitation of its solution can be leveraged in a similar way.</p><p>4 0.8384521 <a title="239-lda-4" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>same-paper 5 0.81510472 <a title="239-lda-5" href="./cvpr-2013-Kernel_Null_Space_Methods_for_Novelty_Detection.html">239 cvpr-2013-Kernel Null Space Methods for Novelty Detection</a></p>
<p>Author: Paul Bodesheim, Alexander Freytag, Erik Rodner, Michael Kemmler, Joachim Denzler</p><p>Abstract: Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in com- prehensive multi-class experiments using the publicly available datasets Caltech-256 and ImageNet. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.</p><p>6 0.79119885 <a title="239-lda-6" href="./cvpr-2013-Optimized_Pedestrian_Detection_for_Multiple_and_Occluded_People.html">318 cvpr-2013-Optimized Pedestrian Detection for Multiple and Occluded People</a></p>
<p>7 0.78787887 <a title="239-lda-7" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>8 0.76958209 <a title="239-lda-8" href="./cvpr-2013-Universality_of_the_Local_Marginal_Polytope.html">448 cvpr-2013-Universality of the Local Marginal Polytope</a></p>
<p>9 0.76824588 <a title="239-lda-9" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>10 0.76761276 <a title="239-lda-10" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>11 0.767564 <a title="239-lda-11" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>12 0.76575822 <a title="239-lda-12" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>13 0.76502043 <a title="239-lda-13" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>14 0.7648657 <a title="239-lda-14" href="./cvpr-2013-Incorporating_Structural_Alternatives_and_Sharing_into_Hierarchy_for_Multiclass_Object_Recognition_and_Detection.html">221 cvpr-2013-Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection</a></p>
<p>15 0.76481175 <a title="239-lda-15" href="./cvpr-2013-Learning_Structured_Hough_Voting_for_Joint_Object_Detection_and_Occlusion_Reasoning.html">256 cvpr-2013-Learning Structured Hough Voting for Joint Object Detection and Occlusion Reasoning</a></p>
<p>16 0.76478082 <a title="239-lda-16" href="./cvpr-2013-Bottom-Up_Segmentation_for_Top-Down_Detection.html">70 cvpr-2013-Bottom-Up Segmentation for Top-Down Detection</a></p>
<p>17 0.76474047 <a title="239-lda-17" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>18 0.76448858 <a title="239-lda-18" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>19 0.7644226 <a title="239-lda-19" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>20 0.76412439 <a title="239-lda-20" href="./cvpr-2013-SLAM%2B%2B%3A_Simultaneous_Localisation_and_Mapping_at_the_Level_of_Objects.html">372 cvpr-2013-SLAM++: Simultaneous Localisation and Mapping at the Level of Objects</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
