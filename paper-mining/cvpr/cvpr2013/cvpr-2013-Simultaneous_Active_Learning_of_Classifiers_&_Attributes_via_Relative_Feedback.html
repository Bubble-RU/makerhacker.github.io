<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-396" href="#">cvpr2013-396</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</h1>
<br/><p>Source: <a title="cvpr-2013-396-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Biswas_Simultaneous_Active_Learning_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Arijit Biswas, Devi Parikh</p><p>Abstract: Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. “I think this is a forest, what do you think?”. If the learner is wrong, the supervisorprovides an explanation e.g. “No, this is too open to be a forest”. With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work, we propose three improvements over this set-up. First, we incorporate a weighting scheme that instead of making a hard decision reasons about the likelihood of an image being a negative example. Second, we do away with pre-trained attributes and instead learn the attribute models on the fly, alleviating overhead and restrictions of a pre-determined attribute vocabulary. Finally, we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. We demonstrate significant improvement in classification accuracy on faces and shoes. We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories.</p><p>Reference: <a title="cvpr-2013-396-reference" href="../cvpr2013_reference/cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However it traditionally views the supervisor simply as a labeling machine. [sent-3, score-0.706]
</p><p>2 Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. [sent-4, score-0.733]
</p><p>3 With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. [sent-12, score-1.034]
</p><p>4 Second, we do away with pre-trained attributes and instead learn the attribute models on the fly, alleviating overhead and restrictions of a pre-determined attribute vocabulary. [sent-16, score-0.814]
</p><p>5 Finally, we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. [sent-17, score-0.539]
</p><p>6 We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories. [sent-19, score-0.406]
</p><p>7 These methods aim to solicit labels from the supervisor on a small but useful set of images, leading to classification performance similar to that of having labeled a larger but random set of images. [sent-26, score-0.771]
</p><p>8 However, most existing active learning settings still view the supervisor as an entity to simply get labels from. [sent-27, score-0.82]
</p><p>9 The supervisor often has much more domain knowledge about an application at hand than just the label, that if communicated to the learner, may allow the learner to learn from even fewer examples. [sent-28, score-0.946]
</p><p>10 In their work [17], at each iteration in active learning, the learner identifies an image that it would like labeled. [sent-32, score-0.378]
</p><p>11 If the learner is wrong, the supervisor identifies a reason that conveys to the learner why it is wrong. [sent-36, score-1.252]
</p><p>12 The supervisor may say “No, this is not a forest image, because it is too open to be a forest image”. [sent-37, score-0.977]
</p><p>13 The learner is assumed to have access to a set of pre-trained relative attribute [16] predictors. [sent-38, score-0.623]
</p><p>14 In this case, it uses the “openness” attribute model to find all images from an unlabeled pool that are even more open than the selected query image, and assumes that those must not be forests either. [sent-39, score-0.714]
</p><p>15 Such a rich communication between the supervisor and the learner was shown to lead to faster learning with fewer labeled examples [17]. [sent-41, score-1.038]
</p><p>16 Moreover, if an image is deemed to not be a forest due to several reasons provided by the supervisor across the active learning iterations, this increases our confidence in it not being a forest. [sent-47, score-0.902]
</p><p>17 2), instead of requiring the learner to have access to a pre-trained set of relative attribute models, we allow the learner to learn the attribute models on the fly, simultaneously with the category models. [sent-50, score-1.179]
</p><p>18 Requiring access to a domain relevant vocabulary of attributes and corresponding attribute models is very restrictive and can be time consuming to acquire. [sent-51, score-0.545]
</p><p>19 Instead, we leverage the following observation: when the supervisor says “This query image is too open to be a forest”, it not only conveys information about forests, but it also conveys informa-  tion about the notion of openness. [sent-52, score-1.066]
</p><p>20 If in previous (or future) iterations the learner has already collected a few example forest images, this feedback from the supervisor indicates that those images must be less open than the query image. [sent-53, score-1.542]
</p><p>21 This allows the supervisor to introduce new attributes as and when necessary without being constrained by a pre-determined vocabulary. [sent-55, score-0.869]
</p><p>22 The learner and supervisor start with nothing but an unlabeled pool of images, and learn both the categories and attribute models from scratch. [sent-57, score-1.404]
</p><p>23 The attributes-based feedback provided by the supervisor is propagated to many unlabeled images by the learner. [sent-61, score-1.063]
</p><p>24 In both cases, we assume that if the learner is correct about its belief about a query image, the supervisor can confirm this. [sent-64, score-1.135]
</p><p>25 But if the learner is incorrect, the supervisor can indicate that and provide an explanation, but can not identify the correct label of the image. [sent-65, score-0.998]
</p><p>26 The supervisor can look at example images of the claimed category and easily verify if the claim is correct or not. [sent-67, score-0.744]
</p><p>27 But the supervisor may not have the time to navigate through a long list of categories, nor have the expertise to do so. [sent-68, score-0.763]
</p><p>28 Attributes 1As part of our experimental setup, we collected and have made publicly available annotations for 29 relative attributes on 60 celebrity faces [13], the largest relative attributes dataset to date. [sent-69, score-0.413]
</p><p>29 For instance, say the learner incorrectly claims a teenager in a surveillance video is Kirabo Smith, and shows previously labeled example images of Kirabo Smith. [sent-71, score-0.366]
</p><p>30 The supervisor can see from the examples that Kirabo Smith is actually a senior citizen. [sent-72, score-0.706]
</p><p>31 The supervisor may not know who the person in the video is, but can easily say that the person is too young to be Kirabo Smith. [sent-73, score-0.747]
</p><p>32 We now provide an overview of the use of attributes for classifier feedback as in [17] and then describe our proposed approach. [sent-79, score-0.441]
</p><p>33 Preliminaries A supervisor is teaching a machine visual concepts. [sent-81, score-0.706]
</p><p>34 As learning aid, there is a pool of unlabeled images that the supervisor will label over the course of the learning process for the learner to learn from. [sent-82, score-1.178]
</p><p>35 It communicates its own belief to the supervisor in the form of a predicted label for this image. [sent-84, score-0.823]
</p><p>36 If rejected, the supervisor communicates an explanation using attributes for why the learner’s belief was wrong. [sent-86, score-0.949]
</p><p>37 The image with the highest entropy of the class distribution pk is chosen to be the query image We will describe our novel criterion for picking the query image in  xq. [sent-98, score-0.45]
</p><p>38 The first is label-based feedback where the supervisor confirms or rejects the learners predicted label for the actively chosen image instance And the sec-  xq. [sent-104, score-1.124]
</p><p>39 We now discuss how the attributes-based feedback is incorporated given that the learner has access to a set of attribute predictors. [sent-106, score-0.837]
</p><p>40 The supervisor identifies an attribute am that he thinks is most appropriate to explain to the learner why xq does not belong to l. [sent-112, score-1.489]
</p><p>41 The supervisor can either say “xq is too am to be l” or “xq is not am enough to be l”, whichever be the case. [sent-114, score-0.77]
</p><p>42 In the former case, the learner computes the strength of am in xq as rm (xq), where rm is a attribute strength predictor for attribute am (Section 2. [sent-115, score-1.292]
</p><p>43 The learner identifies  xq  all images in the currently unlabeled pool of images U with attribute strength of am more than rm (xq). [sent-117, score-1.054]
</p><p>44 Attribute Predictors The feedback provided by the supervisor relates the query image actively selected by the learner to the category the learner believes the query image is from. [sent-130, score-1.772]
</p><p>45 We now describe how these relative attribute predictors are trained offline. [sent-132, score-0.437]
</p><p>46 We will describe our approach to learning these attribute models on the fly in Section 3. [sent-133, score-0.483]
</p><p>47 With this, given any image x in our pool of unlabeled images U, we can compute the relative strength of each of the M attributes as rm(x) = wmTx. [sent-146, score-0.373]
</p><p>48 Incorporating Label-based Feedback As described earlier, we consider a scenario where the supervisor can verify if the classifier’s prediction of is correct or not. [sent-149, score-0.724]
</p><p>49 But when incorrect, it would be very time consuming for or beyond the expertise of the supervisor to seek out the correct category label to provide as feedback. [sent-150, score-0.776]
</p><p>50 Hence the supervisor only confirms or rejects the prediction, and does not provide a correction if rejecting it. [sent-151, score-0.764]
</p><p>51 Weighting Scheme for Negative Examples  As described above, Parkash and Parikh [17] use the attributes-based feedback to fetch unlabeled images and add them as negative examples, all with the same weight. [sent-159, score-0.407]
</p><p>52 In our running example, if the query image is too open to be a forest, images that are significantly more open than the query image are more likely to not be forests than images that are barely more open than the query image. [sent-161, score-0.728]
</p><p>53 Also, as iterations go by, if the same image is deemed to not be a forest due to several reasons provided by the supervisor on different query images, we should be more confident of the image not being a forest. [sent-162, score-0.969]
</p><p>54 It is computed using attributes-based feedback accumulated over all past iterations (indexed by q) where the classifier incorrectly predicted the label of the corresponding query image to be l. [sent-166, score-0.535]
</p><p>55 Without loss ofgenerality, we will assume that for each of these iterations, the supervisor gave the explanation: “xq is too amq to be l”. [sent-167, score-0.732]
</p><p>56 Notice that the attribute selection may have been different at each iteration, indexed by Then  xq  mq. [sent-168, score-0.522]
</p><p>57 1  •  where nq (x) is 0 if: lwas not the predicted label for  xq at iteration  q OR  666444446  l was the predicted label but correctly so (i. [sent-172, score-0.432]
</p><p>58 no attributes-based feedback was provided) OR • x does not have more amq than xq i. [sent-174, score-0.479]
</p><p>59 nq (x) can also be defined to be the difference in attribute scores between the two images i. [sent-178, score-0.38]
</p><p>60 However, since the attribute predictors are trained as ranking functions and not regressors, the difference in scores may be less meaningful. [sent-181, score-0.461]
</p><p>61 The weights of the images that have been labeled by the supervisor via labeled-based feedback are always set to the maximum, which is 1. [sent-183, score-1.038]
</p><p>62 Learning Attribute Models On The Fly We now describe our approach to learning the attribute  models on the fly as opposed to using pre-trained attribute predictors as in [17]. [sent-187, score-0.869]
</p><p>63 Recall that an attribute predictor rm for an attribute am is learnt by using human annotated pairs of images Om = {(xi, xj)} such that (xi , xj) ∈ Om =⇒ xi ? [sent-188, score-0.809]
</p><p>64 We use the following approach to learn attribute predictors on the fly. [sent-192, score-0.386]
</p><p>65 At any iteration, if the supervisor says “xq is too am to be l”, then the learner fetches all images labeled as l, and appends Om with additional constraints Oˆm = Om ∪ {(xq, xj)} s. [sent-193, score-1.107]
</p><p>66 For instance, if the supervisor says, “this query image is too open to be a forest”, then the learner can fetch all images thus far labeled as forests and realize that all these forest images must be less open than the query image. [sent-196, score-1.63]
</p><p>67 Similarly, if the supervisor says “xq is not am enough to be l”, then Oˆm = Om ∪ {(xj , xq)} s. [sent-197, score-0.759]
</p><p>68 We make some notes: 1) If an image in the future is labeled by the supervisor to be forest, Om will be appended accordingly. [sent-200, score-0.771]
</p><p>69 2) As the attribute models are updated at each iteration the weights described in Section 3. [sent-201, score-0.37]
</p><p>70 Learning attributes on the fly gives the supervisor flexibility to use whichever attribute he deems fit, and not be severely restricted by a pre-determined vocabulary of attributes. [sent-205, score-1.38]
</p><p>71 The supervisor can introduce a new attribute at  2When x is just a bit more open than xq and there are many images similar to each other that all fall between x and xq, the difference in scores may be more reliable. [sent-206, score-1.314]
</p><p>72 Moreover, the form of the attributes-feedback conveniently matches the supervision required by a learning to rank formulation to train relative attribute models, allowing us to use it simultaneously as feedback for training classifiers and as annotation for training relative attribute models. [sent-210, score-0.993]
</p><p>73 The traditional criterion of picking the image with the most entropy of its class distribution pk (also used in [17]) does not incorporate our relative attributes-based feedback setup. [sent-215, score-0.428]
</p><p>74 Our contribution is to account for the feedback while efficiently computing the expected reduction in entropy of the system when a query image is labeled. [sent-216, score-0.48]
</p><p>75 Let’s say given a rejection response, the chances ofthe supervisor picking any of the M attributes with the “too” response is p1m+ and with the “not enough” response is p1m− . [sent-231, score-0.964]
</p><p>76 We estimate p0, the probability that the classifier’s predicted label l for xi is correct and hence accepted by the supervisor to be pl (xi). [sent-240, score-0.883]
</p><p>77 To estimate p1m+, the probability that the supervisor will provide feedback “xi is too am to be l”, we use the following  intuition. [sent-242, score-0.953]
</p><p>78 More specifically, say we sort all the images by their attribute values rm (x) in ascending order. [sent-246, score-0.451]
</p><p>79 When the label is rejected, the supervisor provides attributes-based feedback, which is transferred to many images as negative labels. [sent-264, score-0.784]
</p><p>80 However, note that when the supervisor provides this feedback, the relative attribute model rm is also updated, and so the only way to determine the set {xi? [sent-273, score-1.129]
</p><p>81 Anytime the supervisor gives the learner feedback of the form “xi is too am to be l”, the added constraints that are obtained to update rm are of the form (xi ? [sent-284, score-1.267]
</p><p>82 Unlabeled images are clustered along each relative attribute using boundaries marked by labeled images of each class. [sent-331, score-0.454]
</p><p>83 Collecting Attributes-based Feedback To allow for extensive quantitative evaluations while still using feedback from real users, we collect exhaustive attributes-based feedback offline using human subjects on Amazon Mechanical Turk (MTurk) (as in [17]). [sent-354, score-0.52]
</p><p>84 Note that the supervisor provides feedback every time a query image 666444668  ×  is mis-classified, and this feedback depends on the query image and the predicted label. [sent-355, score-1.544]
</p><p>85 To restrict the amount of data to be collected, we make the simplifying assumption that the feedback depends only on the true label of the query image and the predicted label. [sent-357, score-0.474]
</p><p>86 5 The attribute with most people agreeing on one category having a stronger presence of the attribute than the other (and fewest people saying the opposite) is the attribute corresponding to the most obvious difference between the two categories. [sent-363, score-0.948]
</p><p>87 Note that the list of attributes simulates the vocabulary of attributes that the system would end up with at the end of the learning process. [sent-364, score-0.442]
</p><p>88 We note that our attribute annotations for the PubFig-900-60 dataset is the largest relative attribute dataset to the best of our knowledge and is publicly available on our webpage. [sent-369, score-0.665]
</p><p>89 The training set (usually 65-75% of the total dataset) is used as the unlabeled data that will be labeled by the supervisor with label- and attributes-based feedback over the course of the learning iterations. [sent-373, score-1.135]
</p><p>90 In Table 1 we list the various algorithms we eval4We assume that the supervisor is likely to comment on the attribute that makes the predicted category of an image most different from its true category, allowing us to simplify the question posed to MTurk workers. [sent-376, score-1.103]
</p><p>91 5For PubFig-772-8 we used a list of the 11 attributes used in [16, 17] and for PubFig-900-60 we used a list of 29 attributes capturing the same concepts as those of Kumar et al. [sent-377, score-0.404]
</p><p>92 The different comparisons allow us to evaluate the role of attributes-based feedback, of the proposed weight-  ing scheme, of the on-the-fly attribute models and of the proposed query image selection approach as compared to the traditional max-entropy active selection approach. [sent-382, score-0.554]
</p><p>93 2  Attribute Models Trained On The Fly  Figure 2 also demonstrates the impact of learning the attribute models on the fly (black vs. [sent-389, score-0.483]
</p><p>94 While the motivation behind learning the attribute models on the fly was added flexibility and reduced over-head of pre-training  the attributes, surprisingly, we found that they also improve accuracies significantly. [sent-391, score-0.509]
</p><p>95 Our analysis revealed that the attribute models learnt on the fly are biased towards adding fewer but more informative negative examples to the classifiers as compared to the pre-trained attribute models. [sent-392, score-0.817]
</p><p>96 Further, we evaluated the accuracy of the attribute models at predicting the relative attribute strength in pairs of images. [sent-400, score-0.698]
</p><p>97 While being worse attribute predictors, the models trained on the fly are better catered towards providing classifier-feedback. [sent-402, score-0.496]
</p><p>98 We suspect this to be the case due to the nature of our application where the supervisor only accepts or rejects a label for the query image, as opposed to providing the correct label as in traditional labeling tasks. [sent-411, score-1.005]
</p><p>99 This set-up mimics the scenario where the supervisor can truly use any attribute on the fly. [sent-430, score-1.022]
</p><p>100 We also learn attribute models on the fly, which not only provides increased flexibility to the supervisor with less overhead of pre-training attribute predictors, but also leads to significant improvements in classifier performance. [sent-452, score-1.388]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('supervisor', 0.706), ('attribute', 0.316), ('feedback', 0.247), ('learner', 0.24), ('xq', 0.206), ('attributes', 0.163), ('query', 0.151), ('fly', 0.14), ('unlabeled', 0.09), ('active', 0.087), ('xi', 0.083), ('forest', 0.082), ('rm', 0.074), ('predictors', 0.07), ('open', 0.066), ('labeled', 0.065), ('rmq', 0.064), ('entropy', 0.064), ('xj', 0.062), ('ranking', 0.057), ('parkash', 0.055), ('says', 0.053), ('kirabo', 0.051), ('parikh', 0.05), ('pk', 0.048), ('conveys', 0.045), ('nq', 0.044), ('predicted', 0.042), ('om', 0.042), ('say', 0.041), ('rejects', 0.04), ('list', 0.039), ('explanation', 0.039), ('weighting', 0.039), ('niche', 0.038), ('shoes', 0.038), ('wql', 0.038), ('actively', 0.037), ('forests', 0.037), ('access', 0.034), ('pool', 0.034), ('supervisors', 0.034), ('label', 0.034), ('rejected', 0.034), ('strength', 0.033), ('relative', 0.033), ('vocabulary', 0.032), ('tl', 0.031), ('classifier', 0.031), ('iterations', 0.03), ('iteration', 0.03), ('ranker', 0.03), ('resultant', 0.028), ('accounts', 0.027), ('learning', 0.027), ('collect', 0.026), ('mturk', 0.026), ('biswas', 0.026), ('amq', 0.026), ('attributesbased', 0.026), ('fetch', 0.026), ('openness', 0.026), ('wmtxi', 0.026), ('accuracies', 0.026), ('workers', 0.025), ('brute', 0.025), ('scheme', 0.025), ('updated', 0.024), ('negative', 0.024), ('species', 0.023), ('kovashka', 0.023), ('whichever', 0.023), ('fetches', 0.023), ('providing', 0.022), ('think', 0.022), ('passive', 0.022), ('classifiers', 0.021), ('consecutively', 0.021), ('communicates', 0.021), ('identifies', 0.021), ('incorrect', 0.021), ('faces', 0.021), ('belief', 0.02), ('intelligently', 0.02), ('entropies', 0.02), ('images', 0.02), ('overhead', 0.019), ('eliminated', 0.019), ('interface', 0.019), ('criterion', 0.018), ('collecting', 0.018), ('confirms', 0.018), ('expertise', 0.018), ('categories', 0.018), ('response', 0.018), ('correct', 0.018), ('trained', 0.018), ('system', 0.018), ('picking', 0.018), ('gist', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="396-tfidf-1" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>Author: Arijit Biswas, Devi Parikh</p><p>Abstract: Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. “I think this is a forest, what do you think?”. If the learner is wrong, the supervisorprovides an explanation e.g. “No, this is too open to be a forest”. With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work, we propose three improvements over this set-up. First, we incorporate a weighting scheme that instead of making a hard decision reasons about the likelihood of an image being a negative example. Second, we do away with pre-trained attributes and instead learn the attribute models on the fly, alleviating overhead and restrictions of a pre-determined attribute vocabulary. Finally, we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. We demonstrate significant improvement in classification accuracy on faces and shoes. We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories.</p><p>2 0.24896778 <a title="396-tfidf-2" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>Author: Jonghyun Choi, Mohammad Rastegari, Ali Farhadi, Larry S. Davis</p><p>Abstract: We propose a method to expand the visual coverage of training sets that consist of a small number of labeled examples using learned attributes. Our optimization formulation discovers category specific attributes as well as the images that have high confidence in terms of the attributes. In addition, we propose a method to stably capture example-specific attributes for a small sized training set. Our method adds images to a category from a large unlabeled image pool, and leads to significant improvement in category recognition accuracy evaluated on a large-scale dataset, ImageNet.</p><p>3 0.24527211 <a title="396-tfidf-3" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>4 0.20056583 <a title="396-tfidf-4" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>Author: Amir Sadovnik, Andrew Gallagher, Tsuhan Chen</p><p>Abstract: Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition. Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human. Since many attributes are nameable, the computer is able to communicate these concepts through language. However, this is not a trivial task. Given a set of attributes, selecting a subset to be communicated is task dependent. Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty. We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others. We introduce an efficient, principled methodfor choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers. We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions.</p><p>5 0.18538651 <a title="396-tfidf-5" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>Author: Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</p><p>Abstract: Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently, and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query, which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions without explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discrimina- tive binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improveperformance significantly over several strong baselines, while being an order of magnitude faster than exhaustively searching through all possible conjunctions.</p><p>6 0.17615853 <a title="396-tfidf-6" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>7 0.17524554 <a title="396-tfidf-7" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>8 0.15909271 <a title="396-tfidf-8" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>9 0.15176757 <a title="396-tfidf-9" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>10 0.14790499 <a title="396-tfidf-10" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>11 0.14700457 <a title="396-tfidf-11" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>12 0.14281407 <a title="396-tfidf-12" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>13 0.13907652 <a title="396-tfidf-13" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>14 0.12688634 <a title="396-tfidf-14" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>15 0.12537092 <a title="396-tfidf-15" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>16 0.099483207 <a title="396-tfidf-16" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>17 0.089981601 <a title="396-tfidf-17" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>18 0.089228727 <a title="396-tfidf-18" href="./cvpr-2013-POOF%3A_Part-Based_One-vs.-One_Features_for_Fine-Grained_Categorization%2C_Face_Verification%2C_and_Attribute_Estimation.html">323 cvpr-2013-POOF: Part-Based One-vs.-One Features for Fine-Grained Categorization, Face Verification, and Attribute Estimation</a></p>
<p>19 0.085729077 <a title="396-tfidf-19" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>20 0.080538407 <a title="396-tfidf-20" href="./cvpr-2013-Scene_Coordinate_Regression_Forests_for_Camera_Relocalization_in_RGB-D_Images.html">380 cvpr-2013-Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, -0.12), (2, -0.04), (3, -0.023), (4, 0.131), (5, 0.104), (6, -0.264), (7, 0.03), (8, 0.073), (9, 0.166), (10, -0.048), (11, 0.105), (12, -0.034), (13, 0.026), (14, 0.022), (15, -0.017), (16, -0.038), (17, -0.073), (18, -0.027), (19, 0.014), (20, 0.019), (21, -0.012), (22, -0.045), (23, 0.063), (24, 0.006), (25, -0.006), (26, -0.022), (27, 0.03), (28, 0.019), (29, 0.057), (30, 0.023), (31, -0.02), (32, -0.02), (33, 0.007), (34, -0.011), (35, 0.038), (36, -0.018), (37, -0.062), (38, -0.032), (39, -0.006), (40, -0.036), (41, 0.01), (42, 0.07), (43, 0.033), (44, -0.005), (45, 0.049), (46, 0.001), (47, 0.003), (48, 0.048), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96006525 <a title="396-lsi-1" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>Author: Arijit Biswas, Devi Parikh</p><p>Abstract: Active learning provides useful tools to reduce annotation costs without compromising classifier performance. However it traditionally views the supervisor simply as a labeling machine. Recently a new interactive learning paradigm was introduced that allows the supervisor to additionally convey useful domain knowledge using attributes. The learner first conveys its belief about an actively chosen image e.g. “I think this is a forest, what do you think?”. If the learner is wrong, the supervisorprovides an explanation e.g. “No, this is too open to be a forest”. With access to a pre-trained set of relative attribute predictors, the learner fetches all unlabeled images more open than the query image, and uses them as negative examples of forests to update its classifier. This rich human-machine communication leads to better classification performance. In this work, we propose three improvements over this set-up. First, we incorporate a weighting scheme that instead of making a hard decision reasons about the likelihood of an image being a negative example. Second, we do away with pre-trained attributes and instead learn the attribute models on the fly, alleviating overhead and restrictions of a pre-determined attribute vocabulary. Finally, we propose an active learning framework that accounts for not just the label- but also the attributes-based feedback while selecting the next query image. We demonstrate significant improvement in classification accuracy on faces and shoes. We also collect and make available the largest relative attributes dataset containing 29 attributes of faces from 60 categories.</p><p>2 0.87840402 <a title="396-lsi-2" href="./cvpr-2013-Designing_Category-Level_Attributes_for_Discriminative_Visual_Recognition.html">116 cvpr-2013-Designing Category-Level Attributes for Discriminative Visual Recognition</a></p>
<p>Author: Felix X. Yu, Liangliang Cao, Rogerio S. Feris, John R. Smith, Shih-Fu Chang</p><p>Abstract: Attribute-based representation has shown great promises for visual recognition due to its intuitive interpretation and cross-category generalization property. However, human efforts are usually involved in the attribute designing process, making the representation costly to obtain. In this paper, we propose a novel formulation to automatically design discriminative “category-level attributes ”, which can be efficiently encoded by a compact category-attribute matrix. The formulation allows us to achieve intuitive and critical design criteria (category-separability, learnability) in a principled way. The designed attributes can be used for tasks of cross-category knowledge transfer, achieving superior performance over well-known attribute dataset Animals with Attributes (AwA) and a large-scale ILSVRC2010 dataset (1.2M images). This approach also leads to state-ofthe-art performance on the zero-shot learning task on AwA.</p><p>3 0.87261873 <a title="396-lsi-3" href="./cvpr-2013-Multi-attribute_Queries%3A_To_Merge_or_Not_to_Merge%3F.html">293 cvpr-2013-Multi-attribute Queries: To Merge or Not to Merge?</a></p>
<p>Author: Mohammad Rastegari, Ali Diba, Devi Parikh, Ali Farhadi</p><p>Abstract: Users often have very specific visual content in mind that they are searching for. The most natural way to communicate this content to an image search engine is to use keywords that specify various properties or attributes of the content. A naive way of dealing with such multi-attribute queries is the following: train a classifier for each attribute independently, and then combine their scores on images to judge their fit to the query. We argue that this may not be the most effective or efficient approach. Conjunctions of attribute often correspond to very characteristic appearances. It would thus be beneficial to train classifiers that detect these conjunctions as a whole. But not all conjunctions result in such tight appearance clusters. So given a multi-attribute query, which conjunctions should we model? An exhaustive evaluation of all possible conjunctions would be time consuming. Hence we propose an optimization approach that identifies beneficial conjunctions without explicitly training the corresponding classifier. It reasons about geometric quantities that capture notions similar to intra- and inter-class variances. We exploit a discrimina- tive binary space to compute these geometric quantities efficiently. Experimental results on two challenging datasets of objects and birds show that our proposed approach can improveperformance significantly over several strong baselines, while being an order of magnitude faster than exhaustively searching through all possible conjunctions.</p><p>4 0.86028135 <a title="396-lsi-4" href="./cvpr-2013-Label-Embedding_for_Attribute-Based_Classification.html">241 cvpr-2013-Label-Embedding for Attribute-Based Classification</a></p>
<p>Author: Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid</p><p>Abstract: Attributes are an intermediate representation, which enables parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function which measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. The label embedding framework offers other advantages such as the ability to leverage alternative sources of information in addition to attributes (e.g. class hierarchies) or to transition smoothly from zero-shot learning to learning with large quantities of data.</p><p>5 0.85952288 <a title="396-lsi-5" href="./cvpr-2013-Attribute-Based_Detection_of_Unfamiliar_Classes_with_Humans_in_the_Loop.html">48 cvpr-2013-Attribute-Based Detection of Unfamiliar Classes with Humans in the Loop</a></p>
<p>Author: Catherine Wah, Serge Belongie</p><p>Abstract: Recent work in computer vision has addressed zero-shot learning or unseen class detection, which involves categorizing objects without observing any training examples. However, these problems assume that attributes or defining characteristics of these unobserved classes are known, leveraging this information at test time to detect an unseen class. We address the more realistic problem of detecting categories that do not appear in the dataset in any form. We denote such a category as an unfamiliar class; it is neither observed at train time, nor do we possess any knowledge regarding its relationships to attributes. This problem is one that has received limited attention within the computer vision community. In this work, we propose a novel ap. ucs d .edu Unfamiliar? or?not? UERY?IMAGQ IMmFaAtgMechs?inIlLatsrA?inYRESg MFNaAotc?ihntIlraLsin?A YRgES UMNaotFc?hAinMltarsIinL?NIgAOR AKNTAWDNO ?Train g?imagesn U(se)alc?n)eSs(Long?bilCas n?a’t lrfyibuteIn?mfoartesixNearwter proach to the unfamiliar class detection task that builds on attribute-based classification methods, and we empirically demonstrate how classification accuracy is impacted by attribute noise and dataset “difficulty,” as quantified by the separation of classes in the attribute space. We also present a method for incorporating human users to overcome deficiencies in attribute detection. We demonstrate results superior to existing methods on the challenging CUB-200-2011 dataset.</p><p>6 0.84383023 <a title="396-lsi-6" href="./cvpr-2013-Object-Centric_Anomaly_Detection_by_Attribute-Based_Reasoning.html">310 cvpr-2013-Object-Centric Anomaly Detection by Attribute-Based Reasoning</a></p>
<p>7 0.83259892 <a title="396-lsi-7" href="./cvpr-2013-It%27s_Not_Polite_to_Point%3A_Describing_People_with_Uncertain_Attributes.html">229 cvpr-2013-It's Not Polite to Point: Describing People with Uncertain Attributes</a></p>
<p>8 0.80303192 <a title="396-lsi-8" href="./cvpr-2013-Adding_Unlabeled_Samples_to_Categories_by_Learned_Attributes.html">36 cvpr-2013-Adding Unlabeled Samples to Categories by Learned Attributes</a></p>
<p>9 0.72356218 <a title="396-lsi-9" href="./cvpr-2013-Weakly_Supervised_Learning_for_Attribute_Localization_in_Outdoor_Scenes.html">461 cvpr-2013-Weakly Supervised Learning for Attribute Localization in Outdoor Scenes</a></p>
<p>10 0.67892641 <a title="396-lsi-10" href="./cvpr-2013-Cumulative_Attribute_Space_for_Age_and_Crowd_Density_Estimation.html">101 cvpr-2013-Cumulative Attribute Space for Age and Crowd Density Estimation</a></p>
<p>11 0.67673165 <a title="396-lsi-11" href="./cvpr-2013-Cross-View_Image_Geolocalization.html">99 cvpr-2013-Cross-View Image Geolocalization</a></p>
<p>12 0.63514775 <a title="396-lsi-12" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>13 0.63107955 <a title="396-lsi-13" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>14 0.61640608 <a title="396-lsi-14" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>15 0.49875963 <a title="396-lsi-15" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>16 0.47215515 <a title="396-lsi-16" href="./cvpr-2013-Weakly_Supervised_Learning_of_Mid-Level_Features_with_Beta-Bernoulli_Process_Restricted_Boltzmann_Machines.html">462 cvpr-2013-Weakly Supervised Learning of Mid-Level Features with Beta-Bernoulli Process Restricted Boltzmann Machines</a></p>
<p>17 0.46400428 <a title="396-lsi-17" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<p>18 0.43795949 <a title="396-lsi-18" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>19 0.43605363 <a title="396-lsi-19" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>20 0.43153343 <a title="396-lsi-20" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.084), (16, 0.013), (26, 0.04), (33, 0.24), (67, 0.055), (69, 0.03), (72, 0.012), (77, 0.011), (87, 0.403)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94299591 <a title="396-lda-1" href="./cvpr-2013-Lost%21_Leveraging_the_Crowd_for_Probabilistic_Visual_Self-Localization.html">274 cvpr-2013-Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization</a></p>
<p>Author: Marcus A. Brubaker, Andreas Geiger, Raquel Urtasun</p><p>Abstract: In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.</p><p>2 0.93426883 <a title="396-lda-2" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>Author: Christian Häne, Christopher Zach, Andrea Cohen, Roland Angst, Marc Pollefeys</p><p>Abstract: Both image segmentation and dense 3D modeling from images represent an intrinsically ill-posed problem. Strong regularizers are therefore required to constrain the solutions from being ’too noisy’. Unfortunately, these priors generally yield overly smooth reconstructions and/or segmentations in certain regions whereas they fail in other areas to constrain the solution sufficiently. In this paper we argue that image segmentation and dense 3D reconstruction contribute valuable information to each other’s task. As a consequence, we propose a rigorous mathematical framework to formulate and solve a joint segmentation and dense reconstruction problem. Image segmentations provide geometric cues about which surface orientations are more likely to appear at a certain location in space whereas a dense 3D reconstruction yields a suitable regularization for the segmentation problem by lifting the labeling from 2D images to 3D space. We show how appearance-based cues and 3D surface orientation priors can be learned from training data and subsequently used for class-specific regularization. Experimental results on several real data sets highlight the advantages of our joint formulation.</p><p>3 0.92866009 <a title="396-lda-3" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>Author: Eno Töppe, Claudia Nieuwenhuis, Daniel Cremers</p><p>Abstract: We introduce the concept of relative volume constraints in order to account for insufficient information in the reconstruction of 3D objects from a single image. The key idea is to formulate a variational reconstruction approach with shape priors in form of relative depth profiles or volume ratios relating object parts. Such shape priors can easily be derived either from a user sketch or from the object’s shading profile in the image. They can handle textured or shadowed object regions by propagating information. We propose a convex relaxation of the constrained optimization problem which can be solved optimally in a few seconds on graphics hardware. In contrast to existing single view reconstruction algorithms, the proposed algorithm provides substantially more flexibility to recover shape details such as self-occlusions, dents and holes, which are not visible in the object silhouette.</p><p>4 0.90833795 <a title="396-lda-4" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>Author: Martin Hofmann, Daniel Wolf, Gerhard Rigoll</p><p>Abstract: We generalize the network flow formulation for multiobject tracking to multi-camera setups. In the past, reconstruction of multi-camera data was done as a separate extension. In this work, we present a combined maximum a posteriori (MAP) formulation, which jointly models multicamera reconstruction as well as global temporal data association. A flow graph is constructed, which tracks objects in 3D world space. The multi-camera reconstruction can be efficiently incorporated as additional constraints on the flow graph without making the graph unnecessarily large. The final graph is efficiently solved using binary linear programming. On the PETS 2009 dataset we achieve results that significantly exceed the current state of the art.</p><p>5 0.89828753 <a title="396-lda-5" href="./cvpr-2013-Dictionary_Learning_from_Ambiguously_Labeled_Data.html">125 cvpr-2013-Dictionary Learning from Ambiguously Labeled Data</a></p>
<p>Author: Yi-Chen Chen, Vishal M. Patel, Jaishanker K. Pillai, Rama Chellappa, P. Jonathon Phillips</p><p>Abstract: We propose a novel dictionary-based learning method for ambiguously labeled multiclass classification, where each training sample has multiple labels and only one of them is the correct label. The dictionary learning problem is solved using an iterative alternating algorithm. At each iteration of the algorithm, two alternating steps are performed: a confidence update and a dictionary update. The confidence of each sample is defined as the probability distribution on its ambiguous labels. The dictionaries are updated using either soft (EM-based) or hard decision rules. Extensive evaluations on existing datasets demonstrate that the proposed method performs significantly better than state-of-the-art ambiguously labeled learning approaches.</p><p>6 0.88748729 <a title="396-lda-6" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>7 0.88441646 <a title="396-lda-7" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>8 0.87943208 <a title="396-lda-8" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>9 0.85708106 <a title="396-lda-9" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>same-paper 10 0.83251357 <a title="396-lda-10" href="./cvpr-2013-Simultaneous_Active_Learning_of_Classifiers_%26_Attributes_via_Relative_Feedback.html">396 cvpr-2013-Simultaneous Active Learning of Classifiers & Attributes via Relative Feedback</a></p>
<p>11 0.8185662 <a title="396-lda-11" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>12 0.78665167 <a title="396-lda-12" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>13 0.77605891 <a title="396-lda-13" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>14 0.76579475 <a title="396-lda-14" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>15 0.75755966 <a title="396-lda-15" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>16 0.75265133 <a title="396-lda-16" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>17 0.75261611 <a title="396-lda-17" href="./cvpr-2013-SWIGS%3A_A_Swift_Guided_Sampling_Method.html">373 cvpr-2013-SWIGS: A Swift Guided Sampling Method</a></p>
<p>18 0.75122344 <a title="396-lda-18" href="./cvpr-2013-Manhattan_Scene_Understanding_via_XSlit_Imaging.html">279 cvpr-2013-Manhattan Scene Understanding via XSlit Imaging</a></p>
<p>19 0.7480129 <a title="396-lda-19" href="./cvpr-2013-Wide-Baseline_Hair_Capture_Using_Strand-Based_Refinement.html">467 cvpr-2013-Wide-Baseline Hair Capture Using Strand-Based Refinement</a></p>
<p>20 0.74799597 <a title="396-lda-20" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
