<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>219 cvpr-2013-In Defense of 3D-Label Stereo</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-219" href="#">cvpr2013-219</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>219 cvpr-2013-In Defense of 3D-Label Stereo</h1>
<br/><p>Source: <a title="cvpr-2013-219-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Olsson_In_Defense_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Carl Olsson, Johannes Ulén, Yuri Boykov</p><p>Abstract: It is commonly believed that higher order smoothness should be modeled using higher order interactions. For example, 2nd order derivatives for deformable (active) contours are represented by triple cliques. Similarly, the 2nd order regularization methods in stereo predominantly use MRF models with scalar (1D) disparity labels and triple clique interactions. In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels, e.g. tangent planes. This general paradigm has been criticized due to perceived computational complexity of optimization in higher-dimensional label space. Contrary to popular beliefs, we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization problems with (nearly) submodular pairwise interactions. Our theoretical and experimental re- sults demonstrate advantages over state-of-the-art methods for 2nd order smoothness stereo. 1</p><p>Reference: <a title="cvpr-2013-219-reference" href="../cvpr2013_reference/cvpr-2013-In_Defense_of_3D-Label_Stereo_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 s e  Abstract It is commonly believed that higher order smoothness should be modeled using higher order interactions. [sent-5, score-0.16]
</p><p>2 For example, 2nd order derivatives for deformable (active) contours are represented by triple cliques. [sent-6, score-0.161]
</p><p>3 Similarly, the 2nd order regularization methods in stereo predominantly use MRF models with scalar (1D) disparity labels and triple clique interactions. [sent-7, score-0.557]
</p><p>4 In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels, e. [sent-8, score-0.399]
</p><p>5 Contrary to popular beliefs, we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization problems with (nearly) submodular pairwise interactions. [sent-12, score-0.429]
</p><p>6 One reason for their popularity is that when applying movemaking algorithms such as α-expansion [4] or fusion moves [8] they often result in submodular moves, allowing efficient computation using min-cut/max-flow algorithms [4]. [sent-18, score-0.333]
</p><p>7 Many basic optimization methods for stereo use scalar (1D) disparity labels. [sent-19, score-0.369]
</p><p>8 ca  regularization potentials assign higher cost to surfaces with larger tilt [4]. [sent-27, score-0.165]
</p><p>9 To address more general scenes our paper follows the popular trend of using 2nd derivative surface regularization for stereo [9, 15]. [sent-29, score-0.308]
</p><p>10 [15] retain the scalar disparity labels while using triple-cliques to penalize 2nd derivatives of the reconstructed surface. [sent-32, score-0.384]
</p><p>11 In contrast, Li and Zucker [9] use 3D-labels corresponding to tangent planes to encode 2nd order disparity map smoothness as pairwise interactions. [sent-35, score-0.776]
</p><p>12 The first term computes the difference of the disparity assignment, and the disparity predicted by the neighboring tangent plane. [sent-38, score-0.817]
</p><p>13 ) The second term penalizes the (squared) angular difference between neighboring tangent plane normals. [sent-40, score-0.459]
</p><p>14 [15] that only use a disparity estimate at each pixel, the approach by Li and Zucker [9] requires discretization of a much larger label space. [sent-44, score-0.243]
</p><p>15 As shown in [15], this specific approach results in disparity maps that are inferior to those of Woodford et al. [sent-46, score-0.243]
</p><p>16 The discussed limitations of [9] may have helped to promote the general perception of triple interactions of scalar disparity labels as a superior approach for modeling 2nd order smoothness. [sent-47, score-0.443]
</p><p>17 This makes it possible to precompute and penalize 2nd order smoothness via 111777223088  a unary term. [sent-50, score-0.203]
</p><p>18 There is however no smoothness interaction between models and therefore it is not possible to combine local models into global ones. [sent-51, score-0.15]
</p><p>19 In this paper we propose a new 3D-label stereo algorithm encoding 2nd order smoothness of the disparity map with pairwise interactions. [sent-53, score-0.529]
</p><p>20 We show how to properly measure 2nd derivative of the reconstructed  surface using pairwise cliques when the labels are tangent planes. [sent-55, score-0.498]
</p><p>21 Instead of using a fixed set of locally precomputed tangents, we adaptively generate new surface proposals based on the current surface estimate. [sent-56, score-0.316]
</p><p>22 [15] we show that our formulation is submodular when using planar proposals, and verify experimentally that Roof duality [11] labels much more pixels for general proposals with our formulation. [sent-59, score-0.378]
</p><p>23 We show that the use of even higher order labels (that encode higher order derivatives) further extends the class of submodular functions. [sent-61, score-0.222]
</p><p>24 In addition we present a version of our method that works with depth rather than disparity and, therefore, does not require rectified cameras. [sent-62, score-0.43]
</p><p>25 The possibility to decrease the energy for each fusion move is an attractive feature, however there is no guarantee on how many variables will be labeled in each fusion move. [sent-91, score-0.317]
</p><p>26 For submodular fusion moves we are guaranteed to label all variables. [sent-95, score-0.333]
</p><p>27 A Second  Order  Multi-View Stereo  Smoothness  Prior  for  In this section we present a second order smoothness prior for dense multi-view stereo reconstruction. [sent-98, score-0.223]
</p><p>28 To each viewing ray we will assign a plane that locally represents the surface geometry close to the ray. [sent-101, score-0.334]
</p><p>29 The intersection between the ray and the plane will be the estimated 3D point. [sent-102, score-0.165]
</p><p>30 By interpreting the planes as a tangents of the viewed 3D surface we can encourage smooth solutions by penalizing neighboring 3D-points that deviate largely from neighboring tangents. [sent-103, score-0.485]
</p><p>31 Rectified Cameras and Disparity Maps We will start by assuming that the cameras have been rectified, since this allows us to work in disparity space. [sent-106, score-0.279]
</p><p>32 gT pheix goal no tfh hsete irmeoa gise t Io estimate the function D : I R that gives a disparity value → feostri emaacthe pixel ninc ttihoen image. [sent-111, score-0.264]
</p><p>33 →To R Re tahchat pixel p we wariillty assign a tangent plane that locally approximates this function. [sent-112, score-0.401]
</p><p>34 We can think of these tangents as samples of the disparity function and its derivatives. [sent-113, score-0.368]
</p><p>35 By the function TpD : I R we → twioilnl mean tsh dee tangent sa. [sent-114, score-0.243]
</p><p>36 o→n o Rf wthee whole image, that is  TpD (x) = D (p) + ∇D  (p)T (x − p),  (4)  where D (p) and ∇D (p) is the assigned disparity and disparity gradient (dw ∇ithD respect htoe athssei image csopoarridtyina anted system) at pixel p. [sent-116, score-0.507]
</p><p>37 We define a pairwise interaction between neighboring pixels as Vpq = |TpD (q) − D (q) |,  (5)  That is, Vpq measures the curve’s deviation from the tangent plane, see Figure 1. [sent-117, score-0.384]
</p><p>38 Intuitively, if the surface is smooth then 111777223 919  for parallel viewing rays. [sent-118, score-0.194]
</p><p>39 (7)  That is, Vpq measures the second derivative at p in direction q p of the underlying disparity function. [sent-123, score-0.332]
</p><p>40 However directly penalizing 2nd derivative of the depth function is not a good idea. [sent-128, score-0.228]
</p><p>41 In general the projection of a plane will not yield a linear depth function unless the camera is affine (which can be seen from (11) below). [sent-129, score-0.213]
</p><p>42 Therefore we will instead measure the deviation from the tangent plane along the viewing ray. [sent-131, score-0.417]
</p><p>43 = 1 and ap ∈ R we denote the tangent plane ∈at p given by =the 1 equation npTx + ap = 0. [sent-138, score-0.349]
</p><p>44 between the viewing ray at q and the tangent plane at p. [sent-140, score-0.476]
</p><p>45 We let Tpd : I R+ → abet qth aen depth tfaunncgetinotn olfa nthee a tangent plane atd point p, t Rhat is, Q? [sent-141, score-0.456]
</p><p>46 We can calculate the tangent function using  Tpd(q) = −npTapqh. [sent-143, score-0.243]
</p><p>47 (11)  (Here we are assuming that the viewing ray is not completely contained in the tangent plane. [sent-144, score-0.37]
</p><p>48 In contrast disparity is inversely proportional to depth and will therefore be linear. [sent-146, score-0.35]
</p><p>49 Given the estimated tangent plane at p and the depth at q the interaction computes the distance between the estimated 3D point and the tangent plane along the view-  ing ray. [sent-154, score-0.853]
</p><p>50 The smoothness term will penalize deviations from planes and thereby encourage solutions with small second derivatives. [sent-155, score-0.29]
</p><p>51 Submodularity of Fusion Moves In this section we will show that fusion moves [8] with our interactions are often submodular. [sent-158, score-0.226]
</p><p>52 Given a current disparity function D and a proposal function P the fusion move a flluonwcsti pixels aton change tphoesiarl l a fbuenlcst ifornom P Pth teh tangents of D to the tangents of P. [sent-159, score-0.705]
</p><p>53 In what follows we will use Vpq(D, P) = |TpD (q) − P (q) | ,  (13)  to mean the penalty for assigning p the tangent plane from D and q the tangent plane from P. [sent-160, score-0.723]
</p><p>54 Candidate Planes We first show that fusion moves where the candidate function P is a plane result in submodular terms. [sent-165, score-0.439]
</p><p>55 1 If the proposal P is a plane then the fusion  wPirothp any function Df t hise a rsoupbomsaold Pula isr move. [sent-168, score-0.293]
</p><p>56 General Candidates Next we derive some more general sufficient conditions for submodularity of the fusion move. [sent-173, score-0.183]
</p><p>57 2ca Ivef) b botehtw Deen a p a Pnd a q eth ceonn vtehxe (inorte raaltcetironnasVpq and Vqp are submodular for the fusion move. [sent-176, score-0.263]
</p><p>58 To see this we first note that if both D and P are convex othe sne they are e b foitrhs tb nooutned ethda ftr iofm b obtehlo Dw by dth Peir a tangent planes. [sent-177, score-0.243]
</p><p>59 (26) In the case of a plane proposal P we see that min(Vpq(D, D), t) ≤ (27) min ? [sent-191, score-0.203]
</p><p>60 Epq(D, D) +Epq(P, P) ≤ Epq(D, P) +Epq(P, D), (30) showing that planar proposals also generate submodular interactions with this energy. [sent-201, score-0.391]
</p><p>61 General Order Smoothness Priors In Section 2 we used tangent planes to create our smoothness prior. [sent-210, score-0.441]
</p><p>62 For example, if we to each pixel assign a quadratic function instead of a tangent we have an interaction that penalizes 3rd derivatives. [sent-215, score-0.365]
</p><p>63 1it is easy to see that if our proposals fulfill ApP (q) = P (q) ,  (32)  then the fusion move will be submodular. [sent-218, score-0.301]
</p><p>64 For example if we only use the zero order expansion (constant functions/  fronto-parallel planes) then we find that fusion moves with constant depth proposals are submodular. [sent-220, score-0.482]
</p><p>65 eWriem weniltls use n beoitghh tbhoer disparity v cehrosisoenn (Section 2. [sent-234, score-0.243]
</p><p>66 Depth Depth, 1st derivative Depth, 1st, 2nd derivative . [sent-242, score-0.178]
</p><p>67 Constant functions Constant 1st derivative Constant 2nd derivative . [sent-245, score-0.178]
</p><p>68 Characterization of Pairwise interactions, unary terms and submodular proposals for different types of labels. [sent-248, score-0.336]
</p><p>69 (a) - Image, (b) - depth map using only the data term, (c) - depth map computed with regularization. [sent-251, score-0.258]
</p><p>70 (a)  ×  - Image, (b) - depth map using only the data term, (c) - depth map computed with  The data term Ep is a unary term that depends on the tangent plane at p. [sent-255, score-0.718]
</p><p>71 For each depth we use a planar homography hto\ project one Fofo trh eea neighboring images ainnator thhoem coegnrtearimage. [sent-257, score-0.218]
</p><p>72 In principle we could make the NCC depend on the tilt of the tangent as well, however storing the samples of such a function would require lots of memory. [sent-261, score-0.281]
</p><p>73 We also add an extra cost to assignments of planes which are roughly parallel to the viewing rays. [sent-265, score-0.224]
</p><p>74 We us the extra cost (1 − npTvp)2k, (34) where np is the normal of the plane assigned to p and vp 111777333422  is the direction of the viewing ray in p (in the 3D space the viewing ray direction will be p/ ? [sent-267, score-0.399]
</p><p>75 1  Proposal Generation  To generate proposals we use similar heuristics to those of [15]. [sent-275, score-0.155]
</p><p>76 •  •  To generate planar proposals we randomly select a point annerda a espm laanll neighborhood. [sent-276, score-0.214]
</p><p>77 Using tmhel y be ssetl eloctca al maximum of the normalized cross correlation for each viewing ray we create a 3D cloud to the neighborhood and fit a plane using RANSAC. [sent-277, score-0.262]
</p><p>78 •  We use a filtering process that takes the current assignment, computes gth per corresponding 3 thDe points, aa sndsi gfnoreach pixel fits a plane to its neighboring 3D points. [sent-279, score-0.179]
</p><p>79 Finally we have a proposal that just incFrienaaslelys/decreases the depth/disparity of all proposals with a small random step size. [sent-280, score-0.221]
</p><p>80 For their data sets we computed a depth map for the middle image (nr 4) and used the remaining 6 images to compute the cross correlations needed for the data term. [sent-284, score-0.158]
</p><p>81 The effects of the regularization term can be seen by comparing the surface generated from the data term without regularization (b) and the one with regularization (c) (the data term is particularly weak in the bowling data set because of the large texture less region). [sent-287, score-0.394]
</p><p>82 GlobalStereo also penalize second derivative but use triple cliques with scalar disparity labels. [sent-292, score-0.512]
</p><p>83 The comparison is performed on the Middlebury data set consisting of stereo pairs of rectified images. [sent-294, score-0.172]
</p><p>84 In the fusion moves we use ”improve” [11] after running RD to label all unlabelled variables. [sent-319, score-0.236]
</p><p>85 First we consider the Segpln-proposals which are 14 piecewise planar proposals generated from a segmentation (see [15]). [sent-321, score-0.253]
</p><p>86 In Figure 4 we started from the same randomized disparity function with tangents parallel to the image plane. [sent-322, score-0.389]
</p><p>87 We kept track of how many variables where unlabeled after RD for both methods and presented the numbers in Table 2 and the resulting disparity maps in Figure 4. [sent-324, score-0.269]
</p><p>88 Note that the fusion move for our method is only submodular if we fuse one planar function at a time. [sent-325, score-0.347]
</p><p>89 The SegPln proposals are piecewise planar and the regularization at transitions between planes may not be submodular. [sent-326, score-0.406]
</p><p>90 We also test our regularization on the full pipeline of GlobalStereo which uses all three types of proposals (Segpln, SameUni and Smooth). [sent-327, score-0.212]
</p><p>91 (b-d) are estimated disparity maps after fusing the 14 SegPln proposals. [sent-331, score-0.271]
</p><p>92 In (f-h) we present the unlabelled variables summed over all 14 proposals scaled 0–14. [sent-332, score-0.226]
</p><p>93 TsukubaVenusTeddyConesAverage Non occ  All  Disc  Non occ  All  Disc  Non occ  All  Disc  Non occ  All  Disc  Our4. [sent-334, score-0.236]
</p><p>94 M Tidhed ceblaussreys [ are non goc thcleu sdaemd regions, aalsl , pixels iasn bde regions near depth d%is ocfo nptiixneulsiti beesi. [sent-364, score-0.153]
</p><p>95 Conclusions In this paper we advocated a largely overlooked approach to stereo with 2nd order smoothness regularization. [sent-384, score-0.253]
</p><p>96 In contrast to popular approaches where triple cliques are used for representing 2nd order surface derivatives, we proposed to use pairwise interactions with 3Dlabels. [sent-385, score-0.288]
</p><p>97 We showed that this leads to simpler optimization problems and in many cases (nearly) submodular fusion moves. [sent-386, score-0.286]
</p><p>98 (a) - Image, (b) - depth map using only the data term, (c)  (c) - depth map computed with regularization. [sent-406, score-0.258]
</p><p>99 (a) - Image, (b) - depth  map  using only the data  term,  (c) - depth  map  computed with regularization. [sent-408, score-0.258]
</p><p>100 Global stereo reconstruction under second order smoothness priors. [sent-499, score-0.223]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vpq', 0.624), ('tpd', 0.282), ('tangent', 0.243), ('disparity', 0.243), ('globalstereo', 0.181), ('proposals', 0.155), ('submodular', 0.142), ('epq', 0.132), ('tangents', 0.125), ('fusion', 0.121), ('tpp', 0.121), ('depth', 0.107), ('plane', 0.106), ('smoothness', 0.102), ('segpln', 0.101), ('zucker', 0.101), ('planes', 0.096), ('stereo', 0.092), ('derivative', 0.089), ('rectified', 0.08), ('triple', 0.08), ('woodford', 0.078), ('surface', 0.07), ('moves', 0.07), ('viewing', 0.068), ('proposal', 0.066), ('submodularity', 0.062), ('ncc', 0.062), ('middlebury', 0.06), ('occ', 0.059), ('ray', 0.059), ('planar', 0.059), ('regularization', 0.057), ('disc', 0.053), ('neighboring', 0.052), ('derivatives', 0.052), ('interaction', 0.048), ('non', 0.046), ('bowling', 0.045), ('unlabelled', 0.045), ('teddy', 0.043), ('pairwise', 0.041), ('apd', 0.04), ('maths', 0.04), ('np', 0.039), ('unary', 0.039), ('surfaces', 0.039), ('piecewise', 0.039), ('assignments', 0.039), ('tilt', 0.038), ('cameras', 0.036), ('term', 0.036), ('vqp', 0.036), ('smooth', 0.035), ('interactions', 0.035), ('scalar', 0.034), ('swedish', 0.033), ('penalize', 0.033), ('cliques', 0.033), ('penalizing', 0.032), ('olsson', 0.031), ('canadian', 0.031), ('yuri', 0.031), ('venus', 0.031), ('assign', 0.031), ('min', 0.031), ('overlooked', 0.03), ('tsukuba', 0.03), ('qh', 0.03), ('conf', 0.03), ('cross', 0.029), ('order', 0.029), ('proposition', 0.029), ('fusing', 0.028), ('cones', 0.028), ('cloth', 0.028), ('ionf', 0.028), ('bleyer', 0.027), ('lempitsky', 0.026), ('variables', 0.026), ('move', 0.025), ('regular', 0.025), ('penalty', 0.025), ('energy', 0.024), ('discontinuity', 0.023), ('simpler', 0.023), ('encourage', 0.023), ('labels', 0.022), ('penalizes', 0.022), ('pattern', 0.022), ('map', 0.022), ('rd', 0.022), ('precomputed', 0.021), ('itn', 0.021), ('parallel', 0.021), ('couple', 0.021), ('roof', 0.021), ('pixel', 0.021), ('ph', 0.021), ('rother', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="219-tfidf-1" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>Author: Carl Olsson, Johannes Ulén, Yuri Boykov</p><p>Abstract: It is commonly believed that higher order smoothness should be modeled using higher order interactions. For example, 2nd order derivatives for deformable (active) contours are represented by triple cliques. Similarly, the 2nd order regularization methods in stereo predominantly use MRF models with scalar (1D) disparity labels and triple clique interactions. In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels, e.g. tangent planes. This general paradigm has been criticized due to perceived computational complexity of optimization in higher-dimensional label space. Contrary to popular beliefs, we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization problems with (nearly) submodular pairwise interactions. Our theoretical and experimental re- sults demonstrate advantages over state-of-the-art methods for 2nd order smoothness stereo. 1</p><p>2 0.20934606 <a title="219-tfidf-2" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>Author: Bastian Goldluecke, Sven Wanner</p><p>Abstract: Unlike traditional images which do not offer information for different directions of incident light, a light field is defined on ray space, and implicitly encodes scene geometry data in a rich structure which becomes visible on its epipolar plane images. In this work, we analyze regularization of light fields in variational frameworks and show that their variational structure is induced by disparity, which is in this context best understood as a vector field on epipolar plane image space. We derive differential constraints on this vector field to enable consistent disparity map regularization. Furthermore, we show how the disparity field is related to the regularization of more general vector-valued functions on the 4D ray space of the light field. This way, we derive an efficient variational framework with convex priors, which can serve as a fundament for a large class of inverse problems on ray space.</p><p>3 0.16966406 <a title="219-tfidf-3" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>4 0.15993051 <a title="219-tfidf-4" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>Author: Yuichi Takeda, Shinsaku Hiura, Kosuke Sato</p><p>Abstract: In this paper we propose a novel depth measurement method by fusing depth from defocus (DFD) and stereo. One of the problems of passive stereo method is the difficulty of finding correct correspondence between images when an object has a repetitive pattern or edges parallel to the epipolar line. On the other hand, the accuracy of DFD method is inherently limited by the effective diameter of the lens. Therefore, we propose the fusion of stereo method and DFD by giving different focus distances for left and right cameras of a stereo camera with coded apertures. Two types of depth cues, defocus and disparity, are naturally integrated by the magnification and phase shift of a single point spread function (PSF) per camera. In this paper we give the proof of the proportional relationship between the diameter of defocus and disparity which makes the calibration easy. We also show the outstanding performance of our method which has both advantages of two depth cues through simulation and actual experiments.</p><p>5 0.14015688 <a title="219-tfidf-5" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>Author: Sven Wanner, Christoph Straehle, Bastian Goldluecke</p><p>Abstract: Wepresent thefirst variationalframeworkfor multi-label segmentation on the ray space of 4D light fields. For traditional segmentation of single images, , features need to be extractedfrom the 2Dprojection ofa three-dimensional scene. The associated loss of geometry information can cause severe problems, for example if different objects have a very similar visual appearance. In this work, we show that using a light field instead of an image not only enables to train classifiers which can overcome many of these problems, but also provides an optimal data structure for label optimization by implicitly providing scene geometry information. It is thus possible to consistently optimize label assignment over all views simultaneously. As a further contribution, we make all light fields available online with complete depth and segmentation ground truth data where available, and thus establish the first benchmark data set for light field analysis to facilitate competitive further development of algorithms.</p><p>6 0.12969035 <a title="219-tfidf-6" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>7 0.11803633 <a title="219-tfidf-7" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>8 0.11395466 <a title="219-tfidf-8" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>9 0.10934843 <a title="219-tfidf-9" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>10 0.090725832 <a title="219-tfidf-10" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>11 0.088399909 <a title="219-tfidf-11" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>12 0.085651092 <a title="219-tfidf-12" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>13 0.085239545 <a title="219-tfidf-13" href="./cvpr-2013-Dense_Reconstruction_Using_3D_Object_Shape_Priors.html">111 cvpr-2013-Dense Reconstruction Using 3D Object Shape Priors</a></p>
<p>14 0.082545221 <a title="219-tfidf-14" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>15 0.081149086 <a title="219-tfidf-15" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>16 0.079134919 <a title="219-tfidf-16" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>17 0.078457743 <a title="219-tfidf-17" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>18 0.07545428 <a title="219-tfidf-18" href="./cvpr-2013-Plane-Based_Content_Preserving_Warps_for_Video_Stabilization.html">333 cvpr-2013-Plane-Based Content Preserving Warps for Video Stabilization</a></p>
<p>19 0.075449027 <a title="219-tfidf-19" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>20 0.070486277 <a title="219-tfidf-20" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, 0.158), (2, 0.029), (3, 0.039), (4, 0.011), (5, -0.056), (6, -0.04), (7, 0.054), (8, -0.016), (9, 0.039), (10, 0.022), (11, 0.016), (12, 0.077), (13, 0.04), (14, -0.117), (15, 0.062), (16, -0.135), (17, -0.057), (18, 0.041), (19, -0.025), (20, -0.107), (21, 0.074), (22, 0.125), (23, 0.066), (24, -0.013), (25, -0.011), (26, -0.02), (27, -0.012), (28, 0.027), (29, -0.042), (30, 0.047), (31, 0.026), (32, -0.002), (33, -0.018), (34, 0.004), (35, 0.006), (36, -0.001), (37, -0.02), (38, -0.021), (39, -0.001), (40, 0.087), (41, -0.0), (42, 0.014), (43, 0.033), (44, -0.045), (45, 0.049), (46, 0.001), (47, 0.005), (48, -0.016), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9317131 <a title="219-lsi-1" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>Author: Carl Olsson, Johannes Ulén, Yuri Boykov</p><p>Abstract: It is commonly believed that higher order smoothness should be modeled using higher order interactions. For example, 2nd order derivatives for deformable (active) contours are represented by triple cliques. Similarly, the 2nd order regularization methods in stereo predominantly use MRF models with scalar (1D) disparity labels and triple clique interactions. In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels, e.g. tangent planes. This general paradigm has been criticized due to perceived computational complexity of optimization in higher-dimensional label space. Contrary to popular beliefs, we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization problems with (nearly) submodular pairwise interactions. Our theoretical and experimental re- sults demonstrate advantages over state-of-the-art methods for 2nd order smoothness stereo. 1</p><p>2 0.85364228 <a title="219-lsi-2" href="./cvpr-2013-Fusing_Depth_from_Defocus_and_Stereo_with_Coded_Apertures.html">181 cvpr-2013-Fusing Depth from Defocus and Stereo with Coded Apertures</a></p>
<p>Author: Yuichi Takeda, Shinsaku Hiura, Kosuke Sato</p><p>Abstract: In this paper we propose a novel depth measurement method by fusing depth from defocus (DFD) and stereo. One of the problems of passive stereo method is the difficulty of finding correct correspondence between images when an object has a repetitive pattern or edges parallel to the epipolar line. On the other hand, the accuracy of DFD method is inherently limited by the effective diameter of the lens. Therefore, we propose the fusion of stereo method and DFD by giving different focus distances for left and right cameras of a stereo camera with coded apertures. Two types of depth cues, defocus and disparity, are naturally integrated by the magnification and phase shift of a single point spread function (PSF) per camera. In this paper we give the proof of the proportional relationship between the diameter of defocus and disparity which makes the calibration easy. We also show the outstanding performance of our method which has both advantages of two depth cues through simulation and actual experiments.</p><p>3 0.80595791 <a title="219-lsi-3" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>Author: David Pfeiffer, Stefan Gehrig, Nicolai Schneider</p><p>Abstract: Applications based on stereo vision are becoming increasingly common, ranging from gaming over robotics to driver assistance. While stereo algorithms have been investigated heavily both on the pixel and the application level, far less attention has been dedicated to the use of stereo confidence cues. Mostly, a threshold is applied to the confidence values for further processing, which is essentially a sparsified disparity map. This is straightforward but it does not take full advantage of the available information. In this paper, we make full use of the stereo confidence cues by propagating all confidence values along with the measured disparities in a Bayesian manner. Before using this information, a mapping from confidence values to disparity outlier probability rate is performed based on gathered disparity statistics from labeled video data. We present an extension of the so called Stixel World, a generic 3D intermediate representation that can serve as input for many of the applications mentioned above. This scheme is modified to directly exploit stereo confidence cues in the underlying sensor model during a maximum a poste- riori estimation process. The effectiveness of this step is verified in an in-depth evaluation on a large real-world traffic data base of which parts are made publicly available. We show that using stereo confidence cues allows both reducing the number of false object detections by a factor of six while keeping the detection rate at a near constant level.</p><p>4 0.79987472 <a title="219-lsi-4" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>Author: Ralf Haeusler, Rahul Nair, Daniel Kondermann</p><p>Abstract: With the aim to improve accuracy of stereo confidence measures, we apply the random decision forest framework to a large set of diverse stereo confidence measures. Learning and testing sets were drawnfrom the recently introduced KITTI dataset, which currently poses higher challenges to stereo solvers than other benchmarks with ground truth for stereo evaluation. We experiment with semi global matching stereo (SGM) and a census dataterm, which is the best performing realtime capable stereo method known to date. On KITTI images, SGM still produces a significant amount of error. We obtain consistently improved area under curve values of sparsification measures in comparison to best performing single stereo confidence measures where numbers of stereo errors are large. More specifically, our method performs best in all but one out of 194 frames of the KITTI dataset.</p><p>5 0.7393505 <a title="219-lsi-5" href="./cvpr-2013-Segment-Tree_Based_Cost_Aggregation_for_Stereo_Matching.html">384 cvpr-2013-Segment-Tree Based Cost Aggregation for Stereo Matching</a></p>
<p>Author: Xing Mei, Xun Sun, Weiming Dong, Haitao Wang, Xiaopeng Zhang</p><p>Abstract: This paper presents a novel tree-based cost aggregation method for dense stereo matching. Instead of employing the minimum spanning tree (MST) and its variants, a new tree structure, ”Segment-Tree ”, is proposed for non-local matching cost aggregation. Conceptually, the segment-tree is constructed in a three-step process: first, the pixels are grouped into a set of segments with the reference color or intensity image; second, a tree graph is created for each segment; and in the final step, these independent segment graphs are linked to form the segment-tree structure. In practice, this tree can be efficiently built in time nearly linear to the number of the image pixels. Compared to MST where the graph connectivity is determined with local edge weights, our method introduces some ’non-local’ decision rules: the pixels in one perceptually consistent segment are more likely to share similar disparities, and therefore their connectivity within the segment should be first enforced in the tree construction process. The matching costs are then aggregated over the tree within two passes. Performance evaluation on 19 Middlebury data sets shows that the proposed method is comparable to previous state-of-the-art aggregation methods in disparity accuracy and processing speed. Furthermore, the tree structure can be refined with the estimated disparities, which leads to consistent scene segmentation and significantly better aggregation results.</p><p>6 0.73231262 <a title="219-lsi-6" href="./cvpr-2013-The_Variational_Structure_of_Disparity_and_Regularization_of_4D_Light_Fields.html">431 cvpr-2013-The Variational Structure of Disparity and Regularization of 4D Light Fields</a></p>
<p>7 0.63329613 <a title="219-lsi-7" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>8 0.58699709 <a title="219-lsi-8" href="./cvpr-2013-Patch_Match_Filter%3A_Efficient_Edge-Aware_Filtering_Meets_Randomized_Search_for_Fast_Correspondence_Field_Estimation.html">326 cvpr-2013-Patch Match Filter: Efficient Edge-Aware Filtering Meets Randomized Search for Fast Correspondence Field Estimation</a></p>
<p>9 0.54935127 <a title="219-lsi-9" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>10 0.54270041 <a title="219-lsi-10" href="./cvpr-2013-Joint_3D_Scene_Reconstruction_and_Class_Segmentation.html">230 cvpr-2013-Joint 3D Scene Reconstruction and Class Segmentation</a></p>
<p>11 0.53597277 <a title="219-lsi-11" href="./cvpr-2013-Robust_Monocular_Epipolar_Flow_Estimation.html">362 cvpr-2013-Robust Monocular Epipolar Flow Estimation</a></p>
<p>12 0.49507612 <a title="219-lsi-12" href="./cvpr-2013-An_Iterated_L1_Algorithm_for_Non-smooth_Non-convex_Optimization_in_Computer_Vision.html">41 cvpr-2013-An Iterated L1 Algorithm for Non-smooth Non-convex Optimization in Computer Vision</a></p>
<p>13 0.49496743 <a title="219-lsi-13" href="./cvpr-2013-A_New_Perspective_on_Uncalibrated_Photometric_Stereo.html">21 cvpr-2013-A New Perspective on Uncalibrated Photometric Stereo</a></p>
<p>14 0.48247191 <a title="219-lsi-14" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>15 0.47944295 <a title="219-lsi-15" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>16 0.46808335 <a title="219-lsi-16" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>17 0.44175974 <a title="219-lsi-17" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>18 0.43802032 <a title="219-lsi-18" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>19 0.43560836 <a title="219-lsi-19" href="./cvpr-2013-Discrete_MRF_Inference_of_Marginal_Densities_for_Non-uniformly_Discretized_Variable_Space.html">128 cvpr-2013-Discrete MRF Inference of Marginal Densities for Non-uniformly Discretized Variable Space</a></p>
<p>20 0.41991439 <a title="219-lsi-20" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.268), (10, 0.106), (16, 0.027), (26, 0.031), (28, 0.012), (33, 0.234), (67, 0.025), (69, 0.048), (80, 0.014), (87, 0.114), (92, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78993881 <a title="219-lda-1" href="./cvpr-2013-In_Defense_of_3D-Label_Stereo.html">219 cvpr-2013-In Defense of 3D-Label Stereo</a></p>
<p>Author: Carl Olsson, Johannes Ulén, Yuri Boykov</p><p>Abstract: It is commonly believed that higher order smoothness should be modeled using higher order interactions. For example, 2nd order derivatives for deformable (active) contours are represented by triple cliques. Similarly, the 2nd order regularization methods in stereo predominantly use MRF models with scalar (1D) disparity labels and triple clique interactions. In this paper we advocate a largely overlooked alternative approach to stereo where 2nd order surface smoothness is represented by pairwise interactions with 3D-labels, e.g. tangent planes. This general paradigm has been criticized due to perceived computational complexity of optimization in higher-dimensional label space. Contrary to popular beliefs, we demonstrate that representing 2nd order surface smoothness with 3D labels leads to simpler optimization problems with (nearly) submodular pairwise interactions. Our theoretical and experimental re- sults demonstrate advantages over state-of-the-art methods for 2nd order smoothness stereo. 1</p><p>2 0.73032141 <a title="219-lda-2" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<p>Author: Mingsheng Long, Guiguang Ding, Jianmin Wang, Jiaguang Sun, Yuchen Guo, Philip S. Yu</p><p>Abstract: Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. It has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-ofthe-art methods on three types of computer vision datasets.</p><p>3 0.73008364 <a title="219-lda-3" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>4 0.72299659 <a title="219-lda-4" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>5 0.72187364 <a title="219-lda-5" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>6 0.71968597 <a title="219-lda-6" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>7 0.71850759 <a title="219-lda-7" href="./cvpr-2013-Exploiting_the_Power_of_Stereo_Confidences.html">155 cvpr-2013-Exploiting the Power of Stereo Confidences</a></p>
<p>8 0.71724892 <a title="219-lda-8" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>9 0.71654677 <a title="219-lda-9" href="./cvpr-2013-Alternating_Decision_Forests.html">39 cvpr-2013-Alternating Decision Forests</a></p>
<p>10 0.71618426 <a title="219-lda-10" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>11 0.71604294 <a title="219-lda-11" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>12 0.71590889 <a title="219-lda-12" href="./cvpr-2013-Principal_Observation_Ray_Calibration_for_Tiled-Lens-Array_Integral_Imaging_Display.html">337 cvpr-2013-Principal Observation Ray Calibration for Tiled-Lens-Array Integral Imaging Display</a></p>
<p>13 0.71585494 <a title="219-lda-13" href="./cvpr-2013-Ensemble_Learning_for_Confidence_Measures_in_Stereo_Vision.html">147 cvpr-2013-Ensemble Learning for Confidence Measures in Stereo Vision</a></p>
<p>14 0.71579689 <a title="219-lda-14" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>15 0.71494836 <a title="219-lda-15" href="./cvpr-2013-Globally_Consistent_Multi-label_Assignment_on_the_Ray_Space_of_4D_Light_Fields.html">188 cvpr-2013-Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a></p>
<p>16 0.71479279 <a title="219-lda-16" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>17 0.71443963 <a title="219-lda-17" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>18 0.71400434 <a title="219-lda-18" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>19 0.71398562 <a title="219-lda-19" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>20 0.71390057 <a title="219-lda-20" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
