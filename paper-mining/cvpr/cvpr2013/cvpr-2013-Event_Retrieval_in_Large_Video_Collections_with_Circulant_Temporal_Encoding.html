<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-151" href="#">cvpr2013-151</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</h1>
<br/><p>Source: <a title="cvpr-2013-151-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Revaud_Event_Retrieval_in_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>Reference: <a title="cvpr-2013-151-reference" href="../cvpr2013_reference/cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Event retrieval in large video collections with circulant temporal encoding  Je´ r oˆme Revaud  Matthijs DouzIeNRIACordelia Schmid  Herv e´ J e´gou  Abstract This paper presents an approach for large-scale event retrieval. [sent-1, score-0.895]
</p><p>2 , the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. [sent-4, score-0.576]
</p><p>3 Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. [sent-5, score-0.484]
</p><p>4 It exploits the properties of circulant matrices to compare the videos in the frequency domain. [sent-6, score-0.483]
</p><p>5 Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. [sent-8, score-0.278]
</p><p>6 Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. [sent-9, score-0.524]
</p><p>7 Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset. [sent-10, score-0.29]
</p><p>8 Introduction  This paper introduces an approach for specific event retrieval. [sent-12, score-0.317]
</p><p>9 Examples of events are news items such as the wedding of prince William and Kate, or re-occurring events such as the eruption of a geyser. [sent-13, score-0.494]
</p><p>10 Searching for specific events is related to video copy detection [13] and event category recognition [16], but there are substantial differences with both. [sent-17, score-0.822]
</p><p>11 The goal of video copy detection is to find deformed videos, e. [sent-18, score-0.377]
</p><p>12 Detecting event categories requires a classification approach that captures the large intra-class variability. [sent-21, score-0.29]
</p><p>13 The method introduced in this paper is tailored to specific event retrieval, as it is flexible enough to handle significant viewpoint change while still producing a precise alignment in time. [sent-22, score-0.335]
</p><p>14 Our first contribution is to encode the frame descriptors of a video into a temporal representation and to exploit the properties of circulant matrices to compare videos in the frequency domain. [sent-23, score-0.967]
</p><p>15 The second contribution is a dataset for specific event retrieval in large user-generated video content. [sent-24, score-0.534]
</p><p>16 This dataset, named EVVE, has been collected from Youtube and comprises a set of manually annotated videos of 13 events, as well as 100,000 distractor videos. [sent-25, score-0.304]
</p><p>17 Many techniques for video retrieval represent a video as  a set of descriptors extracted from frames or keyframes [4, 11, 20]. [sent-26, score-0.501]
</p><p>18 Searching in a collection is performed by comparing the query descriptors with those of the dataset. [sent-27, score-0.265]
</p><p>19 , partial alignment [22] or classic voting techniques, such as temporal Hough transform [4], which was popular in the TRECVID video copy detection task [19]. [sent-30, score-0.603]
</p><p>20 Such approaches are costly, since all frame descriptors of the query must be compared to those of the database before performing the temporal verification. [sent-31, score-0.574]
</p><p>21 Frame descriptors are jointly encoded in the frequency domain, where convolutions cast into efficient element-wise multiplications. [sent-35, score-0.216]
</p><p>22 Computing a matching score between videos only requires component-wise operations and a single one-dimensional inverse Fourier transform, avoiding the reconstruction of the descriptor in the temporal domain. [sent-38, score-0.549]
</p><p>23 Recently, transforming a multi-dimensional signal to the Fourier domain to speed up detection was shown useful [5], but to our knowledge, it is new to analyze the temporal aspect of global image descriptors in this way. [sent-42, score-0.334]
</p><p>24 222444555977  The tradeoff between search quality, speed and memory usage is optimized with the product quantization technique [9], which is extended to complex vectors in order to compare our descriptors in the compressed Fourier domain. [sent-43, score-0.444]
</p><p>25 Section 3 describes frame descriptors, Section 4 describes our temporal circulant encoding technique and Section 5 presents our indexing strategy. [sent-46, score-0.516]
</p><p>26 The experiments in Section 6 demonstrate the excellent results of our approach for event retrieval on the EVVE dataset. [sent-47, score-0.394]
</p><p>27 Our approach also significantly outperforms state-of-the-art systems for efficient video copy detection on the TRECVID and CCWEB benchmarks. [sent-48, score-0.377]
</p><p>28 EVVE: an event retrieval dataset This section introduces the EVVE (EVent VidEo) dataset which is dedicated to the retrieval of particular events. [sent-50, score-0.525]
</p><p>29 This differs from recognizing event categories such as “birthday party” or “grooming an animal”, as in the TRECVID  Multimedia event detection task [16]. [sent-51, score-0.611]
</p><p>30 Several of them are localized precisely in time and space as professional reporters and spectators have captured the same event simultaneously. [sent-53, score-0.325]
</p><p>31 An example is the event “Concert of Madonna in Rome 2012”. [sent-54, score-0.29]
</p><p>32 In this case, the videos overlap visually and can be aligned. [sent-55, score-0.232]
</p><p>33 EVVE also includes events for which relevant videos might not correspond to the same instance in place or time. [sent-56, score-0.387]
</p><p>34 For instance, the event ”The major autumn flood in Thailand in 2011” is covered by videos of the flood in different places, and “Austerity riots in Barcelona” includes shots of riots at different places and moments. [sent-57, score-0.809]
</p><p>35 Each event was annotated by one annotator, who first produced a precise definition of the event. [sent-60, score-0.29]
</p><p>36 For example, the event “The wedding of Prince William and Kate Middleton” is defined as:  TtsI(emihnxgatelwrshitnmuofahgKtpeicrnhgoeu. [sent-61, score-0.344]
</p><p>37 In addition to the videos collected for the specific events, we have also retrieved a set of 100,000 “dis-  tractor” videos by querying Youtube with unrelated terms. [sent-69, score-0.495]
</p><p>38 These videos have all been collected before September 2008, which ensures that the distractor set does not contain any of the relevant events of EVVE, since all events are temporally localized after September 2008 (except the  Figure 1. [sent-70, score-0.644]
</p><p>39 The distractor videos representing a similar but distinct event, such as videos of other bomb attacks for Event #9, are counted as negatives. [sent-77, score-0.536]
</p><p>40 Evaluation is performed in a standard retrieval scenario, where we submit one video query at a time and the algorithm returns a list of videos ranked by similarity scores. [sent-79, score-0.624]
</p><p>41 Frame description We represent a video by a sequence of high-dimensional frame descriptors, as described in this section. [sent-85, score-0.226]
</p><p>42 All videos are mapped to a common format, by sampling them at a fixed rate of 15 fps and resizing them to a maximum of 120k pixels, while keeping the aspect ratio. [sent-87, score-0.292]
</p><p>43 Local SIFT descriptors [14] are extracted for each frame on a dense grid [15], every 4 pixels and for 5 scale levels. [sent-89, score-0.203]
</p><p>44 The SIFT descriptors of a frame are encoded using MultiVLAD [8], a variant of the Fisher vector [17]. [sent-93, score-0.231]
</p><p>45 Circulant temporal aggregation The method introduced in this section aims at comparing two sequences of frame descriptors q = [q1, . [sent-99, score-0.408]
</p><p>46 This is the case for Fisher and our Multi-VLAD descriptors (Section 3), but not for other type of descriptors to be compared with complex kernels. [sent-118, score-0.273]
</p><p>47 In practice, this assumption is not well satisfied, because the videos are very self-similar in time, so the similarity proposed in Eqn. [sent-120, score-0.232]
</p><p>48 The encoding technique for sequences of vector descriptors presented in this section, is referred to as Circulant Temporal Encoding (CTE). [sent-123, score-0.258]
</p><p>49 Unfortunately, averaging does not always suffice, as many videos contain only one shot composed of a single frame: the components associated with high frequencies are almost 0 for all dimensions. [sent-215, score-0.317]
</p><p>50 This leads to a regularized score between two video sequences q and b:  sλ(q,b) =d1F−1? [sent-231, score-0.204]
</p><p>51 ] between two videos sequences q and b for all possible temporal shifts. [sent-249, score-0.437]
</p><p>52 In some applications such as video alignment (see Section 6), we also need the boundaries of the matching segments. [sent-251, score-0.215]
</p><p>53 For this purpose, the database descriptors are reconstructed in the temporal domain from F−1 (b? [sent-252, score-0.385]
</p><p>54 Yet, on large datasets this does not impact the overall efficiency, since it is only applied to a short-list of videos with the highest scores. [sent-259, score-0.232]
</p><p>55 Frequency-domain representation A database video b of length n is represented in the Fourier domain by a complex matrix B = 222444666200  [B? [sent-267, score-0.306]
</p><p>56 Therefore, expanded versions of , database descriptors can be generated on the fly and at no cost. [sent-294, score-0.243]
</p><p>57 This asymmetric processing of the videos was chosen for efficiency reasons. [sent-295, score-0.232]
</p><p>58 Unfortunately, this introduces an uncertainty on the alignment of the query and database videos: δ∗ can be determined modulo n only. [sent-296, score-0.302]
</p><p>59 10, we propose two extensions of the product quantization technique [9], which is a compression technique that enables efficient compressed-domain comparison and search. [sent-300, score-0.245]
</p><p>60 The comparison between a query descriptor x and the  database vectors is performed in two stages. [sent-319, score-0.333]
</p><p>61 We learn the k-means centroids for complex vectors by considering a d-dimensional complex vector to be a 2ddimensional real vector, and this for all the frequency vectors that we keep: Cd ≡ R2d and fj ≡ yj . [sent-327, score-0.3]
</p><p>62 At query time, the table T stores complex values. [sent-328, score-0.219]
</p><p>63 Summary of search procedure and complexity Each database video is processed offline as follows: 1. [sent-343, score-0.222]
</p><p>64 The video is pre-processed and each frame is described as a d-dimensional Multi-VLAD descriptor. [sent-344, score-0.226]
</p><p>65 These vectors are separately encoded with a complex product quantizer, producing a compressed representation of p n? [sent-353, score-0.214]
</p><p>66 At query time, the submitted video is described in the  ××  same manner. [sent-355, score-0.288]
</p><p>67 The complexity at query time depends on the number N of database videos, the dimensionality d of the frame descriptor and the video length, that we assume for readability to be constant (n frames): 1. [sent-356, score-0.52]
</p><p>68 O(d n log n) – The query frame descriptors are mapped to the frequency domain by d FFTs. [sent-357, score-0.5]
</p><p>69 ) – This vector is mapped to the temporal domain using a single inverse FFT. [sent-376, score-0.266]
</p><p>70 Experiments In this section we evaluate our approach, both for video copy detection and event retrieval. [sent-385, score-0.667]
</p><p>71 To compare the contributions of the frame descriptors and of the temporal matching, we introduce an additional descriptor obtained by averaging the frame descriptors (see section 3) over the entire video. [sent-386, score-0.651]
</p><p>72 Video copy detection This task is evaluated on two public benchmarks, the CCWEB dataset [21] and the TRECVID 2008 content based copy detection dataset (CCD) [19], see Table 1. [sent-390, score-0.474]
</p><p>73 The transformed versions in the database correspond to user re-posts on video sharing sites. [sent-392, score-0.266]
</p><p>74 We present  results on the camcording subtask, which is most relevant to our context of event retrieval in the presence of significant viewpoint changes. [sent-396, score-0.394]
</p><p>75 The spatial and temporal compression is parametrized by the dimensionality d after PCA, the number p of PQ sub-quantizers and the frame description rate β, which defines the ratio between the number of frequency vectors and the number of video frames. [sent-399, score-0.53]
</p><p>76 For nearduplicate retrieval as well as for event retrieval, Figure 2 shows that intermediate values of λ yield the best performance. [sent-417, score-0.394]
</p><p>77 In contrast, we observe that small values of λ produce the best NDCR performance for the TRECVID copy detection task. [sent-418, score-0.237]
</p><p>78 1 for the near-duplicate and event retrieval tasks, and λ=0. [sent-421, score-0.394]
</p><p>79 On CCWEB, both the temporal and non-temporal versions of our method outperform the state of the art for comparable memory footprints. [sent-425, score-0.23]
</p><p>80 Impact of the parameter λ on the performance  for the large-scale version ofthe dataset are not strictly comparable with those of the original paper [20] because the distractor videos are different (they do not provide theirs). [sent-436, score-0.304]
</p><p>81 Despite this advantage, MMV performs poorly (NDCR close to 1), due to the small overlap between queries and database videos (typically 1%), which dilutes the matching segment in the video descriptor. [sent-441, score-0.484]
</p><p>82 Remark: The performance of CTE mainly depends on the length of the subsequence shared by the query and retrieved videos: Pairs with subsequences shorter than 5 s are correctly found with 62% accuracy, subsequences between 5s and 10s with 80% accuracy and longer subsequences with 93% accuracy. [sent-442, score-0.347]
</p><p>83 , CCWEB with 100k distractors, the bottleneck remains the descriptor computation, which is performed faster than real-time on one processor core (1-2 minute per query on TRECVID and CCWEB). [sent-446, score-0.24]
</p><p>84 On EVVE+100k, this generates a database size of 943 MB and an average query time of 11s. [sent-453, score-0.23]
</p><p>85 The detailed results are presented per event in Table 3 for both the temporal and nontemporal versions of our algorithm. [sent-454, score-0.475]
</p><p>86 Interestingly, MMV performs similarly to CTE on average, at a much lower memory and computational cost, which means that some events are better captured by using a global descriptor of visual appearance. [sent-455, score-0.264]
</p><p>87 For instance, videos from the Shakira concert always feature the crowd in the foreground and the  nuEmvenbterMMVCETVEVEMMV+CTEMEMVVVE+1C0T0E,000M diMstrVa+ctCorTsE  same concert scene behind, so averaging the frame descrip-  tors provides a robust visual summary of the event. [sent-456, score-0.482]
</p><p>88 This is done by adding the normalized scores obtained from MMV and CTE for each database video and for each query. [sent-459, score-0.222]
</p><p>89 Note that CTE also outputs the matching video parts, which is important for the video alignment described in the next section. [sent-464, score-0.355]
</p><p>90 Automatic video alignment For some events from EVVE, many people have filmed the same scene, e. [sent-467, score-0.34]
</p><p>91 We use the CTE method to automatically align the videos on a common timeline. [sent-470, score-0.232]
</p><p>92 We match all possible videos pairs (including all query and database videos), which results in a time shift δ∗ for all pairs (see Section 4. [sent-471, score-0.504]
</p><p>93 Aligning the videos consists in estimating the starting time of each video on the common timeline, so that the time shifts are satisfied. [sent-473, score-0.372]
</p><p>94 During this process, groups of independent videos emerge, where each group corresponds to a distinct scene. [sent-477, score-0.232]
</p><p>95 We use this to display different viewpoints of an event on a shared timeline, as depicted in Figure 3. [sent-478, score-0.29]
</p><p>96 This video representation provides an efficient search scheme that avoids the exhaustive comparison of frames, which is commonly performed when estimating the temporal Hough transform. [sent-483, score-0.281]
</p><p>97 Extensive experiments on two video copy detection benchmarks show that our approach improves over the state of the art with respect to accuracy, search time and mem-  ory usage. [sent-484, score-0.407]
</p><p>98 Moving towards the more challenging  task of  event retrieval, our approach efficiently retrieves instances of events in a large collection of videos, as shown for the EVVE event retrieval dataset introduced in this paper. [sent-485, score-0.839]
</p><p>99 Compact video description for copy detection with precise temporal alignment. [sent-516, score-0.518]
</p><p>100 Tiny Videos: A large data set for nonparametric video retrieval and frame classification. [sent-561, score-0.33]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('evve', 0.379), ('event', 0.29), ('mmv', 0.247), ('videos', 0.232), ('cte', 0.207), ('copy', 0.206), ('trecvid', 0.19), ('circulant', 0.18), ('ccweb', 0.177), ('fourier', 0.16), ('events', 0.155), ('ndcr', 0.152), ('qi', 0.149), ('query', 0.148), ('temporal', 0.141), ('video', 0.14), ('descriptors', 0.117), ('retrieval', 0.104), ('frame', 0.086), ('database', 0.082), ('eruption', 0.076), ('distractor', 0.072), ('frequency', 0.071), ('flood', 0.067), ('pca', 0.065), ('descriptor', 0.064), ('sequences', 0.064), ('wi', 0.063), ('concert', 0.062), ('kate', 0.062), ('quantization', 0.059), ('product', 0.059), ('william', 0.059), ('subsequences', 0.056), ('prince', 0.054), ('wedding', 0.054), ('compression', 0.053), ('autumn', 0.051), ('geyser', 0.051), ('iceland', 0.051), ('middleton', 0.051), ('nmax', 0.051), ('riots', 0.051), ('strokkur', 0.051), ('thailand', 0.051), ('compressed', 0.049), ('fft', 0.049), ('hough', 0.048), ('inverse', 0.047), ('domain', 0.045), ('alignment', 0.045), ('bolme', 0.045), ('timeline', 0.045), ('frequencies', 0.045), ('memory', 0.045), ('versions', 0.044), ('shift', 0.042), ('centroids', 0.042), ('impacting', 0.042), ('padded', 0.042), ('regularization', 0.041), ('douze', 0.041), ('averaging', 0.04), ('transform', 0.04), ('encoding', 0.04), ('complex', 0.039), ('vectors', 0.039), ('smeaton', 0.037), ('archives', 0.037), ('technique', 0.037), ('seam', 0.036), ('gou', 0.036), ('byproduct', 0.036), ('distractors', 0.036), ('civr', 0.036), ('operations', 0.035), ('professional', 0.035), ('ccd', 0.033), ('compress', 0.033), ('mapped', 0.033), ('september', 0.033), ('indexing', 0.032), ('qt', 0.032), ('stores', 0.032), ('multimedia', 0.031), ('retrieved', 0.031), ('yj', 0.031), ('detection', 0.031), ('fisher', 0.031), ('benchmarks', 0.03), ('temporally', 0.03), ('matching', 0.03), ('storing', 0.029), ('filters', 0.028), ('processor', 0.028), ('encoded', 0.028), ('egou', 0.028), ('introduces', 0.027), ('pq', 0.027), ('fps', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="151-tfidf-1" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>2 0.28291011 <a title="151-tfidf-2" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>Author: Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, Nicu Sebe, Alexander G. Hauptmann</p><p>Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.</p><p>3 0.19714576 <a title="151-tfidf-3" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>4 0.17262657 <a title="151-tfidf-4" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>5 0.15232545 <a title="151-tfidf-5" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>Author: Raghuraman Gopalan</p><p>Abstract: While the notion of joint sparsity in understanding common and innovative components of a multi-receiver signal ensemble has been well studied, we investigate the utility of such joint sparse models in representing information contained in a single video signal. By decomposing the content of a video sequence into that observed by multiple spatially and/or temporally distributed receivers, we first recover a collection of common and innovative components pertaining to individual videos. We then present modeling strategies based on subspace-driven manifold metrics to characterize patterns among these components, across other videos in the system, to perform subsequent video analysis. We demonstrate the efficacy of our approach for activity classification and clustering by reporting competitive results on standard datasets such as, HMDB, UCF-50, Olympic Sports and KTH.</p><p>6 0.14943351 <a title="151-tfidf-6" href="./cvpr-2013-Query_Adaptive_Similarity_for_Large_Scale_Object_Retrieval.html">343 cvpr-2013-Query Adaptive Similarity for Large Scale Object Retrieval</a></p>
<p>7 0.14684731 <a title="151-tfidf-7" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>8 0.1373513 <a title="151-tfidf-8" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>9 0.12907794 <a title="151-tfidf-9" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>10 0.12666449 <a title="151-tfidf-10" href="./cvpr-2013-Geometric_Context_from_Videos.html">187 cvpr-2013-Geometric Context from Videos</a></p>
<p>11 0.122967 <a title="151-tfidf-11" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>12 0.12032226 <a title="151-tfidf-12" href="./cvpr-2013-Learning_and_Calibrating_Per-Location_Classifiers_for_Visual_Place_Recognition.html">260 cvpr-2013-Learning and Calibrating Per-Location Classifiers for Visual Place Recognition</a></p>
<p>13 0.12024903 <a title="151-tfidf-13" href="./cvpr-2013-Graph-Based_Discriminative_Learning_for_Location_Recognition.html">189 cvpr-2013-Graph-Based Discriminative Learning for Location Recognition</a></p>
<p>14 0.11524409 <a title="151-tfidf-14" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>15 0.11506071 <a title="151-tfidf-15" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>16 0.11045601 <a title="151-tfidf-16" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>17 0.10964483 <a title="151-tfidf-17" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>18 0.1091188 <a title="151-tfidf-18" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>19 0.10825852 <a title="151-tfidf-19" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>20 0.10793575 <a title="151-tfidf-20" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, -0.07), (2, -0.038), (3, -0.073), (4, -0.062), (5, 0.016), (6, -0.13), (7, -0.134), (8, -0.096), (9, 0.027), (10, 0.022), (11, -0.06), (12, 0.124), (13, 0.016), (14, 0.029), (15, -0.08), (16, 0.088), (17, 0.041), (18, 0.039), (19, -0.22), (20, -0.065), (21, -0.026), (22, -0.012), (23, -0.07), (24, -0.085), (25, 0.006), (26, 0.007), (27, -0.091), (28, -0.004), (29, -0.006), (30, 0.178), (31, -0.061), (32, -0.016), (33, -0.097), (34, -0.105), (35, 0.067), (36, 0.068), (37, 0.039), (38, 0.05), (39, 0.066), (40, 0.107), (41, 0.015), (42, -0.056), (43, -0.075), (44, -0.005), (45, 0.039), (46, -0.136), (47, 0.085), (48, -0.004), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96042323 <a title="151-lsi-1" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>2 0.75610858 <a title="151-lsi-2" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>Author: Suha Kwak, Bohyung Han, Joon Hee Han</p><p>Abstract: We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient subproblems—quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.</p><p>3 0.6810165 <a title="151-lsi-3" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>4 0.66803795 <a title="151-lsi-4" href="./cvpr-2013-Large-Scale_Video_Summarization_Using_Web-Image_Priors.html">243 cvpr-2013-Large-Scale Video Summarization Using Web-Image Priors</a></p>
<p>Author: Aditya Khosla, Raffay Hamid, Chih-Jen Lin, Neel Sundaresan</p><p>Abstract: Given the enormous growth in user-generated videos, it is becoming increasingly important to be able to navigate them efficiently. As these videos are generally of poor quality, summarization methods designed for well-produced videos do not generalize to them. To address this challenge, we propose to use web-images as a prior to facilitate summarization of user-generated videos. Our main intuition is that people tend to take pictures of objects to capture them in a maximally informative way. Such images could therefore be used as prior information to summarize videos containing a similar set of objects. In this work, we apply our novel insight to develop a summarization algorithm that uses the web-image based prior information in an unsupervised manner. Moreover, to automatically evaluate summarization algorithms on a large scale, we propose a framework that relies on multiple summaries obtained through crowdsourcing. We demonstrate the effectiveness of our evaluation framework by comparing its performance to that ofmultiple human evaluators. Finally, wepresent resultsfor our framework tested on hundreds of user-generated videos.</p><p>5 0.64947277 <a title="151-lsi-5" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>6 0.63572371 <a title="151-lsi-6" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>7 0.60840851 <a title="151-lsi-7" href="./cvpr-2013-Story-Driven_Summarization_for_Egocentric_Video.html">413 cvpr-2013-Story-Driven Summarization for Egocentric Video</a></p>
<p>8 0.60755479 <a title="151-lsi-8" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>9 0.57460648 <a title="151-lsi-9" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>10 0.54335886 <a title="151-lsi-10" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>11 0.53078371 <a title="151-lsi-11" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>12 0.50833458 <a title="151-lsi-12" href="./cvpr-2013-Multi-class_Video_Co-segmentation_with_a_Generative_Multi-video_Model.html">294 cvpr-2013-Multi-class Video Co-segmentation with a Generative Multi-video Model</a></p>
<p>13 0.48312733 <a title="151-lsi-13" href="./cvpr-2013-All_About_VLAD.html">38 cvpr-2013-All About VLAD</a></p>
<p>14 0.46187234 <a title="151-lsi-14" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>15 0.46149907 <a title="151-lsi-15" href="./cvpr-2013-Recognize_Human_Activities_from_Partially_Observed_Videos.html">347 cvpr-2013-Recognize Human Activities from Partially Observed Videos</a></p>
<p>16 0.4571121 <a title="151-lsi-16" href="./cvpr-2013-Learning_Binary_Codes_for_High-Dimensional_Data_Using_Bilinear_Projections.html">246 cvpr-2013-Learning Binary Codes for High-Dimensional Data Using Bilinear Projections</a></p>
<p>17 0.44066828 <a title="151-lsi-17" href="./cvpr-2013-Cartesian_K-Means.html">79 cvpr-2013-Cartesian K-Means</a></p>
<p>18 0.44033241 <a title="151-lsi-18" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>19 0.43893105 <a title="151-lsi-19" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>20 0.4376919 <a title="151-lsi-20" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.103), (16, 0.034), (26, 0.041), (33, 0.291), (67, 0.068), (69, 0.033), (77, 0.295), (80, 0.01), (87, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8955074 <a title="151-lda-1" href="./cvpr-2013-A_Max-Margin_Riffled_Independence_Model_for_Image_Tag_Ranking.html">18 cvpr-2013-A Max-Margin Riffled Independence Model for Image Tag Ranking</a></p>
<p>Author: Tian Lan, Greg Mori</p><p>Abstract: We propose Max-Margin Riffled Independence Model (MMRIM), a new method for image tag ranking modeling the structured preferences among tags. The goal is to predict a ranked tag list for a given image, where tags are ordered by their importance or relevance to the image content. Our model integrates the max-margin formalism with riffled independence factorizations proposed in [10], which naturally allows for structured learning and efficient ranking. Experimental results on the SUN Attribute and LabelMe datasets demonstrate the superior performance of the proposed model compared with baseline tag ranking methods. We also apply the predicted rank list of tags to several higher-level computer vision applications in image understanding and retrieval, and demonstrate that MMRIM significantly improves the accuracy of these applications.</p><p>2 0.88764507 <a title="151-lda-2" href="./cvpr-2013-Robust_Canonical_Time_Warping_for_the_Alignment_of_Grossly_Corrupted_Sequences.html">358 cvpr-2013-Robust Canonical Time Warping for the Alignment of Grossly Corrupted Sequences</a></p>
<p>Author: Yannis Panagakis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic</p><p>Abstract: Temporal alignment of human behaviour from visual data is a very challenging problem due to a numerous reasons, including possible large temporal scale differences, inter/intra subject variability and, more importantly, due to the presence of gross errors and outliers. Gross errors are often in abundance due to incorrect localization and tracking, presence of partial occlusion etc. Furthermore, such errors rarely follow a Gaussian distribution, which is the de-facto assumption in machine learning methods. In this paper, building on recent advances on rank minimization and compressive sensing, a novel, robust to gross errors temporal alignment method is proposed. While previous approaches combine the dynamic time warping (DTW) with low-dimensional projections that maximally correlate two sequences, we aim to learn two underlyingprojection matrices (one for each sequence), which not only maximally correlate the sequences but, at the same time, efficiently remove the possible corruptions in any datum in the sequences. The projections are obtained by minimizing the weighted sum of nuclear and ?1 norms, by solving a sequence of convex optimization problems, while the temporal alignment is found by applying the DTW in an alternating fashion. The superiority of the proposed method against the state-of-the-art time alignment methods, namely the canonical time warping and the generalized time warping, is indicated by the experimental results on both synthetic and real datasets.</p><p>3 0.88027561 <a title="151-lda-3" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>same-paper 4 0.84219724 <a title="151-lda-4" href="./cvpr-2013-Event_Retrieval_in_Large_Video_Collections_with_Circulant_Temporal_Encoding.html">151 cvpr-2013-Event Retrieval in Large Video Collections with Circulant Temporal Encoding</a></p>
<p>Author: Jérôme Revaud, Matthijs Douze, Cordelia Schmid, Hervé Jégou</p><p>Abstract: This paper presents an approach for large-scale event retrieval. Given a video clip of a specific event, e.g., the wedding of Prince William and Kate Middleton, the goal is to retrieve other videos representing the same event from a dataset of over 100k videos. Our approach encodes the frame descriptors of a video to jointly represent their appearance and temporal order. It exploits the properties of circulant matrices to compare the videos in the frequency domain. This offers a significant gain in complexity and accurately localizes the matching parts of videos. Furthermore, we extend product quantization to complex vectors in order to compress our descriptors, and to compare them in the compressed domain. Our method outperforms the state of the art both in search quality and query time on two large-scale video benchmarks for copy detection, TRECVID and CCWEB. Finally, we introduce a challenging dataset for event retrieval, EVVE, and report the performance on this dataset.</p><p>5 0.84066784 <a title="151-lda-5" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>Author: Tim Matthews, Mark S. Nixon, Mahesan Niranjan</p><p>Abstract: We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pairwise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence ofattributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.</p><p>6 0.80161256 <a title="151-lda-6" href="./cvpr-2013-Robust_Object_Co-detection.html">364 cvpr-2013-Robust Object Co-detection</a></p>
<p>7 0.78653991 <a title="151-lda-7" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>8 0.77031571 <a title="151-lda-8" href="./cvpr-2013-Complex_Event_Detection_via_Multi-source_Video_Attributes.html">85 cvpr-2013-Complex Event Detection via Multi-source Video Attributes</a></p>
<p>9 0.75765222 <a title="151-lda-9" href="./cvpr-2013-Tag_Taxonomy_Aware_Dictionary_Learning_for_Region_Tagging.html">422 cvpr-2013-Tag Taxonomy Aware Dictionary Learning for Region Tagging</a></p>
<p>10 0.75169897 <a title="151-lda-10" href="./cvpr-2013-Image_Tag_Completion_via_Image-Specific_and_Tag-Specific_Linear_Sparse_Reconstructions.html">213 cvpr-2013-Image Tag Completion via Image-Specific and Tag-Specific Linear Sparse Reconstructions</a></p>
<p>11 0.74925196 <a title="151-lda-11" href="./cvpr-2013-Stochastic_Deconvolution.html">412 cvpr-2013-Stochastic Deconvolution</a></p>
<p>12 0.74603367 <a title="151-lda-12" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>13 0.74499094 <a title="151-lda-13" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>14 0.74488783 <a title="151-lda-14" href="./cvpr-2013-Event_Recognition_in_Videos_by_Learning_from_Heterogeneous_Web_Sources.html">150 cvpr-2013-Event Recognition in Videos by Learning from Heterogeneous Web Sources</a></p>
<p>15 0.74462944 <a title="151-lda-15" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>16 0.74279034 <a title="151-lda-16" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>17 0.74273098 <a title="151-lda-17" href="./cvpr-2013-Fast_Convolutional_Sparse_Coding.html">164 cvpr-2013-Fast Convolutional Sparse Coding</a></p>
<p>18 0.74175727 <a title="151-lda-18" href="./cvpr-2013-A_Divide-and-Conquer_Method_for_Scalable_Low-Rank_Latent_Matrix_Pursuit.html">7 cvpr-2013-A Divide-and-Conquer Method for Scalable Low-Rank Latent Matrix Pursuit</a></p>
<p>19 0.74088651 <a title="151-lda-19" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>20 0.73681134 <a title="151-lda-20" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
