<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-391" href="#">cvpr2013-391</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</h1>
<br/><p>Source: <a title="cvpr-2013-391-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Sensing_and_Recognizing_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Rui Li, Edward H. Adelson</p><p>Abstract: Sensing surface textures by touch is a valuable capability for robots. Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface texxtures. The GelSight sensor, when pressed against a surfacce, delivers a height map. This can be treated as an image, aand processed using the tools of visual texture analysis. WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. Our system can correctly categorize materials fromm this database with high accuracy. This suggests that the GGelSight sensor can be useful for material recognition by roobots.</p><p>Reference: <a title="cvpr-2013-391-reference" href="../cvpr2013_reference/cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Sensing surface textures by touch is a valuable capability for robots. [sent-2, score-0.207]
</p><p>2 Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. [sent-3, score-0.169]
</p><p>3 The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. [sent-4, score-0.103]
</p><p>4 The GelSight sensor, when pressed against a surfacce, delivers a height map. [sent-6, score-0.196]
</p><p>5 This can be treated as an image, aand processed using the tools of visual texture analysis. [sent-7, score-0.19]
</p><p>6 WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. [sent-8, score-0.251]
</p><p>7 We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. [sent-9, score-0.142]
</p><p>8 This suggests that the GGelSight sensor can be useful for material recognition by roobots. [sent-11, score-0.103]
</p><p>9 In this paper we focus on inforrmation about local surface geometry, which we will call suurface texture here. [sent-16, score-0.245]
</p><p>10 To extract surface texture, it is imporrtant to have a touch sensor that is compliant (i. [sent-17, score-0.273]
</p><p>11 The recently developed GelSight sensor [1] is built from softt elastomer, and by using computer vision techniques it offfers unprecedented levels of spatial resolution. [sent-21, score-0.14]
</p><p>12 A GelSight sensor delivers a detailedd height map of the surface being touched, in the form of a function z(x,y), where (x,y) are the point coordinates. [sent-22, score-0.334]
</p><p>13 1(b) shows the height map dderived by GelSight, Edward H. [sent-26, score-0.127]
</p><p>14 1(c) shows the height map displayed as a gray image. [sent-31, score-0.213]
</p><p>15 (d) – (f) show a photo of a  piece of sandpaper, the height map derived by GelSight displayed as a surface plot and tthe height map displayed as a gray image. [sent-33, score-0.529]
</p><p>16 (b) GelSight height map of the denim rendered at a different view. [sent-35, score-0.207]
</p><p>17 (c) 2D gray image of the denim heighht map in (b) with brightness corresponding to the height levels. [sent-36, score-0.27]
</p><p>18 (b) GelSight height mapp of the sandpaper rendered at a different view. [sent-38, score-0.244]
</p><p>19 (f) 2D gray image of the height map in (e) with brightness corresponding to height levels. [sent-39, score-0.341]
</p><p>20 The problem of tactile textture recognition has some specific properties of note. [sent-40, score-0.187]
</p><p>21 MMany vision problems are simplified, since a height map iinvolves no confounds with shading, albedo, distance, etc. [sent-41, score-0.127]
</p><p>22 The spatial scale is fixed, since the sensor is in direct conttact with the surface and the camera inside the GelSight devvice is at a fixed orientation and distance with respect to the sensor. [sent-43, score-0.184]
</p><p>23 In most cases, the orientatiion of the texture will be  unknown. [sent-44, score-0.164]
</p><p>24 For example, a denimm texture might occur with an arbitrary rotation, so we wannt a recognition system that is rotationally invariant. [sent-45, score-0.164]
</p><p>25 Thus we are confronted with a classical texture recognnition problem. [sent-48, score-0.164]
</p><p>26 We cannot recognize the sandpaper with a simple template match; rather, there are certain immage statistics that will characterize the sandpaper evven though each patch is slightly different. [sent-49, score-0.216]
</p><p>27 GelSight Overview The GelSight sensor is a novel tactile sensor to capture surface geometry through the use of a gel and a camera that gives a “sight” with computer vision algorithms. [sent-52, score-0.549]
</p><p>28 It consists of a piece of clear elastomer coated with a reflective membrane. [sent-53, score-0.109]
</p><p>29 When an object is pressed against the membrane, the membrane deforms to take the shape of the object’s surface, which is then recorded by a camera under illumination from LEDs located in different directions. [sent-54, score-0.139]
</p><p>30 A 3-dimensional (3D) height map of the surface can then be reconstructed with a photometric stereo algorithm [1]. [sent-55, score-0.208]
</p><p>31 (a) A cookie is pressed against the membrane of an elastomer block. [sent-59, score-0.251]
</p><p>32 (b) The membrane is deformed to the shape of the cookie surface. [sent-60, score-0.149]
</p><p>33 (c) The shape of the cookie surface is measured using photometric stereo and rendered at a novel viewpoint. [sent-61, score-0.161]
</p><p>34 The GelSight sensor has many important properties that make it attractive for use in tactile sensing. [sent-63, score-0.29]
</p><p>35 The sensor is made with inexpensive materials, and it can give spatial resolution as small as 2 microns. [sent-64, score-0.103]
</p><p>36 In addition, the sensor is not affected by the optical characteristics of the materials being measured as the membrane supplies its own bidirectional reflectance distribution function (BRDF). [sent-65, score-0.235]
</p><p>37 Last but not least, with compliant properties of  the gel sensor, GelSight may be used to measure the roughness and texture of a touched surface, the pressure distribution across the contact region, as well as shear and slip between the sensor and object in contact. [sent-68, score-0.474]
</p><p>38 All these properties make GelSight a very promising candidate to be used in robotic fingertips for tactile sensing. [sent-69, score-0.209]
</p><p>39 Even for the same Furthermore, the relative orientation between the gel and the texture can be different for each measurement. [sent-72, score-0.239]
</p><p>40 Those two characteristics make it desirable to have a texture classification algorithm that is invariant to both gray scales and rotation. [sent-73, score-0.307]
</p><p>41 We next give an overview of texture classification techniques and discuss what may be used in recognizing tactile textures with the use of GelSight. [sent-74, score-0.499]
</p><p>42 There are generally three types of methods adopted for rotation invariant texture classification: statistical, model-based and structural methods. [sent-80, score-0.251]
</p><p>43 [10] described texture images using features like center-symmetric auto-correlation, local binary pattern  (LBP), and gray-level difference, which are locally invariant to rotation. [sent-84, score-0.261]
</p><p>44 They propose a feature distribution method based on the G statistics to test those features for rotation-invariant texture analysis. [sent-85, score-0.194]
</p><p>45 [2] extended the work by using multiresolution gray-scale and rotation invariant LBP at circular neighborhoods of different radius and neighbor density, and achieved a relatively high classification rate. [sent-87, score-0.17]
</p><p>46 This had then become the state-of-the-art method, based on which a number of improved texture classification algorithms were developed. [sent-88, score-0.209]
</p><p>47 Yet one common issue of all these LBP-based methods is that they mostly deal with microstructures of texture images by considering patterns within a small neighborhood (e. [sent-90, score-0.363]
</p><p>48 , up to 3 pixels away) but not macrostructures with a large neighborhood. [sent-92, score-0.168]
</p><p>49 In this work, we propose a multi-scale local binary pattern (MLBP) operator that can capture both micro- and macrostructures with the use of pyramid levels. [sent-93, score-0.311]
</p><p>50 Section 4 presents the experiment results on Outex databases and GelSight texture images. [sent-96, score-0.204]
</p><p>51 Local Binary Pattern LBP [2] is a texture operator for gray-scale and rotation invariant texture classification. [sent-99, score-0.444]
</p><p>52 It characterizes local  structure of the texture image by considering a small circularly symmetric neighbor set of P members on a circle  isomufraf gdec esfo,crawmnhiat hivoens dligfhofetlryenthdegifraegyre lnevtaenlfdsod/rocuer st,otedtxhitfeufre rgernastyul-resfvcae cles . [sent-100, score-0.164]
</p><p>53 Intuitively, the larger the R is, the larger the size of the patterns examined; a small R corresponds to microstructures and a large R macrostructures. [sent-662, score-0.199]
</p><p>54 ndly, an efficient iwmipthle (mP,eRn)t =at i(o3n2 ,w4i)t, hth ae lsoiozek oufp thtaeb l oe ookfu ? [sent-684, score-0.208]
</p><p>55 l7e(, iwmipthl e(Pm,eRn)t a=t i(o3n2 ,w4)i,t hth ae lsoiozek uofp thtaeb lleo ookfu ? [sent-699, score-0.186]
</p><p>56 However, this limits the capabilities of using macrostructures with R > 3 as texture features with larger P and R. [sent-720, score-0.332]
</p><p>57 In fact, many texture images in the real world may contain similar microstructures but different macrostructures. [sent-721, score-0.332]
</p><p>58 4 shows an example of two visually very different textures that have similar microstructures but very different macrostructures. [sent-723, score-0.271]
</p><p>59 1 1 12 2 24 4 413 1  (a) (b) (c) (d) Figure 4: Illustration of two textures with siimilar microstructures but very different macrostructures. [sent-724, score-0.271]
</p><p>60 (c) Histogram of LBP values for (P,R) = (16,2) on the original images of texture 1 and 2. [sent-727, score-0.164]
</p><p>61 This represents statistics of the microstructures with R = 2. [sent-728, score-0.198]
</p><p>62 Equivallently this represents statistics of the macrostructures with R = 8 iin the original image. [sent-731, score-0.198]
</p><p>63 To achieve a high classification raate, it is practically desirable to have operators that includde statistics of both microstructures and macrostructures as the texture features  without increasing the values of P aand R. [sent-732, score-0.601]
</p><p>64 The Algorithm As discussed in Section 2, the convenntional LBP may not deal with macrostructures effectively ffoor R > 3. [sent-736, score-0.168]
</p><p>65 By considering histograms of LBP values aat different pyramid levels of the original image, we can takke into consideration both micro- and macrostructurres of different sizes. [sent-743, score-0.125]
</p><p>66 [2] also tries to combine statistics of struuctures of different sizes, its fundamental limitation is that itt does not go beyond R > 3 due to the reasons discussed in the beginning of Section 3 and, therefore, does not take intto consideration statistics of macrostructures. [sent-747, score-0.125]
</p><p>67 , based on statistics of the texture dattabase, but we do not see significant improvement over the simple scheme above, while adding in a new class might change the weights completely. [sent-918, score-0.194]
</p><p>68 The Outex databases contain 2D texture images that are used to compare performance of MLBP with that of other methods. [sent-1141, score-0.204]
</p><p>69 The GelSight images are of real interest for tactile sensing and are used to validate the performance of MLBP. [sent-1142, score-0.226]
</p><p>70 Here we convert GelSight 3D height maps to 2D gray images by using brightness levels to represent the height information. [sent-1143, score-0.378]
</p><p>71 While there is a clear distinction between 2D visual textures such as those in the Outex databases and the 3D surface textures in GelSight, the basic principle of texture classification remains the same. [sent-1144, score-0.536]
</p><p>72 Experiment on Outex Databases The Outex database is a publicly available framework for experimental evaluation of texture analysis algorithms [5]. [sent-1147, score-0.164]
</p><p>73 We are particularly interested in the following two that are most popular for evaluating texture classification algorithms in terms of invariance to gray scales and rotation: n1e. [sent-1149, score-0.264]
</p><p>74 P samples of the 24 textures in FOiuguterxe T5C: 00? [sent-1248, score-0.137]
</p><p>75 Our goal is to maximize the classification rate, defined as the ratio of the number of correctly classified samples to the total number of samples for classification. [sent-1269, score-0.113]
</p><p>76 MR8 is the state-of-the-art statistical algorithm for texture classification. [sent-1307, score-0.164]
</p><p>77 Hence it becomes beneficial for us to use GelSight height images combined with MLBP to classify those textures. [sent-1341, score-0.127]
</p><p>78 Experiment on GelSight Images We obtained 40 classes of GelSight texture images from the GelSight portable device (Fig. [sent-1344, score-0.164]
</p><p>79 Figure 6: GelSight exture dat base with 40 dif erent exture  classes, comprised of, from left to right and up to down, 14 fabrics, 13 foams, 3 tissue papers, 7 sandpapers, 1 plastic and 2 wood textures. [sent-1387, score-0.138]
</p><p>80 The actual images obtained from GelSight are height maps. [sent-1388, score-0.127]
</p><p>81 We convert them to 2D images with brightness of pixels indicating the height levels: the brighter the pixel in the corresponding 2D image, the larger the height is. [sent-1389, score-0.286]
</p><p>82 6 1 1 12 2 24 4 4 6 4  shows samples of the 40 texture classes. [sent-1391, score-0.198]
</p><p>83 Note that the database contains some really similar textures, such as textures 1 and 2, 17 and 19, 15, 16, 18, and 25, etc. [sent-1393, score-0.103]
</p><p>84 Among the 24 samples for each texture class, some are used as the training samples and the rest as testing samples. [sent-1395, score-0.256]
</p><p>85 Table 2 shows the correct classification rate for different numbers of training and testing samples. [sent-1396, score-0.099]
</p><p>86 Table 2: Correct classification rate (%) of MLBP for different numbers of training samples with the highest rate highlighted in bold. [sent-1398, score-0.139]
</p><p>87 Number of training samples  MLBP (P,R) = (16,2)  MLBP (P,R) = (8,1)  per texture  11862 999999. [sent-1399, score-0.198]
</p><p>88 As the number of training samples increases, the correct classification rate is expected to increase as well, as there are more samples to be compared with. [sent-1407, score-0.143]
</p><p>89 In practice, we will find a compromise between the number of training samples used and speed especially when the classification is performed in real time, such as in the case of robotic tactile sensing. [sent-1409, score-0.288]
</p><p>90 With the compliant properties of gel elastomers that mimic human fingers, GelSight is a promising candidate for tactile sensing and material perception. [sent-1413, score-0.367]
</p><p>91 This work focuses on the classification of surface textures, where the texture data is based on height maps attained by touching a surface with a GelSight sensor. [sent-1414, score-0.498]
</p><p>92 Conventional LBP and improved versions such as LBP-HF and DLBP mainly look at microstructures  of textures and overlook the macrostructures that may be important distinguishing features for different textures. [sent-1416, score-0.439]
</p><p>93 In this work, we presented a novel multi-scale operator, MLBP, that takes into consideration both microstructures and macrostructures for feature extraction. [sent-1417, score-0.364]
</p><p>94 To compare our algorithm with current techniques in the visual texture literature, we used the Outex databases. [sent-1419, score-0.164]
</p><p>95 MLBP performed the best among several classical methods for texture classification. [sent-1420, score-0.164]
</p><p>96 We also built a database of GelSight surface textures, with 40 classes of different materials, and achieved a classification rate as high as 99. [sent-1421, score-0.156]
</p><p>97 Although the database is small, the high classification rate indicates that our system is well suited to the task of recognizing high-resolution surface textures, and may help to deliver a rich form of information for robotics. [sent-1423, score-0.156]
</p><p>98 Rotation invariant texture classification using LBP variance (LBPV) with global matching. [sent-1473, score-0.252]
</p><p>99 A statistical approach to texture classification from single images. [sent-1488, score-0.209]
</p><p>100 Xu, Rotation-Invariant texture classification using feature distributions, Pattern Recognition 33 (2000) 43–52. [sent-1493, score-0.209]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gelsight', 0.635), ('mlbp', 0.355), ('lbp', 0.22), ('tactile', 0.187), ('macrostructures', 0.168), ('microstructures', 0.168), ('texture', 0.164), ('outex', 0.133), ('height', 0.127), ('hellinger', 0.116), ('textures', 0.103), ('sensor', 0.103), ('ojala', 0.103), ('membrane', 0.093), ('sandpaper', 0.093), ('surface', 0.081), ('gel', 0.075), ('compliant', 0.066), ('pyramid', 0.06), ('cookie', 0.056), ('denim', 0.056), ('dlbp', 0.056), ('elastomer', 0.056), ('gray', 0.055), ('piece', 0.053), ('pietikainen', 0.048), ('ae', 0.047), ('pressed', 0.046), ('classification', 0.045), ('rotation', 0.044), ('invariant', 0.043), ('databases', 0.04), ('materials', 0.039), ('sensing', 0.039), ('multiresolution', 0.038), ('abce', 0.037), ('adigme', 0.037), ('foams', 0.037), ('hrb', 0.037), ('lbpv', 0.037), ('llbp', 0.037), ('lsoiozek', 0.037), ('nopon', 0.037), ('ofst', 0.037), ('ookfu', 0.037), ('owbtea', 0.037), ('pft', 0.037), ('pyrammid', 0.037), ('sandpapers', 0.037), ('struuctures', 0.037), ('thtaeb', 0.037), ('tsz', 0.037), ('xsa', 0.037), ('levels', 0.037), ('samples', 0.034), ('touched', 0.033), ('maenpaa', 0.033), ('slip', 0.033), ('brightness', 0.032), ('patterns', 0.031), ('displayed', 0.031), ('sss', 0.031), ('exture', 0.031), ('illuminants', 0.031), ('inca', 0.031), ('fabrics', 0.031), ('rate', 0.03), ('statistics', 0.03), ('operator', 0.029), ('fd', 0.029), ('oifm', 0.029), ('wood', 0.029), ('hs', 0.028), ('consideration', 0.028), ('er', 0.028), ('hth', 0.028), ('fc', 0.027), ('binary', 0.027), ('pattern', 0.027), ('aand', 0.026), ('finger', 0.026), ('dfg', 0.026), ('son', 0.025), ('rendered', 0.024), ('plastic', 0.024), ('tthe', 0.024), ('ef', 0.024), ('testing', 0.024), ('rre', 0.023), ('dif', 0.023), ('ale', 0.023), ('touch', 0.023), ('oh', 0.023), ('robots', 0.023), ('delivers', 0.023), ('oe', 0.022), ('re', 0.022), ('ee', 0.022), ('robotic', 0.022), ('adelson', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="391-tfidf-1" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>Author: Rui Li, Edward H. Adelson</p><p>Abstract: Sensing surface textures by touch is a valuable capability for robots. Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface texxtures. The GelSight sensor, when pressed against a surfacce, delivers a height map. This can be treated as an image, aand processed using the tools of visual texture analysis. WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. Our system can correctly categorize materials fromm this database with high accuracy. This suggests that the GGelSight sensor can be useful for material recognition by roobots.</p><p>2 0.13687472 <a title="391-tfidf-2" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>Author: Tim Matthews, Mark S. Nixon, Mahesan Niranjan</p><p>Abstract: We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pairwise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence ofattributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.</p><p>3 0.086950153 <a title="391-tfidf-3" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>Author: Chao Liu, Geifei Yang, Jinwei Gu</p><p>Abstract: We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials, such as wood, fabric, and granite. At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.</p><p>4 0.060983077 <a title="391-tfidf-4" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>Author: Tony Tung, Takashi Matsuyama</p><p>Abstract: This paper presents a novel approach to characterize deformable surface using intrinsic property dynamics. 3D dynamic surfaces representing humans in motion can be obtained using multiple view stereo reconstruction methods or depth cameras. Nowadays these technologies have become capable to capture surface variations in real-time, and give details such as clothing wrinkles and deformations. Assuming repetitive patterns in the deformations, we propose to model complex surface variations using sets of linear dynamical systems (LDS) where observations across time are given by surface intrinsic properties such as local curvatures. We introduce an approach based on bags of dynamical systems, where each surface feature to be represented in the codebook is modeled by a set of LDS equipped with timing structure. Experiments are performed on datasets of real-world dynamical surfaces and show compelling results for description, classification and segmentation.</p><p>5 0.053743131 <a title="391-tfidf-5" href="./cvpr-2013-Ensemble_Video_Object_Cut_in_Highly_Dynamic_Scenes.html">148 cvpr-2013-Ensemble Video Object Cut in Highly Dynamic Scenes</a></p>
<p>Author: Xiaobo Ren, Tony X. Han, Zhihai He</p><p>Abstract: We consider video object cut as an ensemble of framelevel background-foreground object classifiers which fuses information across frames and refine their segmentation results in a collaborative and iterative manner. Our approach addresses the challenging issues of modeling of background with dynamic textures and segmentation of foreground objects from cluttered scenes. We construct patch-level bagof-words background models to effectively capture the background motion and texture dynamics. We propose a foreground salience graph (FSG) to characterize the similarity of an image patch to the bag-of-words background models in the temporal domain and to neighboring image patches in the spatial domain. We incorporate this similarity information into a graph-cut energy minimization framework for foreground object segmentation. The background-foreground classification results at neighboring frames are fused together to construct a foreground probability map to update the graph weights. The resulting object shapes at neighboring frames are also used as constraints to guide the energy minimization process during graph cut. Our extensive experimental results and performance comparisons over a diverse set of challenging videos with dynamic scenes, including the new Change Detection Challenge Dataset, demonstrate that the proposed ensemble video object cut method outperforms various state-ofthe-art algorithms.</p><p>6 0.053512901 <a title="391-tfidf-6" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>7 0.052376803 <a title="391-tfidf-7" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>8 0.047833301 <a title="391-tfidf-8" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>9 0.046893749 <a title="391-tfidf-9" href="./cvpr-2013-What_Object_Motion_Reveals_about_Shape_with_Unknown_BRDF_and_Lighting.html">465 cvpr-2013-What Object Motion Reveals about Shape with Unknown BRDF and Lighting</a></p>
<p>10 0.04540484 <a title="391-tfidf-10" href="./cvpr-2013-Learning_Class-to-Image_Distance_with_Object_Matchings.html">247 cvpr-2013-Learning Class-to-Image Distance with Object Matchings</a></p>
<p>11 0.04540034 <a title="391-tfidf-11" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>12 0.041831721 <a title="391-tfidf-12" href="./cvpr-2013-Statistical_Textural_Distinctiveness_for_Salient_Region_Detection_in_Natural_Images.html">411 cvpr-2013-Statistical Textural Distinctiveness for Salient Region Detection in Natural Images</a></p>
<p>13 0.041789174 <a title="391-tfidf-13" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>14 0.040772077 <a title="391-tfidf-14" href="./cvpr-2013-Deep_Learning_Shape_Priors_for_Object_Segmentation.html">105 cvpr-2013-Deep Learning Shape Priors for Object Segmentation</a></p>
<p>15 0.040144533 <a title="391-tfidf-15" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>16 0.039501254 <a title="391-tfidf-16" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>17 0.038751502 <a title="391-tfidf-17" href="./cvpr-2013-The_SVM-Minus_Similarity_Score_for_Video_Face_Recognition.html">430 cvpr-2013-The SVM-Minus Similarity Score for Video Face Recognition</a></p>
<p>18 0.038451381 <a title="391-tfidf-18" href="./cvpr-2013-Texture_Enhanced_Image_Denoising_via_Gradient_Histogram_Preservation.html">427 cvpr-2013-Texture Enhanced Image Denoising via Gradient Histogram Preservation</a></p>
<p>19 0.037682824 <a title="391-tfidf-19" href="./cvpr-2013-Deformable_Spatial_Pyramid_Matching_for_Fast_Dense_Correspondences.html">107 cvpr-2013-Deformable Spatial Pyramid Matching for Fast Dense Correspondences</a></p>
<p>20 0.037190042 <a title="391-tfidf-20" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.102), (1, 0.029), (2, -0.004), (3, 0.028), (4, 0.014), (5, -0.024), (6, -0.045), (7, -0.006), (8, 0.007), (9, -0.014), (10, -0.023), (11, -0.035), (12, -0.01), (13, -0.032), (14, 0.037), (15, -0.002), (16, 0.03), (17, -0.017), (18, 0.042), (19, 0.016), (20, -0.007), (21, 0.022), (22, 0.006), (23, -0.002), (24, -0.042), (25, -0.019), (26, 0.002), (27, -0.002), (28, -0.009), (29, -0.019), (30, -0.029), (31, 0.004), (32, -0.023), (33, -0.007), (34, 0.002), (35, 0.011), (36, -0.019), (37, 0.03), (38, -0.012), (39, 0.004), (40, 0.006), (41, 0.027), (42, 0.044), (43, 0.05), (44, 0.009), (45, -0.039), (46, 0.023), (47, 0.017), (48, -0.005), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92167723 <a title="391-lsi-1" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>Author: Rui Li, Edward H. Adelson</p><p>Abstract: Sensing surface textures by touch is a valuable capability for robots. Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface texxtures. The GelSight sensor, when pressed against a surfacce, delivers a height map. This can be treated as an image, aand processed using the tools of visual texture analysis. WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. Our system can correctly categorize materials fromm this database with high accuracy. This suggests that the GGelSight sensor can be useful for material recognition by roobots.</p><p>2 0.74489969 <a title="391-lsi-2" href="./cvpr-2013-Learning_Discriminative_Illumination_and_Filters_for_Raw_Material_Classification_with_Optimal_Projections_of_Bidirectional_Texture_Functions.html">251 cvpr-2013-Learning Discriminative Illumination and Filters for Raw Material Classification with Optimal Projections of Bidirectional Texture Functions</a></p>
<p>Author: Chao Liu, Geifei Yang, Jinwei Gu</p><p>Abstract: We present a computational imaging method for raw material classification using features of Bidirectional Texture Functions (BTF). Texture is an intrinsic feature for many materials, such as wood, fabric, and granite. At appropriate scales, even “uniform” materials will also exhibit texture features that can be helpful for recognition, such as paper, metal, and ceramic. To cope with the high-dimensionality of BTFs, in this paper, we proposed to learn discriminative illumination patterns and texture filters, with which we can directly measure optimal projections of BTFs for classification. We also studied the effects of texture rotation and scale variation for material classification. We built an LED-based multispectral dome, with which we have acquired a BTF database of a variety of materials and demonstrated the effectiveness of the proposed approach for material classification.</p><p>3 0.72993493 <a title="391-lsi-3" href="./cvpr-2013-Towards_Contactless%2C_Low-Cost_and_Accurate_3D_Fingerprint_Identification.html">435 cvpr-2013-Towards Contactless, Low-Cost and Accurate 3D Fingerprint Identification</a></p>
<p>Author: Ajay Kumar, Cyril Kwong</p><p>Abstract: In order to avail the benefits of higher user convenience, hygiene, and improved accuracy, contactless 3D fingerprint recognition techniques have recently been introduced. One of the key limitations of these emerging 3D fingerprint technologies to replace the conventional 2D fingerprint system is their bulk and high cost, which mainly results from the use of multiple imaging cameras or structured lighting employed in these systems. This paper details the development of a contactless 3D fingerprint identification system that uses only single camera. We develop a new representation of 3D finger surface features using Finger Surface Codes and illustrate its effectiveness in matching 3D fingerprints. Conventional minutiae representation is extended in 3D space to accurately match the recovered 3D minutiae. Multiple 2D fingerprint images (with varying illumination profile) acquired to build 3D fingerprints can themselves be used recover 2D features for further improving 3D fingerprint identification and has been illustrated in this paper. The experimental results are shown on a database of 240 client fingerprints and confirm the advantages of the single camera based 3D fingerprint identification.</p><p>4 0.69890493 <a title="391-lsi-4" href="./cvpr-2013-Rotation%2C_Scaling_and_Deformation_Invariant_Scattering_for_Texture_Discrimination.html">369 cvpr-2013-Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination</a></p>
<p>Author: Laurent Sifre, Stéphane Mallat</p><p>Abstract: An affine invariant representation is constructed with a cascade of invariants, which preserves information for classification. A joint translation and rotation invariant representation of image patches is calculated with a scattering transform. It is implemented with a deep convolution network, which computes successive wavelet transforms and modulus non-linearities. Invariants to scaling, shearing and small deformations are calculated with linear operators in the scattering domain. State-of-the-art classification results are obtained over texture databases with uncontrolled viewing conditions.</p><p>5 0.64995432 <a title="391-lsi-5" href="./cvpr-2013-Pattern-Driven_Colorization_of_3D_Surfaces.html">327 cvpr-2013-Pattern-Driven Colorization of 3D Surfaces</a></p>
<p>Author: George Leifman, Ayellet Tal</p><p>Abstract: Colorization refers to the process of adding color to black & white images or videos. This paper extends the term to handle surfaces in three dimensions. This is important for applications in which the colors of an object need to be restored and no relevant image exists for texturing it. We focus on surfaces with patterns and propose a novel algorithm for adding colors to these surfaces. The user needs only to scribble a few color strokes on one instance of each pattern, and the system proceeds to automatically colorize the whole surface. For this scheme to work, we address not only the problem of colorization, but also the problem of pattern detection on surfaces.</p><p>6 0.64846116 <a title="391-lsi-6" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<p>7 0.6413976 <a title="391-lsi-7" href="./cvpr-2013-Multi-resolution_Shape_Analysis_via_Non-Euclidean_Wavelets%3A_Applications_to_Mesh_Segmentation_and_Surface_Alignment_Problems.html">297 cvpr-2013-Multi-resolution Shape Analysis via Non-Euclidean Wavelets: Applications to Mesh Segmentation and Surface Alignment Problems</a></p>
<p>8 0.63994563 <a title="391-lsi-8" href="./cvpr-2013-Multi-view_Photometric_Stereo_with_Spatially_Varying_Isotropic_Materials.html">303 cvpr-2013-Multi-view Photometric Stereo with Spatially Varying Isotropic Materials</a></p>
<p>9 0.6344046 <a title="391-lsi-9" href="./cvpr-2013-Local_Fisher_Discriminant_Analysis_for_Pedestrian_Re-identification.html">270 cvpr-2013-Local Fisher Discriminant Analysis for Pedestrian Re-identification</a></p>
<p>10 0.63346964 <a title="391-lsi-10" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>11 0.62395912 <a title="391-lsi-11" href="./cvpr-2013-Axially_Symmetric_3D_Pots_Configuration_System_Using_Axis_of_Symmetry_and_Break_Curve.html">52 cvpr-2013-Axially Symmetric 3D Pots Configuration System Using Axis of Symmetry and Break Curve</a></p>
<p>12 0.61594486 <a title="391-lsi-12" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>13 0.61132425 <a title="391-lsi-13" href="./cvpr-2013-Enriching_Texture_Analysis_with_Semantic_Data.html">146 cvpr-2013-Enriching Texture Analysis with Semantic Data</a></p>
<p>14 0.60903001 <a title="391-lsi-14" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>15 0.60645354 <a title="391-lsi-15" href="./cvpr-2013-Analytic_Bilinear_Appearance_Subspace_Construction_for_Modeling_Image_Irradiance_under_Natural_Illumination_and_Non-Lambertian_Reflectance.html">42 cvpr-2013-Analytic Bilinear Appearance Subspace Construction for Modeling Image Irradiance under Natural Illumination and Non-Lambertian Reflectance</a></p>
<p>16 0.6054126 <a title="391-lsi-16" href="./cvpr-2013-Efficient_Computation_of_Shortest_Path-Concavity_for_3D_Meshes.html">141 cvpr-2013-Efficient Computation of Shortest Path-Concavity for 3D Meshes</a></p>
<p>17 0.60351753 <a title="391-lsi-17" href="./cvpr-2013-Three-Dimensional_Bilateral_Symmetry_Plane_Estimation_in_the_Phase_Domain.html">432 cvpr-2013-Three-Dimensional Bilateral Symmetry Plane Estimation in the Phase Domain</a></p>
<p>18 0.60299653 <a title="391-lsi-18" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>19 0.58977437 <a title="391-lsi-19" href="./cvpr-2013-The_Generalized_Laplacian_Distance_and_Its_Applications_for_Visual_Matching.html">429 cvpr-2013-The Generalized Laplacian Distance and Its Applications for Visual Matching</a></p>
<p>20 0.58827919 <a title="391-lsi-20" href="./cvpr-2013-BRDF_Slices%3A_Accurate_Adaptive_Anisotropic_Appearance_Acquisition.html">54 cvpr-2013-BRDF Slices: Accurate Adaptive Anisotropic Appearance Acquisition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.078), (16, 0.035), (26, 0.047), (28, 0.014), (29, 0.34), (33, 0.179), (65, 0.012), (67, 0.058), (69, 0.044), (77, 0.018), (87, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69373709 <a title="391-lda-1" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>Author: Rui Li, Edward H. Adelson</p><p>Abstract: Sensing surface textures by touch is a valuable capability for robots. Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface texxtures. The GelSight sensor, when pressed against a surfacce, delivers a height map. This can be treated as an image, aand processed using the tools of visual texture analysis. WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. Our system can correctly categorize materials fromm this database with high accuracy. This suggests that the GGelSight sensor can be useful for material recognition by roobots.</p><p>2 0.6800341 <a title="391-lda-2" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>3 0.645953 <a title="391-lda-3" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>4 0.63232642 <a title="391-lda-4" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Amir Averbuch, Shai Dekel</p><p>Abstract: One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms.</p><p>5 0.60592258 <a title="391-lda-5" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>Author: Shaokang Chen, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell</p><p>Abstract: Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once, with fixed clusters used for matching with other image sets. However, this may result in the two closest clusters to represent different characteristics of an object, due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem, we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace, we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error, we approach the nearest point on a Grassmann manifold. Experiments on Honda, ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques, such as Affine Hull based Image Set Distance (AHISD), Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).</p><p>6 0.59042829 <a title="391-lda-6" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>7 0.54196203 <a title="391-lda-7" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>8 0.53832418 <a title="391-lda-8" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>9 0.53743184 <a title="391-lda-9" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>10 0.53471458 <a title="391-lda-10" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>11 0.53431767 <a title="391-lda-11" href="./cvpr-2013-3D_Visual_Proxemics%3A_Recognizing_Human_Interactions_in_3D_from_a_Single_Image.html">4 cvpr-2013-3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</a></p>
<p>12 0.53405309 <a title="391-lda-12" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>13 0.53369671 <a title="391-lda-13" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>14 0.53350824 <a title="391-lda-14" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>15 0.53344822 <a title="391-lda-15" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>16 0.5332765 <a title="391-lda-16" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>17 0.53313369 <a title="391-lda-17" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>18 0.53265631 <a title="391-lda-18" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>19 0.53210801 <a title="391-lda-19" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>20 0.53209609 <a title="391-lda-20" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
