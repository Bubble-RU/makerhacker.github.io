<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-356" href="#">cvpr2013-356</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</h1>
<br/><p>Source: <a title="cvpr-2013-356-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lucey_Representing_and_Discovering_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Patrick Lucey, Alina Bialkowski, Peter Carr, Stuart Morgan, Iain Matthews, Yaser Sheikh</p><p>Abstract: In this paper, we describe a method to represent and discover adversarial group behavior in a continuous domain. In comparison to other types of behavior, adversarial behavior is heavily structured as the location of a player (or agent) is dependent both on their teammates and adversaries, in addition to the tactics or strategies of the team. We present a method which can exploit this relationship through the use of a spatiotemporal basis model. As players constantly change roles during a match, we show that employing a “role-based” representation instead of one based on player “identity” can best exploit the playing structure. As vision-based systems currently do not provide perfect detection/tracking (e.g. missed or false detections), we show that our compact representation can effectively “denoise ” erroneous detections as well as enabling temporal analysis, which was previously prohibitive due to the dimensionality of the signal. To evaluate our approach, we used a fully instrumented field-hockey pitch with 8 fixed highdefinition (HD) cameras and evaluated our approach on approximately 200,000 frames of data from a state-of-the- art real-time player detector and compare it to manually labelled data.</p><p>Reference: <a title="cvpr-2013-356-reference" href="../cvpr2013_reference/cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In comparison to other types of behavior, adversarial behavior is heavily structured as the location of a player (or agent) is dependent both on their teammates and adversaries, in addition to the tactics or strategies of the team. [sent-11, score-0.767]
</p><p>2 As players constantly change roles during a match, we show that employing a “role-based” representation instead of one based on player “identity” can best exploit the playing structure. [sent-13, score-1.126]
</p><p>3 missed or false detections), we show that our compact representation can effectively “denoise ” erroneous detections as well as enabling temporal analysis, which was previously prohibitive due to the dimensionality of the signal. [sent-16, score-0.237]
</p><p>4 To evaluate our approach, we used a fully instrumented field-hockey pitch with 8 fixed highdefinition (HD) cameras and evaluated our approach on approximately 200,000 frames of data from a state-of-the-  art real-time player detector and compare it to manually labelled data. [sent-17, score-0.636]
</p><p>5 In the former case, each individual pursues an individual goal on their own schedule; in the latter, the teams engage in adversarial goal-seeking usually under the synchronized direction of a captain or a coach. [sent-25, score-0.298]
</p><p>6 We can identify a player by their name or number (e. [sent-27, score-0.514]
</p><p>7 , using player identity (1, 2, and 3) the two snapshots will look different as the players have swapped positions. [sent-33, score-0.968]
</p><p>8 emergent patterns of play is critical to understanding the evolving game for fans, players, coaches, and broadcasters (including commentators, camera operators, producers, and game statisticians). [sent-35, score-0.291]
</p><p>9 The behavior of a team may be described by how its members cooperate and contribute in a particular situation. [sent-36, score-0.446]
</p><p>10 In team sports, the overall style of a team can be characterized by a formation: a coarse spatial structure which the  players maintain over the course of the match. [sent-37, score-1.208]
</p><p>11 Additionally, player movements are governed by physical limits, such as acceleration, which makes trajectories smooth over time. [sent-38, score-0.538]
</p><p>12 These two observations suggest significant correlation (and therefore redundancy) in the spatiotemporal signal of player movement data. [sent-39, score-0.64]
</p><p>13 A core contribution of this work is to recover a low-dimensional approximation for a time series of player locations. [sent-40, score-0.514]
</p><p>14 The compact representation is critical for understanding team behavior. [sent-41, score-0.457]
</p><p>15 222777000644  A key insight of this work is that even perfect tracking data is not sufficient for understanding team behavior. [sent-44, score-0.43]
</p><p>16 A formation implicitly defines a set of roles or individual responsibilities which are then distributed amongst the players by the captain or coach. [sent-45, score-0.828]
</p><p>17 In dynamic games like soccer or field hockey, it may be opportunistic for players to swap roles (either temporarily or permanently). [sent-46, score-0.735]
</p><p>18 As a result, when analyzing the strategy of a particular game situation, players are typically identified by the role they are currently playing and not necessarily by an individualistic attribute like name (e. [sent-47, score-0.668]
</p><p>19 Identifying formations and plays quickly from a large repository could enhance sports commentary by highlighting recurrent team strategies and long term trends in a sport. [sent-51, score-0.739]
</p><p>20 We demonstrate our ideas on approximately 200k frames of data acquired from a state-of-the-art realtime player detector [10] and compare it to manually labelled data. [sent-54, score-0.66]
</p><p>21 In the initial work by Intille and Bobick [14], they recognized a single football play, using a Bayesian network to model the interactions between the players trajectories. [sent-66, score-0.433]
</p><p>22 [16] used the global motion of all players in a soccer match to predict where the play will evolve in the short-term. [sent-73, score-0.503]
</p><p>23 [7] proposed a system which aims to track player and ball positions via a vision system for the use of automatic analysis of soccer matches. [sent-75, score-0.554]
</p><p>24 [24] used trajectories of player movement to recognize three types of team offensive patterns. [sent-77, score-1.014]
</p><p>25 [13] also used player trajectories to recognize low-level team activities using a hierarchical parallel semi-Markov model. [sent-80, score-1.001]
</p><p>26 t4omr [ ×105] generated a series O of observations where each observation ecroantesidst ead s oerfi an O(x, oyf) ground tlioocnasti wonh,e a timestamp etr,v aantido a team affiliation estimate τ ∈ {α, β}. [sent-88, score-0.439]
</p><p>27 Amt a any given etsitmime aitnest τan ∈t t, αth,eβ set of detected player locations Ot = {xA, yA, xB , yB , . [sent-89, score-0.544]
</p><p>28 equal to the number of players P because some players may not have been detected and/or background clutter may have been incorrectly classified as a player. [sent-97, score-0.792]
</p><p>29 Typically, the goal is to track all 2P players over the duration of the match. [sent-98, score-0.396]
</p><p>30 In field hockey, that corresponds to 20 players (P = 10 per team ignoring goalkeepers) and two 35 minute long halves. [sent-99, score-0.825]
</p><p>31 The task of tracking all players across  time is equivalent to generating a vector of ordered player  yP]T  locations ptτ = [x1, y1, x2, y2, . [sent-100, score-0.94]
</p><p>32 , xP, for each team τ from the noisy detections Ot at each time instant. [sent-103, score-0.577]
</p><p>33 The particular ordering oetfe players Ois arbitrary, but must be consistent across time. [sent-104, score-0.429]
</p><p>34 Therefore, we will refer to ptτ as a static labeling of player locations. [sent-105, score-0.514]
</p><p>35 It is important to point out that 1These works only capture a portion of the field, making group analysis difficult as all active players are rarely present in the all frames. [sent-106, score-0.432]
</p><p>36 222777000755  switch roles and responsibilities on occasion, for example, the left halfback LH overlaps with the inside left IL to exploit a possible opportunity. [sent-107, score-0.261]
</p><p>37 We manually labelled player location, identity and role at each frame for parts of four games from an international fieldhockey tournament. [sent-110, score-0.834]
</p><p>38 If a player was not detected, an algorithm pwlyil a s soumbseehto owf Ohave to infer the (x, y) location of the unseen player based on spatiotemporal correlations. [sent-112, score-1.125]
</p><p>39 We focus on generic team behaviors and assume any observed arrangement of players from team α could also have been observed for players from team β. [sent-113, score-2.037]
</p><p>40 For any given vector of player locations ptτ, there is an equivalent complement ? [sent-115, score-0.544]
</p><p>41 pτt from rotating all (x, y) locations about the center of the field and swapping the associated team affiliations. [sent-116, score-0.485]
</p><p>42 Formations and Roles In the majority of team sports, the coach or captain designates an overall structure or system of play for a team. [sent-119, score-0.523]
</p><p>43 In field hockey, the structure is described as a formation involving roles or individual responsibilities (see Fig. [sent-120, score-0.405]
</p><p>44 For instance, the 5:3:2 formation defines a set ofroles R = {left ibnasctkan (cLeB,) t,h right 2bafockrm m(RatBi)o,n ledfetf nhaelsfbaascekt o(LfrHo),l sceRnte =r {hlaelfftback (CH), right halfback (RH), inside left (IL), inside right (IR), left wing (LW), center forward (CF), right wing (RW)}. [sent-122, score-0.285]
</p><p>45 Each player is assigned exactly one role, and every role) }is. [sent-123, score-0.514]
</p><p>46 ×  During a match, players may swap roles and temporarily adopt the responsibilities of another player. [sent-126, score-0.686]
</p><p>47 Mathematically, assigning roles is equivalent to permuting the player ordering ptτ. [sent-127, score-0.708]
</p><p>48 d Wesec driebfeisn eth ae players pienr tmeurmtast oofn ro mlaestr rtτ rtτ = xtτptτ (1) By definition, each element xtτ (i, j) is a binary variable, and every column and row in xtτ must sum to one. [sent-129, score-0.396]
</p><p>49 If xtτ (i, j) = 1then player iis assigned role j. [sent-130, score-0.625]
</p><p>50 ptτ, we refer to rtτ as a dynamic labeling of player locations. [sent-133, score-0.514]
</p><p>51 Because the spatial relationships of a formation are defined in terms of roles (and not individualistic attributes like name) and players swap roles during the game, we expect the spatiotemporal patterns in {rτ1, r2τ, . [sent-134, score-1.037]
</p><p>52 As a result, position data rtτ expressed relative to the mean (x, y) location of the team should be even more compressible. [sent-143, score-0.406]
</p><p>53 To test these conjectures, we manually tracked all players over 25000 time-steps (which equates to 8 25000 = 200, 000 frames across 8 cameras), aeqnud aateskse tod a f×iel2d5 hockey expert t0o f assign a rcorleoss sto 8 t chaem player locations in each frame. [sent-144, score-1.055]
</p><p>54 For brevity, we explain the analysis in terms of roles rtτ since the original player ordering ptτ is just a special non-permuted case xtτ = I. [sent-146, score-0.708]
</p><p>55 Incorporating Adversarial Behavior A player’s movements are correlated not only to teammates but to opposition players as well. [sent-178, score-0.452]
</p><p>56 Therefore, we anticipate that player location data can be further compressed 222777000866  FigureCH4RLB. [sent-179, score-0.514]
</p><p>57 if the locations of players on teams A and B are concatenated into a single vector rtAB = [rtA, rtB]T. [sent-181, score-0.524]
</p><p>58 In Figure 4, we show the mean formations for the identity and role representation. [sent-182, score-0.319]
</p><p>59 We can see that the role representation has a more uniform spread between the players, while the identity representation has a more crowded shape, which highlights the constant swapping of roles during a match. [sent-183, score-0.41]
</p><p>60 In terms of compressibility, Table 2 shows that using an adversarial representation gains better compressibility for both cases, and that using both a role and adversarial  representation yields the most compressibility. [sent-184, score-0.568]
</p><p>61 [4], presented a bilinear spatiotemporal basis model which captures and exploits the dependencies across both the spatial and temporal dimensions in an efficient and elegant manner, which can be applied to our problem domain. [sent-189, score-0.309]
</p><p>62 Given we have P players per team, we can form our role-based adversarial representation, x, as a spatiotemporal structure S, given 2P total players sampled at F time instances as  SF×2P=⎣⎢⎡x . [sent-190, score-1.039]
</p><p>63 One way to exploit the regularity in spatiotemporal data is to represent the 2D formation or shape at each time instance as a linear combination of a small number of shape? [sent-197, score-0.278]
</p><p>64 This equation d Kescribes the bilinear spatiotemporal basis, which contains both shape and trajectory bases linked together by a common set of coefficients. [sent-213, score-0.266]
</p><p>65 Plot showing the mean reconstruction error of the test data as the number of temporal basis (Kt) and spatial basis (Ks) vary for 5 second plays (i. [sent-224, score-0.28]
</p><p>66 We magnified the plot to only show the first 10 temporal basis to highlight that only only Kt = 3 is required to represent coarse player motion. [sent-227, score-0.638]
</p><p>67 We now address the problem of automatically assigning roles to an arbitrary ordering of player locations ptτ. [sent-230, score-0.738]
</p><p>68 Assuming a suitably similar vector rτ of player locations in role order exists, we define the optimal assignment of roles as the permutation matrix xtτ? [sent-231, score-0.912]
</p><p>69 Using the mean formation (see Figure 4) is a reasonable initialization as the team should maintain that basic formation in most circumstances. [sent-243, score-0.714]
</p><p>70 To incorporate these semantics, we used a codebook of formations which consists of every formation within our training set. [sent-245, score-0.304]
</p><p>71 In terms of assignment performance on the test set, this approach works very well compared to using the mean formation for both the identity and role labels as can be seen in Table 3. [sent-256, score-0.392]
</p><p>72 222777001088  tion results in terms of 3D geometry, where players are coarsely modeled as cylinders. [sent-262, score-0.396]
</p><p>73 Based on these detections, we assign player role for each team. [sent-263, score-0.625]
</p><p>74 To evaluate our approach, we employed a real-time state-of-the-art player detector [10] that detects player positions at 30fps by interpreting background subtraction results based on the coarse 3D geometry of a person (Figure 7). [sent-272, score-1.052]
</p><p>75 Once the locations of all players were determined, we classified the players into their respective teams using a color model for each team. [sent-273, score-0.92]
</p><p>76 Each player image was represented as a histogram in LAB color space and K-means clustering using the Bhattacharyya distance was performed to learn a generalized model for each team and camera. [sent-274, score-0.92]
</p><p>77 The precision and recall rates for the detector and the team affiliation are given in the left side of Table 4. [sent-275, score-0.463]
</p><p>78 In this work, we consider a detection to be made if a player was was within two meters of a ground-truth label. [sent-276, score-0.514]
</p><p>79 the part of the field most of the players are located). [sent-281, score-0.419]
</p><p>80 As the centroids of both the clean (solid) and noisy (dashed) of both teams (blue=team1, red=team2) are roughly equivalent, we learn a mapping matrix using linear regression to find a formation from the training set which can best describe the noisy test formation. [sent-285, score-0.367]
</p><p>81 Using this assumption, we can obtain a reasonable prototypical formation to make our  player assignments. [sent-289, score-0.668]
</p><p>82 To counter this, we employed an “exhaustive” approach, where if we have fewer detections than the number of players in the prototype, we find all the possible combinations that the labels could be assigned then use the combination which yielded the lowest cost from the assignments made. [sent-292, score-0.577]
</p><p>83 Conversely, if we had more detections than the number of players, we find all the possible combinations that the detections could be and then use the combination of detections which had the lowest cost. [sent-293, score-0.439]
</p><p>84 Given our noisy detections (black), using our bilinear model we can estimate the trajectory of each player over time. [sent-319, score-0.854]
</p><p>85 However, sometimes we get false positives which means that even though we may get 10 detections for a team we may only have 7 or 8 valid candidates. [sent-329, score-0.535]
</p><p>86 Denoising the Detections While our precision and recall rates from the detector are relatively high, to do useful analysis we need a continuous estimate of the player label at each time step to do formation and play analysis. [sent-334, score-0.759]
</p><p>87 Given the spatial bases, the bilinear coefficients and an initial estimate of the player labels, we can use an Expectation Maximization (EM) algorithm to denoise the detections. [sent-336, score-0.645]
</p><p>88 As the recall rate of the denoised data is 100%, we are interested to see how precise our method is in inferring player position based on their label. [sent-341, score-0.587]
</p><p>89 Precision accuracy vs the distance threshold from ground-truth for: (left) the overall detections, (right) the detections based on team affiliation. [sent-349, score-0.535]
</p><p>90 precision rate for the detections and the denoised detections against a distance threshold - that is, the minimum distance a player had to be to ground-truth to be recognized as a correct detection). [sent-351, score-0.845]
</p><p>91 As can be seen from these figures, the detections from the player detector are very accurate and do not vary with respect to the error threshold (i. [sent-353, score-0.667]
</p><p>92 it either detects a player very precisely or not at all). [sent-355, score-0.514]
</p><p>93 Formation and Play Analysis To check the usefulness of our cleaned-up signal, we conducted cluster analysis on both static formations and dynamic plays to see whether we could replicate what could achieve with manually labelled data. [sent-359, score-0.33]
</p><p>94 From the  figure it can be seen that despite small differences, we go close to replicating what we get from manually labelled data formations 1 correspond and 3 and 2 are reversed. [sent-364, score-0.248]
</p><p>95 The x’s and the o’s refer the position of the player at the end of the 10 second play. [sent-373, score-0.514]
</p><p>96 Summary and Future Work In this paper, we presented a representation which utilized player role labels to exploit the heavy spatiotemporal correlations that exist within adversarial domains. [sent-375, score-0.899]
</p><p>97 As this representation is highly correlated in both space and time, we showed that a spatiotemporal bilinear basis model can leverage this trait to compress the incoming signal by up to two orders of magnitude without much loss of information. [sent-376, score-0.315]
</p><p>98 Our final contribution of this paper was the use of the bilinear model to effectively clean up noisy player detections from a state-of-the-art detector, which enables analysis of static formations as well as temporal plays. [sent-377, score-1.004]
</p><p>99 The implications  of this work are important, as having the ability to identify formations  and plays from a large repository  realtime commentary  can enhance  in sports by helping highlight recur-  rent team strategies and long-term trends. [sent-379, score-0.763]
</p><p>100 The process of  post-game play annotations , which coaches and their teams spend hours performing manually could be automated. [sent-380, score-0.248]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('player', 0.514), ('team', 0.406), ('players', 0.396), ('roles', 0.161), ('formation', 0.154), ('formations', 0.15), ('adversarial', 0.15), ('detections', 0.129), ('role', 0.111), ('compressibility', 0.103), ('game', 0.1), ('teams', 0.098), ('spatiotemporal', 0.097), ('bilinear', 0.088), ('pt', 0.087), ('hockey', 0.082), ('plays', 0.082), ('trajectory', 0.081), ('basis', 0.074), ('denoised', 0.073), ('assignment', 0.069), ('sports', 0.068), ('play', 0.067), ('kt', 0.067), ('responsibilities', 0.067), ('ks', 0.066), ('rt', 0.066), ('labelled', 0.065), ('identity', 0.058), ('xt', 0.056), ('games', 0.053), ('akhter', 0.052), ('temporal', 0.05), ('captain', 0.05), ('coaches', 0.05), ('hungarian', 0.05), ('wing', 0.049), ('sport', 0.048), ('prototype', 0.047), ('activity', 0.046), ('offensive', 0.044), ('denoise', 0.043), ('noisy', 0.042), ('compressible', 0.041), ('cf', 0.041), ('lw', 0.041), ('behavior', 0.04), ('soccer', 0.04), ('football', 0.037), ('group', 0.036), ('swap', 0.035), ('khan', 0.034), ('ordering', 0.033), ('affiliation', 0.033), ('alina', 0.033), ('beetz', 0.033), ('chrbl', 0.033), ('commentary', 0.033), ('halfback', 0.033), ('individualistic', 0.033), ('sadilek', 0.033), ('stuart', 0.033), ('tactics', 0.033), ('manually', 0.033), ('crowd', 0.033), ('individuals', 0.032), ('rw', 0.031), ('clean', 0.031), ('missed', 0.031), ('activities', 0.031), ('locations', 0.03), ('hervieu', 0.03), ('carr', 0.03), ('perse', 0.03), ('teammates', 0.03), ('yaser', 0.03), ('signal', 0.029), ('playing', 0.028), ('sheikh', 0.028), ('combinations', 0.028), ('event', 0.028), ('representation', 0.027), ('permutation', 0.027), ('behaviors', 0.027), ('temporarily', 0.027), ('intille', 0.027), ('regularity', 0.027), ('recognize', 0.026), ('swapping', 0.026), ('opposition', 0.026), ('american', 0.025), ('focussed', 0.025), ('raw', 0.024), ('understanding', 0.024), ('detector', 0.024), ('trajectories', 0.024), ('realtime', 0.024), ('lh', 0.024), ('lowest', 0.024), ('field', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000019 <a title="356-tfidf-1" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>Author: Patrick Lucey, Alina Bialkowski, Peter Carr, Stuart Morgan, Iain Matthews, Yaser Sheikh</p><p>Abstract: In this paper, we describe a method to represent and discover adversarial group behavior in a continuous domain. In comparison to other types of behavior, adversarial behavior is heavily structured as the location of a player (or agent) is dependent both on their teammates and adversaries, in addition to the tactics or strategies of the team. We present a method which can exploit this relationship through the use of a spatiotemporal basis model. As players constantly change roles during a match, we show that employing a “role-based” representation instead of one based on player “identity” can best exploit the playing structure. As vision-based systems currently do not provide perfect detection/tracking (e.g. missed or false detections), we show that our compact representation can effectively “denoise ” erroneous detections as well as enabling temporal analysis, which was previously prohibitive due to the dimensionality of the signal. To evaluate our approach, we used a fully instrumented field-hockey pitch with 8 fixed highdefinition (HD) cameras and evaluated our approach on approximately 200,000 frames of data from a state-of-the- art real-time player detector and compare it to manually labelled data.</p><p>2 0.59967273 <a title="356-tfidf-2" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>Author: Jingchen Liu, Peter Carr, Robert T. Collins, Yanxi Liu</p><p>Abstract: We employ hierarchical data association to track players in team sports. Player movements are often complex and highly correlated with both nearby and distant players. A single model would require many degrees of freedom to represent the full motion diversity and could be difficult to use in practice. Instead, we introduce a set of Game Context Features extracted from noisy detections to describe the current state of the match, such as how the players are spatially distributed. Our assumption is that players react to the current situation in only a finite number of ways. As a result, we are able to select an appropriate simplified affinity model for each player and time instant using a random decisionforest based on current track and game contextfeatures. Our context-conditioned motion models implicitly incorporate complex inter-object correlations while remaining tractable. We demonstrate significant performance improvements over existing multi-target tracking algorithms on basketball and field hockey sequences several minutes in duration and containing 10 and 20 players respectively.</p><p>3 0.16718905 <a title="356-tfidf-3" href="./cvpr-2013-Fine-Grained_Crowdsourcing_for_Fine-Grained_Recognition.html">174 cvpr-2013-Fine-Grained Crowdsourcing for Fine-Grained Recognition</a></p>
<p>Author: Jia Deng, Jonathan Krause, Li Fei-Fei</p><p>Abstract: Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles ” that reveals discriminative features humans use. The player’s goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ( “bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.</p><p>4 0.15287368 <a title="356-tfidf-4" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>Author: Vignesh Ramanathan, Bangpeng Yao, Li Fei-Fei</p><p>Abstract: We deal with the problem of recognizing social roles played by people in an event. Social roles are governed by human interactions, and form a fundamental component of human event description. We focus on a weakly supervised setting, where we are provided different videos belonging to an event class, without training role labels. Since social roles are described by the interaction between people in an event, we propose a Conditional Random Field to model the inter-role interactions, along with person specific social descriptors. We develop tractable variational inference to simultaneously infer model weights, as well as role assignment to all people in the videos. We also present a novel YouTube social roles dataset with ground truth role annotations, and introduce annotations on a subset of videos from the TRECVID-MED11 [1] event kits for evaluation purposes. The performance of the model is compared against different baseline methods on these datasets.</p><p>5 0.12614408 <a title="356-tfidf-5" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>Author: Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</p><p>Abstract: We present data-driven techniques to augment Bag of Words (BoW) models, which allow for more robust modeling and recognition of complex long-term activities, especially when the structure and topology of the activities are not known a priori. Our approach specifically addresses the limitations of standard BoW approaches, which fail to represent the underlying temporal and causal information that is inherent in activity streams. In addition, we also propose the use ofrandomly sampled regular expressions to discover and encode patterns in activities. We demonstrate the effectiveness of our approach in experimental evaluations where we successfully recognize activities and detect anomalies in four complex datasets.</p><p>6 0.11916425 <a title="356-tfidf-6" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>7 0.10731428 <a title="356-tfidf-7" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>8 0.093762182 <a title="356-tfidf-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.078314707 <a title="356-tfidf-9" href="./cvpr-2013-Non-rigid_Structure_from_Motion_with_Diffusion_Maps_Prior.html">306 cvpr-2013-Non-rigid Structure from Motion with Diffusion Maps Prior</a></p>
<p>10 0.075958863 <a title="356-tfidf-10" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>11 0.072573625 <a title="356-tfidf-11" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>12 0.070054755 <a title="356-tfidf-12" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>13 0.069989853 <a title="356-tfidf-13" href="./cvpr-2013-Bilinear_Programming_for_Human_Activity_Recognition_with_Unknown_MRF_Graphs.html">62 cvpr-2013-Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</a></p>
<p>14 0.068256684 <a title="356-tfidf-14" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>15 0.068021052 <a title="356-tfidf-15" href="./cvpr-2013-Online_Dominant_and_Anomalous_Behavior_Detection_in_Videos.html">313 cvpr-2013-Online Dominant and Anomalous Behavior Detection in Videos</a></p>
<p>16 0.067697912 <a title="356-tfidf-16" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>17 0.064132161 <a title="356-tfidf-17" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>18 0.062010188 <a title="356-tfidf-18" href="./cvpr-2013-Exploring_Weak_Stabilization_for_Motion_Feature_Extraction.html">158 cvpr-2013-Exploring Weak Stabilization for Motion Feature Extraction</a></p>
<p>19 0.061992869 <a title="356-tfidf-19" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>20 0.060634151 <a title="356-tfidf-20" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, -0.012), (2, -0.016), (3, -0.08), (4, -0.06), (5, -0.013), (6, 0.003), (7, -0.092), (8, -0.007), (9, 0.108), (10, 0.041), (11, -0.041), (12, -0.007), (13, -0.062), (14, 0.061), (15, 0.071), (16, 0.001), (17, 0.11), (18, 0.016), (19, -0.127), (20, 0.032), (21, 0.086), (22, -0.049), (23, 0.132), (24, 0.067), (25, 0.108), (26, 0.118), (27, -0.21), (28, -0.065), (29, -0.2), (30, -0.008), (31, -0.027), (32, -0.083), (33, -0.181), (34, 0.163), (35, -0.104), (36, -0.144), (37, -0.162), (38, 0.037), (39, 0.123), (40, 0.094), (41, -0.184), (42, -0.024), (43, -0.011), (44, -0.018), (45, 0.039), (46, 0.092), (47, -0.028), (48, -0.06), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94588131 <a title="356-lsi-1" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>Author: Patrick Lucey, Alina Bialkowski, Peter Carr, Stuart Morgan, Iain Matthews, Yaser Sheikh</p><p>Abstract: In this paper, we describe a method to represent and discover adversarial group behavior in a continuous domain. In comparison to other types of behavior, adversarial behavior is heavily structured as the location of a player (or agent) is dependent both on their teammates and adversaries, in addition to the tactics or strategies of the team. We present a method which can exploit this relationship through the use of a spatiotemporal basis model. As players constantly change roles during a match, we show that employing a “role-based” representation instead of one based on player “identity” can best exploit the playing structure. As vision-based systems currently do not provide perfect detection/tracking (e.g. missed or false detections), we show that our compact representation can effectively “denoise ” erroneous detections as well as enabling temporal analysis, which was previously prohibitive due to the dimensionality of the signal. To evaluate our approach, we used a fully instrumented field-hockey pitch with 8 fixed highdefinition (HD) cameras and evaluated our approach on approximately 200,000 frames of data from a state-of-the- art real-time player detector and compare it to manually labelled data.</p><p>2 0.81440991 <a title="356-lsi-2" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>Author: Jingchen Liu, Peter Carr, Robert T. Collins, Yanxi Liu</p><p>Abstract: We employ hierarchical data association to track players in team sports. Player movements are often complex and highly correlated with both nearby and distant players. A single model would require many degrees of freedom to represent the full motion diversity and could be difficult to use in practice. Instead, we introduce a set of Game Context Features extracted from noisy detections to describe the current state of the match, such as how the players are spatially distributed. Our assumption is that players react to the current situation in only a finite number of ways. As a result, we are able to select an appropriate simplified affinity model for each player and time instant using a random decisionforest based on current track and game contextfeatures. Our context-conditioned motion models implicitly incorporate complex inter-object correlations while remaining tractable. We demonstrate significant performance improvements over existing multi-target tracking algorithms on basketball and field hockey sequences several minutes in duration and containing 10 and 20 players respectively.</p><p>3 0.63033724 <a title="356-lsi-3" href="./cvpr-2013-Fine-Grained_Crowdsourcing_for_Fine-Grained_Recognition.html">174 cvpr-2013-Fine-Grained Crowdsourcing for Fine-Grained Recognition</a></p>
<p>Author: Jia Deng, Jonathan Krause, Li Fei-Fei</p><p>Abstract: Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles ” that reveals discriminative features humans use. The player’s goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions ( “bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.</p><p>4 0.54927176 <a title="356-lsi-4" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>Author: Xinchu Shi, Haibin Ling, Junling Xing, Weiming Hu</p><p>Abstract: In this paper we formulate multi-target tracking (MTT) as a rank-1 tensor approximation problem and propose an ?1 norm tensor power iteration solution. In particular, a high order tensor is constructed based on trajectories in the time window, with each tensor element as the affinity of the corresponding trajectory candidate. The local assignment variables are the ?1 normalized vectors, which are used to approximate the rank-1 tensor. Our approach provides a flexible and effective formulation where both pairwise and high-order association energies can be used expediently. We also show the close relation between our formulation and the multi-dimensional assignment (MDA) model. To solve the optimization in the rank-1 tensor approximation, we propose an algorithm that iteratively powers the intermediate solution followed by an ?1 normalization. Aside from effectively capturing high-order motion information, the proposed solver runs efficiently with proved convergence. The experimental validations are conducted on two challenging datasets and our method demonstrates promising performances on both.</p><p>5 0.45213258 <a title="356-lsi-5" href="./cvpr-2013-Detection-_and_Trajectory-Level_Exclusion_in_Multiple_Object_Tracking.html">121 cvpr-2013-Detection- and Trajectory-Level Exclusion in Multiple Object Tracking</a></p>
<p>Author: Anton Milan, Konrad Schindler, Stefan Roth</p><p>Abstract: When tracking multiple targets in crowded scenarios, modeling mutual exclusion between distinct targets becomes important at two levels: (1) in data association, each target observation should support at most one trajectory and each trajectory should be assigned at most one observation per frame; (2) in trajectory estimation, two trajectories should remain spatially separated at all times to avoid collisions. Yet, existing trackers often sidestep these important constraints. We address this using a mixed discrete-continuous conditional randomfield (CRF) that explicitly models both types of constraints: Exclusion between conflicting observations with supermodular pairwise terms, and exclusion between trajectories by generalizing global label costs to suppress the co-occurrence of incompatible labels (trajectories). We develop an expansion move-based MAP estimation scheme that handles both non-submodular constraints and pairwise global label costs. Furthermore, we perform a statistical analysis of ground-truth trajectories to derive appropriate CRF potentials for modeling data fidelity, target dynamics, and inter-target occlusion.</p><p>6 0.42121321 <a title="356-lsi-6" href="./cvpr-2013-Social_Role_Discovery_in_Human_Events.html">402 cvpr-2013-Social Role Discovery in Human Events</a></p>
<p>7 0.36442247 <a title="356-lsi-7" href="./cvpr-2013-Augmenting_Bag-of-Words%3A_Data-Driven_Discovery_of_Temporal_and_Structural_Information_for_Activity_Recognition.html">49 cvpr-2013-Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</a></p>
<p>8 0.35676166 <a title="356-lsi-8" href="./cvpr-2013-Fast_Rigid_Motion_Segmentation_via_Incrementally-Complex_Local_Models.html">170 cvpr-2013-Fast Rigid Motion Segmentation via Incrementally-Complex Local Models</a></p>
<p>9 0.34787545 <a title="356-lsi-9" href="./cvpr-2013-Hierarchical_Video_Representation_with_Trajectory_Binary_Partition_Tree.html">203 cvpr-2013-Hierarchical Video Representation with Trajectory Binary Partition Tree</a></p>
<p>10 0.33429375 <a title="356-lsi-10" href="./cvpr-2013-Multi-agent_Event_Detection%3A_Localization_and_Role_Assignment.html">292 cvpr-2013-Multi-agent Event Detection: Localization and Role Assignment</a></p>
<p>11 0.32339317 <a title="356-lsi-11" href="./cvpr-2013-Decoding_Children%27s_Social_Behavior.html">103 cvpr-2013-Decoding Children's Social Behavior</a></p>
<p>12 0.32161584 <a title="356-lsi-12" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>13 0.31775773 <a title="356-lsi-13" href="./cvpr-2013-Hypergraphs_for_Joint_Multi-view_Reconstruction_and_Multi-object_Tracking.html">209 cvpr-2013-Hypergraphs for Joint Multi-view Reconstruction and Multi-object Tracking</a></p>
<p>14 0.30212703 <a title="356-lsi-14" href="./cvpr-2013-Detecting_Pulse_from_Head_Motions_in_Video.html">118 cvpr-2013-Detecting Pulse from Head Motions in Video</a></p>
<p>15 0.28914866 <a title="356-lsi-15" href="./cvpr-2013-Tracking_People_and_Their_Objects.html">440 cvpr-2013-Tracking People and Their Objects</a></p>
<p>16 0.28790691 <a title="356-lsi-16" href="./cvpr-2013-Finding_Group_Interactions_in_Social_Clutter.html">172 cvpr-2013-Finding Group Interactions in Social Clutter</a></p>
<p>17 0.27369544 <a title="356-lsi-17" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>18 0.26838079 <a title="356-lsi-18" href="./cvpr-2013-Harry_Potter%27s_Marauder%27s_Map%3A_Localizing_and_Tracking_Multiple_Persons-of-Interest_by_Nonnegative_Discretization.html">199 cvpr-2013-Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization</a></p>
<p>19 0.25704116 <a title="356-lsi-19" href="./cvpr-2013-Recognizing_Activities_via_Bag_of_Words_for_Attribute_Dynamics.html">348 cvpr-2013-Recognizing Activities via Bag of Words for Attribute Dynamics</a></p>
<p>20 0.25029325 <a title="356-lsi-20" href="./cvpr-2013-Relative_Hidden_Markov_Models_for_Evaluating_Motion_Skill.html">353 cvpr-2013-Relative Hidden Markov Models for Evaluating Motion Skill</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.102), (16, 0.033), (19, 0.258), (26, 0.044), (28, 0.011), (33, 0.203), (67, 0.086), (69, 0.052), (77, 0.026), (86, 0.023), (87, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.844338 <a title="356-lda-1" href="./cvpr-2013-What%27s_in_a_Name%3F_First_Names_as_Facial_Attributes.html">463 cvpr-2013-What's in a Name? First Names as Facial Attributes</a></p>
<p>Author: Huizhong Chen, Andrew C. Gallagher, Bernd Girod</p><p>Abstract: This paper introduces a new idea in describing people using their first names, i.e., the name assigned at birth. We show that describing people in terms of similarity to a vector of possible first names is a powerful description of facial appearance that can be used for face naming and building facial attribute classifiers. We build models for 100 common first names used in the United States and for each pair, construct a pairwise firstname classifier. These classifiers are built using training images downloaded from the internet, with no additional user interaction. This gives our approach important advantages in building practical systems that do not require additional human intervention for labeling. We use the scores from each pairwise name classifier as a set of facial attributes. We show several surprising results. Our name attributes predict the correct first names of test faces at rates far greater than chance. The name attributes are applied to gender recognition and to age classification, outperforming state-of-the-art methods with all training images automatically gathered from the internet.</p><p>same-paper 2 0.80522066 <a title="356-lda-2" href="./cvpr-2013-Representing_and_Discovering_Adversarial_Team_Behaviors_Using_Player_Roles.html">356 cvpr-2013-Representing and Discovering Adversarial Team Behaviors Using Player Roles</a></p>
<p>Author: Patrick Lucey, Alina Bialkowski, Peter Carr, Stuart Morgan, Iain Matthews, Yaser Sheikh</p><p>Abstract: In this paper, we describe a method to represent and discover adversarial group behavior in a continuous domain. In comparison to other types of behavior, adversarial behavior is heavily structured as the location of a player (or agent) is dependent both on their teammates and adversaries, in addition to the tactics or strategies of the team. We present a method which can exploit this relationship through the use of a spatiotemporal basis model. As players constantly change roles during a match, we show that employing a “role-based” representation instead of one based on player “identity” can best exploit the playing structure. As vision-based systems currently do not provide perfect detection/tracking (e.g. missed or false detections), we show that our compact representation can effectively “denoise ” erroneous detections as well as enabling temporal analysis, which was previously prohibitive due to the dimensionality of the signal. To evaluate our approach, we used a fully instrumented field-hockey pitch with 8 fixed highdefinition (HD) cameras and evaluated our approach on approximately 200,000 frames of data from a state-of-the- art real-time player detector and compare it to manually labelled data.</p><p>3 0.77445328 <a title="356-lda-3" href="./cvpr-2013-Hallucinated_Humans_as_the_Hidden_Context_for_Labeling_3D_Scenes.html">197 cvpr-2013-Hallucinated Humans as the Hidden Context for Labeling 3D Scenes</a></p>
<p>Author: Yun Jiang, Hema Koppula, Ashutosh Saxena</p><p>Abstract: For scene understanding, one popular approach has been to model the object-object relationships. In this paper, we hypothesize that such relationships are only an artifact of certain hidden factors, such as humans. For example, the objects, monitor and keyboard, are strongly spatially correlated only because a human types on the keyboard while watching the monitor. Our goal is to learn this hidden human context (i.e., the human-object relationships), and also use it as a cue for labeling the scenes. We present Infinite Factored Topic Model (IFTM), where we consider a scene as being generated from two types of topics: human configurations and human-object relationships. This enables our algorithm to hallucinate the possible configurations of the humans in the scene parsimoniously. Given only a dataset of scenes containing objects but not humans, we show that our algorithm can recover the human object relationships. We then test our algorithm on the task ofattribute and object labeling in 3D scenes and show consistent improvements over the state-of-the-art.</p><p>4 0.71946919 <a title="356-lda-4" href="./cvpr-2013-Block_and_Group_Regularized_Sparse_Modeling_for_Dictionary_Learning.html">66 cvpr-2013-Block and Group Regularized Sparse Modeling for Dictionary Learning</a></p>
<p>Author: Yu-Tseh Chi, Mohsen Ali, Ajit Rajwade, Jeffrey Ho</p><p>Abstract: This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppression Dictionary Learning (ICS-DL) algorithm. An important and distinguishing feature of the proposed framework is that all dictionary blocks are trained simultaneously with respect to each data group while the intra-block coherence being explicitly minimized as an important objective. We provide both empirical evidence and heuristic support for this feature that can be considered as a direct consequence of incorporating both the group structure for the input data and the block structure for the dictionary in the learning process. The optimization problems for both the dictionary learning and sparse coding can be solved efficiently using block-gradient descent, and the details of the optimization algorithms are presented. We evaluate the proposed methods using well-known datasets, and favorable comparisons with state-of-the-art dictionary learning methods demonstrate the viability and validity of the proposed framework.</p><p>5 0.71580029 <a title="356-lda-5" href="./cvpr-2013-Sample-Specific_Late_Fusion_for_Visual_Category_Recognition.html">377 cvpr-2013-Sample-Specific Late Fusion for Visual Category Recognition</a></p>
<p>Author: Dong Liu, Kuan-Ting Lai, Guangnan Ye, Ming-Syan Chen, Shih-Fu Chang</p><p>Abstract: Late fusion addresses the problem of combining the prediction scores of multiple classifiers, in which each score is predicted by a classifier trained with a specific feature. However, the existing methods generally use a fixed fusion weight for all the scores of a classifier, and thus fail to optimally determine the fusion weight for the individual samples. In this paper, we propose a sample-specific late fusion method to address this issue. Specifically, we cast the problem into an information propagation process which propagates the fusion weights learned on the labeled samples to individual unlabeled samples, while enforcing that positive samples have higher fusion scores than negative samples. In this process, we identify the optimal fusion weights for each sample and push positive samples to top positions in the fusion score rank list. We formulate our problem as a L∞ norm constrained optimization problem and apply the Alternating Direction Method of Multipliers for the optimization. Extensive experiment results on various visual categorization tasks show that the proposed method consis- tently and significantly beats the state-of-the-art late fusion methods. To the best knowledge, this is the first method supporting sample-specific fusion weight learning.</p><p>6 0.71547276 <a title="356-lda-6" href="./cvpr-2013-Blessing_of_Dimensionality%3A_High-Dimensional_Feature_and_Its_Efficient_Compression_for_Face_Verification.html">64 cvpr-2013-Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification</a></p>
<p>7 0.71347469 <a title="356-lda-7" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>8 0.6936571 <a title="356-lda-8" href="./cvpr-2013-Tracking_Sports_Players_with_Context-Conditioned_Motion_Models.html">441 cvpr-2013-Tracking Sports Players with Context-Conditioned Motion Models</a></p>
<p>9 0.69136518 <a title="356-lda-9" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>10 0.68845254 <a title="356-lda-10" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>11 0.68653977 <a title="356-lda-11" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>12 0.68637151 <a title="356-lda-12" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>13 0.68541282 <a title="356-lda-13" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>14 0.68535995 <a title="356-lda-14" href="./cvpr-2013-Modeling_Mutual_Visibility_Relationship_in_Pedestrian_Detection.html">288 cvpr-2013-Modeling Mutual Visibility Relationship in Pedestrian Detection</a></p>
<p>15 0.6843363 <a title="356-lda-15" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>16 0.68425316 <a title="356-lda-16" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>17 0.68402267 <a title="356-lda-17" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>18 0.68395782 <a title="356-lda-18" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>19 0.68220133 <a title="356-lda-19" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>20 0.68180549 <a title="356-lda-20" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
