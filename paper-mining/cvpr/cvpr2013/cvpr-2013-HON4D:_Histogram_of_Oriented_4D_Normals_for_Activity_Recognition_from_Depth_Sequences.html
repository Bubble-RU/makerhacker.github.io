<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-196" href="#">cvpr2013-196</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</h1>
<br/><p>Source: <a title="cvpr-2013-196-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oreifej_HON4D_Histogram_of_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><p>Reference: <a title="cvpr-2013-196-reference" href="../cvpr2013_reference/cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract We present a new descriptor for activity recognition from videos acquired by a depth sensor. [sent-3, score-0.546]
</p><p>2 In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. [sent-5, score-0.891]
</p><p>3 We initialize the projectors using the vertices of a regular polychoron. [sent-7, score-0.694]
</p><p>4 Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. [sent-8, score-1.426]
</p><p>5 Through extensive experiments, we demonstrate that our descriptor better captures the joint  shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks. [sent-9, score-0.495]
</p><p>6 However, the recent emergence of low-cost depth sensors such as Kinect [18], triggered significant attention to revisit problems such as object detection and activity recognition using depth images as input instead of color. [sent-13, score-0.695]
</p><p>7 Compared with conventional color images, depth maps provide several advantages. [sent-14, score-0.274]
</p><p>8 For example, depth maps reflect pure geometry and shape cues, which can often be more discriminative than color and texture in many problems including segmentation, object detection, and activity recognition. [sent-15, score-0.523]
</p><p>9 Moreover, depth images are insensitive to changes in lighting conditions. [sent-16, score-0.274]
</p><p>10 In this context, it seems natural to employ depth data in many computer vision problems. [sent-17, score-0.274]
</p><p>11 The surface normals capture the shape cues at a specific time instance, while the change in the surface normal over time captures the motion cues. [sent-20, score-0.866]
</p><p>12 Note that in the figure we illustrate 3D surface normals since it is difficult to visualize the 4D normals used in the paper. [sent-22, score-0.532]
</p><p>13 ventional RGB-based methods would also perform well in depth sequences? [sent-23, score-0.274]
</p><p>14 In activity recognition, which is the topic of this paper, two significant aspects arise when adopting conventional color-based methods for depth sequences. [sent-24, score-0.421]
</p><p>15 First, the captured depth images are often contaminated with undefined depth points, which appear in the sequence as spatially and temporally discontinues black regions [1]. [sent-25, score-0.653]
</p><p>16 To handle that, recent approaches resorted to selecting the informative points using the joints from a skeleton detector [18], as in [24], or using a discriminative sampling scheme as in [23]. [sent-29, score-0.342]
</p><p>17 7 7 7 1 1 1 64 464  Second, and more importantly, the depth images provide natural surfaces which can be exploited to capture the geometrical features of the observed scene in a rich descriptor. [sent-30, score-0.314]
</p><p>18 For instance, it was recently shown in [20] that for the purpose of object detection, the shape of the object can be better described using the normal vectors in depth images, instead of the gradients in color images. [sent-31, score-0.528]
</p><p>19 We propose a novel activity descriptor for depth sequences, which is analogous to the histogram of gradients in color sequences [4, 9], and extends the histogram of normals in static images [20]. [sent-33, score-1.03]
</p><p>20 As the depth sequence represents a depth function of space and time, we propose to capture the observed changing structure using a histogram of oriented 4D surface normals (HON4D). [sent-34, score-1.072]
</p><p>21 Our proposed histogram operates in the 4D space, thus, jointly capturing the distribution of the changing shape and motion cues along with their correlations, instead of an adhoc concatenation of features as in [24]. [sent-38, score-0.267]
</p><p>22 Compared to the other holistic methods, we model the distribution of the normal vectors for each cell in the 4D space, which is richer and more discriminative than the average 4D occupancy used in [21]. [sent-41, score-0.408]
</p><p>23 The main contributions of this paper are: First, we propose a novel descriptor for activity recognition from depth sequences, in which we encode the distribution of the surface normal orientation in the 4D space of depth, time, and spatial coordinates. [sent-44, score-0.972]
</p><p>24 Related Work Early methods for activity recognition from depth sequences attempted to adopt techniques originally developed for color sequences. [sent-53, score-0.497]
</p><p>25 proposed to obtain a bag of 3D points (analogous to a bag of words) by sampling points from the silhouette of the depth images, then clustering the points in order to obtain salient postures (vocabulary). [sent-55, score-0.385]
</p><p>26 Adopting local interest point-based methods to operate in depth sequences is difficult because, as discussed earlier,  detectors such as STIP [10] and Dollar [5] are not reliable in depth sequences. [sent-59, score-0.624]
</p><p>27 Additionally, standard methods for automatically acquiring motion trajectories in color images as in [25, 22] are also not reliable in depth sequences. [sent-60, score-0.35]
</p><p>28 Therefore, recent methods for activity recognition in depth sequences resorted to alternative approaches in order to obtain reliable interest points and tracks. [sent-61, score-0.537]
</p><p>29 extract the skeleton of the human using the skeleton tracking algorithm in [18]. [sent-63, score-0.402]
</p><p>30 On the other hand, in [23], random subvolumes are selected from the space of all possible subvolumes in the depth sequences. [sent-67, score-0.392]
</p><p>31 Furthermore, holistic approaches for activity recognition from depth sequences are recently becoming popular. [sent-69, score-0.55]
</p><p>32 For example, in [26], the depth video is summarized in one image (a motion map), which is the average difference between the depth frames. [sent-71, score-0.624]
</p><p>33 We demonstrate that our method captures the complex and articulated structure and motion within the sequence using a richer and more discriminative descriptor than [21] and [26]. [sent-77, score-0.369]
</p><p>34 The 4D Surface Normal  Given a sequence of depth images {I1, I2 . [sent-82, score-0.345]
</p><p>35 IN} containing a person performing an activity, our goal is to} compute a global descriptor which is able to discriminate the class of action being performed. [sent-85, score-0.27]
</p><p>36 The depth sequence can be considered as a function R3 → R1 : z = f(x, y, t), which constitutes a surface in th→e 4 RD space represented as the set of points (x, y, t, z) satisfying S(x, y, t, z) = f(x, y, t) − z = 0. [sent-86, score-0.481]
</p><p>37 The normal to the surface S is computed as n = ∇S =  (∂∂xz,∂∂yz,∂∂tz,−1)T. [sent-87, score-0.317]
</p><p>38 (1)  Only the orientation ofthe normal is relevant to the shape of the 4D surface S; therefore, we normalize the computed normal to a unit length normal Note that the components of the surface normal are the gradients in space and time, along with a scalar (−1). [sent-88, score-1.244]
</p><p>39 Therefore, the normal oriteinmtaet,io anlo might falsely appear as equivalent htoe t nheo gradient orientation, and thus one might expect a histogram of 4D normal orientation (HON4D) to coincide with a histogram of 3D gradient orientation (HOG3D). [sent-89, score-0.96]
</p><p>40 The normal orientation has one extra dimension; therefore, the corresponding distribution over the bins is significantly different. [sent-91, score-0.369]
</p><p>41 To better illustrate that, consider for example the shapes in figure 3, which shows two space-time surfaces, where surface 1has a higher inclination than surface 2. [sent-98, score-0.312]
</p><p>42 The gradient orientation is similar for both surfaces because the component of the gradient along the shape dimension is negligible. [sent-99, score-0.355]
</p><p>43 In contrast, the orientation of the normal is significantly different. [sent-100, score-0.322]
</p><p>44 Therefore, a histogram of normal orientation can differentiate between these surfaces, while a histogram of gradient orientation cannot. [sent-101, score-0.692]
</p><p>45 In the coming section we demonstrate how we compute the histogram of oriented normals in the 4D space. [sent-103, score-0.317]
</p><p>46 Histogram of 4D Normals Given the surface normals computed as in equation 1using finite gray-value difference over all voxels in the depth sequence, we compute the corresponding distribution of 4D surface normal orientation. [sent-105, score-0.925]
</p><p>47 In contrast, in the depth se7 7 7 1 1 1 86 686  orientation histogram and the normal orientation histogram. [sent-111, score-0.825]
</p><p>48 For  better visualization, in this example, we assume we have only one spatial dimension; therefore, we have 2D gradients and 3D normals instead of the actual 3D gradients and 4D normals of a depth sequence. [sent-112, score-0.74]
</p><p>49 The orientation of the gradient is determined by an angle Θ, while the orientation of the normal is determined by two angles, Θ and Φ. [sent-113, score-0.516]
</p><p>50 Top: Two surfaces produced as a result of a shape (line) moving in time, where surface 1has a higher inclination than surface 2. [sent-114, score-0.39]
</p><p>51 Middle: The histogram of gradient orientation for surface 1(left), and surface 2 (right). [sent-115, score-0.554]
</p><p>52 Bottom: The histogram of normal orientation for surface 1 (left), and surface 2 (right). [sent-116, score-0.682]
</p><p>53 On the other hand, the direction of the normal for surface 1is significantly different than surface 2, and the corresponding histogram of normal orientation evidently captures this difference. [sent-119, score-0.947]
</p><p>54 A regular polychoron divides the 4D space uniformly with its vertices; therefore, it is a proper quantization of the 4D space. [sent-123, score-0.261]
</p><p>55 In [3], it is shown that in the 4D space, the vertices of a 600-cell centered at the origin are given as:  •  •  •  8 vertices obtained by permutations of (0, 0, 0, ±1). [sent-126, score-0.267]
</p><p>56 (2) Therefore, the distribution of the 4D normal orientation for a depth sequence is estimated by accumulating the contributions from the computed normals, followed by a normalization using the sum across all projectors, such that the final distribution sums to one:  Pr(pi|N) =? [sent-131, score-0.704]
</p><p>57 In order to further introduce cues from the spatiotemporal context, we divide the sequence into w h t spatiotemporal cells, and obtain a separate HON4D fwor× eha×cth. [sent-138, score-0.346]
</p><p>58 7 7 7 1 1 1 97 797  Finding the optimal projectors (bins of the histogram) is unarguably a highly non-convex optimization process, since in principle, it should involve learning both the classifier and the projectors jointly. [sent-146, score-1.08]
</p><p>59 Therefore, finding the optimal projectors is still an open-ended problem, which  we leave for future work, and instead, we resort to relaxing the problem into refining the projectors to better capture the distribution of the normals in a discriminative manner. [sent-148, score-1.43]
</p><p>60 In particular, given a dataset with training HON4D descriptors X = {xk}, note that each descriptor xk is obtained for a Xvide =o {kx by projecting t ehaec corresponding set of surface normals Nk = { nˆj } on the projectors P = {pi} as in equation 2m. [sent-149, score-1.077]
</p><p>61 Tlsh Neref=ore {, nˆ we can compute tohres density o}f tahse i projectors by estimating how many unit normals fall into each of them  D(pi) =k? [sent-150, score-0.856]
</p><p>62 ∈Njc( nˆk,j,pi),  (4)  where nk,j is the unit normal number j from depth sequence k. [sent-152, score-0.56]
</p><p>63 In other words, not only the projector with higher discriminative density Ddisc has higher accumulation of normal vectors, but also it has a higher contribution in the final classification score. [sent-166, score-0.406]
</p><p>64 To that end, we sort the projectors according to their discriminative density, and induce m random perturbations of each of the highest lprojectors according to their density, where m is computed for a projector pi as:  m(pi) =? [sent-168, score-0.739]
</p><p>65 pvD∈(PpDi)(pv) i f i i ≤ > l l, and λ is a parameters reflecting the total number of projectors to be induced. [sent-170, score-0.54]
</p><p>66 The random perturbations for projector pi constitute a new set of projectors {pi,q |q = 1. [sent-171, score-0.675]
</p><p>67 We augment the density-learned projectors to the original 120 projectors, and obtain the final set of projectors. [sent-178, score-0.54]
</p><p>68 The final SVM is trained on newly induced projectors which have never been seen in the initial SVM. [sent-181, score-0.54]
</p><p>69 We use a polynomial kernel in all experiments, though the proposed method of refining the projectors using the discriminative density is general enough to apply to any kernel. [sent-184, score-0.776]
</p><p>70 The actions in the new dataset are selected in pairs such that the two actions of each pair are similar in motion (have similar trajectories) and shape (have similar objects); how-  ×  ever, the motion-shape relation is different. [sent-188, score-0.362]
</p><p>71 This emphasizes the importance of capturing the shape and the motion cues jointly in the activity sequence as in HON4D, in contrast to capturing these features independently as in most previous methods. [sent-189, score-0.397]
</p><p>72 fe In all experiments, we initialize the projectors using the 120 vertices of the 600-cell polychoron, and compute the initial HON4D descriptors. [sent-196, score-0.649]
</p><p>73 We finally end up with a number of projectors typically ∼ 300, which becomes the dimensionality octfo otrhse yHpOicNal4lDy descriptor oicbhta bineecdo per c thelel. [sent-198, score-0.633]
</p><p>74 [26], where motion maps are obtained by accumulating the differences in the depth frames, and then HOG is used to describe the motion maps. [sent-203, score-0.463]
</p><p>75 [24], where the local occupancy pattern features (LOP) are used over the skeleton joints. [sent-207, score-0.282]
</p><p>76 [23], where the depth sequence is randomly sampled then the most discriminative samples are selected and described using LOP descriptor. [sent-209, score-0.409]
</p><p>77 MSR Action 3D Dataset MSR Action 3D dataset [24] is an action dataset of depth sequences captured by a depth camera. [sent-214, score-0.869]
</p><p>78 Example depth sequences from this dataset are shown in figure 4. [sent-217, score-0.384]
</p><p>79 In this dataset, the background is pre-processed to clear the discontinuities created from undefined depth regions. [sent-218, score-0.308]
</p><p>80 We further conduct a cross validation experiment to verify that the process of refining the projectors does not depend on specific training data. [sent-229, score-0.661]
</p><p>81 w Citohn refining t,h we projectors haet each fold, and obtain an average accuracy of 82. [sent-236, score-0.628]
</p><p>82 eTahcish provides a bcltaeainr eavni adveenrcaeg teh aact cthurea rceyfi onfed 82 projectors %do. [sent-239, score-0.54]
</p><p>83 MSR Hand Gesture Dataset The Gesture3D dataset [23] is a hand gesture dataset of depth sequences captured by a depth camera. [sent-243, score-0.789]
</p><p>84 In total, the dataset contains 333 depth sequences, and is considered challenging mainly because of self-occlusion issues. [sent-246, score-0.308]
</p><p>85 Second, the motion and the shape cues are correlated in the depth sequences, and it is rather insufficient to capture them independently. [sent-261, score-0.453]
</p><p>86 For example, “Pick up” and “Put down” actions have similar motion and shape; however, the co-occurrence of the object shape and the hand motion is in different spatiotemporal order (refer to figure 6). [sent-263, score-0.423]
</p><p>87 Each action is performed three  times using ten different actors, where the first five actors are used for testing, and the rest for training. [sent-266, score-0.271]
</p><p>88 Top: Pair-wise skeleton features and LOP features from [24] without temporal pyramid (left), and with pyramid (right). [sent-285, score-0.332]
</p><p>89 Bottom: HON4D features as is (left), and after refining the projectors using the discriminative density (right). [sent-286, score-0.776]
</p><p>90 Conclusion We presented a novel, simple, and easily implementable descriptor for activity recognition from depth sequences. [sent-311, score-0.514]
</p><p>91 Our descriptor captures motion and geometry cues jointly using a histogram of normal orientation in the 4D space of depth, time, and spatial coordinates. [sent-312, score-0.68]
</p><p>92 We initially quantize the 4D space using the vertices of a 600-cell polychoron, and use that to compute the distribution of the 4D normal orientation for each depth sequence. [sent-313, score-0.759]
</p><p>93 Consequently, we estimate the discriminative density at each vertex of the polychoron, and induce further vertices accordingly, thus placing more emphasis on the discriminative bins of the histogram. [sent-314, score-0.368]
</p><p>94 Efficient spatiotemporal hole filling strategy for kinect depth maps. [sent-320, score-0.41]
</p><p>95 A 3-dimensional sift descriptor and its application to action recognition. [sent-429, score-0.27]
</p><p>96 Real-time human pose recognition in parts from single depth images. [sent-440, score-0.274]
</p><p>97 Histogram of oriented normal vectors for object recognition with a depth sensor. [sent-457, score-0.486]
</p><p>98 Stop: Space-time occupancy patterns for 3d action recognition from depth map sequences. [sent-470, score-0.532]
</p><p>99 Mining actionlet ensemble for action recognition with depth cameras. [sent-492, score-0.451]
</p><p>100 Recognizing actions using depth motion maps-based histograms of oriented gradients. [sent-504, score-0.472]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('projectors', 0.54), ('depth', 0.274), ('msr', 0.205), ('skeleton', 0.201), ('normals', 0.198), ('normal', 0.181), ('action', 0.177), ('lop', 0.149), ('activity', 0.147), ('polychoron', 0.144), ('orientation', 0.141), ('surface', 0.136), ('vertices', 0.109), ('spatiotemporal', 0.105), ('actors', 0.094), ('descriptor', 0.093), ('actions', 0.091), ('histogram', 0.088), ('refining', 0.088), ('density', 0.084), ('occupancy', 0.081), ('projector', 0.077), ('motion', 0.076), ('sequences', 0.076), ('quantization', 0.072), ('consequently', 0.072), ('sequence', 0.071), ('temporal', 0.067), ('dollar', 0.065), ('cues', 0.065), ('discriminative', 0.064), ('gesture', 0.06), ('subvolumes', 0.059), ('pi', 0.058), ('quantize', 0.054), ('holistic', 0.053), ('gradient', 0.053), ('methodaccuracy', 0.051), ('daily', 0.05), ('permutations', 0.049), ('ddisc', 0.048), ('njc', 0.048), ('polychorons', 0.048), ('evidently', 0.048), ('gestures', 0.048), ('bins', 0.047), ('regular', 0.045), ('wave', 0.044), ('stip', 0.043), ('postures', 0.043), ('surfaces', 0.04), ('descriptors', 0.04), ('inclination', 0.04), ('resorted', 0.04), ('kick', 0.04), ('swing', 0.04), ('shape', 0.038), ('accumulating', 0.037), ('hand', 0.037), ('joints', 0.037), ('xk', 0.036), ('activities', 0.036), ('isbn', 0.036), ('captures', 0.036), ('additionally', 0.035), ('gradients', 0.035), ('pick', 0.035), ('bag', 0.034), ('throw', 0.034), ('oreifej', 0.034), ('stick', 0.034), ('falsely', 0.034), ('undefined', 0.034), ('wear', 0.034), ('unit', 0.034), ('dataset', 0.034), ('verify', 0.033), ('videos', 0.032), ('put', 0.032), ('pyramid', 0.032), ('pairs', 0.032), ('quantized', 0.032), ('ore', 0.032), ('tennis', 0.032), ('therefore', 0.031), ('kinect', 0.031), ('analogous', 0.031), ('oriented', 0.031), ('fourier', 0.03), ('jc', 0.03), ('tracker', 0.03), ('dimension', 0.03), ('richer', 0.029), ('klaser', 0.028), ('fold', 0.028), ('confusion', 0.027), ('pv', 0.027), ('hmm', 0.027), ('joint', 0.027), ('draw', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="196-tfidf-1" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><p>2 0.24598558 <a title="196-tfidf-2" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>Author: Simon Hadfield, Richard Bowden</p><p>Abstract: Action recognition in unconstrained situations is a difficult task, suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent emergence of 3D data, both in broadcast content, and commercial depth sensors, provides the possibility to overcome this issue. This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use ofthe 3D data, andfive new interestpoint detection strategies are alsoproposed, that extend to the 3D data. Our evaluation compares all 4 feature descriptors, using 7 different types of interest point, over a variety of threshold levels, for the Hollywood3D dataset. We make the dataset including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community.</p><p>3 0.23663138 <a title="196-tfidf-3" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>4 0.23613852 <a title="196-tfidf-4" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>5 0.20862515 <a title="196-tfidf-5" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>6 0.19255093 <a title="196-tfidf-6" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>7 0.18866968 <a title="196-tfidf-7" href="./cvpr-2013-An_Approach_to_Pose-Based_Action_Recognition.html">40 cvpr-2013-An Approach to Pose-Based Action Recognition</a></p>
<p>8 0.16642165 <a title="196-tfidf-8" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>9 0.16490805 <a title="196-tfidf-9" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>10 0.16347305 <a title="196-tfidf-10" href="./cvpr-2013-Context-Aware_Modeling_and_Recognition_of_Activities_in_Video.html">94 cvpr-2013-Context-Aware Modeling and Recognition of Activities in Video</a></p>
<p>11 0.16284248 <a title="196-tfidf-11" href="./cvpr-2013-First-Person_Activity_Recognition%3A_What_Are_They_Doing_to_Me%3F.html">175 cvpr-2013-First-Person Activity Recognition: What Are They Doing to Me?</a></p>
<p>12 0.16228303 <a title="196-tfidf-12" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>13 0.15800786 <a title="196-tfidf-13" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>14 0.15739624 <a title="196-tfidf-14" href="./cvpr-2013-Sampling_Strategies_for_Real-Time_Action_Recognition.html">378 cvpr-2013-Sampling Strategies for Real-Time Action Recognition</a></p>
<p>15 0.15183309 <a title="196-tfidf-15" href="./cvpr-2013-Joint_Sparsity-Based_Representation_and_Analysis_of_Unconstrained_Activities.html">233 cvpr-2013-Joint Sparsity-Based Representation and Analysis of Unconstrained Activities</a></p>
<p>16 0.14046547 <a title="196-tfidf-16" href="./cvpr-2013-Better_Exploiting_Motion_for_Better_Action_Recognition.html">59 cvpr-2013-Better Exploiting Motion for Better Action Recognition</a></p>
<p>17 0.13994034 <a title="196-tfidf-17" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>18 0.13847505 <a title="196-tfidf-18" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>19 0.13846029 <a title="196-tfidf-19" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>20 0.13438147 <a title="196-tfidf-20" href="./cvpr-2013-Uncalibrated_Photometric_Stereo_for_Unknown_Isotropic_Reflectances.html">443 cvpr-2013-Uncalibrated Photometric Stereo for Unknown Isotropic Reflectances</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.269), (1, 0.114), (2, 0.016), (3, -0.106), (4, -0.255), (5, -0.08), (6, -0.146), (7, 0.101), (8, -0.036), (9, -0.046), (10, -0.053), (11, -0.138), (12, -0.046), (13, 0.104), (14, 0.078), (15, -0.022), (16, -0.085), (17, 0.055), (18, -0.005), (19, -0.021), (20, -0.02), (21, -0.011), (22, 0.002), (23, 0.023), (24, 0.021), (25, 0.093), (26, -0.026), (27, 0.035), (28, 0.024), (29, 0.015), (30, 0.051), (31, 0.076), (32, 0.059), (33, 0.006), (34, 0.039), (35, -0.029), (36, -0.007), (37, -0.038), (38, -0.034), (39, 0.001), (40, 0.018), (41, -0.026), (42, 0.026), (43, 0.085), (44, -0.041), (45, -0.055), (46, 0.025), (47, -0.01), (48, 0.043), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96687603 <a title="196-lsi-1" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><p>2 0.87434262 <a title="196-lsi-2" href="./cvpr-2013-Spatio-temporal_Depth_Cuboid_Similarity_Feature_for_Activity_Recognition_Using_Depth_Camera.html">407 cvpr-2013-Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</a></p>
<p>Author: Lu Xia, J.K. Aggarwal</p><p>Abstract: Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPsfrom depth videos (calledDSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms stateof-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.</p><p>3 0.80664122 <a title="196-lsi-3" href="./cvpr-2013-Hollywood_3D%3A_Recognizing_Actions_in_3D_Natural_Scenes.html">205 cvpr-2013-Hollywood 3D: Recognizing Actions in 3D Natural Scenes</a></p>
<p>Author: Simon Hadfield, Richard Bowden</p><p>Abstract: Action recognition in unconstrained situations is a difficult task, suffering from massive intra-class variations. It is made even more challenging when complex 3D actions are projected down to the image plane, losing a great deal of information. The recent emergence of 3D data, both in broadcast content, and commercial depth sensors, provides the possibility to overcome this issue. This paper presents a new dataset, for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use ofthe 3D data, andfive new interestpoint detection strategies are alsoproposed, that extend to the 3D data. Our evaluation compares all 4 feature descriptors, using 7 different types of interest point, over a variety of threshold levels, for the Hollywood3D dataset. We make the dataset including stereo video, estimated depth maps and all code required to reproduce the benchmark results, available to the wider community.</p><p>4 0.68941039 <a title="196-lsi-4" href="./cvpr-2013-Shading-Based_Shape_Refinement_of_RGB-D_Images.html">394 cvpr-2013-Shading-Based Shape Refinement of RGB-D Images</a></p>
<p>Author: Lap-Fai Yu, Sai-Kit Yeung, Yu-Wing Tai, Stephen Lin</p><p>Abstract: We present a shading-based shape refinement algorithm which uses a noisy, incomplete depth map from Kinect to help resolve ambiguities in shape-from-shading. In our framework, the partial depth information is used to overcome bas-relief ambiguity in normals estimation, as well as to assist in recovering relative albedos, which are needed to reliably estimate the lighting environment and to separate shading from albedo. This refinement of surface normals using a noisy depth map leads to high-quality 3D surfaces. The effectiveness of our algorithm is demonstrated through several challenging real-world examples.</p><p>5 0.67196435 <a title="196-lsi-5" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>Author: Ju Shen, Sen-Ching S. Cheung</p><p>Abstract: The recent popularity of structured-light depth sensors has enabled many new applications from gesture-based user interface to 3D reconstructions. The quality of the depth measurements of these systems, however, is far from perfect. Some depth values can have significant errors, while others can be missing altogether. The uncertainty in depth measurements among these sensors can significantly degrade the performance of any subsequent vision processing. In this paper, we propose a novel probabilistic model to capture various types of uncertainties in the depth measurement process among structured-light systems. The key to our model is the use of depth layers to account for the differences between foreground objects and background scene, the missing depth value phenomenon, and the correlation between color and depth channels. The depth layer labeling is solved as a maximum a-posteriori estimation problem, and a Markov Random Field attuned to the uncertainty in measurements is used to spatially smooth the labeling process. Using the depth-layer labels, we propose a depth correction and completion algorithm that outperforms oth- er techniques in the literature.</p><p>6 0.65190899 <a title="196-lsi-6" href="./cvpr-2013-Depth_Acquisition_from_Density_Modulated_Binary_Patterns.html">114 cvpr-2013-Depth Acquisition from Density Modulated Binary Patterns</a></p>
<p>7 0.64903688 <a title="196-lsi-7" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>8 0.64355159 <a title="196-lsi-8" href="./cvpr-2013-Joint_Geodesic_Upsampling_of_Depth_Images.html">232 cvpr-2013-Joint Geodesic Upsampling of Depth Images</a></p>
<p>9 0.63270819 <a title="196-lsi-9" href="./cvpr-2013-Bayesian_Depth-from-Defocus_with_Shading_Constraints.html">56 cvpr-2013-Bayesian Depth-from-Defocus with Shading Constraints</a></p>
<p>10 0.62955832 <a title="196-lsi-10" href="./cvpr-2013-Relative_Volume_Constraints_for_Single_View_3D_Reconstruction.html">354 cvpr-2013-Relative Volume Constraints for Single View 3D Reconstruction</a></p>
<p>11 0.60805184 <a title="196-lsi-11" href="./cvpr-2013-Depth_Super_Resolution_by_Rigid_Body_Self-Similarity_in_3D.html">115 cvpr-2013-Depth Super Resolution by Rigid Body Self-Similarity in 3D</a></p>
<p>12 0.60773933 <a title="196-lsi-12" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>13 0.60646379 <a title="196-lsi-13" href="./cvpr-2013-Modeling_Actions_through_State_Changes.html">287 cvpr-2013-Modeling Actions through State Changes</a></p>
<p>14 0.59507787 <a title="196-lsi-14" href="./cvpr-2013-Simultaneous_Super-Resolution_of_Depth_and_Images_Using_a_Single_Camera.html">397 cvpr-2013-Simultaneous Super-Resolution of Depth and Images Using a Single Camera</a></p>
<p>15 0.59274709 <a title="196-lsi-15" href="./cvpr-2013-Detecting_Changes_in_3D_Structure_of_a_Scene_from_Multi-view_Images_Captured_by_a_Vehicle-Mounted_Camera.html">117 cvpr-2013-Detecting Changes in 3D Structure of a Scene from Multi-view Images Captured by a Vehicle-Mounted Camera</a></p>
<p>16 0.58696359 <a title="196-lsi-16" href="./cvpr-2013-Poselet_Key-Framing%3A_A_Model_for_Human_Activity_Recognition.html">336 cvpr-2013-Poselet Key-Framing: A Model for Human Activity Recognition</a></p>
<p>17 0.58618975 <a title="196-lsi-17" href="./cvpr-2013-Motionlets%3A_Mid-level_3D_Parts_for_Human_Motion_Recognition.html">291 cvpr-2013-Motionlets: Mid-level 3D Parts for Human Motion Recognition</a></p>
<p>18 0.58423722 <a title="196-lsi-18" href="./cvpr-2013-Template-Based_Isometric_Deformable_3D_Reconstruction_with_Sampling-Based_Focal_Length_Self-Calibration.html">423 cvpr-2013-Template-Based Isometric Deformable 3D Reconstruction with Sampling-Based Focal Length Self-Calibration</a></p>
<p>19 0.5733431 <a title="196-lsi-19" href="./cvpr-2013-Intrinsic_Characterization_of_Dynamic_Surfaces.html">226 cvpr-2013-Intrinsic Characterization of Dynamic Surfaces</a></p>
<p>20 0.57149035 <a title="196-lsi-20" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (10, 0.131), (16, 0.022), (26, 0.067), (33, 0.266), (67, 0.108), (69, 0.058), (87, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8900674 <a title="196-lda-1" href="./cvpr-2013-Unconstrained_Monocular_3D_Human_Pose_Estimation_by_Action_Detection_and_Cross-Modality_Regression_Forest.html">444 cvpr-2013-Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</a></p>
<p>Author: Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</p><p>Abstract: This work addresses the challenging problem of unconstrained 3D human pose estimation (HPE)from a novelperspective. Existing approaches struggle to operate in realistic applications, mainly due to their scene-dependent priors, such as background segmentation and multi-camera network, which restrict their use in unconstrained environments. We therfore present a framework which applies action detection and 2D pose estimation techniques to infer 3D poses in an unconstrained video. Action detection offers spatiotemporal priors to 3D human pose estimation by both recognising and localising actions in space-time. Instead of holistic features, e.g. silhouettes, we leverage the flexibility of deformable part model to detect 2D body parts as a feature to estimate 3D poses. A new unconstrained pose dataset has been collected to justify the feasibility of our method, which demonstrated promising results, significantly outperforming the relevant state-of-the-arts.</p><p>2 0.88297987 <a title="196-lda-2" href="./cvpr-2013-Discriminative_Segment_Annotation_in_Weakly_Labeled_Video.html">133 cvpr-2013-Discriminative Segment Annotation in Weakly Labeled Video</a></p>
<p>Author: Kevin Tang, Rahul Sukthankar, Jay Yagnik, Li Fei-Fei</p><p>Abstract: The ubiquitous availability of Internet video offers the vision community the exciting opportunity to directly learn localized visual concepts from real-world imagery. Unfortunately, most such attempts are doomed because traditional approaches are ill-suited, both in terms of their computational characteristics and their inability to robustly contend with the label noise that plagues uncurated Internet content. We present CRANE, a weakly supervised algorithm that is specifically designed to learn under such conditions. First, we exploit the asymmetric availability of real-world training data, where small numbers of positive videos tagged with the concept are supplemented with large quantities of unreliable negative data. Second, we ensure that CRANE is robust to label noise, both in terms of tagged videos that fail to contain the concept as well as occasional negative videos that do. Finally, CRANE is highly parallelizable, making it practical to deploy at large scale without sacrificing the quality of the learned solution. Although CRANE is general, this paper focuses on segment annotation, where we show state-of-the-art pixel-level segmentation results on two datasets, one of which includes a training set of spatiotemporal segments from more than 20,000 videos.</p><p>same-paper 3 0.87897408 <a title="196-lda-3" href="./cvpr-2013-HON4D%3A_Histogram_of_Oriented_4D_Normals_for_Activity_Recognition_from_Depth_Sequences.html">196 cvpr-2013-HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</a></p>
<p>Author: Omar Oreifej, Zicheng Liu</p><p>Abstract: We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently; thus, they often fail to capture the complex joint shapemotion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.</p><p>4 0.8788237 <a title="196-lda-4" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>Author: Ian Endres, Kevin J. Shih, Johnston Jiaa, Derek Hoiem</p><p>Abstract: We propose a method to learn a diverse collection of discriminative parts from object bounding box annotations. Part detectors can be trained and applied individually, which simplifies learning and extension to new features or categories. We apply the parts to object category detection, pooling part detections within bottom-up proposed regions and using a boosted classifier with proposed sigmoid weak learners for scoring. On PASCAL VOC 2010, we evaluate the part detectors ’ ability to discriminate and localize annotated keypoints. Our detection system is competitive with the best-existing systems, outperforming other HOG-based detectors on the more deformable categories.</p><p>5 0.87165296 <a title="196-lda-5" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>Author: Lu Zhang, Laurens van_der_Maaten</p><p>Abstract: Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (based on tracking-by-detection) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation ofour structure-preserving object tracker (SPOT) reveals significant performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object.</p><p>6 0.87027001 <a title="196-lda-6" href="./cvpr-2013-Detecting_and_Aligning_Faces_by_Image_Retrieval.html">119 cvpr-2013-Detecting and Aligning Faces by Image Retrieval</a></p>
<p>7 0.8687315 <a title="196-lda-7" href="./cvpr-2013-Learning_SURF_Cascade_for_Fast_and_Accurate_Object_Detection.html">254 cvpr-2013-Learning SURF Cascade for Fast and Accurate Object Detection</a></p>
<p>8 0.86861753 <a title="196-lda-8" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>9 0.86810118 <a title="196-lda-9" href="./cvpr-2013-Class_Generative_Models_Based_on_Feature_Regression_for_Pose_Estimation_of_Object_Categories.html">82 cvpr-2013-Class Generative Models Based on Feature Regression for Pose Estimation of Object Categories</a></p>
<p>10 0.86768144 <a title="196-lda-10" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>11 0.86688381 <a title="196-lda-11" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<p>12 0.8664428 <a title="196-lda-12" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>13 0.8661536 <a title="196-lda-13" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>14 0.86599803 <a title="196-lda-14" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>15 0.86549473 <a title="196-lda-15" href="./cvpr-2013-Detection_Evolution_with_Multi-order_Contextual_Co-occurrence.html">122 cvpr-2013-Detection Evolution with Multi-order Contextual Co-occurrence</a></p>
<p>16 0.86523795 <a title="196-lda-16" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>17 0.86386746 <a title="196-lda-17" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>18 0.86330849 <a title="196-lda-18" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>19 0.86315566 <a title="196-lda-19" href="./cvpr-2013-Online_Object_Tracking%3A_A_Benchmark.html">314 cvpr-2013-Online Object Tracking: A Benchmark</a></p>
<p>20 0.86268467 <a title="196-lda-20" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
