<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-26" href="#">cvpr2013-26</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</h1>
<br/><p>Source: <a title="cvpr-2013-26-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Predoehl_A_Statistical_Model_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Andrew Predoehl, Scott Morris, Kobus Barnard</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="cvpr-2013-26-reference" href="../cvpr2013_reference/cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We combine that with a prior model of trail length and smoothness, yielding a posterior distribution for trails, given an image. [sent-7, score-0.889]
</p><p>2 Our experiments, on trail images and groundtruth collected in the western continental USA, show substantial improvement over those of the previous best trail-finding method. [sent-9, score-0.974]
</p><p>3 Introduction Recreational trails like the one shown in Figure 1a represent a challenge for computer vision. [sent-11, score-0.164]
</p><p>4 Automatic identification of trails would be useful not only for bikers, hikers, and land managers, but also as a model problem in other domains that depend on identifying locally-linear structures in images, such as blood vessels or neurons grown in vitro. [sent-15, score-0.244]
</p><p>5 In this paper we present a statistical model describing trails and trail images, based on a segmentation of images by textures. [sent-16, score-1.032]
</p><p>6 1c), and, when it does so, the direction in which that trail is oriented (Fig. [sent-19, score-0.884]
</p><p>7 Example trail (a), trail image (b), and learned characteristics of image textons (c, d). [sent-28, score-1.785]
</p><p>8 In (c), color indicates the ratio of frequencies, for each superpixel’s texton label, at which that texton generates on- and off-trail pixels. [sent-29, score-0.236]
</p><p>9 In (d), hue and saturation show the trail direction of each superpixel’s texton, conditioned on it generating a trail pixel. [sent-30, score-1.832]
</p><p>10 ) In order to infer the route of a latent trail between two known endpoints, we use a sampling approach to search this posterior for a good value. [sent-33, score-0.962]
</p><p>11 [29], we build a graph from the image evidence, and generate inference proposals as simple paths in this graph. [sent-36, score-0.17]
</p><p>12 1) we produce a graph in which short paths bgeetw weeeignh hntsod (§e3s . [sent-38, score-0.163]
</p><p>13 Using our model to judge among the proposals, we retain the most probable route as the best one. [sent-42, score-0.116]
</p><p>14 However, there are substantial differences between trail extraction and vessel segmentation. [sent-49, score-0.857]
</p><p>15 In addition, the clutter surrounding trails tends to be more variable, which motivates our approach based on textons and superpixels. [sent-51, score-0.264]
</p><p>16 In  contrast, each invocation of our quasi-Dijkstra procedure produces a random path that is essentially one realization of an implicit decision tree. [sent-59, score-0.144]
</p><p>17 Third, our image likelihood function uses a statistical model of trail direction learned for the textons. [sent-65, score-0.963]
</p><p>18 Fourth, our trail proposal scheme differs substantially from theirs. [sent-66, score-0.888]
</p><p>19 Many road-finding methods are based on solving a dynamic programming or shortest path problem in a pixel or similar lattice. [sent-67, score-0.245]
</p><p>20 Similarly, our trail proposer finds short, but not necessarily shortest, paths in the superpixel graph, as a heuristic method to explore the space of routes between trail termini. [sent-68, score-1.947]
</p><p>21 -  tion of paths or path lengths bears some similarity to the  Figure 2. [sent-72, score-0.242]
</p><p>22 We use a bank of oriented filters to generate pixel features, which we use to segment the image into superpixels of contiguous texture. [sent-74, score-0.147]
</p><p>23 We generate independent trail proposals T(i) for i = 1, 2, . [sent-75, score-0.885]
</p><p>24 by searching for nearly-shortest paths in the superpixel graph, and keep the proposal with the highest posterior measure. [sent-78, score-0.277]
</p><p>25 stochastic shortest path problem [5, 10, 3 1], but the latter research has more to do with finding an optimal traversal policy. [sent-79, score-0.22]
</p><p>26 There are a few similarities between our approach to trail-finding and robotic path planning algorithms such as Rapidly-exploring Random Trees (RRTs) [24] and related approaches [4, 22]. [sent-82, score-0.182]
</p><p>27 In each case, a stochastically-built tree provides a route to a goal point. [sent-83, score-0.111]
</p><p>28 With ordinary RRTs, as with other standard robotic path planning, candidate paths are cleanly partitioned into feasible and infeasible categories. [sent-85, score-0.31]
</p><p>29 Also, in robotics, one usually seeks a feasible path that minimizes a cost function; whereas in the present work, we want a path that maximizes a probability distribution. [sent-87, score-0.304]
</p><p>30 Data and Problem Definition We used groundtruth derived from GPS tracks collected by Morris and Barnard [29] from the Great Divide Mountain Bike Route (GDMBR), which traverses the western continental United States. [sent-90, score-0.117]
</p><p>31 This route was partitioned into trail pieces that each fit into a 2 km square bounding box aligned north-south. [sent-91, score-0.973]
</p><p>32 Grayscale aerial imagery surrounding each trail piece, originating from the US Geological Survey, was downloaded from Microsoft Research Maps [8]. [sent-92, score-0.887]
</p><p>33 The result is 1526 trail pieces and images (one trail piece per image), at a resolution of 1m/pixel. [sent-93, score-1.754]
</p><p>34 Given the image and the endpoints of the corresponding trail piece, how does the trail connect 333333888  those endpoints? [sent-95, score-1.736]
</p><p>35 We represent a trail piece T as a sequence of eightconnected pixel locations in the image. [sent-97, score-0.947]
</p><p>36 All other pixel locations in T are determined by using Bresenham’s line algorithm [11] between successive trail vertices. [sent-101, score-0.923]
</p><p>37 ca Tthioens lik, as long as the path has a well-defined tangent everywhere. [sent-107, score-0.166]
</p><p>38 Image and trail models We model the textures of the image using a Gaussian mixture (GMM). [sent-109, score-0.84]
</p><p>39 Each kernel has a sigma of 4 pixels in the narrow direction, which is comparable to the width of typical paths in our image data. [sent-111, score-0.14]
</p><p>40 Because the oriented Gaussian kernels roughly match the appearance of visible paths, the textons also tend to cluster trail pixels according to the direction of trail. [sent-114, score-0.991]
</p><p>41 Next we label each image pixel with its most probable texton (GMM mode). [sent-115, score-0.207]
</p><p>42 During training, we then learn for each texton some additional characteristics: the frequencies with which it generates on-trail and off-trail pixels, and the axial direction (i. [sent-116, score-0.225]
</p><p>43 The latter we model as a von Mises distribution [27] (an angular distribution) using a maximum-likelihood (ML) fit of axial directions sampled from groundtruth trail pixels. [sent-120, score-1.043]
</p><p>44 For texton label k, let μk , Σk denote the mean and covariance of the feature vectors generated by the mode. [sent-122, score-0.135]
</p><p>45 When k does generate an on-trail pixel, its axial direction is modeled by von Mises parameters μk, the expected direction,  ×  and κk, the concentration. [sent-127, score-0.182]
</p><p>46 We denote the PDF of an axial von Mises distribution evaluated at angle θ ∈ [0, π) by fAM (θ; μ, κ) = 2M (2θ; 2μ, κ) ,  (1)  where M(θ; μ, κ) represents the PDF of an ordinary (radwiahle)r von Mθ;iμse,sκ )d risetprribeusteinotns. [sent-128, score-0.23]
</p><p>47 A superpixel s is an 8-connected region of pixels sharing a common label, whose size |s| we limit to oatf mpixoestl s16 sh3a8r4in pixels. [sent-136, score-0.129]
</p><p>48 Mmaoxnim laable clo,n wnheocsteed si regions eo lfi pixels that share a common label but exceed that size are partitioned into multiple superpixels by a 128 128 grid. [sent-137, score-0.172]
</p><p>49 Segmenting the image by a learned palette of textons lets us relax our independence assumptions, to respect the correlations in the imagery, yet still partition the superpixels into on- and off-trail classes. [sent-145, score-0.249]
</p><p>50 Trail model Since the trail representation approximates a polygonal path, it has a well-defined tangent direction at every pixel location strictly between two trail vertices: let trail pixel q be between successive vertices vi = [xi , yi]t and vi+1 . [sent-148, score-2.869]
</p><p>51 We can also define a direction at the trail vertices, as a composite of the directions of the neighboring path edges: at vertex vi with predecessor and successor vertices vi−1 and vi+1, we define the direction at vi as that of vector ? [sent-152, score-1.224]
</p><p>52 el that prior knowledge by pc(T), a product of von Mises distributions on the differences of successive vertex angles at interior vertices (Fig. [sent-159, score-0.262]
</p><p>53 , φN−2 discretely approximate the drerivative of path curvature. [sent-169, score-0.144]
</p><p>54 For a trail T defined by N = 50 vertices, with interior angles φ1 , φ2 , . [sent-171, score-0.883]
</p><p>55 In addition, we model prior knowledge about a trail’s polygonal path length ? [sent-179, score-0.22]
</p><p>56 By construction, each trail piece extends across a square bounding box Lmin = 1900 meters on a side. [sent-181, score-0.897]
</p><p>57 Image likelihood We develop the likelihood p(I|T), where I a grayscale is image surrounding a teralihil hypothesis, wTh. [sent-189, score-0.121]
</p><p>58 , s|S| }, and further partition S  according tso S Sw =heth {ser a superpixel }in,t aenrdse cftusr tThe. [sent-193, score-0.121]
</p><p>59 pWaret idtieonno Ste the superpixels containing trail pixels by S1 = {s ∈ S : s ∩ T ∅}. [sent-194, score-0.963]
</p><p>60 ∈  in which Ds represents the features of superpixel s, l0 (s) represents the likelihood of Ds in a off-trail region, and rl1e (psr;e Tse)n represents tihheo oldik eolfih Dood of Ds when it intersects one or more pixels o thf eT . [sent-204, score-0.199]
</p><p>61 s∈S l0 (s) is independent of the trail hypothesis T. [sent-206, score-0.84]
</p><p>62 In both l0 and l1, for each pixel we will account for three characteristics found there: the pixel’s features as conditioned by its texton label, the pixel’s label as conditioned by trail overlap, and the pixel’s label as conditioned by trail directionality. [sent-216, score-2.066]
</p><p>63 We begin with likelihood l0 (s) of the image data in offtrail superpixel s. [sent-218, score-0.187]
</p><p>64 Let q be any pixel in s, and let c(q) and  c(s) respectively denote the texton label of q and the texton label common to all pixels of s. [sent-219, score-0.346]
</p><p>65 When s intersects T at pixel q, we assess the likelihood of the directional appearance of Dq by assuming uniform prior ed idstirreicbutitoionnasl aofp pteexatruarnec deior efcDt ion and trail direction, in which case, p(direction of Dq | T) = p(θT (q) | direction of c(s)) . [sent-226, score-1.057]
</p><p>66 (8) Then the likelihood of image data within superpixel s is a product of the likelihoods of its on-trail and off-trail pixels:  l1(s;T) =q1∈? [sent-227, score-0.154]
</p><p>67 (11) Intuitively the two kinds of ratios in this product can be interpreted as a logical conjunction: not only should the image textures along T “look like” trail (i. [sent-259, score-0.86]
</p><p>68 We take a random sample of 50 such axes, shown in (b), to which we fit an axial von Mises. [sent-272, score-0.138]
</p><p>69 Inference Using the prior (3) and likelihood (11), we define posterior distribution  p(T|I) =Z1p(T)p(I|T)1/|T|  (12)  in which exponent 1/|T| makes the likelihood neutral with respect hto e txrpaoiln seinzte, 1 /an|Td| n moarmkeasli tzhaeti loinke flaihcotoord Z n iust rlealft w unknown. [sent-274, score-0.151]
</p><p>70 (12) for inferring an unknown trail T, given image I the endpoints of T. [sent-276, score-0.896]
</p><p>71 Instead, we explore the space of trails that bridge between two known endpoints by searching for short paths in a carefully weighted graph of superpixels (an approximately dual problem). [sent-278, score-0.48]
</p><p>72 Edge weights in superpixel graph Our proposer is inspired by Morris and Barnard [29], who also computed shortest paths for trail inference. [sent-284, score-1.185]
</p><p>73 We use their same basic idea: an edge that is highly likely to be on a trail should get low weight. [sent-285, score-0.858]
</p><p>74 However, we face two  technical challenges when translating the likelihood ratio of (11) to edge weights: we require an inverse relationship that remains finite, and we lack the directional factor fAM (θT) (i. [sent-286, score-0.121]
</p><p>75 , while creating a trail proposal, we cannot yet know its tangent). [sent-288, score-0.84]
</p><p>76 In place of factor fAM (θT), we compute a statistical approximation based on superpixel geometry. [sent-289, score-0.131]
</p><p>77 corresponds to a trail proposal whose geometry overlaps both superpixels, so we model a random trail proposal connecting s and s? [sent-291, score-1.776]
</p><p>78 ) of an axial von Mises distribution of a random straight track traversing both s and s? [sent-296, score-0.138]
</p><p>79 Then we compare this geometric model with the directional model learned for texton c(s? [sent-298, score-0.14]
</p><p>80 If the two distributions are similar, it is more likely that a path entering s would extend into s? [sent-300, score-0.144]
</p><p>81 Weight w also needs a  dair reeclatt relationship wwit ∼h superpixel size, otherwise the proposer would have a bias in favor of large superpixels. [sent-315, score-0.169]
</p><p>82 ||1,/ 2ar, ebae coafu sse superpixels tend to be narrow. [sent-321, score-0.115]
</p><p>83 ) |Isn o| trhdaenr |tso |b alance the sizes of the weights in image regions likely and unlikely to be trail, we include parameters α and γ, which are trained by grid search so as to minimize Hausdorff distance between groundtruth and the shortest path in the superpixel graph. [sent-322, score-0.363]
</p><p>84 Thus our choice for edge weight between superpixels s and s? [sent-323, score-0.115]
</p><p>85 Sampling short paths We have developed a variation on Dijkstra’s algorithm to sample paths in this weighted graph that are short but not necessarily the shortest. [sent-334, score-0.299]
</p><p>86 Our idea is to alter the priority queue to support an EXTRACT-NEAR-MIN operation that, at each iteration, extracts a vertex (superpixel) selected randomly, with a preference for vertices of smaller distance. [sent-338, score-0.223]
</p><p>87 For a vertex s with distance d(s) in the queue, its probability of being the next vertex removed from the queue U is given by a power law,  Pr(s will be drawn next) =? [sent-339, score-0.133]
</p><p>88 The stochastic priority queue is implemented with a redblack tree that stores non-normalized probability mass d(s)−β with entry s, and maintains at each tree node a sum of all subtree nodes’ probability masses. [sent-345, score-0.173]
</p><p>89 Thus we can perform an EXTRACT-NEAR-MIN operation in time O(log |U|), where |U| is the number of vertices (superpixeOls()l oing t|Uhe| priority queue. [sent-346, score-0.121]
</p><p>90 tSheinc nue tmhebe superpixel graph eisr pla333444111  Figure 5. [sent-347, score-0.13]
</p><p>91 Examples of intermediate and final results of inference process: (a) the groundtruth pixel footprint of superpixels of a typical trail piece, i. [sent-348, score-1.073]
</p><p>92 , all pixels of the superpixels touching groundtruth; (b) 200 short paths generated by the quasi-Dijkstra method; (c) shortest path found by Dijkstra’s algorithm; (d) short path approximately maximizing eq. [sent-350, score-0.636]
</p><p>93 ) nar, sampling a short path with this implementation uses time O(|S| log |S| ). [sent-354, score-0.182]
</p><p>94 ail proposal, we use either this quasiDijkstra algorithm, or the unmodified Dijkstra’s algorithm, to find a short path P in the superpixel graph bridging between endpoints. [sent-356, score-0.312]
</p><p>95 P is a simple path of superpixels, P = (si1, si2, . [sent-357, score-0.144]
</p><p>96 Because our prior model requires a polygonal path with a fixed number of vertices, we take some additional steps to reduce the superpixel path into a polygonal path of pixels. [sent-364, score-0.666]
</p><p>97 rtest path in the pixel graph, using 8-way adjacency and Euclidean distance. [sent-371, score-0.194]
</p><p>98 Results and Conclusions We use the Hausdorff distance metric between groundtruth and inferred path for evaluation; a perfectly inferred path will have a distance of zero. [sent-377, score-0.353]
</p><p>99 6 shows the success rate of three methods of generating and assessing trail proposals. [sent-379, score-0.84]
</p><p>100 The top curve, labeled “QD-MAP,” denotes the success rate when we generate 200 trail proposals by the quasi-Dijkstra procedure described above, and keep the proposal with Table 1. [sent-381, score-0.933]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('trail', 0.84), ('trails', 0.164), ('fam', 0.15), ('path', 0.144), ('texton', 0.108), ('superpixel', 0.103), ('paths', 0.098), ('superpixels', 0.097), ('route', 0.094), ('dijkstra', 0.091), ('textons', 0.081), ('vvii', 0.077), ('axial', 0.073), ('vertices', 0.069), ('mises', 0.068), ('proposer', 0.066), ('groundtruth', 0.065), ('von', 0.065), ('piece', 0.057), ('endpoints', 0.056), ('dq', 0.056), ('polygonal', 0.055), ('queue', 0.055), ('priority', 0.052), ('shortest', 0.051), ('likelihood', 0.051), ('pixel', 0.05), ('conditioned', 0.05), ('hausdorff', 0.049), ('proposal', 0.048), ('barnard', 0.045), ('morris', 0.045), ('proposals', 0.045), ('direction', 0.044), ('independence', 0.042), ('short', 0.038), ('vessels', 0.035), ('gdmbr', 0.033), ('lmin', 0.033), ('offtrail', 0.033), ('predoehl', 0.033), ('roadfinding', 0.033), ('successive', 0.033), ('directional', 0.032), ('ds', 0.031), ('vertex', 0.031), ('continental', 0.029), ('kobus', 0.029), ('radians', 0.029), ('recreational', 0.029), ('rrts', 0.029), ('hue', 0.029), ('lets', 0.029), ('pdf', 0.029), ('saturation', 0.029), ('aerial', 0.028), ('posterior', 0.028), ('statistical', 0.028), ('label', 0.027), ('ona', 0.027), ('graph', 0.027), ('ordinary', 0.027), ('vi', 0.026), ('pixels', 0.026), ('gmm', 0.025), ('pc', 0.025), ('traversal', 0.025), ('neurons', 0.024), ('characteristics', 0.024), ('arizona', 0.023), ('concentration', 0.023), ('numerator', 0.023), ('western', 0.023), ('probable', 0.022), ('tangent', 0.022), ('roads', 0.022), ('interior', 0.022), ('partitioned', 0.022), ('fp', 0.022), ('blood', 0.021), ('kb', 0.021), ('angles', 0.021), ('footprint', 0.021), ('prior', 0.021), ('ratio', 0.02), ('ratios', 0.02), ('ml', 0.02), ('intersects', 0.019), ('planning', 0.019), ('robotic', 0.019), ('surrounding', 0.019), ('sse', 0.018), ('tso', 0.018), ('edge', 0.018), ('tree', 0.017), ('pieces', 0.017), ('substantial', 0.017), ('probability', 0.016), ('narrow', 0.016), ('preference', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="26-tfidf-1" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>Author: Andrew Predoehl, Scott Morris, Kobus Barnard</p><p>Abstract: unkown-abstract</p><p>2 0.11746766 <a title="26-tfidf-2" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>Author: Gautam Singh, Jana Kosecka</p><p>Abstract: This paper presents a nonparametric approach to semantic parsing using small patches and simple gradient, color and location features. We learn the relevance of individual feature channels at test time using a locally adaptive distance metric. To further improve the accuracy of the nonparametric approach, we examine the importance of the retrieval set used to compute the nearest neighbours using a novel semantic descriptor to retrieve better candidates. The approach is validated by experiments on several datasets used for semantic parsing demonstrating the superiority of the method compared to the state of art approaches.</p><p>3 0.079611279 <a title="26-tfidf-3" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>Author: Jason Chang, Donglai Wei, John W. Fisher_III</p><p>Abstract: We develop a generative probabilistic model for temporally consistent superpixels in video sequences. In contrast to supervoxel methods, object parts in different frames are tracked by the same temporal superpixel. We explicitly model flow between frames with a bilateral Gaussian process and use this information to propagate superpixels in an online fashion. We consider four novel metrics to quantify performance of a temporal superpixel representation and demonstrate superior performance when compared to supervoxel methods.</p><p>4 0.075318262 <a title="26-tfidf-4" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>Author: David Weiss, Ben Taskar</p><p>Abstract: We propose SCALPEL, a flexible method for object segmentation that integrates rich region-merging cues with mid- and high-level information about object layout, class, and scale into the segmentation process. Unlike competing approaches, SCALPEL uses a cascade of bottom-up segmentation models that is capable of learning to ignore boundaries early on, yet use them as a stopping criterion once the object has been mostly segmented. Furthermore, we show how such cascades can be learned efficiently. When paired with a novel method that generates better localized shapepriors than our competitors, our method leads to a concise, accurate set of segmentation proposals; these proposals are more accurate on the PASCAL VOC2010 dataset than state-of-the-art methods that use re-ranking to filter much larger bags of proposals. The code for our algorithm is available online.</p><p>5 0.073081523 <a title="26-tfidf-5" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>Author: Guang Shu, Afshin Dehghan, Mubarak Shah</p><p>Abstract: We propose an approach to improve the detection performance of a generic detector when it is applied to a particular video. The performance of offline-trained objects detectors are usually degraded in unconstrained video environments due to variant illuminations, backgrounds and camera viewpoints. Moreover, most object detectors are trained using Haar-like features or gradient features but ignore video specificfeatures like consistent colorpatterns. In our approach, we apply a Superpixel-based Bag-of-Words (BoW) model to iteratively refine the output of a generic detector. Compared to other related work, our method builds a video-specific detector using superpixels, hence it can handle the problem of appearance variation. Most importantly, using Conditional Random Field (CRF) along with our super pixel-based BoW model, we develop and algorithm to segment the object from the background . Therefore our method generates an output of the exact object regions instead of the bounding boxes generated by most detectors. In general, our method takes detection bounding boxes of a generic detector as input and generates the detection output with higher average precision and precise object regions. The experiments on four recent datasets demonstrate the effectiveness of our approach and significantly improves the state-of-art detector by 5-16% in average precision.</p><p>6 0.072301432 <a title="26-tfidf-6" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>7 0.0700716 <a title="26-tfidf-7" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>8 0.065582678 <a title="26-tfidf-8" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>9 0.061165012 <a title="26-tfidf-9" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>10 0.058404062 <a title="26-tfidf-10" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>11 0.056541774 <a title="26-tfidf-11" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>12 0.054945827 <a title="26-tfidf-12" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>13 0.054482754 <a title="26-tfidf-13" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>14 0.050132263 <a title="26-tfidf-14" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>15 0.047380339 <a title="26-tfidf-15" href="./cvpr-2013-Revisiting_Depth_Layers_from_Occlusions.html">357 cvpr-2013-Revisiting Depth Layers from Occlusions</a></p>
<p>16 0.046779227 <a title="26-tfidf-16" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>17 0.044940379 <a title="26-tfidf-17" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<p>18 0.044557203 <a title="26-tfidf-18" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>19 0.041676212 <a title="26-tfidf-19" href="./cvpr-2013-Fast_Patch-Based_Denoising_Using_Approximated_Patch_Geodesic_Paths.html">169 cvpr-2013-Fast Patch-Based Denoising Using Approximated Patch Geodesic Paths</a></p>
<p>20 0.040332444 <a title="26-tfidf-20" href="./cvpr-2013-Augmenting_CRFs_with_Boltzmann_Machine_Shape_Priors_for_Image_Labeling.html">50 cvpr-2013-Augmenting CRFs with Boltzmann Machine Shape Priors for Image Labeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.091), (1, 0.018), (2, 0.017), (3, 0.003), (4, 0.056), (5, 0.002), (6, 0.018), (7, 0.031), (8, -0.047), (9, 0.007), (10, 0.073), (11, -0.027), (12, -0.011), (13, 0.035), (14, -0.017), (15, -0.008), (16, 0.04), (17, -0.036), (18, -0.013), (19, 0.067), (20, 0.039), (21, 0.041), (22, -0.059), (23, 0.019), (24, -0.077), (25, 0.028), (26, -0.047), (27, -0.043), (28, -0.003), (29, 0.07), (30, 0.002), (31, 0.003), (32, 0.019), (33, -0.026), (34, -0.002), (35, 0.01), (36, -0.028), (37, 0.01), (38, 0.032), (39, -0.024), (40, 0.023), (41, -0.032), (42, -0.036), (43, -0.039), (44, -0.043), (45, 0.014), (46, 0.004), (47, -0.032), (48, -0.014), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8951245 <a title="26-lsi-1" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>Author: Andrew Predoehl, Scott Morris, Kobus Barnard</p><p>Abstract: unkown-abstract</p><p>2 0.79834062 <a title="26-lsi-2" href="./cvpr-2013-A_Video_Representation_Using_Temporal_Superpixels.html">29 cvpr-2013-A Video Representation Using Temporal Superpixels</a></p>
<p>Author: Jason Chang, Donglai Wei, John W. Fisher_III</p><p>Abstract: We develop a generative probabilistic model for temporally consistent superpixels in video sequences. In contrast to supervoxel methods, object parts in different frames are tracked by the same temporal superpixel. We explicitly model flow between frames with a bilateral Gaussian process and use this information to propagate superpixels in an online fashion. We consider four novel metrics to quantify performance of a temporal superpixel representation and demonstrate superior performance when compared to supervoxel methods.</p><p>3 0.76364511 <a title="26-lsi-3" href="./cvpr-2013-Voxel_Cloud_Connectivity_Segmentation_-_Supervoxels_for_Point_Clouds.html">458 cvpr-2013-Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</a></p>
<p>Author: Jeremie Papon, Alexey Abramov, Markus Schoeler, Florentin Wörgötter</p><p>Abstract: Unsupervised over-segmentation of an image into regions of perceptually similar pixels, known as superpixels, is a widely used preprocessing step in segmentation algorithms. Superpixel methods reduce the number of regions that must be considered later by more computationally expensive algorithms, with a minimal loss of information. Nevertheless, as some information is inevitably lost, it is vital that superpixels not cross object boundaries, as such errors will propagate through later steps. Existing methods make use of projected color or depth information, but do not consider three dimensional geometric relationships between observed data points which can be used to prevent superpixels from crossing regions of empty space. We propose a novel over-segmentation algorithm which uses voxel relationships to produce over-segmentations which are fully consistent with the spatial geometry of the scene in three dimensional, rather than projective, space. Enforcing the constraint that segmented regions must have spatial connectivity prevents label flow across semantic object boundaries which might otherwise be violated. Additionally, as the algorithm works directly in 3D space, observations from several calibrated RGB+D cameras can be segmented jointly. Experiments on a large data set of human annotated RGB+D images demonstrate a significant reduction in occurrence of clusters crossing object boundaries, while maintaining speeds comparable to state-of-the-art 2D methods.</p><p>4 0.6937803 <a title="26-lsi-4" href="./cvpr-2013-Robust_Region_Grouping_via_Internal_Patch_Statistics.html">366 cvpr-2013-Robust Region Grouping via Internal Patch Statistics</a></p>
<p>Author: Xiaobai Liu, Liang Lin, Alan L. Yuille</p><p>Abstract: In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of superpixels, followed by seeking the optimal superpixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level superpixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.</p><p>5 0.68783695 <a title="26-lsi-5" href="./cvpr-2013-Probabilistic_Graphlet_Cut%3A_Exploiting_Spatial_Structure_Cue_for_Weakly_Supervised_Image_Segmentation.html">339 cvpr-2013-Probabilistic Graphlet Cut: Exploiting Spatial Structure Cue for Weakly Supervised Image Segmentation</a></p>
<p>Author: Luming Zhang, Mingli Song, Zicheng Liu, Xiao Liu, Jiajun Bu, Chun Chen</p><p>Abstract: Weakly supervised image segmentation is a challenging problem in computer vision field. In this paper, we present a new weakly supervised image segmentation algorithm by learning the distribution of spatially structured superpixel sets from image-level labels. Specifically, we first extract graphlets from each image where a graphlet is a smallsized graph consisting of superpixels as its nodes and it encapsulates the spatial structure of those superpixels. Then, a manifold embedding algorithm is proposed to transform graphlets of different sizes into equal-length feature vectors. Thereafter, we use GMM to learn the distribution of the post-embedding graphlets. Finally, we propose a novel image segmentation algorithm, called graphlet cut, that leverages the learned graphlet distribution in measuring the homogeneity of a set of spatially structured superpixels. Experimental results show that the proposed approach outperforms state-of-the-art weakly supervised image segmentation methods, and its performance is comparable to those of the fully supervised segmentation models.</p><p>6 0.68241984 <a title="26-lsi-6" href="./cvpr-2013-Maximum_Cohesive_Grid_of_Superpixels_for_Fast_Object_Localization.html">280 cvpr-2013-Maximum Cohesive Grid of Superpixels for Fast Object Localization</a></p>
<p>7 0.66337067 <a title="26-lsi-7" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>8 0.64798719 <a title="26-lsi-8" href="./cvpr-2013-A_Higher-Order_CRF_Model_for_Road_Network_Extraction.html">13 cvpr-2013-A Higher-Order CRF Model for Road Network Extraction</a></p>
<p>9 0.64458507 <a title="26-lsi-9" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>10 0.60112232 <a title="26-lsi-10" href="./cvpr-2013-Weakly-Supervised_Dual_Clustering_for_Image_Semantic_Segmentation.html">460 cvpr-2013-Weakly-Supervised Dual Clustering for Image Semantic Segmentation</a></p>
<p>11 0.53132695 <a title="26-lsi-11" href="./cvpr-2013-Composite_Statistical_Inference_for_Semantic_Segmentation.html">86 cvpr-2013-Composite Statistical Inference for Semantic Segmentation</a></p>
<p>12 0.52031189 <a title="26-lsi-12" href="./cvpr-2013-Nonparametric_Scene_Parsing_with_Adaptive_Feature_Relevance_and_Semantic_Context.html">309 cvpr-2013-Nonparametric Scene Parsing with Adaptive Feature Relevance and Semantic Context</a></p>
<p>13 0.5167374 <a title="26-lsi-13" href="./cvpr-2013-Improving_an_Object_Detector_and_Extracting_Regions_Using_Superpixels.html">217 cvpr-2013-Improving an Object Detector and Extracting Regions Using Superpixels</a></p>
<p>14 0.49262083 <a title="26-lsi-14" href="./cvpr-2013-Recovering_Line-Networks_in_Images_by_Junction-Point_Processes.html">351 cvpr-2013-Recovering Line-Networks in Images by Junction-Point Processes</a></p>
<p>15 0.49162978 <a title="26-lsi-15" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>16 0.48704052 <a title="26-lsi-16" href="./cvpr-2013-SCALPEL%3A_Segmentation_Cascades_with_Localized_Priors_and_Efficient_Learning.html">370 cvpr-2013-SCALPEL: Segmentation Cascades with Localized Priors and Efficient Learning</a></p>
<p>17 0.47895721 <a title="26-lsi-17" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>18 0.47405383 <a title="26-lsi-18" href="./cvpr-2013-Graph-Based_Optimization_with_Tubularity_Markov_Tree_for_3D_Vessel_Segmentation.html">190 cvpr-2013-Graph-Based Optimization with Tubularity Markov Tree for 3D Vessel Segmentation</a></p>
<p>19 0.44528788 <a title="26-lsi-19" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>20 0.43975216 <a title="26-lsi-20" href="./cvpr-2013-Spatial_Inference_Machines.html">406 cvpr-2013-Spatial Inference Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.128), (16, 0.025), (26, 0.063), (33, 0.225), (55, 0.252), (59, 0.013), (67, 0.028), (69, 0.039), (87, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83306289 <a title="26-lda-1" href="./cvpr-2013-Expressive_Visual_Text-to-Speech_Using_Active_Appearance_Models.html">159 cvpr-2013-Expressive Visual Text-to-Speech Using Active Appearance Models</a></p>
<p>Author: Robert Anderson, Björn Stenger, Vincent Wan, Roberto Cipolla</p><p>Abstract: This paper presents a complete system for expressive visual text-to-speech (VTTS), which is capable of producing expressive output, in the form of a ‘talking head’, given an input text and a set of continuous expression weights. The face is modeled using an active appearance model (AAM), and several extensions are proposed which make it more applicable to the task of VTTS. The model allows for normalization with respect to both pose and blink state which significantly reduces artifacts in the resulting synthesized sequences. We demonstrate quantitative improvements in terms of reconstruction error over a million frames, as well as in large-scale user studies, comparing the output of different systems.</p><p>same-paper 2 0.8268351 <a title="26-lda-2" href="./cvpr-2013-A_Statistical_Model_for_Recreational_Trails_in_Aerial_Images.html">26 cvpr-2013-A Statistical Model for Recreational Trails in Aerial Images</a></p>
<p>Author: Andrew Predoehl, Scott Morris, Kobus Barnard</p><p>Abstract: unkown-abstract</p><p>3 0.80710858 <a title="26-lda-3" href="./cvpr-2013-Understanding_Bayesian_Rooms_Using_Composite_3D_Object_Models.html">445 cvpr-2013-Understanding Bayesian Rooms Using Composite 3D Object Models</a></p>
<p>Author: Luca Del_Pero, Joshua Bowdish, Bonnie Kermgard, Emily Hartley, Kobus Barnard</p><p>Abstract: We develop a comprehensive Bayesian generative model for understanding indoor scenes. While it is common in this domain to approximate objects with 3D bounding boxes, we propose using strong representations with finer granularity. For example, we model a chair as a set of four legs, a seat and a backrest. We find that modeling detailed geometry improves recognition and reconstruction, and enables more refined use of appearance for scene understanding. We demonstrate this with a new likelihood function that re- wards 3D object hypotheses whose 2D projection is more uniform in color distribution. Such a measure would be confused by background pixels if we used a bounding box to represent a concave object like a chair. Complex objects are modeled using a set or re-usable 3D parts, and we show that this representation captures much of the variation among object instances with relatively few parameters. We also designed specific data-driven inference mechanismsfor eachpart that are shared by all objects containing that part, which helps make inference transparent to the modeler. Further, we show how to exploit contextual relationships to detect more objects, by, for example, proposing chairs around and underneath tables. We present results showing the benefits of each of these innovations. The performance of our approach often exceeds that of state-of-the-art methods on the two tasks of room layout estimation and object recognition, as evaluated on two bench mark data sets used in this domain. work. 1) Detailed geometric models, such as tables with legs and top (bottom left), provide better reconstructions than plain boxes (top right), when supported by image features such as geometric context [5] (top middle), or an approach to using color introduced here. 2) Non convex models allow for complex configurations, such as a chair under a table (bottom middle). 3) 3D contextual relationships, such as chairs being around a table, allow identifying objects supported by little image evidence, like the chair behind the table (bottom right). Best viewed in color.</p><p>4 0.77196419 <a title="26-lda-4" href="./cvpr-2013-Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment.html">420 cvpr-2013-Supervised Descent Method and Its Applications to Face Alignment</a></p>
<p>Author: Xuehan Xiong, Fernando De_la_Torre</p><p>Abstract: Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization ofa general smoothfunction. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, thispaperproposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-ofthe-art performance in the problem of facial feature detec- tion. The code is available at www. .human sen sin g. . cs . cmu . edu/in t ra fa ce.</p><p>5 0.73252231 <a title="26-lda-5" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>Author: Horst Possegger, Sabine Sternig, Thomas Mauthner, Peter M. Roth, Horst Bischof</p><p>Abstract: Combining foreground images from multiple views by projecting them onto a common ground-plane has been recently applied within many multi-object tracking approaches. These planar projections introduce severe artifacts and constrain most approaches to objects moving on a common 2D ground-plane. To overcome these limitations, we introduce the concept of an occupancy volume exploiting the full geometry and the objects ’ center of mass and develop an efficient algorithm for 3D object tracking. Individual objects are tracked using the local mass density scores within a particle filter based approach, constrained by a Voronoi partitioning between nearby trackers. Our method benefits from the geometric knowledge given by the occupancy volume to robustly extract features and train classifiers on-demand, when volumetric information becomes unreliable. We evaluate our approach on several challenging real-world scenarios including the public APIDIS dataset. Experimental evaluations demonstrate significant improvements compared to state-of-theart methods, while achieving real-time performance. – –</p><p>6 0.73139709 <a title="26-lda-6" href="./cvpr-2013-Multi-scale_Curve_Detection_on_Surfaces.html">298 cvpr-2013-Multi-scale Curve Detection on Surfaces</a></p>
<p>7 0.72996438 <a title="26-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.72909945 <a title="26-lda-8" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<p>9 0.72869998 <a title="26-lda-9" href="./cvpr-2013-A_Minimum_Error_Vanishing_Point_Detection_Approach_for_Uncalibrated_Monocular_Images_of_Man-Made_Environments.html">19 cvpr-2013-A Minimum Error Vanishing Point Detection Approach for Uncalibrated Monocular Images of Man-Made Environments</a></p>
<p>10 0.7280764 <a title="26-lda-10" href="./cvpr-2013-Separating_Signal_from_Noise_Using_Patch_Recurrence_across_Scales.html">393 cvpr-2013-Separating Signal from Noise Using Patch Recurrence across Scales</a></p>
<p>11 0.72770345 <a title="26-lda-11" href="./cvpr-2013-Intrinsic_Scene_Properties_from_a_Single_RGB-D_Image.html">227 cvpr-2013-Intrinsic Scene Properties from a Single RGB-D Image</a></p>
<p>12 0.72617525 <a title="26-lda-12" href="./cvpr-2013-Minimum_Uncertainty_Gap_for_Robust_Visual_Tracking.html">285 cvpr-2013-Minimum Uncertainty Gap for Robust Visual Tracking</a></p>
<p>13 0.72599828 <a title="26-lda-13" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>14 0.72586638 <a title="26-lda-14" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>15 0.72582704 <a title="26-lda-15" href="./cvpr-2013-Beyond_Point_Clouds%3A_Scene_Understanding_by_Reasoning_Geometry_and_Physics.html">61 cvpr-2013-Beyond Point Clouds: Scene Understanding by Reasoning Geometry and Physics</a></p>
<p>16 0.72571337 <a title="26-lda-16" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>17 0.72537071 <a title="26-lda-17" href="./cvpr-2013-Efficient_Large-Scale_Structured_Learning.html">143 cvpr-2013-Efficient Large-Scale Structured Learning</a></p>
<p>18 0.72537023 <a title="26-lda-18" href="./cvpr-2013-Mirror_Surface_Reconstruction_from_a_Single_Image.html">286 cvpr-2013-Mirror Surface Reconstruction from a Single Image</a></p>
<p>19 0.72506982 <a title="26-lda-19" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>20 0.7250542 <a title="26-lda-20" href="./cvpr-2013-Motion_Estimation_for_Self-Driving_Cars_with_a_Generalized_Camera.html">290 cvpr-2013-Motion Estimation for Self-Driving Cars with a Generalized Camera</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
