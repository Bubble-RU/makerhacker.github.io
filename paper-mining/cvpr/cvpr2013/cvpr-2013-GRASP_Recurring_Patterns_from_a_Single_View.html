<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 cvpr-2013-GRASP Recurring Patterns from a Single View</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-183" href="#">cvpr2013-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 cvpr-2013-GRASP Recurring Patterns from a Single View</h1>
<br/><p>Source: <a title="cvpr-2013-183-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Liu_GRASP_Recurring_Patterns_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>Reference: <a title="cvpr-2013-183-reference" href="../cvpr2013_reference/cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 GRASP Recurring Patterns from a Single View Jingchen Liu1 Yanxi Liu1,2 1 Computer Science and Engineering, 2 Electrical Engineering The Pennsylvania State University University Park, PA 16802, USA { j ingchen , yanxi } @ c s e . [sent-1, score-0.105]
</p><p>2 edu  Abstract We propose a novel unsupervised method for discovering recurring patterns from a single view. [sent-3, score-1.024]
</p><p>3 A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. [sent-4, score-1.281]
</p><p>4 The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. [sent-5, score-0.133]
</p><p>5 We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). [sent-6, score-0.079]
</p><p>6 We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images. [sent-7, score-1.102]
</p><p>7 Introduction Similar yet non-identical objects, such as animals in a herd, cars on the street, faces in a crowd or goods on a  supermarket shelf, are ubiquitous. [sent-9, score-0.038]
</p><p>8 There has been a surge of interest in unsupervised visual perception of such nearidentical objects [1, 2, 3, 4, 5, 6, 7, 8], echoing an observation that much of our understanding of the world is based on the perception and recognition of shared or repeated structures [9]. [sent-10, score-0.244]
</p><p>9 To capture the recurrence nature within such patterns, we use the term recurring pattern to refer to the ensemble of multiple instances of a common visual object or object for short, which may or may not correspond to a complete physical object. [sent-11, score-1.147]
</p><p>10 As shown in Figure 1, each object of a recurring pattern is a geometric composition (green arcs) of visual words (distinct red iconic shapes), where partial matching among the objects is permitted. [sent-12, score-1.217]
</p><p>11 The recognition of recurring patterns has applications in effective image segmentation [4], compression and super-resolution [2], retrieval [5] and organization of unlabeled data [7]. [sent-13, score-1.001]
</p><p>12 More fundamentally, a recurring pattern is a domain independent representation for semantically meaningful mid-level grouping  (a) a 6-instance recur ing pat ern  (b) an 8-instance recur ing pat ern Figure 1. [sent-14, score-1.336]
</p><p>13 Unsupervised discovery of recurring patterns in real images by our proposed algorithm, where partial matching and low visual word recall rates (75% for (a), 71% for (b)) are allowed. [sent-15, score-1.307]
</p><p>14 Two classic approaches for recurring pattern detection are: (A) pairwise visual-word-matching which matches pairs of visual words across all objects [7]; and (B) pairwise object-matching which matches feature point correspondences between a pair of objects [12, 5, 4]. [sent-17, score-1.332]
</p><p>15 (2) Visual wordpair matching also suffers from missing feature points (low visual word recall rate), as shown by our quantitative evaluations (Section 4). [sent-19, score-0.317]
</p><p>16 (3) Whether it is better to match object-pairs or visual word-pairs is unknown in advance, and due to the lack of a global decision mechanism, current pairwise-matching systems do not afford flexible and adaptive switching between the two. [sent-20, score-0.139]
</p><p>17 We are thus motivated to propose an alternative jointoptimization framework for recurring pattern discovery by matching along both visual word and object dimensions si222000000311  multaneously (Fig. [sent-21, score-1.325]
</p><p>18 Related Work Recurring pattern discovery has been referred to in the literature as common visual pattern discovery [14, 5], corecognition/segmentation of objects [15, 16, 4], and highorder structural semantics learning [7]. [sent-25, score-0.496]
</p><p>19 [15, 17, 18] achieve unsupervised detection/segmentation of two objects in two separate images. [sent-26, score-0.07]
</p><p>20 Yuan and Wu [14] use spatial random partitioning to detect object pair(s) from one or a pair of images; Cho et al. [sent-27, score-0.018]
</p><p>21 formulate the same problem as correspondence association solved by MCMC exploration [16] and graph matching [3], respectively. [sent-28, score-0.143]
</p><p>22 [5] adopts graph matching to detect multiple recurring patterns between two images. [sent-29, score-1.055]
</p><p>23 To detect more than 2 recurring in-  stances, Cho et al. [sent-30, score-0.925]
</p><p>24 generalize feature correspondence association under a many-to-many constraint and perform multiple object matching using agglomerative clustering [19] and MCMC association [4]. [sent-31, score-0.191]
</p><p>25 [7] use the approach of pairwise visual word-matching, while assuming that visual words can be detected on all recurring instances (i. [sent-34, score-1.161]
</p><p>26 Our method differs from previous work in two significant ways: (1) it solves a simultaneous visual word-object assignment problem; and (2) it explicitly and effectively deals with missing/spurious feature points in recurring patterns (feature recall rate from an image can be lower than 100%). [sent-37, score-1.218]
</p><p>27 Another line of related work is unsupervised category discovery, e. [sent-38, score-0.043]
</p><p>28 Our approach We start with a formalization of the concept of a recurring pattern and its components (Fig. [sent-44, score-1.051]
</p><p>29 The key technical steps are the selection and grouping of representative feature points into key visual words and the exploration of the structural consistency among their topology/geometry by using GRASP optimization to  discover recurring patterns. [sent-47, score-1.248]
</p><p>30 Formalization of Recurring Patterns We define a recurring pattern to have at least two visual objects. [sent-50, score-1.061]
</p><p>31 Likewise each object of a recurring pattern is required to have at least two distinct visual words. [sent-51, score-1.082]
</p><p>32 Thus, the smallest recurring pattern is conceptually a 4-tuple structure satisfying certain affinity constraints (Figure 2 (a)). [sent-52, score-1.083]
</p><p>33 The visual word distinctiveness requirement forces each object of a recurring pattern to have a compact representation (no nested recurrence of visual words within each object), thus qualifying it to serve as a structural-primitive for recurring pattern discovery. [sent-53, score-2.456]
</p><p>34 More importantly, this definition ensures the uniqueness of each recurring pattern while maximizing number of object instances. [sent-54, score-1.015]
</p><p>35 Mathematically, we construct a recurring pattern Ω as a 2D feature-assignment matrix where each row corresponds to a visual word and each column corresponds to a visual object (Figure 2 (b)), that is, ΩM,N (m, n) = fi, where fi corresponds to a feature point, m = 1. [sent-55, score-1.395]
</p><p>36 N, and M and N are the number of visual words and objects, respectively. [sent-61, score-0.131]
</p><p>37 ΩM,N (m, n) = 0 is used to indicate a corresponding feature point is missing. [sent-62, score-0.031]
</p><p>38 (a) two potential objects of a smallest recurring pattern, n1, n2, each of which contains two visual words m1 , m2 ; (b) The 2D feature assignment matrix, where each row corresponds to a visual word and each column to a visual object. [sent-64, score-1.446]
</p><p>39 Visual Word Extraction Given a set of feature points F = {fi |i = 1, . [sent-67, score-0.058]
</p><p>40 , vSeInFT a), s a ovifsu faeal uwroerd p oWint sis F a =sub {sfet| iof = =F 1 s,u. [sent-72, score-0.049]
</p><p>41 tKha}t all feature points in W share strong appearance similarity. [sent-75, score-0.058]
</p><p>42 Let vi be the normalized descriptor of fi, such that ? [sent-76, score-0.021]
</p><p>43 An overview of the proposed method: (a) input image; (b) extracted and clustered feature points with top 20 clusters color coded; (c) GRASP optimization framework; (d) automatically discovered recurring pattern after a joint optimization process. [sent-79, score-1.118]
</p><p>44 1, we define a normalized affinity metric between features fi, fj as  A(i,j) =vTivsjt−d a{vvpTg{vvq|pTp,vqq| =p,q 1, =2, 1. [sent-80, score-0.041]
</p><p>45 (2)  Starting with an initial assignment of W = {i, j} where A(iS, tjar) i sn gm waxithim aunm in among aigl n mfeeantutr oef pairs i n{ ,Fj, we use a forward selection scheme where new feature points fi are sequentially included into W that maximizes Eqn. [sent-89, score-0.185]
</p><p>46 2 can no longer be increased, the growing of the current W stops and the extraction process then continues on F − W to find the next visual word. [sent-92, score-0.11]
</p><p>47 Our visual uweosrd o forward-selection mtheeth noexd d vififseurasl significantly ifrsuoaml K-means, in that we only extract inlier subsets of F to form a vocabulary of key visual-words for recurring patterns, while ignoring a considerable amount of background noise or outliers. [sent-93, score-1.012]
</p><p>48 For efficiency, the affinity matrix A can be made sparse  by setting A(i, j) = 0 for A(i, j) < τ. [sent-94, score-0.041]
</p><p>49 In our experiments, we set τ = 2 to remove feature pairs with distance that exceeds ‘two-sigma’ . [sent-95, score-0.052]
</p><p>50 Given the sparsity of A, typically 30 ∼ 200 valid visual words can be extracted from a single image depending on tlh we image nco bnete enxtt raancdt ereds forolumtio an. [sent-96, score-0.177]
</p><p>51 s Ideally, different feature points from the same visualword W should be present in the corresponding relative locations of all N objects of a recurring pattern, i. [sent-97, score-0.992]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('recurring', 0.907), ('grasp', 0.163), ('word', 0.106), ('discovery', 0.088), ('pattern', 0.088), ('patterns', 0.074), ('yanxi', 0.07), ('visual', 0.066), ('words', 0.065), ('fi', 0.062), ('recurrence', 0.059), ('recur', 0.059), ('formalization', 0.056), ('ern', 0.052), ('exploration', 0.048), ('pat', 0.047), ('assignment', 0.044), ('unsupervised', 0.043), ('mcmc', 0.043), ('affinity', 0.041), ('association', 0.041), ('cho', 0.035), ('tkha', 0.035), ('stressed', 0.035), ('multaneously', 0.035), ('ingchen', 0.035), ('jingchen', 0.035), ('qualifying', 0.035), ('surge', 0.035), ('matching', 0.035), ('stances', 0.033), ('randomized', 0.031), ('feature', 0.031), ('recall', 0.031), ('highorder', 0.031), ('moves', 0.03), ('pairwise', 0.03), ('iof', 0.029), ('iconic', 0.029), ('gau', 0.029), ('obt', 0.028), ('instances', 0.027), ('arcs', 0.027), ('distinctiveness', 0.027), ('shelf', 0.027), ('adaptive', 0.027), ('perception', 0.027), ('points', 0.027), ('objects', 0.027), ('afford', 0.026), ('pennsylvania', 0.026), ('grouping', 0.025), ('continues', 0.025), ('sub', 0.025), ('quantified', 0.025), ('smallest', 0.024), ('greedy', 0.024), ('agglomerative', 0.024), ('corresponds', 0.023), ('tlh', 0.023), ('formal', 0.023), ('nested', 0.023), ('nco', 0.023), ('conceptually', 0.023), ('joint', 0.023), ('optimization', 0.021), ('missing', 0.021), ('fundamentally', 0.021), ('likewise', 0.021), ('exceeds', 0.021), ('distinct', 0.021), ('matches', 0.021), ('adopts', 0.021), ('gm', 0.021), ('potential', 0.021), ('vi', 0.021), ('inlier', 0.02), ('switching', 0.02), ('sis', 0.02), ('structural', 0.02), ('engineering', 0.02), ('organization', 0.02), ('uniqueness', 0.02), ('wa', 0.019), ('crowd', 0.019), ('treatment', 0.019), ('forces', 0.019), ('key', 0.019), ('systematically', 0.019), ('stops', 0.019), ('animals', 0.019), ('rate', 0.019), ('coded', 0.019), ('mathematically', 0.019), ('world', 0.019), ('correspondence', 0.019), ('classic', 0.019), ('distortions', 0.019), ('deals', 0.019), ('detect', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="183-tfidf-1" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>2 0.072591007 <a title="183-tfidf-2" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>3 0.069390908 <a title="183-tfidf-3" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>4 0.056974262 <a title="183-tfidf-4" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>Author: Quannan Li, Jiajun Wu, Zhuowen Tu</p><p>Abstract: Obtaining effective mid-level representations has become an increasingly important task in computer vision. In this paper, we propose a fully automatic algorithm which harvests visual concepts from a large number of Internet images (more than a quarter of a million) using text-based queries. Existing approaches to visual concept learning from Internet images either rely on strong supervision with detailed manual annotations or learn image-level classifiers only. Here, we take the advantage of having massive wellorganized Google and Bing image data; visual concepts (around 14, 000) are automatically exploited from images using word-based queries. Using the learned visual concepts, we show state-of-the-art performances on a variety of benchmark datasets, which demonstrate the effectiveness of the learned mid-level representations: being able to generalize well to general natural images. Our method shows significant improvement over the competing systems in image classification, including those with strong supervision.</p><p>5 0.056468762 <a title="183-tfidf-5" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>6 0.042528447 <a title="183-tfidf-6" href="./cvpr-2013-Fast_Energy_Minimization_Using_Learned_State_Filters.html">165 cvpr-2013-Fast Energy Minimization Using Learned State Filters</a></p>
<p>7 0.041478373 <a title="183-tfidf-7" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>8 0.041024439 <a title="183-tfidf-8" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>9 0.040747017 <a title="183-tfidf-9" href="./cvpr-2013-Category_Modeling_from_Just_a_Single_Labeling%3A_Use_Depth_Information_to_Guide_the_Learning_of_2D_Models.html">80 cvpr-2013-Category Modeling from Just a Single Labeling: Use Depth Information to Guide the Learning of 2D Models</a></p>
<p>10 0.037762471 <a title="183-tfidf-10" href="./cvpr-2013-A_Bayesian_Approach_to_Multimodal_Visual_Dictionary_Learning.html">5 cvpr-2013-A Bayesian Approach to Multimodal Visual Dictionary Learning</a></p>
<p>11 0.037668254 <a title="183-tfidf-11" href="./cvpr-2013-Part_Discovery_from_Partial_Correspondence.html">325 cvpr-2013-Part Discovery from Partial Correspondence</a></p>
<p>12 0.037127934 <a title="183-tfidf-12" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>13 0.035117894 <a title="183-tfidf-13" href="./cvpr-2013-Multi-target_Tracking_by_Rank-1_Tensor_Approximation.html">301 cvpr-2013-Multi-target Tracking by Rank-1 Tensor Approximation</a></p>
<p>14 0.03511022 <a title="183-tfidf-14" href="./cvpr-2013-A_New_Model_and_Simple_Algorithms_for_Multi-label_Mumford-Shah_Problems.html">20 cvpr-2013-A New Model and Simple Algorithms for Multi-label Mumford-Shah Problems</a></p>
<p>15 0.035016712 <a title="183-tfidf-15" href="./cvpr-2013-Efficient_2D-to-3D_Correspondence_Filtering_for_Scalable_3D_Object_Recognition.html">138 cvpr-2013-Efficient 2D-to-3D Correspondence Filtering for Scalable 3D Object Recognition</a></p>
<p>16 0.034846485 <a title="183-tfidf-16" href="./cvpr-2013-Deformable_Graph_Matching.html">106 cvpr-2013-Deformable Graph Matching</a></p>
<p>17 0.032936007 <a title="183-tfidf-17" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>18 0.032135736 <a title="183-tfidf-18" href="./cvpr-2013-Jointly_Aligning_and_Segmenting_Multiple_Web_Photo_Streams_for_the_Inference_of_Collective_Photo_Storylines.html">235 cvpr-2013-Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines</a></p>
<p>19 0.031761691 <a title="183-tfidf-19" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>20 0.031632446 <a title="183-tfidf-20" href="./cvpr-2013-Occlusion_Patterns_for_Object_Class_Detection.html">311 cvpr-2013-Occlusion Patterns for Object Class Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.081), (1, -0.011), (2, 0.003), (3, 0.006), (4, 0.025), (5, 0.001), (6, -0.013), (7, -0.018), (8, -0.017), (9, -0.002), (10, 0.005), (11, 0.011), (12, 0.003), (13, -0.014), (14, -0.013), (15, -0.049), (16, 0.007), (17, 0.014), (18, 0.051), (19, -0.021), (20, 0.047), (21, -0.011), (22, 0.012), (23, -0.025), (24, 0.025), (25, 0.002), (26, 0.011), (27, 0.026), (28, -0.024), (29, -0.035), (30, 0.013), (31, 0.019), (32, -0.031), (33, 0.026), (34, 0.04), (35, 0.031), (36, -0.009), (37, 0.045), (38, 0.012), (39, -0.057), (40, -0.039), (41, -0.03), (42, -0.005), (43, 0.025), (44, -0.025), (45, 0.011), (46, -0.016), (47, -0.039), (48, -0.027), (49, -0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91219068 <a title="183-lsi-1" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>2 0.7776072 <a title="183-lsi-2" href="./cvpr-2013-A_Fast_Approximate_AIB_Algorithm_for_Distributional_Word_Clustering.html">8 cvpr-2013-A Fast Approximate AIB Algorithm for Distributional Word Clustering</a></p>
<p>Author: Lei Wang, Jianjia Zhang, Luping Zhou, Wanqing Li</p><p>Abstract: Distributional word clustering merges the words having similar probability distributions to attain reliable parameter estimation, compact classification models and even better classification performance. Agglomerative Information Bottleneck (AIB) is one of the typical word clustering algorithms and has been applied to both traditional text classification and recent image recognition. Although enjoying theoretical elegance, AIB has one main issue on its computational efficiency, especially when clustering a large number of words. Different from existing solutions to this issue, we analyze the characteristics of its objective function the loss of mutual information, and show that by merely using the ratio of word-class joint probabilities of each word, good candidate word pairs for merging can be easily identified. Based on this finding, we propose a fast approximate AIB algorithm and show that it can significantly improve the computational efficiency of AIB while well maintaining or even slightly increasing its classification performance. Experimental study on both text and image classification benchmark data sets shows that our algorithm can achieve more than 100 times speedup on large real data sets over the state-of-the-art method.</p><p>3 0.71314692 <a title="183-lsi-3" href="./cvpr-2013-Lp-Norm_IDF_for_Large_Scale_Image_Search.html">275 cvpr-2013-Lp-Norm IDF for Large Scale Image Search</a></p>
<p>Author: Liang Zheng, Shengjin Wang, Ziqiong Liu, Qi Tian</p><p>Abstract: The Inverse Document Frequency (IDF) is prevalently utilized in the Bag-of-Words based image search. The basic idea is to assign less weight to terms with high frequency, and vice versa. However, the estimation of visual word frequency is coarse and heuristic. Therefore, the effectiveness of the conventional IDF routine is marginal, and far from optimal. To tackle thisproblem, thispaper introduces a novel IDF expression by the use of Lp-norm pooling technique. . edu . cn qit i @ c s an . ut s a . edu ? ? ? ? ? ? ? ? Carefully designed, the proposed IDF takes into account the term frequency, document frequency, the complexity of images, as well as the codebook information. Optimizing the IDF function towards optimal balancing between TF and pIDF weights yields the so-called Lp-norm IDF (pIDF). WpIDe sFho wwe ithghatts sth yeie clodsnv tehnetio son-acla IlDleFd i Ls a special case of our generalized version, and two novel IDFs, i.e. the average IDF and the max IDF, can also be derived from our formula. Further, by counting for the term-frequency in each image, the proposed Lp-norm IDF helps to alleviate the viismuaalg we,o trhde b purrosptionseesds phenomenon. Our method is evaluated through extensive experiments on three benchmark datasets (Oxford 5K, Paris 6K and Flickr 1M). We report a performance improvement of as large as 27.1% over the baseline approach. Moreover, since the Lp-norm IDF is computed offline, no extra computation or memory cost is introduced to the system at all.</p><p>4 0.67803347 <a title="183-lsi-4" href="./cvpr-2013-Topical_Video_Object_Discovery_from_Key_Frames_by_Modeling_Word_Co-occurrence_Prior.html">434 cvpr-2013-Topical Video Object Discovery from Key Frames by Modeling Word Co-occurrence Prior</a></p>
<p>Author: Gangqiang Zhao, Junsong Yuan, Gang Hua</p><p>Abstract: A topical video object refers to an object that is frequently highlighted in a video. It could be, e.g., the product logo and the leading actor/actress in a TV commercial. We propose a topic model that incorporates a word co-occurrence prior for efficient discovery of topical video objects from a set of key frames. Previous work using topic models, such as Latent Dirichelet Allocation (LDA), for video object discovery often takes a bag-of-visual-words representation, which ignored important co-occurrence information among the local features. We show that such data driven co-occurrence information from bottom-up can conveniently be incorporated in LDA with a Gaussian Markov prior, which combines top down probabilistic topic modeling with bottom up priors in a unified model. Our experiments on challenging videos demonstrate that the proposed approach can discover different types of topical objects despite variations in scale, view-point, color and lighting changes, or even partial occlusions. The efficacy of the co-occurrence prior is clearly demonstrated when comparing with topic models without such priors.</p><p>5 0.67043358 <a title="183-lsi-5" href="./cvpr-2013-Visual_Place_Recognition_with_Repetitive_Structures.html">456 cvpr-2013-Visual Place Recognition with Repetitive Structures</a></p>
<p>Author: Akihiko Torii, Josef Sivic, Tomáš Pajdla, Masatoshi Okutomi</p><p>Abstract: Repeated structures such as building facades, fences or road markings often represent a significant challenge for place recognition. Repeated structures are notoriously hard for establishing correspondences using multi-view geometry. Even more importantly, they violate thefeature independence assumed in the bag-of-visual-words representation which often leads to over-counting evidence and significant degradation of retrieval performance. In this work we show that repeated structures are not a nuisance but, when appropriately represented, theyform an importantdistinguishing feature for many places. We describe a representation of repeated structures suitable for scalable retrieval. It is based on robust detection of repeated image structures and a simple modification of weights in the bag-of-visual-word model. Place recognition results are shown on datasets of street-level imagery from Pittsburgh and San Francisco demonstrating significant gains in recognition performance compared to the standard bag-of-visual-words baseline and more recently proposed burstiness weighting.</p><p>6 0.66360235 <a title="183-lsi-6" href="./cvpr-2013-BFO_Meets_HOG%3A_Feature_Extraction_Based_on_Histograms_of_Oriented_p.d.f._Gradients_for_Image_Classification.html">53 cvpr-2013-BFO Meets HOG: Feature Extraction Based on Histograms of Oriented p.d.f. Gradients for Image Classification</a></p>
<p>7 0.65825945 <a title="183-lsi-7" href="./cvpr-2013-Harvesting_Mid-level_Visual_Concepts_from_Large-Scale_Internet_Images.html">200 cvpr-2013-Harvesting Mid-level Visual Concepts from Large-Scale Internet Images</a></p>
<p>8 0.58849806 <a title="183-lsi-8" href="./cvpr-2013-Scene_Text_Recognition_Using_Part-Based_Tree-Structured_Character_Detection.html">382 cvpr-2013-Scene Text Recognition Using Part-Based Tree-Structured Character Detection</a></p>
<p>9 0.58380187 <a title="183-lsi-9" href="./cvpr-2013-Capturing_Layers_in_Image_Collections_with_Componential_Models%3A_From_the_Layered_Epitome_to_the_Componential_Counting_Grid.html">78 cvpr-2013-Capturing Layers in Image Collections with Componential Models: From the Layered Epitome to the Componential Counting Grid</a></p>
<p>10 0.58236426 <a title="183-lsi-10" href="./cvpr-2013-Subcategory-Aware_Object_Classification.html">417 cvpr-2013-Subcategory-Aware Object Classification</a></p>
<p>11 0.57573235 <a title="183-lsi-11" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>12 0.56700295 <a title="183-lsi-12" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>13 0.55964214 <a title="183-lsi-13" href="./cvpr-2013-From_Local_Similarity_to_Global_Coding%3A_An_Application_to_Image_Classification.html">178 cvpr-2013-From Local Similarity to Global Coding: An Application to Image Classification</a></p>
<p>14 0.55874932 <a title="183-lsi-14" href="./cvpr-2013-A_Genetic_Algorithm-Based_Solver_for_Very_Large_Jigsaw_Puzzles.html">11 cvpr-2013-A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles</a></p>
<p>15 0.54641342 <a title="183-lsi-15" href="./cvpr-2013-Exploring_Implicit_Image_Statistics_for_Visual_Representativeness_Modeling.html">157 cvpr-2013-Exploring Implicit Image Statistics for Visual Representativeness Modeling</a></p>
<p>16 0.53339458 <a title="183-lsi-16" href="./cvpr-2013-Unsupervised_Joint_Object_Discovery_and_Segmentation_in_Internet_Images.html">450 cvpr-2013-Unsupervised Joint Object Discovery and Segmentation in Internet Images</a></p>
<p>17 0.52860802 <a title="183-lsi-17" href="./cvpr-2013-Adaptive_Active_Learning_for_Image_Classification.html">34 cvpr-2013-Adaptive Active Learning for Image Classification</a></p>
<p>18 0.52173311 <a title="183-lsi-18" href="./cvpr-2013-Discriminative_Brain_Effective_Connectivity_Analysis_for_Alzheimer%27s_Disease%3A_A_Kernel_Learning_Approach_upon_Sparse_Gaussian_Bayesian_Network.html">129 cvpr-2013-Discriminative Brain Effective Connectivity Analysis for Alzheimer's Disease: A Kernel Learning Approach upon Sparse Gaussian Bayesian Network</a></p>
<p>19 0.51970536 <a title="183-lsi-19" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>20 0.51760709 <a title="183-lsi-20" href="./cvpr-2013-Transfer_Sparse_Coding_for_Robust_Image_Representation.html">442 cvpr-2013-Transfer Sparse Coding for Robust Image Representation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.1), (16, 0.023), (26, 0.031), (28, 0.015), (33, 0.247), (67, 0.067), (69, 0.052), (80, 0.292), (87, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83419394 <a title="183-lda-1" href="./cvpr-2013-Long-Term_Occupancy_Analysis_Using_Graph-Based_Optimisation_in_Thermal_Imagery.html">272 cvpr-2013-Long-Term Occupancy Analysis Using Graph-Based Optimisation in Thermal Imagery</a></p>
<p>Author: Rikke Gade, Anders Jørgensen, Thomas B. Moeslund</p><p>Abstract: This paper presents a robust occupancy analysis system for thermal imaging. Reliable detection of people is very hard in crowded scenes, due to occlusions and segmentation problems. We therefore propose a framework that optimises the occupancy analysis over long periods by including information on the transition in occupancy, whenpeople enter or leave the monitored area. In stable periods, with no activity close to the borders, people are detected and counted which contributes to a weighted histogram. When activity close to the border is detected, local tracking is applied in order to identify a crossing. After a full sequence, the number of people during all periods are estimated using a probabilistic graph search optimisation. The system is tested on a total of 51,000 frames, captured in sports arenas. The mean error for a 30-minute period containing 3-13 people is 4.44 %, which is a half of the error percentage optained by detection only, and better than the results of comparable work. The framework is also tested on a public available dataset from an outdoor scene, which proves the generality of the method.</p><p>same-paper 2 0.80801702 <a title="183-lda-2" href="./cvpr-2013-GRASP_Recurring_Patterns_from_a_Single_View.html">183 cvpr-2013-GRASP Recurring Patterns from a Single View</a></p>
<p>Author: Jingchen Liu, Yanxi Liu</p><p>Abstract: We propose a novel unsupervised method for discovering recurring patterns from a single view. A key contribution of our approach is the formulation and validation of a joint assignment optimization problem where multiple visual words and object instances of a potential recurring pattern are considered simultaneously. The optimization is achieved by a greedy randomized adaptive search procedure (GRASP) with moves specifically designed for fast convergence. We have quantified systematically the performance of our approach under stressed conditions of the input (missing features, geometric distortions). We demonstrate that our proposed algorithm outperforms state of the art methods for recurring pattern discovery on a diverse set of 400+ real world and synthesized test images.</p><p>3 0.79280317 <a title="183-lda-3" href="./cvpr-2013-Poselet_Conditioned_Pictorial_Structures.html">335 cvpr-2013-Poselet Conditioned Pictorial Structures</a></p>
<p>Author: Leonid Pishchulin, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele</p><p>Abstract: In this paper we consider the challenging problem of articulated human pose estimation in still images. We observe that despite high variability of the body articulations, human motions and activities often simultaneously constrain the positions of multiple body parts. Modelling such higher order part dependencies seemingly comes at a cost of more expensive inference, which resulted in their limited use in state-of-the-art methods. In this paper we propose a model that incorporates higher order part dependencies while remaining efficient. We achieve this by defining a conditional model in which all body parts are connected a-priori, but which becomes a tractable tree-structured pictorial structures model once the image observations are available. In order to derive a set of conditioning variables we rely on the poselet-based features that have been shown to be effective for people detection but have so far found limited application for articulated human pose estimation. We demon- strate the effectiveness of our approach on three publicly available pose estimation benchmarks improving or being on-par with state of the art in each case.</p><p>4 0.77994877 <a title="183-lda-4" href="./cvpr-2013-Expanded_Parts_Model_for_Human_Attribute_and_Action_Recognition_in_Still_Images.html">153 cvpr-2013-Expanded Parts Model for Human Attribute and Action Recognition in Still Images</a></p>
<p>Author: Gaurav Sharma, Frédéric Jurie, Cordelia Schmid</p><p>Abstract: We propose a new model for recognizing human attributes (e.g. wearing a suit, sitting, short hair) and actions (e.g. running, riding a horse) in still images. The proposed model relies on a collection of part templates which are learnt discriminatively to explain specific scale-space locations in the images (in human centric coordinates). It avoids the limitations of highly structured models, which consist of a few (i.e. a mixture of) ‘average ’ templates. To learn our model, we propose an algorithm which automatically mines out parts and learns corresponding discriminative templates with their respective locations from a large number of candidate parts. We validate the method on recent challenging datasets: (i) Willow 7 actions [7], (ii) 27 Human Attributes (HAT) [25], and (iii) Stanford 40 actions [37]. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.</p><p>5 0.77766466 <a title="183-lda-5" href="./cvpr-2013-Illumination_Estimation_Based_on_Bilayer_Sparse_Coding.html">210 cvpr-2013-Illumination Estimation Based on Bilayer Sparse Coding</a></p>
<p>Author: Bing Li, Weihua Xiong, Weiming Hu, Houwen Peng</p><p>Abstract: Computational color constancy is a very important topic in computer vision and has attracted many researchers ’ attention. Recently, lots of research has shown the effects of using high level visual content cues for improving illumination estimation. However, nearly all the existing methods are essentially combinational strategies in which image ’s content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image ’s scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on real-world image sets show that our algorithm is superior to some prevailing illumination estimation methods, even better than some combinational methods.</p><p>6 0.76350582 <a title="183-lda-6" href="./cvpr-2013-Looking_Beyond_the_Image%3A_Unsupervised_Learning_for_Object_Saliency_and_Detection.html">273 cvpr-2013-Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection</a></p>
<p>7 0.73842812 <a title="183-lda-7" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>8 0.73213059 <a title="183-lda-8" href="./cvpr-2013-Human_Pose_Estimation_Using_Body_Parts_Dependent_Joint_Regressors.html">206 cvpr-2013-Human Pose Estimation Using Body Parts Dependent Joint Regressors</a></p>
<p>9 0.71654612 <a title="183-lda-9" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>10 0.71317029 <a title="183-lda-10" href="./cvpr-2013-Beyond_Physical_Connections%3A_Tree_Models_in_Human_Pose_Estimation.html">60 cvpr-2013-Beyond Physical Connections: Tree Models in Human Pose Estimation</a></p>
<p>11 0.71048695 <a title="183-lda-11" href="./cvpr-2013-Pose_from_Flow_and_Flow_from_Pose.html">334 cvpr-2013-Pose from Flow and Flow from Pose</a></p>
<p>12 0.70614511 <a title="183-lda-12" href="./cvpr-2013-PISA%3A_Pixelwise_Image_Saliency_by_Aggregating_Complementary_Appearance_Contrast_Measures_with_Spatial_Priors.html">322 cvpr-2013-PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors</a></p>
<p>13 0.70542526 <a title="183-lda-13" href="./cvpr-2013-A_Thousand_Frames_in_Just_a_Few_Words%3A_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching.html">28 cvpr-2013-A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</a></p>
<p>14 0.70467496 <a title="183-lda-14" href="./cvpr-2013-Tracking_Human_Pose_by_Tracking_Symmetric_Parts.html">439 cvpr-2013-Tracking Human Pose by Tracking Symmetric Parts</a></p>
<p>15 0.70457667 <a title="183-lda-15" href="./cvpr-2013-Computationally_Efficient_Regression_on_a_Dependency_Graph_for_Human_Pose_Estimation.html">89 cvpr-2013-Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation</a></p>
<p>16 0.70438933 <a title="183-lda-16" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>17 0.70290798 <a title="183-lda-17" href="./cvpr-2013-Human_Pose_Estimation_Using_a_Joint_Pixel-wise_and_Part-wise_Formulation.html">207 cvpr-2013-Human Pose Estimation Using a Joint Pixel-wise and Part-wise Formulation</a></p>
<p>18 0.70288628 <a title="183-lda-18" href="./cvpr-2013-Watching_Unlabeled_Video_Helps_Learn_New_Human_Actions_from_Very_Few_Labeled_Snapshots.html">459 cvpr-2013-Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</a></p>
<p>19 0.69932628 <a title="183-lda-19" href="./cvpr-2013-Representing_Videos_Using_Mid-level_Discriminative_Patches.html">355 cvpr-2013-Representing Videos Using Mid-level Discriminative Patches</a></p>
<p>20 0.69850141 <a title="183-lda-20" href="./cvpr-2013-Detecting_and_Naming_Actors_in_Movies_Using_Generative_Appearance_Models.html">120 cvpr-2013-Detecting and Naming Actors in Movies Using Generative Appearance Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
