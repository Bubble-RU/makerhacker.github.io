<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</title>
</head>

<body>
<p><a title="cvpr" href="../cvpr_home.html">cvpr</a> <a title="cvpr-2013" href="../home/cvpr2013_home.html">cvpr2013</a> <a title="cvpr-2013-140" href="#">cvpr2013-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</h1>
<br/><p>Source: <a title="cvpr-2013-140-pdf" href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Efficient_Color_Boundary_2013_CVPR_paper.pdf">pdf</a></p><p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>Reference: <a title="cvpr-2013-140-reference" href="../cvpr2013_reference/cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. [sent-6, score-0.265]
</p><p>2 In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. [sent-7, score-0.342]
</p><p>3 The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. [sent-8, score-0.92]
</p><p>4 The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). [sent-9, score-0.568]
</p><p>5 Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes. [sent-10, score-0.584]
</p><p>6 Introduction In natural scenes, color information plays an important role in human visual perception such as shape, texture, and object recognition [1]. [sent-12, score-0.286]
</p><p>7 From the viewpoint of engineering,  color information is also absolutely necessary for various image processing tasks, such as edge detection [2, 3], image segmentation [4], junction/corner detection [2, 5], etc. [sent-13, score-0.298]
</p><p>8 flowers) in color natural images are lost in the gray-scale space, especially for those boundaries between the local regions with different colors but equal luminance. [sent-19, score-0.397]
</p><p>9 In order to detect edges from color images, many early studies focused on extending those standard edge detectors, such as Sobel, Laplace, Canny operators [6], etc. [sent-20, score-0.277]
</p><p>10 These methods are inherently difficult to discriminate salient object boundaries and texture edges due that they respond to all the edges at the luminance or color changes. [sent-22, score-0.601]
</p><p>11 In recent decades, many new approaches have been developed for edge detection in complex color scenes. [sent-23, score-0.25]
</p><p>12 Color image and its boundary map (Top) provide more object information than gray-level image and its boundary map (Down). [sent-27, score-0.244]
</p><p>13 Along another line, the success of biologically based methods for edge detection in gray-level images [14-17] inspired us to build a biologically plausible framework for color boundary detection in natural images in an effect way. [sent-40, score-0.707]
</p><p>14 [18] proposed a new color descriptor based on color-opponent mechanisms, which obviously improves the performances of several classical object recognition and boundary detection systems. [sent-42, score-0.37]
</p><p>15 The receptive field of single-opponent cells of Type I (a) and Type II (b) in LGN (lateral geniculate nucleus), and oriented  double-opponent cells in V1 with balanced cone-input weightings (c) and unbalanced cone-input weightings (d). [sent-44, score-1.258]
</p><p>16 , Red-Green (R-G) and Blue-Yellow (B-Y) channels found in human visual system [21], exhibit exciting performance on color boundary detection. [sent-47, score-0.412]
</p><p>17 [3] computed the color gradients in these two opponent channels for color boundary detection. [sent-49, score-0.838]
</p><p>18 Zhou and Mel [22] applied custom “pairwise difference” oriented edge detector on the smoothed R-G and B-Y opponent channels. [sent-50, score-0.379]
</p><p>19 In order to obtain the complete contours of objects, these methods had to spend extra computational cost to combine more cues to detect  luminance boundaries [3]. [sent-52, score-0.489]
</p><p>20 It has been known that there are two color opponent channels for transmitting color information along the Retina-LGN (Lateral Geniculate Nucleus)-Cortex pathway, i. [sent-61, score-0.716]
</p><p>21 Many researches have reported that color information is processed in the visual pathway in opponent manners. [sent-66, score-0.519]
</p><p>22 The ganglion or LGN cells are found to have single-opponent receptive field (RF) and cells in the primary visual cortex (V1) have double-opponent RF [19, 26]. [sent-67, score-0.928]
</p><p>23 There are mainly two types of single-opponent cells in LGN: Type Icells have center-surround opponent RF (Figure 2(a)) and in contrast, Type II cells have center-only opponent RF (Figure 2(b) [27, 28]. [sent-68, score-1.018]
</p><p>24 In V1, the RF of neurons  shows more complex properties. [sent-69, score-0.282]
</p><p>25 Their RFs are both chromatically and spatially opponent [28-30]. [sent-70, score-0.344]
</p><p>26 Especially, It has been reported that some neurons in V1, called oriented double-opponent neurons, are orientation-selective for both chromatic and achromatic patterns [3 1], which was considered to play a crucial role in boundary detection in (color) natural scenes. [sent-71, score-0.932]
</p><p>27 The RF structure with balanced cone-input weightings is shown in Figure 2(c), which can respond well to color-defined boundaries [20]. [sent-74, score-0.511]
</p><p>28 Subsequently, RF structure with unbalanced cone weightings was also reported (Figure 2(d)) [19, 26], and the cells with such RF structure may respond to both achromatic and iso-luminant gratings. [sent-75, score-0.951]
</p><p>29 In this paper, we introduce a new biologically plausible model that exhibits better performance on capturing simultaneously both chromatic and achromatic boundaries. [sent-76, score-0.535]
</p><p>30 In detail, in the first layer (Cone layer), Gaussian filters simulating cone RFs are used to obtain local information on individual color component (red, green, blue and yellow) of the input color image. [sent-78, score-0.633]
</p><p>31 In the Ganglion/LGN layer, the responses of single-opponent neurons are computed with two pairs of opponent color components: R-G/G-R and B-Y/Y-B. [sent-79, score-0.832]
</p><p>32 In the  last layer (Cortex layer), multiple oriented double-opponent filters are used to extract boundaries and a max operator is used to combine boundaries over all orientations in each opponent channel. [sent-80, score-0.926]
</p><p>33 Finally, we compute the maximum to combine the boundaries across all opponent channels. [sent-81, score-0.487]
</p><p>34 As briefly mentioned above, in this work, we simulate the biological mechanisms of color information processing along the Retina-LGN-Cortex visual pathway and propose a feedforward hierarchical system for boundary detection in real natural scenes. [sent-82, score-0.725]
</p><p>35 Boundary Detection System Our framework is a feedforward hierarchical model including three layers, which correspond to the level of retina, LGN and primary visual cortex (V1) of the visual system, respectively. [sent-88, score-0.287]
</p><p>36 The general flowchart is summarized in Figure 3, in which we just show the computational steps in the R-G channel, and the other channels share the similar computational steps. [sent-90, score-0.207]
</p><p>37 In order to obtain the local color information, Gaussian filters are used to simulate the receptive field of the cones in the retina [19, 20]. [sent-92, score-0.6]
</p><p>38 Ganglion/LGN Layer: Generally, the retinal ganglion cells and LGN cells have similar RF properties. [sent-101, score-0.518]
</p><p>39 The cells in the ganglion/LGN layer have single-opponent receptive fields and show spatially low-pass property. [sent-103, score-0.547]
</p><p>40 The cells in this layer receive the cone outputs, and their responses can be described as S( x , y )  ? [sent-104, score-0.57]
</p><p>41 [0,1]  (1)  (2)  where w1 and w2 are the connection weightings from cones to ganglion cells. [sent-117, score-0.334]
</p><p>42 0 , we get the responses of R-off/G-on cells ( Figure 3). [sent-123, score-0.307]
</p><p>43 Single-opponent cells in ganglion/LGN layer are important for separating color and achromatic information, which is clearly shown by Equation 1. [sent-124, score-0.762]
</p><p>44 When the ganglion/LGN cells have balanced cone-input weightings, i. [sent-125, score-0.251]
</p><p>45 w1 w2 , the ganglion/LGN cells may be blind to achromatic information, because achromatic information provides same component values in different channels and w1 and w2 are in opposite sign. [sent-127, score-0.913]
</p><p>46 In contrast, when the cone-input weightings are unbalanced (i. [sent-128, score-0.256]
</p><p>47 Cortex Layer: In the cortex layer of V1, the receptive fields of most color- and color-luminance-sensitive neurons are both chromatically and spatially opponent. [sent-134, score-0.824]
</p><p>48 In particular, the oriented double-opponent cells are considered to play an important role in color boundary detection [3 1]. [sent-135, score-0.628]
</p><p>49 We simulate the receptive fields of these oriented double-opponent V1 cells shown in Figure 2(c-d) as  RF( x , y ;? [sent-136, score-0.509]
</p><p>50 The full computational steps of the proposed system for color boundary detection. [sent-169, score-0.359]
</p><p>51 1 means that V1 neurons have larger RF than that of the ganglion/LGN cells. [sent-188, score-0.282]
</p><p>52 Figure 4(left) shows a simple example of RF model of the oriented double-opponent neurons in V1 with vertical orientation. [sent-189, score-0.329]
</p><p>53 ) to detect the boundaries defined by the same color pairs but with different polarities. [sent-208, score-0.398]
</p><p>54 The boundary responses at each orientation is given by D( x , y ;? [sent-209, score-0.251]
</p><p>55 Then, a max mechanism is used across all orientations to obtain the responses to the boundaries in R-G opponent channel according to D( x , y ) ? [sent-263, score-0.655]
</p><p>56 Left: The receptive field model of the oriented double-opponent neurons in V1 with vertical orientation. [sent-274, score-0.507]
</p><p>57 It includes two spatially separated parts, which receive opposite opponent outputs from LGN. [sent-275, score-0.334]
</p><p>58 Right: Two neurons with 180 degree difference between their preferred orientations ( ? [sent-276, score-0.321]
</p><p>59 ) detect the color boundaries with the same orientation but different polarities (i. [sent-284, score-0.435]
</p><p>60 Then the boundaries are detected in four channels (i. [sent-290, score-0.287]
</p><p>61 {rg, gr, by, yb})  (9)  We call our system as CO(w) , which means the color opponent (CO) system with a cone-input weighting of w . [sent-309, score-0.522]
</p><p>62 Experiments To begin with, we evaluate the effect of the cone-input weightings (i. [sent-311, score-0.212]
</p><p>63 Figure 6 illustrates the different responses of our model CO(w) to color and achromatic boundaries with different w . [sent-314, score-0.714]
</p><p>64 and luminance-defined boundaries simultaneously with an opponent way. [sent-333, score-0.487]
</p><p>65 For the pure color and brightness boundaries (top row in Figure 6), the model shows equivalent responses to the two types of boundaries when w ? [sent-334, score-0.726]
</p><p>66 In contrast, both the color- and luminance-defined boundaries in the natural image (bottom row in Figure 6) are well responded when w ? [sent-336, score-0.233]
</p><p>67 This may because that the absolute values of color difference are normally weaker than that of the brightness difference in natural images. [sent-340, score-0.321]
</p><p>68 Figure 7 clearly shows that our model has wonderful ability of suppressing the noises while reconstructing the broken portions of the  achromatic boundaries with various cone-input weightings. [sent-342, score-0.531]
</p><p>69 From left to right: Original images and the responses of CO(w) with different cone-input weightings ( w ): -1. [sent-343, score-0.304]
</p><p>70 The boundaries of each image with different w are presented in Figure 8 (the forth to the eighth column), and the optimal results are marked with bold rectangles. [sent-373, score-0.227]
</p><p>71 It is clear from Figure 8 that our model exhibits more powerful ability of extracting the structured object boundaries with lower color or brightness contrast. [sent-374, score-0.502]
</p><p>72 Furthermore, our model has the ability to detect the color or achromatic boundaries in a flexible manner. [sent-375, score-0.693]
</p><p>73 In contrast, the Pb method detects brightness, color and texture boundaries separately, and then combines them with a specific supervised learning technique. [sent-377, score-0.357]
</p><p>74 In addition, in order to obtain the color and brightness boundaries of objects, more computational costs are required to detect color and brightness information separately, train and combine multiple cues, such as [3]. [sent-379, score-0.771]
</p><p>75 5 and different cone-input weightings  w)  with Pb detector [3]. [sent-388, score-0.212]
</p><p>76 Although our method responds to more texture edges, which may be useless for some high-level visual perceptions (such as shape-based object recognition), the responses to texture edges are usually quite weaker than boundaries (the fifth column in Figure 9). [sent-399, score-0.36]
</p><p>77 Left: The performances of our methods with different cone-input weightings at a certain scale ( ? [sent-436, score-0.248]
</p><p>78 Conclusions and Future Work  This paper presented a novel biologically plausible computational model for contour detection of color images. [sent-472, score-0.503]
</p><p>79 We try to build an efficient model by simulating the processing manners from the retina to the primary cortex (V1) of the human visual system. [sent-473, score-0.425]
</p><p>80 Local information is processed hierarchically with the single-opponent cells at the ganglion/LGN levels and the double-opponent cells in V1. [sent-474, score-0.43]
</p><p>81 Our model exhibits excellent capability of detecting both color and luminance boundaries synchronously in a time-saving manner. [sent-475, score-0.494]
</p><p>82 Color-luminance cells (29%) in V1 respond to both the color and achromatic information, but there are also specific neurons in V1 responding exclusively to either color (11%) or luminance (60%) information [3 1]. [sent-476, score-1.266]
</p><p>83 integrate color and luminance information in V1 or higher level cortexes of the visual system is less known. [sent-482, score-0.302]
</p><p>84 ) [34, 35] and the context influence between neurons also arouse widely concern [36, 37]. [sent-484, score-0.282]
</p><p>85 Tomasi, "Edge, junction, and corner detection using color distributions," IEEE Trans. [sent-502, score-0.212]
</p><p>86 Malik, "Learning to detect natural image boundaries using local brightness, color, and texture cues," IEEE Trans. [sent-511, score-0.274]
</p><p>87 Ren, "Multi-scale improves boundary detection in natural  [8]  [9]  [10]  [11] [12] [13] [14]  [15]  [16]  [17]  [18]  images," in ECCV, 2008, pp. [sent-538, score-0.21]
</p><p>88 Neri, "A biologically motivated multiresolution approach to contour detection," EURASIP Journal on Applied Signal Processing, vol. [sent-581, score-0.211]
</p><p>89 Westenberg, "Suppression of contour perception by band-limited noise and its relation to nonclassical receptive field inhibition," Biological Cybernetics, vol. [sent-587, score-0.385]
</p><p>90 Westenberg, "Contour detection based on nonclassical receptive field inhibition," IEEE Trans. [sent-594, score-0.276]
</p><p>91 , "Advances in color science: from retina to behavior," The Journal of Neuroscience, vol. [sent-605, score-0.313]
</p><p>92 Mel, "Cue combination and color edge detection in natural scenes," Journal of Vision, vol. [sent-627, score-0.29]
</p><p>93 Lennie, "Chromatic mechanisms in lateral geniculate nucleus of macaque," The Journal of Physiology, vol. [sent-647, score-0.354]
</p><p>94 Hubel, "Spatial and chromatic interactions in the lateral geniculate body of the rhesus monkey," Journal of Neurophysiology, vol. [sent-659, score-0.263]
</p><p>95 Conway, "Spatial structure of cone inputs to color cells in alert macaque primary visual cortex (V-1)," The Journal of Neuroscience, vol. [sent-664, score-0.866]
</p><p>96 Shapley, "The orientation selectivity of color-responsive neurons in macaque V1," The Journal of Neuroscience, vol. [sent-672, score-0.429]
</p><p>97 Hawken, "Neural mechanisms for color perception in the primary visual cortex," Current Opinion in Neurobiology, vol. [sent-677, score-0.418]
</p><p>98 Shapley, "The spatial transformation of color in the primary visual cortex of the macaque monkey," Nature Neuroscience, vol. [sent-685, score-0.506]
</p><p>99 Li, "Cue invariant detection of centre–surround discontinuity by V1 neurons in awake macaque monkey," The Journal of Physiology, vol. [sent-707, score-0.44]
</p><p>100 Frégnac, "The “silent” surround of V1 receptive fields: theory and experiments," Journal of physiology-Paris, vol. [sent-722, score-0.211]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('opponent', 0.294), ('neurons', 0.282), ('achromatic', 0.265), ('cells', 0.215), ('weightings', 0.212), ('rf', 0.199), ('boundaries', 0.193), ('receptive', 0.178), ('lgn', 0.174), ('color', 0.164), ('cortex', 0.16), ('retina', 0.149), ('pb', 0.148), ('cone', 0.145), ('mechanisms', 0.132), ('co', 0.131), ('boundary', 0.122), ('layer', 0.118), ('macaque', 0.11), ('contour', 0.107), ('luminance', 0.106), ('biologically', 0.104), ('geniculate', 0.099), ('hawken', 0.099), ('shapley', 0.099), ('chromatic', 0.096), ('channels', 0.094), ('responses', 0.092), ('ganglion', 0.088), ('brightness', 0.084), ('neuroscience', 0.08), ('primary', 0.072), ('contours', 0.072), ('respond', 0.07), ('lateral', 0.068), ('petkov', 0.066), ('martin', 0.063), ('pathway', 0.061), ('monkey', 0.061), ('rfs', 0.058), ('inhibition', 0.058), ('saving', 0.058), ('sr', 0.056), ('nucleus', 0.055), ('feedforward', 0.055), ('malik', 0.053), ('fowlkes', 0.051), ('perception', 0.05), ('chromatically', 0.05), ('conway', 0.05), ('inflexible', 0.05), ('lennie', 0.05), ('nonclassical', 0.05), ('westenberg', 0.05), ('detection', 0.048), ('oriented', 0.047), ('unbalanced', 0.044), ('manners', 0.044), ('mel', 0.044), ('noised', 0.044), ('broken', 0.043), ('filters', 0.042), ('detect', 0.041), ('untangling', 0.041), ('physiology', 0.041), ('responds', 0.041), ('computational', 0.041), ('natural', 0.04), ('opposite', 0.04), ('orientations', 0.039), ('plausible', 0.039), ('journal', 0.039), ('pocv', 0.039), ('capacity', 0.038), ('biological', 0.038), ('edge', 0.038), ('channel', 0.037), ('china', 0.037), ('neuron', 0.037), ('ultrametric', 0.037), ('orientation', 0.037), ('cues', 0.036), ('fields', 0.036), ('performances', 0.036), ('balanced', 0.036), ('blind', 0.034), ('eighth', 0.034), ('cones', 0.034), ('edges', 0.034), ('normally', 0.033), ('surround', 0.033), ('canny', 0.033), ('simulate', 0.033), ('zoomed', 0.032), ('role', 0.032), ('system', 0.032), ('exhibits', 0.031), ('flowchart', 0.031), ('ability', 0.03), ('cg', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="140-tfidf-1" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>2 0.13236052 <a title="140-tfidf-2" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>3 0.12335005 <a title="140-tfidf-3" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>4 0.098879389 <a title="140-tfidf-4" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>Author: Saurabh Gupta, Pablo Arbeláez, Jitendra Malik</p><p>Abstract: We address the problems of contour detection, bottomup grouping and semantic segmentation using RGB-D data. We focus on the challenging setting of cluttered indoor scenes, and evaluate our approach on the recently introduced NYU-Depth V2 (NYUD2) dataset [27]. We propose algorithms for object boundary detection and hierarchical segmentation that generalize the gPb − ucm approach of [se2]g mbeyn mtaatkioinng t effective use oef t dheep gthP information. Wroea schho owf that our system can label each contour with its type (depth, normal or albedo). We also propose a generic method for long-range amodal completion of surfaces and show its effectiveness in grouping. We then turn to the problem of semantic segmentation and propose a simple approach that classifies superpixels into the 40 dominant object categories in NYUD2. We use both generic and class-specific features to encode the appearance and geometry of objects. We also show how our approach can be used for scene classification, and how this contextual information in turn improves object recognition. In all of these tasks, we report significant improvements over the state-of-the-art.</p><p>5 0.08723034 <a title="140-tfidf-5" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>Author: Camillo Jose Taylor</p><p>Abstract: In this paper we explore approaches to accelerating segmentation and edge detection algorithms based on the gPb framework. The paper characterizes the performance of a simple but effective edge detection scheme which can be computed rapidly and offers performance that is competitive with the pB detector. The paper also describes an approach for computing a reduced order normalized cut that captures the essential features of the original problem but can be computed in less than half a second on a standard computing platform.</p><p>6 0.086682372 <a title="140-tfidf-6" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>7 0.085113853 <a title="140-tfidf-7" href="./cvpr-2013-Semi-supervised_Node_Splitting_for_Random_Forest_Construction.html">390 cvpr-2013-Semi-supervised Node Splitting for Random Forest Construction</a></p>
<p>8 0.084020838 <a title="140-tfidf-8" href="./cvpr-2013-Deep_Convolutional_Network_Cascade_for_Facial_Point_Detection.html">104 cvpr-2013-Deep Convolutional Network Cascade for Facial Point Detection</a></p>
<p>9 0.083822154 <a title="140-tfidf-9" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>10 0.083747171 <a title="140-tfidf-10" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>11 0.083626032 <a title="140-tfidf-11" href="./cvpr-2013-Evaluation_of_Color_STIPs_for_Human_Action_Recognition.html">149 cvpr-2013-Evaluation of Color STIPs for Human Action Recognition</a></p>
<p>12 0.079535738 <a title="140-tfidf-12" href="./cvpr-2013-Hierarchical_Saliency_Detection.html">202 cvpr-2013-Hierarchical Saliency Detection</a></p>
<p>13 0.077473633 <a title="140-tfidf-13" href="./cvpr-2013-Semi-supervised_Learning_of_Feature_Hierarchies_for_Object_Detection_in_a_Video.html">388 cvpr-2013-Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video</a></p>
<p>14 0.07282909 <a title="140-tfidf-14" href="./cvpr-2013-Recovering_Stereo_Pairs_from_Anaglyphs.html">352 cvpr-2013-Recovering Stereo Pairs from Anaglyphs</a></p>
<p>15 0.072327368 <a title="140-tfidf-15" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>16 0.072242588 <a title="140-tfidf-16" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>17 0.070856683 <a title="140-tfidf-17" href="./cvpr-2013-Layer_Depth_Denoising_and_Completion_for_Structured-Light_RGB-D_Cameras.html">245 cvpr-2013-Layer Depth Denoising and Completion for Structured-Light RGB-D Cameras</a></p>
<p>18 0.064058274 <a title="140-tfidf-18" href="./cvpr-2013-Explicit_Occlusion_Modeling_for_3D_Object_Class_Representations.html">154 cvpr-2013-Explicit Occlusion Modeling for 3D Object Class Representations</a></p>
<p>19 0.060406651 <a title="140-tfidf-19" href="./cvpr-2013-Video_Object_Segmentation_through_Spatially_Accurate_and_Temporally_Dense_Extraction_of_Primary_Object_Regions.html">455 cvpr-2013-Video Object Segmentation through Spatially Accurate and Temporally Dense Extraction of Primary Object Regions</a></p>
<p>20 0.058876462 <a title="140-tfidf-20" href="./cvpr-2013-SCaLE%3A_Supervised_and_Cascaded_Laplacian_Eigenmaps_for_Visual_Object_Recognition_Based_on_Nearest_Neighbors.html">371 cvpr-2013-SCaLE: Supervised and Cascaded Laplacian Eigenmaps for Visual Object Recognition Based on Nearest Neighbors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/cvpr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.143), (1, 0.007), (2, 0.04), (3, 0.019), (4, 0.031), (5, 0.005), (6, 0.01), (7, 0.037), (8, -0.02), (9, -0.019), (10, 0.026), (11, -0.054), (12, 0.012), (13, -0.037), (14, 0.076), (15, 0.031), (16, -0.009), (17, -0.034), (18, 0.057), (19, 0.079), (20, 0.018), (21, 0.062), (22, -0.071), (23, -0.067), (24, 0.018), (25, 0.097), (26, 0.121), (27, -0.015), (28, -0.013), (29, 0.042), (30, 0.008), (31, 0.052), (32, -0.054), (33, 0.022), (34, 0.08), (35, 0.055), (36, 0.047), (37, 0.145), (38, 0.031), (39, 0.075), (40, -0.016), (41, 0.072), (42, -0.009), (43, 0.03), (44, 0.017), (45, 0.041), (46, 0.02), (47, -0.008), (48, -0.03), (49, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9330318 <a title="140-lsi-1" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>2 0.78335339 <a title="140-lsi-2" href="./cvpr-2013-Boundary_Detection_Benchmarking%3A_Beyond_F-Measures.html">72 cvpr-2013-Boundary Detection Benchmarking: Beyond F-Measures</a></p>
<p>Author: Xiaodi Hou, Alan Yuille, Christof Koch</p><p>Abstract: For an ill-posed problem like boundary detection, human labeled datasets play a critical role. Compared with the active research on finding a better boundary detector to refresh the performance record, there is surprisingly little discussion on the boundary detection benchmark itself. The goal of this paper is to identify the potential pitfalls of today’s most popular boundary benchmark, BSDS 300. In the paper, we first introduce a psychophysical experiment to show that many of the “weak” boundary labels are unreliable and may contaminate the benchmark. Then we analyze the computation of f-measure and point out that the current benchmarking protocol encourages an algorithm to bias towards those problematic “weak” boundary labels. With this evidence, we focus on a new problem of detecting strong boundaries as one alternative. Finally, we assess the performances of 9 major algorithms on different ways of utilizing the dataset, suggesting new directions for improvements.</p><p>3 0.77460128 <a title="140-lsi-3" href="./cvpr-2013-Winding_Number_for_Region-Boundary_Consistent_Salient_Contour_Extraction.html">468 cvpr-2013-Winding Number for Region-Boundary Consistent Salient Contour Extraction</a></p>
<p>Author: Yansheng Ming, Hongdong Li, Xuming He</p><p>Abstract: This paper aims to extract salient closed contours from an image. For this vision task, both region segmentation cues (e.g. color/texture homogeneity) and boundary detection cues (e.g. local contrast, edge continuity and contour closure) play important and complementary roles. In this paper we show how to combine both cues in a unified framework. The main focus is given to how to maintain the consistency (compatibility) between the region cues and the boundary cues. To this ends, we introduce the use of winding number–a well-known concept in topology–as a powerful mathematical device. By this device, the region-boundary consistency is represented as a set of simple linear relationships. Our method is applied to the figure-ground segmentation problem. The experiments show clearly improved results.</p><p>4 0.76375842 <a title="140-lsi-4" href="./cvpr-2013-Incorporating_User_Interaction_and_Topological_Constraints_within_Contour_Completion_via_Discrete_Calculus.html">222 cvpr-2013-Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus</a></p>
<p>Author: Jia Xu, Maxwell D. Collins, Vikas Singh</p><p>Abstract: We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ∼ 1000 images, our experimOenn tasn suggest vthea dt a assmetal ol fa m∼ou 1n0t0 of ismidaeg knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.</p><p>5 0.71892715 <a title="140-lsi-5" href="./cvpr-2013-Towards_Fast_and_Accurate_Segmentation.html">437 cvpr-2013-Towards Fast and Accurate Segmentation</a></p>
<p>Author: Camillo Jose Taylor</p><p>Abstract: In this paper we explore approaches to accelerating segmentation and edge detection algorithms based on the gPb framework. The paper characterizes the performance of a simple but effective edge detection scheme which can be computed rapidly and offers performance that is competitive with the pB detector. The paper also describes an approach for computing a reduced order normalized cut that captures the essential features of the original problem but can be computed in less than half a second on a standard computing platform.</p><p>6 0.69314373 <a title="140-lsi-6" href="./cvpr-2013-Sketch_Tokens%3A_A_Learned_Mid-level_Representation_for_Contour_and_Object_Detection.html">401 cvpr-2013-Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection</a></p>
<p>7 0.63719213 <a title="140-lsi-7" href="./cvpr-2013-Active_Contours_with_Group_Similarity.html">33 cvpr-2013-Active Contours with Group Similarity</a></p>
<p>8 0.57909691 <a title="140-lsi-8" href="./cvpr-2013-Perceptual_Organization_and_Recognition_of_Indoor_Scenes_from_RGB-D_Images.html">329 cvpr-2013-Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</a></p>
<p>9 0.57411808 <a title="140-lsi-9" href="./cvpr-2013-Measures_and_Meta-Measures_for_the_Supervised_Evaluation_of_Image_Segmentation.html">281 cvpr-2013-Measures and Meta-Measures for the Supervised Evaluation of Image Segmentation</a></p>
<p>10 0.53549677 <a title="140-lsi-10" href="./cvpr-2013-Learning_the_Change_for_Automatic_Image_Cropping.html">263 cvpr-2013-Learning the Change for Automatic Image Cropping</a></p>
<p>11 0.53381348 <a title="140-lsi-11" href="./cvpr-2013-Image_Understanding_from_Experts%27_Eyes_by_Modeling_Perceptual_Skill_of_Diagnostic_Reasoning_Processes.html">214 cvpr-2013-Image Understanding from Experts' Eyes by Modeling Perceptual Skill of Diagnostic Reasoning Processes</a></p>
<p>12 0.53352988 <a title="140-lsi-12" href="./cvpr-2013-Keypoints_from_Symmetries_by_Wave_Propagation.html">240 cvpr-2013-Keypoints from Symmetries by Wave Propagation</a></p>
<p>13 0.52452087 <a title="140-lsi-13" href="./cvpr-2013-Joint_Spectral_Correspondence_for_Disparate_Image_Matching.html">234 cvpr-2013-Joint Spectral Correspondence for Disparate Image Matching</a></p>
<p>14 0.5090543 <a title="140-lsi-14" href="./cvpr-2013-Pixel-Level_Hand_Detection_in_Ego-centric_Videos.html">332 cvpr-2013-Pixel-Level Hand Detection in Ego-centric Videos</a></p>
<p>15 0.50566334 <a title="140-lsi-15" href="./cvpr-2013-Reconstructing_Loopy_Curvilinear_Structures_Using_Integer_Programming.html">350 cvpr-2013-Reconstructing Loopy Curvilinear Structures Using Integer Programming</a></p>
<p>16 0.5014025 <a title="140-lsi-16" href="./cvpr-2013-Vantage_Feature_Frames_for_Fine-Grained_Categorization.html">452 cvpr-2013-Vantage Feature Frames for Fine-Grained Categorization</a></p>
<p>17 0.49822041 <a title="140-lsi-17" href="./cvpr-2013-Classification_of_Tumor_Histology_via_Morphometric_Context.html">83 cvpr-2013-Classification of Tumor Histology via Morphometric Context</a></p>
<p>18 0.4970848 <a title="140-lsi-18" href="./cvpr-2013-Image_Segmentation_by_Cascaded_Region_Agglomeration.html">212 cvpr-2013-Image Segmentation by Cascaded Region Agglomeration</a></p>
<p>19 0.4892669 <a title="140-lsi-19" href="./cvpr-2013-Discriminative_Color_Descriptors.html">130 cvpr-2013-Discriminative Color Descriptors</a></p>
<p>20 0.48438716 <a title="140-lsi-20" href="./cvpr-2013-Dense_Segmentation-Aware_Descriptors.html">112 cvpr-2013-Dense Segmentation-Aware Descriptors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/cvpr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.106), (16, 0.031), (26, 0.043), (29, 0.303), (33, 0.217), (67, 0.074), (69, 0.04), (87, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78550756 <a title="140-lda-1" href="./cvpr-2013-Sensing_and_Recognizing_Surface_Textures_Using_a_GelSight_Sensor.html">391 cvpr-2013-Sensing and Recognizing Surface Textures Using a GelSight Sensor</a></p>
<p>Author: Rui Li, Edward H. Adelson</p><p>Abstract: Sensing surface textures by touch is a valuable capability for robots. Until recently it wwas difficult to build a compliant sensor with high sennsitivity and high resolution. The GelSight sensor is coompliant and offers sensitivity and resolution exceeding that of the human fingertips. This opens the possibility of measuring and recognizing highly detailed surface texxtures. The GelSight sensor, when pressed against a surfacce, delivers a height map. This can be treated as an image, aand processed using the tools of visual texture analysis. WWe have devised a simple yet effective texture recognitioon system based on local binary patterns, and enhanced it by the use of a multi-scale pyramid and a Hellinger ddistance metric. We built a database with 40 classes of taactile textures using materials such as fabric, wood, and sanndpaper. Our system can correctly categorize materials fromm this database with high accuracy. This suggests that the GGelSight sensor can be useful for material recognition by roobots.</p><p>same-paper 2 0.78129572 <a title="140-lda-2" href="./cvpr-2013-Efficient_Color_Boundary_Detection_with_Color-Opponent_Mechanisms.html">140 cvpr-2013-Efficient Color Boundary Detection with Color-Opponent Mechanisms</a></p>
<p>Author: Kaifu Yang, Shaobing Gao, Chaoyi Li, Yongjie Li</p><p>Abstract: Color information plays an important role in better understanding of natural scenes by at least facilitating discriminating boundaries of objects or areas. In this study, we propose a new framework for boundary detection in complex natural scenes based on the color-opponent mechanisms of the visual system. The red-green and blue-yellow color opponent channels in the human visual system are regarded as the building blocks for various color perception tasks such as boundary detection. The proposed framework is a feedforward hierarchical model, which has direct counterpart to the color-opponent mechanisms involved in from the retina to the primary visual cortex (V1). Results show that our simple framework has excellent ability to flexibly capture both the structured chromatic and achromatic boundaries in complex scenes.</p><p>3 0.75025684 <a title="140-lda-3" href="./cvpr-2013-Submodular_Salient_Region_Detection.html">418 cvpr-2013-Submodular Salient Region Detection</a></p>
<p>Author: Zhuolin Jiang, Larry S. Davis</p><p>Abstract: The problem of salient region detection is formulated as the well-studied facility location problem from operations research. High-level priors are combined with low-level features to detect salient regions. Salient region detection is achieved by maximizing a submodular objective function, which maximizes the total similarities (i.e., total profits) between the hypothesized salient region centers (i.e., facility locations) and their region elements (i.e., clients), and penalizes the number of potential salient regions (i.e., the number of open facilities). The similarities are efficiently computedbyfinding a closed-form harmonic solution on the constructed graph for an input image. The saliency of a selected region is modeled in terms of appearance and spatial location. By exploiting the submodularity properties of the objectivefunction, a highly efficient greedy-based optimization algorithm can be employed. This algorithm is guaranteed to be at least a (e − 1)/e ≈ 0.632-approximation to t heeed optimum. lEeaxpster aim (een −tal 1 r)e/seult ≈s d 0e.m63o2n-satrpaptero txhimata our approach outperforms several recently proposed saliency detection approaches.</p><p>4 0.74160028 <a title="140-lda-4" href="./cvpr-2013-Adaptive_Compressed_Tomography_Sensing.html">35 cvpr-2013-Adaptive Compressed Tomography Sensing</a></p>
<p>Author: Oren Barkan, Jonathan Weill, Amir Averbuch, Shai Dekel</p><p>Abstract: One of the main challenges in Computed Tomography (CT) is how to balance between the amount of radiation the patient is exposed to during scan time and the quality of the CT image. We propose a mathematical model for adaptive CT acquisition whose goal is to reduce dosage levels while maintaining high image quality at the same time. The adaptive algorithm iterates between selective limited acquisition and improved reconstruction, with the goal of applying only the dose level required for sufficient image quality. The theoretical foundation of the algorithm is nonlinear Ridgelet approximation and a discrete form of Ridgelet analysis is used to compute the selective acquisition steps that best capture the image edges. We show experimental results where for the same number of line projections, the adaptive model produces higher image quality, when compared with standard limited angle, non-adaptive acquisition algorithms.</p><p>5 0.70331705 <a title="140-lda-5" href="./cvpr-2013-Improved_Image_Set_Classification_via_Joint_Sparse_Approximated_Nearest_Subspaces.html">215 cvpr-2013-Improved Image Set Classification via Joint Sparse Approximated Nearest Subspaces</a></p>
<p>Author: Shaokang Chen, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell</p><p>Abstract: Existing multi-model approaches for image set classification extract local models by clustering each image set individually only once, with fixed clusters used for matching with other image sets. However, this may result in the two closest clusters to represent different characteristics of an object, due to different undesirable environmental conditions (such as variations in illumination and pose). To address this problem, we propose to constrain the clustering of each query image set by forcing the clusters to have resemblance to the clusters in the gallery image sets. We first define a Frobenius norm distance between subspaces over Grassmann manifolds based on reconstruction error. We then extract local linear subspaces from a gallery image set via sparse representation. For each local linear subspace, we adaptively construct the corresponding closest subspace from the samples of a probe image set by joint sparse representation. We show that by minimising the sparse representation reconstruction error, we approach the nearest point on a Grassmann manifold. Experiments on Honda, ETH-80 and Cambridge-Gesture datasets show that the proposed method consistently outperforms several other recent techniques, such as Affine Hull based Image Set Distance (AHISD), Sparse Approximated Nearest Points (SANP) and Manifold Discriminant Analysis (MDA).</p><p>6 0.69986343 <a title="140-lda-6" href="./cvpr-2013-Probabilistic_Elastic_Matching_for_Pose_Variant_Face_Verification.html">338 cvpr-2013-Probabilistic Elastic Matching for Pose Variant Face Verification</a></p>
<p>7 0.66308409 <a title="140-lda-7" href="./cvpr-2013-Learning_Collections_of_Part_Models_for_Object_Recognition.html">248 cvpr-2013-Learning Collections of Part Models for Object Recognition</a></p>
<p>8 0.66202235 <a title="140-lda-8" href="./cvpr-2013-Robust_Real-Time_Tracking_of_Multiple_Objects_by_Volumetric_Mass_Densities.html">365 cvpr-2013-Robust Real-Time Tracking of Multiple Objects by Volumetric Mass Densities</a></p>
<p>9 0.65938288 <a title="140-lda-9" href="./cvpr-2013-A_Global_Approach_for_the_Detection_of_Vanishing_Points_and_Mutually_Orthogonal_Vanishing_Directions.html">12 cvpr-2013-A Global Approach for the Detection of Vanishing Points and Mutually Orthogonal Vanishing Directions</a></p>
<p>10 0.65603495 <a title="140-lda-10" href="./cvpr-2013-Integrating_Grammar_and_Segmentation_for_Human_Pose_Estimation.html">225 cvpr-2013-Integrating Grammar and Segmentation for Human Pose Estimation</a></p>
<p>11 0.65562111 <a title="140-lda-11" href="./cvpr-2013-Cross-View_Action_Recognition_via_a_Continuous_Virtual_Path.html">98 cvpr-2013-Cross-View Action Recognition via a Continuous Virtual Path</a></p>
<p>12 0.65509349 <a title="140-lda-12" href="./cvpr-2013-Label_Propagation_from_ImageNet_to_3D_Point_Clouds.html">242 cvpr-2013-Label Propagation from ImageNet to 3D Point Clouds</a></p>
<p>13 0.6548928 <a title="140-lda-13" href="./cvpr-2013-Boundary_Cues_for_3D_Object_Shape_Recovery.html">71 cvpr-2013-Boundary Cues for 3D Object Shape Recovery</a></p>
<p>14 0.65428871 <a title="140-lda-14" href="./cvpr-2013-Spatiotemporal_Deformable_Part_Models_for_Action_Detection.html">408 cvpr-2013-Spatiotemporal Deformable Part Models for Action Detection</a></p>
<p>15 0.65376627 <a title="140-lda-15" href="./cvpr-2013-Structure_Preserving_Object_Tracking.html">414 cvpr-2013-Structure Preserving Object Tracking</a></p>
<p>16 0.65375638 <a title="140-lda-16" href="./cvpr-2013-Single_Image_Calibration_of_Multi-axial_Imaging_Systems.html">400 cvpr-2013-Single Image Calibration of Multi-axial Imaging Systems</a></p>
<p>17 0.65291238 <a title="140-lda-17" href="./cvpr-2013-MODEC%3A_Multimodal_Decomposable_Models_for_Human_Pose_Estimation.html">277 cvpr-2013-MODEC: Multimodal Decomposable Models for Human Pose Estimation</a></p>
<p>18 0.65278471 <a title="140-lda-18" href="./cvpr-2013-Understanding_Indoor_Scenes_Using_3D_Geometric_Phrases.html">446 cvpr-2013-Understanding Indoor Scenes Using 3D Geometric Phrases</a></p>
<p>19 0.65255725 <a title="140-lda-19" href="./cvpr-2013-A_Joint_Model_for_2D_and_3D_Pose_Estimation_from_a_Single_Image.html">14 cvpr-2013-A Joint Model for 2D and 3D Pose Estimation from a Single Image</a></p>
<p>20 0.65240234 <a title="140-lda-20" href="./cvpr-2013-Physically_Plausible_3D_Scene_Tracking%3A_The_Single_Actor_Hypothesis.html">331 cvpr-2013-Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
