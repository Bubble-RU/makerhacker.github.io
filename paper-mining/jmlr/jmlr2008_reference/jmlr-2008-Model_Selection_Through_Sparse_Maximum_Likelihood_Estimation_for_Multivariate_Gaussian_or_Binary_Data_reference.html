<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-62" href="../jmlr2008/jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">jmlr2008-62</a> <a title="jmlr-2008-62-reference" href="#">jmlr2008-62-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</h1>
<br/><p>Source: <a title="jmlr-2008-62-pdf" href="http://jmlr.org/papers/volume9/banerjee08a/banerjee08a.pdf">pdf</a></p><p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><br/>
<h2>reference text</h2><p>M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S. Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E. Richardson, M. Ringwald, G. M. Rubin, and G. Sherlock. Gene ontology: Tool for the uniﬁcation of biology. Nature Genet., 25:25–29, 2000. D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1998. P. J. Bickel and E. Levina. Regularized estimation of large covariance matrices. Annals of Statistics, 36(1):199–227, 2008. J. Dahl, V. Roychowdhury, and L. Vandenberghe. Covariance selection for non-chordal graphs via chordal embedding. To appear in Optimization Methods and Software, Revised 2007. 515  BANERJEE , E L G HAOUI , AND D ’A SPREMONT  A. d’Aspremont, L. El Ghaoui, M.I. Jordan, and G. R. G. Lanckriet. A direct formulation for sparse PCA using semideﬁnite programming. Advances in Neural Information Processing Systems, 17, 2004. A. Dobra and M. West. Bayesian covariance selection. Technical Report, 2004. J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 2007. T. R. Hughes, M. J. Marton, A. R. Jones, C. J. Roberts, R. Stoughton, C. D. Armour, H. A. Bennett, E. Coffey, H. Dai, Y. D. He, M. J. Kidd, A. M. King, M. R. Meyer, D. Slade, P. Y. Lum, S. B. Stepaniants, D. D. Shoemaker, D. Gachotte, K. Chakraburtty, J. Simon, M. Bard, and S. H. Friend. Functional discovery via a compendium of expression proﬁles. Cell, 102(1):109–126, 2000. S. Lauritzen. Graphical Models. Springer Verlag, 1996. H. Li and J. Gui. Gradient directed regularization for sparse gaussian concentration graphs, with applications to inference of genetic networks. University of Pennsylvania Technical Report, 2005. Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, 72(1):7–35, 1992. N. Meinshausen and P. B¨ hlmann. High dimensional graphs and variable selection with the lasso. u Annals of statistics, 34:1436–1462, 2006. G. Natsoulis, L. El Ghaoui, G. Lanckriet, A. Tolley, F. Leroy, S. Dunlea, B. Eynon, C. Pearson, S. Tugendreich, and K. Jarnagin. Classiﬁcation of a large microarray data set: algorithm comparison and analysis of drug signatures. Genome Research, 15:724 –736, 2005. Y. Nesterov. Smooth minimization of non-smooth functions. Math. Prog., 103(1):127–152, 2005. M. R. Osborne, B. Presnell, and B. A. Turlach. On the lasso and its dual. Journal of Computational and Graphical Statistics, 9(2):319–337, 2000. T. P. Speed and H. T. Kiiveri. Gaussian markov distributions over ﬁnite graphs. Annals of Statistics, 14(1):138–150, 1986. R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal statistical society, series B, 58(267-288), 1996. L. Vandenberghe, S. Boyd, and S.-P. Wu. Determinant maximization with linear matrix inequality constraints. SIAM Journal on Matrix Analysis and Applications, 19(4):499 – 533, 1998. M. Wainwright and M. Jordan. Log-determinant relaxation for approximate inference in discrete markov random ﬁelds. IEEE Transactions on Signal Processing, 2006. M. Wainwright, P. Ravikumar, and J. D. Lafferty. High-dimensional graphical model selection using 1 -regularized logistic regression. Proceedings of Advances in Neural Information Processing Systems, 2006.  516</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
