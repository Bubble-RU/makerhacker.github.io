<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-17" href="../jmlr2008/jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">jmlr2008-17</a> <a title="jmlr-2008-17-reference" href="#">jmlr2008-17-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</h1>
<br/><p>Source: <a title="jmlr-2008-17-pdf" href="http://jmlr.org/papers/volume9/hoyle08a/hoyle08a.pdf">pdf</a></p><p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><br/>
<h2>reference text</h2><p>T.W. Anderson. Asymptotic theory for principal component analysis. Annals of Mathematical Statistics, 34:122–148, 1963. 2756  AUTOMATIC PCA D IMENSIONALITY S ELECTION  5  3.5  (a)  (b) E(N Σj>1ln(λ1 - λj ))  3  2  d = 1000 d = 2000 Theory  3  2.5  -1  -1  E(N Σj>iln(λi-λj))  4  1  2  0 0  0.1  0.2  α  0.3  0.4  1.5 0  0.5  5  10  15  20  25  A  Figure 6: Comparison of simulation averages of N −1 Eξ ∑ j>i ln(λi − λ j ) with the limiting theoretical estimates given in Equations (28) and (29). Figure a shows behaviour of the expectation value with α for i = 1, . . . , 5. Solid symbols show simulation averages whilst the solid lines show the corresponding theoretical estimates. We have set d = 1000 and σ2 = 1. The population covariance contains three signal components with A1 = 50, A2 = 30, A3 = 20. Figure b shows comparison of the theoretical result with simulation for different signal strengths, at two different values of the data dimensionality d. We have set α = 0.1, σ2 = 1. The population covariance contains a single signal component with signal strength A. For both Figure a and Figure b simulation averages are taken over 1000 matrices, and error bars of the simulation averages are smaller than the size of the plotted symbols.  J. Baik and J.W. Silverstein. Eigenvalues of large sample covariance matrices of spiked population models. Journal of Multivariate Analysis, 97:1382–1408, 2006. J. Baik, G. Ben Arous, and S. Peche. Phase transition of the largest eigenvalue for non-null complex sample covariance matrices. Annals of Probability, 33:1643–1697, 2005. C.M. Bishop. Bayesian PCA. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in Neural Information Processing Systems, pages 382–388. MIT Press, 1999a. C.M. Bishop. Variational principal components. In Proceedings Ninth International Conference on Artiﬁcial Neural Networks, ICANN’99, pages 509–514. IEE, 1999b. A. Engel and C. Van den Broeck. Statistical Mechanics of Learning. CUP, Cambridge, 2001. T.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M.L. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomﬁeld, and E.S. Lander. Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring. Science, 286: 531–537, 1999. 2757  H OYLE  J.A. Hertz, A. Krogh, and R.G. Palmer. Introduction to the Theory of Neural Computation (Santa Fe Institute Studies in the Sciences of Complexity). Addison-Wesley, Redwood City, CA, 1991. D.C. Hoyle and M. Rattray. PCA learning for sparse high-dimensional data. Europhysics Letters, 62:117–123, 2003. D.C. Hoyle and M. Rattray. Principal-component-analysis eigenvalue spectra from data with symmetry-breaking structure. Physical Review E, 69:026124, 2004a. D.C. Hoyle and M. Rattray. Statistical mechanics of learning multiple orthogonal signals : asymptotic theory and ﬂuctuation effects. Physical Review E, 75:016101, 2007. D.C. Hoyle and M. Rattray. A statistical mechanics analysis of gram matrix eigenvalue spectra. In J. Shawe-Taylor and Y. Singer, editors, Proceedings of COLT’04, Conference on Learning Theory, Banff, Canada, 2004. Lecture Notes in Artiﬁcial Intelligence. Springer-Verlag, 2004b. A.T. James. Normal multivariate analysis and the orthogonal group. Annals of Mathematical Statistics, 25:40–75, 1954. S. John. Some optimal multivariate tests. Biometrika, 58:123–127, 1971. I.M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis. Annals of Statistics, 29:295–327, 2001. ´ I.M. Johnstone. High dimensional statistical inference and random matrices. In M. Sanz-Sol e, J. Soria, J.L. Varona, and J. Verdera, editors, Proceedings of International Congress of Mathematicians, Madrid, 2006. European Mathematical Society Publishing House, 2006. I.T. Joliffe. Principal Component Analysis. Springer-Verlag, New York, 1986. D. Landgrebe. Hyperspectral image data analysis as a high dimensional signal processing problem. IEEE Signal Processing Magazine, 19:17–28, 2002. O. Ledoit and M. Wolf. Some hypothesis tests for the covariance matrix when the dimension is large compared to the sample size. Annals of Statistics, 30:1081–1102, 2002. D.J.C. MacKay. Choice of basis for Laplace approximation. Machine Learning, 33:77–86, 1998. D.J.C MacKay. Bayesian interpolation. Neural Computation, 4:415–447, 1992. V.A. Marˇ enko and L.A. Pastur. Distribution of eigenvalues for some sets of random matrices. c Math. USSR-Sb, 1:457–483, 1967. M. Mezard, G. Parisi, and M. Virasoro. Spin Glass Theory and Beyond. World Scientiﬁc Publishing, Singapore, 1987. T.P. Minka. Automatic choice of dimensionality for PCA. In T.K. Leen, T.G. Dietterich, and V. Tresp, editors, NIPS 13, pages 598–604. MIT Press, 2001a. T.P. Minka. Automatic choice of dimensionality for PCA. Technical Report TR514, M.I.T. Media Laboratory Perceptual Computing Section, 2000. Available from http://vismod.media.mit.edu/tech-reports/TR-514-ABSTRACT.html. 2758  AUTOMATIC PCA D IMENSIONALITY S ELECTION  T.P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th Conference in Uncertainty in Artiﬁcial Intelligence, UAI-2001, pages 362–369, 2001b. H. Nagao. On some test criteria for covariance matrix. Annals of Statistics, 1:700–709, 1973. A.L. Price, N.J. Patterson, R.M. Plenge, M.A. Weinblatt, N.A. Shadick, and D. Reich. Principal components analysis corrects for stratiﬁcation in genome-wide association studies. Nature Genetics, 38:904–909, 2006. P. Reimann, C. Van den Broeck, and G.J. Bex. A gaussian scenario for unsupervised learning. Journal of Physics A:Mathematical and General., 29:3521–3535, 1996. S. Roweis. EM algorithms for PCA and SPCA. In M I. Jordan, M J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems, volume 10. MIT Press, 1998. B. Scholk¨ pf, A. Smola, and K.-R. M¨ ller. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Computation, 10:1299–1319, 1998. M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. J. Royal Statistical Society B, 61:611–622, 1999a. M.E. Tipping and C.M. Bishop. Mixtures of probabilistic principal component analysers. Neural Computation, 11:443–482, 1999b. C.A. Tracy and H. Widom. On orthogonal and symplectic matrix ensembles. Communications in Mathematical Physics, 177:727–754, 1996. R. Urbanczik. Statistical physics of independent component analysis. Europhysics Letters, 64: 564–570, 2003. ˇ ı V. Sm´dl and A. Quinn. On Bayesian principal component analysis. Computational Statistics and Data Analysis, 51:4101–4123, 2007. K.W. Wachter. In David C. Hoaglin & Roy E. Welsch, editor, Proceedings of the Ninth Interface Symposium Computer Science and Statistics, page 299, Boston, 1976. Prindle, Weber and Schmidt. K.W. Wachter. The strong limits of random matrix spectra for sample matrices of independent elements. Annnals Probability, 6:1–18, 1978. R. Wong. Asymptotic Approximations of Integrals. Academic Press, Boston, MA, 1989.  2759</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
